{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Fluid Transformers and Creative Analogies: Exploring Large Language Models' Capacity for Augmenting Cross-Domain Analogical Creativity. (arXiv:2302.12832v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12832","description":"<p>Cross-domain analogical reasoning is a core creative ability that can be\nchallenging for humans. Recent work has shown some proofs-of concept of Large\nlanguage Models' (LLMs) ability to generate cross-domain analogies. However,\nthe reliability and potential usefulness of this capacity for augmenting human\ncreative work has received little systematic exploration. In this paper, we\nsystematically explore LLMs capacity to augment cross-domain analogical\nreasoning. Across three studies, we found: 1) LLM-generated cross-domain\nanalogies were frequently judged as helpful in the context of a problem\nreformulation task (median 4 out of 5 helpfulness rating), and frequently (~80%\nof cases) led to observable changes in problem formulations, and 2) there was\nan upper bound of 25% of outputs bring rated as potentially harmful, with a\nmajority due to potentially upsetting content, rather than biased or toxic\ncontent. These results demonstrate the potential utility -- and risks -- of\nLLMs for augmenting cross-domain analogical creativity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zijian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1\">Arvind Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacNeil_S/0/1/0/all/0/1\">Stephen MacNeil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Joel Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HULAT at SemEval-2023 Task 10: Data augmentation for pre-trained transformers applied to the detection of sexism in social media. (arXiv:2302.12840v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12840","description":"<p>This paper describes our participation in SemEval-2023 Task 10, whose goal is\nthe detection of sexism in social media. We explore some of the most popular\ntransformer models such as BERT, DistilBERT, RoBERTa, and XLNet. We also study\ndifferent data augmentation techniques to increase the training dataset. During\nthe development phase, our best results were obtained by using RoBERTa and data\naugmentation for tasks B and C. However, the use of synthetic data does not\nimprove the results for task C. We participated in the three subtasks. Our\napproach still has much room for improvement, especially in the two\nfine-grained classifications. All our code is available in the repository\nhttps://github.com/isegura/hulat_edos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Segura_Bedmar_I/0/1/0/all/0/1\">Isabel Segura-Bedmar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NoPPA: Non-Parametric Pairwise Attention Random Walk Model for Sentence Representation. (arXiv:2302.12903v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12903","description":"<p>We propose a novel non-parametric/un-trainable language model, named\nNon-Parametric Pairwise Attention Random Walk Model (NoPPA), to generate\nsentence embedding only with pre-trained word embedding and pre-counted word\nfrequency. To the best we know, this study is the first successful attempt to\nbreak the constraint on bag-of-words assumption with a non-parametric attention\nmechanism. We evaluate our method on eight different downstream classification\ntasks. The experiment results show that NoPPA outperforms all kinds of\nbag-of-words-based methods in each dataset and provides a comparable or better\nperformance than the state-of-the-art non-parametric methods on average.\nFurthermore, visualization supports that NoPPA can understand contextual\ntopics, common phrases, and word causalities. Our model is available at\nhttps://github.com/JacksonWuxs/NoPPA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuansheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhiyi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Finetuning for Few-Shot Emotional Speech Recognition. (arXiv:2302.12921v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12921","description":"<p>Speech models have long been known to overfit individual speakers for many\nclassification tasks. This leads to poor generalization in settings where the\nspeakers are out-of-domain or out-of-distribution, as is common in production\nenvironments. We view speaker adaptation as a few-shot learning problem and\npropose investigating transfer learning approaches inspired by recent success\nwith pre-trained models in natural language tasks. We propose pre-finetuning\nspeech models on difficult tasks to distill knowledge into few-shot downstream\nclassification objectives. We pre-finetune Wav2Vec2.0 on every permutation of\nfour multiclass emotional speech recognition corpora and evaluate our\npre-finetuned models through 33,600 few-shot fine-tuning trials on the\nEmotional Speech Dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maximillian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robot Behavior-Tree-Based Task Generation with Large Language Models. (arXiv:2302.12927v1 [cs.RO])","link":"http://arxiv.org/abs/2302.12927","description":"<p>Nowadays, the behavior tree is gaining popularity as a representation for\nrobot tasks due to its modularity and reusability. Designing behavior-tree\ntasks manually is time-consuming for robot end-users, thus there is a need for\ninvestigating automatic behavior-tree-based task generation. Prior\nbehavior-tree-based task generation approaches focus on fixed primitive tasks\nand lack generalizability to new task domains. To cope with this issue, we\npropose a novel behavior-tree-based task generation approach that utilizes\nstate-of-the-art large language models. We propose a Phase-Step prompt design\nthat enables a hierarchical-structured robot task generation and further\nintegrate it with behavior-tree-embedding-based search to set up the\nappropriate prompt. In this way, we enable an automatic and cross-domain\nbehavior-tree task generation. Our behavior-tree-based task generation approach\ndoes not require a set of pre-defined primitive tasks. End-users only need to\ndescribe an abstract desired task and our proposed approach can swiftly\ngenerate the corresponding behavior tree. A full-process case study is provided\nto demonstrate our proposed approach. An ablation study is conducted to\nevaluate the effectiveness of our Phase-Step prompts. Assessment on Phase-Step\nprompts and the limitation of large language models are presented and\ndiscussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">C.S. George Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dependency Dialogue Acts -- Annotation Scheme and Case Study. (arXiv:2302.12944v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12944","description":"<p>In this paper, we introduce Dependency Dialogue Acts (DDA), a novel framework\nfor capturing the structure of speaker-intentions in multi-party dialogues. DDA\ncombines and adapts features from existing dialogue annotation frameworks, and\nemphasizes the multi-relational response structure of dialogues in addition to\nthe dialogue acts and rhetorical relations. It represents the functional,\ndiscourse, and response structure in multi-party multi-threaded conversations.\nA few key features distinguish DDA from existing dialogue annotation frameworks\nsuch as SWBD-DAMSL and the ISO 24617-2 standard. First, DDA prioritizes the\nrelational structure of the dialogue units and the dialog context, annotating\nboth dialog acts and rhetorical relations as response relations to particular\nutterances. Second, DDA embraces overloading in dialogues, encouraging\nannotators to specify multiple response relations and dialog acts for each\ndialog unit. Lastly, DDA places an emphasis on adequately capturing how a\nspeaker is using the full dialog context to plan and organize their speech.\nWith these features, DDA is highly expressive and recall-oriented with regard\nto conversation dynamics between multiple speakers. In what follows, we present\nthe DDA annotation framework and case studies annotating DDA structures in\nmulti-party, multi-threaded conversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jon Z. Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_B/0/1/0/all/0/1\">Brendan King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perkoff_M/0/1/0/all/0/1\">Margaret Perkoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudy_S/0/1/0/all/0/1\">Shiran Dudy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grace_M/0/1/0/all/0/1\">Marie Grace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wojarnik_N/0/1/0/all/0/1\">Natalia Wojarnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_A/0/1/0/all/0/1\">Ananya Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1\">James H. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1\">Martha Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_M/0/1/0/all/0/1\">Marilyn Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1\">Jeffrey Flanigan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust language-based mental health assessments in time and space through social media. (arXiv:2302.12952v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12952","description":"<p>Compared to physical health, population mental health measurement in the U.S.\nis very coarse-grained. Currently, in the largest population surveys, such as\nthose carried out by the Centers for Disease Control or Gallup, mental health\nis only broadly captured through \"mentally unhealthy days\" or \"sadness\", and\nlimited to relatively infrequent state or metropolitan estimates. Through the\nlarge scale analysis of social media data, robust estimation of population\nmental health is feasible at much higher resolutions, up to weekly estimates\nfor counties. In the present work, we validate a pipeline that uses a sample of\n1.2 billion Tweets from 2 million geo-located users to estimate mental health\nchanges for the two leading mental health conditions, depression and anxiety.\nWe find moderate to large associations between the language-based mental health\nassessments and survey scores from Gallup for multiple levels of granularity,\ndown to the county-week (fixed effects $\\beta = .25$ to $1.58$; $p&lt;.001$).\nLanguage-based assessment allows for the cost-effective and scalable monitoring\nof population mental health at weekly time scales. Such spatially fine-grained\ntime series are well suited to monitor effects of societal events and policies\nas well as enable quasi-experimental study designs in population health and\nother disciplines. Beyond mental health in the U.S., this method generalizes to\na broad set of psychological outcomes and allows for community measurement in\nunder-resourced settings where no traditional survey measures - but social\nmedia data - are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mangalik_S/0/1/0/all/0/1\">Siddharth Mangalik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eichstaedt_J/0/1/0/all/0/1\">Johannes C. Eichstaedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_S/0/1/0/all/0/1\">Salvatore Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mun_J/0/1/0/all/0/1\">Jihu Mun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Farhan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gill_G/0/1/0/all/0/1\">Gilvir Gill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_A/0/1/0/all/0/1\">Adithya V. Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subrahmanya_S/0/1/0/all/0/1\">Shashanka Subrahmanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soni_N/0/1/0/all/0/1\">Nikita Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clouston_S/0/1/0/all/0/1\">Sean A. P. Clouston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H. Andrew Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locale Encoding For Scalable Multilingual Keyword Spotting Models. (arXiv:2302.12961v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12961","description":"<p>A Multilingual Keyword Spotting (KWS) system detects spokenkeywords over\nmultiple locales. Conventional monolingual KWSapproaches do not scale well to\nmultilingual scenarios because ofhigh development/maintenance costs and lack of\nresource sharing.To overcome this limit, we propose two locale-conditioned\nuniversalmodels with locale feature concatenation and feature-wise\nlinearmodulation (FiLM). We compare these models with two baselinemethods:\nlocale-specific monolingual KWS, and a single universalmodel trained over all\ndata. Experiments over 10 localized languagedatasets show that\nlocale-conditioned models substantially improveaccuracy over baseline methods\nacross all locales in different noiseconditions.FiLMperformed the best,\nimproving on average FRRby 61% (relative) compared to monolingual KWS models of\nsimilarsizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyun Jin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_A/0/1/0/all/0/1\">Alex Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarpati_A/0/1/0/all/0/1\">Angelo Scorza Scarpati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_I/0/1/0/all/0/1\">Ignacio Lopez Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Optimizing Translations and Speech Timing to Improve Isochrony in Automatic Dubbing. (arXiv:2302.12979v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12979","description":"<p>Automatic dubbing (AD) is the task of translating the original speech in a\nvideo into target language speech. The new target language speech should\nsatisfy isochrony; that is, the new speech should be time aligned with the\noriginal video, including mouth movements, pauses, hand gestures, etc. In this\npaper, we propose training a model that directly optimizes both the translation\nas well as the speech duration of the generated translations. We show that this\nsystem generates speech that better matches the timing of the original speech,\ncompared to prior work, while simplifying the system architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chronopoulou_A/0/1/0/all/0/1\">Alexandra Chronopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1\">Brian Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_P/0/1/0/all/0/1\">Prashant Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virkar_Y/0/1/0/all/0/1\">Yogesh Virkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakew_S/0/1/0/all/0/1\">Surafel M. Lakew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatAug: Leveraging ChatGPT for Text Data Augmentation. (arXiv:2302.13007v1 [cs.CL])","link":"http://arxiv.org/abs/2302.13007","description":"<p>Text data augmentation is an effective strategy for overcoming the challenge\nof limited sample sizes in many natural language processing (NLP) tasks. This\nchallenge is especially prominent in the few-shot learning scenario, where the\ndata in the target domain is generally much scarcer and of lowered quality. A\nnatural and widely-used strategy to mitigate such challenges is to perform data\naugmentation on the training data to better capture the data invariance and\nincrease the sample size. However, current text data augmentation methods\neither can not ensure the correct labeling of the generated data (lacking\nfaithfulness) or can not ensure sufficient diversity in the generated data\n(lacking completeness), or both. Inspired by the recent success of large\nlanguage models, especially the development of ChatGPT, which demonstrated\nimproved language comprehension abilities, in this work, we propose a text data\naugmentation approach based on ChatGPT (named ChatAug). ChatGPT is trained on\ndata with unparalleled linguistic richness and employs a reinforcement training\nprocess with large-scale human feedback, which endows the model with affinity\nto the naturalness of human language. Our text data augmentation approach\nChatAug rephrases each sentence in the training samples into multiple\nconceptually similar but semantically different samples. The augmented samples\ncan then be used in downstream model training. Experiment results on few-shot\nlearning text classification tasks show the superior performance of the\nproposed ChatAug approach over state-of-the-art text data augmentation methods\nin terms of testing accuracy and distribution of the augmented samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haixing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wenxiong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoke Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongmin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Quanzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Choice Fusion as Knowledge for Zero-Shot Dialogue State Tracking. (arXiv:2302.13013v1 [cs.CL])","link":"http://arxiv.org/abs/2302.13013","description":"<p>With the demanding need for deploying dialogue systems in new domains with\nless cost, zero-shot dialogue state tracking (DST), which tracks user's\nrequirements in task-oriented dialogues without training on desired domains,\ndraws attention increasingly. Although prior works have leveraged\nquestion-answering (QA) data to reduce the need for in-domain training in DST,\nthey fail to explicitly model knowledge transfer and fusion for tracking\ndialogue states. To address this issue, we propose CoFunDST, which is trained\non domain-agnostic QA datasets and directly uses candidate choices of\nslot-values as knowledge for zero-shot dialogue-state generation, based on a T5\npre-trained language model. Specifically, CoFunDST selects highly-relevant\nchoices to the reference context and fuses them to initialize the decoder to\nconstrain the model outputs. Our experimental results show that our proposed\nmodel achieves outperformed joint goal accuracy compared to existing zero-shot\nDST approaches in most domains on the MultiWOZ 2.1. Extensive analyses\ndemonstrate the effectiveness of our proposed approach for improving zero-shot\nDST learning from QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1\">Ruolin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Ting-Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juang_B/0/1/0/all/0/1\">Biing-Hwang Juang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SynGen: A Syntactic Plug-and-play Module for Generative Aspect-based Sentiment Analysis. (arXiv:2302.13032v1 [cs.CL])","link":"http://arxiv.org/abs/2302.13032","description":"<p>Aspect-based Sentiment Analysis (ABSA) is a sentiment analysis task at\nfine-grained level. Recently, generative frameworks have attracted increasing\nattention in ABSA due to their ability to unify subtasks and their continuity\nto upstream pre-training tasks. However, these generative models suffer from\nthe neighboring dependency problem that induces neighboring words to get higher\nattention. In this paper, we propose SynGen, a plug-and-play syntactic\ninformation aware module. As a plug-in module, our SynGen can be easily applied\nto any generative framework backbones. The key insight of our module is to add\nsyntactic inductive bias to attention assignment and thus direct attention to\nthe correct target words. To the best of our knowledge, we are the first one to\nintroduce syntactic information to generative ABSA frameworks. Our module\ndesign is based on two main principles: (1) maintaining the structural\nintegrity of backbone PLMs and (2) disentangling the added syntactic\ninformation and original semantic information. Empirical results on four\npopular ABSA datasets demonstrate that SynGen enhanced model achieves a\ncomparable performance to the state-of-the-art model with relaxed labeling\nspecification and less training consumption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chengze Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Taiqiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiayi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xingyu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-in-the-Loop Schema Induction. (arXiv:2302.13048v1 [cs.HC])","link":"http://arxiv.org/abs/2302.13048","description":"<p>Schema induction builds a graph representation explaining how events unfold\nin a scenario. Existing approaches have been based on information retrieval\n(IR) and information extraction(IE), often with limited human curation. We\ndemonstrate a human-in-the-loop schema induction system powered by GPT-3. We\nfirst describe the different modules of our system, including prompting to\ngenerate schematic elements, manual edit of those elements, and conversion of\nthose into a schema graph. By qualitatively comparing our system to previous\nones, we show that our system not only transfers to new domains more easily\nthan previous approaches, but also reduces efforts of human curation thanks to\nour interactive interface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tham_I/0/1/0/all/0/1\">Isaac Tham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhaoyi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jiaxuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Liyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hainiu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Lara J. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dror_R/0/1/0/all/0/1\">Rotem Dror</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1\">Martha Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_S/0/1/0/all/0/1\">Susan Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suchocki_R/0/1/0/all/0/1\">Reece Suchocki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HADES: Homologous Automated Document Exploration and Summarization. (arXiv:2302.13099v1 [cs.CL])","link":"http://arxiv.org/abs/2302.13099","description":"<p>This paper introduces HADES, a novel tool for automatic comparative documents\nwith similar structures. HADES is designed to streamline the work of\nprofessionals dealing with large volumes of documents, such as policy\ndocuments, legal acts, and scientific papers. The tool employs a multi-step\npipeline that begins with processing PDF documents using topic modeling,\nsummarization, and analysis of the most important words for each topic. The\nprocess concludes with an interactive web app with visualizations that\nfacilitate the comparison of the documents. HADES has the potential to\nsignificantly improve the productivity of professionals dealing with high\nvolumes of documents, reducing the time and effort required to complete tasks\nrelated to comparative document analysis. Our package is publically available\non GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilczynski_P/0/1/0/all/0/1\">Piotr Wilczy&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zolkowski_A/0/1/0/all/0/1\">Artur &#x17b;&#xf3;&#x142;kowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krzyzinski_M/0/1/0/all/0/1\">Mateusz Krzyzi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisnios_E/0/1/0/all/0/1\">Emilia Wi&#x15b;nios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pielinski_B/0/1/0/all/0/1\">Bartosz Pieli&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gizinski_S/0/1/0/all/0/1\">Stanis&#x142;aw Gizi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sienkiewicz_J/0/1/0/all/0/1\">Julian Sienkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biecek_P/0/1/0/all/0/1\">Przemys&#x142;aw Biecek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic-Selective Graph Network for Topic-Focused Summarization. (arXiv:2302.13106v1 [cs.CL])","link":"http://arxiv.org/abs/2302.13106","description":"<p>Due to the success of the pre-trained language model (PLM), existing\nPLM-based summarization models show their powerful generative capability.\nHowever, these models are trained on general-purpose summarization datasets,\nleading to generated summaries failing to satisfy the needs of different\nreaders. To generate summaries with topics, many efforts have been made on\ntopic-focused summarization. However, these works generate a summary only\nguided by a prompt comprising topic words. Despite their success, these methods\nstill ignore the disturbance of sentences with non-relevant topics and only\nconduct cross-interaction between tokens by attention module. To address this\nissue, we propose a topic-arc recognition objective and topic-selective graph\nnetwork. First, the topic-arc recognition objective is used to model training,\nwhich endows the capability to discriminate topics for the model. Moreover, the\ntopic-selective graph network can conduct topic-guided cross-interaction on\nsentences based on the results of topic-arc recognition. In the experiments, we\nconduct extensive evaluations on NEWTS and COVIDET datasets. Results show that\nour methods achieve state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zesheng_S/0/1/0/all/0/1\">Shi Zesheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yucheng_Z/0/1/0/all/0/1\">Zhou Yucheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Query Encoding For Complex Query Answering on Knowledge Graphs. (arXiv:2302.13114v1 [cs.CL])","link":"http://arxiv.org/abs/2302.13114","description":"<p>Query encoding (QE) is proposed as a fast and robust solution to CQA. In the\nencoding process, most existing QE methods first parse the logical query into\nan executable computational direct-acyclic graph (DAG), then use neural\nnetworks to parameterize the operators, and finally, recursively execute these\nneuralized operators. However, the parameterization-and-execution paradigm may\nbe potentially over-complicated, as it can be structurally simplified by a\nsingle neural network encoder. Meanwhile, sequence encoders, like LSTM and\nTransformer, proved to be effective for encoding semantic graphs in related\ntasks. Motivated by this, we propose sequential query encoding (SQE) as an\nalternative to encode queries for CQA. Instead of parameterizing and executing\nthe computational graph, SQE first uses a search-based algorithm to linearize\nthe computational graph to a sequence of tokens and then uses a sequence\nencoder to compute its vector representation. Then this vector representation\nis used as a query embedding to retrieve answers from the embedding space\naccording to similarity scores. Despite its simplicity, SQE demonstrates\nstate-of-the-art neural query encoding performance on FB15k, FB15k-237, and\nNELL on an extended benchmark including twenty-nine types of in-distribution\nqueries. Further experiment shows that SQE also demonstrates comparable\nknowledge inference capability on out-of-distribution queries, whose query\ntypes are not observed during the training process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaxin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tianshi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abstractive Text Summarization using Attentive GRU based Encoder-Decoder. (arXiv:2302.13117v1 [cs.CL])","link":"http://arxiv.org/abs/2302.13117","description":"<p>In todays era huge volume of information exists everywhere. Therefore, it is\nvery crucial to evaluate that information and extract useful, and often\nsummarized, information out of it so that it may be used for relevant purposes.\nThis extraction can be achieved through a crucial technique of artificial\nintelligence, namely, machine learning. Indeed automatic text summarization has\nemerged as an important application of machine learning in text processing. In\nthis paper, an english text summarizer has been built with GRU-based encoder\nand decoder. Bahdanau attention mechanism has been added to overcome the\nproblem of handling long sequences in the input text. A news-summary dataset\nhas been used to train the model. The output is observed to outperform\ncompetitive models in the literature. The generated summary can be used as a\nnewspaper headline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rehman_T/0/1/0/all/0/1\">Tohida Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Suchandan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_D/0/1/0/all/0/1\">Debarshi Kumar Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Samiran Chattopadhyay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Fairness in Text Generation via Mutual Information Minimization based on Importance Sampling. (arXiv:2302.13136v1 [cs.CL])","link":"http://arxiv.org/abs/2302.13136","description":"<p>Pretrained language models (PLMs), such as GPT2, have achieved remarkable\nempirical performance in text generation tasks. However, pretrained on\nlarge-scale natural language corpora, the generated text from PLMs may exhibit\nsocial bias against disadvantaged demographic groups. To improve the fairness\nof PLMs in text generation, we propose to minimize the mutual information\nbetween the semantics in the generated text sentences and their demographic\npolarity, i.e., the demographic group to which the sentence is referring. In\nthis way, the mentioning of a demographic group (e.g., male or female) is\nencouraged to be independent from how it is described in the generated text,\nthus effectively alleviating the social bias. Moreover, we propose to\nefficiently estimate the upper bound of the above mutual information via\nimportance sampling, leveraging a natural language corpus. We also propose a\ndistillation mechanism that preserves the language modeling ability of the PLMs\nafter debiasing. Empirical results on real-world benchmarks demonstrate that\nthe proposed method yields superior performance in term of both fairness and\nlanguage modeling ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pengyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henao_R/0/1/0/all/0/1\">Ricardo Henao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-based Learning for Text Readability Assessment. (arXiv:2302.13139v1 [cs.CL])","link":"http://arxiv.org/abs/2302.13139","description":"<p>We propose the novel adaptation of a pre-trained seq2seq model for\nreadability assessment. We prove that a seq2seq model - T5 or BART - can be\nadapted to discern which text is more difficult from two given texts\n(pairwise). As an exploratory study to prompt-learn a neural network for text\nreadability in a text-to-text manner, we report useful tips for future work in\nseq2seq training and ranking-based approach to readability assessment.\nSpecifically, we test nine input-output formats/prefixes and show that they can\nsignificantly influence the final model performance.\n</p>\n<p>Also, we argue that the combination of text-to-text training and pairwise\nranking setup 1) enables leveraging multiple parallel text simplification data\nfor teaching readability and 2) trains a neural model for the general concept\nof readability (therefore, better cross-domain generalization). At last, we\nreport a 99.6% pairwise classification accuracy on Newsela and a 98.7% for\nOneStopEnglish, through a joint training approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bruce W. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason Hyung-Jong Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STACC: Code Comment Classification using SentenceTransformers. (arXiv:2302.13149v1 [cs.SE])","link":"http://arxiv.org/abs/2302.13149","description":"<p>Code comments are a key resource for information about software artefacts.\nDepending on the use case, only some types of comments are useful. Thus,\nautomatic approaches to classify these comments are proposed. In this work, we\naddress this need by proposing, STACC, a set of SentenceTransformers-based\nbinary classifiers. These lightweight classifiers are trained and tested on the\nNLBSE Code Comment Classification tool competition dataset, and surpass the\nbaseline by a significant margin, achieving an average F1 score of 0.74 against\nthe baseline of 0.31, which is an improvement of 139%. A replication package,\nas well as the models themselves, are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Kaswan_A/0/1/0/all/0/1\">Ali Al-Kaswan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izadi_M/0/1/0/all/0/1\">Maliheh Izadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deursen_A/0/1/0/all/0/1\">Arie van Deursen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Machine-Paraphrased Plagiarism. (arXiv:2103.11909v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11909","description":"<p>Employing paraphrasing tools to conceal plagiarized text is a severe threat\nto academic integrity. To enable the detection of machine-paraphrased text, we\nevaluate the effectiveness of five pre-trained word embedding models combined\nwith machine-learning classifiers and eight state-of-the-art neural language\nmodels. We analyzed preprints of research papers, graduation theses, and\nWikipedia articles, which we paraphrased using different configurations of the\ntools SpinBot and SpinnerChief. The best-performing technique, Longformer,\nachieved an average F1 score of 81.0% (F1=99.7% for SpinBot and F1=71.6% for\nSpinnerChief cases), while human evaluators achieved F1=78.4% for SpinBot and\nF1=65.6% for SpinnerChief cases. We show that the automated classification\nalleviates shortcomings of widely-used text-matching systems, such as Turnitin\nand PlagScan. To facilitate future research, all data, code, and two web\napplications showcasing our contributions are openly available at\nhttps://github.com/jpwahle/iconf22-paraphrase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foltynek_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Folt&#xfd;nek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness Challenges in Model Distillation and Pruning for Natural Language Understanding. (arXiv:2110.08419v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08419","description":"<p>Recent work has focused on compressing pre-trained language models (PLMs)\nlike BERT where the major focus has been to improve the in-distribution\nperformance for downstream tasks. However, very few of these studies have\nanalyzed the impact of compression on the generalizability and robustness of\ncompressed models for out-of-distribution (OOD) data. Towards this end, we\nstudy two popular model compression techniques including knowledge distillation\nand pruning and show that the compressed models are significantly less robust\nthan their PLM counterparts on OOD test sets although they obtain similar\nperformance on in-distribution development sets for a task. Further analysis\nindicates that the compressed models overfit on the shortcut samples and\ngeneralize poorly on the hard ones. We further leverage this observation to\ndevelop a regularization strategy for robust model compression based on sample\nuncertainty. Experimental results on several natural language understanding\ntasks demonstrate that our bias mitigation framework improves the OOD\ngeneralization of the compressed models, while not sacrificing the\nin-distribution task performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Mengnan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1\">Milad Shokouhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Question Answering-Based Signals into Abstractive Summarization via Salient Span Selection. (arXiv:2111.07935v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07935","description":"<p>In this work, we propose a method for incorporating question-answering (QA)\nsignals into a summarization model. Our method identifies salient noun phrases\n(NPs) in the input document by automatically generating wh-questions that are\nanswered by the NPs and automatically determining whether those questions are\nanswered in the gold summaries. This QA-based signal is incorporated into a\ntwo-stage summarization model which first marks salient NPs in the input\ndocument using a classification model, then conditionally generates a summary.\nOur experiments demonstrate that the models trained using QA-based supervision\ngenerate higher-quality summaries than baseline methods of identifying salient\nspans on benchmark summarization datasets. Further, we show that the content of\nthe generated summaries can be controlled based on which NPs are marked in the\ninput document. Finally, we propose a method of augmenting the training data so\nthe gold summaries are more consistent with the marked input spans used during\ntraining and show how this results in models which learn to better exclude\nunmarked document content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploration into Translation-Equivariant Image Quantization. (arXiv:2112.00384v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00384","description":"<p>This is an exploratory study that discovers the current image quantization\n(vector quantization) do not satisfy translation equivariance in the quantized\nspace due to aliasing. Instead of focusing on anti-aliasing, we propose a\nsimple yet effective way to achieve translation-equivariant image quantization\nby enforcing orthogonality among the codebook embeddings. To explore the\nadvantages of translation-equivariant image quantization, we conduct three\nproof-of-concept experiments with a carefully controlled dataset: (1)\ntext-to-image generation, where the quantized image indices are the target to\npredict, (2) image-to-text generation, where the quantized image indices are\ngiven as a condition, (3) using a smaller training set to analyze sample\nefficiency. From the strictly controlled experiments, we empirically verify\nthat the translation-equivariant image quantizer improves not only sample\nefficiency but also the accuracy over VQGAN up to +11.9% in text-to-image\ngeneration and +3.9% in image-to-text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Woncheol Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gyubok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyou_E/0/1/0/all/0/1\">Eunyi Lyou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joonseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIREX: A Unified Learning Framework for Language Model Rationale Extraction. (arXiv:2112.08802v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08802","description":"<p>An extractive rationale explains a language model's (LM's) prediction on a\ngiven task instance by highlighting the text inputs that most influenced the\nprediction. Ideally, rationale extraction should be faithful (reflective of\nLM's actual behavior) and plausible (convincing to humans), without\ncompromising the LM's (i.e., task model's) task performance. Although\nattribution algorithms and select-predict pipelines are commonly used in\nrationale extraction, they both rely on certain heuristics that hinder them\nfrom satisfying all three desiderata. In light of this, we propose UNIREX, a\nflexible learning framework that generalizes rationale extractor optimization\nas follows: (1) specify architecture for a learned rationale extractor; (2)\nselect explainability objectives (i.e., faithfulness and plausibility\ncriteria); and (3) jointly the train task model and rationale extractor on the\ntask using the selected objectives. UNIREX enables replacing prior works'\nheuristic design choices with a generic learned rationale extractor in (1) and\noptimizing it for all three desiderata in (2)-(3). To facilitate comparison\nbetween methods with respect to multiple desiderata, we introduce the\nNormalized Relative Gain (NRG) metric. Across five text classification\ndatasets, our best UNIREX configuration outperforms baselines by an average of\n32.9% NRG. Plus, we find that UNIREX-trained rationale extractors can even\ngeneralize to unseen datasets and tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1\">Maziar Sanjabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathias_L/0/1/0/all/0/1\">Lambert Mathias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Liang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Shaoliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaochang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resources for Turkish Natural Language Processing: A critical survey. (arXiv:2204.05042v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05042","description":"<p>This paper presents a comprehensive survey of corpora and lexical resources\navailable for Turkish. We review a broad range of resources, focusing on the\nones that are publicly available. In addition to providing information about\nthe available linguistic resources, we present a set of recommendations, and\nidentify gaps in the data available for conducting research and building\napplications in Turkish Linguistics and Natural Language Processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coltekin_C/0/1/0/all/0/1\">&#xc7;a&#x11f;r&#x131; &#xc7;&#xf6;ltekin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogruoz_A/0/1/0/all/0/1\">A. Seza Do&#x11f;ru&#xf6;z</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cetinoglu_O/0/1/0/all/0/1\">&#xd6;zlem &#xc7;etino&#x11f;lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XQA-DST: Multi-Domain and Multi-Lingual Dialogue State Tracking. (arXiv:2204.05895v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05895","description":"<p>Dialogue State Tracking (DST), a crucial component of task-oriented dialogue\n(ToD) systems, keeps track of all important information pertaining to dialogue\nhistory: filling slots with the most probable values throughout the\nconversation. Existing methods generally rely on a predefined set of values and\nstruggle to generalise to previously unseen slots in new domains. To overcome\nthese challenges, we propose a domain-agnostic extractive question answering\n(QA) approach with shared weights across domains. To disentangle the complex\ndomain information in ToDs, we train our DST with a novel domain filtering\nstrategy by excluding out-of-domain question samples. With an independent\nclassifier that predicts the presence of multiple domains given the context,\nour model tackles DST by extracting spans in active domains. Empirical results\ndemonstrate that our model can efficiently leverage domain-agnostic QA datasets\nby two-stage fine-tuning while being both domain-scalable and open-vocabulary\nin DST. It shows strong transferability by achieving zero-shot\ndomain-adaptation results on MultiWOZ 2.1 with an average JGA of 36.7%. It\nfurther achieves cross-lingual transfer with state-of-the-art zero-shot\nresults, 66.2% JGA from English to German and 75.7% JGA from English to Italian\non WOZ 2.0.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Han Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iacobacci_I/0/1/0/all/0/1\">Ignacio Iacobacci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Structure Distillation with Centered Kernel Alignment in BERT Transferring. (arXiv:2204.08922v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08922","description":"<p>Knowledge distillation is an approach to transfer information on\nrepresentations from a teacher to a student by reducing their difference. A\nchallenge of this approach is to reduce the flexibility of the student's\nrepresentations inducing inaccurate learning of the teacher's knowledge. To\nresolve it in transferring, we investigate distillation of structures of\nrepresentations specified to three types: intra-feature, local inter-feature,\nglobal inter-feature structures. To transfer them, we introduce feature\nstructure distillation methods based on the Centered Kernel Alignment, which\nassigns a consistent value to similar features structures and reveals more\ninformative relations. In particular, a memory-augmented transfer method with\nclustering is implemented for the global structures. The methods are\nempirically analyzed on the nine tasks for language understanding of the GLUE\ndataset with Bidirectional Encoder Representations from Transformers (BERT),\nwhich is a representative neural language model. In the results, the proposed\nmethods effectively transfer the three types of structures and improve\nperformance compared to state-of-the-art distillation methods. Indeed, the code\nfor the methods is available in https://github.com/maroo-sky/FSD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hee-Jun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_S/0/1/0/all/0/1\">Seung-Hoon Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kangil Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-level Alignment Training Scheme for Video-and-Language Grounding. (arXiv:2204.10938v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10938","description":"<p>To solve video-and-language grounding tasks, the key is for the network to\nunderstand the connection between the two modalities. For a pair of video and\nlanguage description, their semantic relation is reflected by their encodings'\nsimilarity. A good multi-modality encoder should be able to well capture both\ninputs' semantics and encode them in the shared feature space where embedding\ndistance gets properly translated into their semantic similarity. In this work,\nwe focused on this semantic connection between video and language, and\ndeveloped a multi-level alignment training scheme to directly shape the\nencoding process. Global and segment levels of video-language alignment pairs\nwere designed, based on the information similarity ranging from high-level\ncontext to fine-grained semantics. The contrastive loss was used to contrast\nthe encodings' similarities between the positive and negative alignment pairs,\nand to ensure the network is trained in such a way that similar information is\nencoded closely in the shared feature space while information of different\nsemantics is kept apart. Our multi-level alignment training can be applied to\nvarious video-and-language grounding tasks. Together with the task-specific\ntraining loss, our framework achieved comparable performance to previous\nstate-of-the-arts on multiple video QA and retrieval datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yubo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_F/0/1/0/all/0/1\">Feiyang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_Q/0/1/0/all/0/1\">Qing Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handling and Presenting Harmful Text in NLP Research. (arXiv:2204.14256v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.14256","description":"<p>Text data can pose a risk of harm. However, the risks are not fully\nunderstood, and how to handle, present, and discuss harmful text in a safe way\nremains an unresolved issue in the NLP community. We provide an analytical\nframework categorising harms on three axes: (1) the harm type (e.g.,\nmisinformation, hate speech or racial stereotypes); (2) whether a harm is\n\\textit{sought} as a feature of the research design if explicitly studying\nharmful content (e.g., training a hate speech classifier), versus\n\\textit{unsought} if harmful content is encountered when working on unrelated\nproblems (e.g., language generation or part-of-speech tagging); and (3) who it\naffects, from people (mis)represented in the data to those handling the data\nand those publishing on the data. We provide advice for practitioners, with\nconcrete steps for mitigating harm in research and in publication. To assist\nimplementation we introduce \\textsc{HarmCheck} -- a documentation standard for\nhandling and presenting harmful text in research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birhane_A/0/1/0/all/0/1\">Abeba Birhane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertie Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually-Augmented Language Modeling. (arXiv:2205.10178v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10178","description":"<p>Human language is grounded on multimodal knowledge including visual knowledge\nlike colors, sizes, and shapes. However, current large-scale pre-trained\nlanguage models rely on text-only self-supervised training with massive text\ndata, which precludes them from utilizing relevant visual information when\nnecessary. To address this, we propose a novel pre-training framework, named\nVaLM, to Visually-augment text tokens with retrieved relevant images for\nLanguage Modeling. Specifically, VaLM builds on a novel latent text-image\nalignment method via an image retrieval module to fetch corresponding images\ngiven a textual context. With the visually-augmented context, VaLM uses a\nvisual knowledge fusion layer to enable multimodal grounded language modeling\nby attending to both text context and visual knowledge in images. We evaluate\nVaLM on various visual knowledge-intensive commonsense reasoning tasks, which\nrequire visual information to excel. The experimental results illustrate that\nVaLM outperforms all strong language-only and vision-language baselines with\nsubstantial gains in reasoning object commonsense including color, size, and\nshape. Our code is available at https://github.com/Victorwz/VaLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haoyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ER-Test: Evaluating Explanation Regularization Methods for Language Models. (arXiv:2205.12542v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12542","description":"<p>By explaining how humans would solve a given task, human rationales can\nprovide strong learning signal for neural language models (LMs). Explanation\nregularization (ER) aims to improve LM generalization by pushing the LM's\nmachine rationales (Which input tokens did the LM focus on?) to align with\nhuman rationales (Which input tokens would humans focus on?). Though prior\nworks primarily study ER via in-distribution (ID) evaluation,\nout-of-distribution (OOD) generalization is often more critical in real-world\nscenarios, yet ER's effect on OOD generalization has been underexplored. In\nthis paper, we introduce ER-Test, a framework for evaluating ER models' OOD\ngeneralization along three dimensions: unseen dataset tests, contrast set\ntests, and functional tests. Using ER-Test, we extensively analyze how ER\nmodels' OOD generalization varies with different ER design choices. Across two\ntasks and six datasets, ER-Test shows that ER has little impact on ID\nperformance but can yield large OOD performance gains. Also, we find that ER\ncan improve OOD performance even with limited rationale supervision. ER-Test's\nresults help demonstrate ER's utility and establish best practices for using ER\neffectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_B/0/1/0/all/0/1\">Brihi Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Shaoliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1\">Maziar Sanjabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning. (arXiv:2205.12679v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12679","description":"<p>There is a rising interest in further exploring the zero-shot learning\npotential of large pre-trained language models (PLMs). A new paradigm called\ndata-generation-based zero-shot learning has achieved impressive success. In\nthis paradigm, the synthesized data from the PLM acts as the carrier of\nknowledge, which is used to train a task-specific model with orders of\nmagnitude fewer parameters than the PLM, achieving both higher performance and\nefficiency than prompt-based zero-shot learning methods on PLMs. The main\nhurdle of this approach is that the synthesized data from PLM usually contains\na significant portion of low-quality samples. Fitting on such data will greatly\nhamper the performance of the task-specific model, making it unreliable for\ndeployment. Previous methods remedy this issue mainly by filtering synthetic\ndata using heuristic metrics(e.g., output confidence), or refining the data\nwith the help of a human expert, which comes with excessive manual tuning or\nexpensive costs. In this paper, we propose a novel noise-robust re-weighting\nframework SunGen to automatically construct high-quality data for zero-shot\nclassification problems. Our framework features the ability to learn the sample\nweights indicating data quality without requiring any human annotation. We\ntheoretically and empirically verify the ability of our method to help\nconstruct good-quality synthetic datasets. Notably, SunGen-LSTM yields a 9.8%\nrelative improvement than the baseline on average accuracy across eight\ndifferent established text classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pi_R/0/1/0/all/0/1\">Renjie Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiacheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weizhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Probability of Agreement. (arXiv:2208.06161v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.06161","description":"<p>Measuring inter-annotator agreement is important for annotation tasks, but\nmany metrics require a fully-annotated set of data, where all annotators\nannotate all samples. We define Sparse Probability of Agreement, SPA, which\nestimates the probability of agreement when not all annotator-item-pairs are\navailable. We show that under certain conditions, SPA is an unbiased estimator,\nand we provide multiple weighing schemes for handling data with various degrees\nof annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Norregaard_J/0/1/0/all/0/1\">Jeppe N&#xf8;rregaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NECE: Narrative Event Chain Extraction Toolkit. (arXiv:2208.08063v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2208.08063","description":"<p>To understand a narrative, it is essential to comprehend its main characters\nand the associated major events; however, this can be challenging with lengthy\nand unstructured narrative texts. To address this, we introduce NECE, an\nopen-access, document-level toolkit that automatically extracts and aligns\nnarrative events in the temporal order of their occurrence using sliding window\nmethod. Through extensive human evaluations, we have confirmed the high quality\nof the NECE toolkit, and external validation has demonstrated its potential for\napplication in downstream tasks such as question answering and bias analysis.\nThe NECE toolkit includes both a Python library and a user-friendly web\ninterface; the latter offers custom visualizations of event chains and easy\nnavigation between graphics and text to improve reading efficiency and\nexperience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangxuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isaza_P/0/1/0/all/0/1\">Paulina Toro Isaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Moshi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oloko_A/0/1/0/all/0/1\">Akintoye Oloko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanctos_C/0/1/0/all/0/1\">Cassia Sanctos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adebiyi_A/0/1/0/all/0/1\">Aminat Adebiyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yufang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction and Applications of Billion-Scale Pre-trained Multimodal Business Knowledge Graph. (arXiv:2209.15214v5 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2209.15214","description":"<p>Business Knowledge Graphs (KGs) are important to many enterprises today,\nproviding factual knowledge and structured data that steer many products and\nmake them more intelligent. Despite their promising benefits, building business\nKG necessitates solving prohibitive issues of deficient structure and multiple\nmodalities. In this paper, we advance the understanding of the practical\nchallenges related to building KG in non-trivial real-world systems. We\nintroduce the process of building an open business knowledge graph (OpenBG)\nderived from a well-known enterprise, Alibaba Group. Specifically, we define a\ncore ontology to cover various abstract products and consumption demands, with\nfine-grained taxonomy and multimodal facts in deployed applications. OpenBG is\nan open business KG of unprecedented scale: 2.6 billion triples with more than\n88 million entities covering over 1 million core classes/concepts and 2,681\ntypes of relations. We release all the open resources (OpenBG benchmarks)\nderived from it for the community and report experimental results of KG-centric\ntasks. We also run up an online competition based on OpenBG benchmarks, and has\nattracted thousands of teams. We further pre-train OpenBG and apply it to many\nKG- enhanced downstream tasks in business scenarios, demonstrating the\neffectiveness of billion-scale multimodal knowledge for e-commerce. All the\nresources with codes have been released at\n\\url{https://github.com/OpenBGBenchmark/OpenBG}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zelin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hehong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PQLM -- Multilingual Decentralized Portable Quantum Language Model for Privacy Protection. (arXiv:2210.03221v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.03221","description":"<p>With careful manipulation, malicious agents can reverse engineer private\ninformation encoded in pre-trained language models. Security concerns motivate\nthe development of quantum pre-training. In this work, we propose a highly\nPortable Quantum Language Model (PQLM) that can easily transmit information to\ndownstream tasks on classical machines. The framework consists of a cloud PQLM\nbuilt with random Variational Quantum Classifiers (VQC) and local models for\ndownstream applications. We demonstrate the ad hoc portability of the quantum\nmodel by extracting only the word embeddings and effectively applying them to\ndownstream tasks on classical machines. Our PQLM exhibits comparable\nperformance to its classical counterpart on both intrinsic evaluation (loss,\nperplexity) and extrinsic evaluation (multilingual sentiment analysis accuracy)\nmetrics. We also perform ablation studies on the factors affecting PQLM\nperformance to analyze model stability. Our work establishes a theoretical\nfoundation for a portable quantum pre-trained language model that could be\ntrained on private data and made available for public use with privacy\nprotection guarantees.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyue Stella Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_H/0/1/0/all/0/1\">Hongchao Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1\">Ruixing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hexin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_L/0/1/0/all/0/1\">Leibny Paola Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Segmentation Approaches for Neural Machine Translation of Code-Switched Egyptian Arabic-English Text. (arXiv:2210.06990v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06990","description":"<p>Data sparsity is one of the main challenges posed by code-switching (CS),\nwhich is further exacerbated in the case of morphologically rich languages. For\nthe task of machine translation (MT), morphological segmentation has proven\nsuccessful in alleviating data sparsity in monolingual contexts; however, it\nhas not been investigated for CS settings. In this paper, we study the\neffectiveness of different segmentation approaches on MT performance, covering\nmorphology-based and frequency-based segmentation techniques. We experiment on\nMT from code-switched Arabic-English to English. We provide detailed analysis,\nexamining a variety of conditions, such as data size and sentences with\ndifferent degrees of CS. Empirical results show that morphology-aware\nsegmenters perform the best in segmentation tasks but under-perform in MT.\nNevertheless, we find that the choice of the segmentation setup to use for MT\nis highly dependent on the data size. For extreme low-resource scenarios, a\ncombination of frequency and morphology-based segmentations is shown to perform\nthe best. For more resourced settings, such a combination does not bring\nsignificant improvements over the use of frequency-based segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaser_M/0/1/0/all/0/1\">Marwa Gaser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mager_M/0/1/0/all/0/1\">Manuel Mager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamed_I/0/1/0/all/0/1\">Injy Hamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdennadher_S/0/1/0/all/0/1\">Slim Abdennadher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task. (arXiv:2210.13382v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.13382","description":"<p>Language models show a surprising range of capabilities, but the source of\ntheir apparent competence is unclear. Do these networks just memorize a\ncollection of surface statistics, or do they rely on internal representations\nof the process that generates the sequences they see? We investigate this\nquestion by applying a variant of the GPT model to the task of predicting legal\nmoves in a simple board game, Othello. Although the network has no a priori\nknowledge of the game or its rules, we uncover evidence of an emergent\nnonlinear internal representation of the board state. Interventional\nexperiments indicate this representation can be used to control the output of\nthe network and create \"latent saliency maps\" that can help explain predictions\nin human terms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kenneth Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopkins_A/0/1/0/all/0/1\">Aspen K. Hopkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viegas_F/0/1/0/all/0/1\">Fernanda Vi&#xe9;gas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1\">Martin Wattenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Kernels and Channel Attention for Low Resource Speaker Verification. (arXiv:2211.02000v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2211.02000","description":"<p>State-of-the-art speaker verification frameworks have typically focused on\ndeveloping models with increasingly deeper (more layers) and wider (number of\nchannels) models to improve their verification performance. Instead, this paper\nproposes an approach to increase the model resolution capability using\nattention-based dynamic kernels in a convolutional neural network to adapt the\nmodel parameters to be feature-conditioned. The attention weights on the\nkernels are further distilled by channel attention and multi-layer feature\naggregation to learn global features from speech. This approach provides an\nefficient solution to improving representation capacity with lower data\nresources. This is due to the self-adaptation to inputs of the structures of\nthe model parameters. The proposed dynamic convolutional model achieved 1.62\\%\nEER and 0.18 miniDCF on the VoxCeleb1 test set and has a 17\\% relative\nimprovement compared to the ECAPA-TDNN using the same training resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ollerenshaw_A/0/1/0/all/0/1\">Anna Ollerenshaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalal_M/0/1/0/all/0/1\">Md Asif Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hain_T/0/1/0/all/0/1\">Thomas Hain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VieCap4H-VLSP 2021: ObjectAoA -- Enhancing performance of Object Relation Transformer with Attention on Attention for Vietnamese image captioning. (arXiv:2211.05405v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.05405","description":"<p>Image captioning is currently a challenging task that requires the ability to\nboth understand visual information and use human language to describe this\nvisual information in the image. In this paper, we propose an efficient way to\nimprove the image understanding ability of transformer-based method by\nextending Object Relation Transformer architecture with Attention on Attention\nmechanism. Experiments on the VieCap4H dataset show that our proposed method\nsignificantly outperforms its original structure on both the public test and\nprivate test of the Image Captioning shared task held by VLSP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nghia Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Duong T.D. Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1\">Minh-Quan Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discharge Summary Hospital Course Summarisation of In Patient Electronic Health Record Text with Clinical Concept Guided Deep Pre-Trained Transformer Models. (arXiv:2211.07126v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07126","description":"<p>Brief Hospital Course (BHC) summaries are succinct summaries of an entire\nhospital encounter, embedded within discharge summaries, written by senior\nclinicians responsible for the overall care of a patient. Methods to\nautomatically produce summaries from inpatient documentation would be\ninvaluable in reducing clinician manual burden of summarising documents under\nhigh time-pressure to admit and discharge patients. Automatically producing\nthese summaries from the inpatient course, is a complex, multi-document\nsummarisation task, as source notes are written from various perspectives (e.g.\nnursing, doctor, radiology), during the course of the hospitalisation. We\ndemonstrate a range of methods for BHC summarisation demonstrating the\nperformance of deep learning summarisation models across extractive and\nabstractive summarisation scenarios. We also test a novel ensemble extractive\nand abstractive summarisation model that incorporates a medical concept\nontology (SNOMED) as a clinical guidance signal and shows superior performance\nin 2 real-world clinical data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Searle_T/0/1/0/all/0/1\">Thomas Searle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_Z/0/1/0/all/0/1\">Zina Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_J/0/1/0/all/0/1\">James Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1\">Richard Dobson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Mutual Learning for Cued Speech Recognition. (arXiv:2212.01083v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2212.01083","description":"<p>Automatic Cued Speech Recognition (ACSR) provides an intelligent\nhuman-machine interface for visual communications, where the Cued Speech (CS)\nsystem utilizes lip movements and hand gestures to code spoken language for\nhearing-impaired people. Previous ACSR approaches often utilize direct feature\nconcatenation as the main fusion paradigm. However, the asynchronous modalities\ni.e., lip, hand shape and hand position) in CS may cause interference for\nfeature concatenation. To address this challenge, we propose a transformer\nbased cross-modal mutual learning framework to prompt multi-modal interaction.\nCompared with the vanilla self-attention, our model forces modality-specific\ninformation of different modalities to pass through a modality-invariant\ncodebook, collating linguistic representations for tokens of each modality.\nThen the shared linguistic knowledge is used to re-synchronize multi-modal\nsequences. Moreover, we establish a novel large-scale multi-speaker CS dataset\nfor Mandarin Chinese. To our knowledge, this is the first work on ACSR for\nMandarin Chinese. Extensive experiments are conducted for different languages\ni.e., Chinese, French, and British English). Results demonstrate that our model\nexhibits superior recognition performance to the state-of-the-art by a large\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Knowledge Graph Reasoning on Graph Types: Static, Dynamic, and Multimodal. (arXiv:2212.05767v6 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.05767","description":"<p>Knowledge graph reasoning (KGR), aiming to deduce new facts from existing\nfacts based on mined logic rules underlying knowledge graphs (KGs), has become\na fast-growing research direction. It has been proven to significantly benefit\nthe usage of KGs in many AI applications, such as question answering and\nrecommendation systems, etc. According to the graph types, the existing KGR\nmodels can be roughly divided into three categories, i.e., static models,\ntemporal models, and multi-modal models. The early works in this domain mainly\nfocus on static KGR and tend to directly apply general knowledge graph\nembedding models to the reasoning task. However, these models are not suitable\nfor more complex but practical tasks, such as inductive static KGR, temporal\nKGR, and multi-modal KGR. To this end, multiple works have been developed\nrecently, but no survey papers and open-source repositories comprehensively\nsummarize and discuss models in this important direction. To fill the gap, we\nconduct a survey for knowledge graph reasoning tracing from static to temporal\nand then to multi-modal KGs. Concretely, the preliminaries, summaries of KGR\nmodels, and typical datasets are introduced and discussed consequently.\nMoreover, we discuss the challenges and potential opportunities. The\ncorresponding open-source repository is shared on GitHub:\nhttps://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Ke Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingyuan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wenxuan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sihang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling. (arXiv:2301.00591v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.00591","description":"<p>This work profoundly analyzes discrete self-supervised speech representations\nthrough the eyes of Generative Spoken Language Modeling (GSLM). Following the\nfindings of such an analysis, we propose practical improvements to the discrete\nunit for the GSLM. First, we start comprehending these units by analyzing them\nin three axes: interpretation, visualization, and resynthesis. Our analysis\nfinds a high correlation between the speech units to phonemes and phoneme\nfamilies, while their correlation with speaker or gender is weaker.\nAdditionally, we found redundancies in the extracted units and claim that one\nreason may be the units' context. Following this analysis, we propose a new,\nunsupervised metric to measure unit redundancies. Finally, we use this metric\nto develop new methods that improve the robustness of units clustering and show\nsignificant improvement considering zero-resource speech metrics such as ABX.\nCode and analysis tools are available under the following link.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sicherman_A/0/1/0/all/0/1\">Amitay Sicherman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Conversational Search Behavior For Domain Exploration. (arXiv:2301.04098v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.04098","description":"<p>Conversational search has evolved as a new information retrieval paradigm,\nmarking a shift from traditional search systems towards interactive dialogues\nwith intelligent search agents. This change especially affects exploratory\ninformation-seeking contexts, where conversational search systems can guide the\ndiscovery of unfamiliar domains. In these scenarios, users find it often\ndifficult to express their information goals due to insufficient background\nknowledge. Conversational interfaces can provide assistance by eliciting\ninformation needs and narrowing down the search space. However, due to the\ncomplexity of information-seeking behavior, the design of conversational\ninterfaces for retrieving information remains a great challenge. Although prior\nwork has employed user studies to empirically ground the system design, most\nexisting studies are limited to well-defined search tasks or known domains,\nthus being less exploratory in nature. Therefore, we conducted a laboratory\nstudy to investigate open-ended search behavior for navigation through unknown\ninformation landscapes. The study comprised of 26 participants who were\nrestricted in their search to a text chat interface. Based on the collected\ndialogue transcripts, we applied statistical analyses and process mining\ntechniques to uncover general information-seeking patterns across five\ndifferent domains. We not only identify core dialogue acts and their\ninterrelations that enable users to discover domain knowledge, but also derive\ndesign suggestions for conversational search systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Phillip Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afzal_A/0/1/0/all/0/1\">Anum Afzal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vladika_J/0/1/0/all/0/1\">Juraj Vladika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_D/0/1/0/all/0/1\">Daniel Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Memorize Entailment and Discourse Relations for Persona-Consistent Dialogues. (arXiv:2301.04871v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.04871","description":"<p>Maintaining engagement and consistency is particularly important in dialogue\nsystems. Existing works have improved the performance of dialogue systems by\nintentionally learning interlocutor personas with sophisticated network\nstructures. One issue with this approach is that it requires more personal\ncorpora with annotations. Additionally, these models typically perform the next\nutterance prediction to generate a response but neglect the discourse coherence\nin the entire conversation. To address these issues, this study proposes a\nmethod of learning to memorize entailment and discourse relations for\npersona-consistent dialogue tasks. Entailment text pairs in natural language\ninference dataset were applied to learn latent entailment relations as external\nmemories by premise-to-hypothesis generation task. Furthermore, an internal\nmemory with a similar architecture was applied to the discourse information in\nthe dialogue. Placing orthogonality restrictions on these two memory spaces\nensures that the latent entailment relations remain dialogue-independent. Both\nmemories collaborate to obtain entailment and discourse representation for the\ngeneration, allowing a deeper understanding of both consistency and coherence.\nExperiments on two large public datasets, PersonaChat and DSTC7-AVSD,\ndemonstrated the effectiveness of the proposed method. Both automatic and human\nevaluations indicate that the proposed model outperforms several strong\nbaselines in terms of both persona consistency and response coherence. Our\nsource code is available at https://github.com/Chenrj233/LMEDR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Liang-Chih Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuejie Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection. (arXiv:2301.07779v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.07779","description":"<p>Neural sequence generation models are known to \"hallucinate\", by producing\noutputs that are unrelated to the source text. These hallucinations are\npotentially harmful, yet it remains unclear in what conditions they arise and\nhow to mitigate their impact. In this work, we first identify internal model\nsymptoms of hallucinations by analyzing the relative token contributions to the\ngeneration in contrastive hallucinated vs. non-hallucinated outputs generated\nvia source perturbations. We then show that these symptoms are reliable\nindicators of natural hallucinations, by using them to design a lightweight\nhallucination detector which outperforms both model-free baselines and strong\nclassifiers based on quality estimation or large pre-trained models on manually\nannotated English-Chinese and German-English translation test beds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weijia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sweta Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briakou_E/0/1/0/all/0/1\">Eleftheria Briakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martindale_M/0/1/0/all/0/1\">Marianna J. Martindale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1\">Marine Carpuat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heterogeneous Federated Knowledge Graph Embedding Learning and Unlearning. (arXiv:2302.02069v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.02069","description":"<p>Federated Learning (FL) recently emerges as a paradigm to train a global\nmachine learning model across distributed clients without sharing raw data.\nKnowledge Graph (KG) embedding represents KGs in a continuous vector space,\nserving as the backbone of many knowledge-driven applications. As a promising\ncombination, federated KG embedding can fully take advantage of knowledge\nlearned from different clients while preserving the privacy of local data.\nHowever, realistic problems such as data heterogeneity and knowledge forgetting\nstill remain to be concerned. In this paper, we propose FedLU, a novel FL\nframework for heterogeneous KG embedding learning and unlearning. To cope with\nthe drift between local optimization and global convergence caused by data\nheterogeneity, we propose mutual knowledge distillation to transfer local\nknowledge to global, and absorb global knowledge back. Moreover, we present an\nunlearning method based on cognitive neuroscience, which combines retroactive\ninterference and passive decay to erase specific knowledge from local clients\nand propagate to the global model by reusing knowledge distillation. We\nconstruct new datasets for assessing realistic performance of the\nstate-of-the-arts. Extensive experiments show that FedLU achieves superior\nresults in both link prediction and knowledge forgetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiangrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guangyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.02676","description":"<p>Learning from human preferences is important for language models to be\nhelpful and useful for humans, and to align with human and social values. Prior\nwork have achieved remarkable successes by learning from human feedback to\nunderstand and follow instructions. Nonetheless, these methods are either\nfounded on hand-picked model generations that are favored by human annotators,\nrendering them ineffective in terms of data utilization and challenging to\napply in general, or they depend on reward functions and reinforcement\nlearning, which are prone to imperfect reward function and extremely\nchallenging to optimize. In this work, we propose a novel technique, Chain of\nHindsight, that is easy to optimize and can learn from any form of feedback,\nregardless of its polarity. Our idea is inspired by how humans learn from\nextensive feedback presented in the form of languages. We convert all types of\nfeedback into sentences, which are then used to fine-tune the model, allowing\nus to take advantage of the language comprehension capabilities of language\nmodels. We condition the model on a sequence of model generations paired with\nfeedback. By doing so, models are trained to generate outputs based on\nfeedback, and models can learn to identify and correct negative attributes or\nerrors. Applying our method to large language models, we observed that Chain of\nHindsight significantly surpasses previous methods in aligning language models\nwith human preferences. We observed significant improvements on summarization\nand dialogue tasks and our approach is markedly preferred in human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sferrazza_C/0/1/0/all/0/1\">Carmelo Sferrazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks. (arXiv:2302.08043v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.08043","description":"<p>Graphs can model complex relationships between objects, enabling a myriad of\nWeb applications such as online page/article classification and social\nrecommendation. While graph neural networks(GNNs) have emerged as a powerful\ntool for graph representation learning, in an end-to-end supervised setting,\ntheir performance heavily rely on a large amount of task-specific supervision.\nTo reduce labeling requirement, the \"pre-train, fine-tune\" and \"pre-train,\nprompt\" paradigms have become increasingly common. In particular, prompting is\na popular alternative to fine-tuning in natural language processing, which is\ndesigned to narrow the gap between pre-training and downstream objectives in a\ntask-specific manner. However, existing study of prompting on graphs is still\nlimited, lacking a universal treatment to appeal to different downstream tasks.\nIn this paper, we propose GraphPrompt, a novel pre-training and prompting\nframework on graphs. GraphPrompt not only unifies pre-training and downstream\ntasks into a common task template, but also employs a learnable prompt to\nassist a downstream task in locating the most relevant knowledge from the\npre-train model in a task-specific manner. Finally, we conduct extensive\nexperiments on five public datasets to evaluate and analyze GraphPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zemin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xingtong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinming Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks. (arXiv:2302.08399v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.08399","description":"<p>Intuitive psychology is a pillar of common-sense reasoning. The replication\nof this reasoning in machine intelligence is an important stepping-stone on the\nway to human-like artificial intelligence. Several recent tasks and benchmarks\nfor examining this reasoning in Large-Large Models have focused in particular\non belief attribution in Theory-of-Mind tasks. These tasks have shown both\nsuccesses and failures. We consider in particular a recent purported success\ncase, and show that small variations that maintain the principles of ToM turn\nthe results on their head. We argue that in general, the zero-hypothesis for\nmodel evaluation in intuitive psychology should be skeptical, and that outlying\nfailure cases should outweigh average success rates. We also consider what\npossible future successes on Theory-of-Mind tasks by more powerful LLMs would\nmean for ToM tasks with people.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1\">Tomer Ullman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark. (arXiv:2302.09432v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.09432","description":"<p>To advance Chinese financial natural language processing (NLP), we introduce\nBBT-FinT5, a new Chinese financial pre-training language model based on the T5\nmodel. To support this effort, we have built BBT-FinCorpus, a large-scale\nfinancial corpus with approximately 300GB of raw text from four different\nsources. In general domain NLP, comprehensive benchmarks like GLUE and\nSuperGLUE have driven significant advancements in language model pre-training\nby enabling head-to-head comparisons among models. Drawing inspiration from\nthese benchmarks, we propose BBT-CFLEB, a Chinese Financial Language\nunderstanding and generation Evaluation Benchmark, which includes six datasets\ncovering both understanding and generation tasks. Our aim is to facilitate\nresearch in the development of NLP within the Chinese financial domain. Our\nmodel, corpus and benchmark are released at\nhttps://github.com/ssymmetry/BBT-FinCUGE-Applications. Our work belongs to the\nBig Bang Transformer (BBT), a large-scale pre-trained language model project.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Dakuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hengkui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yipei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yipeng Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Mengkun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1\">Yingsi Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E2E Spoken Entity Extraction for Virtual Agents. (arXiv:2302.10186v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2302.10186","description":"<p>This paper reimagines some aspects of speech processing using speech\nencoders, specifically about extracting entities directly from speech, with no\nintermediate textual representation. In human-computer conversations,\nextracting entities such as names, postal addresses and email addresses from\nspeech is a challenging task. In this paper, we study the impact of fine-tuning\npre-trained speech encoders on extracting spoken entities in human-readable\nform directly from speech without the need for text transcription. We\nillustrate that such a direct approach optimizes the encoder to transcribe only\nthe entity relevant portions of speech, ignoring the superfluous portions such\nas carrier phrases and spellings of entities. In the context of dialogs from an\nenterprise virtual agent, we demonstrate that the 1-step approach outperforms\nthe typical 2-step cascade of first generating lexical transcriptions followed\nby text-based entity extraction for identifying spoken entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Singla_K/0/1/0/all/0/1\">Karan Singla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_Y/0/1/0/all/0/1\">Yeon-Jun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Price_R/0/1/0/all/0/1\">Ryan Price</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jalalvand_S/0/1/0/all/0/1\">Shahab Jalalvand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bangalore_S/0/1/0/all/0/1\">Srinivas Bangalore</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2302.11713","description":"<p>Large language models have demonstrated an emergent capability in answering\nknowledge intensive questions. With recent progress on web-scale visual and\nlanguage pre-training, do these models also understand how to answer visual\ninformation seeking questions? To answer this question, we present InfoSeek, a\nVisual Question Answering dataset that focuses on asking information-seeking\nquestions, where the information can not be answered by common sense knowledge.\nWe perform a multi-stage human annotation to collect a natural distribution of\nhigh-quality visual information seeking question-answer pairs. We also\nconstruct a large-scale, automatically collected dataset by combining existing\nvisual entity recognition datasets and Wikidata, which provides over one\nmillion examples for model fine-tuning and validation. Based on InfoSeek, we\nanalyzed various pre-trained Visual QA systems to gain insights into the\ncharacteristics of different pre-trained models. Our analysis shows that it is\nchallenging for the state-of-the-art multi-modal pre-trained models to answer\nvisual information seeking questions, but this capability is improved through\nfine-tuning on the automated InfoSeek dataset. We hope our analysis paves the\nway to understand and develop the next generation of multi-modal pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1\">Yi Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haitian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (arXiv:2302.12095v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.12095","description":"<p>ChatGPT is a recent chatbot service released by OpenAI and is receiving\nincreasing attention over the past few months. While evaluations of various\naspects of ChatGPT have been done, its robustness, i.e., the performance to\nunexpected inputs, is still unclear to the public. Robustness is of particular\nconcern in responsible AI, especially for safety-critical applications. In this\npaper, we conduct a thorough evaluation of the robustness of ChatGPT from the\nadversarial and out-of-distribution (OOD) perspective. To do so, we employ the\nAdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart\nreview and DDXPlus medical diagnosis datasets for OOD evaluation. We select\nseveral popular foundation models as baselines. Results show that ChatGPT shows\nconsistent advantages on most adversarial and OOD classification and\ntranslation tasks. However, the absolute performance is far from perfection,\nwhich suggests that adversarial and OOD robustness remains a significant threat\nto foundation models. Moreover, ChatGPT shows astounding performance in\nunderstanding dialogue-related texts and we find that it tends to provide\ninformal suggestions for medical tasks instead of definitive answers. Finally,\nwe present in-depth discussions of possible research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xixu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Runkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haojun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxin Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Prompting with Chain-of-Thought for Large Language Models. (arXiv:2302.12246v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12246","description":"<p>The increasing scale of large language models (LLMs) brings emergent\nabilities to various complex tasks requiring reasoning, such as arithmetic and\ncommonsense reasoning. It is known that the effective design of task-specific\nprompts is critical for LLMs' ability to produce high-quality answers. In\nparticular, an effective approach for complex question-and-answer tasks is\nexample-based prompting with chain-of-thought (CoT) reasoning, which\nsignificantly improves the performance of LLMs. However, current CoT methods\nrely on a fixed set of human-annotated exemplars, which are not necessarily the\nmost effective examples for different tasks. This paper proposes a new method,\nActive-Prompt, to adapt LLMs to different tasks with task-specific example\nprompts (annotated with human-designed CoT reasoning). For this purpose, we\npropose a solution to the key problem of determining which questions are the\nmost important and helpful ones to annotate from a pool of task-specific\nqueries. By borrowing ideas from the related problem of uncertainty-based\nactive learning, we introduce several metrics to characterize the uncertainty\nso as to select the most uncertain questions for annotation. Experimental\nresults demonstrate the superiority of our proposed method, achieving\nstate-of-the-art on eight complex reasoning tasks. Further analyses of\ndifferent uncertainty metrics, pool sizes, zero-shot learning, and\naccuracy-uncertainty relationship demonstrate the effectiveness of our method.\nOur code will be available at https://github.com/shizhediao/active-prompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1\">Shizhe Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengcheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing AI performance on less frequent aspects of language reveals insensitivity to underlying meaning. (arXiv:2302.12313v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12313","description":"<p>Advances in computational methods and big data availability have recently\ntranslated into breakthroughs in AI applications. With successes in bottom-up\nchallenges partially overshadowing shortcomings, the 'human-like' performance\nof Large Language Models has raised the question of how linguistic performance\nis achieved by algorithms. Given systematic shortcomings in generalization\nacross many AI systems, in this work we ask whether linguistic performance is\nindeed guided by language knowledge in Large Language Models. To this end, we\nprompt GPT-3 with a grammaticality judgement task and comprehension questions\non less frequent constructions that are thus unlikely to form part of Large\nLanguage Models' training data. These included grammatical 'illusions',\nsemantic anomalies, complex nested hierarchies and self-embeddings. GPT-3\nfailed for every prompt but one, often offering answers that show a critical\nlack of understanding even of high-frequency words used in these less frequent\ngrammatical constructions. The present work sheds light on the boundaries of\nthe alleged AI human-like linguistic competence and argues that, far from\nhuman-like, the next-word prediction abilities of LLMs may face issues of\nrobustness, when pushed beyond training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dentella_V/0/1/0/all/0/1\">Vittoria Dentella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_E/0/1/0/all/0/1\">Elliot Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_G/0/1/0/all/0/1\">Gary Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leivada_E/0/1/0/all/0/1\">Evelina Leivada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models are Few-shot Learners for Prognostic Prediction. (arXiv:2302.12692v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12692","description":"<p>Clinical prediction is an essential task in the healthcare industry. However,\nthe recent success of transformers, on which large language models are built,\nhas not been extended to this domain. In this research, we explore the use of\ntransformers and language models in prognostic prediction for immunotherapy\nusing real-world patients' clinical data and molecular profiles. This paper\ninvestigates the potential of transformers to improve clinical prediction\ncompared to conventional machine learning approaches and addresses the\nchallenge of few-shot learning in predicting rare disease areas. The study\nbenchmarks the efficacy of baselines and language models on prognostic\nprediction across multiple cancer types and investigates the impact of\ndifferent pretrained language models under few-shot regimes. The results\ndemonstrate significant improvements in accuracy and highlight the potential of\nNLP in clinical research to improve early detection and intervention for\ndifferent diseases. Anonymous codes are available at\n\\url{https://anonymous.4open.science/r/table2text-88ED}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zekai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balan_M/0/1/0/all/0/1\">Mariann Micsinai Balan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_K/0/1/0/all/0/1\">Kevin Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Transfer of Cognitive Processing Complexity. (arXiv:2302.12695v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12695","description":"<p>When humans read a text, their eye movements are influenced by the structural\ncomplexity of the input sentences. This cognitive phenomenon holds across\nlanguages and recent studies indicate that multilingual language models utilize\nstructural similarities between languages to facilitate cross-lingual transfer.\nWe use sentence-level eye-tracking patterns as a cognitive indicator for\nstructural complexity and show that the multilingual model XLM-RoBERTa can\nsuccessfully predict varied patterns for 13 typologically diverse languages,\ndespite being fine-tuned only on English data. We quantify the sensitivity of\nthe model to structural complexity and distinguish a range of complexity\ncharacteristics. Our results indicate that the model develops a meaningful bias\ntowards sentence length but also integrates cross-lingual differences. We\nconduct a control experiment with randomized word order and find that the model\nseems to additionally capture more complex structural information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pouw_C/0/1/0/all/0/1\">Charlotte Pouw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollenstein_N/0/1/0/all/0/1\">Nora Hollenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beinborn_L/0/1/0/all/0/1\">Lisa Beinborn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Massively Multilingual ASR With Auxiliary CTC Objectives. (arXiv:2302.12829v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12829","description":"<p>Multilingual Automatic Speech Recognition (ASR) models have extended the\nusability of speech technologies to a wide variety of languages. With how many\nlanguages these models have to handle, however, a key to understanding their\nimbalanced performance across different languages is to examine if the model\nactually knows which language it should transcribe. In this paper, we introduce\nour work on improving performance on FLEURS, a 102-language open ASR benchmark,\nby conditioning the entire model on language identity (LID). We investigate\ntechniques inspired from recent Connectionist Temporal Classification (CTC)\nstudies to help the model handle the large number of languages, conditioning on\nthe LID predictions of auxiliary tasks. Our experimental results demonstrate\nthe effectiveness of our technique over standard CTC/Attention-based hybrid\nmodels. Furthermore, our state-of-the-art systems using self-supervised models\nwith the Conformer architecture improve over the results of prior work on\nFLEURS by a relative 28.4% CER. Trained models and reproducible recipes are\navailable at https://github.com/espnet/espnet/tree/master/egs2/fleurs/asr1 .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">William Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiti_S/0/1/0/all/0/1\">Soumi Maiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}