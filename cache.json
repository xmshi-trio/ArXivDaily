{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-23T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"DEPAC: a Corpus for Depression and Anxiety Detection from Speech. (arXiv:2306.12443v1 [eess.AS])","link":"http://arxiv.org/abs/2306.12443","description":"<p>Mental distress like depression and anxiety contribute to the largest\nproportion of the global burden of diseases. Automated diagnosis systems of\nsuch disorders, empowered by recent innovations in Artificial Intelligence, can\npave the way to reduce the sufferings of the affected individuals. Development\nof such systems requires information-rich and balanced corpora. In this work,\nwe introduce a novel mental distress analysis audio dataset DEPAC, labeled\nbased on established thresholds on depression and anxiety standard screening\ntools. This large dataset comprises multiple speech tasks per individual, as\nwell as relevant demographic information. Alongside, we present a feature set\nconsisting of hand-curated acoustic and linguistic features, which were found\neffective in identifying signs of mental illnesses in human speech. Finally, we\njustify the quality and effectiveness of our proposed audio corpus and feature\nset in predicting depression severity by comparing the performance of baseline\nmachine learning models built on this dataset with baseline models trained on\nother well-known depression corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tasnim_M/0/1/0/all/0/1\">Mashrura Tasnim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ehghaghi_M/0/1/0/all/0/1\">Malikeh Ehghaghi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diep_B/0/1/0/all/0/1\">Brian Diep</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Novikova_J/0/1/0/all/0/1\">Jekaterina Novikova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Misinformation as Information Pollution. (arXiv:2306.12466v1 [cs.SI])","link":"http://arxiv.org/abs/2306.12466","description":"<p>Social media feed algorithms are designed to optimize online social\nengagements for the purpose of maximizing advertising profits, and therefore\nhave an incentive to promote controversial posts including misinformation. By\nthinking about misinformation as information pollution, we can draw parallels\nwith environmental policy for countering pollution such as carbon taxes.\nSimilar to pollution, a Pigouvian tax on misinformation provides economic\nincentives for social media companies to control the spread of misinformation\nmore effectively to avoid or reduce their misinformation tax, while preserving\nsome degree of freedom in platforms' response. In this paper, we highlight a\nbird's eye view of a Pigouvian misinformation tax and discuss the key questions\nand next steps for implementing such a taxing scheme.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1\">Ashkan Kazemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference. (arXiv:2306.12509v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12509","description":"<p>We view large language models (LLMs) as stochastic \\emph{language layers} in\na network, where the learnable parameters are the natural language\n\\emph{prompts} at each layer. We stack two such layers, feeding the output of\none layer to the next. We call the stacked architecture a \\emph{Deep Language\nNetwork} (DLN). We first show how to effectively perform prompt optimization\nfor a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs\n(DLN-2), where two prompts must be learnt. We consider the output of the first\nlayer as a latent variable to marginalize, and devise a variational inference\nalgorithm for joint prompt training. A DLN-2 reaches higher performance than a\nsingle layer, sometimes comparable to few-shot GPT-4 even when each LLM in the\nnetwork is smaller and less powerful. The DLN code is open source:\nhttps://github.com/microsoft/deep-language-networks .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_M/0/1/0/all/0/1\">Matheus Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trischler_A/0/1/0/all/0/1\">Adam Trischler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Ziang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_A/0/1/0/all/0/1\">Arian Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niedtner_F/0/1/0/all/0/1\">Friederike Niedtner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1\">Nicolas Le Roux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Evaluation of Document Classification using RVL-CDIP. (arXiv:2306.12550v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12550","description":"<p>The RVL-CDIP benchmark is widely used for measuring performance on the task\nof document classification. Despite its widespread use, we reveal several\nundesirable characteristics of the RVL-CDIP benchmark. These include (1)\nsubstantial amounts of label noise, which we estimate to be 8.1% (ranging\nbetween 1.6% to 16.9% per document category); (2) presence of many ambiguous or\nmulti-label documents; (3) a large overlap between test and train splits, which\ncan inflate model performance metrics; and (4) presence of sensitive\npersonally-identifiable information like US Social Security numbers (SSNs). We\nargue that there is a risk in using RVL-CDIP for benchmarking document\nclassifiers, as its limited scope, presence of errors (state-of-the-art models\nnow achieve accuracy error rates that are within our estimated label error\nrate), and lack of diversity make it less than ideal for benchmarking. We\nfurther advocate for the creation of a new document classification benchmark,\nand provide recommendations for what characteristics such a resource should\ninclude.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Larson_S/0/1/0/all/0/1\">Stefan Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_G/0/1/0/all/0/1\">Gordon Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leach_K/0/1/0/all/0/1\">Kevin Leach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning. (arXiv:2306.12552v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12552","description":"<p>Recently, commonsense reasoning in text generation has attracted much\nattention. Generative commonsense reasoning is the task that requires machines,\ngiven a group of keywords, to compose a single coherent sentence with\ncommonsense plausibility. While existing datasets targeting generative\ncommonsense reasoning focus on everyday scenarios, it is unclear how well\nmachines reason under specific geographical and temporal contexts. We formalize\nthis challenging task as SituatedGen, where machines with commonsense should\ngenerate a pair of contrastive sentences given a group of keywords including\ngeographical or temporal entities. We introduce a corresponding English dataset\nconsisting of 8,268 contrastive sentence pairs, which are built upon several\nexisting commonsense reasoning benchmarks with minimal manual labor.\nExperiments show that state-of-the-art generative language models struggle to\ngenerate sentences with commonsense plausibility and still lag far behind human\nperformance. Our dataset is publicly available at\nhttps://github.com/yunx-z/situated_gen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases. (arXiv:2306.12567v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12567","description":"<p>This paper investigates whether current large language models exhibit biases\nin logical reasoning, similar to humans. Specifically, we focus on syllogistic\nreasoning, a well-studied form of inference in the cognitive science of human\ndeduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO,\noriginally designed for psychological experiments that assess human logical\nabilities in syllogistic reasoning. The dataset consists of syllogistic\ninferences in both English and Japanese. We examine three types of biases\nobserved in human syllogistic reasoning: belief biases, conversion errors, and\natmosphere effects. Our findings demonstrate that current large language models\nstruggle more with problems involving these three types of biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ando_R/0/1/0/all/0/1\">Risako Ando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morishita_T/0/1/0/all/0/1\">Takanobu Morishita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abe_H/0/1/0/all/0/1\">Hirohiko Abe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mineshima_K/0/1/0/all/0/1\">Koji Mineshima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okada_M/0/1/0/all/0/1\">Mitsuhiro Okada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NoRefER: a Referenceless Quality Metric for Automatic Speech Recognition via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning. (arXiv:2306.12577v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12577","description":"<p>This paper introduces NoRefER, a novel referenceless quality metric for\nautomatic speech recognition (ASR) systems. Traditional reference-based metrics\nfor evaluating ASR systems require costly ground-truth transcripts. NoRefER\novercomes this limitation by fine-tuning a multilingual language model for\npair-wise ranking ASR hypotheses using contrastive learning with Siamese\nnetwork architecture. The self-supervised NoRefER exploits the known quality\nrelationships between hypotheses from multiple compression levels of an ASR for\nlearning to rank intra-sample hypotheses by quality, which is essential for\nmodel comparisons. The semi-supervised version also uses a referenced dataset\nto improve its inter-sample quality ranking, which is crucial for selecting\npotentially erroneous samples. The results indicate that NoRefER correlates\nhighly with reference-based metrics and their intra-sample ranks, indicating a\nhigh potential for referenceless ASR evaluation or a/b testing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuksel_K/0/1/0/all/0/1\">Kamer Ali Yuksel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_T/0/1/0/all/0/1\">Thiago Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javadi_G/0/1/0/all/0/1\">Golara Javadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Badrashiny_M/0/1/0/all/0/1\">Mohamed El-Badrashiny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_A/0/1/0/all/0/1\">Ahmet Gunduz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morphological Inflection with Phonological Features. (arXiv:2306.12581v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12581","description":"<p>Recent years have brought great advances into solving morphological tasks,\nmostly due to powerful neural models applied to various tasks as (re)inflection\nand analysis. Yet, such morphological tasks cannot be considered solved,\nespecially when little training data is available or when generalizing to\npreviously unseen lemmas. This work explores effects on performance obtained\nthrough various ways in which morphological models get access to subcharacter\nphonological features that are the targets of morphological processes. We\ndesign two methods to achieve this goal: one that leaves models as is but\nmanipulates the data to include features instead of characters, and another\nthat manipulates models to take phonological features into account when\nbuilding representations for phonemes. We elicit phonemic data from standard\ngraphemic data using language-specific grammars for languages with shallow\ngrapheme-to-phoneme mapping, and we experiment with two reinflection models\nover eight languages. Our results show that our methods yield comparable\nresults to the grapheme-based baseline overall, with minor improvements in some\nof the languages. All in all, we conclude that patterns in character\ndistributions are likely to allow models to infer the underlying phonological\ncharacteristics, even when phonemes are not explicitly represented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guriel_D/0/1/0/all/0/1\">David Guriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews. (arXiv:2306.12587v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12587","description":"<p>Revising scientific papers based on peer feedback is a challenging task that\nrequires not only deep scientific knowledge and reasoning, but also the ability\nto recognize the implicit requests in high-level feedback and to choose the\nbest of many possible ways to update the manuscript in response. We introduce\nthis task for large language models and release ARIES, a dataset of review\ncomments and their corresponding paper edits, to enable training and evaluating\nmodels. We study two versions of the task: comment-edit alignment and edit\ngeneration, and evaluate several baselines, including GPT-4. We find that\nmodels struggle even to identify the edits that correspond to a comment,\nespecially in cases where the comment is phrased in an indirect way or where\nthe edit addresses the spirit of a comment but not the precise request. When\ntasked with generating edits, GPT-4 often succeeds in addressing comments on a\nsurface level, but it rigidly follows the wording of the feedback rather than\nthe underlying intent, and includes fewer technical details than human-written\nedits. We hope that our formalization, dataset, and analysis will form a\nfoundation for future work in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DArcy_M/0/1/0/all/0/1\">Mike D&#x27;Arcy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Alexis Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bransom_E/0/1/0/all/0/1\">Erin Bransom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1\">Jonathan Bragg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hierarchical Approach to exploiting Multiple Datasets from TalkBank. (arXiv:2306.12596v1 [cs.DB])","link":"http://arxiv.org/abs/2306.12596","description":"<p>TalkBank is an online database that facilitates the sharing of linguistics\nresearch data. However, the existing TalkBank's API has limited data filtering\nand batch processing capabilities. To overcome these limitations, this paper\nintroduces a pipeline framework that employs a hierarchical search approach,\nenabling efficient complex data selection. This approach involves a quick\npreliminary screening of relevant corpora that a researcher may need, and then\nperform an in-depth search for target data based on specific criteria. The\nidentified files are then indexed, providing easier access for future analysis.\nFurthermore, the paper demonstrates how data from different studies curated\nwith the framework can be integrated by standardizing and cleaning metadata,\nallowing researchers to extract insights from a large, integrated dataset.\nWhile being designed for TalkBank, the framework can also be adapted to process\ndata from other open-science platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_M/0/1/0/all/0/1\">Man Ho Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Incremental Learning based on Label Generation. (arXiv:2306.12619v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12619","description":"<p>Despite the great success of pre-trained language models, it is still a\nchallenge to use these models for continual learning, especially for the\nclass-incremental learning (CIL) setting due to catastrophic forgetting (CF).\nThis paper reports our finding that if we formulate CIL as a continual label\ngeneration problem, CF is drastically reduced and the generalizable\nrepresentations of pre-trained models can be better retained. We thus propose a\nnew CIL method (VAG) that also leverages the sparsity of vocabulary to focus\nthe generation and creates pseudo-replay samples by using label semantics.\nExperimental results show that VAG outperforms baselines by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yijia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiduo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying and Extracting Rare Disease Phenotypes with Large Language Models. (arXiv:2306.12656v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12656","description":"<p>Rare diseases (RDs) are collectively common and affect 300 million people\nworldwide. Accurate phenotyping is critical for informing diagnosis and\ntreatment, but RD phenotypes are often embedded in unstructured text and\ntime-consuming to extract manually. While natural language processing (NLP)\nmodels can perform named entity recognition (NER) to automate extraction, a\nmajor bottleneck is the development of a large, annotated corpus for model\ntraining. Recently, prompt learning emerged as an NLP paradigm that can lead to\nmore generalizable results without any (zero-shot) or few labeled samples\n(few-shot). Despite growing interest in ChatGPT, a revolutionary large language\nmodel capable of following complex human prompts and generating high-quality\nresponses, none have studied its NER performance for RDs in the zero- and\nfew-shot settings. To this end, we engineered novel prompts aimed at extracting\nRD phenotypes and, to the best of our knowledge, are the first the establish a\nbenchmark for evaluating ChatGPT's performance in these settings. We compared\nits performance to the traditional fine-tuning approach and conducted an\nin-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted in\nhigher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the\nzero- and few-shot settings, respectively). Despite this, ChatGPT achieved\nsimilar or higher accuracy for certain entities (i.e., rare diseases and signs)\nin the one-shot setting (F1 of 0.776 and 0.725). This suggests that with\nappropriate prompt engineering, ChatGPT has the potential to match or\noutperform fine-tuned language models for certain entity types with just one\nlabeled sample. While the proliferation of large language models may provide\nopportunities for supporting RD diagnosis and treatment, researchers and\nclinicians should critically evaluate model outputs and be well-informed of\ntheir limitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shyr_C/0/1/0/all/0/1\">Cathy Shyr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_P/0/1/0/all/0/1\">Paul A. Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models. (arXiv:2306.12659v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12659","description":"<p>Sentiment analysis is a vital tool for uncovering insights from financial\narticles, news, and social media, shaping our understanding of market\nmovements. Despite the impressive capabilities of large language models (LLMs)\nin financial natural language processing (NLP), they still struggle with\naccurately interpreting numerical values and grasping financial context,\nlimiting their effectiveness in predicting financial sentiment. In this paper,\nwe introduce a simple yet effective instruction tuning approach to address\nthese issues. By transforming a small portion of supervised financial sentiment\nanalysis data into instruction data and fine-tuning a general-purpose LLM with\nthis method, we achieve remarkable advancements in financial sentiment\nanalysis. In the experiment, our approach outperforms state-of-the-art\nsupervised sentiment analysis models, as well as widely used LLMs like ChatGPT\nand LLaMAs, particularly in scenarios where numerical understanding and\ncontextual comprehension are vital.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongyang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao-Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought. (arXiv:2306.12672v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12672","description":"<p>How does language inform our downstream thinking? In particular, how do\nhumans make meaning from language -- and how can we leverage a theory of\nlinguistic meaning to build machines that think in more human-like ways? In\nthis paper, we propose \\textit{rational meaning construction}, a computational\nframework for language-informed thinking that combines neural models of\nlanguage with probabilistic models for rational inference. We frame linguistic\nmeaning as a context-sensitive mapping from natural language into a\n\\textit{probabilistic language of thought} (PLoT) -- a general-purpose symbolic\nsubstrate for probabilistic, generative world modeling. Our architecture\nintegrates two powerful computational tools that have not previously come\ntogether: we model thinking with \\textit{probabilistic programs}, an expressive\nrepresentation for flexible commonsense reasoning; and we model meaning\nconstruction with \\textit{large language models} (LLMs), which support\nbroad-coverage translation from natural language utterances to code expressions\nin a probabilistic programming language. We illustrate our framework in action\nthrough examples covering four core domains from cognitive science:\nprobabilistic reasoning, logical and relational reasoning, visual and physical\nreasoning, and social reasoning about agents and their plans. In each, we show\nthat LLMs can generate context-sensitive translations that capture\npragmatically-appropriate linguistic meanings, while Bayesian inference with\nthe generated programs supports coherent and robust commonsense reasoning. We\nextend our framework to integrate cognitively-motivated symbolic modules to\nprovide a unified commonsense thinking interface from language. Finally, we\nexplore how language can drive the construction of world models themselves.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1\">Lionel Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grand_G/0/1/0/all/0/1\">Gabriel Grand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lew_A/0/1/0/all/0/1\">Alexander K. Lew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansinghka_V/0/1/0/all/0/1\">Vikash K. Mansinghka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Colloquial Dataset for Persian Sentiment Analysis of Social Microblogs. (arXiv:2306.12679v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12679","description":"<p>Introduction: Microblogging websites have massed rich data sources for\nsentiment analysis and opinion mining. In this regard, sentiment classification\nhas frequently proven inefficient because microblog posts typically lack\nsyntactically consistent terms and representatives since users on these social\nnetworks do not like to write lengthy statements. Also, there are some\nlimitations to low-resource languages. The Persian language has exceptional\ncharacteristics and demands unique annotated data and models for the sentiment\nanalysis task, which are distinctive from text features within the English\ndialect. Method: This paper first constructs a user opinion dataset called\nITRC-Opinion by collaborative environment and insource way. Our dataset\ncontains 60,000 informal and colloquial Persian texts from social microblogs\nsuch as Twitter and Instagram. Second, this study proposes a new deep\nconvolutional neural network (CNN) model for more effective sentiment analysis\nof colloquial text in social microblog posts. The constructed datasets are used\nto evaluate the presented model. Furthermore, some models, such as LSTM,\nCNN-RNN, BiLSTM, and BiGRU with different word embeddings, including Fasttext,\nGlove, and Word2vec, investigated our dataset and evaluated the results.\nResults: The results demonstrate the benefit of our dataset and the proposed\nmodel (72% accuracy), displaying meaningful improvement in sentiment\nclassification performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mazoochi_M/0/1/0/all/0/1\">Mojtaba Mazoochi</a> (ICT Research Institute, Tehran, Iran), <a href=\"http://arxiv.org/find/cs/1/au:+Rabiei_L/0/1/0/all/0/1\">Leyla Rabiei</a> (Iran Telecommunication Research Center (ITRC), Tehran, Iran), <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_F/0/1/0/all/0/1\">Farzaneh Rahmani</a> (Iran Telecommunication Research Center (ITRC), Tehran, Iran), <a href=\"http://arxiv.org/find/cs/1/au:+Rajabi_Z/0/1/0/all/0/1\">Zeinab Rajabi</a> (Iran Telecommunication Research Center (ITRC), Tehran, Iran)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vec2Vec: A Compact Neural Network Approach for Transforming Text Embeddings with High Fidelity. (arXiv:2306.12689v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12689","description":"<p>Vector embeddings have become ubiquitous tools for many language-related\ntasks. A leading embedding model is OpenAI's text-ada-002 which can embed\napproximately 6,000 words into a 1,536-dimensional vector. While powerful,\ntext-ada-002 is not open source and is only available via API. We trained a\nsimple neural network to convert open-source 768-dimensional MPNet embeddings\ninto text-ada-002 embeddings. We compiled a subset of 50,000 online food\nreviews. We calculated MPNet and text-ada-002 embeddings for each review and\ntrained a simple neural network to for 75 epochs. The neural network was\ndesigned to predict the corresponding text-ada-002 embedding for a given MPNET\nembedding. Our model achieved an average cosine similarity of 0.932 on 10,000\nunseen reviews in our held-out test dataset. We manually assessed the quality\nof our predicted embeddings for vector search over text-ada-002-embedded\nreviews. While not as good as real text-ada-002 embeddings, predicted\nembeddings were able to retrieve highly relevant reviews. Our final model,\nVec2Vec, is lightweight (&lt;80 MB) and fast. Future steps include training a\nneural network with a more sophisticated architecture and a larger dataset of\npaired embeddings to achieve greater performance. The ability to convert\nbetween and align embedding spaces may be helpful for interoperability,\nlimiting dependence on proprietary models, protecting data privacy, reducing\ncosts, and offline operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_A/0/1/0/all/0/1\">Andrew Kean Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Neural Machine Translation System for Indic to Indic Languages. (arXiv:2306.12693v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12693","description":"<p>This paper gives an Indic-to-Indic (IL-IL) MNMT baseline model for 11 ILs\nimplemented on the Samanantar corpus and analyzed on the Flores-200 corpus. All\nthe models are evaluated using the BLEU score. In addition, the languages are\nclassified under three groups namely East Indo- Aryan (EI), Dravidian (DR), and\nWest Indo-Aryan (WI). The effect of language relatedness on MNMT model\nefficiency is studied. Owing to the presence of large corpora from English (EN)\nto ILs, MNMT IL-IL models using EN as a pivot are also built and examined. To\nachieve this, English- Indic (EN-IL) models are also developed, with and\nwithout the usage of related languages. Results reveal that using related\nlanguages is beneficial for the WI group only, while it is detrimental for the\nEI group and shows an inconclusive effect on the DR group, but it is useful for\nEN-IL models. Thus, related language groups are used to develop pivot MNMT\nmodels. Furthermore, the IL corpora are transliterated from the corresponding\nscripts to a modified ITRANS script, and the best MNMT models from the previous\napproaches are built on the transliterated corpus. It is observed that the\nusage of pivot models greatly improves MNMT baselines with AS-TA achieving the\nminimum BLEU score and PA-HI achieving the maximum score. Among languages, AS,\nML, and TA achieve the lowest BLEU score, whereas HI, PA, and GU perform the\nbest. Transliteration also helps the models with few exceptions. The best\nincrement of scores is observed in ML, TA, and BN and the worst average\nincrement is observed in KN, HI, and PA, across all languages. The best model\nobtained is the PA-HI language pair trained on PAWI transliterated corpus which\ngives 24.29 BLEU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sudhansu Bala Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_D/0/1/0/all/0/1\">Divyajyoti Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_T/0/1/0/all/0/1\">Tapas Kumar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Bidyut Kr. Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Generation for Advertising: A Survey. (arXiv:2306.12719v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12719","description":"<p>Natural language generation methods have emerged as effective tools to help\nadvertisers increase the number of online advertisements they produce. This\nsurvey entails a review of the research trends on this topic over the past\ndecade, from template-based to extractive and abstractive approaches using\nneural networks. Additionally, key challenges and directions revealed through\nthe survey, including metric optimization, faithfulness, diversity,\nmultimodality, and the development of benchmark datasets, are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murakami_S/0/1/0/all/0/1\">Soichiro Murakami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshino_S/0/1/0/all/0/1\">Sho Hoshino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peinan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Multimodal Entity Linking. (arXiv:2306.12725v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12725","description":"<p>Multimodal Entity Linking (MEL) is the task of mapping mentions with\nmultimodal contexts to the referent entities from a knowledge base (e.g.,\nWikipedia). Prior MEL methods mainly focus on designing complex multimodal\ninteraction mechanisms and require fine-tuning all model parameters, which can\nbe prohibitively costly and difficult to scale in the era of Large Language\nModels (LLMs). In this work, we propose GEMEL, a simple yet effective\nGenerative Multimodal Entity Linking method, which leverages the capabilities\nof LLMs from large-scale pre-training to directly generate target entity names.\nWe keep the vision and language model frozen and only train a linear layer to\nenable cross-modality interactions. To adapt LLMs to the MEL task, we take\nadvantage of the emerging in-context learning (ICL) capability of LLMs by\nretrieving multimodal instances as demonstrations. Extensive experiments show\nthat with only ~0.3% of the model parameters fine-tuned, GEMEL achieves\nstate-of-the-art results on two well-established MEL datasets (4.1% accuracy\ngains on WikiDiverse and 15.4% accuracy gains on WikiMEL). Our approach is\ncompatible with any off-the-shelf language model, paving the way towards an\nefficient and general solution for utilizing LLMs in the MEL task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Senbao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective. (arXiv:2306.12756v1 [cs.IR])","link":"http://arxiv.org/abs/2306.12756","description":"<p>Recently, we have witnessed generative retrieval increasingly gaining\nattention in the information retrieval (IR) field, which retrieves documents by\ndirectly generating their identifiers. So far, much effort has been devoted to\ndeveloping effective generative retrieval models. There has been less attention\npaid to the robustness perspective. When a new retrieval paradigm enters into\nthe real-world application, it is also critical to measure the\nout-of-distribution (OOD) generalization, i.e., how would generative retrieval\nmodels generalize to new distributions. To answer this question, firstly, we\ndefine OOD robustness from three perspectives in retrieval problems: 1) The\nquery variations; 2) The unforeseen query types; and 3) The unforeseen tasks.\nBased on this taxonomy, we conduct empirical studies to analyze the OOD\nrobustness of several representative generative retrieval models against dense\nretrieval models. The empirical results indicate that the OOD robustness of\ngenerative retrieval models requires enhancement. We hope studying the OOD\nrobustness of generative retrieval models would be advantageous to the IR\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-An Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping and Cleaning Open Commonsense Knowledge Bases with Generative Translation. (arXiv:2306.12766v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12766","description":"<p>Structured knowledge bases (KBs) are the backbone of many\nknow\\-ledge-intensive applications, and their automated construction has\nreceived considerable attention. In particular, open information extraction\n(OpenIE) is often used to induce structure from a text. However, although it\nallows high recall, the extracted knowledge tends to inherit noise from the\nsources and the OpenIE algorithm. Besides, OpenIE tuples contain an open-ended,\nnon-canonicalized set of relations, making the extracted knowledge's downstream\nexploitation harder. In this paper, we study the problem of mapping an open KB\ninto the fixed schema of an existing KB, specifically for the case of\ncommonsense knowledge. We propose approaching the problem by generative\ntranslation, i.e., by training a language model to generate fixed-schema\nassertions from open ones. Experiments show that this approach occupies a sweet\nspot between traditional manual, rule-based, or classification-based\ncanonicalization and purely generative KB construction like COMET. Moreover, it\nproduces higher mapping accuracy than the former while avoiding the\nassociation-based noise of the latter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Romero_J/0/1/0/all/0/1\">Julien Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4. (arXiv:2306.12794v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12794","description":"<p>The advent and fast development of neural networks have revolutionized the\nresearch on dialogue systems and subsequently have triggered various challenges\nregarding their automatic evaluation. Automatic evaluation of open-domain\ndialogue systems as an open challenge has been the center of the attention of\nmany researchers. Despite the consistent efforts to improve automatic metrics'\ncorrelations with human evaluation, there have been very few attempts to assess\ntheir robustness over multiple domains and dimensions. Also, their focus is\nmainly on the English language. All of these challenges prompt the development\nof automatic evaluation metrics that are reliable in various domains,\ndimensions, and languages. This track in the 11th Dialogue System Technology\nChallenge (DSTC11) is part of the ongoing effort to promote robust and\nmultilingual automatic evaluation metrics. This article describes the datasets\nand baselines provided to participants and discusses the submission and result\ndetails of the two proposed subtasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Cantelar_M/0/1/0/all/0/1\">Mario Rodr&#xed;guez-Cantelar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chengguang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Ke Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1\">Sarik Ghazarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DHaro_L/0/1/0/all/0/1\">Luis Fernando D&#x27;Haro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing in Electronic Health Records in Relation to Healthcare Decision-making: A Systematic Review. (arXiv:2306.12834v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12834","description":"<p>Background: Natural Language Processing (NLP) is widely used to extract\nclinical insights from Electronic Health Records (EHRs). However, the lack of\nannotated data, automated tools, and other challenges hinder the full\nutilisation of NLP for EHRs. Various Machine Learning (ML), Deep Learning (DL)\nand NLP techniques are studied and compared to understand the limitations and\nopportunities in this space comprehensively.\n</p>\n<p>Methodology: After screening 261 articles from 11 databases, we included 127\npapers for full-text review covering seven categories of articles: 1) medical\nnote classification, 2) clinical entity recognition, 3) text summarisation, 4)\ndeep learning (DL) and transfer learning architecture, 5) information\nextraction, 6) Medical language translation and 7) other NLP applications. This\nstudy follows the Preferred Reporting Items for Systematic Reviews and\nMeta-Analyses (PRISMA) guidelines.\n</p>\n<p>Result and Discussion: EHR was the most commonly used data type among the\nselected articles, and the datasets were primarily unstructured. Various ML and\nDL methods were used, with prediction or classification being the most common\napplication of ML or DL. The most common use cases were: the International\nClassification of Diseases, Ninth Revision (ICD-9) classification, clinical\nnote analysis, and named entity recognition (NER) for clinical descriptions and\nresearch on psychiatric disorders.\n</p>\n<p>Conclusion: We find that the adopted ML models were not adequately assessed.\nIn addition, the data imbalance problem is quite important, yet we must find\ntechniques to address this underlining problem. Future studies should address\nkey limitations in studies, primarily identifying Lupus Nephritis, Suicide\nAttempts, perinatal self-harmed and ICD-9 classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1\">Elias Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_R/0/1/0/all/0/1\">Rajib Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higgins_N/0/1/0/all/0/1\">Niall Higgins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soar_J/0/1/0/all/0/1\">Jeffrey Soar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barua_P/0/1/0/all/0/1\">Prabal Datta Barua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pisani_A/0/1/0/all/0/1\">Anthony R. Pisani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+D_P/0/1/0/all/0/1\">Ph.D</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner%7D_K/0/1/0/all/0/1\">Kathryn Turner}</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unveiling Global Narratives: A Multilingual Twitter Dataset of News Media on the Russo-Ukrainian Conflict. (arXiv:2306.12886v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12886","description":"<p>The ongoing Russo-Ukrainian conflict has been a subject of intense media\ncoverage worldwide. Understanding the global narrative surrounding this topic\nis crucial for researchers that aim to gain insights into its multifaceted\ndimensions. In this paper, we present a novel dataset that focuses on this\ntopic by collecting and processing tweets posted by news or media companies on\nsocial media across the globe. We collected tweets from February 2022 to May\n2023 to acquire approximately 1.5 million tweets in 60 different languages.\nEach tweet in the dataset is accompanied by processed tags, allowing for the\nidentification of entities, stances, concepts, and sentiments expressed. The\navailability of the dataset serves as a valuable resource for researchers\naiming to investigate the global narrative surrounding the ongoing conflict\nfrom various aspects such as who are the prominent entities involved, what\nstances are taken, where do these stances originate, and how are the different\nconcepts related to the event portrayed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hakimov_S/0/1/0/all/0/1\">Sherzod Hakimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheema_G/0/1/0/all/0/1\">Gullal S. Cheema</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages. (arXiv:2306.12907v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12907","description":"<p>We introduce a new proxy score for evaluating bitext mining based on\nsimilarity in a multilingual embedding space: xSIM++. In comparison to xSIM,\nthis improved proxy leverages rule-based approaches to extend English sentences\nin any evaluation set with synthetic, hard-to-distinguish examples which more\nclosely mirror the scenarios we encounter during large-scale mining. We\nvalidate this proxy by running a significant number of bitext mining\nexperiments for a set of low-resource languages, and subsequently train NMT\nsystems on the mined data. In comparison to xSIM, we show that xSIM++ is better\ncorrelated with the downstream BLEU scores of translation systems trained on\nmined bitexts, providing a reliable proxy of bitext mining performance without\nneeding to run expensive bitext mining pipelines. xSIM++ also reports\nperformance for different error types, offering more fine-grained feedback for\nmodel development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_K/0/1/0/all/0/1\">Kevin Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celebi_O/0/1/0/all/0/1\">Onur &#xc7;elebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mourachko_A/0/1/0/all/0/1\">Alex Mourachko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit spoken language diarization. (arXiv:2306.12913v1 [eess.AS])","link":"http://arxiv.org/abs/2306.12913","description":"<p>Spoken language diarization (LD) and related tasks are mostly explored using\nthe phonotactic approach. Phonotactic approaches mostly use explicit way of\nlanguage modeling, hence requiring intermediate phoneme modeling and\ntranscribed data. Alternatively, the ability of deep learning approaches to\nmodel temporal dynamics may help for the implicit modeling of language\ninformation through deep embedding vectors. Hence this work initially explores\nthe available speaker diarization frameworks that capture speaker information\nimplicitly to perform LD tasks. The performance of the LD system on synthetic\ncode-switch data using the end-to-end x-vector approach is 6.78% and 7.06%, and\nfor practical data is 22.50% and 60.38%, in terms of diarization error rate and\nJaccard error rate (JER), respectively. The performance degradation is due to\nthe data imbalance and resolved to some extent by using pre-trained wave2vec\nembeddings that provide a relative improvement of 30.74% in terms of JER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mishra_J/0/1/0/all/0/1\">Jagabandhu Mishra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_A/0/1/0/all/0/1\">Amartya Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasanna_S/0/1/0/all/0/1\">S. R. Mahadeva Prasanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation. (arXiv:2306.12916v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12916","description":"<p>While summarization has been extensively researched in natural language\nprocessing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a\nlargely unexplored area that has the potential to improve cross-cultural\naccessibility, information sharing, and understanding. This paper\ncomprehensively addresses the CLCTS task, including dataset creation, modeling,\nand evaluation. We build the first CLCTS corpus, leveraging historical fictive\ntexts and Wikipedia summaries in English and German, and examine the\neffectiveness of popular transformer end-to-end models with different\nintermediate task finetuning tasks. Additionally, we explore the potential of\nChatGPT for CLCTS as a summarizer and an evaluator. Overall, we report\nevaluations from humans, ChatGPT, and several recent automatic evaluation\nmetrics where we find our intermediate task finetuned end-to-end models\ngenerate bad to moderate quality summaries; ChatGPT as a summarizer (without\nany finetuning) provides moderate to good quality outputs and as an evaluator\ncorrelates moderately with human evaluations though it is prone to giving lower\nscores. ChatGPT also seems to be very adept at normalizing historical text. We\nfinally test ChatGPT in a scenario with adversarially attacked and unseen\nsource documents and find that ChatGPT is better at omission and entity swap\nthan negating against its prior knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouni_J/0/1/0/all/0/1\">Jihed Ouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AudioPaLM: A Large Language Model That Can Speak and Listen. (arXiv:2306.12925v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12925","description":"<p>We introduce AudioPaLM, a large language model for speech understanding and\ngeneration. AudioPaLM fuses text-based and speech-based language models, PaLM-2\n[Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified\nmultimodal architecture that can process and generate text and speech with\napplications including speech recognition and speech-to-speech translation.\nAudioPaLM inherits the capability to preserve paralinguistic information such\nas speaker identity and intonation from AudioLM and the linguistic knowledge\npresent only in text large language models such as PaLM-2. We demonstrate that\ninitializing AudioPaLM with the weights of a text-only large language model\nimproves speech processing, successfully leveraging the larger quantity of text\ntraining data used in pretraining to assist with the speech tasks. The\nresulting model significantly outperforms existing systems for speech\ntranslation tasks and has the ability to perform zero-shot speech-to-text\ntranslation for many languages for which input/target language combinations\nwere not seen in training. AudioPaLM also demonstrates features of audio\nlanguage models, such as transferring a voice across languages based on a short\nspoken prompt. We release examples of our method at\nhttps://google-research.github.io/seanet/audiopalm/examples\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rubenstein_P/0/1/0/all/0/1\">Paul K. Rubenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asawaroengchai_C/0/1/0/all/0/1\">Chulayuth Asawaroengchai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc Dung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borsos_Z/0/1/0/all/0/1\">Zal&#xe1;n Borsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quitry_F/0/1/0/all/0/1\">F&#xe9;lix de Chaumont Quitry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peter Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badawy_D/0/1/0/all/0/1\">Dalia El Badawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muckenhirn_H/0/1/0/all/0/1\">Hannah Muckenhirn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padfield_D/0/1/0/all/0/1\">Dirk Padfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozenberg_D/0/1/0/all/0/1\">Danny Rozenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schalkwyk_J/0/1/0/all/0/1\">Johan Schalkwyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifi_M/0/1/0/all/0/1\">Matt Sharifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_M/0/1/0/all/0/1\">Marco Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tudor_A/0/1/0/all/0/1\">Alexandru Tudor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velimirovic_M/0/1/0/all/0/1\">Mihajlo Velimirovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_D/0/1/0/all/0/1\">Damien Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zayats_V/0/1/0/all/0/1\">Vicky Zayats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeghidour_N/0/1/0/all/0/1\">Neil Zeghidour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhishuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zilka_L/0/1/0/all/0/1\">Lukas Zilka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_C/0/1/0/all/0/1\">Christian Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. (arXiv:2306.12929v1 [cs.LG])","link":"http://arxiv.org/abs/2306.12929","description":"<p>Transformer models have been widely adopted in various domains over the last\nyears, and especially large language models have advanced the field of AI\nsignificantly. Due to their size, the capability of these networks has\nincreased tremendously, but this has come at the cost of a significant increase\nin necessary compute. Quantization is one of the most effective ways to reduce\nthe computational time and memory consumption of neural networks. Many studies\nhave shown, however, that modern transformer models tend to learn strong\noutliers in their activations, making them difficult to quantize. To retain\nacceptable performance, the existence of these outliers requires activations to\nbe in higher bitwidth or the use of different numeric formats, extra\nfine-tuning, or other workarounds. We show that strong outliers are related to\nvery specific behavior of attention heads that try to learn a \"no-op\" or just a\npartial update of the residual. To achieve the exact zeros needed in the\nattention matrix for a no-update, the input to the softmax is pushed to be\nlarger and larger during training, causing outliers in other parts of the\nnetwork. Based on these observations, we propose two simple (independent)\nmodifications to the attention mechanism - clipped softmax and gated attention.\nWe empirically show that models pre-trained using our methods learn\nsignificantly smaller outliers while maintaining and sometimes even improving\nthe floating-point task performance. This enables us to quantize transformers\nto full INT8 quantization of the activations without any additional effort. We\ndemonstrate the effectiveness of our methods on both language models (BERT,\nOPT) and vision transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bondarenko_Y/0/1/0/all/0/1\">Yelysei Bondarenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagel_M/0/1/0/all/0/1\">Markus Nagel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1\">Tijmen Blankevoort</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracking public attitudes toward ChatGPT on Twitter using sentiment analysis and topic modeling. (arXiv:2306.12951v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12951","description":"<p>ChatGPT sets a new record with the fastest-growing user base, as a chatbot\npowered by a large language model (LLM). While it demonstrates state-of-the-art\ncapabilities in a variety of language-generating tasks, it also raises\nwidespread public concerns regarding its societal impact. In this paper, we\nutilize natural language processing approaches to investigate the public\nattitudes towards ChatGPT by applying sentiment analysis and topic modeling\ntechniques to Twitter data. Our result shows that the overall sentiment is\nlargely neutral to positive, which also holds true across different occupation\ngroups. Among a wide range of topics mentioned in tweets, the most popular\ntopics are Artificial Intelligence, Search Engines, Education, Writing, and\nQuestion Answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koonchanok_R/0/1/0/all/0/1\">Ratanond Koonchanok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yanling Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hyeju Jang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversation Derailment Forecasting with Graph Convolutional Networks. (arXiv:2306.12982v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12982","description":"<p>Online conversations are particularly susceptible to derailment, which can\nmanifest itself in the form of toxic communication patterns like disrespectful\ncomments or verbal abuse. Forecasting conversation derailment predicts signs of\nderailment in advance enabling proactive moderation of conversations. Current\nstate-of-the-art approaches to address this problem rely on sequence models\nthat treat dialogues as text streams. We propose a novel model based on a graph\nconvolutional neural network that considers dialogue user dynamics and the\ninfluence of public perception on conversation utterances. Through empirical\nevaluation, we show that our model effectively captures conversation dynamics\nand outperforms the state-of-the-art models on the CGA and CMV benchmark\ndatasets by 1.5\\% and 1.7\\%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Altarawneh_E/0/1/0/all/0/1\">Enas Altarawneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ammeta Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkin_M/0/1/0/all/0/1\">Michael Jenkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papagelis_M/0/1/0/all/0/1\">Manos Papagelis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Emotion Diarization: Which Emotion Appears When?. (arXiv:2306.12991v1 [cs.CL])","link":"http://arxiv.org/abs/2306.12991","description":"<p>Speech Emotion Recognition (SER) typically relies on utterance-level\nsolutions. However, emotions conveyed through speech should be considered as\ndiscrete speech events with definite temporal boundaries, rather than\nattributes of the entire utterance. To reflect the fine-grained nature of\nspeech emotions, we propose a new task: Speech Emotion Diarization (SED). Just\nas Speaker Diarization answers the question of \"Who speaks when?\", Speech\nEmotion Diarization answers the question of \"Which emotion appears when?\". To\nfacilitate the evaluation of the performance and establish a common benchmark\nfor researchers, we introduce the Zaion Emotion Dataset (ZED), an openly\naccessible speech emotion dataset that includes non-acted emotions recorded in\nreal-life conditions, along with manually-annotated boundaries of emotion\nsegments within the utterance. We provide competitive baselines and open-source\nthe code and the pre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravanelli_M/0/1/0/all/0/1\">Mirco Ravanelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nfissi_A/0/1/0/all/0/1\">Alaa Nfissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yacoubi_A/0/1/0/all/0/1\">Alya Yacoubi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Apolitical Intelligence? Auditing Delphi's responses on controversial political issues in the US. (arXiv:2306.13000v1 [cs.CY])","link":"http://arxiv.org/abs/2306.13000","description":"<p>As generative language models are deployed in ever-wider contexts, concerns\nabout their political values have come to the forefront with critique from all\nparts of the political spectrum that the models are biased and lack neutrality.\nHowever, the question of what neutrality is and whether it is desirable remains\nunderexplored. In this paper, I examine neutrality through an audit of Delphi\n[<a href=\"/abs/2110.07574\">arXiv:2110.07574</a>], a large language model designed for crowdsourced ethics. I\nanalyse how Delphi responds to politically controversial questions compared to\ndifferent US political subgroups. I find that Delphi is poorly calibrated with\nrespect to confidence and exhibits a significant political skew. Based on these\nresults, I examine the question of neutrality from a data-feminist lens, in\nterms of how notions of neutrality shift power and further marginalise unheard\nvoices. These findings can hopefully contribute to a more reflexive debate\nabout the normative questions of alignment and what role we want generative\nmodels to play in society.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rystrom_J/0/1/0/all/0/1\">Jonathan H. Rystr&#xf8;m</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Explainable Evaluation Metrics for Machine Translation. (arXiv:2306.13041v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13041","description":"<p>Unlike classical lexical overlap metrics such as BLEU, most current\nevaluation metrics for machine translation (for example, COMET or BERTScore)\nare based on black-box large language models. They often achieve strong\ncorrelations with human judgments, but recent research indicates that the\nlower-quality classical metrics remain dominant, one of the potential reasons\nbeing that their decision processes are more transparent. To foster more\nwidespread acceptance of novel high-quality metrics, explainability thus\nbecomes crucial. In this concept paper, we identify key properties as well as\nkey goals of explainable machine translation metrics and provide a\ncomprehensive synthesis of recent techniques, relating them to our established\ngoals and properties. In this context, we also discuss the latest\nstate-of-the-art approaches to explainable metrics based on generative models\nsuch as ChatGPT and GPT4. Finally, we contribute a vision of next-generation\napproaches, including natural language explanations. We hope that our work can\nhelp catalyze and guide future research on explainable evaluation metrics and,\nmediately, also contribute to better and more transparent machine translation\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leiter_C/0/1/0/all/0/1\">Christoph Leiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lertvittayakumjorn_P/0/1/0/all/0/1\">Piyawat Lertvittayakumjorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fomicheva_M/0/1/0/all/0/1\">Marina Fomicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions. (arXiv:2306.13047v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13047","description":"<p>Multiple Choice examinations are a ubiquitous form of assessment that is used\nto measure the ability of candidates across various domains and tasks.\nMaintaining the quality of proposed questions is of great importance to test\ndesigners, and therefore newly proposed questions go through several pre-test\nevaluation stages before they can be deployed into real-world exams. This\nprocess is currently quite manual, which can lead to time lags in the question\ndevelopment cycle. Automating this process would lead to a large improvement in\nefficiency, however, current datasets do not contain sufficient pre-test\nanalysis information. In this paper, we introduce CamChoice; a multiple-choice\ncomprehension dataset with questions at different target levels, where\nquestions have the true candidate selected options distributions. We introduce\nthe task of candidate distribution matching, propose several evaluation metrics\nfor the task, and demonstrate that automatic systems trained on RACE++ can be\nleveraged as baselines for our task. We further demonstrate that these\nautomatic systems can be used for practical pre-test evaluation tasks such as\ndetecting underperforming distractors, where our detection systems can\nautomatically identify poor distractors that few candidates select. We release\nthe data publicly for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liusie_A/0/1/0/all/0/1\">Adian Liusie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vatsal Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullooly_A/0/1/0/all/0/1\">Andrew Mullooly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knill_K/0/1/0/all/0/1\">Kate Knill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named entity recognition in resumes. (arXiv:2306.13062v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13062","description":"<p>Named entity recognition (NER) is used to extract information from various\ndocuments and texts such as names and dates. It is important to extract\neducation and work experience information from resumes in order to filter them.\nConsidering the fact that all information in a resume has to be entered to the\ncompanys system manually, automatizing this process will save time of the\ncompanies. In this study, a deep learning-based semi-automatic named entity\nrecognition system has been implemented with a focus on resumes in the field of\nIT. Firstly, resumes of employees from five different IT related fields has\nbeen annotated. Six transformer based pre-trained models have been adapted to\nnamed entity recognition problem using the annotated data. These models have\nbeen selected among popular models in the natural language processing field.\nThe obtained system can recognize eight different entity types which are city,\ndate, degree, diploma major, job title, language, country and skill. Models\nused in the experiments are compared using micro, macro and weighted F1 scores\nand the performance of the methods was evaluated. Taking these scores into\naccount for test set the best micro and weighted F1 score is obtained by\nRoBERTa and the best macro F1 score is obtained by Electra model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kesim_E/0/1/0/all/0/1\">Ege Kesim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deliahmetoglu_A/0/1/0/all/0/1\">Aysu Deliahmetoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. (arXiv:2306.13063v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13063","description":"<p>The task of empowering large language models (LLMs) to accurately express\ntheir confidence, referred to as confidence elicitation, is essential in\nensuring reliable and trustworthy decision-making processes. Previous methods,\nwhich primarily rely on model logits, have become less suitable for LLMs and\neven infeasible with the rise of closed-source LLMs (e.g., commercialized LLM\nAPIs). This leads to a growing need to explore the untapped area of\n\\emph{non-logit-based} approaches to estimate the uncertainty of LLMs. Hence,\nin this study, we investigate approaches for confidence elicitation that do not\nrequire model fine-tuning or access to proprietary information. We introduce\nthree categories of methods: verbalize-based, consistency-based, and their\nhybrid methods for benchmarking, and evaluate their performance across five\ntypes of datasets and four widely-used LLMs. Our analysis of these methods\nuncovers several key insights: 1) LLMs often exhibit a high degree of\noverconfidence when verbalizing their confidence; 2) Prompting strategies such\nas CoT, Top-K and Multi-step confidences improve calibration of verbalized\nconfidence; 3) Consistency-based methods outperform the verbalized confidences\nin most cases, with particularly notable improvements on the arithmetic\nreasoning task; 4) Hybrid methods consistently deliver the best performance\nover their baselines, thereby emerging as a promising state-of-the-art\napproach; 5) Despite these advancements, all investigated methods continue to\nstruggle with challenging tasks, such as those requiring professional\nknowledge, leaving significant scope for improvement of confidence elicitation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_M/0/1/0/all/0/1\">Miao Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xinyang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-automated extraction of research topics and trends from NCI funding in radiological sciences from 2000-2020. (arXiv:2306.13075v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13075","description":"<p>Investigators, funders, and the public desire knowledge on topics and trends\nin publicly funded research but current efforts in manual categorization are\nlimited in scale and understanding. We developed a semi-automated approach to\nextract and name research topics, and applied this to \\$1.9B of NCI funding\nover 21 years in the radiological sciences to determine micro- and macro-scale\nresearch topics and funding trends. Our method relies on sequential clustering\nof existing biomedical-based word embeddings, naming using subject matter\nexperts, and visualization to discover trends at a macroscopic scale above\nindividual topics. We present results using 15 and 60 cluster topics, where we\nfound that 2D projection of grant embeddings reveals two dominant axes:\nphysics-biology and therapeutic-diagnostic. For our dataset, we found that\nfunding for therapeutics- and physics-based research have outpaced diagnostics-\nand biology-based research, respectively. We hope these results may (1) give\ninsight to funders on the appropriateness of their funding allocation, (2)\nassist investigators in contextualizing their work and explore neighboring\nresearch domains, and (3) allow the public to review where their tax dollars\nare being allocated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Mark Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beidler_P/0/1/0/all/0/1\">Peter Beidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_J/0/1/0/all/0/1\">Joseph Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">August Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Daniel Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinahan_P/0/1/0/all/0/1\">Paul Kinahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">John Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning. (arXiv:2306.13089v1 [cs.LG])","link":"http://arxiv.org/abs/2306.13089","description":"<p>Molecule property prediction has gained significant attention in recent\nyears. The main bottleneck is the label insufficiency caused by expensive lab\nexperiments. In order to alleviate this issue and to better leverage textual\nknowledge for tasks, this study investigates the feasibility of employing\nnatural language instructions to accomplish molecule-related tasks in a\nzero-shot setting. We discover that existing molecule-text models perform\npoorly in this setting due to inadequate treatment of instructions and limited\ncapacity for graphs. To overcome these issues, we propose GIMLET, which unifies\nlanguage models for both graph and text data. By adopting generalized position\nembedding, our model is extended to encode both graph structures and\ninstruction text without additional graph encoding modules. GIMLET also\ndecouples encoding of the graph from tasks instructions in the attention\nmechanism, enhancing the generalization of graph features across novel tasks.\nWe construct a dataset consisting of more than two thousand molecule tasks with\ncorresponding instructions derived from task descriptions. We pretrain GIMLET\non the molecule tasks along with instructions, enabling the model to transfer\neffectively to a broad range of tasks. Experimental results demonstrate that\nGIMLET significantly outperforms molecule-text baselines in instruction-based\nzero-shot learning, even achieving closed results to supervised GNN models on\ntasks such as toxcast and muv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiteng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengchao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hannan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhi-Hong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmTract: Extracting Emotions from Social Media. (arXiv:2112.03868v3 [q-fin.PR] UPDATED)","link":"http://arxiv.org/abs/2112.03868","description":"<p>We develop an open-source tool (EmTract) that extracts emotions from social\nmedia text tailed for financial context. To do so, we annotate ten thousand\nshort messages from a financial social media platform (StockTwits) and combine\nit with open-source emotion data. We then use a pre-tuned NLP model,\nDistilBERT, augment its embedding space by including 4,861 tokens (emojis and\nemoticons), and then fit it first on the open-source emotion data, then\ntransfer it to our annotated financial social media data. Our model outperforms\ncompeting open-source state-of-the-art emotion classifiers, such as Emotion\nEnglish DistilRoBERTa-base on both human and chatGPT annotated data. Compared\nto dictionary based methods, our methodology has three main advantages for\nresearch in finance. First, our model is tailored to financial social media\ntext; second, it incorporates key aspects of social media data, such as\nnon-standard phrases, emojis, and emoticons; and third, it operates by\nsequentially learning a latent representation that includes features such as\nword order, word usage, and local context. Using EmTract, we explore the\nrelationship between investor emotions expressed on social media and asset\nprices. We show that firm-specific investor emotions are predictive of daily\nprice movements. Our findings show that emotions and market dynamics are\nclosely related, and we provide a tool to help study the role emotions play in\nfinancial markets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Vamossy_D/0/1/0/all/0/1\">Domonkos F. Vamossy</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Skog_R/0/1/0/all/0/1\">Rolf Skog</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation. (arXiv:2205.12593v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12593","description":"<p>Recent research has revealed that deep neural networks often take dataset\nbiases as a shortcut to make decisions rather than understand tasks, leading to\nfailures in real-world applications. In this study, we focus on the spurious\ncorrelation between word features and labels that models learn from the biased\ndata distribution of training data. In particular, we define the word highly\nco-occurring with a specific label as biased word, and the example containing\nbiased word as biased example. Our analysis shows that biased examples are\neasier for models to learn, while at the time of prediction, biased words make\na significantly higher contribution to the models' predictions, and models tend\nto assign predicted labels over-relying on the spurious correlation between\nwords and labels. To mitigate models' over-reliance on the shortcut (i.e.\nspurious correlation), we propose a training strategy Less-Learn-Shortcut\n(LLS): our strategy quantifies the biased degree of the biased examples and\ndown-weights them accordingly. Experimental results on Question Matching,\nNatural Language Inference and Sentiment Analysis tasks show that LLS is a\ntask-agnostic strategy and can improve the model performance on adversarial\ndata while maintaining good performance on in-domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yanrui Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jing Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sendong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qiaoqiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations. (arXiv:2207.00221v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.00221","description":"<p>Vision-Language Pretraining (VLP) models have recently successfully\nfacilitated many cross-modal downstream tasks. Most existing works evaluated\ntheir systems by comparing the fine-tuned downstream task performance. However,\nonly average downstream task accuracy provides little information about the\npros and cons of each VLP method, let alone provides insights on how the\ncommunity can improve the systems in the future. Inspired by the CheckList for\ntesting natural language processing, we exploit VL-CheckList, a novel framework\nto understand the capabilities of VLP models. The proposed method divides the\nimage-texting ability of a VLP model into three categories: objects,\nattributes, and relations, and uses a novel taxonomy to further break down\nthese three aspects. We conduct comprehensive studies to analyze seven recently\npopular VLP models via the proposed framework. Results confirm the\neffectiveness of the proposed method by revealing fine-grained differences\namong the compared models that were not visible from downstream task-only\nevaluation. Further results show promising research direction in building\nbetter VLP models. Our data and code are available at:\nhttps://github.com/om-ai-lab/VL-CheckList.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiancheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Haozhan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyusong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianwei Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequentially Sampled Chunk Conformer for Streaming End-to-End ASR. (arXiv:2211.11419v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2211.11419","description":"<p>This paper presents an in-depth study on a Sequentially Sampled Chunk\nConformer, SSC-Conformer, for streaming End-to-End (E2E) ASR. The SSC-Conformer\nfirst demonstrates the significant performance gains from using the\nsequentially sampled chunk-wise multi-head self-attention (SSC-MHSA) in the\nConformer encoder by allowing efficient cross-chunk interactions while keeping\nlinear complexities. Furthermore, it explores taking advantage of chunked\nconvolution to make use of the chunk-wise future context and integrates with\ncasual convolution in the convolution layers to further reduce CER. We verify\nthe proposed SSC-Conformer on the AISHELL-1 benchmark and experimental results\nshow that a state-of-the-art performance for streaming E2E ASR is achieved with\nCER 5.33% without LM rescoring. And, owing to its linear complexity, the\nSSC-Conformer can train with large batch sizes and infer more efficiently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fangyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Proactive Dialog Agents Using Socially-Aware Reinforcement Learning. (arXiv:2211.15359v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15359","description":"<p>The next step for intelligent dialog agents is to escape their role as silent\nbystanders and become proactive. Well-defined proactive behavior may improve\nhuman-machine cooperation, as the agent takes a more active role during\ninteraction and takes off responsibility from the user. However, proactivity is\na double-edged sword because poorly executed pre-emptive actions may have a\ndevastating effect not only on the task outcome but also on the relationship\nwith the user. For designing adequate proactive dialog strategies, we propose a\nnovel approach including both social as well as task-relevant features in the\ndialog. Here, the primary goal is to optimize proactive behavior so that it is\ntask-oriented - this implies high task success and efficiency - while also\nbeing socially effective by fostering user trust. Including both aspects in the\nreward function for training a proactive dialog agent using reinforcement\nlearning showed the benefit of our approach for more successful human-machine\ncooperation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Matthias Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_N/0/1/0/all/0/1\">Nicolas Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riekenbrauck_R/0/1/0/all/0/1\">Ron Riekenbrauck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minker_W/0/1/0/all/0/1\">Wolfgang Minker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Deep Learning for Mathematical Reasoning. (arXiv:2212.10535v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.10535","description":"<p>Mathematical reasoning is a fundamental aspect of human intelligence and is\napplicable in various fields, including science, engineering, finance, and\neveryday life. The development of artificial intelligence (AI) systems capable\nof solving math problems and proving theorems has garnered significant interest\nin the fields of machine learning and natural language processing. For example,\nmathematics serves as a testbed for aspects of reasoning that are challenging\nfor powerful deep learning models, driving new algorithmic and modeling\nadvances. On the other hand, recent advances in large-scale neural language\nmodels have opened up new benchmarks and opportunities to use deep learning for\nmathematical reasoning. In this survey paper, we review the key tasks,\ndatasets, and methods at the intersection of mathematical reasoning and deep\nlearning over the past decade. We also evaluate existing benchmarks and\nmethods, and discuss future research directions in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemSup-XC: Semantic Supervision for Zero and Few-shot Extreme Classification. (arXiv:2301.11309v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11309","description":"<p>Extreme classification (XC) involves predicting over large numbers of classes\n(thousands to millions), with real-world applications like news article\nclassification and e-commerce product tagging. The zero-shot version of this\ntask requires generalization to novel classes without additional supervision.\nIn this paper, we develop SemSup-XC, a model that achieves state-of-the-art\nzero-shot and few-shot performance on three XC datasets derived from legal,\ne-commerce, and Wikipedia data. To develop SemSup-XC, we use automatically\ncollected semantic class descriptions to represent classes and facilitate\ngeneralization through a novel hybrid matching module that matches input\ninstances to class descriptions using a combination of semantic and lexical\nsimilarity. Trained with contrastive learning, SemSup-XC significantly\noutperforms baselines and establishes state-of-the-art performance on all three\ndatasets considered, gaining up to 12 precision points on zero-shot and more\nthan 10 precision points on one-shot tests, with similar gains for recall@10.\nOur ablation studies highlight the relative importance of our hybrid matching\nmodule and automatically collected class descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_P/0/1/0/all/0/1\">Pranjal Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abstract Visual Reasoning Enabled by Language. (arXiv:2303.04091v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2303.04091","description":"<p>While artificial intelligence (AI) models have achieved human or even\nsuperhuman performance in many well-defined applications, they still struggle\nto show signs of broad and flexible intelligence. The Abstraction and Reasoning\nCorpus (ARC), a visual intelligence benchmark introduced by Fran\\c{c}ois\nChollet, aims to assess how close AI systems are to human-like cognitive\nabilities. Most current approaches rely on carefully handcrafted\ndomain-specific program searches to brute-force solutions for the tasks present\nin ARC. In this work, we propose a general learning-based framework for solving\nARC. It is centered on transforming tasks from the vision to the language\ndomain. This composition of language and vision allows for pre-trained models\nto be leveraged at each stage, enabling a shift from handcrafted priors towards\nthe learned priors of the models. While not yet beating state-of-the-art models\non ARC, we demonstrate the potential of our approach, for instance, by solving\nsome ARC tasks that have not been solved previously.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camposampiero_G/0/1/0/all/0/1\">Giacomo Camposampiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houmard_L/0/1/0/all/0/1\">Loic Houmard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estermann_B/0/1/0/all/0/1\">Benjamin Estermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathys_J/0/1/0/all/0/1\">Jo&#xeb;l Mathys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish. (arXiv:2304.13994v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.13994","description":"<p>We present SweCTRL-Mini, a large Swedish language model that can be used for\ninference and fine-tuning on a single consumer-grade GPU. The model is based on\nthe CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019),\nwhich means that users of the SweCTRL-Mini model can control the genre of the\ngenerated text by inserting special tokens in the generation prompts.\nSweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a\nset of Swedish novels. In this article, we provide (1) a detailed account of\nthe utilized training data and text pre-processing steps, to the extent that it\nis possible to check whether a specific phrase/source was a part of the\ntraining data, and (2) an evaluation of the model on both discriminative tasks,\nusing automatic evaluation methods, and generative tasks, using human referees.\nWe also compare the generative capabilities of the model with those of GPT-3.\nSweCTRL-Mini is fully open and available for download.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalpakchi_D/0/1/0/all/0/1\">Dmytro Kalpakchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boye_J/0/1/0/all/0/1\">Johan Boye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using ChatGPT for Entity Matching. (arXiv:2305.03423v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03423","description":"<p>Entity Matching is the task of deciding if two entity descriptions refer to\nthe same real-world entity. State-of-the-art entity matching methods often rely\non fine-tuning Transformer models such as BERT or RoBERTa. Two major drawbacks\nof using these models for entity matching are that (i) the models require\nsignificant amounts of fine-tuning data for reaching a good performance and\n(ii) the fine-tuned models are not robust concerning out-of-distribution\nentities. In this paper, we investigate using ChatGPT for entity matching as a\nmore robust, training data-efficient alternative to traditional Transformer\nmodels. We perform experiments along three dimensions: (i) general prompt\ndesign, (ii) in-context learning, and (iii) provision of higher-level matching\nknowledge. We show that ChatGPT is competitive with a fine-tuned RoBERTa model,\nreaching a zero-shot performance of 82.35% F1 on a challenging matching task on\nwhich RoBERTa requires 2000 training examples for reaching a similar\nperformance. Adding in-context demonstrations to the prompts further improves\nthe F1 by up to 7.85% when using similarity-based example selection. Always\nusing the same set of 10 handpicked demonstrations leads to an improvement of\n4.92% over the zero-shot performance. Finally, we show that ChatGPT can also be\nguided by adding higher-level matching knowledge in the form of rules to the\nprompts. Providing matching rules leads to similar performance gains as\nproviding in-context demonstrations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peeters_R/0/1/0/all/0/1\">Ralph Peeters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bizer_C/0/1/0/all/0/1\">Christian Bizer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Human-Like Translation Strategy with Large Language Models. (arXiv:2305.04118v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04118","description":"<p>Large language models (LLMs) have demonstrated impressive capabilities in\ngeneral scenarios, exhibiting a level of aptitude that approaches, in some\naspects even surpasses, human-level intelligence. Among their numerous skills,\nthe translation abilities of LLMs have received considerable attention. In\ncontrast to traditional machine translation that focuses solely on\nsource-target mapping, LLM-based translation can potentially mimic the human\ntranslation process that takes many preparatory steps to ensure high-quality\ntranslation. This work aims to explore this possibility by proposing the MAPS\nframework, which stands for Multi-Aspect Prompting and Selection. Specifically,\nwe enable LLMs to first analyze the given source text and extract three aspects\nof translation-related knowledge: keywords, topics and relevant demonstrations\nto guide the translation process. To filter out the noisy and unhelpful\nknowledge, we employ a selection mechanism based on quality estimation.\nExperiments suggest that MAPS brings significant and consistent improvements\nover text-davinci-003 and Alpaca on eight translation directions from the\nlatest WMT22 test sets. Our further analysis shows that the extracted knowledge\nis critical in resolving up to 59% of hallucination mistakes in translation.\nCode is available at https://github.com/zwhe99/MAPS-mt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token-Level Fitting Issues of Seq2seq Models. (arXiv:2305.04493v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04493","description":"<p>Sequence-to-sequence (seq2seq) models have been widely used for natural\nlanguage processing, computer vision, and other deep learning tasks. We find\nthat seq2seq models trained with early-stopping suffer from issues at the token\nlevel. In particular, while some tokens in the vocabulary demonstrate\noverfitting, others underfit when training is stopped. Experiments show that\nthe phenomena are pervasive in different models, even in fine-tuned large\npretrained-models. We identify three major factors that influence token-level\nfitting, which include token frequency, parts-of-speech, and prediction\ndiscrepancy. Further, we find that external factors such as language, model\nsize, domain, data scale, and pretraining can also influence the fitting of\ntokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_G/0/1/0/all/0/1\">Guangsheng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhiyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05480","description":"<p>We would like to explore how morphemes can affect the performance of a\nlanguage model. We trained GPT-2 and Bert model with StateMorph for both\nFinnish and Russian, which is a morpheme segmenting algorithm. As a comparison,\nwe also trained a model with BPE and Morfessor. Our preliminary result shows\nthat StateMorph can help the model to converge more efficiently and achieve a\nbetter validation score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jue Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katinskaia_A/0/1/0/all/0/1\">Anisia Katinskaia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_A/0/1/0/all/0/1\">Anh-Duc Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yangarber_R/0/1/0/all/0/1\">Roman Yangarber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v4 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2305.09770","description":"<p>Despite a surge collection of XAI methods, users still struggle to obtain\nrequired AI explanations. Previous research suggests chatbots as dynamic\nsolutions, but the effective design of conversational XAI agents for practical\nhuman needs remains under-explored. This paper focuses on Conversational XAI\nfor AI-assisted scientific writing tasks. Drawing from human linguistic\ntheories and formative studies, we identify four design rationales:\n\"multifaceted\", \"controllability\", \"mix-initiative\", \"context-aware\ndrill-down\". We incorporate them into an interactive prototype, ConvXAI, which\nfacilitates heterogeneous AI explanations for scientific writing through\ndialogue. In two studies with 21 users, ConvXAI outperforms a GUI-based\nbaseline on improving human-perceived understanding and writing improvement.\nThe paper further discusses the practical human usage patterns in interacting\nwith ConvXAI for scientific co-writing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chieh-Yang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11554","description":"<p>Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to solving complex problems. However, traditional methods,\nwhich finetune LLMs with tool demonstration data, can be both costly and\nrestricted to a predefined set of tools. Recent in-context learning paradigm\nalleviates these issues, but the limited context length only allows for a few\nshots of demonstrations, leading to suboptimal understandings of the tools.\nMoreover, when there are numerous tools to choose from, in-context learning\ncould completely fail to work. In this paper, we propose an alternative\napproach, $\\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our\napproach represents each $\\underline{tool}$ as a to$\\underline{ken}$\n($\\textit{toolken}$) and learns an embedding for it, enabling tool calls in the\nsame way as generating a regular word token. Once a toolken is triggered, the\nLLM is prompted to complete arguments for the tool to execute. ToolkenGPT\noffers the flexibility to plug in an arbitrary number of tools by expanding the\nset of toolkens on the fly. In addition, it improves tool use by allowing\nextensive demonstration data for learning the toolken embeddings. In diverse\ndomains, including numerical reasoning, knowledge-based question answering, and\nembodied plan generation, our approach effectively augments LLMs with tools and\nsubstantially outperforms various latest baselines. ToolkenGPT demonstrates the\npromising ability to use relevant tools from a large tool set in complex\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shibo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.09442","description":"<p>Deploying Large language models (LLMs) can pose hazards from harmful outputs\nsuch as toxic or dishonest speech. Prior work has introduced tools that elicit\nharmful outputs in order to identify and mitigate these risks. While this is a\nvaluable step toward securing language models, these approaches typically rely\non a pre-existing classifier for undesired outputs. This limits their\napplication to situations where the type of harmful behavior is known with\nprecision beforehand. However, this skips a central challenge of red teaming:\ndeveloping a contextual understanding of the behaviors that a model can\nexhibit. Furthermore, when such a classifier already exists, red teaming has\nlimited marginal value because the classifier could simply be used to filter\ntraining data or model outputs. In this work, we consider red teaming under the\nassumption that the adversary is working from a high-level, abstract\nspecification of undesired behavior. The red team is expected to refine/extend\nthis specification and identify methods to elicit this behavior from the model.\nOur red teaming framework consists of three steps: 1) Exploring the model's\nbehavior in the desired context; 2) Establishing a measurement of undesired\nbehavior (e.g., a classifier trained to reflect human evaluations); and 3)\nExploiting the model's flaws using this measure and an established red teaming\nmethodology. We apply this approach to red team GPT-2 and GPT-3 models to\nsystematically discover classes of prompts that elicit toxic and dishonest\nstatements. In doing so, we also construct and release the CommonClaim dataset\nof 20,000 statements that have been labeled by human subjects as\ncommon-knowledge-true, common-knowledge-false, or neither. Code is available at\nhttps://github.com/thestephencasper/explore_establish_exploit_llms. CommonClaim\nis available at https://github.com/Algorithmic-Alignment-Lab/CommonClaim.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1\">Stephen Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jason Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Joe Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culp_G/0/1/0/all/0/1\">Gatlen Culp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1\">Dylan Hadfield-Menell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Legal Concepts with Augmented Large Language Models (GPT-4). (arXiv:2306.09525v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.09525","description":"<p>Interpreting the meaning of legal open-textured terms is a key task of legal\nprofessionals. An important source for this interpretation is how the term was\napplied in previous court cases. In this paper, we evaluate the performance of\nGPT-4 in generating factually accurate, clear and relevant explanations of\nterms in legislation. We compare the performance of a baseline setup, where\nGPT-4 is directly asked to explain a legal term, to an augmented approach,\nwhere a legal information retrieval module is used to provide relevant context\nto the model, in the form of sentences from case law. We found that the direct\napplication of GPT-4 yields explanations that appear to be of very high quality\non their surface. However, detailed analysis uncovered limitations in terms of\nthe factual accuracy of the explanations. Further, we found that the\naugmentation leads to improved quality, and appears to eliminate the issue of\nhallucination, where models invent incorrect statements. These findings open\nthe door to the building of systems that can autonomously retrieve relevant\nsentences from case law and condense them into a useful explanation for legal\nscholars, educators or practicing lawyers alike.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savelka_J/0/1/0/all/0/1\">Jaromir Savelka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashley_K/0/1/0/all/0/1\">Kevin D. Ashley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_M/0/1/0/all/0/1\">Morgan A. Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westermann_H/0/1/0/all/0/1\">Hannes Westermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huihui Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.09896","description":"<p>Large Language Models (LLMs) have shown remarkable aptitude in code\ngeneration but still struggle on challenging programming tasks. Self-repair --\nin which the model debugs and fixes mistakes in its own code -- has recently\nbecome a popular way to boost performance in these settings. However, only very\nlimited studies on how and when self-repair works effectively exist in the\nliterature, and one might wonder to what extent a model is really capable of\nproviding accurate feedback on why the code is wrong when that code was\ngenerated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's\nability to perform self-repair on APPS, a challenging dataset consisting of\ndiverse coding challenges. To do so, we first establish a new evaluation\nstrategy dubbed pass@t that measures the pass rate of the tasks against the\ntotal number of tokens sampled from the model, enabling a fair comparison to\npurely sampling-based approaches. With this evaluation strategy, we find that\nthe effectiveness of self-repair is only seen in GPT-4. We also observe that\nself-repair is bottlenecked by the feedback stage; using GPT-4 to give feedback\non the programs generated by GPT-3.5 and using expert human programmers to give\nfeedback on the programs generated by GPT-4, we unlock significant performance\ngains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olausson_T/0/1/0/all/0/1\">Theo X. Olausson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inala_J/0/1/0/all/0/1\">Jeevana Priya Inala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solar_Lezama_A/0/1/0/all/0/1\">Armando Solar-Lezama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quilt-1M: One Million Image-Text Pairs for Histopathology. (arXiv:2306.11207v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2306.11207","description":"<p>Recent accelerations in multi-modal applications have been made possible with\nthe plethora of image and text data available online. However, the scarcity of\nanalogous data in the medical field, specifically in histopathology, has halted\ncomparable progress. To enable similar representation learning for\nhistopathology, we turn to YouTube, an untapped resource of videos, offering\n$1,087$ hours of valuable educational histopathology videos from expert\nclinicians. From YouTube, we curate Quilt: a large-scale vision-language\ndataset consisting of $768,826$ image and text pairs. Quilt was automatically\ncurated using a mixture of models, including large language models, handcrafted\nalgorithms, human knowledge databases, and automatic speech recognition. In\ncomparison, the most comprehensive datasets curated for histopathology amass\nonly around $200$K samples. We combine Quilt with datasets from other sources,\nincluding Twitter, research papers, and the internet in general, to create an\neven larger dataset: Quilt-1M, with $1$M paired image-text samples, marking it\nas the largest vision-language histopathology dataset to date. We demonstrate\nthe value of Quilt-1M by fine-tuning a pre-trained CLIP model. Our model\noutperforms state-of-the-art models on both zero-shot and linear probing tasks\nfor classifying new histopathology images across $13$ diverse patch-level\ndatasets of $8$ different sub-pathologies and cross-modal retrieval tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ikezogwo_W/0/1/0/all/0/1\">Wisdom Oluchi Ikezogwo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seyfioglu_M/0/1/0/all/0/1\">Mehmet Saygin Seyfioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghezloo_F/0/1/0/all/0/1\">Fatemeh Ghezloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geva_D/0/1/0/all/0/1\">Dylan Stefan Chan Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_F/0/1/0/all/0/1\">Fatwir Sheikh Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_P/0/1/0/all/0/1\">Pavan Kumar Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapiro_L/0/1/0/all/0/1\">Linda Shapiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}