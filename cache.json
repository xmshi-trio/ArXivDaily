{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-08-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"PIPPA: A Partially Synthetic Conversational Dataset. (arXiv:2308.05884v1 [cs.CL])","link":"http://arxiv.org/abs/2308.05884","description":"<p>With the emergence of increasingly powerful large language models, there is a\nburgeoning interest in leveraging these models for casual conversation and\nrole-play applications. However, existing conversational and role-playing\ndatasets often fail to capture the diverse and nuanced interactions typically\nexhibited by real-world role-play participants. To address this limitation and\ncontribute to the rapidly growing field, we introduce a partially-synthetic\ndataset named PIPPA (Personal Interaction Pairs between People and AI). PIPPA\nis a result of a community-driven crowdsourcing effort involving a group of\nrole-play enthusiasts. The dataset comprises over 1 million utterances that are\ndistributed across 26,000 conversation sessions and provides a rich resource\nfor researchers and AI developers to explore and refine conversational AI\nsystems in the context of role-play scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gosling_T/0/1/0/all/0/1\">Tear Gosling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dale_A/0/1/0/all/0/1\">Alpin Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts. (arXiv:2308.05935v1 [cs.CL])","link":"http://arxiv.org/abs/2308.05935","description":"<p>Teaching assistants have played essential roles in the long history of\neducation. However, few MOOC platforms are providing human or virtual teaching\nassistants to support learning for massive online students due to the\ncomplexity of real-world online education scenarios and the lack of training\ndata. In this paper, we present a virtual MOOC teaching assistant, LittleMu\nwith minimum labeled training data, to provide question answering and chit-chat\nservices. Consisting of two interactive modules of heterogeneous retrieval and\nlanguage model prompting, LittleMu first integrates structural, semi- and\nunstructured knowledge sources to support accurate answers for a wide range of\nquestions. Then, we design delicate demonstrations named \"Chain of Teach\"\nprompts to exploit the large-scale pre-trained model to handle complex\nuncollected questions. Except for question answering, we develop other\neducational services such as knowledge-grounded chit-chat. We test the system's\nperformance via both offline evaluation and online deployment. Since May 2020,\nour LittleMu system has served over 80,000 users with over 300,000 queries from\nover 500 courses on XuetangX MOOC platform, which continuously contributes to a\nmore convenient and fair education. Our code, services, and dataset will be\navailable at https://github.com/THU-KEG/VTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1\">Shangqing Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zijun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tweet Sentiment Extraction using Viterbi Algorithm with Transfer Learning. (arXiv:2308.05973v1 [cs.AI])","link":"http://arxiv.org/abs/2308.05973","description":"<p>Tweet sentiment extraction extracts the most significant portion of the\nsentence, determining whether the sentiment is positive or negative. This\nresearch aims to identify the part of tweet sentences that strikes any emotion.\nTo reach this objective, we continue improving the Viterbi algorithm previously\nmodified by the author to make it able to receive pre-trained model parameters.\nWe introduce the confidence score and vector as two indicators responsible for\nevaluating the model internally before assessing the final results. We then\npresent a method to fine-tune this nonparametric model. We found that the model\ngets highly explainable as the confidence score vector reveals precisely where\nthe least confidence predicted states are and if the modifications approved\nameliorate the confidence score or if the tuning is going in the wrong\ndirection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baklouti_Z/0/1/0/all/0/1\">Zied Baklouti</a> (UPCit&#xe9;, ENIT)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study. (arXiv:2308.06017v1 [cs.CL])","link":"http://arxiv.org/abs/2308.06017","description":"<p>In machine translation tasks, the relationship between model complexity and\nperformance is often presumed to be linear, driving an increase in the number\nof parameters and consequent demands for computational resources like multiple\nGPUs. To explore this assumption, this study systematically investigates the\neffects of hyperparameters through ablation on a sequence-to-sequence machine\ntranslation pipeline, utilizing a single NVIDIA A100 GPU. Contrary to\nexpectations, our experiments reveal that combinations with the most parameters\nwere not necessarily the most effective. This unexpected insight prompted a\ncareful reduction in parameter sizes, uncovering \"sweet spots\" that enable\ntraining sophisticated models on a single GPU without compromising translation\nquality. The findings demonstrate an intricate relationship between\nhyperparameter selection, model size, and computational resource needs. The\ninsights from this study contribute to the ongoing efforts to make machine\ntranslation more accessible and cost-effective, emphasizing the importance of\nprecise hyperparameter tuning over mere scaling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verma_L/0/1/0/all/0/1\">Luv Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolhatkar_K/0/1/0/all/0/1\">Ketaki N. Kolhatkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v1 [cs.AI])","link":"http://arxiv.org/abs/2308.06032","description":"<p>Large Language Models (LLMs) could enhance access to the legal system.\nHowever, empirical research on their effectiveness in conducting legal tasks is\nscant. We study securities cases involving cryptocurrencies as one of numerous\ncontexts where AI could support the legal process, studying LLMs' legal\nreasoning and drafting capabilities. We examine whether a) an LLM can\naccurately determine which laws are potentially being violated from a fact\npattern, and b) whether there is a difference in juror decision-making based on\ncomplaints written by a lawyer compared to an LLM. We feed fact patterns from\nreal-life cases to GPT-3.5 and evaluate its ability to determine correct\npotential violations from the scenario and exclude spurious violations. Second,\nwe had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's\nlegal reasoning skills proved weak, though we expect improvement in future\nmodels, particularly given the violations it suggested tended to be correct (it\nmerely missed additional, correct violations). GPT-3.5 performed better at\nlegal drafting, and jurors' decisions were not statistically significantly\nassociated with the author of the document upon which they based their\ndecisions. Because LLMs cannot satisfactorily conduct legal reasoning tasks,\nthey would be unable to replace lawyers at this stage. However, their drafting\nskills (though, perhaps, still inferior to lawyers), could provide access to\njustice for more individuals by reducing the cost of legal services. Our\nresearch is the first to systematically study LLMs' legal drafting and\nreasoning capabilities in litigation, as well as in securities law and\ncryptocurrency-related misconduct.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trozze_A/0/1/0/all/0/1\">Arianna Trozze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davies_T/0/1/0/all/0/1\">Toby Davies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_B/0/1/0/all/0/1\">Bennett Kleinberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing. (arXiv:2308.06035v1 [cs.AI])","link":"http://arxiv.org/abs/2308.06035","description":"<p>The advanced language processing abilities of large language models (LLMs)\nhave stimulated debate over their capacity to replicate human-like cognitive\nprocesses. One differentiating factor between language processing in LLMs and\nhumans is that language input is often grounded in more than one perceptual\nmodality, whereas most LLMs process solely text-based information. Multimodal\ngrounding allows humans to integrate - e.g. visual context with linguistic\ninformation and thereby place constraints on the space of upcoming words,\nreducing cognitive load and improving perception and comprehension. Recent\nmultimodal LLMs (mLLMs) combine visual and linguistic embedding spaces with a\ntransformer type attention mechanism for next-word prediction. To what extent\ndoes predictive language processing based on multimodal input align in mLLMs\nand humans? To answer this question, 200 human participants watched short\naudio-visual clips and estimated the predictability of an upcoming verb or\nnoun. The same clips were processed by the mLLM CLIP, with predictability\nscores based on a comparison of image and text feature vectors. Eye-tracking\nwas used to estimate what visual features participants attended to, and CLIP's\nvisual attention weights were recorded. We find that human estimates of\npredictability align significantly with CLIP scores, but not for a unimodal LLM\nof comparable parameter size. Further, alignment vanished when CLIP's visual\nattention weights were perturbed, and when the same input was fed to a\nmultimodal model without attention. Analysing attention patterns, we find a\nsignificant spatial overlap between CLIP's visual attention weights and human\neye-tracking data. Results suggest that comparable processes of integrating\nmultimodal information, guided by attention to relevant visual features,\nsupports predictive language processing in mLLMs and humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kewenig_V/0/1/0/all/0/1\">Viktor Kewenig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edwards_C/0/1/0/all/0/1\">Christopher Edwards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DEstalenx_Q/0/1/0/all/0/1\">Quitterie Lacome DEstalenx</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rechardt_A/0/1/0/all/0/1\">Akilles Rechardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skipper_J/0/1/0/all/0/1\">Jeremy I Skipper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vigliocco_G/0/1/0/all/0/1\">Gabriella Vigliocco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Guide Human Experts via Personalized Large Language Models. (arXiv:2308.06039v1 [cs.AI])","link":"http://arxiv.org/abs/2308.06039","description":"<p>In learning to defer, a predictor identifies risky decisions and defers them\nto a human expert. One key issue with this setup is that the expert may end up\nover-relying on the machine's decisions, due to anchoring bias. At the same\ntime, whenever the machine chooses the deferral option the expert has to take\ndecisions entirely unassisted. As a remedy, we propose learning to guide (LTG),\nan alternative framework in which -- rather than suggesting ready-made\ndecisions -- the machine provides guidance useful to guide decision-making, and\nthe human is entirely responsible for coming up with a decision. We also\nintroduce SLOG, an LTG implementation that leverages (a small amount of) human\nsupervision to convert a generic large language model into a module capable of\ngenerating textual guidance, and present preliminary but promising results on a\nmedical diagnosis task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debodeep Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teso_S/0/1/0/all/0/1\">Stefano Teso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1\">Andrea Passerini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Case Study on Context Encoding in Multi-Encoder based Document-Level Neural Machine Translation. (arXiv:2308.06063v1 [cs.CL])","link":"http://arxiv.org/abs/2308.06063","description":"<p>Recent studies have shown that the multi-encoder models are agnostic to the\nchoice of context, and the context encoder generates noise which helps improve\nthe models in terms of BLEU score. In this paper, we further explore this idea\nby evaluating with context-aware pronoun translation test set by training\nmulti-encoder models trained on three different context settings viz, previous\ntwo sentences, random two sentences, and a mix of both as context.\nSpecifically, we evaluate the models on the ContraPro test set to study how\ndifferent contexts affect pronoun translation accuracy. The results show that\nthe model can perform well on the ContraPro test set even when the context is\nrandom. We also analyze the source representations to study whether the context\nencoder generates noise. Our analysis shows that the context encoder provides\nsufficient information to learn discourse-level information. Additionally, we\nobserve that mixing the selected context (the previous two sentences in this\ncase) and the random context is generally better than the other settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Appicharla_R/0/1/0/all/0/1\">Ramakrishna Appicharla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1\">Baban Gain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Santanu Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling. (arXiv:2308.06077v1 [cs.CL])","link":"http://arxiv.org/abs/2308.06077","description":"<p>Generative language models (LMs) have become omnipresent across data science.\nFor a wide variety of tasks, inputs can be phrased as natural language prompts\nfor an LM, from whose output the solution can then be extracted. LM performance\nhas consistently been increasing with model size - but so has the monetary cost\nof querying the ever larger models. Importantly, however, not all inputs are\nequally hard: some require larger LMs for obtaining a satisfactory solution,\nwhereas for others smaller LMs suffice. Based on this fact, we design a\nframework for Cost-Effective Language Model Choice (CELMOC). Given a set of\ninputs and a set of candidate LMs, CELMOC judiciously assigns each input to an\nLM predicted to do well on the input according to a so-called meta-model,\naiming to achieve high overall performance at low cost. The cost-performance\ntrade-off can be flexibly tuned by the user. Options include, among others,\nmaximizing total expected performance (or the number of processed inputs) while\nstaying within a given cost budget, or minimizing total cost while processing\nall inputs. We evaluate CELMOC on 14 datasets covering five natural language\ntasks, using four candidate LMs of vastly different size and cost. With CELMOC,\nwe match the performance of the largest available LM while achieving a cost\nreduction of 63%. Via our publicly available library, researchers as well as\npractitioners can thus save large amounts of money without sacrificing\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakota_M/0/1/0/all/0/1\">Marija &#x160;akota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes. (arXiv:2308.06095v1 [cs.CL])","link":"http://arxiv.org/abs/2308.06095","description":"<p>Recent conditional language models are able to continue any kind of text\nsource in an often seemingly fluent way. This fact encouraged research in the\narea of open-domain conversational systems that are based on powerful language\nmodels and aim to imitate an interlocutor by generating appropriate\ncontributions to a written dialogue. From a linguistic perspective, however,\nthe complexity of contributing to a conversation is high. In this survey, we\ninterpret Grice's maxims of cooperative conversation from the perspective of\nthis specific research area and systematize the literature under the aspect of\nwhat makes a contribution appropriate: A neural conversation model has to be\nfluent, informative, consistent, coherent, and follow social norms. In order to\nensure these qualities, recent approaches try to tame the underlying language\nmodels at various intervention points, such as data, training regime or\ndecoding. Sorted by these categories and intervention points, we discuss\npromising attempts and suggest novel ways for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galetzka_F/0/1/0/all/0/1\">Fabian Galetzka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_A/0/1/0/all/0/1\">Anne Beyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlangen_D/0/1/0/all/0/1\">David Schlangen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models. (arXiv:2308.06111v1 [cs.CL])","link":"http://arxiv.org/abs/2308.06111","description":"<p>Auditing financial documents is a very tedious and time-consuming process. As\nof today, it can already be simplified by employing AI-based solutions to\nrecommend relevant text passages from a report for each legal requirement of\nrigorous accounting standards. However, these methods need to be fine-tuned\nregularly, and they require abundant annotated data, which is often lacking in\nindustrial environments. Hence, we present ZeroShotALI, a novel recommender\nsystem that leverages a state-of-the-art large language model (LLM) in\nconjunction with a domain-specifically optimized transformer-based\ntext-matching solution. We find that a two-step approach of first retrieving a\nnumber of best matching document sections per legal requirement with a custom\nBERT-based model and second filtering these selections using an LLM yields\nsignificant performance improvements over existing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hillebrand_L/0/1/0/all/0/1\">Lars Hillebrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_A/0/1/0/all/0/1\">Armin Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deusser_T/0/1/0/all/0/1\">Tobias Deu&#xdf;er</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilmaghani_T/0/1/0/all/0/1\">Tim Dilmaghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaled_M/0/1/0/all/0/1\">Mohamed Khaled</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kliem_B/0/1/0/all/0/1\">Bernd Kliem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loitz_R/0/1/0/all/0/1\">R&#xfc;diger Loitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pielka_M/0/1/0/all/0/1\">Maren Pielka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonhard_D/0/1/0/all/0/1\">David Leonhard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauckhage_C/0/1/0/all/0/1\">Christian Bauckhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifa_R/0/1/0/all/0/1\">Rafet Sifa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping. (arXiv:2308.06112v1 [cs.SD])","link":"http://arxiv.org/abs/2308.06112","description":"<p>Visual Speech Recognition (VSR) differs from the common perception tasks as\nit requires deeper reasoning over the video sequence, even by human experts.\nDespite the recent advances in VSR, current approaches rely on labeled data to\nfully train or finetune their models predicting the target speech. This hinders\ntheir ability to generalize well beyond the training set and leads to\nperformance degeneration under out-of-distribution challenging scenarios.\nUnlike previous works that involve auxiliary losses or complex training\nprocedures and architectures, we propose a simple approach, named Lip2Vec that\nis based on learning a prior model. Given a robust visual speech encoder, this\nnetwork maps the encoded latent representations of the lip sequence to their\ncorresponding latents from the audio pair, which are sufficiently invariant for\neffective text decoding. The generated audio representation is then decoded to\ntext using an off-the-shelf Audio Speech Recognition (ASR) model. The proposed\nmodel compares favorably with fully-supervised learning methods on the LRS3\ndataset achieving 26 WER. Unlike SoTA approaches, our model keeps a reasonable\nperformance on the VoxCeleb test set. We believe that reprogramming the VSR as\nan ASR task narrows the performance gap between the two and paves the way for\nmore flexible formulations of lip reading.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Djilali_Y/0/1/0/all/0/1\">Yasser Abdelaziz Dahou Djilali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1\">Sanath Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boussaid_H/0/1/0/all/0/1\">Haithem Boussaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almazrouei_E/0/1/0/all/0/1\">Ebtessam Almazrouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debbah_M/0/1/0/all/0/1\">Merouane Debbah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Joint Speech-Text Representations Without Alignment. (arXiv:2308.06125v1 [cs.CL])","link":"http://arxiv.org/abs/2308.06125","description":"<p>The last year has seen astonishing progress in text-prompted image generation\npremised on the idea of a cross-modal representation space in which the text\nand image domains are represented jointly. In ASR, this idea has found\napplication as joint speech-text encoders that can scale to the capacities of\nvery large parameter models by being trained on both unpaired speech and text.\nWhile these methods show promise, they have required special treatment of the\nsequence-length mismatch inherent in speech and text, either by up-sampling\nheuristics or an explicit alignment model. In this work, we offer evidence that\njoint speech-text encoders naturally achieve consistent representations across\nmodalities by disregarding sequence length, and argue that consistency losses\ncould forgive length differences and simply assume the best alignment. We show\nthat such a loss improves downstream WER in both a large-parameter monolingual\nand multilingual system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peyser_C/0/1/0/all/0/1\">Cal Peyser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Ke Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1\">Michael Picheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models. (arXiv:2308.06144v1 [cs.IR])","link":"http://arxiv.org/abs/2308.06144","description":"<p>The Forum for Information Retrieval (FIRE) started a shared task this year\nfor classification of comments of different code segments. This is binary text\nclassification task where the objective is to identify whether comments given\nfor certain code segments are relevant or not. The BioNLP-IISERB group at the\nIndian Institute of Science Education and Research Bhopal (IISERB) participated\nin this task and submitted five runs for five different models. The paper\npresents the overview of the models and other significant findings on the\ntraining corpus. The methods involve different feature engineering schemes and\ntext classification techniques. The performance of the classical bag of words\nmodel and transformer-based models were explored to identify significant\nfeatures from the given training corpus. We have explored different classifiers\nviz., random forest, support vector machine and logistic regression using the\nbag of words model. Furthermore, the pre-trained transformer based models like\nBERT, RoBERT and ALBERT were also used by fine-tuning them on the given\ntraining corpus. The performance of different such models over the training\ncorpus were reported and the best five models were implemented on the given\ntest corpus. The empirical results show that the bag of words model outperforms\nthe transformer based models, however, the performance of our runs are not\nreasonably well in both training and test corpus. This paper also addresses the\nlimitations of the models and scope for further improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1\">Sruthi S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_T/0/1/0/all/0/1\">Tanmay Basu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Conditioned BERT for Joint Intent Detection and Slot-filling. (arXiv:2308.06165v1 [cs.CL])","link":"http://arxiv.org/abs/2308.06165","description":"<p>Dialogue systems need to deal with the unpredictability of user intents to\ntrack dialogue state and the heterogeneity of slots to understand user\npreferences. In this paper we investigate the hypothesis that solving these\nchallenges as one unified model will allow the transfer of parameter support\ndata across the different tasks. The proposed principled model is based on a\nTransformer encoder, trained on multiple tasks, and leveraged by a rich input\nthat conditions the model on the target inferences. Conditioning the\nTransformer encoder on multiple target inferences over the same corpus, i.e.,\nintent and multiple slot types, allows learning richer language interactions\nthan a single-task model would be able to. In fact, experimental results\ndemonstrate that conditioning the model on an increasing number of dialogue\ninference tasks leads to improved results: on the MultiWOZ dataset, the joint\nintent and slot detection can be improved by 3.2\\% by conditioning on intent,\n10.8\\% by conditioning on slot and 14.4\\% by conditioning on both intent and\nslots. Moreover, on real conversations with Farfetch costumers, the proposed\nconditioned BERT can achieve high joint-goal and intent detection performance\nthroughout a dialogue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tavares_D/0/1/0/all/0/1\">Diogo Tavares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azevedo_P/0/1/0/all/0/1\">Pedro Azevedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semedo_D/0/1/0/all/0/1\">David Semedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sousa_R/0/1/0/all/0/1\">Ricardo Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magalhaes_J/0/1/0/all/0/1\">Jo&#xe3;o Magalh&#xe3;es</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Guest Nationality Composition from Hotel Reviews. (arXiv:2308.06175v1 [cs.CL])","link":"http://arxiv.org/abs/2308.06175","description":"<p>Many hotels target guest acquisition efforts to specific markets in order to\nbest anticipate individual preferences and needs of their guests. Likewise,\nsuch strategic positioning is a prerequisite for efficient marketing budget\nallocation. Official statistics report on the number of visitors from different\ncountries, but no fine-grained information on the guest composition of\nindividual businesses exists. There is, however, growing interest in such data\nfrom competitors, suppliers, researchers and the general public. We demonstrate\nhow machine learning can be leveraged to extract references to guest\nnationalities from unstructured text reviews in order to dynamically assess and\nmonitor the dynamics of guest composition of individual businesses. In\nparticular, we show that a rather simple architecture of pre-trained embeddings\nand stacked LSTM layers provides a better performance-runtime tradeoff than\nmore complex state-of-the-art language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Groger_F/0/1/0/all/0/1\">Fabian Gr&#xf6;ger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pouly_M/0/1/0/all/0/1\">Marc Pouly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinner_F/0/1/0/all/0/1\">Flavia Tinner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandes_L/0/1/0/all/0/1\">Leif Brandes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Text Classification on Free Text Comments in Patient-Reported Outcome Measures. (arXiv:2308.06199v1 [cs.CL])","link":"http://arxiv.org/abs/2308.06199","description":"<p>Free text comments (FTC) in patient-reported outcome measures (PROMs) data\nare typically analysed using manual methods, such as content analysis, which is\nlabour-intensive and time-consuming. Machine learning analysis methods are\nlargely unsupervised, necessitating post-analysis interpretation. Weakly\nsupervised text classification (WSTC) can be a valuable method of analysis to\nclassify domain-specific text data in which there is limited labelled data. In\nthis paper, we apply five WSTC techniques to FTC in PROMs data to identify\nhealth-related quality of life (HRQoL) themes reported by colorectal cancer\npatients. The WSTC methods label all the themes mentioned in the FTC. The\nresults showed moderate performance on the PROMs data, mainly due to the\nprecision of the models, and variation between themes. Evaluation of the\nclassification performance illustrated the potential and limitations of keyword\nbased WSTC to label PROMs FTC when labelled data is limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Linton_A/0/1/0/all/0/1\">Anna-Grace Linton</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrova_V/0/1/0/all/0/1\">Vania Dimitrova</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Downing_A/0/1/0/all/0/1\">Amy Downing</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Wagland_R/0/1/0/all/0/1\">Richard Wagland</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Glaser_A/0/1/0/all/0/1\">Adam Glaser</a> (3) ((1) UKRI CDT in AI for Medical Diagnosis and Care, University of Leeds, UK, (2) School of Computing, University of Leeds, UK, (3) School of Medicine, University of Leeds, UK, (4) School of Health Sciences, University of Southampton, UK)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thinking Like an Expert:Multimodal Hypergraph-of-Thought (HoT) Reasoning to boost Foundation Modals. (arXiv:2308.06207v1 [cs.CL])","link":"http://arxiv.org/abs/2308.06207","description":"<p>Reasoning ability is one of the most crucial capabilities of a foundation\nmodel, signifying its capacity to address complex reasoning tasks.\nChain-of-Thought (CoT) technique is widely regarded as one of the effective\nmethods for enhancing the reasoning ability of foundation models and has\ngarnered significant attention. However, the reasoning process of CoT is\nlinear, step-by-step, similar to personal logical reasoning, suitable for\nsolving general and slightly complicated problems. On the contrary, the\nthinking pattern of an expert owns two prominent characteristics that cannot be\nhandled appropriately in CoT, i.e., high-order multi-hop reasoning and\nmultimodal comparative judgement. Therefore, the core motivation of this paper\nis transcending CoT to construct a reasoning paradigm that can think like an\nexpert. The hyperedge of a hypergraph could connect various vertices, making it\nnaturally suitable for modelling high-order relationships. Inspired by this,\nthis paper innovatively proposes a multimodal Hypergraph-of-Thought (HoT)\nreasoning paradigm, which enables the foundation models to possess the\nexpert-level ability of high-order multi-hop reasoning and multimodal\ncomparative judgement. Specifically, a textual hypergraph-of-thought is\nconstructed utilizing triple as the primary thought to model higher-order\nrelationships, and a hyperedge-of-thought is generated through multi-hop\nwalking paths to achieve multi-hop inference. Furthermore, we devise a visual\nhypergraph-of-thought to interact with the textual hypergraph-of-thought via\nCross-modal Co-Attention Graph Learning for multimodal comparative\nverification. Experimentations on the ScienceQA benchmark demonstrate the\nproposed HoT-based T5 outperforms CoT-based GPT3.5 and chatGPT, which is on par\nwith CoT-based GPT4 with a lower model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_F/0/1/0/all/0/1\">Fanglong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Changyuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jintao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zequn Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Li Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Large Language Model Enhanced Conversational Recommender System. (arXiv:2308.06212v1 [cs.IR])","link":"http://arxiv.org/abs/2308.06212","description":"<p>Conversational recommender systems (CRSs) aim to recommend high-quality items\nto users through a dialogue interface. It usually contains multiple sub-tasks,\nsuch as user preference elicitation, recommendation, explanation, and item\ninformation search. To develop effective CRSs, there are some challenges: 1)\nhow to properly manage sub-tasks; 2) how to effectively solve different\nsub-tasks; and 3) how to correctly generate responses that interact with users.\nRecently, Large Language Models (LLMs) have exhibited an unprecedented ability\nto reason and generate, presenting a new opportunity to develop more powerful\nCRSs. In this work, we propose a new LLM-based CRS, referred to as LLMCRS, to\naddress the above challenges. For sub-task management, we leverage the\nreasoning ability of LLM to effectively manage sub-task. For sub-task solving,\nwe collaborate LLM with expert models of different sub-tasks to achieve the\nenhanced performance. For response generation, we utilize the generation\nability of LLM as a language interface to better interact with users.\nSpecifically, LLMCRS divides the workflow into four stages: sub-task detection,\nmodel matching, sub-task execution, and response generation. LLMCRS also\ndesigns schema-based instruction, demonstration-based instruction, dynamic\nsub-task and model matching, and summary-based generation to instruct LLM to\ngenerate desired results in the workflow. Finally, to adapt LLM to\nconversational recommendations, we also propose to fine-tune LLM with\nreinforcement learning from CRSs performance feedback, referred to as RLPF.\nExperimental results on benchmark datasets show that LLMCRS with RLPF\noutperforms the existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhenghai Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1\">Qingpeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lantao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1\">Kun Gai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KETM:A Knowledge-Enhanced Text Matching method. (arXiv:2308.06235v1 [cs.CL])","link":"http://arxiv.org/abs/2308.06235","description":"<p>Text matching is the task of matching two texts and determining the\nrelationship between them, which has extensive applications in natural language\nprocessing tasks such as reading comprehension, and Question-Answering systems.\nThe mainstream approach is to compute text representations or to interact with\nthe text through attention mechanism, which is effective in text matching\ntasks. However, the performance of these models is insufficient for texts that\nrequire commonsense knowledge-based reasoning. To this end, in this paper, We\nintroduce a new model for text matching called the Knowledge Enhanced Text\nMatching model (KETM), to enrich contextual representations with real-world\ncommon-sense knowledge from external knowledge sources to enhance our model\nunderstanding and reasoning. First, we use Wiktionary to retrieve the text word\ndefinitions as our external knowledge. Secondly, we feed text and knowledge to\nthe text matching module to extract their feature vectors. The text matching\nmodule is used as an interaction module by integrating the encoder layer, the\nco-attention layer, and the aggregation layer. Specifically, the interaction\nprocess is iterated several times to obtain in-depth interaction information\nand extract the feature vectors of text and knowledge by multi-angle pooling.\nThen, we fuse text and knowledge using a gating mechanism to learn the ratio of\ntext and knowledge fusion by a neural network that prevents noise generated by\nknowledge. After that, experimental validation on four datasets are carried\nout, and the experimental results show that our proposed model performs well on\nall four datasets, and the performance of our method is improved compared to\nthe base model without adding external knowledge, which validates the\neffectiveness of our proposed method. The code is available at\nhttps://github.<a href=\"/abs/com/1094701\">com/1094701</a>018/KETM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kexin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yahui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1\">Guozhe Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Rongyi Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Covid-19 Public Sentiment Analysis for Indian Tweets Classification. (arXiv:2308.06241v1 [cs.CL])","link":"http://arxiv.org/abs/2308.06241","description":"<p>When any extraordinary event takes place in the world wide area, it is the\nsocial media that acts as the fastest carrier of the news along with the\nconsequences dealt with that event. One can gather much information through\nsocial networks regarding the sentiments, behavior, and opinions of the people.\nIn this paper, we focus mainly on sentiment analysis of twitter data of India\nwhich comprises of COVID-19 tweets. We show how Twitter data has been extracted\nand then run sentimental analysis queries on it. This is helpful to analyze the\ninformation in the tweets where opinions are highly unstructured,\nheterogeneous, and are either positive or negative or neutral in some cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akhter_M/0/1/0/all/0/1\">Mohammad Maksood Akhter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Devpriya Kanojia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Alignment with Instruction Backtranslation. (arXiv:2308.06259v1 [cs.CL])","link":"http://arxiv.org/abs/2308.06259","description":"<p>We present a scalable method to build a high quality instruction following\nlanguage model by automatically labelling human-written text with corresponding\ninstructions. Our approach, named instruction backtranslation, starts with a\nlanguage model finetuned on a small amount of seed data, and a given web\ncorpus. The seed model is used to construct training examples by generating\ninstruction prompts for web documents (self-augmentation), and then selecting\nhigh quality examples from among these candidates (self-curation). This data is\nthen used to finetune a stronger model. Finetuning LLaMa on two iterations of\nour approach yields a model that outperforms all other LLaMa-based models on\nthe Alpaca leaderboard not relying on distillation data, demonstrating highly\neffective self-alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Ping Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07306","description":"<p>A major challenge in structured prediction is to represent the\ninterdependencies within output structures. When outputs are structured as\nsequences, linear-chain conditional random fields (CRFs) are a widely used\nmodel class which can learn \\textit{local} dependencies in the output. However,\nthe CRF's Markov assumption makes it impossible for CRFs to represent\ndistributions with \\textit{nonlocal} dependencies, and standard CRFs are unable\nto respect nonlocal constraints of the data (such as global arity constraints\non output labels). We present a generalization of CRFs that can enforce a broad\nclass of constraints, including nonlocal ones, by specifying the space of\npossible output structures as a regular language $\\mathcal{L}$. The resulting\nregular-constrained CRF (RegCCRF) has the same formal properties as a standard\nCRF, but assigns zero probability to all label sequences not in $\\mathcal{L}$.\nNotably, RegCCRFs can incorporate their constraints during training, while\nrelated models only enforce constraints during decoding. We prove that\nconstrained training is never worse than constrained decoding, and show\nempirically that it can be substantially better in practice. Additionally, we\ndemonstrate a practical benefit on downstream tasks by incorporating a RegCCRF\ninto a deep neural model for semantic role labeling, exceeding state-of-the-art\nresults on a standard dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papay_S/0/1/0/all/0/1\">Sean Papay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Compact End-to-End Model with Local and Global Context for Spoken Language Identification. (arXiv:2210.15781v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2210.15781","description":"<p>We introduce TitaNet-LID, a compact end-to-end neural network for Spoken\nLanguage Identification (LID) that is based on the ContextNet architecture.\nTitaNet-LID employs 1D depth-wise separable convolutions and\nSqueeze-and-Excitation layers to effectively capture local and global context\nwithin an utterance. Despite its small size, TitaNet-LID achieves performance\nsimilar to state-of-the-art models on the VoxLingua107 dataset while being 10\ntimes smaller. Furthermore, it can be easily adapted to new acoustic conditions\nand unseen languages through simple fine-tuning, achieving a state-of-the-art\naccuracy of 88.2% on the FLEURS benchmark. Our model is scalable and can\nachieve a better trade-off between accuracy and speed. TitaNet-LID performs\nwell even on short utterances less than 5s in length, indicating its robustness\nto input length.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jia_F/0/1/0/all/0/1\">Fei Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koluguri_N/0/1/0/all/0/1\">Nithin Rao Koluguri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balam_J/0/1/0/all/0/1\">Jagadeesh Balam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"There is more than one kind of robustness: Fooling Whisper with adversarial examples. (arXiv:2210.17316v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2210.17316","description":"<p>Whisper is a recent Automatic Speech Recognition (ASR) model displaying\nimpressive robustness to both out-of-distribution inputs and random noise. In\nthis work, we show that this robustness does not carry over to adversarial\nnoise. We show that we can degrade Whisper performance dramatically, or even\ntranscribe a target sentence of our choice, by generating very small input\nperturbations with Signal Noise Ratio of 35-45dB. We also show that by fooling\nthe Whisper language detector we can very easily degrade the performance of\nmultilingual models. These vulnerabilities of a widely popular open-source\nmodel have practical security implications and emphasize the need for\nadversarially robust ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Olivier_R/0/1/0/all/0/1\">Raphael Olivier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia. (arXiv:2211.00732v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2211.00732","description":"<p>Online encyclopedias, such as Wikipedia, have been well-developed and\nresearched in the last two decades. One can find any attributes or other\ninformation of a wiki item on a wiki page edited by a community of volunteers.\nHowever, the traditional text, images and tables can hardly express some\naspects of an wiki item. For example, when we talk about ``Shiba Inu'', one may\ncare more about ``How to feed it'' or ``How to train it not to protect its\nfood''. Currently, short-video platforms have become a hallmark in the online\nworld. Whether you're on TikTok, Instagram, Kuaishou, or YouTube Shorts,\nshort-video apps have changed how we consume and create content today. Except\nfor producing short videos for entertainment, we can find more and more authors\nsharing insightful knowledge widely across all walks of life. These short\nvideos, which we call knowledge videos, can easily express any aspects (e.g.\nhair or how-to-feed) consumers want to know about an item (e.g. Shiba Inu), and\nthey can be systematically analyzed and organized like an online encyclopedia.\nIn this paper, we propose Kuaipedia, a large-scale multi-modal encyclopedia\nconsisting of items, aspects, and short videos lined to them, which was\nextracted from billions of videos of Kuaishou (Kwai), a well-known short-video\nplatform in China. We first collected items from multiple sources and mined\nuser-centered aspects from millions of users' queries to build an item-aspect\ntree. Then we propose a new task called ``multi-modal item-aspect linking'' as\nan expansion of ``entity linking'' to link short videos into item-aspect pairs\nand build the whole short-video encyclopedia. Intrinsic evaluations show that\nour encyclopedia is of large scale and highly accurate. We also conduct\nsufficient extrinsic experiments to show how Kuaipedia can help fundamental\napplications such as entity typing and entity linking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Haojie Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Z/0/1/0/all/0/1\">Zepeng Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuzhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Ruiji Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers are Short Text Classifiers: A Study of Inductive Short Text Classifiers on Benchmarks and Real-world Datasets. (arXiv:2211.16878v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.16878","description":"<p>Short text classification is a crucial and challenging aspect of Natural\nLanguage Processing. For this reason, there are numerous highly specialized\nshort text classifiers. However, in recent short text research, State of the\nArt (SOTA) methods for traditional text classification, particularly the pure\nuse of Transformers, have been unexploited. In this work, we examine the\nperformance of a variety of short text classifiers as well as the top\nperforming traditional text classifier. We further investigate the effects on\ntwo new real-world short text datasets in an effort to address the issue of\nbecoming overly dependent on benchmark datasets with a limited number of\ncharacteristics. Our experiments unambiguously demonstrate that Transformers\nachieve SOTA accuracy on short text classification tasks, raising the question\nof whether specialized short text techniques are necessary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karl_F/0/1/0/all/0/1\">Fabian Karl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RT-1: Robotics Transformer for Real-World Control at Scale. (arXiv:2212.06817v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2212.06817","description":"<p>By transferring knowledge from large, diverse, task-agnostic datasets, modern\nmachine learning models can solve specific downstream tasks either zero-shot or\nwith small task-specific datasets to a high level of performance. While this\ncapability has been demonstrated in other fields such as computer vision,\nnatural language processing or speech recognition, it remains to be shown in\nrobotics, where the generalization capabilities of the models are particularly\ncritical due to the difficulty of collecting real-world robotic data. We argue\nthat one of the keys to the success of such general robotic models lies with\nopen-ended task-agnostic training, combined with high-capacity architectures\nthat can absorb all of the diverse, robotic data. In this paper, we present a\nmodel class, dubbed Robotics Transformer, that exhibits promising scalable\nmodel properties. We verify our conclusions in a study of different model\nclasses and their ability to generalize as a function of the data size, model\nsize, and data diversity based on a large-scale data collection on real robots\nperforming real-world tasks. The project's website and videos can be found at\nrobotics-transformer1.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brohan_A/0/1/0/all/0/1\">Anthony Brohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Noah Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbajal_J/0/1/0/all/0/1\">Justice Carbajal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chebotar_Y/0/1/0/all/0/1\">Yevgen Chebotar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabis_J/0/1/0/all/0/1\">Joseph Dabis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Keerthana Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1\">Karol Hausman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzog_A/0/1/0/all/0/1\">Alex Herzog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_J/0/1/0/all/0/1\">Jasmine Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibarz_J/0/1/0/all/0/1\">Julian Ibarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irpan_A/0/1/0/all/0/1\">Alex Irpan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_T/0/1/0/all/0/1\">Tomas Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jesmonth_S/0/1/0/all/0/1\">Sally Jesmonth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nikhil J Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Julian_R/0/1/0/all/0/1\">Ryan Julian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalashnikov_D/0/1/0/all/0/1\">Dmitry Kalashnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Y/0/1/0/all/0/1\">Yuheng Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_I/0/1/0/all/0/1\">Isabel Leal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kuang-Huei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malla_U/0/1/0/all/0/1\">Utsav Malla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunath_D/0/1/0/all/0/1\">Deeksha Manjunath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1\">Ofir Nachum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parada_C/0/1/0/all/0/1\">Carolina Parada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peralta_J/0/1/0/all/0/1\">Jodilyn Peralta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Emily Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pertsch_K/0/1/0/all/0/1\">Karl Pertsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quiambao_J/0/1/0/all/0/1\">Jornell Quiambao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">Kanishka Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salazar_G/0/1/0/all/0/1\">Grecia Salazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanketi_P/0/1/0/all/0/1\">Pannag Sanketi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayed_K/0/1/0/all/0/1\">Kevin Sayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jaspiar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sontakke_S/0/1/0/all/0/1\">Sumedh Sontakke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_A/0/1/0/all/0/1\">Austin Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Clayton Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Huong Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanhoucke_V/0/1/0/all/0/1\">Vincent Vanhoucke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vega_S/0/1/0/all/0/1\">Steve Vega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_Q/0/1/0/all/0/1\">Quan Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Ted Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Sichun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitkovich_B/0/1/0/all/0/1\">Brianna Zitkovich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Which Features are Learned by CodeBert: An Empirical Study of the BERT-based Source Code Representation Learning. (arXiv:2301.08427v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.08427","description":"<p>The Bidirectional Encoder Representations from Transformers (BERT) were\nproposed in the natural language process (NLP) and shows promising results.\nRecently researchers applied the BERT to source-code representation learning\nand reported some good news on several downstream tasks. However, in this\npaper, we illustrated that current methods cannot effectively understand the\nlogic of source codes. The representation of source code heavily relies on the\nprogrammer-defined variable and function names. We design and implement a set\nof experiments to demonstrate our conjecture and provide some insights for\nfuture works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Chen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking. (arXiv:2302.07189v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07189","description":"<p>Discovering entity mentions that are out of a Knowledge Base (KB) from texts\nplays a critical role in KB maintenance, but has not yet been fully explored.\nThe current methods are mostly limited to the simple threshold-based approach\nand feature-based classification, and the datasets for evaluation are\nrelatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL)\nmethod which can identify mentions that do not have corresponding KB entities\nby matching them to a special NIL entity. To better utilize BERT, we propose\nnew techniques including NIL entity representation and classification, with\nsynonym enhancement. We also apply KB Pruning and Versioning strategies to\nautomatically construct out-of-KB datasets from common in-KB EL datasets.\nResults on five datasets of clinical notes, biomedical publications, and\nWikipedia articles in various domains show the advantages of BLINKout over\nexisting methods to identify out-of-KB mentions for the medical ontologies,\nUMLS, SNOMED CT, and the general KB, WikiData.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1\">Ian Horrocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Contrastive Learning for Multimodal Fake News Detection. (arXiv:2302.14057v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.14057","description":"<p>Automatic detection of multimodal fake news has gained a widespread attention\nrecently. Many existing approaches seek to fuse unimodal features to produce\nmultimodal news representations. However, the potential of powerful cross-modal\ncontrastive learning methods for fake news detection has not been well\nexploited. Besides, how to aggregate features from different modalities to\nboost the performance of the decision-making process is still an open question.\nTo address that, we propose COOLANT, a cross-modal contrastive learning\nframework for multimodal fake news detection, aiming to achieve more accurate\nimage-text alignment. To further improve the alignment precision, we leverage\nan auxiliary task to soften the loss term of negative samples during the\ncontrast process. A cross-modal fusion module is developed to learn the\ncross-modality correlations. An attention mechanism with an attention guidance\nmodule is implemented to help effectively and interpretably aggregate the\naligned unimodal representations and the cross-modality correlations. Finally,\nwe evaluate the COOLANT and conduct a comparative study on two widely used\ndatasets, Twitter and Weibo. The experimental results demonstrate that our\nCOOLANT outperforms previous approaches by a large margin and achieves new\nstate-of-the-art results on the two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longzheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongxiu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaohan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Verifying the Robustness of Automatic Credibility Assessment. (arXiv:2303.08032v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08032","description":"<p>Text classification methods have been widely investigated as a way to detect\ncontent of low credibility: fake news, social media bots, propaganda, etc.\nQuite accurate models (likely based on deep neural networks) help in moderating\npublic electronic platforms and often cause content creators to face rejection\nof their submissions or removal of already published texts. Having the\nincentive to evade further detection, content creators try to come up with a\nslightly modified version of the text (known as an attack with an adversarial\nexample) that exploit the weaknesses of classifiers and result in a different\noutput. Here we systematically test the robustness of popular text classifiers\nagainst available attacking techniques and discover that, indeed, in some cases\ninsignificant changes in input text can mislead the models. We also introduce\nBODEGA: a benchmark for testing both victim models and attack methods on four\nmisinformation detection tasks in an evaluation framework designed to simulate\nreal use-cases of content moderation. Finally, we manually analyse a subset\nadversarial examples and check what kinds of modifications are used in\nsuccessful attacks. The BODEGA code and data is openly shared in hope of\nenhancing the comparability and replicability of further research in this area\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Przybyla_P/0/1/0/all/0/1\">Piotr Przyby&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shvets_A/0/1/0/all/0/1\">Alexander Shvets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saggion_H/0/1/0/all/0/1\">Horacio Saggion</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.16618","description":"<p>Language models that are sensitive to external context can more effectively\ncapture the speaking patterns of individuals with specific characteristics or\nin particular environments. However, obtaining and leveraging such annotations\ncan be challenging. In this work, we show how to leverage rich character and\nfilm annotations to personalise language models in a scalable manner. Our best\nmodel can reduce perplexity by up to 6.5% compared to a parameter-matched\nlanguage model. Our approach performs on par with speaker-specific fine-tuning\nwhen the fine-tuning data (i.e. past dialogue) for individual speakers is\navailable. On top of that, it also generalises well to a scenario with no such\ndata, relying on combinations of demographic characteristics expressed via\nmetadata. Our findings are consistent across two corpora, one of which is also\na contribution of this paper: Cornell-rich contains rich manual annotations for\n863 speaking characters from the Cornell Movie Dialog Corpus, including\nfeatures such as characteristic quotes and character descriptions, along with\nsix automatically extracted metadata features for over 95% of the featured\nfilms. Finally, we also present a cost-benefit analysis highlighting which\nannotations are most cost-effective in reducing perplexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vincent_S/0/1/0/all/0/1\">Sebastian Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumner_R/0/1/0/all/0/1\">Rowanne Sumner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dowek_A/0/1/0/all/0/1\">Alice Dowek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1\">Charlotte Blundell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preston_E/0/1/0/all/0/1\">Emily Preston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayliss_C/0/1/0/all/0/1\">Chris Bayliss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oakley_C/0/1/0/all/0/1\">Chris Oakley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ML-SUPERB: Multilingual Speech Universal PERformance Benchmark. (arXiv:2305.10615v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2305.10615","description":"<p>Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard\nto benchmark the performance of Self-Supervised Learning (SSL) models on\nvarious speech processing tasks. However, SUPERB largely considers English\nspeech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB),\ncovering 143 languages (ranging from high-resource to endangered), and\nconsidering both automatic speech recognition and language identification.\nFollowing the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and\nemploys a simple framework for multilingual tasks by learning a shallow\ndownstream model. Similar to the SUPERB benchmark, we find speech SSL models\ncan significantly improve performance compared to FBANK features. Furthermore,\nwe find that multilingual models do not always perform better than their\nmonolingual counterparts. We will release ML-SUPERB as a challenge with\norganized datasets and reproducible training scripts for future multilingual\nrepresentation research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrebbi_D/0/1/0/all/0/1\">Dan Berrebbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">William Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Ho-Lam Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">En-Pei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Ping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2306.09927","description":"<p>Attention-based neural networks such as transformers have demonstrated a\nremarkable ability to exhibit in-context learning (ICL): Given a short prompt\nsequence of tokens from an unseen task, they can formulate relevant per-token\nand next-token predictions without any parameter updates. By embedding a\nsequence of labeled training data and unlabeled test data as a prompt, this\nallows for transformers to behave like supervised learning algorithms. Indeed,\nrecent work has shown that when training transformer architectures over random\ninstances of linear regression problems, these models' predictions mimic those\nof ordinary least squares.\n</p>\n<p>Towards understanding the mechanisms underlying this phenomenon, we\ninvestigate the dynamics of ICL in transformers with a single linear\nself-attention layer trained by gradient flow on linear regression tasks. We\nshow that despite non-convexity, gradient flow with a suitable random\ninitialization finds a global minimum of the objective function. At this global\nminimum, when given a test prompt of labeled examples from a new prediction\ntask, the transformer achieves prediction error competitive with the best\nlinear predictor over the test prompt distribution. We additionally\ncharacterize the robustness of the trained transformer to a variety of\ndistribution shifts and show that although a number of shifts are tolerated,\nshifts in the covariate distribution of the prompts are not. Motivated by this,\nwe consider a generalized ICL setting where the covariate distributions can\nvary across prompts. We show that although gradient flow succeeds at finding a\nglobal minimum in this setting, the trained transformer is still brittle under\nmild covariate shifts. We complement this finding with experiments on large,\nnonlinear transformer architectures which we show are more robust under\ncovariate shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiqi Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Frei_S/0/1/0/all/0/1\">Spencer Frei</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1\">Peter L. Bartlett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement. (arXiv:2306.14704v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.14704","description":"<p>Mentions of new concepts appear regularly in texts and require automated\napproaches to harvest and place them into Knowledge Bases (KB), e.g.,\nontologies and taxonomies. Existing datasets suffer from three issues, (i)\nmostly assuming that a new concept is pre-discovered and cannot support\nout-of-KB mention discovery; (ii) only using the concept label as the input\nalong with the KB and thus lacking the contexts of a concept label; and (iii)\nmostly focusing on concept placement w.r.t a taxonomy of atomic concepts,\ninstead of complex concepts, i.e., with logical operators. To address these\nissues, we propose a new benchmark, adapting MedMentions dataset (PubMed\nabstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases\nsub-category and the broader categories of Clinical finding, Procedure, and\nPharmaceutical / biologic product. We provide usage on the evaluation with the\ndataset for out-of-KB mention discovery and concept placement, adapting recent\nLarge Language Model based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1\">Ian Horrocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Domain Adaptation of Sentence Embeddings Using Adapters. (arXiv:2307.03104v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.03104","description":"<p>Sentence embeddings enable us to capture the semantic similarity of short\ntexts. Most sentence embedding models are trained for general semantic textual\nsimilarity (STS) tasks. Therefore, to use sentence embeddings in a particular\ndomain, the model must be adapted to it in order to achieve good results.\nUsually, this is done by fine-tuning the entire sentence embedding model for\nthe domain of interest. While this approach yields state-of-the-art results,\nall of the model's weights are updated during fine-tuning, making this method\nresource-intensive. Therefore, instead of fine-tuning entire sentence embedding\nmodels for each target domain individually, we propose to train lightweight\nadapters. These domain-specific adapters do not require fine-tuning all\nunderlying sentence embedding model parameters. Instead, we only train a small\nnumber of additional parameters while keeping the weights of the underlying\nsentence embedding model fixed. Training domain-specific adapters allows always\nusing the same base model and only exchanging the domain-specific adapters to\nadapt sentence embeddings to a specific domain. We show that using adapters for\nparameter-efficient domain adaptation of sentence embeddings yields competitive\nperformance within 1% of a domain-adapted, entirely fine-tuned sentence\nembedding model while only training approximately 3.6% of the parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1\">Tim Schopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_D/0/1/0/all/0/1\">Dennis N. Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.12267","description":"<p>The recent large language models (LLMs), e.g., ChatGPT, have been able to\ngenerate human-like and fluent responses when provided with specific\ninstructions. While admitting the convenience brought by technological\nadvancement, educators also have concerns that students might leverage LLMs to\ncomplete their writing assignments and pass them off as their original work.\nAlthough many AI content detection studies have been conducted as a result of\nsuch concerns, most of these prior studies modeled AI content detection as a\nclassification problem, assuming that a text is either entirely human-written\nor entirely AI-generated. In this study, we investigated AI content detection\nin a rarely explored yet realistic setting where the text to be detected is\ncollaboratively written by human and generative LLMs (i.e., hybrid text). We\nfirst formalized the detection task as identifying the transition points\nbetween human-written content and AI-generated content from a given hybrid text\n(boundary detection). Then we proposed a two-step approach where we (1)\nseparated AI-generated content from human-written content during the encoder\ntraining process; and (2) calculated the distances between every two adjacent\nprototypes and assumed that the boundaries exist between the two adjacent\nprototypes that have the furthest distance from each other. Through extensive\nexperiments, we observed the following main findings: (1) the proposed approach\nconsistently outperformed the baseline methods across different experiment\nsettings; (2) the encoder training process can significantly boost the\nperformance of the proposed approach; (3) when detecting boundaries for\nsingle-boundary hybrid essays, the proposed approach could be enhanced by\nadopting a relatively large prototype size, leading to a 22% improvement in the\nIn-Domain evaluation and an 18% improvement in the Out-of-Domain evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zijie Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1\">Lele Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaixun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasevic_D/0/1/0/all/0/1\">Dragan Ga&#x161;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanliang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.00158","description":"<p>Translation Quality Estimation (TQE) is an important step before deploying\nthe output translation into usage. TQE is also critical in assessing machine\ntranslation (MT) and human translation (HT) quality without seeing the\nreference translations. In this work, we examine if the state-of-the-art large\nlanguage models (LLMs) can be fine-tuned for the TQE task and their capability.\nWe take ChatGPT as one example and approach TQE as a binary classification\ntask. Using English to Italian, German, French, Japanese, Dutch, Portuguese,\nTurkish, and Chinese training corpora, our experimental results show that\nfine-tuned ChatGPT via its API can achieve a relatively high score on\npredicting translation quality, i.e. if the translation needs to be edited, but\nthere is definitely much space to improve the accuracy. English-Italiano\nbilingual Abstract is available in the paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1\">Serge Gladkoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1\">Gleb Erofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causality Guided Disentanglement for Cross-Platform Hate Speech Detection. (arXiv:2308.02080v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.02080","description":"<p>Social media platforms, despite their value in promoting open discourse, are\noften exploited to spread harmful content. Current deep learning and natural\nlanguage processing models used for detecting this harmful content overly rely\non domain-specific terms affecting their capabilities to adapt to generalizable\nhate speech detection. This is because they tend to focus too narrowly on\nparticular linguistic signals or the use of certain categories of words.\nAnother significant challenge arises when platforms lack high-quality annotated\ndata for training, leading to a need for cross-platform models that can adapt\nto different distribution shifts. Our research introduces a cross-platform hate\nspeech detection model capable of being trained on one platform's data and\ngeneralizing to multiple unseen platforms. To achieve good generalizability\nacross platforms, one way is to disentangle the input representations into\ninvariant and platform-dependent features. We also argue that learning causal\nrelationships, which remain constant across diverse environments, can\nsignificantly aid in understanding invariant representations in hate speech. By\ndisentangling input into platform-dependent features (useful for predicting\nhate targets) and platform-independent features (used to predict the presence\nof hate), we learn invariant representations resistant to distribution shifts.\nThese features are then used to predict hate speech across unseen platforms.\nOur extensive experiments across four platforms highlight our model's enhanced\nefficacy compared to existing state-of-the-art methods in detecting generalized\nhate speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheth_P/0/1/0/all/0/1\">Paras Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1\">Tharindu Kumarage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1\">Raha Moraffah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Generalist Foundation Model for Radiology. (arXiv:2308.02463v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2308.02463","description":"<p>In this study, we aim to initiate the development of Radiology Foundation\nModel, termed as RadFM.We consider the construction of foundational models from\nthe perspectives of data, model design, and evaluation thoroughly. Our\ncontribution can be concluded as follows: (i), we construct a large-scale\nMedical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans.\nTo the best of our knowledge, this is the first multi-modal dataset containing\n3D medical scans. (ii), We propose an architecture that enables visually\nconditioned generative pre-training, allowing for the integration of text input\ninterleaved with 2D or 3D medical scans to generate response for diverse\nradiologic tasks. The model was initially pre-trained on MedMD and subsequently\ndomain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD,\ncontaining 3M radiologic visual-language pairs. (iii), we propose a new\nevaluation benchmark that comprises five tasks, aiming to comprehensively\nassess the capability of foundation models in handling practical clinical\nproblems. Our experimental results confirm that RadFM significantly outperforms\nexisting multi-modal foundation models. The codes, data, and model checkpoint\nwill all be made publicly available to promote further research and development\nin the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chaoyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoman Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-EX : A Unified Dataset of Definitions and Dictionary Examples. (arXiv:2308.03043v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.03043","description":"<p>Definitions are a fundamental building block in lexicography, linguistics and\ncomputational semantics. In NLP, they have been used for retrofitting word\nembeddings or augmenting contextual representations in language models.\nHowever, lexical resources containing definitions exhibit a wide range of\nproperties, which has implications in the behaviour of models trained and\nevaluated on them. In this paper, we introduce 3D- EX , a dataset that aims to\nfill this gap by combining well-known English resources into one centralized\nknowledge repository in the form of &lt;term, definition, example&gt; triples. 3D- EX\nis a unified evaluation framework with carefully pre-computed\ntrain/validation/test splits to prevent memorization. We report experimental\nresults that suggest that this dataset could be effectively leveraged in\ndownstream NLP tasks. Code and data are available at\nhttps://github.com/F-Almeman/3D-EX .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almeman_F/0/1/0/all/0/1\">Fatemah Almeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikhi_H/0/1/0/all/0/1\">Hadi Sheikhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1\">Luis Espinosa-Anke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLASSLA-Stanza: The Next Step for Linguistic Processing of South Slavic Languages. (arXiv:2308.04255v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.04255","description":"<p>We present CLASSLA-Stanza, a pipeline for automatic linguistic annotation of\nthe South Slavic languages, which is based on the Stanza natural language\nprocessing pipeline. We describe the main improvements in CLASSLA-Stanza with\nrespect to Stanza, and give a detailed description of the model training\nprocess for the latest 2.1 release of the pipeline. We also report performance\nscores produced by the pipeline for different languages and varieties.\nCLASSLA-Stanza exhibits consistently high performance across all the supported\nlanguages and outperforms or expands its parent pipeline Stanza at all the\nsupported tasks. We also present the pipeline's new functionality enabling\nefficient processing of web data and the reasons that led to its\nimplementation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tercon_L/0/1/0/all/0/1\">Luka Ter&#x10d;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ljubesic_N/0/1/0/all/0/1\">Nikola Ljube&#x161;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Generation Capabilities of Large Chinese Language Models. (arXiv:2308.04823v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.04823","description":"<p>This paper presents CG-Eval, the first comprehensive evaluation of the\ngeneration capabilities of large Chinese language models across a wide range of\nacademic disciplines. The models' performance was assessed based on their\nability to generate accurate and relevant responses to different types of\nquestions in six disciplines, namely, Science and Engineering, Humanities and\nSocial Sciences, Mathematical Calculations, Medical Practitioner Qualification\nExamination, Judicial Examination, and Certified Public Accountant Examination.\nThis paper also presents Gscore, a composite index derived from the weighted\nsum of multiple metrics to measure the quality of model's generation against a\nreference. The test data and test results can be found at\n<a href=\"http://cgeval.besteasy.com/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hui Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jingyuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_M/0/1/0/all/0/1\">Meng Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_B/0/1/0/all/0/1\">Bin Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Na Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis. (arXiv:2308.05476v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.05476","description":"<p>Deceptive text classification is a critical task in natural language\nprocessing that aims to identify deceptive o fraudulent content. This study\npresents a comparative analysis of machine learning and transformer-based\napproaches for deceptive text classification. We investigate the effectiveness\nof traditional machine learning algorithms and state-of-the-art transformer\nmodels, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive\ntext. A labeled dataset consisting of deceptive and non-deceptive texts is used\nfor training and evaluation purposes. Through extensive experimentation, we\ncompare the performance metrics, including accuracy, precision, recall, and F1\nscore, of the different approaches. The results of this study shed light on the\nstrengths and limitations of machine learning and transformer-based methods for\ndeceptive text classification, enabling researchers and practitioners to make\ninformed decisions when dealing with deceptive content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1\">Anusuya Krishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM As DBA. (arXiv:2308.05481v2 [cs.DB] UPDATED)","link":"http://arxiv.org/abs/2308.05481","description":"<p>Database administrators (DBAs) play a crucial role in managing, maintaining\nand optimizing a database system to ensure data availability, performance, and\nreliability. However, it is hard and tedious for DBAs to manage a large number\nof database instances (e.g., millions of instances on the cloud databases).\nRecently large language models (LLMs) have shown great potential to understand\nvaluable documents and accordingly generate reasonable answers. Thus, we\npropose D-Bot, a LLM-based database administrator that can continuously acquire\ndatabase maintenance experience from textual sources, and provide reasonable,\nwell-founded, in-time diagnosis and optimization advice for target databases.\nThis paper presents a revolutionary LLM-centric framework for database\nmaintenance, including (i) database maintenance knowledge detection from\ndocuments and tools, (ii) tree of thought reasoning for root cause analysis,\nand (iii) collaborative diagnosis among multiple LLMs. Our preliminary\nexperimental results that D-Bot can efficiently and effectively diagnose the\nroot causes and our code is available at\ngithub.com/TsinghuaDatabaseGroup/DB-GPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuanhe Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-08-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}