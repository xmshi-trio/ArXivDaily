{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature. (arXiv:2304.05406v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05406","description":"<p>We demonstrate the potential of the state-of-the-art OpenAI GPT-4 large\nlanguage model to engage in meaningful interactions with Astronomy papers using\nin-context prompting. To optimize for efficiency, we employ a distillation\ntechnique that effectively reduces the size of the original input paper by\n50\\%, while maintaining the paragraph structure and overall semantic integrity.\nWe then explore the model's responses using a multi-document context (ten\ndistilled documents). Our findings indicate that GPT-4 excels in the\nmulti-document domain, providing detailed answers contextualized within the\nframework of related research findings. Our results showcase the potential of\nlarge language models for the astronomical community, offering a promising\navenue for further exploration, particularly the possibility of utilizing the\nmodels for hypothesis generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciuca_I/0/1/0/all/0/1\">Ioana Ciuc&#x103;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting_Y/0/1/0/all/0/1\">Yuan-Sen Ting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Temporal Relation Extraction with ChatGPT. (arXiv:2304.05454v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05454","description":"<p>The goal of temporal relation extraction is to infer the temporal relation\nbetween two events in the document. Supervised models are dominant in this\ntask. In this work, we investigate ChatGPT's ability on zero-shot temporal\nrelation extraction. We designed three different prompt techniques to break\ndown the task and evaluate ChatGPT. Our experiments show that ChatGPT's\nperformance has a large gap with that of supervised methods and can heavily\nrely on the design of prompts. We further demonstrate that ChatGPT can infer\nmore small relation classes correctly than supervised methods. The current\nshortcomings of ChatGPT on temporal relation extraction are also discussed in\nthis paper. We found that ChatGPT cannot keep consistency during temporal\ninference and it fails in actively long-dependency temporal inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chenhan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Resources and Methods for Natural Language Processing of Serbian Language. (arXiv:2304.05468v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05468","description":"<p>The Serbian language is a Slavic language spoken by over 12 million speakers\nand well understood by over 15 million people. In the area of natural language\nprocessing, it can be considered a low-resourced language. Also, Serbian is\nconsidered a high-inflectional language. The combination of many word\ninflections and low availability of language resources makes natural language\nprocessing of Serbian challenging. Nevertheless, over the past three decades,\nthere have been a number of initiatives to develop resources and methods for\nnatural language processing of Serbian, ranging from developing a corpus of\nfree text from books and the internet, annotated corpora for classification and\nnamed entity recognition tasks to various methods and models performing these\ntasks. In this paper, we review the initiatives, resources, methods, and their\navailability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marovac_U/0/1/0/all/0/1\">Ulfeta A. Marovac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avdic_A/0/1/0/all/0/1\">Aldina R. Avdi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milosevic_N/0/1/0/all/0/1\">Nikola Lj. Milo&#x161;evi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User Adaptive Language Learning Chatbots with a Curriculum. (arXiv:2304.05489v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05489","description":"<p>Along with the development of systems for natural language understanding and\ngeneration, dialog systems have been widely adopted for language learning and\npracticing. Many current educational dialog systems perform chitchat, where the\ngenerated content and vocabulary are not constrained. However, for learners in\na school setting, practice through dialog is more effective if it aligns with\nstudents' curriculum and focuses on textbook vocabulary. Therefore, we adapt\nlexically constrained decoding to a dialog system, which urges the dialog\nsystem to include curriculum-aligned words and phrases in its generated\nutterances. We adopt a generative dialog system, BlenderBot3, as our backbone\nmodel and evaluate our curriculum-based dialog system with middle school\nstudents learning English as their second language. The constrained words and\nphrases are derived from their textbooks, suggested by their English teachers.\nThe evaluation result demonstrates that the dialog system with curriculum\ninfusion improves students' understanding of target words and increases their\ninterest in practicing English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shea_R/0/1/0/all/0/1\">Ryan Shea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fryer_L/0/1/0/all/0/1\">Luke Kutszik Fryer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"chatIPCC: Grounding Conversational AI in Climate Science. (arXiv:2304.05510v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05510","description":"<p>Large Language Models (LLMs) have made significant progress in recent years,\nachieving remarkable results in question-answering tasks (QA). However, they\nstill face two major challenges: hallucination and outdated information after\nthe training phase. These challenges take center stage in critical domains like\nclimate change, where obtaining accurate and up-to-date information from\nreliable sources in a limited time is essential and difficult. To overcome\nthese barriers, one potential solution is to provide LLMs with access to\nexternal, scientifically accurate, and robust sources (long-term memory) to\ncontinuously update their knowledge and prevent the propagation of inaccurate,\nincorrect, or outdated information. In this study, we enhanced GPT-4 by\nintegrating the information from the Sixth Assessment Report of the\nIntergovernmental (IPCC AR6), the most comprehensive, up-to-date, and reliable\nsource in this domain. We present our conversational AI prototype, available at\nwww.chatclimate.ai/ipcc and demonstrate its ability to answer challenging\nquestions accurately in three different QA scenarios: asking from 1) GPT-4, 2)\nchatIPCC, and 3) hybrid chatIPCC. The answers and their sources were evaluated\nby our team of IPCC authors, who used their expert knowledge to score the\naccuracy of the answers from 1 (very-low) to 5 (very-high). The evaluation\nshowed that the hybrid chatIPCC provided more accurate answers, highlighting\nthe effectiveness of our solution. This approach can be easily scaled for\nchatbots in specific domains, enabling the delivery of reliable and accurate\ninformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaghefi_S/0/1/0/all/0/1\">Saeid Ashraf Vaghefi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muccione_V/0/1/0/all/0/1\">Veruska Muccione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingwei Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Mathias Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingler_J/0/1/0/all/0/1\">Julia Bingler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schimanski_T/0/1/0/all/0/1\">Tobias Schimanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colesanti_Senni_C/0/1/0/all/0/1\">Chiara Colesanti-Senni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webersinke_N/0/1/0/all/0/1\">Nicolas Webersinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huggel_C/0/1/0/all/0/1\">Christrian Huggel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leippold_M/0/1/0/all/0/1\">Markus Leippold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mathematical and Linguistic Characterization of Orhan Pamuk's Nobel Works. (arXiv:2304.05512v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05512","description":"<p>In this study, Nobel Laureate Orhan Pamuk's works are chosen as examples of\nTurkish literature. By counting the number of letters and words in his texts,\nwe find it possible to study his works statistically. It has been known that\nthere is a geometrical order in text structures. Here the method based on the\nbasic assumption of fractal geometry is introduced for calculating the fractal\ndimensions of Pamuk's texts. The results are compared with the applications of\nZipf's law, which is successfully applied for letters and words, where two\nconcepts, namely Zipf's dimension and Zipf's order, are introduced. The Zipf\ndimension of the novel My Name is Red is found to be much different than his\nother novels. However, it is linguistically observed that there is no\nfundamental difference between his corpora. The results are interpreted in\nterms of fractal dimensions and the Turkish language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arsan_T/0/1/0/all/0/1\">Taner Arsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simsek_S/0/1/0/all/0/1\">Sehnaz Sismanoglu Simsek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pekcan_O/0/1/0/all/0/1\">Onder Pekcan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoMo: A shared encoder Model for text, image and multi-Modal representations. (arXiv:2304.05523v1 [cs.CV])","link":"http://arxiv.org/abs/2304.05523","description":"<p>We propose a self-supervised shared encoder model that achieves strong\nresults on several visual, language and multimodal benchmarks while being data,\nmemory and run-time efficient. We make three key contributions. First, in\ncontrast to most existing works, we use a single transformer with all the\nencoder layers processing both the text and the image modalities. Second, we\npropose a stage-wise training strategy where the model is first trained on\nimages, then jointly with unimodal text and image datasets and finally jointly\nwith text and text-image datasets. Third, to preserve information across both\nthe modalities, we propose a training pipeline that learns simultaneously from\ngradient updates of different modalities at each training update step. The\nresults on downstream text-only, image-only and multimodal tasks show that our\nmodel is competitive with several strong models while using fewer parameters\nand lesser pre-training data. For example, MoMo performs competitively with\nFLAVA on multimodal (+3.1), image-only (+1.1) and text-only (-0.1) tasks\ndespite having 2/5th the number of parameters and using 1/3rd the image-text\ntraining pairs. Finally, we ablate various design choices and further show that\nincreasing model size produces significant performance gains indicating\npotential for substantial improvements with larger models using our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chada_R/0/1/0/all/0/1\">Rakesh Chada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhaoheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Pradeep Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Causality with Large Language Models: Feasibility and Opportunities. (arXiv:2304.05524v1 [cs.LG])","link":"http://arxiv.org/abs/2304.05524","description":"<p>We assess the ability of large language models (LLMs) to answer causal\nquestions by analyzing their strengths and weaknesses against three types of\ncausal question. We believe that current LLMs can answer causal questions with\nexisting causal knowledge as combined domain experts. However, they are not yet\nable to provide satisfactory answers for discovering new knowledge or for\nhigh-stakes decision-making tasks with high precision. We discuss possible\nfuture directions and opportunities, such as enabling explicit and implicit\ncausal modules as well as deep causal-aware LLMs. These will not only enable\nLLMs to answer many different types of causal questions for greater impact but\nalso enable LLMs to be more trustworthy and efficient in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiangfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_W/0/1/0/all/0/1\">Wenbo Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilmkil_A/0/1/0/all/0/1\">Agrin Hilmkil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jennings_J/0/1/0/all/0/1\">Joel Jennings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minka_T/0/1/0/all/0/1\">Tom Minka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawlowski_N/0/1/0/all/0/1\">Nick Pawlowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaughan_J/0/1/0/all/0/1\">James Vaughan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distinguishing ChatGPT(-3.5, -4)-generated and human-written papers through Japanese stylometric analysis. (arXiv:2304.05534v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05534","description":"<p>Text-generative artificial intelligence (AI), including ChatGPT, equipped\nwith GPT-3.5 and GPT-4, from OpenAI, has attracted considerable attention\nworldwide. In this study, first, we compared Japanese stylometric features\ngenerated by GPT (-3.5 and -4) and those written by humans. In this work, we\nperformed multi-dimensional scaling (MDS) to confirm the classification of 216\ntexts into three classes (72 academic papers written by 36 single authors, 72\ntexts generated by GPT-3.5, and 72 texts generated by GPT-4 on the basis of the\ntitles of the aforementioned papers) focusing on the following stylometric\nfeatures: (1) bigrams of parts-of-speech, (2) bigram of postpositional particle\nwords, (3) positioning of commas, and (4) rate of function words. MDS revealed\ndistinct distributions at each stylometric feature of GPT (-3.5 and -4) and\nhuman. Although GPT-4 is more powerful than GPT-3.5 because it has more\nparameters, both GPT (-3.5 and -4) distributions are likely to overlap. These\nresults indicate that although the number of parameters may increase in the\nfuture, AI-generated texts may not be close to that written by humans in terms\nof stylometric features. Second, we verified the classification performance of\nrandom forest (RF) for two classes (GPT and human) focusing on Japanese\nstylometric features. This study revealed the high performance of RF in each\nstylometric feature. Furthermore, the RF classifier focusing on the rate of\nfunction words achieved 98.1% accuracy. The RF classifier focusing on all\nstylometric features reached 100% in terms of all performance indexes\n(accuracy, recall, precision, and F1 score). This study concluded that at this\nstage we human discriminate ChatGPT from human limited to Japanese language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaitsu_W/0/1/0/all/0/1\">Wataru Zaitsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Mingzhe Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Informativeness Matter? Active Learning for Educational Dialogue Act Classification. (arXiv:2304.05578v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05578","description":"<p>Dialogue Acts (DAs) can be used to explain what expert tutors do and what\nstudents know during the tutoring process. Most empirical studies adopt the\nrandom sampling method to obtain sentence samples for manual annotation of DAs,\nwhich are then used to train DA classifiers. However, these studies have paid\nlittle attention to sample informativeness, which can reflect the information\nquantity of the selected samples and inform the extent to which a classifier\ncan learn patterns. Notably, the informativeness level may vary among the\nsamples and the classifier might only need a small amount of low informative\nsamples to learn the patterns. Random sampling may overlook sample\ninformativeness, which consumes human labelling costs and contributes less to\ntraining the classifiers. As an alternative, researchers suggest employing\nstatistical sampling methods of Active Learning (AL) to identify the\ninformative samples for training the classifiers. However, the use of AL\nmethods in educational DA classification tasks is under-explored. In this\npaper, we examine the informativeness of annotated sentence samples. Then, the\nstudy investigates how the AL methods can select informative samples to support\nDA classifiers in the AL sampling process. The results reveal that most\nannotated sentences present low informativeness in the training dataset and the\npatterns of these sentences can be easily captured by the DA classifier. We\nalso demonstrate how AL methods can reduce the cost of manual annotation in the\nAL sampling process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jionghao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_D/0/1/0/all/0/1\">David Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasevic_D/0/1/0/all/0/1\">Dragan Gasevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Feature Verification in FLAN-T5. (arXiv:2304.05591v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05591","description":"<p>This study evaluates the potential of a large language model for aiding in\ngeneration of semantic feature norms - a critical tool for evaluating\nconceptual structure in cognitive science. Building from an existing\nhuman-generated dataset, we show that machine-verified norms capture aspects of\nconceptual structure beyond what is expressed in human norms alone, and better\nexplain human judgments of semantic similarity amongst items that are distally\nrelated. The results suggest that LLMs can greatly enhance traditional methods\nof semantic feature norm verification, with implications for our understanding\nof conceptual representation in humans and machines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1\">Siddharth Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_K/0/1/0/all/0/1\">Kushin Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_T/0/1/0/all/0/1\">Timothy T. Rogers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning. (arXiv:2304.05613v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05613","description":"<p>Over the last few years, large language models (LLMs) have emerged as the\nmost important breakthroughs in natural language processing (NLP) that\nfundamentally transform research and developments in the field. ChatGPT\nrepresents one of the most exciting LLM systems developed recently to showcase\nimpressive skills for language generation and highly attract public attention.\nAmong various exciting applications discovered for ChatGPT in English, the\nmodel can process and generate texts for multiple languages due to its\nmultilingual training data. Given the broad adoption of ChatGPT for English in\ndifferent problems and areas, a natural question is whether ChatGPT can also be\napplied effectively for other languages or it is necessary to develop more\nlanguage-specific technologies. The answer to this question requires a thorough\nevaluation of ChatGPT over multiple tasks with diverse languages and large\ndatasets (i.e., beyond reported anecdotes), which is still missing or limited\nin current research. Our work aims to fill this gap for the evaluation of\nChatGPT and similar LLMs to provide more comprehensive information for\nmultilingual NLP applications. While this work will be an ongoing effort to\ninclude additional experiments in the future, our current paper evaluates\nChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,\nlow, and extremely low resources. We also focus on the zero-shot learning\nsetting for ChatGPT to improve reproducibility and better simulate the\ninteractions of general users. Compared to the performance of previous models,\nour extensive experimental results demonstrate a worse performance of ChatGPT\nfor different NLP tasks and languages, calling for further research to develop\nbetter models and understanding for multilingual learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1\">Viet Dac Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_N/0/1/0/all/0/1\">Nghia Trung Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veyseh_A/0/1/0/all/0/1\">Amir Pouran Ben Veyseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Man_H/0/1/0/all/0/1\">Hieu Man</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Prompt Cell: A Portable Control Module for Effective Prompt. (arXiv:2304.05642v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05642","description":"<p>As a novel approach to tuning pre-trained models, prompt tuning involves\nfreezing the parameters in downstream tasks while inserting trainable\nembeddings into inputs in the first layer.However,previous methods have mainly\nfocused on the initialization of prompt embeddings. The question of how to\ntrain and utilize prompt embeddings in a reasonable way has become aa limiting\nfactor in the effectiveness of prompt tuning. To address this issue, we\nintroduce the Global Prompt Cell (GPC), a portable control module for prompt\ntuning that selectively preserves prompt information across all encoder layers.\nOur experimental results demonstrate a 5.8% improvement on SuperGLUE datasets\ncompared to vanilla prompt tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haochun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_N/0/1/0/all/0/1\">Nuwa Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sendong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Normative and Descriptive Biases in Language Models Using Census Data. (arXiv:2304.05764v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05764","description":"<p>We investigate in this paper how distributions of occupations with respect to\ngender is reflected in pre-trained language models. Such distributions are not\nalways aligned to normative ideals, nor do they necessarily reflect a\ndescriptive assessment of reality. In this paper, we introduce an approach for\nmeasuring to what degree pre-trained language models are aligned to normative\nand descriptive occupational distributions. To this end, we use official\ndemographic information about gender--occupation distributions provided by the\nnational statistics agencies of France, Norway, United Kingdom, and the United\nStates. We manually generate template-based sentences combining gendered\npronouns and nouns with occupations, and subsequently probe a selection of ten\nlanguage models covering the English, French, and Norwegian languages. The\nscoring system we introduce in this work is language independent, and can be\nused on any combination of template-based sentences, occupations, and\nlanguages. The approach could also be extended to other dimensions of national\ncensus data and other demographic variables.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Touileb_S/0/1/0/all/0/1\">Samia Touileb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovrelid_L/0/1/0/all/0/1\">Lilja &#xd8;vrelid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velldal_E/0/1/0/all/0/1\">Erik Velldal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Gender Bias in West Slavic Language Models. (arXiv:2304.05783v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05783","description":"<p>Pre-trained language models have been known to perpetuate biases from the\nunderlying datasets to downstream tasks. However, these findings are\npredominantly based on monolingual language models for English, whereas there\nare few investigative studies of biases encoded in language models for\nlanguages beyond English. In this paper, we fill this gap by analysing gender\nbias in West Slavic language models. We introduce the first template-based\ndataset in Czech, Polish, and Slovak for measuring gender bias towards male,\nfemale and non-binary subjects. We complete the sentences using both mono- and\nmultilingual language models and assess their suitability for the masked\nlanguage modelling objective. Next, we measure gender bias encoded in West\nSlavic language models by quantifying the toxicity and genderness of the\ngenerated words. We find that these language models produce hurtful completions\nthat depend on the subject's gender. Perhaps surprisingly, Czech, Slovak, and\nPolish language models produce more hurtful completions with men as subjects,\nwhich, upon inspection, we find is due to completions being related to\nviolence, death, and sickness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinkova_S/0/1/0/all/0/1\">Sandra Martinkov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_K/0/1/0/all/0/1\">Karolina Sta&#x144;czak Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Dense Retrieval's Few-Shot Ability. (arXiv:2304.05845v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05845","description":"<p>Few-shot dense retrieval (DR) aims to effectively generalize to novel search\nscenarios by learning a few samples. Despite its importance, there is little\nstudy on specialized datasets and standardized evaluation protocols. As a\nresult, current methods often resort to random sampling from supervised\ndatasets to create \"few-data\" setups and employ inconsistent training\nstrategies during evaluations, which poses a challenge in accurately comparing\nrecent progress. In this paper, we propose a customized FewDR dataset and a\nunified evaluation benchmark. Specifically, FewDR employs class-wise sampling\nto establish a standardized \"few-shot\" setting with finely-defined classes,\nreducing variability in multiple sampling rounds. Moreover, the dataset is\ndisjointed into base and novel classes, allowing DR models to be continuously\ntrained on ample data from base classes and a few samples in novel classes.\nThis benchmark eliminates the risk of novel class leakage, providing a reliable\nestimation of the DR model's few-shot ability. Our extensive empirical results\nreveal that current state-of-the-art DR models still face challenges in the\nstandard few-shot scene. Our code and data will be open-sourced at\nhttps://github.com/OpenMatch/ANCE-Tele.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Si Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yida Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhonghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Deiming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jie Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Homographic Disambiguation Representation for Neural Machine Translation. (arXiv:2304.05860v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05860","description":"<p>Homographs, words with the same spelling but different meanings, remain\nchallenging in Neural Machine Translation (NMT). While recent works leverage\nvarious word embedding approaches to differentiate word sense in NMT, they do\nnot focus on the pivotal components in resolving ambiguities of homographs in\nNMT: the hidden states of an encoder. In this paper, we propose a novel\napproach to tackle homographic issues of NMT in the latent space. We first\ntrain an encoder (aka \"HDR-encoder\") to learn universal sentence\nrepresentations in a natural language inference (NLI) task. We further\nfine-tune the encoder using homograph-based synset sentences from WordNet,\nenabling it to learn word-level homographic disambiguation representations\n(HDR). The pre-trained HDR-encoder is subsequently integrated with a\ntransformer-based NMT in various schemes to improve translation accuracy.\nExperiments on four translation directions demonstrate the effectiveness of the\nproposed method in enhancing the performance of NMT systems in the BLEU scores\n(up to +2.3 compared to a solid baseline). The effects can be verified by other\nmetrics (F1, precision, and recall) of translation accuracy in an additional\ndisambiguation task. Visualization methods like heatmaps, T-SNE and translation\nexamples are also utilized to demonstrate the effects of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReDWINE: A Clinical Datamart with Text Analytical Capabilities to Facilitate Rehabilitation Research. (arXiv:2304.05929v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05929","description":"<p>Rehabilitation research focuses on determining the components of a treatment\nintervention, the mechanism of how these components lead to recovery and\nrehabilitation, and ultimately the optimal intervention strategies to maximize\npatients' physical, psychologic, and social functioning. Traditional randomized\nclinical trials that study and establish new interventions face several\nchallenges, such as high cost and time commitment. Observational studies that\nuse existing clinical data to observe the effect of an intervention have shown\nseveral advantages over RCTs. Electronic Health Records (EHRs) have become an\nincreasingly important resource for conducting observational studies. To\nsupport these studies, we developed a clinical research datamart, called\nReDWINE (Rehabilitation Datamart With Informatics iNfrastructure for rEsearch),\nthat transforms the rehabilitation-related EHR data collected from the UPMC\nhealth care system to the Observational Health Data Sciences and Informatics\n(OHDSI) Observational Medical Outcomes Partnership (OMOP) Common Data Model\n(CDM) to facilitate rehabilitation research. The standardized EHR data stored\nin ReDWINE will further reduce the time and effort required by investigators to\npool, harmonize, clean, and analyze data from multiple sources, leading to more\nrobust and comprehensive research findings. ReDWINE also includes deployment of\ndata visualization and data analytics tools to facilitate cohort definition and\nclinical data analysis. These include among others the Open Health Natural\nLanguage Processing (OHNLP) toolkit, a high-throughput NLP pipeline, to provide\ntext analytical capabilities at scale in ReDWINE. Using this comprehensive\nrepresentation of patient data in ReDWINE for rehabilitation research will\nfacilitate real-world evidence for health interventions and outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1\">David Oniani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmanto_B/0/1/0/all/0/1\">Bambang Parmanto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saptono_A/0/1/0/all/0/1\">Andi Saptono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bove_A/0/1/0/all/0/1\">Allyn Bove</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freburger_J/0/1/0/all/0/1\">Janet Freburger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cappella_S/0/1/0/all/0/1\">Shyam Visweswaran Nickie Cappella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLay_B/0/1/0/all/0/1\">Brian McLay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silverstein_J/0/1/0/all/0/1\">Jonathan C. Silverstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becich_M/0/1/0/all/0/1\">Michael J. Becich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delitto_A/0/1/0/all/0/1\">Anthony Delitto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skidmore_E/0/1/0/all/0/1\">Elizabeth Skidmore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanshan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign Language Recognition. (arXiv:2304.05934v1 [cs.CV])","link":"http://arxiv.org/abs/2304.05934","description":"<p>Sign languages are used as a primary language by approximately 70 million\nD/deaf people world-wide. However, most communication technologies operate in\nspoken and written languages, creating inequities in access. To help tackle\nthis problem, we release ASL Citizen, the largest Isolated Sign Language\nRecognition (ISLR) dataset to date, collected with consent and containing\n83,912 videos for 2,731 distinct signs filmed by 52 signers in a variety of\nenvironments. We propose that this dataset be used for sign language dictionary\nretrieval for American Sign Language (ASL), where a user demonstrates a sign to\ntheir own webcam with the aim of retrieving matching signs from a dictionary.\nWe show that training supervised machine learning classifiers with our dataset\ngreatly advances the state-of-the-art on metrics relevant for dictionary\nretrieval, achieving, for instance, 62% accuracy and a recall-at-10 of 90%,\nevaluated entirely on videos of users who are not present in the training or\nvalidation sets. An accessible PDF of this article is available at\nhttps://aashakadesai.github.io/research/ASL_Dataset__arxiv_.pdf\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Desai_A/0/1/0/all/0/1\">Aashaka Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_L/0/1/0/all/0/1\">Lauren Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minakov_F/0/1/0/all/0/1\">Fyodor O. Minakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milan_V/0/1/0/all/0/1\">Vanessa Milan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1\">Chinmay Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pumphrey_K/0/1/0/all/0/1\">Kriston Pumphrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladner_R/0/1/0/all/0/1\">Richard E. Ladner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1\">Hal Daum&#xe9; III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1\">Alex X. Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caselli_N/0/1/0/all/0/1\">Naomi Caselli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bragg_D/0/1/0/all/0/1\">Danielle Bragg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Angler: Helping Machine Translation Practitioners Prioritize Model Improvements. (arXiv:2304.05967v1 [cs.HC])","link":"http://arxiv.org/abs/2304.05967","description":"<p>Machine learning (ML) models can fail in unexpected ways in the real world,\nbut not all model failures are equal. With finite time and resources, ML\npractitioners are forced to prioritize their model debugging and improvement\nefforts. Through interviews with 13 ML practitioners at Apple, we found that\npractitioners construct small targeted test sets to estimate an error's nature,\nscope, and impact on users. We built on this insight in a case study with\nmachine translation models, and developed Angler, an interactive visual\nanalytics tool to help practitioners prioritize model improvements. In a user\nstudy with 7 machine translation experts, we used Angler to understand\nprioritization practices when the input space is infinite, and obtaining\nreliable signals of model quality is expensive. Our study revealed that\nparticipants could form more interesting and user-focused hypotheses for\nprioritization by analyzing quantitative summary statistics and qualitatively\nassessing data by reading sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robertson_S/0/1/0/all/0/1\">Samantha Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijie J. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moritz_D/0/1/0/all/0/1\">Dominik Moritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kery_M/0/1/0/all/0/1\">Mary Beth Kery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hohman_F/0/1/0/all/0/1\">Fred Hohman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosted Prompt Ensembles for Large Language Models. (arXiv:2304.05970v1 [cs.CL])","link":"http://arxiv.org/abs/2304.05970","description":"<p>Methods such as chain-of-thought prompting and self-consistency have pushed\nthe frontier of language model reasoning performance with no additional\ntraining. To further improve performance, we propose a prompt ensembling method\nfor large language models, which uses a small dataset to construct a set of few\nshot prompts that together comprise a ``boosted prompt ensemble''. The few shot\nexamples for each prompt are chosen in a stepwise fashion to be ``hard''\nexamples on which the previous step's ensemble is uncertain. We show that this\noutperforms single-prompt output-space ensembles and bagged prompt-space\nensembles on the GSM8k and AQuA datasets, among others. We propose both\ntrain-time and test-time versions of boosted prompting that use different\nlevels of available annotation and conduct a detailed empirical study of our\nalgorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pitis_S/0/1/0/all/0/1\">Silviu Pitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Michael R. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Andrew Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ba_J/0/1/0/all/0/1\">Jimmy Ba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiPrompt: Few-Shot Biomedical Knowledge Fusion via Hierarchy-Oriented Prompting. (arXiv:2304.05973v1 [cs.IR])","link":"http://arxiv.org/abs/2304.05973","description":"<p>Medical decision-making processes can be enhanced by comprehensive biomedical\nknowledge bases, which require fusing knowledge graphs constructed from\ndifferent sources via a uniform index system. The index system often organizes\nbiomedical terms in a hierarchy to provide the aligned entities with\nfine-grained granularity. To address the challenge of scarce supervision in the\nbiomedical knowledge fusion (BKF) task, researchers have proposed various\nunsupervised methods. However, these methods heavily rely on ad-hoc lexical and\nstructural matching algorithms, which fail to capture the rich semantics\nconveyed by biomedical entities and terms. Recently, neural embedding models\nhave proved effective in semantic-rich tasks, but they rely on sufficient\nlabeled data to be adequately trained. To bridge the gap between the\nscarce-labeled BKF and neural embedding models, we propose HiPrompt, a\nsupervision-efficient knowledge fusion framework that elicits the few-shot\nreasoning ability of large language models through hierarchy-oriented prompts.\nEmpirical results on the collected KG-Hi-BKF benchmark datasets demonstrate the\neffectiveness of HiPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiaying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1\">Bo Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wenjing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1\">Steffen Staab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combined Scaling for Zero-shot Transfer Learning. (arXiv:2111.10050v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.10050","description":"<p>We present a combined scaling method - named BASIC - that achieves 85.7%\ntop-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from\nany labeled ImageNet example. This accuracy surpasses best published similar\nmodels - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant\nimprovements in robustness benchmarks. For instance, on 5 test sets with\nnatural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our\nmodel achieves 84.3% top-1 average accuracy, only a small drop from its\noriginal ImageNet accuracy. To achieve these results, we scale up the\ncontrastive learning framework of CLIP and ALIGN in three dimensions: data\nsize, model size, and batch size. Our dataset has 6.6B noisy image-text pairs,\nwhich is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has\n3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than\nALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and\n4x more than ALIGN. We encountered two main challenges with the scaling rules\nof BASIC. First, the main challenge with implementing the combined scaling\nrules of BASIC is the limited memory of accelerators, such as GPUs and TPUs. To\novercome the memory limit, we propose two simple methods which make use of\ngradient checkpointing and model parallelism. Second, while increasing the\ndataset size and the model size has been the defacto method to improve the\nperformance of deep learning models like BASIC, the effect of a large\ncontrastive batch size on such contrastive-trained image-text models is not\nwell-understood. To shed light on the benefits of large contrastive batch\nsizes, we develop a theoretical framework which shows that larger contrastive\nbatch sizes lead to smaller generalization gaps for image-text models such as\nBASIC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1\">Golnaz Ghiasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_M/0/1/0/all/0/1\">Minh-Thang Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingxing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models. (arXiv:2204.14211v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.14211","description":"<p>Language Models (LMs) become outdated as the world changes; they often fail\nto perform tasks requiring recent factual information which was absent or\ndifferent during training, a phenomenon called temporal misalignment. This is\nespecially a challenging problem because the research community still lacks a\ncoherent dataset for assessing the adaptability of LMs to frequently-updated\nknowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a\nlifelong benchmark for ever-evolving LMs that utilizes the difference between\nconsecutive snapshots of English Wikipedia and English Wikidata for training\nand evaluation, respectively. The benchmark hence allows researchers to\nperiodically track an LM's ability to retain previous knowledge and acquire\nupdated/new knowledge at each point in time. We also find that training an LM\non the diff data through continual learning methods achieves similar or better\nperplexity than on the entire snapshot in our benchmark with 12 times less\ncomputational cost, which verifies that factual knowledge in LMs can be safely\nupdated with minimal training data via continual learning. The dataset and the\ncode are available at https://github.com/joeljang/temporalwiki.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Changho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Joongbo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Janghoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeonghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do we need Label Regularization to Fine-tune Pre-trained Language Models?. (arXiv:2205.12428v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.12428","description":"<p>Knowledge Distillation (KD) is a prominent neural model compression technique\nthat heavily relies on teacher network predictions to guide the training of a\nstudent model. Considering the ever-growing size of pre-trained language models\n(PLMs), KD is often adopted in many NLP tasks involving PLMs. However, it is\nevident that in KD, deploying the teacher network during training adds to the\nmemory and computational requirements of training. In the computer vision\nliterature, the necessity of the teacher network is put under scrutiny by\nshowing that KD is a label regularization technique that can be replaced with\nlighter teacher-free variants such as the label-smoothing technique. However,\nto the best of our knowledge, this issue is not investigated in NLP. Therefore,\nthis work concerns studying different label regularization techniques and\nwhether we actually need them to improve the fine-tuning of smaller PLM\nnetworks on downstream tasks. In this regard, we did a comprehensive set of\nexperiments on different PLMs such as BERT, RoBERTa, and GPT with more than 600\ndistinct trials and ran each configuration five times. This investigation led\nto a surprising observation that KD and other label regularization techniques\ndo not play any meaningful role over regular fine-tuning when the student model\nis pre-trained. We further explore this phenomenon in different settings of NLP\nand computer vision tasks and demonstrate that pre-training itself acts as a\nkind of regularization, and additional label regularization is unnecessary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kobyzev_I/0/1/0/all/0/1\">Ivan Kobyzev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_A/0/1/0/all/0/1\">Aref Jafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_Omri_A/0/1/0/all/0/1\">Alan Do-Omri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Peng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1\">Pascal Poupart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Diversity, Equity and Inclusion of NLP Technology: A Case Study for Indian Languages. (arXiv:2205.12676v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12676","description":"<p>In order for NLP technology to be widely applicable, fair, and useful, it\nneeds to serve a diverse set of speakers across the world's languages, be\nequitable, i.e., not unduly biased towards any particular language, and be\ninclusive of all users, particularly in low-resource settings where compute\nconstraints are common. In this paper, we propose an evaluation paradigm that\nassesses NLP technologies across all three dimensions. While diversity and\ninclusion have received attention in recent literature, equity is currently\nunexplored. We propose to address this gap using the Gini coefficient, a\nwell-established metric used for estimating societal wealth inequality. Using\nour paradigm, we highlight the distressed state of current technologies for\nIndian (IN) languages (a linguistically large and diverse set, with a varied\nspeaker population), across all three dimensions. To improve upon these\nmetrics, we demonstrate the importance of region-specific choices in model\nbuilding and dataset creation, and more importantly, propose a novel,\ngeneralisable approach to optimal resource allocation during fine-tuning.\nFinally, we discuss steps to mitigate these biases and encourage the community\nto employ multi-faceted evaluation when building linguistically diverse and\nequitable technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanuja_S/0/1/0/all/0/1\">Simran Khanuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages. (arXiv:2205.15960v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.15960","description":"<p>Natural language processing (NLP) has a significant impact on society via\ntechnologies such as machine translation and search engines. Despite its\nsuccess, NLP technology is only widely available for high-resource languages\nsuch as English and Chinese, while it remains inaccessible to many languages\ndue to the unavailability of data resources and benchmarks. In this work, we\nfocus on developing resources for languages in Indonesia. Despite being the\nsecond most linguistically diverse country, most languages in Indonesia are\ncategorized as endangered and some are even extinct. We develop the first-ever\nparallel resource for 10 low-resource languages in Indonesia. Our resource\nincludes datasets, a multi-task benchmark, and lexicons, as well as a parallel\nIndonesian-English dataset. We provide extensive analyses and describe the\nchallenges when creating such resources. We hope that our work can spark NLP\nresearch on Indonesian and other underrepresented languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendra_R/0/1/0/all/0/1\">Rahmad Mahendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1\">Fajri Koto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romadhony_A/0/1/0/all/0/1\">Ade Romadhony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurniawan_K/0/1/0/all/0/1\">Kemal Kurniawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeljadi_D/0/1/0/all/0/1\">David Moeljadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasojo_R/0/1/0/all/0/1\">Radityo Eko Prasojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Innovations in Neural Data-to-text Generation: A Survey. (arXiv:2207.12571v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.12571","description":"<p>The neural boom that has sparked natural language processing (NLP) research\nthrough the last decade has similarly led to significant innovations in\ndata-to-text generation (DTG). This survey offers a consolidated view into the\nneural DTG paradigm with a structured examination of the approaches, benchmark\ndatasets, and evaluation protocols. This survey draws boundaries separating DTG\nfrom the rest of the natural language generation (NLG) landscape, encompassing\nan up-to-date synthesis of the literature, and highlighting the stages of\ntechnological adoption from within and outside the greater NLG umbrella. With\nthis holistic view, we highlight promising avenues for DTG research that not\nonly focus on the design of linguistically capable systems but also systems\nthat exhibit fairness and accountability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Mandar Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gogineni_A/0/1/0/all/0/1\">Ajay Gogineni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_N/0/1/0/all/0/1\">Naren Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Multi-modal Training from Uncurated Image and Reports Enables Zero-shot Oversight Artificial Intelligence in Radiology. (arXiv:2208.05140v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2208.05140","description":"<p>Oversight AI is an emerging concept in radiology where the AI forms a\nsymbiosis with radiologists by continuously supporting radiologists in their\ndecision-making. Recent advances in vision-language models sheds a light on the\nlong-standing problems of the oversight AI by the understanding both visual and\ntextual concepts and their semantic correspondences. However, there have been\nlimited successes in the application of vision-language models in the medical\ndomain, as the current vision-language models and learning strategies for\nphotographic images and captions call for the web-scale data corpus of image\nand text pairs which was not often feasible in the medical domain. To address\nthis, here we present a model dubbed Medical Cross-attention Vision-Language\nmodel (Medical X-VL), leveraging the key components to be tailored for the\nmedical domain. Our medical X-VL model is based on the following components:\nself-supervised uni-modal models in medical domain and fusion encoder to bridge\nthem, momentum distillation, sentence-wise contrastive learning for medical\nreports, and the sentence similarity-adjusted hard negative mining. We\nexperimentally demonstrated that our model enables various zero-shot tasks for\noversight AI, ranging from the zero-shot classification to zero-shot error\ncorrection. Our model outperformed the current state-of-the-art models in two\ndifferent medical image database, suggesting the novel clinical usage of our\noversight AI model for monitoring human errors. Our method was especially\nsuccessful in the data-limited setting, which is frequently encountered in the\nclinics, suggesting the potential widespread applicability in medical domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Sangjoon Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1\">Eun Sun Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shin_K/0/1/0/all/0/1\">Kyung Sook Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Jeong Eun Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spillover of Antisocial Behavior from Fringe Platforms: The Unintended Consequences of Community Banning. (arXiv:2209.09803v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2209.09803","description":"<p>Online platforms face pressure to keep their communities civil and\nrespectful. Thus, the bannings of problematic online communities from\nmainstream platforms like Reddit and Facebook are often met with enthusiastic\npublic reactions. However, this policy can lead users to migrate to alternative\nfringe platforms with lower moderation standards and where antisocial behaviors\nlike trolling and harassment are widely accepted. As users of these communities\noften remain co-active across mainstream and fringe platforms, antisocial\nbehaviors may spill over onto the mainstream platform. We study this possible\nspillover by analyzing around 70,000 users from three banned communities that\nmigrated to fringe platforms: r/The_Donald, r/GenderCritical, and r/Incels.\nUsing a difference-in-differences design, we contrast co-active users with\nmatched counterparts to estimate the causal effect of fringe platform\nparticipation on users' antisocial behavior on Reddit. Our results show that\nparticipating in the fringe communities increases users' toxicity on Reddit (as\nmeasured by Perspective API) and involvement with subreddits similar to the\nbanned community -- which often also breach platform norms. The effect\nintensifies with time and exposure to the fringe platform. In short, we find\nevidence for a spillover of antisocial behavior from fringe platforms onto\nReddit via co-participation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Russo_G/0/1/0/all/0/1\">Giuseppe Russo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verginer_L/0/1/0/all/0/1\">Luca Verginer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Manoel Horta Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casiraghi_G/0/1/0/all/0/1\">Giona Casiraghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposed Prompting: A Modular Approach for Solving Complex Tasks. (arXiv:2210.02406v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02406","description":"<p>Few-shot prompting is a surprisingly powerful way to use Large Language\nModels (LLMs) to solve various tasks. However, this approach struggles as the\ntask complexity increases or when the individual reasoning steps of the task\nthemselves are hard to learn, especially when embedded in more complex tasks.\nTo address this, we propose Decomposed Prompting, a new approach to solve\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\ncan be delegated to a library of prompting-based LLMs dedicated to these\nsub-tasks. This modular structure allows each prompt to be optimized for its\nspecific sub-task, further decomposed if necessary, and even easily replaced\nwith more effective prompts, trained models, or symbolic functions if desired.\nWe show that the flexibility and modularity of Decomposed Prompting allows it\nto outperform prior work on few-shot prompting using GPT3. On symbolic\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\neven simpler solvable sub-tasks. When the complexity comes from the input\nlength, we can recursively decompose the task into the same task but with\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\ntasks: on long-context multi-hop QA task, we can more effectively teach the\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\nwe can incorporate a symbolic information retrieval within our decomposition\nframework, leading to improved performance on both tasks. Datasets, Code and\nPrompts available at https://github.com/allenai/DecomP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1\">Harsh Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finlayson_M/0/1/0/all/0/1\">Matthew Finlayson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_K/0/1/0/all/0/1\">Kyle Richardson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LittleBird: Efficient Faster & Longer Transformer for Question Answering. (arXiv:2210.11870v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11870","description":"<p>BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a\nlimitation dealing with long inputs due to its attention mechanism. Longformer,\nETC and BigBird addressed this issue and effectively solved the quadratic\ndependency problem. However we find that these models are not sufficient, and\npropose LittleBird, a novel model based on BigBird with improved speed and\nmemory footprint while maintaining accuracy. In particular, we devise a more\nflexible and efficient position representation method based on Attention with\nLinear Biases (ALiBi). We also show that replacing the method of global\ninformation represented in the BigBird with pack and unpack attention is more\neffective. The proposed model can work on long inputs even after being\npre-trained on short inputs, and can be trained efficiently reusing existing\npre-trained language model for short inputs. This is a significant benefit for\nlow-resource languages where large amounts of long text data are difficult to\nobtain. As a result, our experiments show that LittleBird works very well in a\nvariety of languages, achieving high performance in question answering tasks,\nparticularly in KorQuAD2.0, Korean Question Answering Dataset for long\nparagraphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minchul Lee</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kijong Han</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Myeong Cheol Shin</a> (1) ((1) Kakao Enterprise Corp.)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gendered Mental Health Stigma in Masked Language Models. (arXiv:2210.15144v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15144","description":"<p>Mental health stigma prevents many individuals from receiving the appropriate\ncare, and social psychology studies have shown that mental health tends to be\noverlooked in men. In this work, we investigate gendered mental health stigma\nin masked language models. In doing so, we operationalize mental health stigma\nby developing a framework grounded in psychology research: we use clinical\npsychology literature to curate prompts, then evaluate the models' propensity\nto generate gendered words. We find that masked language models capture\nsocietal stigma about gender in mental health: models are consistently more\nlikely to predict female subjects than male in sentences about having a mental\nhealth condition (32% vs. 19%), and this disparity is exacerbated for sentences\nthat indicate treatment-seeking behavior. Furthermore, we find that different\nmodels capture dimensions of stigma differently for men and women, associating\nstereotypes like anger, blame, and pity more with women with mental health\nconditions than with men. In showing the complex nuances of models' gendered\nmental health stigma, we demonstrate that context and overlapping dimensions of\nidentity are important considerations when assessing computational models'\nsocial biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_I/0/1/0/all/0/1\">Inna Wanyin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Njoo_L/0/1/0/all/0/1\">Lucille Njoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ashish Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reinecke_K/0/1/0/all/0/1\">Katharina Reinecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althoff_T/0/1/0/all/0/1\">Tim Althoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Reliability of Large Language Models through Semantic Consistency. (arXiv:2211.05853v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05853","description":"<p>While large pretrained language models (PLMs) demonstrate incredible fluency\nand performance on many natural language tasks, recent work has shown that\nwell-performing PLMs are very sensitive to what prompts are feed into them.\nEven when prompts are semantically identical, language models may give very\ndifferent answers. When considering safe and trustworthy deployments of PLMs we\nwould like their outputs to be consistent under prompts that mean the same\nthing or convey the same intent. While some work has looked into how\nstate-of-the-art PLMs address this need, they have been limited to only\nevaluating lexical equality of single- or multi-word answers and do not address\nconsistency of generative text sequences. In order to understand consistency of\nPLMs under text generation settings, we develop a measure of semantic\nconsistency that allows the comparison of open-ended text outputs. We implement\nseveral versions of this consistency metric to evaluate the performance of a\nnumber of PLMs on paraphrased versions of questions in the TruthfulQA dataset,\nwe find that our proposed metrics are considerably more consistent than\ntraditional metrics embodying lexical consistency, and also correlate with\nhuman evaluation of output consistency to a higher degree.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raj_H/0/1/0/all/0/1\">Harsh Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosati_D/0/1/0/all/0/1\">Domenic Rosati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Subhabrata Majumdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling. (arXiv:2212.02908v5 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2212.02908","description":"<p>Autonomous cars are indispensable when humans go further down the hands-free\nroute. Although existing literature highlights that the acceptance of the\nautonomous car will increase if it drives in a human-like manner, sparse\nresearch offers the naturalistic experience from a passenger's seat perspective\nto examine the human likeness of current autonomous cars. The present study\ntested whether the AI driver could create a human-like ride experience for\npassengers based on 69 participants' feedback in a real-road scenario. We\ndesigned a ride experience-based version of the non-verbal Turing test for\nautomated driving. Participants rode in autonomous cars (driven by either human\nor AI drivers) as a passenger and judged whether the driver was human or AI.\nThe AI driver failed to pass our test because passengers detected the AI driver\nabove chance. In contrast, when the human driver drove the car, the passengers'\njudgement was around chance. We further investigated how human passengers\nascribe humanness in our test. Based on Lewin's field theory, we advanced a\ncomputational model combining signal detection theory with pre-trained language\nmodels to predict passengers' humanness rating behaviour. We employed affective\ntransition between pre-study baseline emotions and corresponding post-stage\nemotions as the signal strength of our model. Results showed that the\npassengers' ascription of humanness would increase with the greater affective\ntransition. Our study suggested an important role of affective transition in\npassengers' ascription of humanness, which might become a future direction for\nautonomous driving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qiaoli Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haiyan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Miner Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_Y/0/1/0/all/0/1\">Yixuan Ku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Momentum Contrastive Pre-training for Question Answering. (arXiv:2212.05762v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05762","description":"<p>Existing pre-training methods for extractive Question Answering (QA) generate\ncloze-like queries different from natural questions in syntax structure, which\ncould overfit pre-trained models to simple keyword matching. In order to\naddress this problem, we propose a novel Momentum Contrastive pRe-training fOr\nqueStion anSwering (MCROSS) method for extractive QA. Specifically, MCROSS\nintroduces a momentum contrastive learning framework to align the answer\nprobability between cloze-like and natural query-passage sample pairs. Hence,\nthe pre-trained models can better transfer the knowledge learned in cloze-like\nsamples to answering natural questions. Experimental results on three\nbenchmarking QA datasets show that our method achieves noticeable improvement\ncompared with all baselines in both supervised and zero-shot scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Minda Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Muzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Pre-training of Language Models. (arXiv:2302.03241v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03241","description":"<p>Language models (LMs) have been instrumental for the rapid advance of natural\nlanguage processing. This paper studies continual pre-training of LMs, in\nparticular, continual domain-adaptive pre-training (or continual DAP-training).\nExisting research has shown that further pre-training an LM using a domain\ncorpus to adapt the LM to the domain can improve the end-task performance in\nthe domain. This paper proposes a novel method to continually DAP-train an LM\nwith a sequence of unlabeled domain corpora to adapt the LM to these domains to\nimprove their end-task performances. The key novelty of our method is a\nsoft-masking mechanism that directly controls the update to the LM. A novel\nproxy is also proposed to preserve the general knowledge in the original LM.\nAdditionally, it contrasts the representations of the previously learned domain\nknowledge (including the general knowledge in the pre-trained LM) and the\nknowledge from the current full network to achieve knowledge integration. The\nmethod not only overcomes catastrophic forgetting, but also achieves knowledge\ntransfer to improve end-task performances. Empirical evaluation demonstrates\nthe effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zixuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yijia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haowei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konishi_T/0/1/0/all/0/1\">Tatsuya Konishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyuhak Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews on Social Media. (arXiv:2302.07731v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07731","description":"<p>Recent advances in generative models such as GPT may be used to fabricate\nindistinguishable fake customer reviews at a much lower cost, thus posing\nchallenges for social media platforms to detect these machine-generated fake\nreviews. We propose to leverage the high-quality elite restaurant reviews\nverified by Yelp to generate fake reviews from the OpenAI GPT review creator\nand ultimately fine-tune a GPT output detector to predict fake reviews that\nsignificantly outperform existing solutions. We further apply the model to\npredict non-elite reviews and identify the patterns across several dimensions,\nsuch as review, user and restaurant characteristics, and writing style. We show\nthat social media platforms are continuously challenged by machine-generated\nfake reviews, although they may implement detection systems to filter out\nsuspicious reviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gambetti_A/0/1/0/all/0/1\">Alessandro Gambetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qiwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLSP2022-EVJVQA Challenge: Multilingual Visual Question Answering. (arXiv:2302.11752v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.11752","description":"<p>Visual Question Answering (VQA) is a challenging task of natural language\nprocessing (NLP) and computer vision (CV), attracting significant attention\nfrom researchers. English is a resource-rich language that has witnessed\nvarious developments in datasets and models for visual question answering.\nVisual question answering in other languages also would be developed for\nresources and models. In addition, there is no multilingual dataset targeting\nthe visual content of a particular country with its own objects and cultural\ncharacteristics. To address the weakness, we provide the research community\nwith a benchmark dataset named EVJVQA, including 33,000+ pairs of\nquestion-answer over three languages: Vietnamese, English, and Japanese, on\napproximately 5,000 images taken from Vietnam for evaluating multilingual VQA\nsystems or models. EVJVQA is used as a benchmark dataset for the challenge of\nmultilingual visual question answering at the 9th Workshop on Vietnamese\nLanguage and Speech Processing (VLSP 2022). This task attracted 62 participant\nteams from various universities and organizations. In this article, we present\ndetails of the organization of the challenge, an overview of the methods\nemployed by shared-task participants, and the results. The highest performances\nare 0.4392 in F1-score and 0.4009 in BLUE on the private test set. The\nmultilingual QA systems proposed by the top 2 teams use ViT for the pre-trained\nvision model and mT5 for the pre-trained language model, a powerful pre-trained\nlanguage model based on the transformer architecture. EVJVQA is a challenging\ndataset that motivates NLP and CV researchers to further explore the\nmultilingual models or systems for visual question answering systems. We\nreleased the challenge on the Codalab evaluation system for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nghia Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Duong T.D Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khanh Quoc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Prototypical Semantic Decoupling Method via Joint Contrastive Learning for Few-Shot Name Entity Recognition. (arXiv:2302.13610v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13610","description":"<p>Few-shot named entity recognition (NER) aims at identifying named entities\nbased on only few labeled instances. Most existing prototype-based sequence\nlabeling models tend to memorize entity mentions which would be easily confused\nby close prototypes. In this paper, we proposed a Prototypical Semantic\nDecoupling method via joint Contrastive learning (PSDC) for few-shot NER.\nSpecifically, we decouple class-specific prototypes and contextual semantic\nprototypes by two masking strategies to lead the model to focus on two\ndifferent semantic information for inference. Besides, we further introduce\njoint contrastive learning objectives to better integrate two kinds of\ndecoupling information and prevent semantic collapse. Experimental results on\ntwo few-shot NER benchmarks demonstrate that PSDC consistently outperforms the\nprevious SOTA methods in terms of overall performance. Extensive analysis\nfurther validates the effectiveness and generalization of PSDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guanting Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zechen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daichi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Dayuan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_T/0/1/0/all/0/1\">Tingfeng Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Keqing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xinyue Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qixiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiran Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind meets machine: Unravelling GPT-4's cognitive psychology. (arXiv:2303.11436v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.11436","description":"<p>Cognitive psychology delves on understanding perception, attention, memory,\nlanguage, problem-solving, decision-making, and reasoning. Large language\nmodels (LLMs) are emerging as potent tools increasingly capable of performing\nhuman-level tasks. The recent development in the form of GPT-4 and its\ndemonstrated success in tasks complex to humans exam and complex problems has\nled to an increased confidence in the LLMs to become perfect instruments of\nintelligence. Although GPT-4 report has shown performance on some cognitive\npsychology tasks, a comprehensive assessment of GPT-4, via the existing\nwell-established datasets is required. In this study, we focus on the\nevaluation of GPT-4's performance on a set of cognitive psychology datasets\nsuch as CommonsenseQA, SuperGLUE, MATH and HANS. In doing so, we understand how\nGPT-4 processes and integrates cognitive psychology with contextual\ninformation, providing insight into the underlying cognitive processes that\nenable its ability to generate the responses. We show that GPT-4 exhibits a\nhigh level of accuracy in cognitive psychology tasks relative to the prior\nstate-of-the-art models. Our results strengthen the already available\nassessments and confidence on GPT-4's cognitive psychology abilities. It has\nsignificant potential to revolutionize the field of AI, by enabling machines to\nbridge the gap between human and machine reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_S/0/1/0/all/0/1\">Sifatkaur Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Manmeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+SB_V/0/1/0/all/0/1\">Vaisakh SB</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malviya_N/0/1/0/all/0/1\">Neetiraj Malviya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gill_S/0/1/0/all/0/1\">Sukhpal Singh Gill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4. (arXiv:2303.12712v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.12712","description":"<p>Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1\">S&#xe9;bastien Bubeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1\">Varun Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldan_R/0/1/0/all/0/1\">Ronen Eldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrke_J/0/1/0/all/0/1\">Johannes Gehrke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Peter Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yin Tat Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundberg_S/0/1/0/all/0/1\">Scott Lundberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_H/0/1/0/all/0/1\">Harsha Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marco Tulio Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capabilities of GPT-4 on Medical Challenge Problems. (arXiv:2303.13375v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13375","description":"<p>Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation across various domains, including\nmedicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art\nLLM, on medical competency examinations and benchmark datasets. GPT-4 is a\ngeneral-purpose model that is not specialized for medical problems through\ntraining or engineered to solve clinical tasks. Our analysis covers two sets of\nofficial practice materials for the USMLE, a three-step examination program\nused to assess clinical competency and grant licensure in the United States. We\nalso evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond\nmeasuring model performance, experiments were conducted to investigate the\ninfluence of test questions containing both text and images on model\nperformance, probe for memorization of content during training, and study\nprobability calibration, which is of critical importance in high-stakes\napplications like medicine. Our results show that GPT-4, without any\nspecialized prompt crafting, exceeds the passing score on USMLE by over 20\npoints and outperforms earlier general-purpose models (GPT-3.5) as well as\nmodels specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned\nversion of Flan-PaLM 540B). In addition, GPT-4 is significantly better\ncalibrated than GPT-3.5, demonstrating a much-improved ability to predict the\nlikelihood that its answers are correct. We also explore the behavior of the\nmodel qualitatively through a case study that shows the ability of GPT-4 to\nexplain medical reasoning, personalize explanations to students, and\ninteractively craft new counterfactual scenarios around a medical case.\nImplications of the findings are discussed for potential uses of GPT-4 in\nmedical education, assessment, and clinical practice, with appropriate\nattention to challenges of accuracy and safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nori_H/0/1/0/all/0/1\">Harsha Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_N/0/1/0/all/0/1\">Nicholas King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKinney_S/0/1/0/all/0/1\">Scott Mayer McKinney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carignan_D/0/1/0/all/0/1\">Dean Carignan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Large Language Models. (arXiv:2303.18223v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.18223","description":"<p>Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yupeng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Yingqian Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zican Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yushuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xinyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01890","description":"<p>We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for\nthe countries of Brazil, Germany, India and Kenya, to aid training and\ninterpretability of models. We demonstrate how our lexicon can be used to\ninterpret model predictions, showing that models developed to classify extreme\nspeech rely heavily on target words when making predictions. Further, we\npropose a method to aid shot selection for training in low-resource settings\nvia HATELEXICON. In few-shot learning, the selection of shots is of paramount\nimportance to model performance. In our work, we simulate a few-shot setting\nfor German and Hindi, using HASOC data for training and the Multilingual\nHateCheck (MHC) as a benchmark. We show that selecting shots based on our\nlexicon leads to models performing better on MHC than models trained on shots\nsampled randomly. Thus, when given only a few training examples, using our\nlexicon to select shots containing more sociocultural information leads to\nbetter few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maronikolakis_A/0/1/0/all/0/1\">Antonis Maronikolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Abdullatif K&#xf6;ksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02213","description":"<p>The amount of data has growing significance in exploring cutting-edge\nmaterials and a number of datasets have been generated either by hand or\nautomated approaches. However, the materials science field struggles to\neffectively utilize the abundance of data, especially in applied disciplines\nwhere materials are evaluated based on device performance rather than their\nproperties. This article presents a new natural language processing (NLP) task\ncalled structured information inference (SII) to address the complexities of\ninformation extraction at the device level in materials science. We\naccomplished this task by tuning GPT-3 on an existing perovskite solar cell\nFAIR (Findable, Accessible, Interoperable, Reusable) dataset with 91.8%\nF1-score and extended the dataset with data published since its release. The\nproduced data is formatted and normalized, enabling its direct utilization as\ninput in subsequent data analysis. This feature empowers materials scientists\nto develop models by selecting high-quality review articles within their\ndomain. Additionally, we designed experiments to predict the electrical\nperformance of solar cells and design materials or devices with targeted\nparameters using large language models (LLMs). Our results demonstrate\ncomparable performance to traditional machine learning methods without feature\nselection, highlighting the potential of LLMs to acquire scientific knowledge\nand design new materials akin to materials scientists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yuwei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yufei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linghu_Q/0/1/0/all/0/1\">Qingyuan Linghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaozhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kit_C/0/1/0/all/0/1\">Chunyu Kit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grazian_C/0/1/0/all/0/1\">Clara Grazian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoex_B/0/1/0/all/0/1\">Bram Hoex</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.05265","description":"<p>The recent large-scale generative modeling has attained unprecedented\nperformance especially in producing high-fidelity images driven by text\nprompts. Text inversion (TI), alongside the text-to-image model backbones, is\nproposed as an effective technique in personalizing the generation when the\nprompts contain user-defined, unseen or long-tail concept tokens. Despite that,\nwe find and show that the deployment of TI remains full of \"dark-magics\" -- to\nname a few, the harsh requirement of additional datasets, arduous human efforts\nin the loop and lack of robustness. In this work, we propose a much-enhanced\nversion of TI, dubbed Controllable Textual Inversion (COTI), in resolving all\nthe aforementioned problems and in turn delivering a robust, data-efficient and\neasy-to-use framework. The core to COTI is a theoretically-guided loss\nobjective instantiated with a comprehensive and novel weighted scoring\nmechanism, encapsulated by an active-learning paradigm. The extensive results\nshow that COTI significantly outperforms the prior TI-related approaches with a\n26.05 decrease in the FID score and a 23.00% boost in the R-precision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haobo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1\">Ruixuan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Gang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junbo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}