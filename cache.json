{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Comparative study on Judgment Text Classification for Transformer Based Models. (arXiv:2306.01739v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01739","description":"<p>This work involves the usage of various NLP models to predict the winner of a\nparticular judgment by the means of text extraction and summarization from a\njudgment document. These documents are useful when it comes to legal\nproceedings. One such advantage is that these can be used for citations and\nprecedence reference in Lawsuits and cases which makes a strong argument for\ntheir case by the ones using it. When it comes to precedence, it is necessary\nto refer to an ample number of documents in order to collect legal points with\nrespect to the case. However, reviewing these documents takes a long time to\nanalyze due to the complex word structure and the size of the document. This\nwork involves the comparative study of 6 different self-attention-based\ntransformer models and how they perform when they are being tweaked in 4\ndifferent activation functions. These models which are trained with 200\njudgement contexts and their results are being judged based on different\nbenchmark parameters. These models finally have a confidence level up to 99%\nwhile predicting the judgment. This can be used to get a particular judgment\ndocument without spending too much time searching relevant cases and reading\nthem completely.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kingston_S/0/1/0/all/0/1\">Stanley Kingston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prassanth/0/1/0/all/0/1\">Prassanth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_S/0/1/0/all/0/1\">Shrinivas A V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MS_B/0/1/0/all/0/1\">Balamurugan MS</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_M/0/1/0/all/0/1\">Manoj Kumar Rajagopal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Negativity: Re-Analysis and Follow-Up Experiments on Hope Speech Detection. (arXiv:2306.01742v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01742","description":"<p>Health experts assert that hope plays a crucial role in enhancing\nindividuals' physical and mental well-being, facilitating their recovery, and\npromoting restoration. Hope speech refers to comments, posts and other social\nmedia messages that offer support, reassurance, suggestions, inspiration, and\ninsight. The detection of hope speech involves the analysis of such textual\ncontent, with the aim of identifying messages that invoke positive emotions in\npeople. Our study aims to find computationally efficient yet\ncomparable/superior methods for hope speech detection. We also make our\ncodebase public at https://github.com/aflah02/Hope_Speech_Detection\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1\">Neemesh Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Aflah Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_D/0/1/0/all/0/1\">Diksha Sethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahni_R/0/1/0/all/0/1\">Raghav Sahni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abugida Normalizer and Parser for Unicode texts. (arXiv:2306.01743v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01743","description":"<p>This paper proposes two libraries to address common and uncommon issues with\nUnicode-based writing schemes for Indic languages. The first is a normalizer\nthat corrects inconsistencies caused by the encoding scheme\nhttps://pypi.org/project/bnunicodenormalizer/ . The second is a grapheme parser\nfor Abugida text https://pypi.org/project/indicparser/ . Both tools are more\nefficient and effective than previously used tools. We report 400% increase in\nspeed and ensure significantly better performance for different language model\nbased downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ansary_N/0/1/0/all/0/1\">Nazmuddoha Ansary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adib_Q/0/1/0/all/0/1\">Quazi Adibur Rahman Adib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reasat_T/0/1/0/all/0/1\">Tahsin Reasat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehnaz_S/0/1/0/all/0/1\">Sazia Mehnaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sushmit_A/0/1/0/all/0/1\">Asif Shahriyar Sushmit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humayun_A/0/1/0/all/0/1\">Ahmed Imtiaz Humayun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1\">Mohammad Mamun Or Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeque_F/0/1/0/all/0/1\">Farig Sadeque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preconditioned Visual Language Inference with Weak Supervision. (arXiv:2306.01753v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01753","description":"<p>Humans can infer the affordance of objects by extracting related contextual\npreconditions for each scenario. For example, upon seeing an image of a broken\ncup, we can infer that this precondition prevents the cup from being used for\ndrinking. Reasoning with preconditions of commonsense is studied in NLP where\nthe model explicitly gets the contextual precondition. However, it is unclear\nif SOTA visual language models (VLMs) can extract such preconditions and infer\nthe affordance of objects with them. In this work, we introduce the task of\npreconditioned visual language inference and rationalization (PVLIR). We\npropose a learning resource based on three strategies to retrieve weak\nsupervision signals for the task and develop a human-verified test set for\nevaluation. Our results reveal the shortcomings of SOTA VLM models in the task\nand draw a road map to address the challenges ahead in improving them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qasemi_E/0/1/0/all/0/1\">Ehsan Qasemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maina_Kilaas_A/0/1/0/all/0/1\">Amani R. Maina-Kilaas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_D/0/1/0/all/0/1\">Devadutta Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsaggaf_K/0/1/0/all/0/1\">Khalid Alsaggaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Priors Predict Text-To-Image Model Performance. (arXiv:2306.01755v1 [cs.CV])","link":"http://arxiv.org/abs/2306.01755","description":"<p>Text-to-image models can often generate some relations, i.e., \"astronaut\nriding horse\", but fail to generate other relations composed of the same basic\nparts, i.e., \"horse riding astronaut\". These failures are often taken as\nevidence that the models rely on training priors rather than constructing novel\nimages compositionally. This paper tests this intuition directly on the\nstablediffusion 2.1 text-to-image model. By looking at the subject-verb-object\n(SVO) triads that form the backbone of these prompts (e.g., \"astronaut\",\n\"ride\", \"horse\"), we find that the more often an SVO triad appears in the\ntraining data, the better the model can generate an image aligned with that\ntriad. Here, by aligned we mean that each of the terms appears in the generated\nimage in the proper relation to each other. However, this increased frequency\nalso diminishes how well the model can generate an image aligned with the\nflipped triad. For example, if \"astronaut riding horse\" appears frequently in\nthe training data, the image for \"horse riding astronaut\" will tend to be\npoorly aligned. We also find that models often struggle to generate terms in\natypical roles, e.g., if \"horse\" is more often the semantic patient (object),\nthe model might struggle to visualize it as a semantic agent (subject). Our\nresults thus show that current models are biased to generate images aligned\nwith relations seen in training and provide important new data in the ongoing\ndebate on whether these text-to-image models employ abstract compositional\nstructure in a traditional sense, or rather, interpolate between relations\nexplicitly seen in the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovering_C/0/1/0/all/0/1\">Charles Lovering</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distinguishing Human Generated Text From ChatGPT Generated Text Using Machine Learning. (arXiv:2306.01761v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01761","description":"<p>ChatGPT is a conversational artificial intelligence that is a member of the\ngenerative pre-trained transformer of the large language model family. This\ntext generative model was fine-tuned by both supervised learning and\nreinforcement learning so that it can produce text documents that seem to be\nwritten by natural intelligence. Although there are numerous advantages of this\ngenerative model, it comes with some reasonable concerns as well. This paper\npresents a machine learning-based solution that can identify the ChatGPT\ndelivered text from the human written text along with the comparative analysis\nof a total of 11 machine learning and deep learning algorithms in the\nclassification process. We have tested the proposed model on a Kaggle dataset\nconsisting of 10,000 texts out of which 5,204 texts were written by humans and\ncollected from news and social media. On the corpus generated by GPT-3.5, the\nproposed algorithm presents an accuracy of 77%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_N/0/1/0/all/0/1\">Niful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutradhar_D/0/1/0/all/0/1\">Debopom Sutradhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noor_H/0/1/0/all/0/1\">Humaira Noor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raya_J/0/1/0/all/0/1\">Jarin Tasnim Raya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maisha_M/0/1/0/all/0/1\">Monowara Tabassum Maisha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farid_D/0/1/0/all/0/1\">Dewan Md Farid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Quantitative Review on Language Model Efficiency Research. (arXiv:2306.01768v1 [cs.LG])","link":"http://arxiv.org/abs/2306.01768","description":"<p>Language models (LMs) are being scaled and becoming powerful. Improving their\nefficiency is one of the core research topics in neural information processing\nsystems. Tay et al. (2022) provided a comprehensive overview of efficient\nTransformers that have become an indispensable staple in the field of NLP.\nHowever, in the section of \"On Evaluation\", they left an open question \"which\nfundamental efficient Transformer one should consider,\" answered by \"still a\nmystery\" because \"many research papers select their own benchmarks.\"\nUnfortunately, there was not quantitative analysis about the performances of\nTransformers on any benchmarks. Moreover, state space models (SSMs) have\ndemonstrated their abilities of modeling long-range sequences with\nnon-attention mechanisms, which were not discussed in the prior review. This\narticle makes a meta analysis on the results from a set of papers on efficient\nTransformers as well as those on SSMs. It provides a quantitative review on LM\nefficiency research and gives suggestions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_H/0/1/0/all/0/1\">Hy Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_L/0/1/0/all/0/1\">Lingbo Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conceptual Design Generation Using Large Language Models. (arXiv:2306.01779v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01779","description":"<p>Concept generation is a creative step in the conceptual design phase, where\ndesigners often turn to brainstorming, mindmapping, or crowdsourcing design\nideas to complement their own knowledge of the domain. Recent advances in\nnatural language processing (NLP) and machine learning (ML) have led to the\nrise of Large Language Models (LLMs) capable of generating seemingly creative\noutputs from textual prompts. The success of these models has led to their\nintegration and application across a variety of domains, including art,\nentertainment, and other creative work. In this paper, we leverage LLMs to\ngenerate solutions for a set of 12 design problems and compare them to a\nbaseline of crowdsourced solutions. We evaluate the differences between\ngenerated and crowdsourced design solutions through multiple perspectives,\nincluding human expert evaluations and computational metrics. Expert\nevaluations indicate that the LLM-generated solutions have higher average\nfeasibility and usefulness while the crowdsourced solutions have more novelty.\nWe experiment with prompt engineering and find that leveraging few-shot\nlearning can lead to the generation of solutions that are more similar to the\ncrowdsourced solutions. These findings provide insight into the quality of\ndesign solutions generated with LLMs and begins to evaluate prompt engineering\ntechniques that could be leveraged by practitioners to generate higher-quality\ndesign solutions synergistically with LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kevin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandi_D/0/1/0/all/0/1\">Daniele Grandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McComb_C/0/1/0/all/0/1\">Christopher McComb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goucher_Lambert_K/0/1/0/all/0/1\">Kosa Goucher-Lambert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edit Distance based RL for RNNT decoding. (arXiv:2306.01789v1 [cs.SD])","link":"http://arxiv.org/abs/2306.01789","description":"<p>RNN-T is currently considered the industry standard in ASR due to its\nexceptional WERs in various benchmark tests and its ability to support seamless\nstreaming and longform transcription. However, its biggest drawback lies in the\nsignificant discrepancy between its training and inference objectives. During\ntraining, RNN-T maximizes all alignment probabilities by teacher forcing, while\nduring inference, it uses beam search which may not necessarily find the\nmaximum probable alignment. Additionally, RNN-T's inability to experience\nmistakes during teacher forcing training makes it more problematic when a\nmistake occurs in inference. To address this issue, this paper proposes a\nReinforcement Learning method that minimizes the gap between training and\ninference time. Our Edit Distance based RL (EDRL) approach computes rewards\nbased on the edit distance, and trains the network at every action level. The\nproposed approach yielded SoTA WERs on LibriSpeech for the 600M Conformer RNN-T\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_C/0/1/0/all/0/1\">Changwan Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes. (arXiv:2306.01805v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01805","description":"<p>As people become more aware of their food choices, food computation models\nhave become increasingly popular in assisting people in maintaining healthy\neating habits. For example, food recommendation systems analyze recipe\ninstructions to assess nutritional contents and provide recipe recommendations.\nThe recent and remarkable successes of generative AI methods, such as\nauto-regressive large language models, can lead to robust methods for a more\ncomprehensive understanding of recipes for healthy food recommendations beyond\nsurface-level nutrition content assessments. In this study, we explore the use\nof generative AI methods to extend current food computation models, primarily\ninvolving the analysis of nutrition and ingredients, to also incorporate\ncooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.).\nCooking actions are notoriously hard to model using statistical learning\nmethods due to irregular data patterns - significantly varying natural language\ndescriptions for the same action (e.g., marinate the meat vs. marinate the meat\nand leave overnight) and infrequently occurring patterns (e.g., add salt occurs\nfar more frequently than marinating the meat). The prototypical approach to\nhandling irregular data patterns is to increase the volume of data that the\nmodel ingests by orders of magnitude. Unfortunately, in the cooking domain,\nthese problems are further compounded with larger data volumes presenting a\nunique challenge that is not easily handled by simply scaling up. In this work,\nwe propose novel aggregation-based generative AI methods, Cook-Gen, that\nreliably generate cooking actions from recipes, despite difficulties with\nirregular data patterns, while also outperforming Large Language Models and\nother strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkataramanan_R/0/1/0/all/0/1\">Revathy Venkataramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_K/0/1/0/all/0/1\">Kanak Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_R/0/1/0/all/0/1\">Renjith Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_Y/0/1/0/all/0/1\">Yuxin Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vignesh Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Embeddings for Banking Industry. (arXiv:2306.01807v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01807","description":"<p>Applications of Natural Language Processing (NLP) are plentiful, from\nsentiment analysis to text classification. Practitioners rely on static word\nembeddings (e.g. Word2Vec or GloVe) or static word representation from\ncontextual models (e.g. BERT or ELMo) to perform many of these NLP tasks. These\nwidely available word embeddings are built from large amount of text, so they\nare likely to have captured most of the vocabulary in different context.\nHowever, how well would they capture domain-specific semantics and word\nrelatedness? This paper explores this idea by creating a bank-specific word\nembeddings and evaluates them against other sources of word embeddings such as\nGloVe and BERT. Not surprising that embeddings built from bank-specific corpora\ndoes a better job of capturing the bank-specific semantics and word\nrelatedness. This finding suggests that bank-specific word embeddings could be\na good stand-alone source or a complement to other widely available embeddings\nwhen performing NLP tasks specific to the banking industry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Avnish Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beta Thalassemia Carriers detection empowered federated Learning. (arXiv:2306.01818v1 [cs.LG])","link":"http://arxiv.org/abs/2306.01818","description":"<p>Thalassemia is a group of inherited blood disorders that happen when\nhemoglobin, the protein in red blood cells that carries oxygen, is not made\nenough. It is found all over the body and is needed for survival. If both\nparents have thalassemia, a child's chance of getting it increases. Genetic\ncounselling and early diagnosis are essential for treating thalassemia and\nstopping it from being passed on to future generations. It may be hard for\nhealthcare professionals to differentiate between people with thalassemia\ncarriers and those without. The current blood tests for beta thalassemia\ncarriers are too expensive, take too long, and require too much screening\nequipment. The World Health Organization says there is a high death rate for\npeople with thalassemia. Therefore, it is essential to find thalassemia\ncarriers to act quickly. High-performance liquid chromatography (HPLC), the\nstandard test method, has problems such as cost, time, and equipment needs. So,\nthere must be a quick and cheap way to find people carrying the thalassemia\ngene. Using federated learning (FL) techniques, this study shows a new way to\nfind people with the beta-thalassemia gene. FL allows data to be collected and\nprocessed on-site while following privacy rules, making it an excellent choice\nfor sensitive health data. Researchers used FL to train a model for\nbeta-thalassemia carriers by looking at the complete blood count results and\nred blood cell indices. The model was 92.38 % accurate at telling the\ndifference between beta-thalassemia carriers and people who did not have the\ndisease. The proposed FL model is better than other published methods in terms\nof how well it works, how reliable it is, and how private it is. This research\nshows a promising, quick, accurate, and low-cost way to find thalassemia\ncarriers and opens the door for screening them on a large scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1\">Muhammad Shoaib Farooq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Younas_H/0/1/0/all/0/1\">Hafiz Ali Younas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Binary and Ternary Natural Language Generation. (arXiv:2306.01841v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01841","description":"<p>Ternary and binary neural networks enable multiplication-free computation and\npromise multiple orders of magnitude efficiency gains over full-precision\nnetworks if implemented on specialized hardware. However, since both the\nparameter and the output space are highly discretized, such networks have\nproven very difficult to optimize. The difficulties are compounded for the\nclass of transformer text generation models due to the sensitivity of the\nattention operation to quantization and the noise-compounding effects of\nautoregressive decoding in the high-cardinality output space. We approach the\nproblem with a mix of statistics-based quantization for the weights and elastic\nquantization of the activations and demonstrate the first ternary and binary\ntransformer models on the downstream tasks of summarization and machine\ntranslation. Our ternary BART base achieves an R1 score of 41 on the\nCNN/DailyMail benchmark, which is merely 3.9 points behind the full model while\nbeing 16x more efficient. Our binary model, while less accurate, achieves a\nhighly non-trivial score of 35.6. For machine translation, we achieved BLEU\nscores of 21.7 and 17.6 on the WMT16 En-Ro benchmark, compared with a full\nprecision mBART model score of 26.8. We also compare our approach in the 8-bit\nactivation setting, where our ternary and even binary weight models can match\nor outperform the best existing 8-bit weight models in the literature. Our code\nand models are available at:\nhttps://github.com/facebookresearch/Ternary_Binary_Transformer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappu_A/0/1/0/all/0/1\">Aasish Pappu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yangyang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1\">Raghuraman Krishnamoorthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"5IDER: Unified Query Rewriting for Steering, Intent Carryover, Disfluencies, Entity Carryover and Repair. (arXiv:2306.01855v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01855","description":"<p>Providing voice assistants the ability to navigate multi-turn conversations\nis a challenging problem. Handling multi-turn interactions requires the system\nto understand various conversational use-cases, such as steering, intent\ncarryover, disfluencies, entity carryover, and repair. The complexity of this\nproblem is compounded by the fact that these use-cases mix with each other,\noften appearing simultaneously in natural language. This work proposes a\nnon-autoregressive query rewriting architecture that can handle not only the\nfive aforementioned tasks, but also complex compositions of these use-cases. We\nshow that our proposed model has competitive single task performance compared\nto the baseline approach, and even outperforms a fine-tuned T5 model in\nuse-case compositions, despite being 15 times smaller in parameters and 25\ntimes faster in latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiarui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_B/0/1/0/all/0/1\">Bo-Hsiang Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1\">Joel Ruben Antony Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xueyun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbacak_M/0/1/0/all/0/1\">Murat Akbacak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge of cultural moral norms in large language models. (arXiv:2306.01857v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01857","description":"<p>Moral norms vary across cultures. A recent line of work suggests that English\nlarge language models contain human-like moral biases, but these studies\ntypically do not examine moral variation in a diverse cultural setting. We\ninvestigate the extent to which monolingual English language models contain\nknowledge about moral norms in different countries. We consider two levels of\nanalysis: 1) whether language models capture fine-grained moral variation\nacross countries over a variety of topics such as ``homosexuality'' and\n``divorce''; 2) whether language models capture cultural diversity and shared\ntendencies in which topics people around the globe tend to diverge or agree on\nin their moral judgment. We perform our analyses with two public datasets from\nthe World Values Survey (across 55 countries) and PEW global surveys (across 40\ncountries) on morality. We find that pre-trained English language models\npredict empirical moral norms across countries worse than the English moral\nnorms reported previously. However, fine-tuning language models on the survey\ndata improves inference across countries at the expense of a less accurate\nestimate of the English moral norms. We discuss the relevance and challenges of\nincorporating cultural knowledge into the automated inference of moral norms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_A/0/1/0/all/0/1\">Aida Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores. (arXiv:2306.01879v1 [cs.CV])","link":"http://arxiv.org/abs/2306.01879","description":"<p>Vision-language models (VLMs) discriminatively pre-trained with contrastive\nimage-text matching losses such as $P(\\text{match}|\\text{text}, \\text{image})$\nhave been criticized for lacking compositional understanding. This means they\nmight output similar scores even if the original caption is rearranged into a\ndifferent semantic statement. To address this, we propose to use the ${\\bf\nV}$isual ${\\bf G}$enerative ${\\bf P}$re-${\\bf T}$raining Score (${\\bf\nVisualGPTScore}$) of $P(\\text{text}|\\text{image})$, a $\\textit{multimodal\ngenerative}$ score that captures the likelihood of a text caption conditioned\non an image using an image-conditioned language model. Contrary to the belief\nthat VLMs are mere bag-of-words models, our off-the-shelf VisualGPTScore\ndemonstrates top-tier performance on recently proposed image-text retrieval\nbenchmarks like ARO and Crepe that assess compositional reasoning. Furthermore,\nwe factorize VisualGPTScore into a product of the $\\textit{marginal}$ P(text)\nand the $\\textit{Pointwise Mutual Information}$ (PMI). This helps to (a)\ndiagnose datasets with strong language bias, and (b) debias results on other\nbenchmarks like Winoground using an information-theoretic framework.\nVisualGPTScore provides valuable insights and serves as a strong baseline for\nfuture evaluation of visio-linguistic compositionality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiqiu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v19 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1906.02358","description":"<p>Sinhala is the native language of the Sinhalese people who make up the\nlargest ethnic group of Sri Lanka. The language belongs to the globe-spanning\nlanguage tree, Indo-European. However, due to poverty in both linguistic and\neconomic capital, Sinhala, in the perspective of Natural Language Processing\ntools and research, remains a resource-poor language which has neither the\neconomic drive its cousin English has nor the sheer push of the law of numbers\na language such as Chinese has. A number of research groups from Sri Lanka have\nnoticed this dearth and the resultant dire need for proper tools and research\nfor Sinhala natural language processing. However, due to various reasons, these\nattempts seem to lack coordination and awareness of each other. The objective\nof this paper is to fill that gap of a comprehensive literature survey of the\npublicly available Sinhala natural language tools and research so that the\nresearchers working in this field can better utilize contributions of their\npeers. As such, we shall be uploading this paper to arXiv and perpetually\nupdate it periodically to reflect the advances made in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraphrastic Representations at Scale. (arXiv:2104.15114v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.15114","description":"<p>We present a system that allows users to train their own state-of-the-art\nparaphrastic sentence representations in a variety of languages. We also\nrelease trained models for English, Arabic, German, French, Spanish, Russian,\nTurkish, and Chinese. We train these models on large amounts of data, achieving\nsignificantly improved performance from the original papers proposing the\nmethods on a suite of monolingual semantic similarity, cross-lingual semantic\nsimilarity, and bitext mining tasks. Moreover, the resulting models surpass all\nprior work on unsupervised semantic textual similarity, significantly\noutperforming even BERT-based models like Sentence-BERT (Reimers and Gurevych,\n2019). Additionally, our models are orders of magnitude faster than prior work\nand can be used on CPU with little difference in inference speed (even improved\nspeed over GPU when using more CPU cores), making these models an attractive\nchoice for users without access to GPUs or for use on embedded devices.\nFinally, we add significantly increased functionality to the code bases for\ntraining paraphrastic sentence models, easing their use for both inference and\nfor training them for any desired language with parallel data. We also include\ncode to automatically download and preprocess training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data. (arXiv:2202.02842v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02842","description":"<p>Selecting suitable architecture parameters and training hyperparameters is\nessential for enhancing machine learning (ML) model performance. Several recent\nempirical studies conduct large-scale correlational analysis on neural networks\n(NNs) to search for effective \\emph{generalization metrics} that can guide this\ntype of model selection. Effective metrics are typically expected to correlate\nstrongly with test performance. In this paper, we expand on prior analyses by\nexamining generalization-metric-based model selection with the following\nobjectives: (i) focusing on natural language processing (NLP) tasks, as prior\nwork primarily concentrates on computer vision (CV) tasks; (ii) considering\nmetrics that directly predict \\emph{test error} instead of the\n\\emph{generalization gap}; (iii) exploring metrics that do not need access to\ndata to compute. From these objectives, we are able to provide the first model\nselection results on large pretrained Transformers from Huggingface using\ngeneralization metrics. Our analyses consider (I) hundreds of Transformers\ntrained in different settings, in which we systematically vary the amount of\ndata, the model size and the optimization hyperparameters, (II) a total of 51\npretrained Transformers from eight families of Huggingface NLP models,\nincluding GPT2, BERT, etc., and (III) a total of 28 existing and novel\ngeneralization metrics. Despite their niche status, we find that metrics\nderived from the heavy-tail (HT) perspective are particularly useful in NLP\ntasks, exhibiting stronger correlations than other, more popular metrics. To\nfurther examine these metrics, we extend prior formulations relying on power\nlaw (PL) spectral distributions to exponential (EXP) and\nexponentially-truncated power law (E-TPL) families.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaoqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theisen_R/0/1/0/all/0/1\">Ryan Theisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodgkinson_L/0/1/0/all/0/1\">Liam Hodgkinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_C/0/1/0/all/0/1\">Charles H. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMER: Multimodal Multi-task Learning for Speech Emotion Recognition. (arXiv:2203.16794v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16794","description":"<p>In this paper, we propose MMER, a novel Multimodal Multi-task learning\napproach for Speech Emotion Recognition. MMER leverages a novel multimodal\nnetwork based on early-fusion and cross-modal self-attention between text and\nacoustic modalities and solves three novel auxiliary tasks for learning emotion\nrecognition from spoken utterances. In practice, MMER outperforms all our\nbaselines and achieves state-of-the-art performance on the IEMOCAP benchmark.\nAdditionally, we conduct extensive ablation studies and results analysis to\nprove the effectiveness of our proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_U/0/1/0/all/0/1\">Utkarsh Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaneswaran_S/0/1/0/all/0/1\">S Ramaneswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_H/0/1/0/all/0/1\">Harshvardhan Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are We Really Making Much Progress in Text Classification? A Comparative Review. (arXiv:2204.03954v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03954","description":"<p>This study reviews and compares methods for single-label and multi-label text\nclassification, categorized into bag-of-words, sequence-based, graph-based, and\nhierarchical methods. The comparison aggregates results from the literature\nover five single-label and seven multi-label datasets and complements them with\nnew experiments. The findings reveal that all recently proposed graph-based and\nhierarchy-based methods fail to outperform pre-trained language models and\nsometimes perform worse than standard machine learning methods like a\nmultilayer perceptron on a bag-of-words. To assess the true scientific progress\nin text classification, future work should thoroughly test against strong\nbag-of-words baselines and state-of-the-art pre-trained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diera_A/0/1/0/all/0/1\">Andor Diera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bao Xin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khera_B/0/1/0/all/0/1\">Bhakti Khera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuser_T/0/1/0/all/0/1\">Tim Meuser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1\">Tushar Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karl_F/0/1/0/all/0/1\">Fabian Karl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference. (arXiv:2204.05428v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05428","description":"<p>Most evaluations of attribution methods focus on the English language. In\nthis work, we present a multilingual approach for evaluating attribution\nmethods for the Natural Language Inference (NLI) task in terms of faithfulness\nand plausibility. First, we introduce a novel cross-lingual strategy to measure\nfaithfulness based on word alignments, which eliminates the drawbacks of\nerasure-based evaluations.We then perform a comprehensive evaluation of\nattribution methods, considering different output mechanisms and aggregation\nmethods. Finally, we augment the XNLI dataset with highlight-based\nexplanations, providing a multilingual NLI dataset with highlights, to support\nfuture exNLP studies. Our results show that attribution methods performing best\nfor plausibility and faithfulness are different.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaman_K/0/1/0/all/0/1\">Kerem Zaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heterformer: Transformer-based Deep Node Representation Learning on Heterogeneous Text-Rich Networks. (arXiv:2205.10282v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10282","description":"<p>Representation learning on networks aims to derive a meaningful vector\nrepresentation for each node, thereby facilitating downstream tasks such as\nlink prediction, node classification, and node clustering. In heterogeneous\ntext-rich networks, this task is more challenging due to (1) presence or\nabsence of text: Some nodes are associated with rich textual information, while\nothers are not; (2) diversity of types: Nodes and edges of multiple types form\na heterogeneous network structure. As pretrained language models (PLMs) have\ndemonstrated their effectiveness in obtaining widely generalizable text\nrepresentations, a substantial amount of effort has been made to incorporate\nPLMs into representation learning on text-rich networks. However, few of them\ncan jointly consider heterogeneous structure (network) information as well as\nrich textual semantic information of each node effectively. In this paper, we\npropose Heterformer, a Heterogeneous Network-Empowered Transformer that\nperforms contextualized text encoding and heterogeneous structure encoding in a\nunified model. Specifically, we inject heterogeneous structure information into\neach Transformer layer when encoding node texts. Meanwhile, Heterformer is\ncapable of characterizing node/edge type heterogeneity and encoding nodes with\nor without texts. We conduct comprehensive experiments on three tasks (i.e.,\nlink prediction, node classification, and node clustering) on three large-scale\ndatasets from different domains, where Heterformer outperforms competitive\nbaselines significantly and consistently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Bowen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnifieR: A Unified Retriever for Large-Scale Retrieval. (arXiv:2205.11194v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2205.11194","description":"<p>Large-scale retrieval is to recall relevant documents from a huge collection\ngiven a query. It relies on representation learning to embed documents and\nqueries into a common semantic encoding space. According to the encoding space,\nrecent retrieval methods based on pre-trained language models (PLM) can be\ncoarsely categorized into either dense-vector or lexicon-based paradigms. These\ntwo paradigms unveil the PLMs' representation capability in different\ngranularities, i.e., global sequence-level compression and local word-level\ncontexts, respectively. Inspired by their complementary global-local\ncontextualization and distinct representing views, we propose a new learning\nframework, UnifieR which unifies dense-vector and lexicon-based retrieval in\none model with a dual-representing capability. Experiments on passage retrieval\nbenchmarks verify its effectiveness in both paradigms. A uni-retrieval scheme\nis further presented with even better retrieval quality. We lastly evaluate the\nmodel on BEIR benchmark to verify its transferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Anisotropic Cross-Lingual Model Editing. (arXiv:2205.12677v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12677","description":"<p>Multilingual pre-trained language models can learn task-specific abilities or\nmemorize facts across multiple languages but inevitably make undesired\npredictions with specific inputs. Under similar observation, model editing aims\nto post-hoc calibrate a model targeted to specific inputs with keeping the\nmodel's raw behavior. However, existing work only studies the monolingual\nscenario, which lacks the cross-lingual transferability to perform editing\nsimultaneously across languages. In this work, we focus on cross-lingual model\nediting. Firstly, we define the cross-lingual model editing task and\ncorresponding metrics, where an edit in one language propagates to the others.\nNext, we propose a framework to naturally adapt monolingual model editing\napproaches to the cross-lingual scenario using parallel corpus. Further, we\npropose language anisotropic editing to improve cross-lingual editing by\namplifying different subsets of parameters for each language. On the newly\ndefined cross-lingual model editing task, we empirically demonstrate the\nfailure of monolingual baselines in propagating the edit to multiple languages\nand the effectiveness of the proposed language anisotropic model editing. Our\ncode is publicly available at https://github.com/franklear/LiME.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yutai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks. (arXiv:2205.15171v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.15171","description":"<p>Societal biases are reflected in large pre-trained language models and their\nfine-tuned versions on downstream tasks. Common in-processing bias mitigation\napproaches, such as adversarial training and mutual information removal,\nintroduce additional optimization criteria, and update the model to reach a new\ndebiased state. However, in practice, end-users and practitioners might prefer\nto switch back to the original model, or apply debiasing only on a specific\nsubset of protected attributes. To enable this, we propose a novel modular bias\nmitigation approach, consisting of stand-alone highly sparse debiasing\nsubnetworks, where each debiasing module can be integrated into the core model\non-demand at inference time. Our approach draws from the concept of \\emph{diff}\npruning, and proposes a novel training regime adaptable to various\nrepresentation disentanglement optimizations. We conduct experiments on three\nclassification tasks with gender, race, and age as protected attributes. The\nresults show that our modular approach, while maintaining task performance,\nimproves (or at least remains on-par with) the effectiveness of bias mitigation\nin comparison with baseline finetuning. Particularly on a two-attribute\ndataset, our approach with separately learned debiasing subnetworks shows\neffective utilization of either or both the subnetworks for selective bias\nmitigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hauzenberger_L/0/1/0/all/0/1\">Lukas Hauzenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masoudian_S/0/1/0/all/0/1\">Shahed Masoudian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Deepak Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1\">Markus Schedl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tutel: Adaptive Mixture-of-Experts at Scale. (arXiv:2206.03382v2 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2206.03382","description":"<p>Sparsely-gated mixture-of-experts (MoE) has been widely adopted to scale deep\nlearning models to trillion-plus parameters with fixed computational cost. The\nalgorithmic performance of MoE relies on its token routing mechanism that\nforwards each input token to the right sub-models or experts. While token\nrouting dynamically determines the amount of expert workload at runtime,\nexisting systems suffer inefficient computation due to their static execution,\nnamely static parallelism and pipelining, which does not adapt to the dynamic\nworkload. We present Flex, a highly scalable stack design and implementation\nfor MoE with dynamically adaptive parallelism and pipelining. Flex designs an\nidentical layout for distributing MoE model parameters and input data, which\ncan be leveraged by all possible parallelism or pipelining methods without any\nmathematical inequivalence or tensor migration overhead. This enables adaptive\nparallelism/pipelining optimization at zero cost during runtime. Based on this\nkey design, Flex also implements various MoE acceleration techniques.\nAggregating all techniques, Flex finally delivers huge speedup at any scale --\n4.96x and 5.75x speedup of a single MoE layer over 16 and 2,048 A100 GPUs,\nrespectively, over the previous state-of-the-art. Our evaluation shows that\nFlex efficiently and effectively runs a real-world MoE-based model named\nSwinV2-MoE, built upon Swin Transformer V2, a state-of-the-art computer vision\narchitecture. On efficiency, Flex accelerates SwinV2-MoE, achieving up to 1.55x\nand 2.11x speedup in training and inference over Fairseq, respectively. On\neffectiveness, the SwinV2-MoE model achieves superior accuracy in both\npre-training and down-stream computer vision tasks such as COCO object\ndetection than the counterpart dense model, indicating the readiness of Flex\nfor end-to-end real-world model training and inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_C/0/1/0/all/0/1\">Changho Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yifan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salas_R/0/1/0/all/0/1\">Rafael Salas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1\">Jithin Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1\">Prabhat Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_J/0/1/0/all/0/1\">Joe Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Peng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yongqiang Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Generative Patent Language Models. (arXiv:2206.14578v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14578","description":"<p>Generative language models are promising for assisting human writing in\nvarious domains. This manuscript aims to build generative language models in\nthe patent domain and evaluate model performance from a human-centric\nperspective. The perspective is to measure the ratio of keystrokes that can be\nsaved by autocompletion based on generative patent language models. A higher\nratio means a more effective model which can save more keystrokes. This metric\ncan be used to benchmark model performance. The metric is different from\nconventional machine-centric metrics that are token-based instead of\nkeystroke-based. In terms of model size, the largest model built in this\nmanuscript is 6B, which is state-of-the-art in the patent domain. Based on the\nmetric, it is found that the largest model is not necessarily the best for the\nhuman-centric metric. The finding means that keeping increasing model sizes in\nthe patent domain might be unnecessary if the purpose is to assist human\nwriting with autocompletion. Several patent language models are pre-trained\nfrom scratch in this research. The pre-trained models are released for future\nresearchers. Several visualization tools are also provided. The importance of\nbuilding a generative language model in the patent domain is the potential to\nfacilitate creativity and innovations in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jieh-Sheng Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction. (arXiv:2209.05135v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2209.05135","description":"<p>Learning fine-grained movements is a challenging topic in robotics,\nparticularly in the context of robotic hands. One specific instance of this\nchallenge is the acquisition of fingerspelling sign language in robots. In this\npaper, we propose an approach for learning dexterous motor imitation from video\nexamples without additional information. To achieve this, we first build a URDF\nmodel of a robotic hand with a single actuator for each joint. We then leverage\npre-trained deep vision models to extract the 3D pose of the hand from RGB\nvideos. Next, using state-of-the-art reinforcement learning algorithms for\nmotion imitation (namely, proximal policy optimization and soft actor-critic),\nwe train a policy to reproduce the movement extracted from the demonstrations.\nWe identify the optimal set of hyperparameters for imitation based on a\nreference motion. Finally, we demonstrate the generalizability of our approach\nby testing it on six different tasks, corresponding to fingerspelled letters.\nOur results show that our approach is able to successfully imitate these\nfine-grained movements without additional information, highlighting its\npotential for real-world applications in robotics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tavella_F/0/1/0/all/0/1\">Federico Tavella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galata_A/0/1/0/all/0/1\">Aphrodite Galata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cangelosi_A/0/1/0/all/0/1\">Angelo Cangelosi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Generative & Dense Retrieval for Query Rewriting in Sponsored Search. (arXiv:2209.05861v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.05861","description":"<p>Sponsored search is a key revenue source for search engines, where\nadvertisers bid on keywords to target users or search queries of interest.\nHowever, finding relevant keywords for a given query is challenging due to the\nlarge and dynamic keyword space, ambiguous user/advertiser intents, and diverse\npossible topics and languages. In this work, we present a comprehensive\ncomparison between two paradigms for online query rewriting: Generative (NLG)\nand Dense Retrieval (DR) methods. We observe that both methods offer\ncomplementary benefits that are additive. As a result, we show that around 40%\nof the high-quality keywords retrieved by the two approaches are unique and not\nretrieved by the other. To leverage the strengths of both methods, we propose\nCLOVER-Unity, a novel approach that unifies generative and dense retrieval\nmethods in one single model. Through offline experiments, we show that the NLG\nand DR components of CLOVER-Unity consistently outperform individually trained\nNLG and DR models on public and internal benchmarks. Furthermore, we show that\nCLOVER-Unity achieves 9.8% higher good keyword density than the ensemble of two\nseparate DR and NLG models while reducing computational costs by almost half.\nWe conduct extensive online A/B experiments on Microsoft Bing in 140+ countries\nand achieve improved user engagement, with an average increase in total clicks\nby 0.89% and increased revenue by 1.27%. We also share our practical lessons\nand optimization tricks for deploying such unified models in production.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohankumar_A/0/1/0/all/0/1\">Akash Kumar Mohankumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodla_B/0/1/0/all/0/1\">Bhargav Dodla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_G/0/1/0/all/0/1\">Gururaj K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amit Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model. (arXiv:2209.06794v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2209.06794","description":"<p>Effective scaling and a flexible task interface enable large language models\nto excel at many tasks. We present PaLI (Pathways Language and Image model), a\nmodel that extends this approach to the joint modeling of language and vision.\nPaLI generates text based on visual and textual inputs, and with this interface\nperforms many vision, language, and multimodal tasks, in many languages. To\ntrain PaLI, we make use of large pre-trained encoder-decoder language models\nand Vision Transformers (ViTs). This allows us to capitalize on their existing\ncapabilities and leverage the substantial cost of training them. We find that\njoint scaling of the vision and language components is important. Since\nexisting Transformers for language are much larger than their vision\ncounterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the\nbenefits from even larger-capacity vision models. To train PaLI, we create a\nlarge multilingual mix of pretraining tasks, based on a new image-text training\nset containing 10B images and texts in over 100 languages. PaLI achieves\nstate-of-the-art in multiple vision and language tasks (such as captioning,\nvisual question-answering, scene-text understanding), while retaining a simple,\nmodular, and scalable design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1\">AJ Piergiovanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padlewski_P/0/1/0/all/0/1\">Piotr Padlewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salz_D/0/1/0/all/0/1\">Daniel Salz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_S/0/1/0/all/0/1\">Sebastian Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grycner_A/0/1/0/all/0/1\">Adam Grycner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1\">Basil Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1\">Joan Puigcerver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_K/0/1/0/all/0/1\">Keran Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbari_H/0/1/0/all/0/1\">Hassan Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1\">Gaurav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Linting Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thapliyal_A/0/1/0/all/0/1\">Ashish Thapliyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradbury_J/0/1/0/all/0/1\">James Bradbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1\">Weicheng Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seyedhosseini_M/0/1/0/all/0/1\">Mojtaba Seyedhosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayan_B/0/1/0/all/0/1\">Burcu Karagol Ayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riquelme_C/0/1/0/all/0/1\">Carlos Riquelme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1\">Andreas Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UCEpic: Unifying Aspect Planning and Lexical Constraints for Generating Explanations in Recommendation. (arXiv:2209.13885v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2209.13885","description":"<p>Personalized natural language generation for explainable recommendations\nplays a key role in justifying why a recommendation might match a user's\ninterests. Existing models usually control the generation process by aspect\nplanning. While promising, these aspect-planning methods struggle to generate\nspecific information correctly, which prevents generated explanations from\nbeing convincing. In this paper, we claim that introducing lexical constraints\ncan alleviate the above issues. We propose a model, UCEpic, that generates\nhigh-quality personalized explanations for recommendation results by unifying\naspect planning and lexical constraints in an insertion-based generation\nmanner.\n</p>\n<p>Methodologically, to ensure text generation quality and robustness to various\nlexical constraints, we pre-train a non-personalized text generator via our\nproposed robust insertion process. Then, to obtain personalized explanations\nunder this framework of insertion-based generation, we design a method of\nincorporating aspect planning and personalized references into the insertion\nprocess. Hence, UCEpic unifies aspect planning and lexical constraints into one\nframework and generates explanations for recommendations under different\nsettings. Compared to previous recommendation explanation generators controlled\nby only aspects, UCEpic incorporates specific information from keyphrases and\nthen largely improves the diversity and informativeness of generated\nexplanations for recommendations on datasets such as RateBeer and Yelp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhankui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning. (arXiv:2210.04183v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.04183","description":"<p>Multimodal representation learning has shown promising improvements on\nvarious vision-language tasks. Most existing methods excel at building\nglobal-level alignment between vision and language while lacking effective\nfine-grained image-text interaction. In this paper, we propose a jointly masked\nmultimodal modeling method to learn fine-grained multimodal representations.\nOur method performs joint masking on image-text input and integrates both\nimplicit and explicit targets for the masked signals to recover. The implicit\ntarget provides a unified and debiased objective for vision and language, where\nthe model predicts latent multimodal representations of the unmasked input. The\nexplicit target further enriches the multimodal representations by recovering\nhigh-level and semantically meaningful information: momentum visual features of\nimage patches and concepts of word tokens. Through such a masked modeling\nprocess, our model not only learns fine-grained multimodal interaction, but\nalso avoids the semantic gap between high-level representations and low- or\nmid-level prediction targets (e.g. image pixels), thus producing semantically\nrich multimodal representations that perform well on both zero-shot and\nfine-tuned settings. Our pre-trained model (named MAMO) achieves\nstate-of-the-art performance on various downstream vision-language tasks,\nincluding image-text retrieval, visual question answering, visual reasoning,\nand weakly-supervised visual grounding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zijia Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Longteng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingjian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Shuai Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Massive Multilingual Pre-Trained Machine Translation Models for Clinical Domain via Transfer Learning. (arXiv:2210.06068v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06068","description":"<p>Massively multilingual pre-trained language models (MMPLMs) are developed in\nrecent years demonstrating superpowers and the pre-knowledge they acquire for\ndownstream tasks. This work investigates whether MMPLMs can be applied to\nclinical domain machine translation (MT) towards entirely unseen languages via\ntransfer learning. We carry out an experimental investigation using Meta-AI's\nMMPLMs ``wmt21-dense-24-wide-en-X and X-en (WMT21fb)'' which were pre-trained\non 7 language pairs and 14 translation directions including English to Czech,\nGerman, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite\ndirection. We fine-tune these MMPLMs towards English-\\textit{Spanish} language\npair which \\textit{did not exist at all} in their original pre-trained corpora\nboth implicitly and explicitly. We prepare carefully aligned \\textit{clinical}\ndomain data for this fine-tuning, which is different from their original mixed\ndomain knowledge. Our experimental result shows that the fine-tuning is very\nsuccessful using just 250k well-aligned in-domain EN-ES segments for three\nsub-task translation testings: clinical cases, clinical terms, and ontology\nconcepts. It achieves very close evaluation scores to another MMPLM NLLB from\nMeta-AI, which included Spanish as a high-resource setting in the pre-training.\nTo the best of our knowledge, this is the first work on using MMPLMs towards\n\\textit{clinical domain transfer-learning NMT} successfully for totally unseen\nlanguages during pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1\">Gleb Erofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorokina_I/0/1/0/all/0/1\">Irina Sorokina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1\">Serge Gladkoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Knowledge Graph Augmentation Service for Boosting Domain-specific NLP Tasks. (arXiv:2212.05251v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05251","description":"<p>By focusing the pre-training process on domain-specific corpora, some\ndomain-specific pre-trained language models (PLMs) have achieved\nstate-of-the-art results. However, it is under-investigated to design a unified\nparadigm to inject domain knowledge in the PLM fine-tuning stage. We propose\nKnowledgeDA, a unified domain language model development service to enhance the\ntask-specific training procedure with domain knowledge graphs. Given\ndomain-specific task texts input, KnowledgeDA can automatically generate a\ndomain-specific language model following three steps: (i) localize domain\nknowledge entities in texts via an embedding-similarity approach; (ii) generate\naugmented samples by retrieving replaceable domain entity pairs from two views\nof both knowledge graph and training data; (iii) select high-quality augmented\nsamples for fine-tuning via confidence-based assessment. We implement a\nprototype of KnowledgeDA to learn language models for two domains, healthcare\nand software development. Experiments on domain-specific text classification\nand QA tasks verify the effectiveness and generalizability of KnowledgeDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Ruiqing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leye Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning. (arXiv:2212.08061v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08061","description":"<p>Generating a Chain of Thought (CoT) has been shown to consistently improve\nlarge language model (LLM) performance on a wide range of NLP tasks. However,\nprior work has mainly focused on logical reasoning tasks (e.g. arithmetic,\ncommonsense QA); it remains unclear whether improvements hold for more diverse\ntypes of reasoning, especially in socially situated contexts. Concretely, we\nperform a controlled evaluation of zero-shot CoT across two socially sensitive\ndomains: harmful questions and stereotype benchmarks. We find that zero-shot\nCoT reasoning in sensitive domains significantly increases a model's likelihood\nto produce harmful or undesirable output, with trends holding across different\nprompt formats and model variants. Furthermore, we show that harmful CoTs\nincrease with model size, but decrease with improved instruction following. Our\nwork suggests that zero-shot CoT should be used with caution on socially\nimportant tasks, especially when marginalized groups or sensitive topics are\ninvolved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_O/0/1/0/all/0/1\">Omar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_W/0/1/0/all/0/1\">William Held</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_M/0/1/0/all/0/1\">Michael Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference. (arXiv:2212.08153v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08153","description":"<p>Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that\nsets the state-of-the-art on many knowledge-intensive NLP tasks. However, the\narchitecture used for FiD was chosen by making minimal modifications to a\nstandard T5 model, which our analysis shows to be highly suboptimal for a\nretrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to\nthe encoder, while the majority of inference time results from memory bandwidth\nconstraints in the decoder. We propose two simple changes to the FiD\narchitecture to alleviate memory bandwidth constraints, and speed up inference\nby 7x. This allows us to use a much larger decoder at modest cost. We denote\nFiD with the above modifications as FiDO, and show that it strongly improves\nperformance over existing FiD models for a wide range of inference budgets. For\nexample, FiDO-Large-XXL performs faster inference than FiD-Base and achieves\nbetter performance than FiD-Large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1\">Michiel de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemlyanskiy_Y/0/1/0/all/0/1\">Yury Zemlyanskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+FitzGerald_N/0/1/0/all/0/1\">Nicholas FitzGerald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghai_S/0/1/0/all/0/1\">Sumit Sanghai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1\">Fei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning. (arXiv:2212.09282v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09282","description":"<p>Logical reasoning of text is an important ability that requires understanding\nthe information present in the text, their interconnections, and then reasoning\nthrough them to infer new conclusions. Prior works on improving the logical\nreasoning ability of language models require complex processing of training\ndata (e.g., aligning symbolic knowledge to text), yielding task-specific data\naugmentation solutions that restrict the learning of general logical reasoning\nskills. In this work, we propose APOLLO, an adaptively pretrained language\nmodel that has improved logical reasoning abilities. We select a subset of\nWikipedia, based on a set of logical inference keywords, for continued\npretraining of a language model. We use two self-supervised loss functions: a\nmodified masked language modeling loss where only specific parts-of-speech\nwords, that would likely require more reasoning than basic language\nunderstanding, are masked, and a sentence-level classification loss that\nteaches the model to distinguish between entailment and contradiction types of\nsentences. The proposed training paradigm is both simple and independent of\ntask formats. We demonstrate the effectiveness of APOLLO by comparing it with\nprior baselines on two logical reasoning datasets. APOLLO performs comparably\non ReClor and outperforms baselines on LogiQA. The code base has been made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NusaCrowd: Open Source Initiative for Indonesian NLP Resources. (arXiv:2212.09648v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09648","description":"<p>We present NusaCrowd, a collaborative initiative to collect and unify\nexisting resources for Indonesian languages, including opening access to\npreviously non-public resources. Through this initiative, we have brought\ntogether 137 datasets and 118 standardized data loaders. The quality of the\ndatasets has been assessed manually and automatically, and their value is\ndemonstrated through multiple experiments. NusaCrowd's data collection enables\nthe creation of the first zero-shot benchmarks for natural language\nunderstanding and generation in Indonesian and the local languages of\nIndonesia. Furthermore, NusaCrowd brings the creation of the first multilingual\nautomatic speech recognition benchmark in Indonesian and the local languages of\nIndonesia. Our work strives to advance natural language processing (NLP)\nresearch for languages that are under-represented despite being widely spoken.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendra_R/0/1/0/all/0/1\">Rahmad Mahendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wibisono_C/0/1/0/all/0/1\">Christian Wibisono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romadhony_A/0/1/0/all/0/1\">Ade Romadhony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincentio_K/0/1/0/all/0/1\">Karissa Vincentio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1\">Fajri Koto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoso_J/0/1/0/all/0/1\">Jennifer Santoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeljadi_D/0/1/0/all/0/1\">David Moeljadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wirawan_C/0/1/0/all/0/1\">Cahya Wirawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudi_F/0/1/0/all/0/1\">Frederikus Hudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmonangan_I/0/1/0/all/0/1\">Ivan Halim Parmonangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfina_I/0/1/0/all/0/1\">Ika Alfina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicaksono_M/0/1/0/all/0/1\">Muhammad Satrio Wicaksono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putra_I/0/1/0/all/0/1\">Ilham Firdausi Putra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmadani_S/0/1/0/all/0/1\">Samsul Rahmadani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oenang_Y/0/1/0/all/0/1\">Yulianti Oenang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Septiandri_A/0/1/0/all/0/1\">Ali Akbar Septiandri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaya_J/0/1/0/all/0/1\">James Jaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1\">Kaustubh D. Dhole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suryani_A/0/1/0/all/0/1\">Arie Ardiyanti Suryani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putri_R/0/1/0/all/0/1\">Rifki Afina Putri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_K/0/1/0/all/0/1\">Keith Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nityasya_M/0/1/0/all/0/1\">Made Nindyatama Nityasya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adilazuarda_M/0/1/0/all/0/1\">Muhammad Farid Adilazuarda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ignatius_R/0/1/0/all/0/1\">Ryan Ignatius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diandaru_R/0/1/0/all/0/1\">Ryandito Diandaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghifari_V/0/1/0/all/0/1\">Vito Ghifari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damapuspita_D/0/1/0/all/0/1\">Dyah Damapuspita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tho_C/0/1/0/all/0/1\">Cuk Tho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karo_I/0/1/0/all/0/1\">Ichwanul Muslim Karo Karo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatyanosa_T/0/1/0/all/0/1\">Tirana Noor Fatyanosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sujaini_H/0/1/0/all/0/1\">Herry Sujaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakti_S/0/1/0/all/0/1\">Sakriani Sakti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1\">Ayu Purwarianti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09849","description":"<p>Fine-tuning pre-trained language models has become the prevalent paradigm for\nbuilding downstream NLP models. Oftentimes fine-tuned models are readily\navailable but their training data is not, due to data privacy or intellectual\nproperty concerns. This creates a barrier to fusing knowledge across individual\nmodels to yield a better single model. In this paper, we study the problem of\nmerging individual models built on different training data sets to obtain a\nsingle model that performs well both across all data set domains and can\ngeneralize on out-of-domain data. We propose a dataless knowledge fusion method\nthat merges models in their parameter space, guided by weights that minimize\nprediction differences between the merged model and the individual models. Over\na battery of evaluation settings, we show that the proposed method\nsignificantly outperforms baselines such as Fisher-weighted averaging or model\nensembling. Further, we find that our method is a promising alternative to\nmulti-task learning that can preserve or sometimes improve over the individual\nmodels without access to the training data. Finally, model merging is more\nefficient than training a multi-task model, thus making it applicable to a\nwider set of scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xisen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preotiuc_Pietro_D/0/1/0/all/0/1\">Daniel Preotiuc-Pietro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pengxiang Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations. (arXiv:2212.09865v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09865","description":"<p>Although large language models can be prompted for both zero- and few-shot\nlearning, performance drops significantly when no demonstrations are available.\nIn this paper, we introduce Z-ICL, a new zero-shot method that closes the gap\nby constructing pseudo-demonstrations for a given test input using a raw text\ncorpus. Concretely, pseudo-demonstrations are constructed by (1) finding the\nnearest neighbors to the test input from the corpus and pairing them with\nrandom task labels, and (2) applying a set of techniques to reduce the amount\nof direct copying the model does from the resulting demonstrations. Evaluation\non nine classification datasets shows that Z-ICL outperforms previous zero-shot\nmethods by a significant margin, and is on par with in-context learning with\nlabeled training data in the few-shot setting. Overall, Z-ICL provides a\nsignificantly higher estimate of the zero-shot performance levels of a model,\nand supports future efforts to develop better pseudo-demonstrations that\nfurther improve zero-shot results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xinxi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BUMP: A Benchmark of Unfaithful Minimal Pairs for Meta-Evaluation of Faithfulness Metrics. (arXiv:2212.09955v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09955","description":"<p>The proliferation of automatic faithfulness metrics for summarization has\nproduced a need for benchmarks to evaluate them. While existing benchmarks\nmeasure the correlation with human judgements of faithfulness on\nmodel-generated summaries, they are insufficient for diagnosing whether metrics\nare: 1) consistent, i.e., indicate lower faithfulness as errors are introduced\ninto a summary, 2) effective on human-written texts, and 3) sensitive to\ndifferent error types (as summaries can contain multiple errors). To address\nthese needs, we present a benchmark of unfaithful minimal pairs (BUMP), a\ndataset of 889 human-written, minimally different summary pairs, where a single\nerror is introduced to a summary from the CNN/DailyMail dataset to produce an\nunfaithful summary. We find BUMP complements existing benchmarks in a number of\nways: 1) the summaries in BUMP are harder to discriminate and less probable\nunder SOTA summarization models, 2) unlike non-pair-based datasets, BUMP can be\nused to measure the consistency of metrics, and reveals that the most\ndiscriminative metrics tend not to be the most consistent, and 3) unlike\ndatasets containing generated summaries with multiple errors, BUMP enables the\nmeasurement of metrics' performance on individual error types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shuyang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logan_R/0/1/0/all/0/1\">Robert L. Logan IV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Di Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_S/0/1/0/all/0/1\">Shihao Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetreault_J/0/1/0/all/0/1\">Joel Tetreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaimes_A/0/1/0/all/0/1\">Alejandro Jaimes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval. (arXiv:2212.10726v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10726","description":"<p>Contrastive learning has been successfully used for retrieval of semantically\naligned sentences, but it often requires large batch sizes or careful\nengineering to work well. In this paper, we instead propose a generative model\nfor learning multilingual text embeddings which can be used to retrieve or\nscore sentence pairs. Our model operates on parallel data in $N$ languages and,\nthrough an approximation we introduce, efficiently encourages source separation\nin this multilingual setting, separating semantic information that is shared\nbetween translations from stylistic or language-specific variation. We show\ncareful large-scale comparisons between contrastive and generation-based\napproaches for learning multilingual text embeddings, a comparison that has not\nbeen done to the best of our knowledge despite the popularity of these\napproaches. We evaluate this method on a suite of tasks including semantic\nsimilarity, bitext mining, and cross-lingual question retrieval -- the last of\nwhich we introduce in this paper. Overall, our Variational Multilingual\nSource-Separation Transformer (VMSST) model outperforms both a strong\ncontrastive and generative baseline on these tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jonathan H. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-hop Evidence Retrieval for Cross-document Relation Extraction. (arXiv:2212.10786v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10786","description":"<p>Relation Extraction (RE) has been extended to cross-document scenarios\nbecause many relations are not simply described in a single document. This\ninevitably brings the challenge of efficient open-space evidence retrieval to\nsupport the inference of cross-document relations, along with the challenge of\nmulti-hop reasoning on top of entities and evidence scattered in an open set of\ndocuments. To combat these challenges, we propose MR.COD (Multi-hop evidence\nretrieval for Cross-document relation extraction), which is a multi-hop\nevidence retrieval method based on evidence path mining and ranking. We explore\nmultiple variants of retrievers to show evidence retrieval is essential in\ncross-document RE. We also propose a contextual dense retriever for this\nsetting. Experiments on CodRED show that evidence retrieval with MR.COD\neffectively acquires crossdocument evidence and boosts end-to-end RE\nperformance in both closed and open settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyu Derek Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute. (arXiv:2301.10448v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10448","description":"<p>Retrieval-augmented language models such as Fusion-in-Decoder are powerful,\nsetting the state of the art on a variety of knowledge-intensive tasks.\nHowever, they are also expensive, due to the need to encode a large number of\nretrieved passages. Some work avoids this cost by pre-encoding a text corpus\ninto a memory and retrieving dense representations directly. However,\npre-encoding memory incurs a severe quality penalty as the memory\nrepresentations are not conditioned on the current input. We propose LUMEN, a\nhybrid between these two extremes, pre-computing the majority of the retrieval\nrepresentation and completing the encoding on the fly using a live encoder that\nis conditioned on the question and fine-tuned for the task. We show that LUMEN\nsignificantly outperforms pure memory on multiple question-answering tasks\nwhile being much cheaper than FiD, and outperforms both for any given compute\nbudget. Moreover, the advantage of LUMEN over FiD increases with model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1\">Michiel de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemlyanskiy_Y/0/1/0/all/0/1\">Yury Zemlyanskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+FitzGerald_N/0/1/0/all/0/1\">Nicholas FitzGerald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghai_S/0/1/0/all/0/1\">Sumit Sanghai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1\">Fei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExaRanker: Explanation-Augmented Neural Ranker. (arXiv:2301.10521v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10521","description":"<p>Recent work has shown that inducing a large language model (LLM) to generate\nexplanations prior to outputting an answer is an effective strategy to improve\nperformance on a wide range of reasoning tasks. In this work, we show that\nneural rankers also benefit from explanations. We use LLMs such as GPT-3.5 to\naugment retrieval datasets with explanations and train a sequence-to-sequence\nranking model to output a relevance label and an explanation for a given\nquery-document pair. Our model, dubbed ExaRanker, finetuned on a few thousand\nexamples with synthetic explanations performs on par with models finetuned on\n3x more examples without explanations. Furthermore, the ExaRanker model incurs\nno additional computational cost during ranking and allows explanations to be\nrequested on demand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferraretto_F/0/1/0/all/0/1\">Fernando Ferraretto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laitz_T/0/1/0/all/0/1\">Thiago Laitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training for Speech Translation: CTC Meets Optimal Transport. (arXiv:2301.11716v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11716","description":"<p>The gap between speech and text modalities is a major challenge in\nspeech-to-text translation (ST). Different methods have been proposed to reduce\nthis gap, but most of them require architectural changes in ST training. In\nthis work, we propose to mitigate this issue at the pre-training stage,\nrequiring no change in the ST model. First, we show that the connectionist\ntemporal classification (CTC) loss can reduce the modality gap by design. We\nprovide a quantitative comparison with the more common cross-entropy loss,\nshowing that pre-training with CTC consistently achieves better final ST\naccuracy. Nevertheless, CTC is only a partial solution and thus, in our second\ncontribution, we propose a novel pre-training method combining CTC and optimal\ntransport to further reduce this gap. Our method pre-trains a Siamese-like\nmodel composed of two encoders, one for acoustic inputs and the other for\ntextual inputs, such that they produce representations that are close to each\nother in the Wasserstein space. Extensive experiments on the standard CoVoST-2\nand MuST-C datasets show that our pre-training method applied to the vanilla\nencoder-decoder Transformer achieves state-of-the-art performance under the\nno-external-data setting, and performs on par with recent strong multi-task\nlearning systems trained with external data. Finally, our method can also be\napplied on top of these multi-task systems, leading to further improvements for\nthese models. Code and pre-trained models are available at\nhttps://github.com/formiel/fairseq.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_P/0/1/0/all/0/1\">Phuong-Hang Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lecouteux_B/0/1/0/all/0/1\">Benjamin Lecouteux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1\">Didier Schwab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-context Example Selection with Influences. (arXiv:2302.11042v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.11042","description":"<p>In-context learning (ICL) is a powerful paradigm emerged from large language\nmodels (LLMs). Despite its promises, ICL performance is known to be highly\nsensitive to input examples. In this work, we use $\\textit{in-context\ninfluences}$ to analyze few-shot ICL performance directly from the in-context\nexamples. Our proposed influence-based example selection method can identify\nboth positive and negative examples, outperforming several baselines when\nevaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a $16.3\\%$\nperformance gap between using the most negative in-context examples compared to\nthe most positive. In a case study, we apply our influence-based framework to\nquantify the phenomena of recency bias in example ordering for few-shot ICL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1\">Eric Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Science of Detecting LLM-Generated Texts. (arXiv:2303.07205v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.07205","description":"<p>The emergence of large language models (LLMs) has resulted in the production\nof LLM-generated texts that is highly sophisticated and almost\nindistinguishable from texts written by humans. However, this has also sparked\nconcerns about the potential misuse of such texts, such as spreading\nmisinformation and causing disruptions in the education system. Although many\ndetection approaches have been proposed, a comprehensive understanding of the\nachievements and challenges is still lacking. This survey aims to provide an\noverview of existing LLM-generated text detection techniques and enhance the\ncontrol and regulation of language generation models. Furthermore, we emphasize\ncrucial considerations for future research, including the development of\ncomprehensive evaluation metrics and the threat posed by open-source LLMs, to\ndrive progress in the area of LLM-generated text detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yu-Neng Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Partial Knowledge Base Inference in Biomedical Entity Linking. (arXiv:2303.10330v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.10330","description":"<p>Biomedical entity linking (EL) consists of named entity recognition (NER) and\nnamed entity disambiguation (NED). EL models are trained on corpora labeled by\na predefined KB. However, it is a common scenario that only entities within a\nsubset of the KB are precious to stakeholders. We name this scenario partial\nknowledge base inference: training an EL model with one KB and inferring on the\npart of it without further training. In this work, we give a detailed\ndefinition and evaluation procedures for this practically valuable but\nsignificantly understudied scenario and evaluate methods from three\nrepresentative EL paradigms. We construct partial KB inference benchmarks and\nwitness a catastrophic degradation in EL performance due to dramatically\nprecision drop. Our findings reveal these EL paradigms can not correctly handle\nunlinkable mentions (NIL), so they are not robust to partial KB inference. We\nalso propose two simple-and-effective redemption methods to combat the NIL\nissue with little computational overhead. Codes are released at\nhttps://github.com/Yuanhy1997/PartialKB-EL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency. (arXiv:2304.02721v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02721","description":"<p>Sequence-to-sequence language models can be used to produce abstractive\nsummaries which are coherent, relevant, and concise. Still, model sizes can\nmake deployment in latency-sensitive or web-scale implementations difficult.\nThis paper studies the relationship between model size, structured pruning,\ninference efficiency, and summarization accuracy on widely used summarization\ndatasets. We show that model accuracy is tied to the encoder size while\ninference efficiency is connected to the decoder. Using asymmetric pruning can\nlead to nearly 3x improvement in inference latency with ~1 point loss in\nRouge-2. Moreover, we find both the average degradation and the role of\nasymmetry to be consistent across model sizes and variations in datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distinguishing ChatGPT(-3.5, -4)-generated and human-written papers through Japanese stylometric analysis. (arXiv:2304.05534v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.05534","description":"<p>In the first half of 2023, text-generative artificial intelligence (AI),\nincluding ChatGPT, equipped with GPT-3.5 and GPT-4, from OpenAI, has attracted\nconsiderable attention worldwide. In this study, first, we compared Japanese\nstylometric features of texts generated by GPT (-3.5 and -4) and those written\nby humans. In this work, we performed multi-dimensional scaling (MDS) to\nconfirm the distributions of 216 texts of three classes (72 academic papers\nwritten by 36 single authors, 72 texts generated by GPT-3.5, and 72 texts\ngenerated by GPT-4 on the basis of the titles of the aforementioned papers)\nfocusing on the following stylometric features: (1) bigrams of parts-of-speech,\n(2) bigram of postpositional particle words, (3) positioning of commas, and (4)\nrate of function words. MDS revealed distinct distributions at each stylometric\nfeature of GPT (-3.5 and -4) and human. Although GPT-4 is more powerful than\nGPT-3.5 because it has more parameters, both GPT (-3.5 and -4) distributions\nare likely to overlap. These results indicate that although the number of\nparameters may increase in the future, GPT-generated texts may not be close to\nthat written by humans in terms of stylometric features. Second, we verified\nthe classification performance of random forest (RF) for two classes (GPT and\nhuman) focusing on Japanese stylometric features. This study revealed the high\nperformance of RF in each stylometric feature: The RF classifier focusing on\nthe rate of function words achieved 98.1% accuracy. Furthermore the RF\nclassifier focusing on all stylometric features reached 100% in terms of all\nperformance indexes (accuracy, recall, precision, and F1 score). This study\nconcluded that at this stage we human discriminate ChatGPT from human limited\nto Japanese language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaitsu_W/0/1/0/all/0/1\">Wataru Zaitsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Mingzhe Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Dependent Embedding Utterance Representations for Emotion Recognition in Conversations. (arXiv:2304.08216v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.08216","description":"<p>Emotion Recognition in Conversations (ERC) has been gaining increasing\nimportance as conversational agents become more and more common. Recognizing\nemotions is key for effective communication, being a crucial component in the\ndevelopment of effective and empathetic conversational agents. Knowledge and\nunderstanding of the conversational context are extremely valuable for\nidentifying the emotions of the interlocutor. We thus approach Emotion\nRecognition in Conversations leveraging the conversational context, i.e.,\ntaking into attention previous conversational turns. The usual approach to\nmodel the conversational context has been to produce context-independent\nrepresentations of each utterance and subsequently perform contextual modeling\nof these. Here we propose context-dependent embedding representations of each\nutterance by leveraging the contextual representational power of pre-trained\ntransformer language models. In our approach, we feed the conversational\ncontext appended to the utterance to be classified as input to the RoBERTa\nencoder, to which we append a simple classification module, thus discarding the\nneed to deal with context after obtaining the embeddings since these constitute\nalready an efficient representation of such context. We also investigate how\nthe number of introduced conversational turns influences our model performance.\nThe effectiveness of our approach is validated on the open-domain DailyDialog\ndataset and on the task-oriented EmoWOZ dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_P/0/1/0/all/0/1\">Patr&#xed;cia Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_H/0/1/0/all/0/1\">Helena Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dias_I/0/1/0/all/0/1\">Isabel Dias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1\">Joao Paulo Carvalho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Inter-Bilingual Semantic Parsing for Indian Languages. (arXiv:2304.13005v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.13005","description":"<p>Despite significant progress in Natural Language Generation for Indian\nlanguages (IndicNLP), there is a lack of datasets around complex structured\ntasks such as semantic parsing. One reason for this imminent gap is the\ncomplexity of the logical form, which makes English to multilingual translation\ndifficult. The process involves alignment of logical forms, intents and slots\nwith translated unstructured utterance. To address this, we propose an\nInter-bilingual Seq2seq Semantic parsing dataset IE-SEMPARSE for 11 distinct\nIndian languages. We highlight the proposed task's practicality, and evaluate\nexisting multilingual seq2seq models across several train-test strategies. Our\nexperiment reveals a high correlation across performance of original\nmultilingual semantic parsing datasets (such as mTOP, multilingual TOP and\nmultiATIS++) and our proposed IE-SEMPARSE suite.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_D/0/1/0/all/0/1\">Divyanshu Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technical Report on Token Position Bias in Transformers. (arXiv:2304.13567v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.13567","description":"<p>Language Models (LMs) have shown state-of-the-art performance in Natural\nLanguage Processing (NLP) tasks. Downstream tasks such as Named Entity\nRecognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data\nimbalance issues, specifically in terms of the ratio of positive to negative\nexamples, and class imbalance. In this paper, we investigate an additional\nspecific issue for language models, namely the position bias of positive\nexamples in token classification tasks. Therefore, we conduct an in-depth\nevaluation of the impact of position bias on the performance of LMs when\nfine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and\nOntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We\npropose an evaluation approach to investigate position bias in Transformer\nmodels. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as\nGPT2 and BLOOM can suffer from this bias with an average drop of 3\\% and 9\\% in\ntheir performance. To mitigate this effect, we propose two methods: Random\nPosition Shifting and Context Perturbation, that we apply on batches during the\ntraining process. The results show an improvement of $\\approx$ 2\\% in the\nperformance of the model on CoNLL03, UD_en, and TweeBank.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amor_M/0/1/0/all/0/1\">Mehdi Ben Amor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granitzer_M/0/1/0/all/0/1\">Michael Granitzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrovic_J/0/1/0/all/0/1\">Jelena Mitrovi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WangLab at MEDIQA-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models. (arXiv:2305.02220v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02220","description":"<p>This paper describes our submission to the MEDIQA-Chat 2023 shared task for\nautomatic clinical note generation from doctor-patient conversations. We report\nresults for two approaches: the first fine-tunes a pre-trained language model\n(PLM) on the shared task data, and the second uses few-shot in-context learning\n(ICL) with a large language model (LLM). Both achieve high performance as\nmeasured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and\nfirst, respectively, of all submissions to the shared task. Expert human\nscrutiny indicates that notes generated via the ICL-based approach with GPT-4\nare preferred about as often as human-written notes, making it a promising path\ntoward automated note generation from doctor-patient conversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_J/0/1/0/all/0/1\">John Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toma_A/0/1/0/all/0/1\">Augustin Toma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Ronald Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sondra S. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1\">Kevin R. An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Grace X. Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SI-LSTM: Speaker Hybrid Long-short Term Memory and Cross Modal Attention for Emotion Recognition in Conversation. (arXiv:2305.03506v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03506","description":"<p>Emotion Recognition in Conversation~(ERC) across modalities is of vital\nimportance for a variety of applications, including intelligent healthcare,\nartificial intelligence for conversation, and opinion mining over chat history.\nThe crux of ERC is to model both cross-modality and cross-time interactions\nthroughout the conversation. Previous methods have made progress in learning\nthe time series information of conversation while lacking the ability to trace\ndown the different emotional states of each speaker in a conversation. In this\npaper, we propose a recurrent structure called Speaker Information Enhanced\nLong-Short Term Memory (SI-LSTM) for the ERC task, where the emotional states\nof the distinct speaker can be tracked in a sequential way to enhance the\nlearning of the emotion in conversation. Further, to improve the learning of\nmultimodal features in ERC, we utilize a cross-modal attention component to\nfuse the features between different modalities and model the interaction of the\nimportant information from different modalities. Experimental results on two\nbenchmark datasets demonstrate the superiority of the proposed SI-LSTM against\nthe state-of-the-art baseline methods in the ERC task on multimodal data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xingwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">You Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Domain-Adaptive Visually-fused Event Detection from Text. (arXiv:2305.03517v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03517","description":"<p>Incorporating auxiliary modalities such as images into event detection models\nhas attracted increasing interest over the last few years. The complexity of\nnatural language in describing situations has motivated researchers to leverage\nthe related visual context to improve event detection performance. However,\ncurrent approaches in this area suffer from data scarcity, where a large amount\nof labelled text-image pairs are required for model training. Furthermore,\nlimited access to the visual context at inference time negatively impacts the\nperformance of such models, which makes them practically ineffective in\nreal-world scenarios. In this paper, we present a novel domain-adaptive\nvisually-fused event detection approach that can be trained on a few labelled\nimage-text paired data points. Specifically, we introduce a visual imaginator\nmethod that synthesises images from text in the absence of visual context.\nMoreover, the imaginator can be customised to a specific domain. In doing so,\nour model can leverage the capabilities of pre-trained vision-language models\nand can be trained in a few-shot setting. This also allows for effective\ninference where only single-modality data (i.e. text) is available. The\nexperimental evaluation on the benchmark M2E2 dataset shows that our model\noutperforms existing state-of-the-art models, by up to 11 points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moghimifar_F/0/1/0/all/0/1\">Farhad Moghimifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiri_F/0/1/0/all/0/1\">Fatemeh Shiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_R/0/1/0/all/0/1\">Reza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Edit: Fault-Aware Code Editor for Code Generation. (arXiv:2305.04087v3 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2305.04087","description":"<p>Large language models (LLMs) have demonstrated an impressive ability to\ngenerate codes on competitive programming tasks. However, with limited sample\nnumbers, LLMs still suffer from poor accuracy. Inspired by the process of human\nprogramming, we propose a generate-and-edit approach named Self-Edit that\nutilizes execution results of the generated code from LLMs to improve the code\nquality on the competitive programming task. We execute the generated code on\nthe example test case provided in the question and wrap execution results into\na supplementary comment. Utilizing this comment as guidance, our fault-aware\ncode editor is employed to correct errors in the generated code. We perform\nextensive evaluations across two competitive programming datasets with nine\ndifferent LLMs. Compared to directly generating from LLMs, our approach can\nimprove the average of pass@1 by 89\\% on APPS-dev, 31\\% on APPS-test, and 48\\%\non HumanEval over nine popular code generation LLMs with parameter sizes\nranging from 110M to 175B. Compared to other post-processing methods, our\nmethod demonstrates superior accuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kechi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhi Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MGR: Multi-generator Based Rationalization. (arXiv:2305.04492v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.04492","description":"<p>Rationalization is to employ a generator and a predictor to construct a\nself-explaining NLP model in which the generator selects a subset of\nhuman-intelligible pieces of the input text to the following predictor.\nHowever, rationalization suffers from two key challenges, i.e., spurious\ncorrelation and degeneration, where the predictor overfits the spurious or\nmeaningless pieces solely selected by the not-yet well-trained generator and in\nturn deteriorates the generator. Although many studies have been proposed to\naddress the two challenges, they are usually designed separately and do not\ntake both of them into account. In this paper, we propose a simple yet\neffective method named MGR to simultaneously solve the two problems. The key\nidea of MGR is to employ multiple generators such that the occurrence stability\nof real pieces is improved and more meaningful pieces are delivered to the\npredictor. Empirically, we show that MGR improves the F1 score by up to 20.9%\nas compared to state-of-the-art methods. Codes are available at\nhttps://github.com/jugechengzi/Rationalization-MGR .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haozhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuankai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Target-Side Augmentation for Document-Level Machine Translation. (arXiv:2305.04505v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04505","description":"<p>Document-level machine translation faces the challenge of data sparsity due\nto its long input length and a small amount of training data, increasing the\nrisk of learning spurious patterns. To address this challenge, we propose a\ntarget-side augmentation method, introducing a data augmentation (DA) model to\ngenerate many potential translations for each source document. Learning on\nthese wider range translations, an MT model can learn a smoothed distribution,\nthereby reducing the risk of data sparsity. We demonstrate that the DA model,\nwhich estimates the posterior distribution, largely improves the MT\nperformance, outperforming the previous best system by 2.30 s-BLEU on News and\nachieving new state-of-the-art on News and Europarl benchmarks. Our code is\navailable at https://github.com/baoguangsheng/target-side-augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_G/0/1/0/all/0/1\">Guangsheng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhiyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Radiology Report Generation by Infusing Comparison Prior. (arXiv:2305.04561v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04561","description":"<p>Recent transformer-based models have made significant strides in generating\nradiology reports from chest X-ray images. However, a prominent challenge\nremains: these models often lack prior knowledge, resulting in the generation\nof synthetic reports that mistakenly reference non-existent prior exams. This\ndiscrepancy can be attributed to a knowledge gap between radiologists and the\ngeneration models. While radiologists possess patient-specific prior\ninformation, the models solely receive X-ray images at a specific time point.\nTo tackle this issue, we propose a novel approach that leverages a rule-based\nlabeler to extract comparison prior information from radiology reports. This\nextracted comparison prior is then seamlessly integrated into state-of-the-art\ntransformer-based models, enabling them to produce more realistic and\ncomprehensive reports. Our method is evaluated on English report datasets, such\nas IU X-ray and MIMIC-CXR. The results demonstrate that our approach surpasses\nbaseline models in terms of natural language generation metrics. Notably, our\nmodel generates reports that are free from false references to non-existent\nprior exams, setting it apart from previous models. By addressing this\nlimitation, our approach represents a significant step towards bridging the gap\nbetween radiologists and generation models in the domain of medical report\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sanghwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nooralahzadeh_F/0/1/0/all/0/1\">Farhad Nooralahzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohanian_M/0/1/0/all/0/1\">Morteza Rohanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujimoto_K/0/1/0/all/0/1\">Koji Fujimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishio_M/0/1/0/all/0/1\">Mizuho Nishio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakamoto_R/0/1/0/all/0/1\">Ryo Sakamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rinaldi_F/0/1/0/all/0/1\">Fabio Rinaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauthammer_M/0/1/0/all/0/1\">Michael Krauthammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A First Look at LLM-Powered Generative News Recommendation. (arXiv:2305.06566v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2305.06566","description":"<p>Personalized news recommendation systems have become essential tools for\nusers to navigate the vast amount of online news content, yet existing news\nrecommenders face significant challenges such as the cold-start problem, user\nprofile modeling, and news content understanding. Previous works have typically\nfollowed an inflexible routine to address a particular challenge through model\ndesign, but are limited in their ability to understand news content and capture\nuser interests. In this paper, we introduce GENRE, an LLM-powered generative\nnews recommendation framework, which leverages pretrained semantic knowledge\nfrom large language models to enrich news data. Our aim is to provide a\nflexible and unified solution for news recommendation by moving from model\ndesign to prompt design. We showcase the use of GENRE for personalized news\ngeneration, user profiling, and news summarization. Extensive experiments with\nvarious popular recommendation models demonstrate the effectiveness of GENRE.\nWe will publish our code and data for other researchers to reproduce our work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qijiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakai_T/0/1/0/all/0/1\">Tetsuya Sakai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing. (arXiv:2305.08195v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08195","description":"<p>Interactive semantic parsing based on natural language (NL) feedback, where\nusers provide feedback to correct the parser mistakes, has emerged as a more\npractical scenario than the traditional one-shot semantic parsing. However,\nprior work has heavily relied on human-annotated feedback data to train the\ninteractive semantic parser, which is prohibitively expensive and not scalable.\nIn this work, we propose a new task of simulating NL feedback for interactive\nsemantic parsing. We accompany the task with a novel feedback evaluator. The\nevaluator is specifically designed to assess the quality of the simulated\nfeedback, based on which we decide the best feedback simulator from our\nproposed variants. On a text-to-SQL dataset, we show that our feedback\nsimulator can generate high-quality NL feedback to boost the error correction\nability of a specific parser. In low-data settings, our feedback simulator can\nhelp achieve comparable error correction performance as trained using the\ncostly, full set of human annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Saurabh Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yintao Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida I. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Ziyu Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions. (arXiv:2305.10614v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10614","description":"<p>While there is much recent interest in studying why Transformer-based large\nlanguage models make predictions the way they do, the complex computations\nperformed within each layer have made their behavior somewhat opaque. To\nmitigate this opacity, this work presents a linear decomposition of final\nhidden states from autoregressive language models based on each initial input\ntoken, which is exact for virtually all contemporary Transformer architectures.\nThis decomposition allows the definition of probability distributions that\nablate the contribution of specific input tokens, which can be used to analyze\ntheir influence on model probabilities over a sequence of upcoming words with\nonly one forward pass from the model. Using the change in next-word probability\nas a measure of importance, this work first examines which context words make\nthe biggest contribution to language model predictions. Regression experiments\nsuggest that Transformer-based language models rely primarily on collocational\nassociations, followed by linguistic factors such as syntactic dependencies and\ncoreference relationships in making next-word predictions. Additionally,\nanalyses using these measures to predict syntactic dependencies and coreferent\nmention spans show that collocational association and repetitions of the same\ntoken largely explain the language models' predictions on these tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_B/0/1/0/all/0/1\">Byung-Doh Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuler_W/0/1/0/all/0/1\">William Schuler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10847","description":"<p>Large Language Models (LLMs) have demonstrated exceptional performance in a\nvariety of tasks, including essay writing and question answering. However, it\nis crucial to address the potential misuse of these models, which can lead to\ndetrimental outcomes such as plagiarism and spamming. Recently, several\ndetectors have been proposed, including fine-tuned classifiers and various\nstatistical methods. In this study, we reveal that with the aid of carefully\ncrafted prompts, LLMs can effectively evade these detection systems. We propose\na novel Substitution-based In-Context example Optimization method (SICO) to\nautomatically generate such prompts. On three real-world tasks where LLMs can\nbe misused, SICO successfully enables ChatGPT to evade six existing detectors,\ncausing a significant 0.54 AUC drop on average. Surprisingly, in most cases\nthese detectors perform even worse than random classifiers. These results\nfirmly reveal the vulnerability of existing detectors. Finally, the strong\nperformance of SICO suggests itself as a reliable evaluation protocol for any\nnew detector in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1\">Ning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Rui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Implicit Sentiment with Chain-of-Thought Prompting. (arXiv:2305.11255v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11255","description":"<p>While sentiment analysis systems try to determine the sentiment polarities of\ngiven targets based on the key opinion expressions in input texts, in implicit\nsentiment analysis (ISA) the opinion cues come in an implicit and obscure\nmanner. Thus detecting implicit sentiment requires the common-sense and\nmulti-hop reasoning ability to infer the latent intent of opinion. Inspired by\nthe recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop\nReasoning (THOR) CoT framework to mimic the human-like reasoning process for\nISA. We design a three-step prompting principle for THOR to step-by-step induce\nthe implicit aspect, opinion, and finally the sentiment polarity. Our\nTHOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on\nsupervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50%\nF1 on zero-shot setting. Our code is at\nhttps://github.com/scofield7419/THOR-ISA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Code-mixed Universal Dependency Forest for Unbiased Cross-lingual Relation Extraction. (arXiv:2305.12258v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12258","description":"<p>Latest efforts on cross-lingual relation extraction (XRE) aggressively\nleverage the language-consistent structural features from the universal\ndependency (UD) resource, while they may largely suffer from biased transfer\n(e.g., either target-biased or source-biased) due to the inevitable linguistic\ndisparity between languages. In this work, we investigate an unbiased UD-based\nXRE transfer by constructing a type of code-mixed UD forest. We first translate\nthe sentence of the source language to the parallel target-side language, for\nboth of which we parse the UD tree respectively. Then, we merge the\nsource-/target-side UD structures as a unified code-mixed UD forest. With such\nforest features, the gaps of UD-based XRE between the training and predicting\nphases can be effectively closed. We conduct experiments on the ACE XRE\nbenchmark datasets, where the results demonstrate that the proposed code-mixed\nUD forests help unbiased UD-based XRE transfer, with which we achieve\nsignificant XRE performance gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning. (arXiv:2305.13628v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13628","description":"<p>In cross-lingual named entity recognition (NER), self-training is commonly\nused to bridge the linguistic gap by training on pseudo-labeled target-language\ndata. However, due to sub-optimal performance on target languages, the pseudo\nlabels are often noisy and limit the overall performance. In this work, we aim\nto improve self-training for cross-lingual NER by combining representation\nlearning and pseudo label refinement in one coherent framework. Our proposed\nmethod, namely ContProto mainly comprises two components: (1) contrastive\nself-training and (2) prototype-based pseudo-labeling. Our contrastive\nself-training facilitates span classification by separating clusters of\ndifferent classes, and enhances cross-lingual transferability by producing\nclosely-aligned representations between the source and target language.\nMeanwhile, prototype-based pseudo-labeling effectively improves the accuracy of\npseudo labels during training. We evaluate ContProto on multiple transfer\npairs, and experimental results show our method brings in substantial\nimprovements over current state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16380","description":"<p>Transformer architecture has shown impressive performance in multiple\nresearch domains and has become the backbone of many neural network models.\nHowever, there is limited understanding on how it works. In particular, with a\nsimple predictive loss, how the representation emerges from the gradient\n\\emph{training dynamics} remains a mystery. In this paper, for 1-layer\ntransformer with one self-attention layer plus one decoder layer, we analyze\nits SGD training dynamics for the task of next token prediction in a\nmathematically rigorous manner. We open the black box of the dynamic process of\nhow the self-attention layer combines input tokens, and reveal the nature of\nunderlying inductive bias. More specifically, with the assumption (a) no\npositional encoding, (b) long input sequence, and (c) the decoder layer learns\nfaster than the self-attention layer, we prove that self-attention acts as a\n\\emph{discriminative scanning algorithm}: starting from uniform attention, it\ngradually attends more to distinct key tokens for a specific next token to be\npredicted, and pays less attention to common key tokens that occur across\ndifferent next tokens. Among distinct tokens, it progressively drops attention\nweights, following the order of low to high co-occurrence between the key and\nthe query token in the training set. Interestingly, this procedure does not\nlead to winner-takes-all, but decelerates due to a \\emph{phase transition} that\nis controllable by the learning rates of the two layers, leaving (almost) fixed\ntoken combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on\nsynthetic and real-world data (WikiText).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beidi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Probing for the Influence of Affect and Specificity on Intergroup Bias. (arXiv:2305.16409v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16409","description":"<p>While existing work on studying bias in NLP focues on negative or pejorative\nlanguage use, Govindarajan et al. (2023) offer a revised framing of bias in\nterms of intergroup social context, and its effects on language behavior. In\nthis paper, we investigate if two pragmatic features (specificity and affect)\nsystematically vary in different intergroup contexts -- thus connecting this\nnew framing of bias to language output. Preliminary analysis finds modest\ncorrelations between specificity and affect of tweets with supervised\nintergroup relationship (IGR) labels. Counterfactual probing further reveals\nthat while neural models finetuned for predicting IGR labels reliably use\naffect in classification, the model's usage of specificity is inconclusive.\nCode and data can be found at: https://github.com/venkatasg/intergroup-probing\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Govindarajan_V/0/1/0/all/0/1\">Venkata S Govindarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaver_D/0/1/0/all/0/1\">David I. Beaver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP Reproducibility For All: Understanding Experiences of Beginners. (arXiv:2305.16579v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16579","description":"<p>As natural language processing (NLP) has recently seen an unprecedented level\nof excitement, and more people are eager to enter the field, it is unclear\nwhether current research reproducibility efforts are sufficient for this group\nof beginners to apply the latest developments. To understand their needs, we\nconducted a study with 93 students in an introductory NLP course, where\nstudents reproduced the results of recent NLP papers. Surprisingly, we find\nthat their programming skill and comprehension of research papers have a\nlimited impact on their effort spent completing the exercise. Instead, we find\naccessibility efforts by research authors to be the key to success, including\ncomplete documentation, better coding practice, and easier access to data\nfiles. Going forward, we recommend that NLP researchers pay close attention to\nthese simple aspects of open-sourcing their work, and use insights from\nbeginners' feedback to provide actionable ideas on how to better support them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Keunwoo Peter Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziqiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Imagine: Visually-Augmented Natural Language Generation. (arXiv:2305.16944v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16944","description":"<p>People often imagine relevant scenes to aid in the writing process. In this\nwork, we aim to utilize visual information for composition in the same manner\nas humans. We propose a method, LIVE, that makes pre-trained language models\n(PLMs) Learn to Imagine for Visuallyaugmented natural language gEneration.\nFirst, we imagine the scene based on the text: we use a diffusion model to\nsynthesize high-quality images conditioned on the input texts. Second, we use\nCLIP to determine whether the text can evoke the imagination in a posterior\nway. Finally, our imagination is dynamic, and we conduct synthesis for each\nsentence rather than generate only one image for an entire paragraph.\nTechnically, we propose a novel plug-and-play fusion layer to obtain\nvisually-augmented representations for each text. Our vision-text fusion layer\nis compatible with Transformerbased architecture. We have conducted extensive\nexperiments on four generation tasks using BART and T5, and the automatic\nresults and human evaluation demonstrate the effectiveness of our proposed\nmethod. We will release the code, model, and data at the link:\nhttps://github.com/RUCAIBox/LIVE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yushuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Copying Problem of Unsupervised NMT: A Training Schedule with a Language Discriminator Loss. (arXiv:2305.17182v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17182","description":"<p>Although unsupervised neural machine translation (UNMT) has achieved success\nin many language pairs, the copying problem, i.e., directly copying some parts\nof the input sentence as the translation, is common among distant language\npairs, especially when low-resource languages are involved. We find this issue\nis closely related to an unexpected copying behavior during online\nback-translation (BT). In this work, we propose a simple but effective training\nschedule that incorporates a language discriminator loss. The loss imposes\nconstraints on the intermediate translation so that the translation is in the\ndesired language. By conducting extensive experiments on different language\npairs, including similar and distant, high and low-resource languages, we find\nthat our method alleviates the copying problem, thus improving the translation\nperformance on low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chronopoulou_A/0/1/0/all/0/1\">Alexandra Chronopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answering Unanswered Questions through Semantic Reformulations in Spoken QA. (arXiv:2305.17393v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17393","description":"<p>Spoken Question Answering (QA) is a key feature of voice assistants, usually\nbacked by multiple QA systems. Users ask questions via spontaneous speech which\ncan contain disfluencies, errors, and informal syntax or phrasing. This is a\nmajor challenge in QA, causing unanswered questions or irrelevant answers, and\nleading to bad user experiences. We analyze failed QA requests to identify core\nchallenges: lexical gaps, proposition types, complex syntactic structure, and\nhigh specificity. We propose a Semantic Question Reformulation (SURF) model\noffering three linguistically-grounded operations (repair, syntactic reshaping,\ngeneralization) to rewrite questions to facilitate answering. Offline\nevaluation on 1M unanswered questions from a leading voice assistant shows that\nSURF significantly improves answer rates: up to 24% of previously unanswered\nquestions obtain relevant answers (75%). Live deployment shows positive impact\nfor millions of customers with unanswered questions; explicit relevance\nfeedback shows high user satisfaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faustini_P/0/1/0/all/0/1\">Pedro Faustini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetahu_B/0/1/0/all/0/1\">Besnik Fetahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rokhlenko_O/0/1/0/all/0/1\">Oleg Rokhlenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malmasi_S/0/1/0/all/0/1\">Shervin Malmasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark. (arXiv:2305.17553v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17553","description":"<p>Recent model editing techniques promise to mitigate the problem of memorizing\nfalse or outdated associations during LLM training. However, we show that these\ntechniques can introduce large unwanted side effects which are not detected by\nexisting specificity benchmarks. We extend the existing CounterFact benchmark\nto include a dynamic component and dub our benchmark CounterFact+.\nAdditionally, we extend the metrics used for measuring specificity by a\nprincipled KL divergence-based metric. We use this improved benchmark to\nevaluate recent model editing techniques and find that they suffer from low\nspecificity. Our findings highlight the need for improved specificity\nbenchmarks that identify and prevent unwanted side effects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoelscher_Obermaier_J/0/1/0/all/0/1\">Jason Hoelscher-Obermaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Persson_J/0/1/0/all/0/1\">Julia Persson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kran_E/0/1/0/all/0/1\">Esben Kran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstas_I/0/1/0/all/0/1\">Ioannis Konstas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1\">Fazl Barez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Analogical Reasoning with Pre-Trained Language Models. (arXiv:2305.17626v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2305.17626","description":"<p>Analogical reasoning is a fundamental capacity of human cognition that allows\nus to reason abstractly about novel situations by relating them to past\nexperiences. While it is thought to be essential for robust reasoning in AI\nsystems, conventional approaches require significant training and/or\nhard-coding of domain knowledge to be applied to benchmark tasks. Inspired by\ncognitive science research that has found connections between human language\nand analogy-making, we explore the use of intuitive language-based abstractions\nto support analogy in AI systems. Specifically, we apply large pre-trained\nlanguage models (PLMs) to visual Raven's Progressive Matrices (RPM), a common\nrelational reasoning test. By simply encoding the perceptual features of the\nproblem into language form, we find that PLMs exhibit a striking capacity for\nzero-shot relational reasoning, exceeding human performance and nearing\nsupervised vision-based methods. We explore different encodings that vary the\nlevel of abstraction over task features, finding that higher-level abstractions\nfurther strengthen PLMs' analogical reasoning. Our detailed analysis reveals\ninsights on the role of model complexity, in-context learning, and prior\nknowledge in solving RPM tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoyang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_R/0/1/0/all/0/1\">Richard L. Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18486","description":"<p>The development of large language models (LLMs) such as ChatGPT has brought a\nlot of attention recently. However, their evaluation in the benchmark academic\ndatasets remains under-explored due to the difficulty of evaluating the\ngenerative outputs produced by this model against the ground truth. In this\npaper, we aim to present a thorough evaluation of ChatGPT's performance on\ndiverse academic datasets, covering tasks like question-answering, text\nsummarization, code generation, commonsense reasoning, mathematical\nproblem-solving, machine translation, bias detection, and ethical\nconsiderations. Specifically, we evaluate ChatGPT across 140 tasks and analyze\n255K responses it generates in these datasets. This makes our work the largest\nevaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate\nthe strengths and weaknesses of ChatGPT in various tasks and provide insights\nfor future research using LLMs. We also report a new emergent ability to follow\nmulti-query instructions that we mostly found in ChatGPT and other\ninstruction-tuned models. Our extensive evaluation shows that even though\nChatGPT is capable of performing a wide variety of tasks, and may obtain\nimpressive performance in several benchmark datasets, it is still far from\nachieving the ability to reliably solve many challenging tasks. By providing a\nthorough assessment of ChatGPT's performance across diverse NLP tasks, this\npaper sets the stage for a targeted deployment of ChatGPT-like LLMs in\nreal-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mizanur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhuiyan_M/0/1/0/all/0/1\">Md Amran Hossen Bhuiyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimmy Xiangji Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction. (arXiv:2306.00418v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00418","description":"<p>Recently, aspect sentiment quad prediction has received widespread attention\nin the field of aspect-based sentiment analysis. Existing studies extract\nquadruplets via pre-trained generative language models to paraphrase the\noriginal sentence into a templated target sequence. However, previous works\nonly focus on what to generate but ignore what not to generate. We argue that\nconsidering the negative samples also leads to potential benefits. In this\nwork, we propose a template-agnostic method to control the token-level\ngeneration, which boosts original learning and reduces mistakes simultaneously.\nSpecifically, we introduce Monte Carlo dropout to understand the built-in\nuncertainty of pre-trained language models, acquiring the noises and errors. We\nfurther propose marginalized unlikelihood learning to suppress the\nuncertainty-aware mistake tokens. Finally, we introduce minimization entropy to\nbalance the effects of marginalized unlikelihood learning. Extensive\nexperiments on four public datasets demonstrate the effectiveness of our\napproach on various generation templates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Mengting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yinhao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yike Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shiwan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT. (arXiv:2306.01393v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01393","description":"<p>Subword tokenization is the de facto standard for tokenization in neural\nlanguage models and machine translation systems. Three advantages are\nfrequently cited in favor of subwords: shorter encoding of frequent tokens,\ncompositionality of subwords, and ability to deal with unknown words. As their\nrelative importance is not entirely clear yet, we propose a tokenization\napproach that enables us to separate frequency (the first advantage) from\ncompositionality. The approach uses Huffman coding to tokenize words, by order\nof frequency, using a fixed amount of symbols. Experiments with CS-DE, EN-FR\nand EN-DE NMT show that frequency alone accounts for 90%-95% of the scores\nreached by BPE, hence compositionality has less importance than previously\nthought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolleb_B/0/1/0/all/0/1\">Benoist Wolleb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_R/0/1/0/all/0/1\">Romain Silvestri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vernikos_G/0/1/0/all/0/1\">Giorgos Vernikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolamic_L/0/1/0/all/0/1\">Ljiljana Dolamic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_Belis_A/0/1/0/all/0/1\">Andrei Popescu-Belis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}