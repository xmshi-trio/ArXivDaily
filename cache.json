{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-23T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Sentiment Analysis for Measuring Hope and Fear from Reddit Posts During the 2022 Russo-Ukrainian Conflict. (arXiv:2301.08347v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08347","description":"<p>This paper proposes a novel lexicon-based unsupervised sentimental analysis\nmethod to measure the $``\\textit{hope}\"$ and $``\\textit{fear}\"$ for the 2022\nUkrainian-Russian Conflict. $\\textit{Reddit.com}$ is utilised as the main\nsource of human reactions to daily events during nearly the first three months\nof the conflict. The top 50 $``hot\"$ posts of six different subreddits about\nUkraine and news (Ukraine, worldnews, Ukraina, UkrainianConflict,\nUkraineWarVideoReport, UkraineWarReports) and their relative comments are\nscraped and a data set is created. On this corpus, multiple analyses such as\n(1) public interest, (2) hope/fear score, (3) stock price interaction are\nemployed. We promote using a dictionary approach, which scores the hopefulness\nof every submitted user post. The Latent Dirichlet Allocation (LDA) algorithm\nof topic modelling is also utilised to understand the main issues raised by\nusers and what are the key talking points. Experimental analysis shows that the\nhope strongly decreases after the symbolic and strategic losses of Azovstal\n(Mariupol) and Severodonetsk. Spikes in hope/fear, both positives and\nnegatives, are present after important battles, but also some non-military\nevents, such as Eurovision and football games.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guerra_A/0/1/0/all/0/1\">Alessio Guerra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karakus_O/0/1/0/all/0/1\">Oktay Karaku&#x15f;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation for Accessible Multi-Language Text Analysis. (arXiv:2301.08416v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08416","description":"<p>English is the international standard of social research, but scholars are\nincreasingly conscious of their responsibility to meet the need for scholarly\ninsight into communication processes globally. This tension is as true in\ncomputational methods as any other area, with revolutionary advances in the\ntools for English language texts leaving most other languages far behind. In\nthis paper, we aim to leverage those very advances to demonstrate that\nmulti-language analysis is currently accessible to all computational scholars.\nWe show that English-trained measures computed after translation to English\nhave adequate-to-excellent accuracy compared to source-language measures\ncomputed on original texts. We show this for three major analytics -- sentiment\nanalysis, topic analysis, and word embeddings -- over 16 languages, including\nSpanish, Chinese, Hindi, and Arabic. We validate this claim by comparing\npredictions on original language tweets and their backtranslations: double\ntranslations from their source language to English and back to the source\nlanguage. Overall, our results suggest that Google Translate, a simple and\nwidely accessible tool, is effective in preserving semantic content across\nlanguages and methods. Modern machine translation can thus help computational\nscholars make more inclusive and general claims about human communication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chew_E/0/1/0/all/0/1\">Edward W. Chew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weisman_W/0/1/0/all/0/1\">William D. Weisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jingying Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frey_S/0/1/0/all/0/1\">Seth Frey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Which Features are Learned by CodeBert: An Empirical Study of the BERT-based Source Code Representation Learning. (arXiv:2301.08427v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08427","description":"<p>The Bidirectional Encoder Representations from Transformers (BERT) were\nproposed in the natural language process (NLP) and shows promising results.\nRecently researchers applied the BERT to source-code representation learning\nand reported some good news on several downstream tasks. However, in this\npaper, we illustrated that current methods cannot effectively understand the\nlogic of source codes. The representation of source code heavily relies on the\nprogrammer-defined variable and function names. We design and implement a set\nof experiments to demonstrate our conjecture and provide some insights for\nfuture works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Chen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Agnostic Data-Driven Inverse Text Normalization. (arXiv:2301.08506v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08506","description":"<p>With the emergence of automatic speech recognition (ASR) models, converting\nthe spoken form text (from ASR) to the written form is in urgent need. This\ninverse text normalization (ITN) problem attracts the attention of researchers\nfrom various fields. Recently, several works show that data-driven ITN methods\ncan output high-quality written form text. Due to the scarcity of labeled\nspoken-written datasets, the studies on non-English data-driven ITN are quite\nlimited. In this work, we propose a language-agnostic data-driven ITN framework\nto fill this gap. Specifically, we leverage the data augmentation in\nconjunction with neural machine translated data for low resource languages.\nMoreover, we design an evaluation method for language agnostic ITN model when\nonly English data is available. Our empirical evaluation shows this language\nagnostic modeling approach is effective for low resource languages while\npreserving the performance for high resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Szu-Jui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1\">Debjyoti Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yutong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_P/0/1/0/all/0/1\">Peng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuedong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transforming Unstructured Text into Data with Context Rule Assisted Machine Learning (CRAML). (arXiv:2301.08549v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08549","description":"<p>We describe a method and new no-code software tools enabling domain experts\nto build custom structured, labeled datasets from the unstructured text of\ndocuments and build niche machine learning text classification models traceable\nto expert-written rules. The Context Rule Assisted Machine Learning (CRAML)\nmethod allows accurate and reproducible labeling of massive volumes of\nunstructured text. CRAML enables domain experts to access uncommon constructs\nburied within a document corpus, and avoids limitations of current\ncomputational approaches that often lack context, transparency, and\ninterpetability. In this research methods paper, we present three use cases for\nCRAML: we analyze recent management literature that draws from text data,\ndescribe and release new machine learning models from an analysis of\nproprietary job advertisement text, and present findings of social and economic\ninterest from a public corpus of franchise documents. CRAML produces\ndocument-level coded tabular datasets that can be used for quantitative\nacademic research, and allows qualitative researchers to scale niche\nclassification schemes over massive text data. CRAML is a low-resource,\nflexible, and scalable methodology for building training data for supervised\nML. We make available as open-source resources: the software, job advertisement\ntext classifiers, a novel corpus of franchise documents, and a fully replicable\nstart-to-finish trained example in the context of no poach clauses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meisenbacher_S/0/1/0/all/0/1\">Stephen Meisenbacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norlander_P/0/1/0/all/0/1\">Peter Norlander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences. (arXiv:2301.08571v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08571","description":"<p>Current work on image-based story generation suffers from the fact that the\nexisting image sequence collections do not have coherent plots behind them. We\nimprove visual story generation by producing a new image-grounded dataset,\nVisual Writing Prompts (VWP). VWP contains almost 2K selected sequences of\nmovie shots, each including 5-10 images. The image sequences are aligned with a\ntotal of 12K stories which were collected via crowdsourcing given the image\nsequences and a set of grounded characters from the corresponding image\nsequence. Our new image sequence collection and filtering process has allowed\nus to obtain stories that are more coherent and have more narrativity compared\nto previous work. We also propose a character-based story generation model\ndriven by coherence as a strong baseline. Evaluations show that our generated\nstories are more coherent, visually grounded, and have more narrativity than\nstories generated with the current state-of-the-art model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xudong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayeed_A/0/1/0/all/0/1\">Asad Sayeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehra_K/0/1/0/all/0/1\">Khushboo Mehra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1\">Vera Demberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Modeling Human Personality: The Dexter Machine. (arXiv:2301.08606v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08606","description":"<p>Modeling human personality is important for several AI challenges, from the\nengineering of artificial psychotherapists to the design of persona bots.\nHowever, the field of computational personality analysis heavily relies on\nlabeled data, which may be expensive, difficult or impossible to get. This\nproblem is amplified when dealing with rare personality types or disorders\n(e.g., the anti-social psychopathic personality disorder). In this context, we\ndeveloped a text-based data augmentation approach for human personality\n(PEDANT). PEDANT doesn't rely on the common type of labeled data but on the\ngenerative pre-trained model (GPT) combined with domain expertise. Testing the\nmethodology on three different datasets, provides results that support the\nquality of the generated data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neuman_Y/0/1/0/all/0/1\">Yair Neuman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozhukhov_V/0/1/0/all/0/1\">Vladyslav Kozhukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilenchik_D/0/1/0/all/0/1\">Dan Vilenchik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reversing The Twenty Questions Game. (arXiv:2301.08718v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08718","description":"<p>Twenty questions is a widely popular verbal game. In recent years, many\ncomputerized versions of this game have been developed in which a user thinks\nof an entity and a computer attempts to guess this entity by asking a series of\nboolean-type (yes/no) questions. In this research, we aim to reverse this game\nby making the computer choose an entity at random. The human aims to guess this\nentity by quizzing the computer with natural language queries which the\ncomputer will then attempt to parse using a boolean question answering model.\nThe game ends when the human is successfully able to guess the entity of the\ncomputer's choice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parikh_P/0/1/0/all/0/1\">Parth Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anisha Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Batch Prompting: Efficient Inference with Large Language Model APIs. (arXiv:2301.08721v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08721","description":"<p>Performing inference on hundreds of thousands of samples with large language\nmodels (LLMs) can be computationally and financially costly. We propose batch\nprompting, a simple alternative prompting approach that enables the LLM to run\ninference in batches, instead of one sample at a time. Our method reduces both\ntoken and time costs while retaining downstream performance. We theoretically\ndemonstrate that under a few-shot in-context learning setting, the inference\ncosts decrease almost inverse linearly with the number of samples in each\nbatch. We extensively validate the effectiveness of batch prompting on ten\ndatasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch\nprompting significantly~(up to $5\\times$ with six samples in batch) reduces the\nLLM (Codex) inference token and time costs while achieving better or comparable\nperformance. Our analysis shows that the number of samples in each batch and\nthe complexity of tasks affect its performance. Further, batch prompting can be\napplied across different LLMs and reasoning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhoujun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Peanuts Fall in Love with Distributional Semantics?. (arXiv:2301.08731v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08731","description":"<p>The context in which a sentence appears can drastically alter our\nexpectations about upcoming words - for example, following a short story\ninvolving an anthropomorphic peanut, experimental participants are more likely\nto expect the sentence 'the peanut was in love' than 'the peanut was salted',\nas indexed by N400 amplitude (Nieuwland &amp; van Berkum, 2006). This rapid and\ndynamic updating of comprehenders' expectations about the kind of events that a\npeanut may take part in based on context has been explained using the construct\nof Situation Models - updated mental representations of key elements of an\nevent under discussion, in this case, the peanut protagonist. However, recent\nwork showing that N400 amplitude can be predicted based on distributional\ninformation alone raises the question whether situation models are in fact\nnecessary for the kinds of contextual effects observed in previous work. To\ninvestigate this question, we attempt to model the results of Nieuwland and van\nBerkum (2006) using six computational language models and three sets of word\nvectors, none of which have explicit situation models or semantic grounding. We\nfind that the effect found by Nieuwland and van Berkum (2006) can be fully\nmodeled by two language models and two sets of word vectors, with others\nshowing a reduced effect. Thus, at least some processing effects normally\nexplained through situation models may not in fact require explicit situation\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James A. Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coulson_S/0/1/0/all/0/1\">Seana Coulson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Dialogue Breakdown Detection with Semi-Supervised Learning. (arXiv:2011.00136v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.00136","description":"<p>Building user trust in dialogue agents requires smooth and consistent\ndialogue exchanges. However, agents can easily lose conversational context and\ngenerate irrelevant utterances. These situations are called dialogue breakdown,\nwhere agent utterances prevent users from continuing the conversation. Building\nsystems to detect dialogue breakdown allows agents to recover appropriately or\navoid breakdown entirely. In this paper we investigate the use of\nsemi-supervised learning methods to improve dialogue breakdown detection,\nincluding continued pre-training on the Reddit dataset and a manifold-based\ndata augmentation method. We demonstrate the effectiveness of these methods on\nthe Dialogue Breakdown Detection Challenge (DBDC) English shared task. Our\nsubmissions to the 2020 DBDC5 shared task place first, beating baselines and\nother submissions by over 12\\% accuracy. In ablations on DBDC4 data from 2019,\nour semi-supervised learning methods improve the performance of a baseline BERT\nmodel by 2\\% accuracy. These methods are applicable generally to any dialogue\ntask and provide a simple way to improve model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ng_N/0/1/0/all/0/1\">Nathan Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1\">Marzyeh Ghassemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thangarajan_N/0/1/0/all/0/1\">Narendran Thangarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jiacheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qi Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Data Augmentation Methods for Low-Resource Maltese ASR. (arXiv:2111.07793v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07793","description":"<p>Recent years have seen an increased interest in the computational speech\nprocessing of Maltese, but resources remain sparse. In this paper, we consider\ndata augmentation techniques for improving speech recognition for low-resource\nlanguages, focusing on Maltese as a test case. We consider three different\ntypes of data augmentation: unsupervised training, multilingual training and\nthe use of synthesized speech as training data. The goal is to determine which\nof these techniques, or combination of them, is the most effective to improve\nspeech recognition for languages where the starting point is a small corpus of\napproximately 7 hours of transcribed speech. Our results show that combining\nthe data augmentation techniques studied here lead us to an absolute WER\nimprovement of 15% without the use of a language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DeMarco_A/0/1/0/all/0/1\">Andrea DeMarco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mena_C/0/1/0/all/0/1\">Carlos Mena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borg_C/0/1/0/all/0/1\">Claudia Borg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Aiden Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plas_L/0/1/0/all/0/1\">Lonneke van der Plas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Gap Between Indexing and Retrieval for Differentiable Search Index with Query Generation. (arXiv:2206.10128v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2206.10128","description":"<p>The Differentiable Search Index (DSI) is an emerging paradigm for information\nretrieval. Unlike traditional retrieval architectures where index and retrieval\nare two different and separate components, DSI uses a single transformer model\nto perform both indexing and retrieval.\n</p>\n<p>In this paper, we identify and tackle an important issue of current DSI\nmodels: the data distribution mismatch that occurs between the DSI indexing and\nretrieval processes. Specifically, we argue that, at indexing, current DSI\nmethods learn to build connections between the text of long documents and the\nidentifier of the documents, but then retrieval of document identifiers is\nbased on queries that are commonly much shorter than the indexed documents.\nThis problem is further exacerbated when using DSI for cross-lingual retrieval,\nwhere document text and query text are in different languages.\n</p>\n<p>To address this fundamental problem of current DSI models, we propose a\nsimple yet effective indexing framework for DSI, called DSI-QG. When indexing,\nDSI-QG represents documents with a number of potentially relevant queries\ngenerated by a query generation model and re-ranked and filtered by a\ncross-encoder ranker. The presence of these queries at indexing allows the DSI\nmodels to connect a document identifier to a set of queries, hence mitigating\ndata distribution mismatches present between the indexing and the retrieval\nphases. Empirical results on popular mono-lingual and cross-lingual passage\nretrieval datasets show that DSI-QG significantly outperforms the original DSI\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Shengyao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Houxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ILLUME: Rationalizing Vision-Language Models through Human Interactions. (arXiv:2208.08241v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2208.08241","description":"<p>Bootstrapping from pre-trained language models has been proven to be an\nefficient approach for building vision-language models (VLM) for tasks such as\nimage captioning or visual question answering. However, outputs of these models\nrarely align with user's rationales for specific answers. In order to improve\nthis alignment and reinforce commonsense reasons, we propose a tuning paradigm\nbased on human interactions with machine generated data. Our ILLUME executes\nthe following loop: Given an image-question-answer prompt, the VLM samples\nmultiple candidate rationales, and a human critic provides minimal feedback via\npreference selection, used for fine-tuning. This loop increases the training\ndata and gradually carves out the VLM's rationalization capabilities that are\naligned with human intend. Our exhaustive experiments demonstrate that ILLUME\nis competitive with standard supervised fine-tuning while using significantly\nfewer training data and only requiring minimal feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1\">Manuel Brack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deiseroth_B/0/1/0/all/0/1\">Bj&#xf6;rn Deiseroth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state. (arXiv:2211.10247v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.10247","description":"<p>Extracting summaries from long documents can be regarded as sentence\nclassification using the structural information of the documents. How to use\nsuch structural information to summarize a document is challenging. In this\npaper, we propose GoSum, a novel graph and reinforcement learning based\nextractive model for long-paper summarization. In particular, GoSum encodes\nsentence states in reinforcement learning by building a heterogeneous graph for\neach input document at different discourse levels. An edge in the graph\nreflects the discourse hierarchy of a document for restraining the semantic\ndrifts across section boundaries. We evaluate GoSum on two datasets of\nscientific articles summarization: PubMed and arXiv. The experimental results\nhave demonstrated that GoSum achieve state-of-the-art results compared with\nstrong baselines of both extractive and abstractive models. The ablation\nstudies further validate that the performance of our GoSum benefits from the\nuse of discourse information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Junyi Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaodi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shanfeng Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards continually learning new languages. (arXiv:2211.11703v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11703","description":"<p>Multilingual speech recognition with neural networks is often implemented\nwith batch-learning, when all of the languages are available before training.\nAn ability to add new languages after the prior training sessions can be\neconomically beneficial, but the main challenge is catastrophic forgetting. In\nthis work, we combine the qualities of weight factorization and elastic weight\nconsolidation in order to counter catastrophic forgetting and facilitate\nlearning new languages quickly. Such combination allowed us to eliminate\ncatastrophic forgetting while still achieving performance for the new languages\ncomparable with having all languages at once, in experiments of learning from\nan initial 10 languages to achieve 26 languages without catastrophic forgetting\nand a reasonable performance compared to training all languages from scratch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Ngoc-Quan Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHAPTER: Exploiting Convolutional Neural Network Adapters for Self-supervised Speech Models. (arXiv:2212.01282v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2212.01282","description":"<p>Self-supervised learning (SSL) is a powerful technique for learning\nrepresentations from unlabeled data. Transformer based models such as HuBERT,\nwhich consist a feature extractor and transformer layers, are leading the field\nin the speech domain. SSL models are fine-tuned on a wide range of downstream\ntasks, which involves re-training the majority of the model for each task.\nPrevious studies have introduced applying adapters, which are small lightweight\nmodules commonly used in Natural Language Processing (NLP) to adapt pre-trained\nmodels to new tasks. However, such efficient tuning techniques only provide\nadaptation at the transformer layer, but failed to perform adaptation at the\nfeature extractor. In this paper, we propose CHAPTER, an efficient tuning\nmethod specifically designed for SSL speech model, by applying CNN adapters at\nthe feature extractor. Using this method, we can only fine-tune fewer than 5%\nof parameters per task compared to fully fine-tuning and achieve better and\nmore stable performance. We empirically found that adding CNN adapters to the\nfeature extractor can help the adaptation on emotion and speaker tasks. For\ninstance, the accuracy of SID is improved from 87.71 to 91.56, and the accuracy\nof ER is improved by 5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zih-Ching Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sung_Y/0/1/0/all/0/1\">Yu-Shun Sung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}