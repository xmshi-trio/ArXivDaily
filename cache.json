{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ChatGPT is a Remarkable Tool -- For Experts. (arXiv:2306.03102v1 [cs.HC])","link":"http://arxiv.org/abs/2306.03102","description":"<p>This paper investigates the capabilities of ChatGPT as an automated assistant\nin diverse domains, including scientific writing, mathematics, education,\nprogramming, and healthcare. We explore the potential of ChatGPT to enhance\nproductivity, streamline problem-solving processes, and improve writing style.\nFurthermore, we highlight the potential risks associated with excessive\nreliance on ChatGPT in these fields. These limitations encompass factors like\nincorrect and fictitious responses, inaccuracies in code, limited logical\nreasoning abilities, overconfidence, and critical ethical concerns of\ncopyrights and privacy violation. We outline areas and objectives where ChatGPT\nproves beneficial, applications where it should be used judiciously, and\nscenarios where its reliability may be limited. In light of observed\nlimitations, and given that the tool's fundamental errors may pose a special\nchallenge for non-experts, ChatGPT should be used with a strategic methodology.\nBy drawing from comprehensive experimental studies, we offer methods and flow\ncharts for effectively using ChatGPT. Our recommendations emphasize iterative\ninteraction with ChatGPT and independent verification of its outputs.\nConsidering the importance of utilizing ChatGPT judiciously and with expertise,\nwe recommend its usage for experts who are well-versed in the respective\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azaria_A/0/1/0/all/0/1\">Amos Azaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azoulay_R/0/1/0/all/0/1\">Rina Azoulay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reches_S/0/1/0/all/0/1\">Shulamit Reches</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sampling and Ranking for Digital Ink Generation on a tight computational budget. (arXiv:2306.03103v1 [cs.HC])","link":"http://arxiv.org/abs/2306.03103","description":"<p>Digital ink (online handwriting) generation has a number of potential\napplications for creating user-visible content, such as handwriting\nautocompletion, spelling correction, and beautification. Writing is personal\nand usually the processing is done on-device. Ink generative models thus need\nto produce high quality content quickly, in a resource constrained environment.\n</p>\n<p>In this work, we study ways to maximize the quality of the output of a\ntrained digital ink generative model, while staying within an inference time\nbudget. We use and compare the effect of multiple sampling and ranking\ntechniques, in the first ablation study of its kind in the digital ink domain.\n</p>\n<p>We confirm our findings on multiple datasets - writing in English and\nVietnamese, as well as mathematical formulas - using two model types and two\ncommon ink data representations. In all combinations, we report a meaningful\nimprovement in the recognizability of the synthetic inks, in some cases more\nthan halving the character error rate metric, and describe a way to select the\noptimal combination of sampling and ranking techniques for any given\ncomputational budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afonin_A/0/1/0/all/0/1\">Andrei Afonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksai_A/0/1/0/all/0/1\">Andrii Maksai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofeev_A/0/1/0/all/0/1\">Aleksandr Timofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training. (arXiv:2306.03166v1 [cs.IR])","link":"http://arxiv.org/abs/2306.03166","description":"<p>Dense retrievers have achieved impressive performance, but their demand for\nabundant training data limits their application scenarios. Contrastive\npre-training, which constructs pseudo-positive examples from unlabeled data,\nhas shown great potential to solve this problem. However, the pseudo-positive\nexamples crafted by data augmentations can be irrelevant. To this end, we\npropose relevance-aware contrastive learning. It takes the intermediate-trained\nmodel itself as an imperfect oracle to estimate the relevance of positive pairs\nand adaptively weighs the contrastive loss of different pairs according to the\nestimated relevance. Our method consistently improves the SOTA unsupervised\nContriever model on the BEIR and open-domain QA retrieval benchmarks. Further\nexploration shows that our method can not only beat BM25 after further\npre-training on the target corpus but also serves as a good few-shot learner.\nOur code is publicly available at https://github.com/Yibin-Lei/ReContriever.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yibin Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_C/0/1/0/all/0/1\">Changtong Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Composition and Deformance: Measuring Imageability with a Text-to-Image Model. (arXiv:2306.03168v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03168","description":"<p>Although psycholinguists and psychologists have long studied the tendency of\nlinguistic strings to evoke mental images in hearers or readers, most\ncomputational studies have applied this concept of imageability only to\nisolated words. Using recent developments in text-to-image generation models,\nsuch as DALLE mini, we propose computational methods that use generated images\nto measure the imageability of both single English words and connected text. We\nsample text prompts for image generation from three corpora: human-generated\nimage captions, news article sentences, and poem lines. We subject these\nprompts to different deformances to examine the model's ability to detect\nchanges in imageability caused by compositional change. We find high\ncorrelation between the proposed computational measures of imageability and\nhuman judgments of individual words. We also find the proposed measures more\nconsistently respond to changes in compositionality than baseline approaches.\nWe discuss possible effects of model training and implications for the study of\ncompositionality in text-to-image models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Si Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_D/0/1/0/all/0/1\">David A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Easy-to-Read in Germany: A Survey on its Current State and Available Resources. (arXiv:2306.03189v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03189","description":"<p>Easy-to-Read Language (E2R) is a controlled language variant that makes any\nwritten text more accessible through the use of clear, direct and simple\nlanguage. It is mainly aimed at people with cognitive or intellectual\ndisabilities, among other target users. Plain Language (PL), on the other hand,\nis a variant of a given language, which aims to promote the use of simple\nlanguage to communicate information. German counts with Leichte Sprache (LS),\nits version of E2R, and Einfache Sprache (ES), its version of PL. In recent\nyears, important developments have been conducted in the field of LS. This\npaper offers an updated overview of the existing Natural Language Processing\n(NLP) tools and resources for LS. Besides, it also aims to set out the\nsituation with regard to LS and ES in Germany.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madina_M/0/1/0/all/0/1\">Margot Madina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Dios_I/0/1/0/all/0/1\">Itziar Gonzalez-Dios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegel_M/0/1/0/all/0/1\">Melanie Siegel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoScrum: Automating Project Planning Using Large Language Models. (arXiv:2306.03197v1 [cs.AI])","link":"http://arxiv.org/abs/2306.03197","description":"<p>Recent advancements in the field of large language models have made it\npossible to use language models for advanced reasoning. In this paper we\nleverage this ability for designing complex project plans based only on knowing\nthe current state and the desired state. Two approaches are demonstrated - a\nscrum based approach and a shortcut plan approach. The scrum based approach\nexecutes an automated process of requirements gathering, user story mapping,\nfeature identification, task decomposition and finally generates questions and\nsearch terms for seeking out domain specific information to assist with task\ncompletion. The shortcut approach looks at most recent snapshot of the current\nand desired state and generates the next most reasonable task to do in order to\nget to the desired state as quickly as possible. In this paper we automate\neverything using a novel concept of \"Language Programs\". These are programs\nwritten in natural language designed to process input data through the language\nmodel. Guidance language is used for all LLM programs. All demo source code for\nthis paper is available at https://github.com/autoscrum/autoscrum\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_M/0/1/0/all/0/1\">Martin Schroder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Static Evaluation of Code Completion by Large Language Models. (arXiv:2306.03203v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03203","description":"<p>Large language models trained on code have shown great potential to increase\nproductivity of software developers. Several execution-based benchmarks have\nbeen proposed to evaluate functional correctness of model-generated code on\nsimple programming problems. Nevertheless, it is expensive to perform the same\nevaluation on complex real-world projects considering the execution cost. On\nthe contrary, static analysis tools such as linters, which can detect errors\nwithout running the program, haven't been well explored for evaluating code\ngeneration models. In this work, we propose a static evaluation framework to\nquantify static errors in Python code completions, by leveraging Abstract\nSyntax Trees. Compared with execution-based evaluation, our method is not only\nmore efficient, but also applicable to code in the wild. For experiments, we\ncollect code context from open source repos to generate one million function\nbodies using public models. Our static analysis reveals that Undefined Name and\nUnused Variable are the most common errors among others made by language\nmodels. Through extensive studies, we also show the impact of sampling\ntemperature, model size, and context on static errors in code completions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Hantian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuchen Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowski_R/0/1/0/all/0/1\">Rob Kwiatkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanathan_M/0/1/0/all/0/1\">Murali Krishna Ramanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Baishakhi Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1\">Parminder Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Sudipta Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification Tasks. (arXiv:2306.03208v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03208","description":"<p>Finetuning large language models inflates the costs of NLU applications and\nremains the bottleneck of development cycles. Recent works in computer vision\nuse data pruning to reduce training time. Pruned data selection with static\nmethods is based on a score calculated for each training example prior to\nfinetuning, which involves important computational overhead. Moreover, the\nscore may not necessarily be representative of sample importance throughout the\nentire training duration. We propose to address these issues with a refined\nversion of dynamic data pruning, a curriculum which periodically scores and\ndiscards unimportant examples during finetuning. Our method leverages an EL2N\nmetric that we extend to the joint intent and slot classification task, and an\ninitial finetuning phase on the full train set. Our results on the GLUE\nbenchmark and four joint NLU datasets show a better time-accuracy trade-off\ncompared to static methods. Our method preserves full accuracy while training\non 50% of the data points and reduces computational times by up to 41%. If we\ntolerate instead a minor drop of accuracy of 1%, we can prune 80% of the\ntraining examples for a reduction in finetuning time reaching 66%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Attendu_J/0/1/0/all/0/1\">Jean-Michel Attendu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corbeil_J/0/1/0/all/0/1\">Jean-Philippe Corbeil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Effectiveness of Early Weight Averaging for Training Large Language Models. (arXiv:2306.03241v1 [cs.LG])","link":"http://arxiv.org/abs/2306.03241","description":"<p>Training LLMs is expensive, and recent evidence indicates training all the\nway to convergence is inefficient. In this paper, we investigate the ability of\na simple idea, checkpoint averaging along the trajectory of a training run to\nimprove the quality of models before they have converged. This approach incurs\nno extra cost during training or inference. Specifically, we analyze the\ntraining trajectories of Pythia LLMs with 1 to 12 billion parameters and\ndemonstrate that, particularly during the early to mid stages of training, this\nidea accelerates convergence and improves both test and zero-shot\ngeneralization. Loss spikes are a well recognized problem in LLM training; in\nour analysis we encountered two instances of this in the underlying\ntrajectories, and both instances were mitigated by our averaging. For a 6.9B\nparameter LLM, for example, our early weight averaging recipe can save upto\n4200 hours of GPU time, which corresponds to significant savings in cloud\ncompute costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Sunny Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1\">Jean Kaddour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1\">Sujay Sanghavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation. (arXiv:2306.03264v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03264","description":"<p>Instruction-tuned generative Large language models (LLMs) like ChatGPT and\nBloomz possess excellent generalization abilities, but they face limitations in\nunderstanding radiology reports, particularly in the task of generating the\nIMPRESSIONS section from the FINDINGS section. They tend to generate either\nverbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to\nmedical text data during training. We present a system which leverages\nlarge-scale medical text data for domain-adaptive pre-training of\ninstruction-tuned LLMs to enhance its medical knowledge and performance on\nspecific medical tasks. We show that this system performs better in a zero-shot\nsetting than a number of pretrain-and-finetune adaptation methods on the\nIMPRESSIONS generation task, and ranks 1st among participating systems in Task\n1B: Radiology Report Summarization at the BioNLP 2023 workshop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karn_S/0/1/0/all/0/1\">Sanjeev Kumar Karn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Rikhiya Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+P_K/0/1/0/all/0/1\">Kusuma P</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farri_O/0/1/0/all/0/1\">Oladimeji Farri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stack Over-Flowing with Results: The Case for Domain-Specific Pre-Training Over One-Size-Fits-All Models. (arXiv:2306.03268v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03268","description":"<p>Large pre-trained neural language models have brought immense progress to\nboth NLP and software engineering. Models in OpenAI's GPT series now dwarf\nGoogle's BERT and Meta's RoBERTa, which previously set new benchmarks on a wide\nrange of NLP applications. These models are trained on massive corpora of\nheterogeneous data from web crawls, which enables them to learn general\nlanguage patterns and semantic relationships. However, the largest models are\nboth expensive to train and deploy and are often closed-source, so we lack\naccess to their data and design decisions. We argue that this trend towards\nlarge, general-purpose models should be complemented with single-purpose, more\nmodestly sized pre-trained models. In this work, we take StackOverflow (SO) as\na domain example in which large volumes of rich aligned code and text data is\navailable. We adopt standard practices for pre-training large language models,\nincluding using a very large context size (2,048 tokens), batch size (0.5M\ntokens) and training set (27B tokens), coupled with a powerful toolkit\n(Megatron-LM), to train two models: SOBertBase, with 109M parameters, and\nSOBertLarge with 762M parameters, at a budget of just $\\$187$ and $\\$800$ each.\nWe compare the performance of our models with both the previous SOTA model\ntrained on SO data exclusively as well general-purpose BERT models and OpenAI's\nChatGPT on four SO-specific downstream tasks - question quality prediction,\nclosed question prediction, named entity recognition and obsoletion prediction\n(a new task we introduce). Not only do our models consistently outperform all\nbaselines, the smaller model is often sufficient for strong results. Both\nmodels are released to the public. These results demonstrate that pre-training\nboth extensively and properly on in-domain data can yield a powerful and\naffordable alternative to leveraging closed-source general-purpose models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_M/0/1/0/all/0/1\">Manisha Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1\">Vincent J. Hellendoorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Scalable and Adaptive System to Infer the Industry Sectors of Companies: Prompt + Model Tuning of Generative Language Models. (arXiv:2306.03313v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03313","description":"<p>The Private Equity (PE) firms operate investment funds by acquiring and\nmanaging companies to achieve a high return upon selling. Many PE funds are\nthematic, meaning investment professionals aim to identify trends by covering\nas many industry sectors as possible, and picking promising companies within\nthese sectors. So, inferring sectors for companies is critical to the success\nof thematic PE funds. In this work, we standardize the sector framework and\ndiscuss the typical challenges; we then introduce our sector inference system\naddressing these challenges. Specifically, our system is built on a\nmedium-sized generative language model, finetuned with a prompt + model tuning\nprocedure. The deployed model demonstrates a superior performance than the\ncommon baselines. The system has been serving many PE professionals for over a\nyear, showing great scalability to data volume and adaptability to any change\nin sector framework and/or annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Lele Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehrenheim_V/0/1/0/all/0/1\">Vilhelm von Ehrenheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berghult_A/0/1/0/all/0/1\">Astrid Berghult</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henje_C/0/1/0/all/0/1\">Cecilia Henje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stahl_R/0/1/0/all/0/1\">Richard Anselmo Stahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wandborg_J/0/1/0/all/0/1\">Joar Wandborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stan_S/0/1/0/all/0/1\">Sebastian Stan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catovic_A/0/1/0/all/0/1\">Armin Catovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferm_E/0/1/0/all/0/1\">Erik Ferm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingelhag_H/0/1/0/all/0/1\">Hannes Ingelhag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few Shot Rationale Generation using Self-Training with Dual Teachers. (arXiv:2306.03315v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03315","description":"<p>Self-rationalizing models that also generate a free-text explanation for\ntheir predicted labels are an important tool to build trustworthy AI\napplications. Since generating explanations for annotated labels is a laborious\nand costly pro cess, recent models rely on large pretrained language models\n(PLMs) as their backbone and few-shot learning. In this work we explore a\nself-training approach leveraging both labeled and unlabeled data to further\nimprove few-shot models, under the assumption that neither human written\nrationales nor annotated task labels are available at scale. We introduce a\nnovel dual-teacher learning framework, which learns two specialized teacher\nmodels for task prediction and rationalization using self-training and distills\ntheir knowledge into a multi-tasking student model that can jointly generate\nthe task label and rationale. Furthermore, we formulate a new loss function,\nMasked Label Regularization (MLR) which promotes explanations to be strongly\nconditioned on predicted labels. Evaluation on three public datasets\ndemonstrate that the proposed methods are effective in modeling task labels and\ngenerating faithful rationales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Veerubhotla_A/0/1/0/all/0/1\">Aditya Srikanth Veerubhotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poddar_L/0/1/0/all/0/1\">Lahari Poddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szarvas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Szarvas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eswaran_S/0/1/0/all/0/1\">Sharanya Eswaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoSiNES: Contrastive Siamese Network for Entity Standardization. (arXiv:2306.03316v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03316","description":"<p>Entity standardization maps noisy mentions from free-form text to standard\nentities in a knowledge base. The unique challenge of this task relative to\nother entity-related tasks is the lack of surrounding context and numerous\nvariations in the surface form of the mentions, especially when it comes to\ngeneralization across domains where labeled data is scarce. Previous research\nmostly focuses on developing models either heavily relying on context, or\ndedicated solely to a specific domain. In contrast, we propose CoSiNES, a\ngeneric and adaptable framework with Contrastive Siamese Network for Entity\nStandardization that effectively adapts a pretrained language model to capture\nthe syntax and semantics of the entities in a new domain.\n</p>\n<p>We construct a new dataset in the technology domain, which contains 640\ntechnical stack entities and 6,412 mentions collected from industrial content\nmanagement systems. We demonstrate that CoSiNES yields higher accuracy and\nfaster runtime than baselines derived from leading methods in this domain.\nCoSiNES also achieves competitive performance in four standard datasets from\nthe chemistry, medicine, and biomedical domains, demonstrating its cross-domain\napplicability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiaqing Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merler_M/0/1/0/all/0/1\">Michele Merler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Mihir Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavuluri_R/0/1/0/all/0/1\">Raju Pavuluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Munindar P. Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vukovic_M/0/1/0/all/0/1\">Maja Vukovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])","link":"http://arxiv.org/abs/2306.03341","description":"<p>We introduce Inference-Time Intervention (ITI), a technique designed to\nenhance the truthfulness of large language models (LLMs). ITI operates by\nshifting model activations during inference, following a set of directions\nacross a limited number of attention heads. This intervention significantly\nimproves the performance of LLaMA models on the TruthfulQA benchmark. On an\ninstruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from\n32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and\ndemonstrate how to balance it by tuning the intervention strength. ITI is\nminimally invasive and computationally inexpensive. Moreover, the technique is\ndata efficient: while approaches like RLHF require extensive annotations, ITI\nlocates truthful directions using only few hundred examples. Our findings\nsuggest that LLMs may have an internal representation of the likelihood of\nsomething being true, even as they produce falsehoods on the surface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kenneth Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_O/0/1/0/all/0/1\">Oam Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viegas_F/0/1/0/all/0/1\">Fernanda Vi&#xe9;gas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1\">Martin Wattenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning. (arXiv:2306.03350v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03350","description":"<p>It has always been an important yet challenging problem to control language\nmodels to avoid generating texts with undesirable attributes, such as toxic\nlanguage and unnatural repetition. We introduce Click for controllable text\ngeneration, which needs no modification to the model architecture and\nfacilitates out-of-the-box use of trained models. It employs a contrastive loss\non sequence likelihood, which fundamentally decreases the generation\nprobability of negative samples (i.e., generations with undesirable\nattributes). It also adopts a novel likelihood ranking-based strategy to\nconstruct contrastive samples from model generations. On the tasks of language\ndetoxification, sentiment steering, and repetition reduction, we show that\nClick outperforms strong baselines of controllable text generation and\ndemonstrate the superiority of Click's sample construction strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BatchSampler: Sampling Mini-Batches for Contrastive Learning in Vision, Language, and Graphs. (arXiv:2306.03355v1 [cs.LG])","link":"http://arxiv.org/abs/2306.03355","description":"<p>In-Batch contrastive learning is a state-of-the-art self-supervised method\nthat brings semantically-similar instances close while pushing dissimilar\ninstances apart within a mini-batch. Its key to success is the negative sharing\nstrategy, in which every instance serves as a negative for the others within\nthe mini-batch. Recent studies aim to improve performance by sampling hard\nnegatives \\textit{within the current mini-batch}, whose quality is bounded by\nthe mini-batch itself. In this work, we propose to improve contrastive learning\nby sampling mini-batches from the input data. We present\nBatchSampler\\footnote{The code is available at\n\\url{https://github.com/THUDM/BatchSampler}} to sample mini-batches of\nhard-to-distinguish (i.e., hard and true negatives to each other) instances. To\nmake each mini-batch have fewer false negatives, we design the proximity graph\nof randomly-selected instances. To form the mini-batch, we leverage random walk\nwith restart on the proximity graph to help sample hard-to-distinguish\ninstances. BatchSampler is a simple and general technique that can be directly\nplugged into existing contrastive learning models in vision, language, and\ngraphs. Extensive experiments on datasets of three modalities show that\nBatchSampler can consistently improve the performance of powerful contrastive\nmodels, as shown by significant improvements of SimCLR on ImageNet-100, SimCSE\non STS (language), and GraphCL and MVGRL on graph datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tinglin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cen_Y/0/1/0/all/0/1\">Yukuo Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yangliao Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\textit{WHAT}$, $\\textit{WHEN}$, and $\\textit{HOW}$ to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue. (arXiv:2306.03361v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03361","description":"<p>This paper presents a method for building a personalized open-domain dialogue\nsystem to address the $\\textit{WWH}$ ($\\textit{WHAT}$, $\\textit{WHEN}$, and\n$\\textit{HOW}$) problem for natural response generation in a commercial\nsetting, where personalized dialogue responses are heavily interleaved with\ncasual response turns. The proposed approach involves weighted dataset\nblending, negative persona information augmentation methods, and the design of\npersonalized conversation datasets to address the challenges of $\\textit{WWH}$\nin personalized, open-domain dialogue systems. Our work effectively balances\ndialogue fluency and tendency to ground, while also introducing a response-type\nlabel to improve the controllability and explainability of the grounded\nresponses. The combination of these methods leads to more fluent conversations,\nas evidenced by subjective human evaluations as well as objective evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_D/0/1/0/all/0/1\">Deuksin Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sunwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Ki Hyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seojin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeyoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1\">Eric Davis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextFormer: A Query-based End-to-End Text Spotter with Mixed Supervision. (arXiv:2306.03377v1 [cs.CV])","link":"http://arxiv.org/abs/2306.03377","description":"<p>End-to-end text spotting is a vital computer vision task that aims to\nintegrate scene text detection and recognition into a unified framework.\nTypical methods heavily rely on Region-of-Interest (RoI) operations to extract\nlocal features and complex post-processing steps to produce final predictions.\nTo address these limitations, we propose TextFormer, a query-based end-to-end\ntext spotter with Transformer architecture. Specifically, using query embedding\nper text instance, TextFormer builds upon an image encoder and a text decoder\nto learn a joint semantic understanding for multi-task modeling. It allows for\nmutual training and optimization of classification, segmentation, and\nrecognition branches, resulting in deeper feature sharing without sacrificing\nflexibility or simplicity. Additionally, we design an Adaptive Global\naGgregation (AGG) module to transfer global features into sequential features\nfor reading arbitrarily-shaped texts, which overcomes the sub-optimization\nproblem of RoI operations. Furthermore, potential corpus information is\nutilized from weak annotations to full labels through mixed supervision,\nfurther improving text detection and end-to-end text spotting results.\nExtensive experiments on various bilingual (i.e., English and Chinese)\nbenchmarks demonstrate the superiority of our method. Especially on TDA-ReCTS\ndataset, TextFormer surpasses the state-of-the-art method in terms of 1-NED by\n13.2%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yukun Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiameng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sanyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xingping Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianbing Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generate-then-Retrieve: Intent-Aware FAQ Retrieval in Product Search. (arXiv:2306.03411v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03411","description":"<p>Customers interacting with product search engines are increasingly\nformulating information-seeking queries. Frequently Asked Question (FAQ)\nretrieval aims to retrieve common question-answer pairs for a user query with\nquestion intent. Integrating FAQ retrieval in product search can not only\nempower users to make more informed purchase decisions, but also enhance user\nretention through efficient post-purchase support. Determining when an FAQ\nentry can satisfy a user's information need within product search, without\ndisrupting their shopping experience, represents an important challenge. We\npropose an intent-aware FAQ retrieval system consisting of (1) an intent\nclassifier that predicts when a user's information need can be answered by an\nFAQ; (2) a reformulation model that rewrites a query into a natural question.\nOffline evaluation demonstrates that our approach improves Hit@1 by 13% on\nretrieving ground-truth FAQs, while reducing latency by 95% compared to\nbaseline systems. These improvements are further validated by real user\nfeedback, where 71% of displayed FAQs on top of product search results received\nexplicit positive user feedback. Overall, our findings show promising\ndirections for integrating FAQ retrieval into product search at scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jason Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetahu_B/0/1/0/all/0/1\">Besnik Fetahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rokhlenko_O/0/1/0/all/0/1\">Oleg Rokhlenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malmasi_S/0/1/0/all/0/1\">Shervin Malmasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient and Interpretable Compressive Text Summarisation with Unsupervised Dual-Agent Reinforcement Learning. (arXiv:2306.03415v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03415","description":"<p>Recently, compressive text summarisation offers a balance between the\nconciseness issue of extractive summarisation and the factual hallucination\nissue of abstractive summarisation. However, most existing compressive\nsummarisation methods are supervised, relying on the expensive effort of\ncreating a new training dataset with corresponding compressive summaries. In\nthis paper, we propose an efficient and interpretable compressive summarisation\nmethod that utilises unsupervised dual-agent reinforcement learning to optimise\na summary's semantic coverage and fluency by simulating human judgment on\nsummarisation quality. Our model consists of an extractor agent and a\ncompressor agent, and both agents have a multi-head attentional pointer-based\nstructure. The extractor agent first chooses salient sentences from a document,\nand then the compressor agent compresses these extracted sentences by selecting\nsalient words to form a summary without using reference summaries to compute\nthe summary reward. To our best knowledge, this is the first work on\nunsupervised compressive summarisation. Experimental results on three widely\nused datasets (e.g., Newsroom, CNN/DM, and XSum) show that our model achieves\npromising performance and a significant improvement on Newsroom in terms of the\nROUGE metric, as well as interpretability of semantic coverage of summarisation\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1\">Peggy Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junbin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiyong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Role of Attention in Prompt-tuning. (arXiv:2306.03435v1 [cs.LG])","link":"http://arxiv.org/abs/2306.03435","description":"<p>Prompt-tuning is an emerging strategy to adapt large language models (LLM) to\ndownstream tasks by learning a (soft-)prompt parameter from data. Despite its\nsuccess in LLMs, there is limited theoretical understanding of the power of\nprompt-tuning and the role of the attention mechanism in prompting. In this\nwork, we explore prompt-tuning for one-layer attention architectures and study\ncontextual mixture-models where each input token belongs to a context-relevant\nor -irrelevant set. We isolate the role of prompt-tuning through a\nself-contained prompt-attention model. Our contributions are as follows: (1) We\nshow that softmax-prompt-attention is provably more expressive than\nsoftmax-self-attention and linear-prompt-attention under our contextual data\nmodel. (2) We analyze the initial trajectory of gradient descent and show that\nit learns the prompt and prediction head with near-optimal sample complexity\nand demonstrate how prompt can provably attend to sparse context-relevant\ntokens. (3) Assuming a known prompt but an unknown prediction head, we\ncharacterize the exact finite sample performance of prompt-attention which\nreveals the fundamental performance limits and the precise benefit of the\ncontext information. We also provide experiments that verify our theoretical\ninsights on real datasets and demonstrate how prompt-tuning enables the model\nto attend to context-relevant information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1\">Samet Oymak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_A/0/1/0/all/0/1\">Ankit Singh Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1\">Mahdi Soltanolkotabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thrampoulidis_C/0/1/0/all/0/1\">Christos Thrampoulidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models of Code Fail at Completing Code with Potential Bugs. (arXiv:2306.03438v1 [cs.LG])","link":"http://arxiv.org/abs/2306.03438","description":"<p>Large language models of code (Code-LLMs) have recently brought tremendous\nadvances to code completion, a fundamental feature of programming assistance\nand code intelligence. However, most existing works ignore the possible\npresence of bugs in the code context for generation, which are inevitable in\nsoftware development. Therefore, we introduce and study the buggy-code\ncompletion problem, inspired by the realistic scenario of real-time code\nsuggestion where the code context contains potential bugs -- anti-patterns that\ncan become bugs in the completed program. To systematically study the task, we\nintroduce two datasets: one with synthetic bugs derived from semantics-altering\noperator changes (buggy-HumanEval) and one with realistic bugs derived from\nuser submissions to coding problems (buggy-FixEval). We find that the presence\nof potential bugs significantly degrades the generation performance of the\nhigh-performing Code-LLMs. For instance, the passing rates of CodeGen-2B-mono\non test cases of buggy-HumanEval drop more than 50% given a single potential\nbug in the context. Finally, we investigate several post-hoc methods for\nmitigating the adverse effect of potential bugs and find that there remains a\nlarge gap in post-mitigation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tuan Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinman Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negrinho_R/0/1/0/all/0/1\">Renato Negrinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lausen_L/0/1/0/all/0/1\">Leonard Lausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1\">Sheng Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alzheimer Disease Classification through ASR-based Transcriptions: Exploring the Impact of Punctuation and Pauses. (arXiv:2306.03443v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03443","description":"<p>Alzheimer's Disease (AD) is the world's leading neurodegenerative disease,\nwhich often results in communication difficulties. Analysing speech can serve\nas a diagnostic tool for identifying the condition. The recent ADReSS challenge\nprovided a dataset for AD classification and highlighted the utility of manual\ntranscriptions. In this study, we used the new state-of-the-art Automatic\nSpeech Recognition (ASR) model Whisper to obtain the transcriptions, which also\ninclude automatic punctuation. The classification models achieved test accuracy\nscores of 0.854 and 0.833 combining the pretrained FastText word embeddings and\nrecurrent neural networks on manual and ASR transcripts respectively.\nAdditionally, we explored the influence of including pause information and\npunctuation in the transcriptions. We found that punctuation only yielded minor\nimprovements in some cases, whereas pause encoding aided AD classification for\nboth manual and ASR transcriptions across all approaches investigated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Zaragoza_L/0/1/0/all/0/1\">Luc&#xed;a G&#xf3;mez-Zaragoz&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wills_S/0/1/0/all/0/1\">Simone Wills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejedor_Garcia_C/0/1/0/all/0/1\">Cristian Tejedor-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_Morales_J/0/1/0/all/0/1\">Javier Mar&#xed;n-Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcaniz_M/0/1/0/all/0/1\">Mariano Alca&#xf1;iz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strik_H/0/1/0/all/0/1\">Helmer Strik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Assessment of Oral Reading Accuracy for Reading Diagnostics. (arXiv:2306.03444v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03444","description":"<p>Automatic assessment of reading fluency using automatic speech recognition\n(ASR) holds great potential for early detection of reading difficulties and\nsubsequent timely intervention. Precise assessment tools are required,\nespecially for languages other than English. In this study, we evaluate six\nstate-of-the-art ASR-based systems for automatically assessing Dutch oral\nreading accuracy using Kaldi and Whisper. Results show our most successful\nsystem reached substantial agreement with human evaluations (MCC = .63). The\nsame system reached the highest correlation between forced decoding confidence\nscores and word correctness (r = .45). This system's language model (LM)\nconsisted of manual orthographic transcriptions and reading prompts of the test\ndata, which shows that including reading errors in the LM improves assessment\nperformance. We discuss the implications for developing automatic assessment\nsystems and identify possible avenues of future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Molenaar_B/0/1/0/all/0/1\">Bo Molenaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejedor_Garcia_C/0/1/0/all/0/1\">Cristian Tejedor-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strik_H/0/1/0/all/0/1\">Helmer Strik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiarini_C/0/1/0/all/0/1\">Catia Cucchiarini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phonetically-Grounded Language Generation: The Case of Tongue Twisters. (arXiv:2306.03457v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03457","description":"<p>Previous work in phonetically-grounded language generation has mainly focused\non domains such as lyrics and poetry. In this paper, we present work on the\ngeneration of tongue twisters - a form of language that is required to be\nphonetically conditioned to maximise sound overlap, whilst maintaining semantic\nconsistency with an input topic, and still being grammatically correct. We\npresent \\textbf{TwistList}, a large annotated dataset of tongue twisters,\nconsisting of 2.1K+ human-authored examples. We additionally present several\nbenchmark systems (referred to as TwisterMisters) for the proposed task of\ntongue twister generation, including models that both do and do not require\ntraining on in-domain data. We present the results of automatic and human\nevaluation to demonstrate the performance of existing mainstream pre-trained\nmodels in this task with limited (or no) task specific training and data, and\nno explicit phonetic knowledge. We find that the task of tongue twister\ngeneration is challenging for models under these conditions, yet some models\nare still capable of generating acceptable examples of this language type.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loakman_T/0/1/0/all/0/1\">Tyler Loakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Commanding via Program Synthesis. (arXiv:2306.03460v1 [cs.LG])","link":"http://arxiv.org/abs/2306.03460","description":"<p>We present Semantic Interpreter, a natural language-friendly AI system for\nproductivity software such as Microsoft Office that leverages large language\nmodels (LLMs) to execute user intent across application features. While LLMs\nare excellent at understanding user intent expressed as natural language, they\nare not sufficient for fulfilling application-specific user intent that\nrequires more than text-to-text transformations. We therefore introduce the\nOffice Domain Specific Language (ODSL), a concise, high-level language\nspecialized for performing actions in and interacting with entities in Office\napplications. Semantic Interpreter leverages an Analysis-Retrieval prompt\nconstruction method with LLMs for program synthesis, translating natural\nlanguage user utterances to ODSL programs that can be transpiled to application\nAPIs and then executed. We focus our discussion primarily on a research\nexploration for Microsoft PowerPoint.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_A/0/1/0/all/0/1\">Apurva Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thong Q. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_H/0/1/0/all/0/1\">Huitian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steen_R/0/1/0/all/0/1\">Robert Steen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatawdekar_A/0/1/0/all/0/1\">Ameya Bhatawdekar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Event Extraction via Structural Semantic Matching. (arXiv:2306.03469v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03469","description":"<p>Event Extraction (EE) is one of the essential tasks in information\nextraction, which aims to detect event mentions from text and find the\ncorresponding argument roles. The EE task can be abstracted as a process of\nmatching the semantic definitions and argument structures of event types with\nthe target text. This paper encodes the semantic features of event types and\nmakes structural matching with target text. Specifically, Semantic Type\nEmbedding (STE) and Dynamic Structure Encoder (DSE) modules are proposed. Also,\nthe Joint Structural Semantic Matching (JSSM) model is built to jointly perform\nevent detection and argument extraction tasks through a bidirectional attention\nlayer. The experimental results on the ACE2005 dataset indicate that our model\nachieves a significant performance improvement\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haochen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingkun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiping Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Putting Humans in the Image Captioning Loop. (arXiv:2306.03476v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03476","description":"<p>Image Captioning (IC) models can highly benefit from human feedback in the\ntraining process, especially in cases where data is limited. We present\nwork-in-progress on adapting an IC system to integrate human feedback, with the\ngoal to make it easily adaptable to user-specific data. Our approach builds on\na base IC model pre-trained on the MS COCO dataset, which generates captions\nfor unseen images. The user will then be able to offer feedback on the image\nand the generated/predicted caption, which will be augmented to create\nadditional training instances for the adaptation of the model. The additional\ninstances are integrated into the model using step-wise updates, and a sparse\nmemory replay component is used to avoid catastrophic forgetting. We hope that\nthis approach, while leading to improved results, will also result in\ncustomizable IC models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anagnostopoulou_A/0/1/0/all/0/1\">Aliki Anagnostopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_M/0/1/0/all/0/1\">Mareike Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1\">Daniel Sonntag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciCap+: A Knowledge Augmented Dataset to Study the Challenges of Scientific Figure Captioning. (arXiv:2306.03491v1 [cs.CV])","link":"http://arxiv.org/abs/2306.03491","description":"<p>In scholarly documents, figures provide a straightforward way of\ncommunicating scientific findings to readers. Automating figure caption\ngeneration helps move model understandings of scientific documents beyond text\nand will help authors write informative captions that facilitate communicating\nscientific findings. Unlike previous studies, we reframe scientific figure\ncaptioning as a knowledge-augmented image captioning task that models need to\nutilize knowledge embedded across modalities for caption generation. To this\nend, we extended the large-scale SciCap\ndataset~\\cite{hsu-etal-2021-scicap-generating} to SciCap+ which includes\nmention-paragraphs (paragraphs mentioning figures) and OCR tokens. Then, we\nconduct experiments with the M4C-Captioner (a multimodal transformer-based\nmodel with a pointer network) as a baseline for our study. Our results indicate\nthat mention-paragraphs serves as additional context knowledge, which\nsignificantly boosts the automatic standard image caption evaluation scores\ncompared to the figure-only baselines. Human evaluations further reveal the\nchallenges of generating figure captions that are informative to readers. The\ncode and SciCap+ dataset will be publicly available at\nhttps://github.com/ZhishenYang/scientific_figure_captioning_dataset\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhishen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1\">Hideki Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Adaptable and Interactive Image Captioning with Data Augmentation and Episodic Memory. (arXiv:2306.03500v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03500","description":"<p>Interactive machine learning (IML) is a beneficial learning paradigm in cases\nof limited data availability, as human feedback is incrementally integrated\ninto the training process. In this paper, we present an IML pipeline for image\ncaptioning which allows us to incrementally adapt a pre-trained image\ncaptioning model to a new data distribution based on user input. In order to\nincorporate user input into the model, we explore the use of a combination of\nsimple data augmentation methods to obtain larger data batches for each newly\nannotated data instance and implement continual learning methods to prevent\ncatastrophic forgetting from repeated updates. For our experiments, we split a\ndomain-specific image captioning dataset, namely VizWiz, into non-overlapping\nparts to simulate an incremental input flow for continually adapting the model\nto new data. We find that, while data augmentation worsens results, even when\nrelatively small amounts of data are available, episodic memory is an effective\nstrategy to retain knowledge from previously seen clusters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anagnostopoulou_A/0/1/0/all/0/1\">Aliki Anagnostopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_M/0/1/0/all/0/1\">Mareike Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1\">Daniel Sonntag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying Standards to Advance Upstream & Downstream Ethics in Large Language Models. (arXiv:2306.03503v1 [cs.CY])","link":"http://arxiv.org/abs/2306.03503","description":"<p>This paper explores how AI-owners can develop safeguards for AI-generated\ncontent by drawing from established codes of conduct and ethical standards in\nother content-creation industries. It delves into the current state of ethical\nawareness on Large Language Models (LLMs). By dissecting the mechanism of\ncontent generation by LLMs, four key areas (upstream/downstream and at user\nprompt/answer), where safeguards could be effectively applied, are identified.\nA comparative analysis of these four areas follows and includes an evaluation\nof the existing ethical safeguards in terms of cost, effectiveness, and\nalignment with established industry practices. The paper's key argument is that\nexisting IT-related ethical codes, while adequate for traditional IT\nengineering, are inadequate for the challenges posed by LLM-based content\ngeneration. Drawing from established practices within journalism, we propose\npotential standards for businesses involved in distributing and selling\nLLM-generated content. Finally, potential conflicts of interest between dataset\ncuration at upstream and ethical benchmarking downstream are highlighted to\nunderscore the need for a broader evaluation beyond mere output. This study\nprompts a nuanced conversation around ethical implications in this rapidly\nevolving field of content generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berengueres_J/0/1/0/all/0/1\">Jose Berengueres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandell_M/0/1/0/all/0/1\">Marybeth Sandell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"A Little is Enough\": Few-Shot Quality Estimation based Corpus Filtering improves Machine Translation. (arXiv:2306.03507v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03507","description":"<p>Quality Estimation (QE) is the task of evaluating the quality of a\ntranslation when reference translation is not available. The goal of QE aligns\nwith the task of corpus filtering, where we assign the quality score to the\nsentence pairs present in the pseudo-parallel corpus. We propose a Quality\nEstimation based Filtering approach to extract high-quality parallel data from\nthe pseudo-parallel corpus. To the best of our knowledge, this is a novel\nadaptation of the QE framework to extract quality parallel corpus from the\npseudo-parallel corpus. By training with this filtered corpus, we observe an\nimprovement in the Machine Translation (MT) system's performance by up to 1.8\nBLEU points, for English-Marathi, Chinese-English, and Hindi-Bengali language\npairs, over the baseline model. The baseline model is the one that is trained\non the whole pseudo-parallel corpus. Our Few-shot QE model transfer learned\nfrom the English-Marathi QE model and fine-tuned on only 500 Hindi-Bengali\ntraining instances, shows an improvement of up to 0.6 BLEU points for\nHindi-Bengali language pair, compared to the baseline model. This demonstrates\nthe promise of transfer learning in the setting under discussion. QE systems\ntypically require in the order of (7K-25K) of training data. Our Hindi-Bengali\nQE is trained on only 500 instances of training that is 1/40th of the normal\nrequirement and achieves comparable performance. All the scripts and datasets\nutilized in this study will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Batheja_A/0/1/0/all/0/1\">Akshay Batheja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciLit: A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation. (arXiv:2306.03535v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03535","description":"<p>Scientific writing involves retrieving, summarizing, and citing relevant\npapers, which can be time-consuming processes in large and rapidly evolving\nfields. By making these processes inter-operable, natural language processing\n(NLP) provides opportunities for creating end-to-end assistive writing tools.\nWe propose SciLit, a pipeline that automatically recommends relevant papers,\nextracts highlights, and suggests a reference sentence as a citation of a\npaper, taking into consideration the user-provided context and keywords. SciLit\nefficiently recommends papers from large databases of hundreds of millions of\npapers using a two-stage pre-fetching and re-ranking literature search system\nthat flexibly deals with addition and removal of a paper database. We provide a\nconvenient user interface that displays the recommended papers as extractive\nsummaries and that offers abstractively-generated citing sentences which are\naligned with the provided context and which mention the chosen keyword(s). Our\nassistive tool for literature discovery and scientific writing is available at\nhttps://scilit.vercel.app\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Nianlong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahnloser_R/0/1/0/all/0/1\">Richard H.R. Hahnloser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Take the Hint: Improving Arabic Diacritization with Partially-Diacritized Text. (arXiv:2306.03557v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03557","description":"<p>Automatic Arabic diacritization is useful in many applications, ranging from\nreading support for language learners to accurate pronunciation predictor for\ndownstream tasks like speech synthesis. While most of the previous works\nfocused on models that operate on raw non-diacritized text, production systems\ncan gain accuracy by first letting humans partly annotate ambiguous words. In\nthis paper, we propose 2SDiac, a multi-source model that can effectively\nsupport optional diacritics in input to inform all predictions. We also\nintroduce Guided Learning, a training scheme to leverage given diacritics in\ninput with different levels of random masking. We show that the provided hints\nduring test affect more output positions than those annotated. Moreover,\nexperiments on two common benchmarks show that our approach i) greatly\noutperforms the baseline also when evaluated on non-diacritized text; and ii)\nachieves state-of-the-art results while reducing the parameter count by over\n60%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahar_P/0/1/0/all/0/1\">Parnia Bahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangi_M/0/1/0/all/0/1\">Mattia Di Gangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossenbach_N/0/1/0/all/0/1\">Nick Rossenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language acquisition: do children and language models follow similar learning stages?. (arXiv:2306.03586v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03586","description":"<p>During language acquisition, children follow a typical sequence of learning\nstages, whereby they first learn to categorize phonemes before they develop\ntheir lexicon and eventually master increasingly complex syntactic structures.\nHowever, the computational principles that lead to this learning trajectory\nremain largely unknown. To investigate this, we here compare the learning\ntrajectories of deep language models to those of children. Specifically, we\ntest whether, during its training, GPT-2 exhibits stages of language\nacquisition comparable to those observed in children aged between 18 months and\n6 years. For this, we train 48 GPT-2 models from scratch and evaluate their\nsyntactic and semantic abilities at each training step, using 96 probes curated\nfrom the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these\nevaluations with the behavior of 54 children during language production. Our\nanalyses reveal three main findings. First, similarly to children, the language\nmodels tend to learn linguistic skills in a systematic order. Second, this\nlearning scheme is parallel: the language tasks that are learned last improve\nfrom the very first training steps. Third, some - but not all - learning stages\nare shared between children and these language models. Overall, these results\nshed new light on the principles of language acquisition, and highlight\nimportant divergences in how humans and modern algorithms learn to process\nnatural language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Evanson_L/0/1/0/all/0/1\">Linnea Evanson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakretz_Y/0/1/0/all/0/1\">Yair Lakretz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_J/0/1/0/all/0/1\">Jean-R&#xe9;mi King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CUE: An Uncertainty Interpretation Framework for Text Classifiers Built on Pre-Trained Language Models. (arXiv:2306.03598v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03598","description":"<p>Text classifiers built on Pre-trained Language Models (PLMs) have achieved\nremarkable progress in various tasks including sentiment analysis, natural\nlanguage inference, and question-answering. However, the occurrence of\nuncertain predictions by these classifiers poses a challenge to their\nreliability when deployed in practical applications. Much effort has been\ndevoted to designing various probes in order to understand what PLMs capture.\nBut few studies have delved into factors influencing PLM-based classifiers'\npredictive uncertainty. In this paper, we propose a novel framework, called\nCUE, which aims to interpret uncertainties inherent in the predictions of\nPLM-based models. In particular, we first map PLM-encoded representations to a\nlatent space via a variational auto-encoder. We then generate text\nrepresentations by perturbing the latent space which causes fluctuation in\npredictive uncertainty. By comparing the difference in predictive uncertainty\nbetween the perturbed and the original text representations, we are able to\nidentify the latent dimensions responsible for uncertainty and subsequently\ntrace back to the input features that contribute to such uncertainty. Our\nextensive experiments on four benchmark datasets encompassing linguistic\nacceptability classification, emotion classification, and natural language\ninference show the feasibility of our proposed framework. Our source code is\navailable at: https://github.com/lijiazheng99/CUE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiazheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhaoyue Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1\">Bin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Quantum-Cognitively Inspired Sentiment Analysis Models. (arXiv:2306.03608v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03608","description":"<p>Quantum theory, originally proposed as a physical theory to describe the\nmotions of microscopic particles, has been applied to various non-physics\ndomains involving human cognition and decision-making that are inherently\nuncertain and exhibit certain non-classical, quantum-like characteristics.\nSentiment analysis is a typical example of such domains. In the last few years,\nby leveraging the modeling power of quantum probability (a non-classical\nprobability stemming from quantum mechanics methodology) and deep neural\nnetworks, a range of novel quantum-cognitively inspired models for sentiment\nanalysis have emerged and performed well. This survey presents a timely\noverview of the latest developments in this fascinating cross-disciplinary\narea. We first provide a background of quantum probability and quantum\ncognition at a theoretical level, analyzing their advantages over classical\ntheories in modeling the cognitive aspects of sentiment analysis. Then, recent\nquantum-cognitively inspired models are introduced and discussed in detail,\nfocusing on how they approach the key challenges of the sentiment analysis\ntask. Finally, we discuss the limitations of the current research and highlight\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yaochen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiuchi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yazhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convergence and Diversity in the Control Hierarchy. (arXiv:2306.03628v1 [cs.FL])","link":"http://arxiv.org/abs/2306.03628","description":"<p>Weir has defined a hierarchy of language classes whose second member\n($\\mathcal{L}_2$) is generated by tree-adjoining grammars (TAG), linear indexed\ngrammars (LIG), combinatory categorial grammars, and head grammars. The\nhierarchy is obtained using the mechanism of control, and $\\mathcal{L}_2$ is\nobtained using a context-free grammar (CFG) whose derivations are controlled by\nanother CFG. We adapt Weir's definition of a controllable CFG to give a\ndefinition of controllable pushdown automata (PDAs). This yields three new\ncharacterizations of $\\mathcal{L}_2$ as the class of languages generated by\nPDAs controlling PDAs, PDAs controlling CFGs, and CFGs controlling PDAs. We\nshow that these four formalisms are not only weakly equivalent but equivalent\nin a stricter sense that we call d-weak equivalence. Furthermore, using an even\nstricter notion of equivalence called d-strong equivalence, we make precise the\nintuition that a CFG controlling a CFG is a TAG, a PDA controlling a PDA is an\nembedded PDA, and a PDA controlling a CFG is a LIG. The fourth member of this\nfamily, a CFG controlling a PDA, does not correspond to any formalism we know\nof, so we invent one and call it a Pushdown Adjoining Automaton.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Butoi_A/0/1/0/all/0/1\">Alexandra Butoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Quantum Probability Driven Framework for Joint Multi-Modal Sarcasm, Sentiment and Emotion Analysis. (arXiv:2306.03650v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03650","description":"<p>Sarcasm, sentiment, and emotion are three typical kinds of spontaneous\naffective responses of humans to external events and they are tightly\nintertwined with each other. Such events may be expressed in multiple\nmodalities (e.g., linguistic, visual and acoustic), e.g., multi-modal\nconversations. Joint analysis of humans' multi-modal sarcasm, sentiment, and\nemotion is an important yet challenging topic, as it is a complex cognitive\nprocess involving both cross-modality interaction and cross-affection\ncorrelation. From the probability theory perspective, cross-affection\ncorrelation also means that the judgments on sarcasm, sentiment, and emotion\nare incompatible. However, this exposed phenomenon cannot be sufficiently\nmodelled by classical probability theory due to its assumption of\ncompatibility. Neither do the existing approaches take it into consideration.\nIn view of the recent success of quantum probability (QP) in modeling human\ncognition, particularly contextual incompatible decision making, we take the\nfirst step towards introducing QP into joint multi-modal sarcasm, sentiment,\nand emotion analysis. Specifically, we propose a QUantum probabIlity driven\nmulti-modal sarcasm, sEntiment and emoTion analysis framework, termed QUIET.\nExtensive experiments on two datasets and the results show that the\neffectiveness and advantages of QUIET in comparison with a wide range of the\nstate-of-the-art baselines. We also show the great potential of QP in\nmulti-affect analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yaochen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yazhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Injecting knowledge into language generation: a case study in auto-charting after-visit care instructions from medical dialogue. (arXiv:2306.03652v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03652","description":"<p>Factual correctness is often the limiting factor in practical applications of\nnatural language generation in high-stakes domains such as healthcare. An\nessential requirement for maintaining factuality is the ability to deal with\nrare tokens. This paper focuses on rare tokens that appear in both the source\nand the reference sequences, and which, when missed during generation, decrease\nthe factual correctness of the output text. For high-stake domains that are\nalso knowledge-rich, we show how to use knowledge to (a) identify which rare\ntokens that appear in both source and reference are important and (b) uplift\ntheir conditional probability. We introduce the ``utilization rate'' that\nencodes knowledge and serves as a regularizer by maximizing the marginal\nprobability of selected tokens. We present a study in a knowledge-rich domain\nof healthcare, where we tackle the problem of generating after-visit care\ninstructions based on patient-doctor dialogues. We verify that, in our dataset,\nspecific medical concepts with high utilization rates are underestimated by\nconventionally trained sequence-to-sequence models. We observe that correcting\nthis with our approach to knowledge injection reduces the uncertainty of the\nmodel as well as improves factuality and coherence without negatively impacting\nfluency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eremeev_M/0/1/0/all/0/1\">Maksim Eremeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valmianski_I/0/1/0/all/0/1\">Ilya Valmianski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amatriain_X/0/1/0/all/0/1\">Xavier Amatriain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1\">Anitha Kannan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Harmful Content On Online Platforms: What Platforms Need Vs. Where Research Efforts Go. (arXiv:2103.00153v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.00153","description":"<p>The proliferation of harmful content on online platforms is a major societal\nproblem, which comes in many different forms including hate speech, offensive\nlanguage, bullying and harassment, misinformation, spam, violence, graphic\ncontent, sexual abuse, self harm, and many other. Online platforms seek to\nmoderate such content to limit societal harm, to comply with legislation, and\nto create a more inclusive environment for their users. Researchers have\ndeveloped different methods for automatically detecting harmful content, often\nfocusing on specific sub-problems or on narrow communities, as what is\nconsidered harmful often depends on the platform and on the context. We argue\nthat there is currently a dichotomy between what types of harmful content\nonline platforms seek to curb, and what research efforts there are to\nautomatically detect such content. We thus survey existing methods as well as\ncontent moderation policies by online platforms in this light and we suggest\ndirections for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Arnav Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_V/0/1/0/all/0/1\">Vibha Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinkov_Y/0/1/0/all/0/1\">Yoan Dinkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zlatkova_D/0/1/0/all/0/1\">Dimitrina Zlatkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dent_K/0/1/0/all/0/1\">Kyle Dent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatawdekar_A/0/1/0/all/0/1\">Ameya Bhatawdekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouchard_G/0/1/0/all/0/1\">Guillaume Bouchard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GigaST: A 10,000-hour Pseudo Speech Translation Corpus. (arXiv:2204.03939v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03939","description":"<p>This paper introduces GigaST, a large-scale pseudo speech translation (ST)\ncorpus. We create the corpus by translating the text in GigaSpeech, an English\nASR corpus, into German and Chinese. The training set is translated by a strong\nmachine translation system and the test set is translated by human. ST models\ntrained with an addition of our corpus obtain new state-of-the-art results on\nthe MuST-C English-German benchmark test set. We provide a detailed description\nof the translation process and verify its quality. We make the translated text\ndata public and hope to facilitate research in speech translation.\nAdditionally, we also release the training scripts on NeurST to make it easy to\nreplicate our systems. GigaST dataset is available at\nhttps://st-benchmark.github.io/resources/GigaST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chutong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Knowledge Grounding for Question Answering. (arXiv:2209.08284v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.08284","description":"<p>Can language models (LM) ground question-answering (QA) tasks in the\nknowledge base via inherent relational reasoning ability? While previous models\nthat use only LMs have seen some success on many QA tasks, more recent methods\ninclude knowledge graphs (KG) to complement LMs with their more logic-driven\nimplicit knowledge. However, effectively extracting information from structured\ndata, like KGs, empowers LMs to remain an open question, and current models\nrely on graph techniques to extract knowledge. In this paper, we propose to\nsolely leverage the LMs to combine the language and knowledge for knowledge\nbased question-answering with flexibility, breadth of coverage and structured\nreasoning. Specifically, we devise a knowledge construction method that\nretrieves the relevant context with a dynamic hop, which expresses more\ncomprehensivenes than traditional GNN-based techniques. And we devise a deep\nfusion mechanism to further bridge the information exchanging bottleneck\nbetween the language and the knowledge. Extensive experiments show that our\nmodel consistently demonstrates its state-of-the-art performance over\nCommensenseQA benchmark, showcasing the possibility to leverage LMs solely to\nrobustly ground QA into the knowledge base.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siqi Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kairui Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Task-aware Layer-wise Distillation for Language Model Compression. (arXiv:2210.01351v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01351","description":"<p>Layer-wise distillation is a powerful tool to compress large models (i.e.\nteacher models) into small ones (i.e., student models). The student distills\nknowledge from the teacher by mimicking the hidden representations of the\nteacher at every intermediate layer. However, layer-wise distillation is\ndifficult. Since the student has a smaller model capacity than the teacher, it\nis often under-fitted. Furthermore, the hidden representations of the teacher\ncontain redundant information that the student does not necessarily need for\nthe target task's learning. To address these challenges, we propose a novel\nTask-aware layEr-wise Distillation (TED). TED designs task-aware filters to\nalign the hidden representations of the student and the teacher at each layer.\nThe filters select the knowledge that is useful for the target task from the\nhidden representations. As such, TED reduces the knowledge gap between the two\nmodels and helps the student to fit better on the target task. We evaluate TED\nin two scenarios: continual pre-training and fine-tuning. TED demonstrates\nsignificant and consistent improvements over existing distillation methods in\nboth scenarios. Code is available at\nhttps://github.com/cliang1453/task-aware-distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners. (arXiv:2210.02969v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02969","description":"<p>Meta-training, which fine-tunes the language model (LM) on various downstream\ntasks by maximizing the likelihood of the target label given the task\ninstruction and input instance, has improved the zero-shot task generalization\nperformance. However, meta-trained LMs still struggle to generalize to\nchallenging tasks containing novel labels unseen during meta-training. In this\npaper, we propose Flipped Learning, an alternative method of meta-training\nwhich trains the LM to generate the task instruction given the input instance\nand label. During inference, the LM trained with Flipped Learning, referred to\nas Flipped, selects the label option that is most likely to generate the task\ninstruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped\noutperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on\naverage by 8.4% and 9.7% points, respectively. Flipped gives particularly large\nimprovements on tasks with unseen labels, outperforming T0-11B by up to +20%\naverage F1 score. This indicates that the strong task generalization of Flipped\ncomes from improved generalization to novel labels. We release our code at\nhttps://github.com/seonghyeonye/Flipped-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Joongbo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Dialogue Simulation with In-Context Learning. (arXiv:2210.04185v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04185","description":"<p>Building dialogue systems requires a large corpus of annotated dialogues.\nSuch datasets are usually created via crowdsourcing, which is expensive and\ntime-consuming. In this paper, we propose \\textsc{Dialogic}, a novel dialogue\nsimulation method based on large language model in-context learning to automate\ndataset creation. Seeded with a few annotated dialogues, \\textsc{Dialogic}\nautomatically selects in-context examples for demonstration and prompts GPT-3\nto generate new dialogues and annotations in a controllable way. Our method can\nrapidly expand a small set of dialogue data with minimum or zero \\textit{human\ninvolvement} and \\textit{parameter update} and is thus much more cost-efficient\nand time-saving than crowdsourcing. Experimental results on the MultiWOZ\ndataset demonstrate that training a model on the simulated dialogues leads to\neven better performance than using the same amount of human-generated dialogues\nunder the challenging low-resource settings, with as few as 85 dialogues as a\nseed. When enough data is available, our method can still serve as an effective\ndata augmentation method. Human evaluation results also show that our simulated\ndialogues have near-human fluency and annotation accuracy. The code and data\nare available at \\textbf{\\url{https://github.com/Leezekun/dialogic}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Kernel-Based View of Language Model Fine-Tuning. (arXiv:2210.05643v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.05643","description":"<p>It has become standard to solve NLP tasks by fine-tuning pre-trained language\nmodels (LMs), especially in low-data settings. There is minimal theoretical\nunderstanding of empirical success, e.g., why fine-tuning a model with $10^8$\nor more parameters on a couple dozen training points does not result in\noverfitting. We investigate whether the Neural Tangent Kernel (NTK) - which\noriginated as a model to study the gradient descent dynamics of infinitely wide\nnetworks with suitable random initialization - describes fine-tuning of\npre-trained LMs. This study was inspired by the decent performance of NTK for\ncomputer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam\nand use Tensor Programs (Yang, 2020) to characterize conditions under which the\nNTK lens may describe fine-tuning updates to pre-trained language models.\nExtensive experiments on 14 NLP tasks validate our theory and show that\nformulating the downstream task as a masked word prediction problem through\nprompting often induces kernel-based dynamics during fine-tuning. Finally, we\nuse this kernel view to propose an explanation for the success of\nparameter-efficient subspace-based fine-tuning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malladi_S/0/1/0/all/0/1\">Sadhika Malladi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wettig_A/0/1/0/all/0/1\">Alexander Wettig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dingli Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Sanjeev Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning. (arXiv:2210.05901v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05901","description":"<p>Intelligent virtual assistants are currently designed to perform tasks or\nservices explicitly mentioned by users, so multiple related domains or tasks\nneed to be performed one by one through a long conversation with many explicit\nintents. Instead, human assistants are capable of reasoning (multiple) implicit\nintents based on user utterances via commonsense knowledge, reducing complex\ninteractions and improving practicality. Therefore, this paper proposes a\nframework of multi-domain dialogue systems, which can automatically infer\nimplicit intents based on user utterances and then perform zero-shot prompting\nusing a large pre-trained language model to trigger suitable single\ntask-oriented bots. The proposed framework is demonstrated effective to realize\nimplicit intents and recommend associated bots in a zero-shot manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1\">Hui-Chi Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Adaptive Named Entity Recognition by Retrieving Unstructured Knowledge. (arXiv:2210.07523v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07523","description":"<p>Although named entity recognition (NER) helps us to extract domain-specific\nentities from text (e.g., artists in the music domain), it is costly to create\na large amount of training data or a structured knowledge base to perform\naccurate NER in the target domain. Here, we propose self-adaptive NER, which\nretrieves external knowledge from unstructured text to learn the usages of\nentities that have not been learned well. To retrieve useful knowledge for NER,\nwe design an effective two-stage model that retrieves unstructured knowledge\nusing uncertain entities as queries. Our model predicts the entities in the\ninput and then finds those of which the prediction is not confident. Then, it\nretrieves knowledge by using these uncertain entities as queries and\nconcatenates the retrieved text to the original input to revise the prediction.\nExperiments on CrossNER datasets demonstrated that our model outperforms strong\nbaselines by 2.35 points in F1 metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kosuke Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshinaga_N/0/1/0/all/0/1\">Naoki Yoshinaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kyosuke Nishida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Universal Discriminator for Zero-Shot Generalization. (arXiv:2211.08099v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08099","description":"<p>Generative modeling has been the dominant approach for large-scale\npretraining and zero-shot generalization. In this work, we challenge this\nconvention by showing that discriminative approaches perform substantially\nbetter than generative ones on a large number of NLP tasks. Technically, we\ntrain a single discriminator to predict whether a text sample comes from the\ntrue data distribution, similar to GANs. Since many NLP tasks can be formulated\nas selecting from a few options, we use this discriminator to predict the\nconcatenation of input and which option has the highest probability of coming\nfrom the true data distribution. This simple formulation achieves\nstate-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by\n16.0\\%, 7.8\\%, and 11.5\\% respectively on different scales. In the finetuning\nsetting, our approach also achieves new state-of-the-art results on a wide\nrange of NLP tasks, with only 1/4 parameters of previous methods. Meanwhile,\nour approach requires minimal prompting efforts, which largely improves\nrobustness and is essential for real-world applications. Furthermore, we also\njointly train a generalized UD in combination with generative tasks, which\nmaintains its advantage on discriminative tasks and simultaneously works on\ngenerative tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haike Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. (arXiv:2211.10438v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.10438","description":"<p>Large language models (LLMs) show excellent performance but are compute- and\nmemory-intensive. Quantization can reduce memory and accelerate inference.\nHowever, existing methods cannot maintain accuracy and hardware efficiency at\nthe same time. We propose SmoothQuant, a training-free, accuracy-preserving,\nand general-purpose post-training quantization (PTQ) solution to enable 8-bit\nweight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that\nweights are easy to quantize while activations are not, SmoothQuant smooths the\nactivation outliers by offline migrating the quantization difficulty from\nactivations to weights with a mathematically equivalent transformation.\nSmoothQuant enables an INT8 quantization of both weights and activations for\nall the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and\nLLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for\nLLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM\nwithin a single node. Our work offers a turn-key solution that reduces hardware\ncosts and democratizes LLMs. Code is available at\nhttps://github.com/mit-han-lab/smoothquant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Guangxuan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Ji Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seznec_M/0/1/0/all/0/1\">Mickael Seznec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demouth_J/0/1/0/all/0/1\">Julien Demouth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-Augmented Multimodal Language Modeling. (arXiv:2211.12561v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.12561","description":"<p>Recent multimodal models such as DALL-E and CM3 have achieved remarkable\nprogress in text-to-image and image-to-text generation. However, these models\nstore all learned knowledge (e.g., the appearance of the Eiffel Tower) in the\nmodel parameters, requiring increasingly larger models and training data to\ncapture more knowledge. To integrate knowledge in a more scalable and modular\nway, we propose a retrieval-augmented multimodal model, which enables a base\nmultimodal model (generator) to refer to relevant text and images fetched by a\nretriever from external memory (e.g., documents on the web). Specifically, for\nthe retriever, we use a pretrained CLIP, and for the generator, we train a CM3\nTransformer on the LAION dataset. Our resulting model, named\nRetrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can\nretrieve and generate both text and images. We show that RA-CM3 significantly\noutperforms baseline multimodal models such as DALL-E and CM3 on both image and\ncaption generation tasks (12 FID and 17 CIDEr improvements on MS-COCO), while\nrequiring much less compute for training (&lt;30% of DALL-E). Moreover, we show\nthat RA-CM3 exhibits novel capabilities, such as faithful image generation and\nmultimodal in-context learning (e.g., image generation from demonstrations).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_R/0/1/0/all/0/1\">Rich James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topological Data Analysis for Speech Processing. (arXiv:2211.17223v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2211.17223","description":"<p>We apply topological data analysis (TDA) to speech classification problems\nand to the introspection of a pretrained speech model, HuBERT. To this end, we\nintroduce a number of topological and algebraic features derived from\nTransformer attention maps and embeddings. We show that a simple linear\nclassifier built on top of such features outperforms a fine-tuned\nclassification head. In particular, we achieve an improvement of about $9\\%$\naccuracy and $5\\%$ ERR on four common datasets; on CREMA-D, the proposed\nfeature set reaches a new state of the art performance with accuracy $80.155$.\nWe also show that topological features are able to reveal functional roles of\nspeech Transformer heads; e.g., we find the heads capable to distinguish\nbetween pairs of sample sources (natural/synthetic) or voices without any\ndownstream fine-tuning. Our results demonstrate that TDA is a promising new\napproach for speech analysis, especially for tasks that require structural\nprediction. Appendices, an introduction to TDA, and other additional materials\nare available here - https://topohubert.github.io/speech-topology-webpages/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tulchinskii_E/0/1/0/all/0/1\">Eduard Tulchinskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_K/0/1/0/all/0/1\">Kristian Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushnareva_L/0/1/0/all/0/1\">Laida Kushnareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherniavskii_D/0/1/0/all/0/1\">Daniil Cherniavskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolenko_S/0/1/0/all/0/1\">Sergey Nikolenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01692","description":"<p>Large language models show an emergent ability to learn a new task from a\nsmall number of input-output demonstrations. However, recent work shows that\nin-context learners largely rely on their pre-trained knowledge, such as the\nsentiment of the labels, instead of finding new associations in the input.\nHowever, the commonly-used few-shot evaluation settings using a random\nselection of in-context demonstrations can not disentangle models' ability to\nlearn a new skill from demonstrations, as most of the randomly-selected\ndemonstrations do not present relations informative for prediction beyond\nexposing the new task distribution.\n</p>\n<p>To disentangle models' in-context learning ability independent of models'\nmemory, we introduce a Conceptual few-shot learning method selecting the\ndemonstrations sharing a possibly-informative concept with the predicted\nsample. We extract a set of such concepts from annotated explanations and\nmeasure how much can models benefit from presenting these concepts in few-shot\ndemonstrations.\n</p>\n<p>We find that smaller models are more sensitive to the presented concepts.\nWhile some of the models are able to benefit from concept-presenting\ndemonstrations for each assessed concept, we find that none of the assessed\nin-context learners can benefit from all presented reasoning concepts\nconsistently, leaving the in-context concept learning an open challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadlcik_M/0/1/0/all/0/1\">Marek Kadl&#x10d;&#xed;k</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation. (arXiv:2212.07981v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07981","description":"<p>Human evaluation is the foundation upon which the evaluation of both\nsummarization systems and automatic metrics rests. However, existing human\nevaluation studies for summarization either exhibit a low inter-annotator\nagreement or have insufficient scale, and an in-depth analysis of human\nevaluation is lacking. Therefore, we address the shortcomings of existing\nsummarization evaluation along the following axes: (1) We propose a modified\nsummarization salience protocol, Atomic Content Units (ACUs), which is based on\nfine-grained semantic units and allows for a high inter-annotator agreement.\n(2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large\nhuman evaluation dataset consisting of 22,000 summary-level annotations over 28\ntop-performing systems on three datasets. (3) We conduct a comparative study of\nfour human evaluation protocols, underscoring potential confounding factors in\nevaluation setups. (4) We evaluate 50 automatic metrics and their variants\nusing the collected human annotations across evaluation protocols and\ndemonstrate how our benchmark leads to more statistically stable and\nsignificant results. The metrics we benchmarked include recent methods based on\nlarge language models (LLMs), GPTScore and G-Eval. Furthermore, our findings\nhave important implications for evaluating LLMs, as we show that LLMs adjusted\nby human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation,\nwhich is affected by the annotators' prior, input-agnostic preferences, calling\nfor more robust, targeted evaluation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yilun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Linyong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Ruilin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Simeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation. (arXiv:2212.08724v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08724","description":"<p>Self-training (ST) has prospered again in language understanding by\naugmenting the fine-tuning of pre-trained language models when labeled data is\ninsufficient. However, it remains challenging to incorporate ST into\nattribute-controllable language generation. Augmented by only self-generated\npseudo text, generation models over-emphasize exploitation of the previously\nlearned space, suffering from a constrained generalization boundary. We revisit\nST and propose a novel method, DuNST to alleviate this problem. DuNST jointly\nmodels text generation and classification with a shared Variational AutoEncoder\nand corrupts the generated pseudo text by two kinds of flexible noise to\ndisturb the space. In this way, our model could construct and utilize both\npseudo text from given labels and pseudo labels from available unlabeled text,\nwhich are gradually refined during the ST process. We theoretically demonstrate\nthat DuNST can be regarded as enhancing exploration towards the potential real\ntext space, providing a guarantee of improved performance. Experiments on three\ncontrollable generation tasks show that DuNST could significantly boost control\naccuracy while maintaining comparable generation fluency and diversity against\nseveral strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yuxi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshmanan_L/0/1/0/all/0/1\">Laks V.S. Lakshmanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Spatial Relationships in Text-to-Image Generation. (arXiv:2212.10015v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2212.10015","description":"<p>Spatial understanding is a fundamental aspect of computer vision and integral\nfor human-level reasoning about images, making it an important component for\ngrounded language understanding. While recent text-to-image synthesis (T2I)\nmodels have shown unprecedented improvements in photorealism, it is unclear\nwhether they have reliable spatial understanding capabilities. We investigate\nthe ability of T2I models to generate correct spatial relationships among\nobjects and present VISOR, an evaluation metric that captures how accurately\nthe spatial relationship described in text is generated in the image. To\nbenchmark existing models, we introduce a dataset, SR2D, that contains\nsentences describing two objects and the spatial relationship between them. We\nconstruct an automated evaluation pipeline to recognize objects and their\nspatial relationships, and employ it in a large-scale evaluation of T2I models.\nOur experiments reveal a surprising finding that, although state-of-the-art T2I\nmodels exhibit high image quality, they are severely limited in their ability\nto generate multiple objects or the specified spatial relations between them.\nOur analyses demonstrate several biases and artifacts of T2I models such as the\ndifficulty with generating multiple objects, a bias towards generating the\nfirst object mentioned, spatially inconsistent outputs for equivalent\nrelationships, and a correlation between object co-occurrence and spatial\nunderstanding capabilities. We conduct a human study that shows the alignment\nbetween VISOR and human judgement about spatial understanding. We offer the\nSR2D dataset and the VISOR metric to the community in support of T2I reasoning\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nushi_B/0/1/0/all/0/1\">Besmira Nushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DISCO: Distilling Counterfactuals with Large Language Models. (arXiv:2212.10534v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10534","description":"<p>Models trained with counterfactually augmented data learn representations of\nthe causal structure of tasks, enabling robust generalization. However,\nhigh-quality counterfactual data is scarce for most tasks and not easily\ngenerated at scale. When crowdsourced, such data is typically limited in scale\nand diversity; when generated using supervised methods, it is computationally\nexpensive to extend to new counterfactual dimensions. In this work, we\nintroduce DISCO (DIStilled COunterfactual Data), a new method for automatically\ngenerating high quality counterfactual data at scale. DISCO engineers prompts\nto generate phrasal perturbations with a large general language model. Then, a\ntask-specific teacher model filters these generations to distill high-quality\ncounterfactual data. While task-agnostic, we apply our pipeline to the task of\nnatural language inference (NLI) and find that on challenging evaluations such\nas the NLI stress test, comparatively smaller student models trained with DISCO\ngenerated counterfactuals are more robust (6% absolute) and generalize better\nacross distributions (2%) compared to models trained without data augmentation.\nFurthermore, DISCO augmented models are 10% more consistent between\ncounterfactual pairs on three evaluation sets, demonstrating that DISCO\naugmentation enables models to more reliably learn causal representations. Our\nrepository is available at: https://github.com/eric11eca/disco\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiyue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_K/0/1/0/all/0/1\">Kyle Richardson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NarrowBERT: Accelerating Masked Language Model Pretraining and Inference. (arXiv:2301.04761v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.04761","description":"<p>Large-scale language model pretraining is a very successful form of\nself-supervised learning in natural language processing, but it is increasingly\nexpensive to perform as the models and pretraining corpora have become larger\nover time. We propose NarrowBERT, a modified transformer encoder that increases\nthe throughput for masked language model pretraining by more than $2\\times$.\nNarrowBERT sparsifies the transformer model such that the self-attention\nqueries and feedforward layers only operate on the masked tokens of each\nsentence during pretraining, rather than all of the tokens as with the usual\ntransformer encoder. We also show that NarrowBERT increases the throughput at\ninference time by as much as $3.5\\times$ with minimal (or no) performance\ndegradation on sentence encoding tasks like MNLI. Finally, we examine the\nperformance of NarrowBERT on the IMDB and Amazon reviews classification and\nCoNLL NER tasks and show that it is also comparable to standard BERT\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keung_P/0/1/0/all/0/1\">Phillip Keung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Daniel Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Watermark for Large Language Models. (arXiv:2301.10226v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2301.10226","description":"<p>Potential harms of large language models can be mitigated by watermarking\nmodel output, i.e., embedding signals into generated text that are invisible to\nhumans but algorithmically detectable from a short span of tokens. We propose a\nwatermarking framework for proprietary language models. The watermark can be\nembedded with negligible impact on text quality, and can be detected using an\nefficient open-source algorithm without access to the language model API or\nparameters. The watermark works by selecting a randomized set of \"green\" tokens\nbefore a word is generated, and then softly promoting use of green tokens\nduring sampling. We propose a statistical test for detecting the watermark with\ninterpretable p-values, and derive an information-theoretic framework for\nanalyzing the sensitivity of the watermark. We test the watermark using a\nmulti-billion parameter model from the Open Pretrained Transformer (OPT)\nfamily, and discuss robustness and security.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirchenbauer_J/0/1/0/all/0/1\">John Kirchenbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_J/0/1/0/all/0/1\">Jonathan Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miers_I/0/1/0/all/0/1\">Ian Miers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech. (arXiv:2301.11462v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11462","description":"<p>When acquiring syntax, children consistently choose hierarchical rules over\ncompeting non-hierarchical possibilities. Is this preference due to a learning\nbias for hierarchical structure, or due to more general biases that interact\nwith hierarchical cues in children's linguistic input? We explore these\npossibilities by training LSTMs and Transformers - two types of neural networks\nwithout a hierarchical bias - on data similar in quantity and content to\nchildren's linguistic input: text from the CHILDES corpus. We then evaluate\nwhat these models have learned about English yes/no questions, a phenomenon for\nwhich hierarchical structure is crucial. We find that, though they perform well\nat capturing the surface statistics of child-directed speech (as measured by\nperplexity), both model types generalize in a way more consistent with an\nincorrect linear rule than the correct hierarchical rule. These results suggest\nthat human-like generalization from text alone requires stronger biases than\nthe general sequence-processing biases of standard neural network\narchitectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yedetore_A/0/1/0/all/0/1\">Aditya Yedetore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_R/0/1/0/all/0/1\">Robert Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCoy_R/0/1/0/all/0/1\">R. Thomas McCoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. (arXiv:2301.12132v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12132","description":"<p>Large pretrained language models are widely used in downstream NLP tasks via\ntask-specific fine-tuning, but such procedures can be costly. Recently,\nParameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task\nperformance while updating a much smaller number of parameters compared to full\nmodel fine-tuning (FFT). However, it is non-trivial to make informed design\nchoices on the PEFT configurations, such as their architecture, the number of\ntunable parameters, and even the layers in which the PEFT modules are inserted.\nConsequently, it is highly likely that the current, manually designed\nconfigurations are suboptimal in terms of their performance-efficiency\ntrade-off. Inspired by advances in neural architecture search, we propose\nAutoPEFT for automatic PEFT configuration selection: we first design an\nexpressive configuration search space with multiple representative PEFT modules\nas building blocks. Using multi-objective Bayesian optimisation in a low-cost\nsetup, we then discover a Pareto-optimal set of configurations with strong\nperformance-cost trade-offs across different numbers of parameters that are\nalso highly transferable across different tasks. Empirically, on GLUE and\nSuperGLUE tasks, we show that AutoPEFT-discovered configurations significantly\noutperform existing PEFT methods and are on par or better than FFT, without\nincurring substantial training efficiency costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Han Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xingchen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Be Easily Distracted by Irrelevant Context. (arXiv:2302.00093v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00093","description":"<p>Large language models have achieved impressive performance on various natural\nlanguage processing tasks. However, so far they have been evaluated primarily\non benchmarks where all information in the input context is relevant for\nsolving the task. In this work, we investigate the distractibility of large\nlanguage models, i.e., how the model problem-solving accuracy can be influenced\nby irrelevant context. In particular, we introduce Grade-School Math with\nIrrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant\ninformation in the problem description. We use this benchmark to measure the\ndistractibility of cutting-edge prompting techniques for large language models,\nand find that the model performance is dramatically decreased when irrelevant\ninformation is included. We also identify several approaches for mitigating\nthis deficiency, such as decoding with self-consistency and adding to the\nprompt an instruction that tells the language model to ignore the irrelevant\ninformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Freda Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_K/0/1/0/all/0/1\">Kanishka Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scales_N/0/1/0/all/0/1\">Nathan Scales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharli_N/0/1/0/all/0/1\">Nathanael Sch&#xe4;rli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DP-BART for Privatized Text Rewriting under Local Differential Privacy. (arXiv:2302.07636v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2302.07636","description":"<p>Privatized text rewriting with local differential privacy (LDP) is a recent\napproach that enables sharing of sensitive textual documents while formally\nguaranteeing privacy protection to individuals. However, existing systems face\nseveral issues, such as formal mathematical flaws, unrealistic privacy\nguarantees, privatization of only individual words, as well as a lack of\ntransparency and reproducibility. In this paper, we propose a new system\n'DP-BART' that largely outperforms existing LDP systems. Our approach uses a\nnovel clipping method, iterative pruning, and further training of internal\nrepresentations which drastically reduces the amount of noise required for DP\nguarantees. We run experiments on five textual datasets of varying sizes,\nrewriting them at different privacy guarantees and evaluating the rewritten\ntexts on downstream text classification tasks. Finally, we thoroughly discuss\nthe privatized text rewriting approach and its limitations, including the\nproblem of the strict text adjacency constraint in the LDP paradigm that leads\nto the high noise requirement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Igamberdiev_T/0/1/0/all/0/1\">Timour Igamberdiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aligning Language Models with Preferences through f-divergence Minimization. (arXiv:2302.08215v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08215","description":"<p>Aligning language models with preferences can be posed as approximating a\ntarget distribution representing some desired behavior. Existing approaches\ndiffer both in the functional form of the target distribution and the algorithm\nused to approximate it. For instance, Reinforcement Learning from Human\nFeedback (RLHF) corresponds to minimizing a reverse KL from an implicit target\ndistribution arising from a KL penalty in the objective. On the other hand,\nGenerative Distributional Control (GDC) has an explicit target distribution and\nminimizes a forward KL from it using the Distributional Policy Gradient (DPG)\nalgorithm. In this paper, we propose a new approach, f-DPG, which allows the\nuse of any f-divergence to approximate any target distribution that can be\nevaluated. f-DPG unifies both frameworks (RLHF, GDC) and the approximation\nmethods (DPG, RL with KL penalties). We show the practical benefits of various\nchoices of divergence objectives and demonstrate that there is no universally\noptimal objective but that different divergences present different alignment\nand diversity trade-offs. We show that Jensen-Shannon divergence strikes a good\nbalance between these objectives, and frequently outperforms forward KL\ndivergence by a wide margin, leading to significant improvements over prior\nwork. These distinguishing characteristics between divergences persist as the\nmodel size increases, highlighting the importance of selecting appropriate\ndivergence objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Go_D/0/1/0/all/0/1\">Dongyoung Go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1\">Tomasz Korbak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruszewski_G/0/1/0/all/0/1\">Germ&#xe1;n Kruszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozen_J/0/1/0/all/0/1\">Jos Rozen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_N/0/1/0/all/0/1\">Nahyeon Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dymetman_M/0/1/0/all/0/1\">Marc Dymetman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.17612","description":"<p>In this paper, we introduce the range of oBERTa language models, an\neasy-to-use set of language models which allows Natural Language Processing\n(NLP) practitioners to obtain between 3.8 and 24.3 times faster models without\nexpertise in model compression. Specifically, oBERTa extends existing work on\npruning, knowledge distillation, and quantization and leverages frozen\nembeddings improves distillation and model initialization to deliver higher\naccuracy on a broad range of transfer tasks. In generating oBERTa, we explore\nhow the highly optimized RoBERTa differs from the BERT for pruning during\npre-training and finetuning. We find it less amenable to compression during\nfine-tuning. We explore the use of oBERTa on seven representative NLP tasks and\nfind that the improved compression techniques allow a pruned oBERTa model to\nmatch the performance of BERTbase and exceed the performance of Prune OFA Large\non the SQUAD V1.1 Question Answering dataset, despite being 8x and 2x,\nrespectively faster in inference. We release our code, training regimes, and\nassociated model for broad usage to encourage usage and experimentation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_A/0/1/0/all/0/1\">Alexandre Marques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_M/0/1/0/all/0/1\">Mark Kurtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PDFVQA: A New Dataset for Real-World VQA on PDF Documents. (arXiv:2304.06447v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.06447","description":"<p>Document-based Visual Question Answering examines the document understanding\nof document images in conditions of natural language questions. We proposed a\nnew document-based VQA dataset, PDF-VQA, to comprehensively examine the\ndocument understanding from various aspects, including document element\nrecognition, document layout structural understanding as well as contextual\nunderstanding and key information extraction. Our PDF-VQA dataset extends the\ncurrent scale of document understanding that limits on the single document page\nto the new scale that asks questions over the full document of multiple pages.\nWe also propose a new graph-based VQA model that explicitly integrates the\nspatial and hierarchically structural relationships between different document\nelements to boost the document structural understanding. The performances are\ncompared with several baselines over different question types and\ntasks\\footnote{The full dataset will be released after paper acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yihao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Siwen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyunsuk Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph2topic: an opensource topic modeling framework based on sentence embedding and community detection. (arXiv:2304.06653v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.06653","description":"<p>It has been reported that clustering-based topic models, which cluster\nhigh-quality sentence embeddings with an appropriate word selection method, can\ngenerate better topics than generative probabilistic topic models. However,\nthese approaches suffer from the inability to select appropriate parameters and\nincomplete models that overlook the quantitative relation between words with\ntopics and topics with text. To solve these issues, we propose graph to topic\n(G2T), a simple but effective framework for topic modelling. The framework is\ncomposed of four modules. First, document representation is acquired using\npretrained language models. Second, a semantic graph is constructed according\nto the similarity between document representations. Third, communities in\ndocument semantic graphs are identified, and the relationship between topics\nand documents is quantified accordingly. Fourth, the word--topic distribution\nis computed based on a variant of TFIDF. Automatic evaluation suggests that G2T\nachieved state-of-the-art performance on both English and Chinese documents\nwith different lengths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Leihang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiapeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiang Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Repetition Suppression and Content Moderation of Large Language Models. (arXiv:2304.10611v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.10611","description":"<p>Natural language generation (NLG) is one of the most impactful fields in NLP,\nand recent years have witnessed its evolution brought about by large language\nmodels (LLMs). As the key instrument for writing assistance applications, they\nare generally prone to replicating or extending offensive content provided in\nthe input. In low-resource data regime, they can also lead to repetitive\noutputs. Usually, offensive content and repetitions are mitigated with post-hoc\nmethods, including n-gram level blocklists, top-k and nucleus sampling. In this\npaper, we apply non-exact repetition suppression using token and sequence level\nunlikelihood loss, and further explore the framework of unlikelihood training\nobjective in order to jointly endow the model with abilities to avoid\ngenerating offensive words and phrases from the beginning. Finally, with\ncomprehensive experiments, we demonstrate that our proposed methods work\nexceptionally in controlling the repetition and content quality of LLM outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Alex Sokolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weixin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si-Qing Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAIST-SIC-Aligned: Automatically-Aligned English-Japanese Simultaneous Interpretation Corpus. (arXiv:2304.11766v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.11766","description":"<p>It remains a question that how simultaneous interpretation (SI) data affects\nsimultaneous machine translation (SiMT). Research has been limited due to the\nlack of a large-scale training corpus. In this work, we aim to fill in the gap\nby introducing NAIST-SIC-Aligned, which is an automatically-aligned parallel\nEnglish-Japanese SI dataset. Starting with a non-aligned corpus NAIST-SIC, we\npropose a two-stage alignment approach to make the corpus parallel and thus\nsuitable for model training. The first stage is coarse alignment where we\nperform a many-to-many mapping between source and target sentences, and the\nsecond stage is fine-grained alignment where we perform intra- and\ninter-sentence filtering to improve the quality of aligned pairs. To ensure the\nquality of the corpus, each step has been validated either quantitatively or\nqualitatively. This is the first open-sourced large-scale parallel SI dataset\nin the literature. We also manually curated a small test set for evaluation\npurposes. We hope our work advances research on SI corpora construction and\nSiMT. Please find our data at \\url{https://github.com/mingzi151/AHC-SI}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_Y/0/1/0/all/0/1\">Yuka Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doi_K/0/1/0/all/0/1\">Kosuke Doi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukuda_R/0/1/0/all/0/1\">Ryo Fukuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1\">Katsuhito Sudoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A logical word embedding for learning grammar. (arXiv:2304.14590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14590","description":"<p>We introduce the logical grammar emdebbing (LGE), a model inspired by\npregroup grammars and categorial grammars to enable unsupervised inference of\nlexical categories and syntactic rules from a corpus of text. LGE produces\ncomprehensible output summarizing its inferences, has a completely transparent\nprocess for producing novel sentences, and can learn from as few as a hundred\nsentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deyo_S/0/1/0/all/0/1\">Sean Deyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elser_V/0/1/0/all/0/1\">Veit Elser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCpdf: Building a High Quality Corpus for Visually Rich Documents from Web Crawl Data. (arXiv:2304.14953v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14953","description":"<p>In recent years, the field of document understanding has progressed a lot. A\nsignificant part of this progress has been possible thanks to the use of\nlanguage models pretrained on large amounts of documents. However, pretraining\ncorpora used in the domain of document understanding are single domain,\nmonolingual, or nonpublic. Our goal in this paper is to propose an efficient\npipeline for creating a big-scale, diverse, multilingual corpus of PDF files\nfrom all over the Internet using Common Crawl, as PDF files are the most\ncanonical types of documents as considered in document understanding. We\nanalysed extensively all of the steps of the pipeline and proposed a solution\nwhich is a trade-off between data quality and processing time. We also share a\nCCpdf corpus in a form or an index of PDF files along with a script for\ndownloading them, which produces a collection useful for language model\npretraining. The dataset and tools published with this paper offer researchers\nthe opportunity to develop even better multilingual language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Turski_M/0/1/0/all/0/1\">Micha&#x142; Turski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanislawek_T/0/1/0/all/0/1\">Tomasz Stanis&#x142;awek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaczmarek_K/0/1/0/all/0/1\">Karol Kaczmarek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyda_P/0/1/0/all/0/1\">Pawe&#x142; Dyda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gralinski_F/0/1/0/all/0/1\">Filip Grali&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Politics of Language Choice: How the Russian-Ukrainian War Influences Ukrainians' Language Use on Twitter. (arXiv:2305.02770v3 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2305.02770","description":"<p>The use of language is innately political and often a vehicle of cultural\nidentity as well as the basis for nation building. Here, we examine language\nchoice and tweeting activity of Ukrainian citizens based on more than 4 million\ngeo-tagged tweets from over 62,000 users before and during the\nRussian-Ukrainian War, from January 2020 to October 2022. Using statistical\nmodels, we disentangle sample effects, arising from the in- and outflux of\nusers on Twitter, from behavioural effects, arising from behavioural changes of\nthe users. We observe a steady shift from the Russian language towards the\nUkrainian language already before the war, which drastically speeds up with its\noutbreak. We attribute these shifts in large part to users' behavioural\nchanges. Notably, we find that more than half of the Russian-tweeting users\nshift towards Ukrainian as a result of the war.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Racek_D/0/1/0/all/0/1\">Daniel Racek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_B/0/1/0/all/0/1\">Brittany I. Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thurner_P/0/1/0/all/0/1\">Paul W. Thurner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kauermann_G/0/1/0/all/0/1\">G&#xf6;ran Kauermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SI-LSTM: Speaker Hybrid Long-short Term Memory and Cross Modal Attention for Emotion Recognition in Conversation. (arXiv:2305.03506v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03506","description":"<p>Emotion Recognition in Conversation~(ERC) across modalities is of vital\nimportance for a variety of applications, including intelligent healthcare,\nartificial intelligence for conversation, and opinion mining over chat history.\nThe crux of ERC is to model both cross-modality and cross-time interactions\nthroughout the conversation. Previous methods have made progress in learning\nthe time series information of conversation while lacking the ability to trace\ndown the different emotional states of each speaker in a conversation. In this\npaper, we propose a recurrent structure called Speaker Information Enhanced\nLong-Short Term Memory (SI-LSTM) for the ERC task, where the emotional states\nof the distinct speaker can be tracked in a sequential way to enhance the\nlearning of the emotion in conversation. Further, to improve the learning of\nmultimodal features in ERC, we utilize a cross-modal attention component to\nfuse the features between different modalities and model the interaction of the\nimportant information from different modalities. Experimental results on two\nbenchmark datasets demonstrate the superiority of the proposed SI-LSTM against\nthe state-of-the-art baseline methods in the ERC task on multimodal data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xingwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">You Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explanation-based Finetuning Makes Models More Robust to Spurious Cues. (arXiv:2305.04990v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04990","description":"<p>Large Language Models (LLMs) are so powerful that they sometimes learn\ncorrelations between labels and features that are irrelevant to the task,\nleading to poor generalization on out-of-distribution data. We propose\nexplanation-based finetuning as a general approach to mitigate LLMs' reliance\non spurious correlations. Unlike standard finetuning where the model only\npredicts the answer given the input, we finetune the model to additionally\ngenerate a free-text explanation supporting its answer. To evaluate our method,\nwe finetune the model on artificially constructed training sets containing\ndifferent types of spurious cues, and test it on a test set without these cues.\nCompared to standard finetuning, our method makes GPT-3 (davinci) remarkably\nmore robust against spurious cues in terms of accuracy drop across four\nclassification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC\n(+6.5). The efficacy generalizes across multiple model families and scales,\nwith greater gains for larger models. Finally, our method also works well with\nexplanations generated by the model, implying its applicability to more\ndatasets without human-written explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ludan_J/0/1/0/all/0/1\">Josh Magnus Ludan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yixuan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Saurabh Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1\">Marianna Apidianaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models. (arXiv:2305.10276v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10276","description":"<p>In this paper, we take the initiative to investigate the performance of LLMs\non complex planning tasks that require LLMs to understand a virtual spatial\nenvironment simulated via natural language and act correspondingly in text. We\npropose a benchmark named Natural Language Planning and Action (Natala)\ncomposed of a set of novel tasks: Brick World, NLVR-based Manipulations, and\nNatural Language Navigation. We found that current popular LLMs such as ChatGPT\nstill lack abilities in complex planning. This arises a question -- do the LLMs\nhave a good understanding of the environments described in natural language, or\nmaybe other alternatives such as symbolic representations are neater and hence\nbetter to be understood by LLMs? To this end, we propose a novel method called\nCoS (Chain-of-Symbol Prompting) that represents the complex environments with\ncondensed symbolic spatial representations during the chained intermediate\nthinking steps. CoS is easy to use and does not need additional training on\nLLMs. Extensive experiments indicate that CoS clearly surpasses the performance\nof the Chain-of-Thought (CoT) Prompting in all three planning tasks with even\nfewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.\nThe performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)\non Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt\nobviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate\nsteps from demonstrations on Brick World.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hanxu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huajian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces. (arXiv:2305.12464v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12464","description":"<p>Self-supervised speech representations are known to encode both speaker and\nphonetic information, but how they are distributed in the high-dimensional\nspace remains largely unexplored. We hypothesize that they are encoded in\northogonal subspaces, a property that lends itself to simple disentanglement.\nApplying principal component analysis to representations of two predictive\ncoding models, we identify two subspaces that capture speaker and phonetic\nvariances, and confirm that they are nearly orthogonal. Based on this property,\nwe propose a new speaker normalization method which collapses the subspace that\nencodes speaker information, without requiring transcriptions. Probing\nexperiments show that our method effectively eliminates speaker information and\noutperforms a previous baseline in phone discrimination tasks. Moreover, the\napproach generalizes and can be used to remove information of unseen speakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_O/0/1/0/all/0/1\">Oli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwater_S/0/1/0/all/0/1\">Sharon Goldwater</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16806","description":"<p>Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose\nlanguage models capable of addressing many natural language generation or\nunderstanding tasks. On the task of Machine Translation (MT), multiple works\nhave investigated few-shot prompting mechanisms to elicit better translations\nfrom LLMs. However, there has been relatively little investigation on how such\ntranslations differ qualitatively from the translations generated by standard\nNeural Machine Translation (NMT) models. In this work, we investigate these\ndifferences in terms of the literalness of translations produced by the two\nsystems. Using literalness measures involving word alignment and monotonicity,\nwe find that translations out of English (E-X) from GPTs tend to be less\nliteral, while exhibiting similar or better scores on MT quality metrics. We\ndemonstrate that this finding is borne out in human evaluations as well. We\nthen show that these differences are especially pronounced when translating\nsentences that contain idiomatic expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raunak_V/0/1/0/all/0/1\">Vikas Raunak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Arul Menezes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2306.00107","description":"<p>Self-supervised learning (SSL) has recently emerged as a promising paradigm\nfor training generalisable models on large-scale data in the fields of vision,\ntext, and speech. Although SSL has been proven effective in speech and audio,\nits application to music audio has yet to be thoroughly explored. This is\nprimarily due to the distinctive challenges associated with modelling musical\nknowledge, particularly its tonal and pitched characteristics of music. To\naddress this research gap, we propose an acoustic Music undERstanding model\nwith large-scale self-supervised Training (MERT), which incorporates teacher\nmodels to provide pseudo labels in the masked language modelling (MLM) style\nacoustic pre-training. In our exploration, we identified a superior combination\nof teacher models, which outperforms conventional speech and audio approaches\nin terms of performance. This combination includes an acoustic teacher based on\nResidual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a musical\nteacher based on the Constant-Q Transform (CQT). These teachers effectively\nguide our student model, a BERT-style transformer encoder, to better model\nmusic audio. In addition, we introduce an in-batch noise mixture augmentation\nto enhance the representation robustness. Furthermore, we explore a wide range\nof settings to overcome the instability in acoustic language model\npre-training, which allows our designed paradigm to scale from 95M to 330M\nparameters. Experimental results indicate that our model can generalise and\nperform well on 14 music understanding tasks and attains state-of-the-art\n(SOTA) overall scores. The code and models are online:\nhttps://github.com/yizhilll/MERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yizhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Ruibin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yinghao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hanzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragni_A/0/1/0/all/0/1\">Anton Ragni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1\">Emmanouil Benetos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyenge_N/0/1/0/all/0/1\">Norbert Gyenge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dannenberg_R/0/1/0/all/0/1\">Roger Dannenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gus Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yemin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CapText: Large Language Model-based Caption Generation From Image Context and Description. (arXiv:2306.00301v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.00301","description":"<p>While deep-learning models have been shown to perform well on image-to-text\ndatasets, it is difficult to use them in practice for captioning images. This\nis because captions traditionally tend to be context-dependent and offer\ncomplementary information about an image, while models tend to produce\ndescriptions that describe the visual features of the image. Prior research in\ncaption generation has explored the use of models that generate captions when\nprovided with the images alongside their respective descriptions or contexts.\nWe propose and evaluate a new approach, which leverages existing large language\nmodels to generate captions from textual descriptions and context alone,\nwithout ever processing the image directly. We demonstrate that after\nfine-tuning, our approach outperforms current state-of-the-art image-text\nalignment models like OSCAR-VinVL on this task on the CIDEr metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shinjini Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anupam_S/0/1/0/all/0/1\">Sagnik Anupam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00477","description":"<p>Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs)\nhas emerged as a highly successful approach, with training only a small number\nof parameters without sacrificing performance and becoming the de-facto\nlearning paradigm with the increasing size of PLMs. However, existing PEFT\nmethods are not memory-efficient, because they still require caching most of\nthe intermediate activations for the gradient calculation, akin to fine-tuning.\nOne effective way to reduce the activation memory is to apply a reversible\nmodel, so the intermediate activations are not necessary to be cached and can\nbe recomputed. Nevertheless, modifying a PLM to its reversible variant with\nPEFT is not straightforward, since the reversible model has a distinct\narchitecture from the currently released PLMs. In this paper, we first\ninvestigate what is a key factor for the success of existing PEFT methods, and\nrealize that it's essential to preserve the PLM's starting point when\ninitializing a PEFT method. With this finding, we propose memory-efficient\nfine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM's\nstarting point and making it reversible without additional pre-training. We\nevaluate MEFT on the GLUE benchmark and five question-answering tasks with\nvarious backbones, BERT, RoBERTa, BART and OPT. MEFT significantly reduces the\nactivation memory up to 84% of full fine-tuning with a negligible amount of\ntrainable parameters. Moreover, MEFT achieves the same score on GLUE and a\ncomparable score on the question-answering tasks as full fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1\">Baohao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shaomu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monz_C/0/1/0/all/0/1\">Christof Monz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Programming eTextbooks with ChatGPT Generated Counterfactual-Thinking-Inspired Questions. (arXiv:2306.00551v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00551","description":"<p>Digital textbooks have become an integral part of everyday learning tasks. In\nthis work, we consider the use of digital textbooks for programming classes.\nGenerally, students struggle with utilizing textbooks on programming to the\nmaximum, with a possible reason being that the example programs provided as\nillustration of concepts in these textbooks don't offer sufficient\ninteractivity for students, and thereby not sufficiently motivating to explore\nor understand these programming examples better. In our work, we explore the\nidea of enhancing the navigability of intelligent textbooks with the use of\n``counterfactual'' questions, to make students think critically about these\nprograms and enhance possible program comprehension. Inspired from previous\nworks on nudging students on counter factual thinking, we present the\npossibility to enhance digital textbooks with questions generated using GPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1\">Arun Balajiee Lekshmi Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrawan_R/0/1/0/all/0/1\">Rully Agus Hendrawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1\">Venktesh V</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multi-Step Reasoning by Solving Arithmetic Tasks. (arXiv:2306.01707v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01707","description":"<p>Mathematical reasoning is regarded as a necessary ability for Language Models\n(LMs). Recent works demonstrate large LMs' impressive performance in solving\nmath problems. The success is attributed to their Chain-of-Thought (CoT)\nreasoning abilities, i.e., the ability to decompose complex questions into\nstep-by-step reasoning chains, but such ability seems only to emerge from\nmodels with abundant parameters. This work investigates how to incorporate\nrelatively small LMs with the capabilities of multi-step reasoning. We propose\nto inject such abilities by continually pre-training LMs on a synthetic dataset\nMsAT which is composed of Multi-step Arithmetic Tasks. Our experiments on four\nmath word problem datasets show the effectiveness of the proposed method in\nenhancing LMs' math reasoning abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianduo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey on Deep Learning for Relation Extraction: Recent Advances and New Frontiers. (arXiv:2306.02051v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02051","description":"<p>Relation extraction (RE) involves identifying the relations between entities\nfrom unstructured texts. RE serves as the foundation for many natural language\nprocessing (NLP) applications, such as knowledge graph completion, question\nanswering, and information retrieval. In recent years, deep neural networks\nhave dominated the field of RE and made noticeable progress. Subsequently, the\nlarge pre-trained language models (PLMs) have taken the state-of-the-art of RE\nto a new level. This survey provides a comprehensive review of existing deep\nlearning techniques for RE. First, we introduce RE resources, including RE\ndatasets and evaluation metrics. Second, we propose a new taxonomy to\ncategorize existing works from three perspectives (text representation, context\nencoding, and triplet prediction). Third, we discuss several important\nchallenges faced by RE and summarize potential techniques to tackle these\nchallenges. Finally, we outline some promising future directions and prospects\nin this field. This survey is expected to facilitate researchers' collaborative\nefforts to tackle the challenges of real-life RE systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiLegalPile: A 689GB Multilingual Legal Corpus. (arXiv:2306.02069v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02069","description":"<p>Large, high-quality datasets are crucial for training Large Language Models\n(LLMs). However, so far, there are few datasets available for specialized\ncritical domains such as law and the available ones are often only for the\nEnglish language. We curate and release MultiLegalPile, a 689GB corpus in 24\nlanguages from 17 jurisdictions. The MultiLegalPile corpus, which includes\ndiverse legal data sources with varying licenses, allows for pretraining NLP\nmodels under fair use, with more permissive licenses for the Eurlex Resources\nand Legal mC4 subsets. We pretrain two RoBERTa models and one Longformer\nmultilingually, and 24 monolingual models on each of the language-specific\nsubsets and evaluate them on LEXTREME. Additionally, we evaluate the English\nand multilingual models on LexGLUE. Our multilingual models set a new SotA on\nLEXTREME and our English models on LexGLUE. We release the dataset, the trained\nmodels, and all of the code under the most open possible licenses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_J/0/1/0/all/0/1\">Joel Niklaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matoshi_V/0/1/0/all/0/1\">Veton Matoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sturmer_M/0/1/0/all/0/1\">Matthias St&#xfc;rmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel E. Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models. (arXiv:2306.02254v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02254","description":"<p>Polyglot is a pioneering project aimed at enhancing the non-English language\nperformance of multilingual language models. Despite the availability of\nvarious multilingual models such as mBERT (Devlin et al., 2019), XGLM (Lin et\nal., 2022), and BLOOM (Scao et al., 2022), researchers and developers often\nresort to building monolingual models in their respective languages due to the\ndissatisfaction with the current multilingual models non-English language\ncapabilities. Addressing this gap, we seek to develop advanced multilingual\nlanguage models that offer improved performance in non-English languages. In\nthis paper, we introduce the Polyglot Korean models, which represent a specific\nfocus rather than being multilingual in nature. In collaboration with TUNiB,\nour team collected 1.2TB of Korean data meticulously curated for our research\njourney. We made a deliberate decision to prioritize the development of Korean\nmodels before venturing into multilingual models. This choice was motivated by\nmultiple factors: firstly, the Korean models facilitated performance\ncomparisons with existing multilingual models; and finally, they catered to the\nspecific needs of Korean companies and researchers. This paper presents our\nwork in developing the Polyglot Korean models, which propose some steps towards\naddressing the non-English language performance gap in multilingual language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hyunwoong Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kichang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_M/0/1/0/all/0/1\">Minho Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1\">Taekyoon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seungmu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyun_J/0/1/0/all/0/1\">Jiwung Hyun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyubyong Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding. (arXiv:2306.02858v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02858","description":"<p>We present Video-LLaMA, a multi-modal framework that empowers Large Language\nModels (LLMs) with the capability of understanding both visual and auditory\ncontent in the video. Video-LLaMA bootstraps cross-modal training from the\nfrozen pre-trained visual &amp; audio encoders and the frozen LLMs. Unlike previous\nvision-LLMs that focus on static image comprehensions such as MiniGPT-4 and\nLLaVA, Video-LLaMA mainly tackles two challenges in video understanding: (1)\ncapturing the temporal changes in visual scenes, (2) integrating audio-visual\nsignals. To counter the first challenge, we propose a Video Q-former to\nassemble the pre-trained image encoder into our video encoder and introduce a\nvideo-to-text generation task to learn video-language correspondence. For the\nsecond challenge, we leverage ImageBind, a universal embedding model aligning\nmultiple modalities as the pre-trained audio encoder, and introduce an Audio\nQ-former on top of ImageBind to learn reasonable auditory query embeddings for\nthe LLM module. To align the output of both visual &amp; audio encoders with LLM's\nembedding space, we train Video-LLaMA on massive video/image-caption pairs as\nwell as visual-instruction-tuning datasets of moderate amount but higher\nquality. We found Video-LLaMA showcases the ability to perceive and comprehend\nvideo content, generating meaningful responses that are grounded in the visual\nand auditory information presented in the videos. This highlights the potential\nof Video-LLaMA as a promising prototype for audio-visual AI assistants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}