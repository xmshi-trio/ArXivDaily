{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-12T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"LexGPT 0.1: pre-trained GPT-J models with Pile of Law. (arXiv:2306.05431v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05431","description":"<p>This research aims to build generative language models specialized for the\nlegal domain. The manuscript presents the development of LexGPT models based on\nGPT-J models and pre-trained with Pile of Law. The foundation model built in\nthis manuscript is the initial step for the development of future applications\nin the legal domain, such as further training with reinforcement learning from\nhuman feedback. Another objective of this manuscript is to assist legal\nprofessionals in utilizing language models through the ``No Code'' approach. By\nfine-tuning models with specialized data and without modifying any source code,\nlegal professionals can create custom language models for downstream tasks with\nminimum effort and technical knowledge. The downstream task in this manuscript\nis to turn a LexGPT model into a classifier, although the performance is\nnotably lower than the state-of-the-art result. How to enhance downstream task\nperformance without modifying the model or its source code is a research topic\nfor future exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jieh-Sheng Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards End-to-end Speech-to-text Summarization. (arXiv:2306.05432v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05432","description":"<p>Speech-to-text (S2T) summarization is a time-saving technique for filtering\nand keeping up with the broadcast news uploaded online on a daily basis. The\nrise of large language models from deep learning with impressive text\ngeneration capabilities has placed the research focus on summarization systems\nthat produce paraphrased compact versions of the document content, also known\nas abstractive summaries. End-to-end (E2E) modelling of S2T abstractive\nsummarization is a promising approach that offers the possibility of generating\nrich latent representations that leverage non-verbal and acoustic information,\nas opposed to the use of only linguistic information from automatically\ngenerated transcripts in cascade systems. However, the few literature on E2E\nmodelling of this task fails on exploring different domains, namely broadcast\nnews, which is challenging domain where large and diversified volumes of data\nare presented to the user every day. We model S2T summarization both with a\ncascade and an E2E system for a corpus of broadcast news in French. Our novel\nE2E model leverages external data by resorting to transfer learning from a\npre-trained T2T summarizer. Experiments show that both our cascade and E2E\nabstractive summarizers are stronger than an extractive baseline. However, the\nperformance of the E2E model still lies behind the cascade one, which is object\nof an extensive analysis that includes future directions to close that gap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_R/0/1/0/all/0/1\">Raul Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pernes_D/0/1/0/all/0/1\">Diogo Pernes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Good is the Model in Model-in-the-loop Event Coreference Resolution Annotation?. (arXiv:2306.05434v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05434","description":"<p>Annotating cross-document event coreference links is a time-consuming and\ncognitively demanding task that can compromise annotation quality and\nefficiency. To address this, we propose a model-in-the-loop annotation approach\nfor event coreference resolution, where a machine learning model suggests\nlikely corefering event pairs only. We evaluate the effectiveness of this\napproach by first simulating the annotation process and then, using a novel\nannotator-centric Recall-Annotation effort trade-off metric, we compare the\nresults of various underlying models and datasets. We finally present a method\nfor obtaining 97\\% recall while substantially reducing the workload required by\na fully manual annotation process. Code and data can be found at\nhttps://github.com/ahmeshaf/model_in_coref\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Shafiuddin Rehan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nath_A/0/1/0/all/0/1\">Abhijnan Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regan_M/0/1/0/all/0/1\">Michael Regan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollins_A/0/1/0/all/0/1\">Adam Pollins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_N/0/1/0/all/0/1\">Nikhil Krishnaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1\">James H. Martin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance. (arXiv:2306.05443v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05443","description":"<p>Although large language models (LLMs) has shown great performance on natural\nlanguage processing (NLP) in the financial domain, there are no publicly\navailable financial tailtored LLMs, instruction tuning datasets, and evaluation\nbenchmarks, which is critical for continually pushing forward the open-source\ndevelopment of financial artificial intelligence (AI). This paper introduces\nPIXIU, a comprehensive framework including the first financial LLM based on\nfine-tuning LLaMA with instruction data, the first instruction data with 136K\ndata samples to support the fine-tuning, and an evaluation benchmark with 5\ntasks and 9 datasets. We first construct the large-scale multi-task instruction\ndata considering a variety of financial tasks, financial document types, and\nfinancial data modalities. We then propose a financial LLM called FinMA by\nfine-tuning LLaMA with the constructed dataset to be able to follow\ninstructions for various financial tasks. To support the evaluation of\nfinancial LLMs, we propose a standardized benchmark that covers a set of\ncritical financial tasks, including five financial NLP tasks and one financial\nprediction task. With this benchmark, we conduct a detailed analysis of FinMA\nand several existing LLMs, uncovering their strengths and weaknesses in\nhandling critical financial tasks. The model, datasets, benchmark, and\nexperimental results are open-sourced to facilitate future research in\nfinancial AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Weiguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yanzhao Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Min Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Lira_A/0/1/0/all/0/1\">Alejandro Lopez-Lira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Phrase Matching for Dysarthric Speech. (arXiv:2306.05446v1 [eess.AS])","link":"http://arxiv.org/abs/2306.05446","description":"<p>Many consumer speech recognition systems are not tuned for people with speech\ndisabilities, resulting in poor recognition and user experience, especially for\nsevere speech differences. Recent studies have emphasized interest in\npersonalized speech models from people with atypical speech patterns. We\npropose a query-by-example-based personalized phrase recognition system that is\ntrained using small amounts of speech, is language agnostic, does not assume a\ntraditional pronunciation lexicon, and generalizes well across speech\ndifference severities. On an internal dataset collected from 32 people with\ndysarthria, this approach works regardless of severity and shows a 60%\nimprovement in recall relative to a commercial speech recognition system. On\nthe public EasyCall dataset of dysarthric speech, our approach improves\naccuracy by 30.5%. Performance degrades as the number of phrases increases, but\nconsistently outperforms ASR systems when trained with 50 unique phrases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lea_C/0/1/0/all/0/1\">Colin Lea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yee_D/0/1/0/all/0/1\">Dianna Yee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Narain_J/0/1/0/all/0/1\">Jaya Narain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zifang Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tooley_L/0/1/0/all/0/1\">Lauren Tooley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bigham_J/0/1/0/all/0/1\">Jeffrey P. Bigham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Findlater_L/0/1/0/all/0/1\">Leah Findlater</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hexatagging: Projective Dependency Parsing as Tagging. (arXiv:2306.05477v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05477","description":"<p>We introduce a novel dependency parser, the hexatagger, that constructs\ndependency trees by tagging the words in a sentence with elements from a finite\nset of possible tags. In contrast to many approaches to dependency parsing, our\napproach is fully parallelizable at training time, i.e., the structure-building\nactions needed to build a dependency parse can be predicted in parallel to each\nother. Additionally, exact decoding is linear in time and space complexity.\nFurthermore, we derive a probabilistic dependency parser that predicts hexatags\nusing no more than a linear model with features from a pretrained language\nmodel, i.e., we forsake a bespoke architecture explicitly designed for the\ntask. Despite the generality and simplicity of our approach, we achieve\nstate-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test\nset. Additionally, our parser's linear time complexity and parallelism\nsignificantly improve computational efficiency, with a roughly 10-times\nspeed-up over previous state-of-the-art models during decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Afra Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Injection attack against LLM-integrated Applications. (arXiv:2306.05499v1 [cs.CR])","link":"http://arxiv.org/abs/2306.05499","description":"<p>Large Language Models (LLMs), renowned for their superior proficiency in\nlanguage comprehension and generation, stimulate a vibrant ecosystem of\napplications around them. However, their extensive assimilation into various\nservices introduces significant security risks. This study deconstructs the\ncomplexities and implications of prompt injection attacks on actual\nLLM-integrated applications. Initially, we conduct an exploratory analysis on\nten commercial applications, highlighting the constraints of current attack\nstrategies in practice. Prompted by these limitations, we subsequently\nformulate HouYi, a novel black-box prompt injection attack technique, which\ndraws inspiration from traditional web injection attacks. HouYi is\ncompartmentalized into three crucial elements: a seamlessly-incorporated\npre-constructed prompt, an injection prompt inducing context partition, and a\nmalicious payload designed to fulfill the attack objectives. Leveraging HouYi,\nwe unveil previously unknown and severe attack outcomes, such as unrestricted\narbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi\non 36 actual LLM-integrated applications and discern 31 applications\nsusceptible to prompt injection. 10 vendors have validated our discoveries,\nincluding Notion, which has the potential to impact millions of users. Our\ninvestigation illuminates both the possible risks of prompt injection attacks\nand the possible tactics for mitigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_G/0/1/0/all/0/1\">Gelei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuekang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yepang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word-Level Explanations for Analyzing Bias in Text-to-Image Models. (arXiv:2306.05500v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05500","description":"<p>Text-to-image models take a sentence (i.e., prompt) and generate images\nassociated with this input prompt. These models have created award wining-art,\nvideos, and even synthetic datasets. However, text-to-image (T2I) models can\ngenerate images that underrepresent minorities based on race and sex. This\npaper investigates which word in the input prompt is responsible for bias in\ngenerated images. We introduce a method for computing scores for each word in\nthe prompt; these scores represent its influence on biases in the model's\noutput. Our method follows the principle of \\emph{explaining by removing},\nleveraging masked language models to calculate the influence scores. We perform\nexperiments on Stable Diffusion to demonstrate that our method identifies the\nreplication of societal stereotypes in generated images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1\">Alexander Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paes_L/0/1/0/all/0/1\">Lucas Monteiro Paes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanneru_S/0/1/0/all/0/1\">Sree Harsha Tanneru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivas_S/0/1/0/all/0/1\">Suraj Srinivas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering. (arXiv:2306.05523v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05523","description":"<p>Combating disinformation is one of the burning societal crises -- about 67%\nof the American population believes that disinformation produces a lot of\nuncertainty, and 10% of them knowingly propagate disinformation. Evidence shows\nthat disinformation can manipulate democratic processes and public opinion,\ncausing disruption in the share market, panic and anxiety in society, and even\ndeath during crises. Therefore, disinformation should be identified promptly\nand, if possible, mitigated. With approximately 3.2 billion images and 720,000\nhours of video shared online daily on social media platforms, scalable\ndetection of multimodal disinformation requires efficient fact verification.\nDespite progress in automatic text-based fact verification (e.g., FEVER, LIAR),\nthe research community lacks substantial effort in multimodal fact\nverification. To address this gap, we introduce FACTIFY 3M, a dataset of 3\nmillion samples that pushes the boundaries of the domain of fact verification\nvia a multimodal fake news dataset, in addition to offering explainability\nthrough the concept of 5W question-answering. Salient features of the dataset\ninclude: (i) textual claims, (ii) ChatGPT-generated paraphrased claims, (iii)\nassociated images, (iv) stable diffusion-generated additional images (i.e.,\nvisual paraphrases), (v) pixel-level image heatmap to foster image-text\nexplainability of the claim, (vi) 5W QA pairs, and (vii) adversarial fake news\nstories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_M/0/1/0/all/0/1\">Megha Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pahwa_K/0/1/0/all/0/1\">Khusbu Pahwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rani_A/0/1/0/all/0/1\">Anku Rani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahor_A/0/1/0/all/0/1\">Adarsh Mahor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakala_A/0/1/0/all/0/1\">Aditya Pakala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Arghya Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_H/0/1/0/all/0/1\">Harshit Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_I/0/1/0/all/0/1\">Ishan Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_J/0/1/0/all/0/1\">Janvita Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurumurthy_P/0/1/0/all/0/1\">Preethi Gurumurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_R/0/1/0/all/0/1\">Ritvik G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Samahriti Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Shreyas Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sensharma_K/0/1/0/all/0/1\">Kinjal Sensharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalal_D/0/1/0/all/0/1\">Dwip Dalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1\">Suryavardan S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shreyash Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1\">Parth Patwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Amitava Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Check Me If You Can: Detecting ChatGPT-Generated Academic Writing using CheckGPT. (arXiv:2306.05524v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05524","description":"<p>With ChatGPT under the spotlight, utilizing large language models (LLMs) for\nacademic writing has drawn a significant amount of discussions and concerns in\nthe community. While substantial research efforts have been stimulated for\ndetecting LLM-Generated Content (LLM-content), most of the attempts are still\nin the early stage of exploration. In this paper, we present a holistic\ninvestigation of detecting LLM-generate academic writing, by providing a\ndataset, evidence, and algorithms, in order to inspire more community effort to\naddress the concern of LLM academic misuse. We first present GPABenchmark, a\nbenchmarking dataset of 600,000 samples of human-written, GPT-written,\nGPT-completed, and GPT-polished abstracts of research papers in CS, physics,\nand humanities and social sciences (HSS). We show that existing open-source and\ncommercial GPT detectors provide unsatisfactory performance on GPABenchmark,\nespecially for GPT-polished text. Moreover, through a user study of 150+\nparticipants, we show that it is highly challenging for human users, including\nexperienced faculty members and researchers, to identify GPT-generated\nabstracts. We then present CheckGPT, a novel LLM-content detector consisting of\na general representation module and an attentive-BiLSTM classification module,\nwhich is accurate, transferable, and interpretable. Experimental results show\nthat CheckGPT achieves an average classification accuracy of 98% to 99% for the\ntask-specific discipline-specific detectors and the unified detectors. CheckGPT\nis also highly transferable that, without tuning, it achieves ~90% accuracy in\nnew domains, such as news articles, while a model tuned with approximately\n2,000 samples in the target domain achieves ~98% accuracy. Finally, we\ndemonstrate the explainability insights obtained from CheckGPT to reveal the\nkey behaviors of how LLM generates texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zijun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data. (arXiv:2306.05535v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05535","description":"<p>A large portion of society united around the same vision and ideas carries\nenormous energy. That is precisely what political figures would like to\naccumulate for their cause. With this goal in mind, they can sometimes resort\nto distorting or hiding the truth, unintentionally or on purpose, which opens\nthe door for misinformation and disinformation. Tools for automatic detection\nof check-worthy claims would be of great help to moderators of debates,\njournalists, and fact-checking organizations. While previous work on detecting\ncheck-worthy claims has focused on text, here we explore the utility of the\naudio signal as an additional information source. We create a new multimodal\ndataset (text and audio in English) containing 48 hours of speech. Our\nevaluation results show that the audio modality together with text yields\nimprovements over text alone in the case of multiple speakers. Moreover, an\naudio-only model could outperform a text-only one for a single speaker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_P/0/1/0/all/0/1\">Petar Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1\">Ivan Koychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization. (arXiv:2306.05537v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05537","description":"<p>The rapid growth of information on the Internet has led to an overwhelming\namount of opinions and comments on various activities, products, and services.\nThis makes it difficult and time-consuming for users to process all the\navailable information when making decisions. Text summarization, a Natural\nLanguage Processing (NLP) task, has been widely explored to help users quickly\nretrieve relevant information by generating short and salient content from long\nor multiple documents. Recent advances in pre-trained language models, such as\nChatGPT, have demonstrated the potential of Large Language Models (LLMs) in\ntext generation. However, LLMs require massive amounts of data and resources\nand are challenging to implement as offline applications. Furthermore, existing\ntext summarization approaches often lack the ``adaptive\" nature required to\ncapture diverse aspects in opinion summarization, which is particularly\ndetrimental to users with specific requirements or preferences. In this paper,\nwe propose an Aspect-adaptive Knowledge-based Opinion Summarization model for\nproduct reviews, which effectively captures the adaptive nature required for\nopinion summarization. The model generates aspect-oriented summaries given a\nset of reviews for a particular product, efficiently providing users with\nuseful information on specific aspects they are interested in, ensuring the\ngenerated summaries are more personalized and informative. Extensive\nexperiments have been conducted using real-world datasets to evaluate the\nproposed model. The results demonstrate that our model outperforms\nstate-of-the-art approaches and is adaptive and efficient in generating\nsummaries that focus on particular aspects, enabling users to make\nwell-informed decisions and catering to their diverse interests and\npreferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_E/0/1/0/all/0/1\">Edmund M-K. Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Quan Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instruction Tuned Models are Quick Learners. (arXiv:2306.05539v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05539","description":"<p>Instruction tuning of language models has demonstrated the ability to enhance\nmodel generalization to unseen tasks via in-context learning using a few\nexamples. However, typical supervised learning still requires a plethora of\ndownstream training data for finetuning. Often in real-world situations, there\nis a scarcity of data available for finetuning, falling somewhere between few\nshot inference and fully supervised finetuning. In this work, we demonstrate\nthe sample efficiency of instruction tuned models over various tasks by\nestimating the minimal downstream training data required by them to perform\ntransfer learning and match the performance of state-of-the-art (SOTA)\nsupervised models. We conduct experiments on 119 tasks from Super Natural\nInstructions (SuperNI) in both the single task learning (STL) and multi task\nlearning (MTL) settings. Our findings reveal that, in the STL setting,\ninstruction tuned models equipped with 25% of the downstream train data surpass\nthe SOTA performance on the downstream tasks. In the MTL setting, an\ninstruction tuned model trained on only 6% of downstream training data achieve\nSOTA, while using 100% of the training data results in a 3.69% points\nimprovement (ROUGE-L 74.68) over the previous SOTA. We conduct an analysis on\nT5 vs Tk-Instruct by developing several baselines to demonstrate that\ninstruction tuning aids in increasing both sample efficiency and transfer\nlearning. Additionally, we observe a consistent ~4% performance increase in\nboth settings when pre-finetuning is performed with instructions. Finally, we\nconduct a categorical study and find that contrary to previous results, tasks\nin the question rewriting and title generation categories suffer from\ninstruction tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawant_S/0/1/0/all/0/1\">Saurabh Arjun Sawant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_M/0/1/0/all/0/1\">Mutsumi Nakamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1\">Arindam Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mashetty_S/0/1/0/all/0/1\">Santosh Mashetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text. (arXiv:2306.05540v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05540","description":"<p>With the rapid progress of large language models (LLMs) and the huge amount\nof text they generated, it becomes more and more impractical to manually\ndistinguish whether a text is machine-generated. Given the growing use of LLMs\nin social media and education, it prompts us to develop methods to detect\nmachine-generated text, preventing malicious usage such as plagiarism,\nmisinformation, and propaganda. Previous work has studied several zero-shot\nmethods, which require no training data. These methods achieve good\nperformance, but there is still a lot of room for improvement. In this paper,\nwe introduce two novel zero-shot methods for detecting machine-generated text\nby leveraging the log rank information. One is called DetectLLM-LRR, which is\nfast and efficient, and the other is called DetectLLM-NPR, which is more\naccurate, but slower due to the need for perturbations. Our experiments on\nthree datasets and seven language models show that our proposed methods improve\nover the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover,\nDetectLLM-NPR needs fewer perturbations than previous work to achieve the same\nlevel of performance, which makes it more practical for real-world use. We also\ninvestigate the efficiency--performance trade-off based on users preference on\nthese two measures and we provide intuition for using them in practice\neffectively. We release the data and the code of both methods in\nhttps://github.com/mbzuai-nlp/DetectLLM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinyan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks. (arXiv:2306.05550v1 [cs.CY])","link":"http://arxiv.org/abs/2306.05550","description":"<p>The rapid deployment of artificial intelligence (AI) models demands a\nthorough investigation of biases and risks inherent in these models to\nunderstand their impact on individuals and society. This study extends the\nfocus of bias evaluation in extant work by examining bias against social\nstigmas on a large scale. It focuses on 93 stigmatized groups in the United\nStates, including a wide range of conditions related to disease, disability,\ndrug use, mental illness, religion, sexuality, socioeconomic status, and other\nrelevant factors. We investigate bias against these groups in English\npre-trained Masked Language Models (MLMs) and their downstream sentiment\nclassification tasks. To evaluate the presence of bias against 93 stigmatized\nconditions, we identify 29 non-stigmatized conditions to conduct a comparative\nanalysis. Building upon a psychology scale of social rejection, the Social\nDistance Scale, we prompt six MLMs: RoBERTa-base, RoBERTa-large, XLNet-large,\nBERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to\nanalyze the predicted words from these models, with which we measure the extent\nof bias against stigmatized groups. When prompts include stigmatized\nconditions, the probability of MLMs predicting negative words is approximately\n20 percent higher than when prompts have non-stigmatized conditions. In the\nsentiment classification tasks, when sentences include stigmatized conditions\nrelated to diseases, disability, education, and mental illness, they are more\nlikely to be classified as negative. We also observe a strong correlation\nbetween bias in MLMs and their downstream sentiment classifiers (r =0.79). The\nevidence indicates that MLMs and their downstream sentiment classification\ntasks exhibit biases against socially stigmatized groups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1\">Katelyn X. Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fereidooni_S/0/1/0/all/0/1\">Sonia Fereidooni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery. (arXiv:2306.05552v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05552","description":"<p>Large language models have been useful in expanding mental health care\ndelivery. ChatGPT, in particular, has gained popularity for its ability to\ngenerate human-like dialogue. However, data-sensitive domains -- including but\nnot limited to healthcare -- face challenges in using ChatGPT due to privacy\nand data-ownership concerns. To enable its utilization, we propose a text\nambiguation framework that preserves user privacy. We ground this in the task\nof addressing stress prompted by user-provided texts to demonstrate the\nviability and helpfulness of privacy-preserved generations. Our results suggest\nthat chatGPT recommendations are still able to be moderately helpful and\nrelevant, even when the original user text is not provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Anaelia Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beikzadeh_M/0/1/0/all/0/1\">Mehrab Beikzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teimouri_P/0/1/0/all/0/1\">Parshan Teimouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarrafzadeh_M/0/1/0/all/0/1\">Majid Sarrafzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion and Sentiment Guided Paraphrasing. (arXiv:2306.05556v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05556","description":"<p>Paraphrase generation, a.k.a. paraphrasing, is a common and important task in\nnatural language processing. Emotional paraphrasing, which changes the emotion\nembodied in a piece of text while preserving its meaning, has many potential\napplications, including moderating online dialogues and preventing\ncyberbullying. We introduce a new task of fine-grained emotional paraphrasing\nalong emotion gradients, that is, altering the emotional intensities of the\nparaphrases in fine-grained settings following smooth variations in affective\ndimensions while preserving the meaning of the original text. We reconstruct\nseveral widely used paraphrasing datasets by augmenting the input and target\ntexts with their fine-grained emotion labels. Then, we propose a framework for\nemotion and sentiment guided paraphrasing by leveraging pre-trained language\nmodels for conditioned text generation. Extensive evaluation of the fine-tuned\nmodels suggests that including fine-grained emotion labels in the paraphrase\ntask significantly improves the likelihood of obtaining high-quality\nparaphrases that reflect the desired emotions while achieving consistently\nbetter scores in paraphrase metrics such as BLEU, ROUGE, and METEOR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Justin J. Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ameeta Agrawal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy- and Utility-Preserving NLP with Anonymized Data: A case study of Pseudonymization. (arXiv:2306.05561v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05561","description":"<p>This work investigates the effectiveness of different pseudonymization\ntechniques, ranging from rule-based substitutions to using pre-trained Large\nLanguage Models (LLMs), on a variety of datasets and models used for two widely\nused NLP tasks: text classification and summarization. Our work provides\ncrucial insights into the gaps between original and anonymized data (focusing\non the pseudonymization technique) and model quality and fosters future\nresearch into higher-quality anonymization techniques to better balance the\ntrade-offs between data protection and utility preservation. We make our code,\npseudonymized datasets, and downstream models publicly available\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yermilov_O/0/1/0/all/0/1\">Oleksandr Yermilov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raheja_V/0/1/0/all/0/1\">Vipul Raheja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernodub_A/0/1/0/all/0/1\">Artem Chernodub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOST: A Mental Health Dataset of Low Self-esteem in Reddit Posts. (arXiv:2306.05596v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05596","description":"<p>Low self-esteem and interpersonal needs (i.e., thwarted belongingness (TB)\nand perceived burdensomeness (PB)) have a major impact on depression and\nsuicide attempts. Individuals seek social connectedness on social media to\nboost and alleviate their loneliness. Social media platforms allow people to\nexpress their thoughts, experiences, beliefs, and emotions. Prior studies on\nmental health from social media have focused on symptoms, causes, and\ndisorders. Whereas an initial screening of social media content for\ninterpersonal risk factors and low self-esteem may raise early alerts and\nassign therapists to at-risk users of mental disturbance. Standardized scales\nmeasure self-esteem and interpersonal needs from questions created using\npsychological theories. In the current research, we introduce a\npsychology-grounded and expertly annotated dataset, LoST: Low Self esTeem, to\nstudy and detect low self-esteem on Reddit. Through an annotation approach\ninvolving checks on coherence, correctness, consistency, and reliability, we\nensure gold-standard for supervised learning. We present results from different\ndeep language models tested using two data augmentation techniques. Our\nfindings suggest developing a class of language models that infuses\npsychological and clinical knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Muskan Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1\">Manas Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_R/0/1/0/all/0/1\">Raxit Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1\">Sunghwan Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Generative Approach to Product Attribute-Value Identification. (arXiv:2306.05605v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05605","description":"<p>Product attribute-value identification (PAVI) has been studied to link\nproducts on e-commerce sites with their attribute values (e.g., &lt;Material,\nCotton&gt;) using product text as clues. Technical demands from real-world\ne-commerce platforms require PAVI methods to handle unseen values,\nmulti-attribute values, and canonicalized values, which are only partly\naddressed in existing extraction- and classification-based approaches.\nMotivated by this, we explore a generative approach to the PAVI task. We\nfinetune a pre-trained generative model, T5, to decode a set of attribute-value\npairs as a target sequence from the given product text. Since the attribute\nvalue pairs are unordered set elements, how to linearize them will matter; we,\nthus, explore methods of composing an attribute-value pair and ordering the\npairs for the task. Experimental results confirm that our generation-based\napproach outperforms the existing extraction and classification-based methods\non large-scale real-world datasets meant for those methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shinzato_K/0/1/0/all/0/1\">Keiji Shinzato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshinaga_N/0/1/0/all/0/1\">Naoki Yoshinaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yandi Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Te Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word sense extension. (arXiv:2306.05609v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05609","description":"<p>Humans often make creative use of words to express novel senses. A\nlong-standing effort in natural language processing has been focusing on word\nsense disambiguation (WSD), but little has been explored about how the sense\ninventory of a word may be extended toward novel meanings. We present a\nparadigm of word sense extension (WSE) that enables words to spawn new senses\ntoward novel context. We develop a framework that simulates novel word sense\nextension by first partitioning a polysemous word type into two pseudo-tokens\nthat mark its different senses, and then inferring whether the meaning of a\npseudo-token can be extended to convey the sense denoted by the token\npartitioned from the same word type. Our framework combines cognitive models of\nchaining with a learning scheme that transforms a language model embedding\nspace to support various types of word sense extension. We evaluate our\nframework against several competitive baselines and show that it is superior in\npredicting plausible novel senses for over 7,500 English words. Furthermore, we\nshow that our WSE framework improves performance over a range of\ntransformer-based WSD models in predicting rare word senses with few or zero\nmentions in the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-rank Adaptation Method for Wav2vec2-based Fake Audio Detection. (arXiv:2306.05617v1 [cs.SD])","link":"http://arxiv.org/abs/2306.05617","description":"<p>Self-supervised speech models are a rapidly developing research topic in fake\naudio detection. Many pre-trained models can serve as feature extractors,\nlearning richer and higher-level speech features. However,when fine-tuning\npre-trained models, there is often a challenge of excessively long training\ntimes and high memory consumption, and complete fine-tuning is also very\nexpensive. To alleviate this problem, we apply low-rank adaptation(LoRA) to the\nwav2vec2 model, freezing the pre-trained model weights and injecting a\ntrainable rank-decomposition matrix into each layer of the transformer\narchitecture, greatly reducing the number of trainable parameters for\ndownstream tasks. Compared with fine-tuning with Adam on the wav2vec2 model\ncontaining 317M training parameters, LoRA achieved similar performance by\nreducing the number of trainable parameters by 198 times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jiangyan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Le Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Ruibo Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Customizing General-Purpose Foundation Models for Medical Report Generation. (arXiv:2306.05642v1 [cs.CV])","link":"http://arxiv.org/abs/2306.05642","description":"<p>Medical caption prediction which can be regarded as a task of medical report\ngeneration (MRG), requires the automatic generation of coherent and accurate\ncaptions for the given medical images. However, the scarcity of labelled\nmedical image-report pairs presents great challenges in the development of deep\nand large-scale neural networks capable of harnessing the potential artificial\ngeneral intelligence power like large language models (LLMs). In this work, we\npropose customizing off-the-shelf general-purpose large-scale pre-trained\nmodels, i.e., foundation models (FMs), in computer vision and natural language\nprocessing with a specific focus on medical report generation. Specifically,\nfollowing BLIP-2, a state-of-the-art vision-language pre-training approach, we\nintroduce our encoder-decoder-based MRG model. This model utilizes a\nlightweight query Transformer to connect two FMs: the giant vision Transformer\nEVA-ViT-g and a bilingual LLM trained to align with human intentions (referred\nto as ChatGLM-6B). Furthermore, we conduct ablative experiments on the\ntrainable components of the model to identify the crucial factors for effective\ntransfer learning. Our findings demonstrate that unfreezing EVA-ViT-g to learn\nmedical image representations, followed by parameter-efficient training of\nChatGLM-6B to capture the writing styles of medical reports, is essential for\nachieving optimal results. Our best attempt (PCLmed Team) achieved the 4th and\nthe 2nd, respectively, out of 13 participating teams, based on the BERTScore\nand ROUGE-1 metrics, in the ImageCLEFmedical Caption 2023 Caption Prediction\nTask competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raza_A/0/1/0/all/0/1\">Asif Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WSPAlign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction. (arXiv:2306.05644v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05644","description":"<p>Most existing word alignment methods rely on manual alignment datasets or\nparallel corpora, which limits their usefulness. Here, to mitigate the\ndependence on manual data, we broaden the source of supervision by relaxing the\nrequirement for correct, fully-aligned, and parallel sentences. Specifically,\nwe make noisy, partially aligned, and non-parallel paragraphs. We then use such\na large-scale weakly-supervised dataset for word alignment pre-training via\nspan prediction. Extensive experiments with various settings empirically\ndemonstrate that our approach, which is named WSPAlign, is an effective and\nscalable way to pre-train word aligners without manual data. When fine-tuned on\nstandard benchmarks, WSPAlign has set a new state-of-the-art by improving upon\nthe best-supervised baseline by 3.3~6.1 points in F1 and 1.5~6.1 points in AER.\nFurthermore, WSPAlign also achieves competitive performance compared with the\ncorresponding baselines in few-shot, zero-shot and cross-lingual tests, which\ndemonstrates that WSPAlign is potentially more practical for low-resource\nlanguages than existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagata_M/0/1/0/all/0/1\">Masaaki Nagata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuruoka_Y/0/1/0/all/0/1\">Yoshimasa Tsuruoka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy Aware Question-Answering System for Online Mental Health Risk Assessment. (arXiv:2306.05652v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05652","description":"<p>Social media platforms have enabled individuals suffering from mental\nillnesses to share their lived experiences and find the online support\nnecessary to cope. However, many users fail to receive genuine clinical\nsupport, thus exacerbating their symptoms. Screening users based on what they\npost online can aid providers in administering targeted healthcare and minimize\nfalse positives. Pre-trained Language Models (LMs) can assess users' social\nmedia data and classify them in terms of their mental health risk. We propose a\nQuestion-Answering (QA) approach to assess mental health risk using the\nUnified-QA model on two large mental health datasets. To protect user data, we\nextend Unified-QA by anonymizing the model training process using differential\nprivacy. Our results demonstrate the effectiveness of modeling risk assessment\nas a QA task, specifically for mental health use cases. Furthermore, the\nmodel's performance decreases by less than 1% with the inclusion of\ndifferential privacy. The proposed system's performance is indicative of a\npromising research direction that will lead to the development of privacy-aware\ndiagnostic systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chhikara_P/0/1/0/all/0/1\">Prateek Chhikara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupulety_U/0/1/0/all/0/1\">Ujjwal Pasupulety</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_J/0/1/0/all/0/1\">John Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaurasia_D/0/1/0/all/0/1\">Dhiraj Chaurasia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumari_S/0/1/0/all/0/1\">Shweta Kumari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models. (arXiv:2306.05659v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05659","description":"<p>Prompt-based learning has been proved to be an effective way in pre-trained\nlanguage models (PLMs), especially in low-resource scenarios like few-shot\nsettings. However, the trustworthiness of PLMs is of paramount significance and\npotential vulnerabilities have been shown in prompt-based templates that could\nmislead the predictions of language models, causing serious security concerns.\nIn this paper, we will shed light on some vulnerabilities of PLMs, by proposing\na prompt-based adversarial attack on manual templates in black box scenarios.\nFirst of all, we design character-level and word-level heuristic approaches to\nbreak manual templates separately. Then we present a greedy algorithm for the\nattack based on the above heuristic destructive approaches. Finally, we\nevaluate our approach with the classification tasks on three variants of BERT\nseries models and eight datasets. And comprehensive experimental results\njustify the effectiveness of our approach in terms of attack success rate and\nattack speed. Further experimental studies indicate that our proposed method\nalso displays good capabilities in scenarios with varying shot counts, template\nlengths and query counts, exhibiting good generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zihao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenbin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongjian Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I run as fast as a rabbit, can you? A Multilingual Simile Dialogue Dataset. (arXiv:2306.05672v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05672","description":"<p>A simile is a figure of speech that compares two different things (called the\ntenor and the vehicle) via shared properties. The tenor and the vehicle are\nusually connected with comparator words such as \"like\" or \"as\". The simile\nphenomena are unique and complex in a real-life dialogue scene where the tenor\nand the vehicle can be verbal phrases or sentences, mentioned by different\nspeakers, exist in different sentences, or occur in reversed order. However,\nthe current simile research usually focuses on similes in a triplet tuple\n(tenor, property, vehicle) or a single sentence where the tenor and vehicle are\nusually entities or noun phrases, which could not reflect complex simile\nphenomena in real scenarios. In this paper, we propose a novel and high-quality\nmultilingual simile dialogue (MSD) dataset to facilitate the study of complex\nsimile phenomena. The MSD is the largest manually annotated simile data\n($\\sim$20K) and it contains both English and Chinese data. Meanwhile, the MSD\ndata can also be used on dialogue tasks to test the ability of dialogue systems\nwhen using similes. We design 3 simile tasks (recognition, interpretation, and\ngeneration) and 2 dialogue tasks (retrieval and generation) with MSD. For each\ntask, we provide experimental results from strong pre-trained or\nstate-of-the-art models. The experiments demonstrate the challenge of MSD and\nwe have released the data/code on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Longxuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuhan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Churui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_C/0/1/0/all/0/1\">Changxin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05685","description":"<p>Evaluating large language model (LLM) based chat assistants is challenging\ndue to their broad capabilities and the inadequacy of existing benchmarks in\nmeasuring human preferences. To address this, we explore using strong LLMs as\njudges to evaluate these models on more open-ended questions. We examine the\nusage and limitations of LLM-as-a-judge, such as position and verbosity biases\nand limited reasoning ability, and propose solutions to migrate some of them.\nWe then verify the agreement between LLM judges and human preferences by\nintroducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot\nArena, a crowdsourced battle platform. Our results reveal that strong LLM\njudges like GPT-4 can match both controlled and crowdsourced human preferences\nwell, achieving over 80\\% agreement, the same level of agreement between\nhumans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate\nhuman preferences, which are otherwise very expensive to obtain. Additionally,\nwe show our benchmark and traditional benchmarks complement each other by\nevaluating several variants of LLaMA/Vicuna. We will publicly release 80\nMT-bench questions, 3K expert votes, and 30K conversations with human\npreferences from Chatbot Arena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lianmin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_W/0/1/0/all/0/1\">Wei-Lin Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1\">Ying Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Siyuan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhanghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yonghao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric. P Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Emotional Representations from Imbalanced Speech Data for Speech Emotion Recognition and Emotional Text-to-Speech. (arXiv:2306.05709v1 [eess.AS])","link":"http://arxiv.org/abs/2306.05709","description":"<p>Effective speech emotional representations play a key role in Speech Emotion\nRecognition (SER) and Emotional Text-To-Speech (TTS) tasks. However, emotional\nspeech samples are more difficult and expensive to acquire compared with\nNeutral style speech, which causes one issue that most related works\nunfortunately neglect: imbalanced datasets. Models might overfit to the\nmajority Neutral class and fail to produce robust and effective emotional\nrepresentations. In this paper, we propose an Emotion Extractor to address this\nissue. We use augmentation approaches to train the model and enable it to\nextract effective and generalizable emotional representations from imbalanced\ndatasets. Our empirical results show that (1) for the SER task, the proposed\nEmotion Extractor surpasses the state-of-the-art baseline on three imbalanced\ndatasets; (2) the produced representations from our Emotion Extractor benefit\nthe TTS model, and enable it to synthesize more expressive speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shijun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu%7B%5Cdh%7Dnason_J/0/1/0/all/0/1\">J&#xf3;n Gu&#xf0;nason</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Borth_D/0/1/0/all/0/1\">Damian Borth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Responses of Large Language Models to Beginner Programmers' Help Requests. (arXiv:2306.05715v1 [cs.CY])","link":"http://arxiv.org/abs/2306.05715","description":"<p>Background and Context: Over the past year, large language models (LLMs) have\ntaken the world by storm. In computing education, like in other walks of life,\nmany opportunities and threats have emerged as a consequence.\n</p>\n<p>Objectives: In this article, we explore such opportunities and threats in a\nspecific area: responding to student programmers' help requests. More\nspecifically, we assess how good LLMs are at identifying issues in problematic\ncode that students request help on.\n</p>\n<p>Method: We collected a sample of help requests and code from an online\nprogramming course. We then prompted two different LLMs (OpenAI Codex and\nGPT-3.5) to identify and explain the issues in the students' code and assessed\nthe LLM-generated answers both quantitatively and qualitatively.\n</p>\n<p>Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently\nfind at least one actual issue in each student program (GPT-3.5 in 90% of the\ncases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57%\nof the time). False positives are common (40% chance for GPT-3.5). The advice\nthat the LLMs provide on the issues is often sensible. The LLMs perform better\non issues involving program logic rather than on output formatting. Model\nsolutions are frequently provided even when the LLM is prompted not to. LLM\nresponses to prompts in a non-English language are only slightly worse than\nresponses to English prompts.\n</p>\n<p>Implications: Our results continue to highlight the utility of LLMs in\nprogramming education. At the same time, the results highlight the\nunreliability of LLMs: LLMs make some of the same mistakes that students do,\nperhaps especially when formatting output as required by automated assessment\nsystems. Our study informs teachers interested in using LLMs as well as future\nefforts to customize LLMs for the needs of programming education.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hellas_A/0/1/0/all/0/1\">Arto Hellas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leinonen_J/0/1/0/all/0/1\">Juho Leinonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarsa_S/0/1/0/all/0/1\">Sami Sarsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutcheme_C/0/1/0/all/0/1\">Charles Koutcheme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kujanpaa_L/0/1/0/all/0/1\">Lilja Kujanp&#xe4;&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorva_J/0/1/0/all/0/1\">Juha Sorva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges and Opportunities for the Design of Smart Speakers. (arXiv:2306.05741v1 [cs.HC])","link":"http://arxiv.org/abs/2306.05741","description":"<p>Advances in voice technology and voice user interfaces (VUIs) -- such as\nAlexa, Siri, and Google Home -- have opened up the potential for many new types\nof interaction. However, despite the potential of these devices reflected by\nthe growing market and body of VUI research, there is a lingering sense that\nthe technology is still underused. In this paper, we conducted a systematic\nliterature review of 35 papers to identify and synthesize 127 VUI design\nguidelines into five themes. Additionally, we conducted semi-structured\ninterviews with 15 smart speaker users to understand their use and non-use of\nthe technology. From the interviews, we distill four design challenges that\ncontribute the most to non-use. Based on their (non-)use, we identify four\nopportunity spaces for designers to explore such as focusing on information\nsupport while multitasking (cooking, driving, childcare, etc), incorporating\nusers' mental models for smart speakers, and integrating calm design\nprinciples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_T/0/1/0/all/0/1\">Tao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilton_L/0/1/0/all/0/1\">Lydia B. Chilton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Time-to-Event Prediction for Chronic Kidney Disease Deterioration. (arXiv:2306.05779v1 [cs.LG])","link":"http://arxiv.org/abs/2306.05779","description":"<p>Deep-learning techniques, particularly the transformer model, have shown\ngreat potential in enhancing the prediction performance of longitudinal health\nrecords. While previous methods have mainly focused on fixed-time risk\nprediction, time-to-event prediction (also known as survival analysis) is often\nmore appropriate for clinical scenarios. Here, we present a novel deep-learning\narchitecture we named STRAFE, a generalizable survival analysis\ntransformer-based architecture for electronic health records. The performance\nof STRAFE was evaluated using a real-world claim dataset of over 130,000\nindividuals with stage 3 chronic kidney disease (CKD) and was found to\noutperform other time-to-event prediction algorithms in predicting the exact\ntime of deterioration to stage 5. Additionally, STRAFE was found to outperform\nbinary outcome algorithms in predicting fixed-time risk, possibly due to its\nability to train on censored data. We show that STRAFE predictions can improve\nthe positive predictive value of high-risk patients by 3-fold, demonstrating\npossible usage to improve targeting for intervention programs. Finally, we\nsuggest a novel visualization approach to predictions on a per-patient basis.\nIn conclusion, STRAFE is a cutting-edge time-to-event prediction algorithm that\nhas the potential to enhance risk predictions in large claims datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zisser_M/0/1/0/all/0/1\">Moshe Zisser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aran_D/0/1/0/all/0/1\">Dvir Aran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. (arXiv:2306.05783v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05783","description":"<p>New Natural Langauge Process~(NLP) benchmarks are urgently needed to align\nwith the rapid development of large language models (LLMs). We present Xiezhi,\nthe most comprehensive evaluation suite designed to assess holistic domain\nknowledge. Xiezhi comprises multiple-choice questions across 516 diverse\ndisciplines ranging from 13 different subjects with 220,000 questions and\naccompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k\nquestions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results\nindicate that LLMs exceed average performance of humans in science,\nengineering, agronomy, medicine, and art, but fall short in economics,\njurisprudence, pedagogy, literature, history, and management. We anticipate\nXiezhi will help analyze important strengths and shortcomings of LLMs, and the\nbenchmark is released in https://github.com/MikeGu721/XiezhiBenchmark .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhouhong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Haoning Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Sihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhuozhi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zihan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weiguo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Hongwei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causality between Sentiment and Cryptocurrency Prices. (arXiv:2306.05803v1 [q-fin.CP])","link":"http://arxiv.org/abs/2306.05803","description":"<p>This study investigates the relationship between narratives conveyed through\nmicroblogging platforms, namely Twitter, and the value of crypto assets. Our\nstudy provides a unique technique to build narratives about cryptocurrency by\ncombining topic modelling of short texts with sentiment analysis. First, we\nused an unsupervised machine learning algorithm to discover the latent topics\nwithin the massive and noisy textual data from Twitter, and then we revealed\n4-5 cryptocurrency-related narratives, including financial investment,\ntechnological advancement related to crypto, financial and political\nregulations, crypto assets, and media coverage. In a number of situations, we\nnoticed a strong link between our narratives and crypto prices. Our work\nconnects the most recent innovation in economics, Narrative Economics, to a new\narea of study that combines topic modelling and sentiment analysis to relate\nconsumer behaviour to narratives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Mondal_L/0/1/0/all/0/1\">Lubdhak Mondal</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Raj_U/0/1/0/all/0/1\">Udeshya Raj</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+S_A/0/1/0/all/0/1\">Abinandhan S</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+S_B/0/1/0/all/0/1\">Began Gowsik S</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+P_S/0/1/0/all/0/1\">Sarwesh P</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Chandra_A/0/1/0/all/0/1\">Abhijeet Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the Exploitation of LLM-based Chatbot for Providing Legal Support to Palestinian Cooperatives. (arXiv:2306.05827v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05827","description":"<p>With the ever-increasing utilization of natural language processing (NLP), we\nstarted to witness over the past few years a significant transformation in our\ninteraction with legal texts. This technology has advanced the analysis and\nenhanced the understanding of complex legal terminology and contexts. The\ndevelopment of recent large language models (LLMs), particularly ChatGPT, has\nalso introduced a revolutionary contribution to the way that legal texts can be\nprocessed and comprehended. In this paper, we present our work on a\ncooperative-legal question-answering LLM-based chatbot, where we developed a\nset of legal questions about Palestinian cooperatives, associated with their\nregulations and compared the auto-generated answers by the chatbot to their\ncorrespondences that are designed by a legal expert. To evaluate the proposed\nchatbot, we have used 50 queries generated by the legal expert and compared the\nanswers produced by the chart to their relevance judgments. Finding\ndemonstrated that an overall accuracy rate of 82% has been achieved when\nanswering the queries, while exhibiting an F1 score equivalent to 79%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qasem_R/0/1/0/all/0/1\">Rabee Qasem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tantour_B/0/1/0/all/0/1\">Banan Tantour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maree_M/0/1/0/all/0/1\">Mohammed Maree</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Large Language Models Infer Causation from Correlation?. (arXiv:2306.05836v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05836","description":"<p>Causal inference is one of the hallmarks of human intelligence. While the\nfield of CausalNLP has attracted much interest in the recent years, existing\ncausal inference datasets in NLP primarily rely on discovering causality from\nempirical knowledge (e.g., commonsense knowledge). In this work, we propose the\nfirst benchmark dataset to test the pure causal inference skills of large\nlanguage models (LLMs). Specifically, we formulate a novel task Corr2Cause,\nwhich takes a set of correlational statements and determines the causal\nrelationship between the variables. We curate a large-scale dataset of more\nthan 400K samples, on which we evaluate seventeen existing LLMs. Through our\nexperiments, we identify a key shortcoming of LLMs in terms of their causal\ninference skills, and show that these models achieve almost close to random\nperformance on the task. This shortcoming is somewhat mitigated when we try to\nre-purpose LLMs for this skill via finetuning, but we find that these models\nstill fail to generalize -- they can only perform causal inference in\nin-distribution settings when variable names and textual expressions used in\nthe queries are similar to those in the training set, but fail in\nout-of-distribution settings generated by perturbing these queries. Corr2Cause\nis a challenging task for LLMs, and would be helpful in guiding future research\non improving LLMs' pure reasoning skills and generalizability. Our data is at\nhttps://huggingface.co/datasets/causalnlp/corr2cause. Our code is at\nhttps://github.com/causalNLP/corr2cause.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiarui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhiheng Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poff_S/0/1/0/all/0/1\">Spencer Poff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Encoder-Decoder and Dual-Path Conformer for Comprehensive Feature Learning in Speech Enhancement. (arXiv:2306.05861v1 [eess.AS])","link":"http://arxiv.org/abs/2306.05861","description":"<p>Current speech enhancement (SE) research has largely neglected channel\nattention and spatial attention, and encoder-decoder architecture-based\nnetworks have not adequately considered how to provide efficient inputs to the\nintermediate enhancement layer. To address these issues, this paper proposes a\ntime-frequency (T-F) domain SE network (DPCFCS-Net) that incorporates improved\ndensely connected blocks, dual-path modules, convolution-augmented transformers\n(conformers), channel attention, and spatial attention. Compared with previous\nmodels, our proposed model has a more efficient encoder-decoder and can learn\ncomprehensive features. Experimental results on the VCTK+DEMAND dataset\ndemonstrate that our method outperforms existing techniques in SE performance.\nFurthermore, the improved densely connected block and two dimensions attention\nmodule developed in this work are highly adaptable and easily integrated into\nexisting networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Junyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect?. (arXiv:2306.05871v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05871","description":"<p>Recent advances in natural language processing (NLP) have led to the\ndevelopment of large language models (LLMs) such as ChatGPT. This paper\nproposes a methodology for developing and evaluating ChatGPT detectors for\nFrench text, with a focus on investigating their robustness on out-of-domain\ndata and against common attack schemes. The proposed method involves\ntranslating an English dataset into French and training a classifier on the\ntranslated data. Results show that the detectors can effectively detect\nChatGPT-generated text, with a degree of robustness against basic attack\ntechniques in in-domain settings. However, vulnerabilities are evident in\nout-of-domain contexts, highlighting the challenge of detecting adversarial\ntext. The study emphasizes caution when applying in-domain testing results to a\nwider variety of content. We provide our translated datasets and models as\nopen-source resources. https://gitlab.inria.fr/wantoun/robust-chatgpt-detection\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antoun_W/0/1/0/all/0/1\">Wissam Antoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouilleron_V/0/1/0/all/0/1\">Virginie Mouilleron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seddah_D/0/1/0/all/0/1\">Djam&#xe9; Seddah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good, but not always Fair: An Evaluation of Gender Bias for three commercial Machine Translation Systems. (arXiv:2306.05882v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05882","description":"<p>Machine Translation (MT) continues to make significant strides in quality and\nis increasingly adopted on a larger scale. Consequently, analyses have been\nredirected to more nuanced aspects, intricate phenomena, as well as potential\nrisks that may arise from the widespread use of MT tools. Along this line, this\npaper offers a meticulous assessment of three commercial MT systems - Google\nTranslate, DeepL, and Modern MT - with a specific focus on gender translation\nand bias. For three language pairs (English/Spanish, English/Italian, and\nEnglish/French), we scrutinize the behavior of such systems at several levels\nof granularity and on a variety of naturally occurring gender phenomena in\ntranslation. Our study takes stock of the current state of online MT tools, by\nrevealing significant discrepancies in the gender translation of the three\nsystems, with each system displaying varying degrees of bias despite their\noverall translation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piazzolla_S/0/1/0/all/0/1\">Silvia Alma Piazzolla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savoldi_B/0/1/0/all/0/1\">Beatrice Savoldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bentivogli_L/0/1/0/all/0/1\">Luisa Bentivogli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Speech Separation Network Based on Recurrent Fusion Dilated Convolution and Channel Attention. (arXiv:2306.05887v1 [eess.AS])","link":"http://arxiv.org/abs/2306.05887","description":"<p>We present an efficient speech separation neural network, ARFDCN, which\ncombines dilated convolutions, multi-scale fusion (MSF), and channel attention\nto overcome the limited receptive field of convolution-based networks and the\nhigh computational cost of transformer-based networks. The suggested network\narchitecture is encoder-decoder based. By using dilated convolutions with\ngradually increasing dilation value to learn local and global features and\nfusing them at adjacent stages, the model can learn rich feature content.\nMeanwhile, by adding channel attention modules to the network, the model can\nextract channel weights, learn more important features, and thus improve its\nexpressive power and robustness. Experimental results indicate that the model\nachieves a decent balance between performance and computational efficiency,\nmaking it a promising alternative to current mainstream models for practical\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Junyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Can Learn Exceptions to Syntactic Rules. (arXiv:2306.05969v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05969","description":"<p>Artificial neural networks can generalize productively to novel contexts. Can\nthey also learn exceptions to those productive rules? We explore this question\nusing the case of restrictions on English passivization (e.g., the fact that\n\"The vacation lasted five days\" is grammatical, but \"*Five days was lasted by\nthe vacation\" is not). We collect human acceptability judgments for passive\nsentences with a range of verbs, and show that the probability distribution\ndefined by GPT-2, a language model, matches the human judgments with high\ncorrelation. We also show that the relative acceptability of a verb in the\nactive vs. passive voice is positively correlated with the relative frequency\nof its occurrence in those voices. These results provide preliminary support\nfor the entrenchment hypothesis, according to which learners track and uses the\ndistributional properties of their input to learn negative exceptions to rules.\nAt the same time, this hypothesis fails to explain the magnitude of\nunpassivizability demonstrated by certain individual verbs, suggesting that\nother cues to exceptionality are available in the linguistic input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leong_C/0/1/0/all/0/1\">Cara Su-Yi Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Labeling of German Chest X-Ray Radiology Reports using Deep Learning. (arXiv:2306.05997v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05997","description":"<p>Radiologists are in short supply globally, and deep learning models offer a\npromising solution to address this shortage as part of clinical\ndecision-support systems. However, training such models often requires\nexpensive and time-consuming manual labeling of large datasets. Automatic label\nextraction from radiology reports can reduce the time required to obtain\nlabeled datasets, but this task is challenging due to semantically similar\nwords and missing annotated data. In this work, we explore the potential of\nweak supervision of a deep learning-based label prediction model, using a\nrule-based labeler. We propose a deep learning-based CheXpert label prediction\nmodel, pre-trained on reports labeled by a rule-based German CheXpert model and\nfine-tuned on a small dataset of manually labeled reports. Our results\ndemonstrate the effectiveness of our approach, which significantly outperformed\nthe rule-based model on all three tasks. Our findings highlight the benefits of\nemploying deep learning-based models even in scenarios with sparse data and the\nuse of the rule-based labeler as a tool for weak supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wollek_A/0/1/0/all/0/1\">Alessandro Wollek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haitzer_P/0/1/0/all/0/1\">Philip Haitzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedlmeyr_T/0/1/0/all/0/1\">Thomas Sedlmeyr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyska_S/0/1/0/all/0/1\">Sardi Hyska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckel_J/0/1/0/all/0/1\">Johannes Rueckel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabel_B/0/1/0/all/0/1\">Bastian Sabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingrisch_M/0/1/0/all/0/1\">Michael Ingrisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasser_T/0/1/0/all/0/1\">Tobias Lasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence for Digital Medicine. (arXiv:2306.06029v1 [cs.CL])","link":"http://arxiv.org/abs/2306.06029","description":"<p>Providing high quality explanations for AI predictions based on machine\nlearning is a challenging and complex task. To work well it requires, among\nother factors: selecting a proper level of generality/specificity of the\nexplanation; considering assumptions about the familiarity of the explanation\nbeneficiary with the AI task under consideration; referring to specific\nelements that have contributed to the decision; making use of additional\nknowledge (e.g. expert evidence) which might not be part of the prediction\nprocess; and providing evidence supporting negative hypothesis. Finally, the\nsystem needs to formulate the explanation in a clearly interpretable, and\npossibly convincing, way. Given these considerations, ANTIDOTE fosters an\nintegrated vision of explainable AI, where low-level characteristics of the\ndeep learning process are combined with higher level schemes proper of the\nhuman argumentation capacity. ANTIDOTE will exploit cross-disciplinary\ncompetences in deep learning and argumentation to support a broader and\ninnovative view of explainable AI, where the need for high-quality explanations\nfor clinical cases deliberation is critical. As a first result of the project,\nwe publish the Antidote CasiMedicos dataset to facilitate research on\nexplainable AI in general, and argumentation in the medical domain in\nparticular.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1\">I&#xf1;igo Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atutxa_A/0/1/0/all/0/1\">Aitziber Atutxa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrondo_A/0/1/0/all/0/1\">Ander Berrondo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estarrona_A/0/1/0/all/0/1\">Ainara Estarrona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Ferrero_I/0/1/0/all/0/1\">Iker Garcia-Ferrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goenaga_I/0/1/0/all/0/1\">Iakes Goenaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gojenola_K/0/1/0/all/0/1\">Koldo Gojenola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oronoz_M/0/1/0/all/0/1\">Maite Oronoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Tejedor_I/0/1/0/all/0/1\">Igor Perez-Tejedor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigau_G/0/1/0/all/0/1\">German Rigau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeginbergenova_A/0/1/0/all/0/1\">Anar Yeginbergenova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FinGPT: Open-Source Financial Large Language Models. (arXiv:2306.06031v1 [q-fin.ST])","link":"http://arxiv.org/abs/2306.06031","description":"<p>Large language models (LLMs) have shown the potential of revolutionizing\nnatural language processing tasks in diverse domains, sparking great interest\nin finance. Accessing high-quality financial data is the first challenge for\nfinancial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken\nadvantage of their unique data accumulation, such privileged access calls for\nan open-source alternative to democratize Internet-scale financial data.\n</p>\n<p>In this paper, we present an open-source large language model, FinGPT, for\nthe finance sector. Unlike proprietary models, FinGPT takes a data-centric\napproach, providing researchers and practitioners with accessible and\ntransparent resources to develop their FinLLMs. We highlight the importance of\nan automatic data curation pipeline and the lightweight low-rank adaptation\ntechnique in building FinGPT. Furthermore, we showcase several potential\napplications as stepping stones for users, such as robo-advising, algorithmic\ntrading, and low-code development. Through collaborative efforts within the\nopen-source AI4Finance community, FinGPT aims to stimulate innovation,\ndemocratize FinLLMs, and unlock new opportunities in open finance. Two\nassociated code repos are \\url{https://github.com/AI4Finance-Foundation/FinGPT}\nand \\url{https://github.com/AI4Finance-Foundation/FinNLP}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Yang_H/0/1/0/all/0/1\">Hongyang Yang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Liu_X/0/1/0/all/0/1\">Xiao-Yang Liu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Wang_C/0/1/0/all/0/1\">Christina Dan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assisting Language Learners: Automated Trans-Lingual Definition Generation via Contrastive Prompt Learning. (arXiv:2306.06058v1 [cs.CL])","link":"http://arxiv.org/abs/2306.06058","description":"<p>The standard definition generation task requires to automatically produce\nmono-lingual definitions (e.g., English definitions for English words), but\nignores that the generated definitions may also consist of unfamiliar words for\nlanguage learners. In this work, we propose a novel task of Trans-Lingual\nDefinition Generation (TLDG), which aims to generate definitions in another\nlanguage, i.e., the native speaker's language. Initially, we explore the\nunsupervised manner of this task and build up a simple implementation of\nfine-tuning the multi-lingual machine translation model. Then, we develop two\nnovel methods, Prompt Combination and Contrastive Prompt Learning, for further\nenhancing the quality of the generation. Our methods are evaluated against the\nbaseline Pipeline method in both rich- and low-resource settings, and we\nempirically establish its superiority in generating higher-quality\ntrans-lingual definitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hengyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_C/0/1/0/all/0/1\">Chenming Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chufan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind2Web: Towards a Generalist Agent for the Web. (arXiv:2306.06070v1 [cs.CL])","link":"http://arxiv.org/abs/2306.06070","description":"<p>We introduce Mind2Web, the first dataset for developing and evaluating\ngeneralist agents for the web that can follow language instructions to complete\ncomplex tasks on any website. Existing datasets for web agents either use\nsimulated websites or only cover a limited set of websites and tasks, thus not\nsuitable for generalist web agents. With over 2,000 open-ended tasks collected\nfrom 137 websites spanning 31 domains and crowdsourced action sequences for the\ntasks, Mind2Web provides three necessary ingredients for building generalist\nweb agents: 1) diverse domains, websites, and tasks, 2) use of real-world\nwebsites instead of simulated and simplified ones, and 3) a broad spectrum of\nuser interaction patterns. Based on Mind2Web, we conduct an initial exploration\nof using large language models (LLMs) for building generalist web agents. While\nthe raw HTML of real-world websites are often too large to be fed to LLMs, we\nshow that first filtering it with a small LM significantly improves the\neffectiveness and efficiency of LLMs. Our solution demonstrates a decent level\nof performance, even on websites or entire domains the model has never seen\nbefore, but there is still a substantial room to improve towards truly\ngeneralizable agents. We open-source our dataset, model implementation, and\ntrained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further\nresearch on building a generalist agent for the web.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Boyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1\">Samuel Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boshi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually-Grounded Descriptions Improve Zero-Shot Image Classification. (arXiv:2306.06077v1 [cs.CV])","link":"http://arxiv.org/abs/2306.06077","description":"<p>Language-vision models like CLIP have made significant progress in zero-shot\nvision tasks, such as zero-shot image classification (ZSIC). However,\ngenerating specific and expressive class descriptions remains a major\nchallenge. Existing approaches suffer from granularity and label ambiguity\nissues. To tackle these challenges, we propose V-GLOSS: Visual Glosses, a novel\nmethod leveraging modern language models and semantic knowledge bases to\nproduce visually-grounded class descriptions. We demonstrate V-GLOSS's\neffectiveness by achieving state-of-the-art results on benchmark ZSIC datasets\nincluding ImageNet and STL-10. In addition, we introduce a silver dataset with\nclass descriptions generated by V-GLOSS, and show its usefulness for vision\ntasks. We make available our code and dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ogezi_M/0/1/0/all/0/1\">Michael Ogezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauer_B/0/1/0/all/0/1\">Bradley Hauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondrak_G/0/1/0/all/0/1\">Grzegorz Kondrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Fairness and Robustness in End-to-End Speech Recognition through unsupervised clustering. (arXiv:2306.06083v1 [cs.SD])","link":"http://arxiv.org/abs/2306.06083","description":"<p>The challenge of fairness arises when Automatic Speech Recognition (ASR)\nsystems do not perform equally well for all sub-groups of the population. In\nthe past few years there have been many improvements in overall speech\nrecognition quality, but without any particular focus on advancing Equality and\nEquity for all user groups for whom systems do not perform well. ASR fairness\nis therefore also a robustness issue. Meanwhile, data privacy also takes\npriority in production systems. In this paper, we present a privacy preserving\napproach to improve fairness and robustness of end-to-end ASR without using\nmetadata, zip codes, or even speaker or utterance embeddings directly in\ntraining. We extract utterance level embeddings using a speaker ID model\ntrained on a public dataset, which we then use in an unsupervised fashion to\ncreate acoustic clusters. We use cluster IDs instead of speaker utterance\nembeddings as extra features during model training, which shows improvements\nfor all demographic groups and in particular for different accents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Veliche_I/0/1/0/all/0/1\">Irina-Elena Veliche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trapping LLM Hallucinations Using Tagged Context Prompts. (arXiv:2306.06085v1 [cs.CL])","link":"http://arxiv.org/abs/2306.06085","description":"<p>Recent advances in large language models (LLMs), such as ChatGPT, have led to\nhighly sophisticated conversation agents. However, these models suffer from\n\"hallucinations,\" where the model generates false or fabricated information.\nAddressing this challenge is crucial, particularly with AI-driven platforms\nbeing adopted across various sectors. In this paper, we propose a novel method\nto recognize and flag instances when LLMs perform outside their domain\nknowledge, and ensuring users receive accurate information.\n</p>\n<p>We find that the use of context combined with embedded tags can successfully\ncombat hallucinations within generative language models. To do this, we\nbaseline hallucination frequency in no-context prompt-response pairs using\ngenerated URLs as easily-tested indicators of fabricated data. We observed a\nsignificant reduction in overall hallucination when context was supplied along\nwith question prompts for tested generative engines. Lastly, we evaluated how\nplacing tags within contexts impacted model responses and were able to\neliminate hallucinations in responses with 98.88% effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feldman_P/0/1/0/all/0/1\">Philip Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foulds_J/0/1/0/all/0/1\">James R. Foulds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shimei Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Developing Speech Processing Pipelines for Police Accountability. (arXiv:2306.06086v1 [cs.CL])","link":"http://arxiv.org/abs/2306.06086","description":"<p>Police body-worn cameras have the potential to improve accountability and\ntransparency in policing. Yet in practice, they result in millions of hours of\nfootage that is never reviewed. We investigate the potential of large\npre-trained speech models for facilitating reviews, focusing on ASR and officer\nspeech detection in footage from traffic stops. Our proposed pipeline includes\ntraining data alignment and filtering, fine-tuning with resource constraints,\nand combining officer speech detection with ASR for a fully automated approach.\nWe find that (1) fine-tuning strongly improves ASR performance on officer\nspeech (WER=12-13%), (2) ASR on officer speech is much more accurate than on\ncommunity member speech (WER=43.55-49.07%), (3) domain-specific tasks like\nofficer speech detection and diarization remain challenging. Our work offers\npractical applications for reviewing body camera footage and general guidance\nfor adapting pre-trained speech models to noisy multi-speaker domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+San_N/0/1/0/all/0/1\">Nay San</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eberhardt_J/0/1/0/all/0/1\">Jennifer L. Eberhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding. (arXiv:2306.06094v1 [cs.CV])","link":"http://arxiv.org/abs/2306.06094","description":"<p>Recently, large language models (LLMs) have made significant advancements in\nnatural language understanding and generation. However, their potential in\ncomputer vision remains largely unexplored. In this paper, we introduce a new,\nexploratory approach that enables LLMs to process images using the Scalable\nVector Graphics (SVG) format. By leveraging the XML-based textual descriptions\nof SVG representations instead of raster images, we aim to bridge the gap\nbetween the visual and textual modalities, allowing LLMs to directly understand\nand manipulate images without the need for parameterized visual components. Our\nmethod facilitates simple image classification, generation, and in-context\nlearning using only LLM capabilities. We demonstrate the promise of our\napproach across discriminative and generative tasks, highlighting its (i)\nrobustness against distribution shift, (ii) substantial improvements achieved\nby tapping into the in-context learning abilities of LLMs, and (iii) image\nunderstanding and generation capabilities with human guidance. Our code, data,\nand models can be found here https://github.com/mu-cai/svg-llm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Mu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zeyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Semiotics. (arXiv:2008.10522v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.10522","description":"<p>Recognizing a basic difference between the semiotics of humans and machines\npresents a possibility to overcome the shortcomings of current speech assistive\ndevices. For the machine, the meaning of a (human) utterance is defined by its\nown scope of actions. Machines, thus, do not need to understand the\nconventional meaning of an utterance. Rather, they draw conversational\nimplicatures in the sense of (neo-)Gricean pragmatics. For speech assistive\ndevices, the learning of machine-specific meanings of human utterances, i.e.\nthe fossilization of conversational implicatures into conventionalized ones by\ntrial and error through lexicalization appears to be sufficient. Using the\nquite trivial example of a cognitive heating device, we show that - based on\ndynamic semantics - this process can be formalized as the reinforcement\nlearning of utterance-meaning pairs (UMP).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Graben_P/0/1/0/all/0/1\">Peter beim Graben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_Liebl_M/0/1/0/all/0/1\">Markus Huber-Liebl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klimczak_P/0/1/0/all/0/1\">Peter Klimczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wirsching_G/0/1/0/all/0/1\">G&#xfc;nther Wirsching</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-NER : Contextual Phrase Generation at Scale. (arXiv:2109.08079v4 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2109.08079","description":"<p>Named Entity Recognition (NER) has seen significant progress in recent years,\nwith numerous state-of-the-art (SOTA) models achieving high performance.\nHowever, very few studies have focused on the generation of entities' context.\nIn this paper, we introduce CONTEXT-NER, a task that aims to generate the\nrelevant context for entities in a sentence, where the context is a phrase\ndescribing the entity but not necessarily present in the sentence. To\nfacilitate research in this task, we also present the EDGAR10-Q dataset, which\nconsists of annual and quarterly reports from the top 1500 publicly traded\ncompanies. The dataset is the largest of its kind, containing 1M sentences,\n2.8M entities, and an average of 35 tokens per sentence, making it a\nchallenging dataset. We propose a baseline approach that combines a phrase\ngeneration algorithm with inferencing using a 220M language model, achieving a\nROUGE-L score of 27% on the test split. Additionally, we perform a one-shot\ninference with ChatGPT, which obtains a 30% ROUGE-L, highlighting the\ndifficulty of the dataset. We also evaluate models such as T5 and BART, which\nachieve a maximum ROUGE-L of 49% after supervised finetuning on EDGAR10-Q. We\nalso find that T5-large, when pre-finetuned on EDGAR10-Q, achieve SOTA results\non downstream finance tasks such as Headline, FPB, and FiQA SA, outperforming\nvanilla version by 10.81 points. To our surprise, this 66x smaller\npre-finetuned model also surpasses the finance-specific LLM BloombergGPT-50B by\n15 points. We hope that our dataset and generated artifacts will encourage\nfurther research in this direction, leading to the development of more\nsophisticated language models for financial text analysis\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1\">Shreyas Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mashetty_S/0/1/0/all/0/1\">Santosh Mashetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delving Deeper into Cross-lingual Visual Question Answering. (arXiv:2202.07630v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07630","description":"<p>Visual question answering (VQA) is one of the crucial vision-and-language\ntasks. Yet, existing VQA research has mostly focused on the English language,\ndue to a lack of suitable evaluation resources. Previous work on cross-lingual\nVQA has reported poor zero-shot transfer performance of current multilingual\nmultimodal Transformers with large gaps to monolingual performance, without any\ndeeper analysis. In this work, we delve deeper into the different aspects of\ncross-lingual VQA, aiming to understand the impact of 1) modeling methods and\nchoices, including architecture, inductive bias, fine-tuning; 2) learning\nbiases: including question types and modality biases in cross-lingual setups.\nThe key results of our analysis are: 1) We show that simple modifications to\nthe standard training setup can substantially reduce the transfer gap to\nmonolingual English performance, yielding +10 accuracy points over existing\nmethods. 2) We analyze cross-lingual VQA across different question types of\nvarying complexity for different multilingual multimodal Transformers, and\nidentify question types that are the most difficult to improve on. 3) We\nprovide an analysis of modality biases present in training data and models,\nrevealing why zero-shot performance gaps remain for certain question types and\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arithmetic-Based Pretraining -- Improving Numeracy of Pretrained Language Models. (arXiv:2205.06733v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.06733","description":"<p>State-of-the-art pretrained language models tend to perform below their\ncapabilities when applied out-of-the-box on tasks that require understanding\nand working with numbers. Recent work suggests two main reasons for this: (1)\npopular tokenisation algorithms have limited expressiveness for numbers, and\n(2) common pretraining objectives do not target numeracy. Approaches that\naddress these shortcomings usually require architectural changes or pretraining\nfrom scratch. In this paper, we propose a new extended pretraining approach\ncalled Arithmetic-Based Pretraining that jointly addresses both in one extended\npretraining step without requiring architectural changes or pretraining from\nscratch. Arithmetic-Based Pretraining combines contrastive learning to improve\nthe number representation, and a novel extended pretraining objective called\nInferable Number Prediction Task to improve numeracy. Our experiments show the\neffectiveness of Arithmetic-Based Pretraining in three different tasks that\nrequire improved numeracy, i.e., reading comprehension in the DROP dataset,\ninference-on-tables in the InfoTabs dataset, and table-to-text generation in\nthe WikiBio and SciGen datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrak_D/0/1/0/all/0/1\">Dominic Petrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_N/0/1/0/all/0/1\">Nafise Sadat Moosavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08657","description":"<p>Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a deep cross-modal encoder, or feed the\nlast-layer uni-modal representations from the deep pre-trained uni-modal\nencoders into the top cross-modal encoder. Both approaches potentially restrict\nvision-language representation learning and limit model performance. In this\npaper, we propose BridgeTower, which introduces multiple bridge layers that\nbuild a connection between the top layers of uni-modal encoders and each layer\nof the cross-modal encoder. This enables effective bottom-up cross-modal\nalignment and fusion between visual and textual representations of different\nsemantic levels of pre-trained uni-modal encoders in the cross-modal encoder.\nPre-trained with only 4M images, BridgeTower achieves state-of-the-art\nperformance on various downstream vision-language tasks. In particular, on the\nVQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming\nthe previous state-of-the-art model METER by 1.09% with the same pre-training\ndata and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, BridgeTower achieves an accuracy of\n81.15%, surpassing models that are pre-trained on orders-of-magnitude larger\ndatasets. Code and checkpoints are available at\nhttps://github.com/microsoft/BridgeTower.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenman_S/0/1/0/all/0/1\">Shachar Rosenman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars. (arXiv:2212.09140v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09140","description":"<p>We study grammar induction with mildly context-sensitive grammars for\nunsupervised discontinuous parsing. Using the probabilistic linear context-free\nrewriting system (LCFRS) formalism, our approach fixes the rule structure in\nadvance and focuses on parameter learning with maximum likelihood. To reduce\nthe computational complexity of both parsing and parameter estimation, we\nrestrict the grammar formalism to LCFRS-2 (i.e., binary LCFRS with fan-out two)\nand further discard rules that require O(n^6) time to parse, reducing inference\nto O(n^5). We find that using a large number of nonterminals is beneficial and\nthus make use of tensor decomposition-based rank-space dynamic programming with\nan embedding-based parameterization of rule probabilities to scale up the\nnumber of nonterminals. Experiments on German and Dutch show that our approach\nis able to induce linguistically meaningful trees with continuous and\ndiscontinuous structures\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger P. Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization. (arXiv:2212.10449v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10449","description":"<p>In long document controllable summarization, where labeled data is scarce,\npretrained models struggle to adapt to the task and effectively respond to user\nqueries. In this paper, we introduce Socratic pretraining, a question-driven,\nunsupervised pretraining objective specifically designed to improve\ncontrollability in summarization tasks. By training a model to generate and\nanswer relevant questions in a given context, Socratic pretraining enables the\nmodel to more effectively adhere to user-provided queries and identify relevant\ncontent to be summarized. We demonstrate the effectiveness of this approach\nthrough extensive experimentation on two summarization domains, short stories\nand dialogue, and multiple control strategies: keywords, questions, and factoid\nQA pairs. Our pretraining method relies only on unlabeled documents and a\nquestion generation system and outperforms pre-finetuning approaches that use\nadditional supervised data. Furthermore, our results show that Socratic\npretraining cuts task-specific labeled data requirements in half, is more\nfaithful to user-provided queries, and achieves state-of-the-art performance on\nQMSum and SQuALITY.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pagnoni_A/0/1/0/all/0/1\">Artidoro Pagnoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-Preserving Domain Adaptation of Semantic Parsers. (arXiv:2212.10520v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10520","description":"<p>Task-oriented dialogue systems often assist users with personal or\nconfidential matters. For this reason, the developers of such a system are\ngenerally prohibited from observing actual usage. So how can they know where\nthe system is failing and needs more training data or new functionality? In\nthis work, we study ways in which realistic user utterances can be generated\nsynthetically, to help increase the linguistic and functional coverage of the\nsystem, without compromising the privacy of actual users. To this end, we\npropose a two-stage Differentially Private (DP) generation method which first\ngenerates latent semantic parses, and then generates utterances based on the\nparses. Our proposed approach improves MAUVE by 2.5$\\times$ and parse tree\nfunction type overlap by 1.3$\\times$ relative to current approaches for private\nsynthetic data generation, improving both on fluency and semantic coverage. We\nfurther validate our approach on a realistic domain adaptation task of adding\nnew functionality from private user data to a semantic parser, and show overall\ngains of 8.5% points in accuracy with the new feature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_R/0/1/0/all/0/1\">Richard Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning. (arXiv:2301.11660v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11660","description":"<p>As the size of the pre-trained language model (PLM) continues to increase,\nnumerous parameter-efficient transfer learning methods have been proposed\nrecently to compensate for the tremendous cost of fine-tuning. Despite the\nimpressive results achieved by large pre-trained language models (PLMs) and\nvarious parameter-efficient transfer learning (PETL) methods on sundry\nbenchmarks, it remains unclear if they can handle inputs that have been\ndistributionally shifted effectively. In this study, we systematically explore\nhow the ability to detect out-of-distribution (OOD) changes as the size of the\nPLM grows or the transfer methods are altered. Specifically, we evaluated\nvarious PETL techniques, including fine-tuning, Adapter, LoRA, and\nprefix-tuning, on three different intention classification tasks, each\nutilizing various language models with different scales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunsoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Choonghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junyeop Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyuhng Joon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-goo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Abstraction and Reasoning through Language. (arXiv:2303.04091v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2303.04091","description":"<p>While Artificial Intelligence (AI) models have achieved human or even\nsuperhuman performance in narrowly defined applications, they still struggle to\nshow signs of broader and more flexible intelligence. The Abstraction and\nReasoning Corpus (ARC), introduced by Fran\\c{c}ois Chollet, aims to assess how\nclose AI systems are to human-like cognitive abilities. Most current approaches\nrely on carefully handcrafted domain-specific languages (DSLs), which are used\nto brute-force solutions to the tasks present in ARC. In this work, we propose\na general framework for solving ARC based on natural language descriptions of\nthe tasks. While not yet beating state-of-the-art DSL models on ARC, we\ndemonstrate the immense potential of our approach hinted at by the ability to\nsolve previously unsolved tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camposampiero_G/0/1/0/all/0/1\">Giacomo Camposampiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houmard_L/0/1/0/all/0/1\">Loic Houmard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estermann_B/0/1/0/all/0/1\">Benjamin Estermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathys_J/0/1/0/all/0/1\">Jo&#xeb;l Mathys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AUTODIAL: Efficient Asynchronous Task-Oriented Dialogue Model. (arXiv:2303.06245v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.06245","description":"<p>As large dialogue models become commonplace in practice, the problems\nsurrounding high compute requirements for training, inference and larger memory\nfootprint still persists. In this work, we present AUTODIAL, a multi-task\ndialogue model that addresses the challenges of deploying dialogue model.\nAUTODIAL utilizes parallel decoders to perform tasks such as dialogue act\nprediction, domain prediction, intent prediction, and dialogue state tracking.\nUsing classification decoders over generative decoders allows AUTODIAL to\nsignificantly reduce memory footprint and achieve faster inference times\ncompared to existing generative approach namely SimpleTOD. We demonstrate that\nAUTODIAL provides 3-6x speedups during inference while having 11x fewer\nparameters on three dialogue tasks compared to SimpleTOD. Our results show that\nextending current dialogue models to have parallel decoders can be a viable\nalternative for deploying them in resource-constrained environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhargava_P/0/1/0/all/0/1\">Prajjwal Bhargava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_P/0/1/0/all/0/1\">Pooyan Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayandeh_S/0/1/0/all/0/1\">Shahin Shayandeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models. (arXiv:2304.13835v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.13835","description":"<p>Current dialogue research primarily studies pairwise (two-party)\nconversations, and does not address the everyday setting where more than two\nspeakers converse together. In this work, we both collect and evaluate\nmulti-party conversations to study this more general case. We use the LIGHT\nenvironment to construct grounded conversations, where each participant has an\nassigned character to role-play. We thus evaluate the ability of language\nmodels to act as one or more characters in such conversations. Models require\ntwo skills that pairwise-trained models appear to lack: (1) being able to\ndecide when to talk; (2) producing coherent utterances grounded on multiple\ncharacters. We compare models trained on our new dataset to existing\npairwise-trained dialogue models, as well as large language models with\nfew-shot prompting. We find that our new dataset, MultiLIGHT, which we will\npublicly release, can help bring significant improvements in the group setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jimmy Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urbanek_J/0/1/0/all/0/1\">Jack Urbanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Mojtaba Komeili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01555","description":"<p>Scaling language models have revolutionized widespread NLP tasks, yet little\ncomprehensively explored few-shot relation extraction with large language\nmodels. In this paper, we investigate principal methodologies, in-context\nlearning and data generation, for few-shot relation extraction via GPT-3.5\nthrough exhaustive experiments. To enhance few-shot performance, we further\npropose task-related instructions and schema-constrained data generation. We\nobserve that in-context learning can achieve performance on par with previous\nprompt learning approaches, and data generation with the large language model\ncan boost previous solutions to obtain new state-of-the-art few-shot results on\nfour widely-studied relation extraction datasets. We hope our work can inspire\nfuture research for the capabilities of large language models in few-shot\nrelation extraction. Code is available in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Multi-bit Natural Language Watermarking through Invariant Features. (arXiv:2305.01904v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01904","description":"<p>Recent years have witnessed a proliferation of valuable original natural\nlanguage contents found in subscription-based media outlets, web novel\nplatforms, and outputs of large language models. However, these contents are\nsusceptible to illegal piracy and potential misuse without proper security\nmeasures. This calls for a secure watermarking system to guarantee copyright\nprotection through leakage tracing or ownership identification. To effectively\ncombat piracy and protect copyrights, a multi-bit watermarking framework should\nbe able to embed adequate bits of information and extract the watermarks in a\nrobust manner despite possible corruption. In this work, we explore ways to\nadvance both payload and robustness by following a well-known proposition from\nimage watermarking and identify features in natural language that are invariant\nto minor corruption. Through a systematic analysis of the possible sources of\nerrors, we further propose a corruption-resistant infill model. Our full method\nimproves upon the previous work on robustness by +16.8% point on average on\nfour datasets, three corruption types, and two corruption ratios. Code\navailable at https://github.com/bangawayoo/nlp-watermarking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">KiYoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_W/0/1/0/all/0/1\">Wonhyuk Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Jiho Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data. (arXiv:2305.03827v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03827","description":"<p>Jointly extracting entity pairs and their relations is challenging when\nworking on distantly-supervised data with ambiguous or noisy labels. To\nmitigate such impact, we propose uncertainty-aware bootstrap learning, which is\nmotivated by the intuition that the higher uncertainty of an instance, the more\nlikely the model confidence is inconsistent with the ground truths.\nSpecifically, we first explore instance-level data uncertainty to create an\ninitial high-confident examples. Such subset serves as filtering noisy\ninstances and facilitating the model to converge fast at the early stage.\nDuring bootstrap learning, we propose self-ensembling as a regularizer to\nalleviate inter-model uncertainty produced by noisy labels. We further define\nprobability variance of joint tagging probabilities to estimate inner-model\nparametric uncertainty, which is used to select and build up new reliable\ntraining instances for the next iteration. Experimental results on two large\ndatasets reveal that our approach outperforms existing strong baselines and\nrelated methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Do In-Context Examples Affect Compositional Generalization?. (arXiv:2305.04835v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04835","description":"<p>Compositional generalization--understanding unseen combinations of seen\nprimitives--is an essential reasoning capability in human intelligence. The AI\ncommunity mainly studies this capability by fine-tuning neural networks on lots\nof training samples, while it is still unclear whether and how in-context\nlearning--the prevailing few-shot paradigm based on large language\nmodels--exhibits compositional generalization. In this paper, we present CoFe,\na test suite to investigate in-context compositional generalization. We find\nthat the compositional generalization performance can be easily affected by the\nselection of in-context examples, thus raising the research question what the\nkey factors are to make good in-context examples for compositional\ngeneralization. We study three potential factors: similarity, diversity and\ncomplexity. Our systematic experiments indicate that in-context examples should\nbe structurally similar to the test case, diverse from each other, and\nindividually simple. Furthermore, two strong limitations are observed:\nin-context compositional generalization on fictional words is much weaker than\nthat on commonly used ones; it is still critical that the in-context examples\nshould cover required linguistic structures, even though the backbone model has\nbeen pre-trained on large corpus. We hope our analysis would facilitate the\nunderstanding and utilization of in-context learning paradigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shengnan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Implicit Sentiment with Chain-of-Thought Prompting. (arXiv:2305.11255v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11255","description":"<p>While sentiment analysis systems try to determine the sentiment polarities of\ngiven targets based on the key opinion expressions in input texts, in implicit\nsentiment analysis (ISA) the opinion cues come in an implicit and obscure\nmanner. Thus detecting implicit sentiment requires the common-sense and\nmulti-hop reasoning ability to infer the latent intent of opinion. Inspired by\nthe recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop\nReasoning (THOR) CoT framework to mimic the human-like reasoning process for\nISA. We design a three-step prompting principle for THOR to step-by-step induce\nthe implicit aspect, opinion, and finally the sentiment polarity. Our\nTHOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on\nsupervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50%\nF1 on zero-shot setting. Our code is open at\nhttps://github.com/scofield7419/THOR-ISA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech-Text Dialog Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment. (arXiv:2305.11579v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11579","description":"<p>Recently, speech-text pre-training methods have shown remarkable success in\nmany speech and natural language processing tasks. However, most previous\npre-trained models are usually tailored for one or two specific tasks, but fail\nto conquer a wide range of speech-text tasks. In addition, existing speech-text\npre-training methods fail to explore the contextual information within a\ndialogue to enrich utterance representations. In this paper, we propose\nSpeech-text dialog Pre-training for spoken dialog understanding with ExpliCiT\ncRoss-Modal Alignment (SPECTRA), which is the first-ever speech-text dialog\npre-training model. Concretely, to consider the temporality of speech modality,\nwe design a novel temporal position prediction task to capture the speech-text\nalignment. This pre-training task aims to predict the start and end time of\neach textual word in the corresponding speech waveform. In addition, to learn\nthe characteristics of spoken dialogs, we generalize a response selection task\nfrom textual dialog pre-training to speech-text dialog pre-training scenarios.\nExperimental results on four different downstream speech-text tasks demonstrate\nthe superiority of SPECTRA in learning speech-text alignment and multi-turn\ndialog context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianshu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Haoyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wentao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiTIN: Hierarchy-aware Tree Isomorphism Network for Hierarchical Text Classification. (arXiv:2305.15182v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15182","description":"<p>Hierarchical text classification (HTC) is a challenging subtask of\nmulti-label classification as the labels form a complex hierarchical structure.\nExisting dual-encoder methods in HTC achieve weak performance gains with huge\nmemory overheads and their structure encoders heavily rely on domain knowledge.\nUnder such observation, we tend to investigate the feasibility of a\nmemory-friendly model with strong generalization capability that could boost\nthe performance of HTC without prior statistics or label semantics. In this\npaper, we propose Hierarchy-aware Tree Isomorphism Network (HiTIN) to enhance\nthe text representations with only syntactic information of the label\nhierarchy. Specifically, we convert the label hierarchy into an unweighted tree\nstructure, termed coding tree, with the guidance of structural entropy. Then we\ndesign a structure encoder to incorporate hierarchy-aware information in the\ncoding tree into text representations. Besides the text encoder, HiTIN only\ncontains a few multi-layer perceptions and linear transformations, which\ngreatly saves memory. We conduct experiments on three commonly used datasets\nand the results demonstrate that HiTIN could achieve better test performance\nand less memory consumption than state-of-the-art (SOTA) methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">He Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junran Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning. (arXiv:2305.18170v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18170","description":"<p>Chain-of-thought (CoT) prompting with large language models has proven\neffective in numerous natural language processing tasks, but designing prompts\nthat generalize well to diverse problem types can be challenging, especially in\nthe context of math word problem (MWP) solving. Additionally, it is common to\nhave a large amount of training data that have a better diversity coverage but\nCoT annotations are not available, which limits the use of supervised learning\ntechniques. To address these issues, we investigate two approaches to leverage\nthe training data in a few-shot prompting scenario: dynamic program prompting\nand program distillation. Our approach is largely inspired by Gao et al.,\n(2022), where they proposed to replace the CoT with the programs as the\nintermediate reasoning step. Such a prompting strategy allows us to accurately\nverify the answer correctness through program execution in MWP solving. Our\ndynamic program prompting involves annotating the training data by sampling\ncorrect programs from a large language model, while program distillation\ninvolves adapting a smaller model to the program-annotated training data. Our\nexperiments on three standard MWP datasets demonstrate the effectiveness of\nthese approaches, yielding significant improvements over previous baselines for\nprompting and fine-tuning. Our results suggest that leveraging a large amount\nof training data can improve the generalization ability of prompts and boost\nthe performance of fine-tuned small models in MWP solving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zhanming Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements. (arXiv:2306.01985v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01985","description":"<p>Warning: This paper contains content that may be offensive or upsetting.\nUnderstanding the harms and offensiveness of statements requires reasoning\nabout the social and situational context in which statements are made. For\nexample, the utterance \"your English is very good\" may implicitly signal an\ninsult when uttered by a white man to a non-white colleague, but uttered by an\nESL teacher to their student would be interpreted as a genuine compliment. Such\ncontextual factors have been largely ignored by previous approaches to toxic\nlanguage detection. We introduce COBRA frames, the first context-aware\nformalism for explaining the intents, reactions, and harms of offensive or\nbiased statements grounded in their social and situational context. We create\nCOBRACORPUS, a dataset of 33k potentially offensive statements paired with\nmachine-generated contexts and free-text explanations of offensiveness, implied\nbiases, speaker intents, and listener reactions. To study the contextual\ndynamics of offensiveness, we train models to generate COBRA explanations, with\nand without access to the context. We find that explanations by\ncontext-agnostic models are significantly worse than by context-aware ones,\nespecially in situations where the context inverts the statement's\noffensiveness (29% accuracy drop). Our work highlights the importance and\nfeasibility of contextualized NLP by modeling social factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuhui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yerukola_A/0/1/0/all/0/1\">Akhila Yerukola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_T/0/1/0/all/0/1\">Thomas Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CELDA: Leveraging Black-box Language Model as Enhanced Classifier without Labels. (arXiv:2306.02693v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02693","description":"<p>Utilizing language models (LMs) without internal access is becoming an\nattractive paradigm in the field of NLP as many cutting-edge LMs are released\nthrough APIs and boast a massive scale. The de-facto method in this type of\nblack-box scenario is known as prompting, which has shown progressive\nperformance enhancements in situations where data labels are scarce or\nunavailable. Despite their efficacy, they still fall short in comparison to\nfully supervised counterparts and are generally brittle to slight\nmodifications. In this paper, we propose Clustering-enhanced Linear\nDiscriminative Analysis, a novel approach that improves the text classification\naccuracy with a very weak-supervision signal (i.e., name of the labels). Our\nframework draws a precise decision boundary without accessing weights or\ngradients of the LM model or data labels. The core ideas of CELDA are twofold:\n(1) extracting a refined pseudo-labeled dataset from an unlabeled dataset, and\n(2) training a lightweight and robust model on the top of LM, which learns an\naccurate decision boundary from an extracted noisy dataset. Throughout in-depth\ninvestigations on various datasets, we demonstrated that CELDA reaches new\nstate-of-the-art in weakly-supervised text classification and narrows the gap\nwith a fully-supervised model. Additionally, our proposed methodology can be\napplied universally to any LM and has the potential to scale to larger models,\nmaking it a more viable option for utilizing large LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunsoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youna Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-goo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs. (arXiv:2306.03984v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.03984","description":"<p>Measurement of interaction quality is a critical task for the improvement of\nspoken dialog systems. Existing approaches to dialog quality estimation either\nfocus on evaluating the quality of individual turns, or collect dialog-level\nquality measurements from end users immediately following an interaction. In\ncontrast to these approaches, we introduce a new dialog-level annotation\nworkflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate\nthe quality of dialogs as a whole, and also label dialogs for attributes such\nas goal completion and user sentiment. In this contribution, we show that: (i)\nwhile dialog quality cannot be completely decomposed into dialog-level\nattributes, there is a strong relationship between some objective dialog\nattributes and judgments of dialog quality; (ii) for the task of dialog-level\nquality estimation, a supervised model trained on dialog-level annotations\noutperforms methods based purely on aggregating turn-level features; and (iii)\nthe proposed evaluation model shows better domain generalization ability\ncompared to the baselines. On the basis of these results, we argue that having\nhigh-quality human-annotated data is an important component of evaluating\ninteraction quality for large industrial-scale voice assistant platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Komma_A/0/1/0/all/0/1\">Abishek Komma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekarasastry_N/0/1/0/all/0/1\">Nagesh Panyam Chandrasekarasastry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leffel_T/0/1/0/all/0/1\">Timothy Leffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Anuj Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metallinou_A/0/1/0/all/0/1\">Angeliki Metallinou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsoukas_S/0/1/0/all/0/1\">Spyros Matsoukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Reliability of Watermarks for Large Language Models. (arXiv:2306.04634v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.04634","description":"<p>As LLMs become commonplace, machine-generated text has the potential to flood\nthe internet with spam, social media bots, and valueless content. Watermarking\nis a simple and effective strategy for mitigating such harms by enabling the\ndetection and documentation of LLM-generated text. Yet a crucial question\nremains: How reliable is watermarking in realistic settings in the wild? There,\nwatermarked text may be modified to suit a user's needs, or entirely rewritten\nto avoid detection.\n</p>\n<p>We study the robustness of watermarked text after it is re-written by humans,\nparaphrased by a non-watermarked LLM, or mixed into a longer hand-written\ndocument. We find that watermarks remain detectable even after human and\nmachine paraphrasing. While these attacks dilute the strength of the watermark,\nparaphrases are statistically likely to leak n-grams or even longer fragments\nof the original text, resulting in high-confidence detections when enough\ntokens are observed. For example, after strong human paraphrasing the watermark\nis detectable after observing 800 tokens on average, when setting a 1e-5 false\npositive rate. We also consider a range of new detection schemes that are\nsensitive to short spans of watermarked text embedded inside a large document,\nand we compare the robustness of watermarking to other kinds of detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirchenbauer_J/0/1/0/all/0/1\">John Kirchenbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1\">Manli Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saifullah_K/0/1/0/all/0/1\">Khalid Saifullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kezhi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_K/0/1/0/all/0/1\">Kasun Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Aniruddha Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RRWKV: Capturing Long-range Dependencies in RWKV. (arXiv:2306.05176v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.05176","description":"<p>Owing to the impressive dot-product attention, the Transformers have been the\ndominant architectures in various natural language processing (NLP) tasks.\nRecently, the Receptance Weighted Key Value (RWKV) architecture follows a\nnon-transformer architecture to eliminate the drawbacks of dot-product\nattention, where memory and computational complexity exhibits quadratic scaling\nwith sequence length. Although RWKV has exploited a linearly tensor-product\nattention mechanism and achieved parallelized computations by deploying the\ntime-sequential mode, it fails to capture long-range dependencies because of\nits limitation on looking back at previous information, compared with full\ninformation obtained by direct interactions in the standard transformer.\nTherefore, the paper devises the Retrospected Receptance Weighted Key Value\n(RRWKV) architecture via incorporating the retrospecting ability into the RWKV\nto effectively absorb information, which maintains memory and computational\nefficiency as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leilei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}