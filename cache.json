{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-10T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Synthesis of Mathematical programs from Natural Language Specifications. (arXiv:2304.03287v1 [cs.AI])","link":"http://arxiv.org/abs/2304.03287","description":"<p>Several decision problems that are encountered in various business domains\ncan be modeled as mathematical programs, i.e. optimization problems. The\nprocess of conducting such modeling often requires the involvement of experts\ntrained in operations research and advanced algorithms. Surprisingly, despite\nthe significant advances in the methods for program and code synthesis, AutoML,\nlearning to optimize etc., there has been little or no attention paid to\nautomating the task of synthesizing mathematical programs. We imagine a\nscenario where the specifications for modeling, i.e. the objective and\nconstraints are expressed in an unstructured form in natural language (NL) and\nthe mathematical program has to be synthesized from such an NL specification.\nIn this work we evaluate the efficacy of employing CodeT5 with data\naugmentation and post-processing of beams. We utilize GPT-3 with back\ntranslation for generation of synthetic examples. Further we apply rules of\nlinear programming to score beams and correct beams based on common error\npatterns. We observe that with these enhancements CodeT5 base gives an\nexecution accuracy of 0.73 which is significantly better than zero-shot\nexecution accuracy of 0.41 by ChatGPT and 0.36 by Codex.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasath_G/0/1/0/all/0/1\">Ganesh Prasath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karande_S/0/1/0/all/0/1\">Shirish Karande</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT-Crawler: Find out if ChatGPT really knows what it's talking about. (arXiv:2304.03325v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03325","description":"<p>Large language models have gained considerable interest for their impressive\nperformance on various tasks. Among these models, ChatGPT developed by OpenAI\nhas become extremely popular among early adopters who even regard it as a\ndisruptive technology in many fields like customer service, education,\nhealthcare, and finance. It is essential to comprehend the opinions of these\ninitial users as it can provide valuable insights into the potential strengths,\nweaknesses, and success or failure of the technology in different areas. This\nresearch examines the responses generated by ChatGPT from different\nConversational QA corpora. The study employed BERT similarity scores to compare\nthese responses with correct answers and obtain Natural Language Inference(NLI)\nlabels. Evaluation scores were also computed and compared to determine the\noverall performance of GPT-3 \\&amp; GPT-4. Additionally, the study identified\ninstances where ChatGPT provided incorrect answers to questions, providing\ninsights into areas where the model may be prone to error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rangapur_A/0/1/0/all/0/1\">Aman Rangapur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoran Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Evaluations of ChatGPT and Emotion-enhanced Prompting for Mental Health Analysis. (arXiv:2304.03347v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03347","description":"<p>Automated mental health analysis shows great potential for enhancing the\nefficiency and accessibility of mental health care, whereas the recent dominant\nmethods utilized pre-trained language models (PLMs) as the backbone and\nincorporated emotional information. The latest large language models (LLMs),\nsuch as ChatGPT, exhibit dramatic capabilities on diverse natural language\nprocessing tasks. However, existing studies on ChatGPT's zero-shot performance\nfor mental health analysis have limitations in inadequate evaluation,\nutilization of emotional information, and explainability of methods. In this\nwork, we comprehensively evaluate the mental health analysis and emotional\nreasoning ability of ChatGPT on 11 datasets across 5 tasks, including binary\nand multi-class mental health condition detection, cause/factor detection of\nmental health conditions, emotion recognition in conversations, and causal\nemotion entailment. We empirically analyze the impact of different prompting\nstrategies with emotional cues on ChatGPT's mental health analysis ability and\nexplainability. Experimental results show that ChatGPT outperforms traditional\nneural network methods but still has a significant gap with advanced\ntask-specific methods. The qualitative analysis shows its potential in\nexplainability compared with advanced black-box methods but also limitations on\nrobustness and inaccurate reasoning. Prompt engineering with emotional cues is\nfound to be effective in improving its performance on mental health analysis\nbut requires the proper way of emotion infusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Opinion Mining and Topic Classification of Course Reviews. (arXiv:2304.03394v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03394","description":"<p>Student opinions for a course are important to educators and administrators,\nregardless of the type of the course or the institution. Reading and manually\nanalyzing open-ended feedback becomes infeasible for massive volumes of\ncomments at institution level or online forums. In this paper, we collected and\npre-processed a large number of course reviews publicly available online. We\napplied machine learning techniques with the goal to gain insight into student\nsentiments and topics. Specifically, we utilized current Natural Language\nProcessing (NLP) techniques, such as word embeddings and deep neural networks,\nand state-of-the-art BERT (Bidirectional Encoder Representations from\nTransformers), RoBERTa (Robustly optimized BERT approach) and XLNet\n(Generalized Auto-regression Pre-training). We performed extensive\nexperimentation to compare these techniques versus traditional approaches. This\ncomparative study demonstrates how to apply modern machine learning approaches\nfor sentiment polarity extraction and topic-based classification utilizing\ncourse feedback. For sentiment polarity, the top model was RoBERTa with 95.5\\%\naccuracy and 84.7\\% F1-macro, while for topic classification, an SVM (Support\nVector Machine) was the top classifier with 79.8\\% accuracy and 80.6\\%\nF1-macro. We also provided an in-depth exploration of the effect of certain\nhyperparameters on the model performance and discussed our observations. These\nfindings can be used by institutions and course providers as a guide for\nanalyzing their own course feedback using NLP models towards self-evaluation\nand improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koufakou_A/0/1/0/all/0/1\">Anna Koufakou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using LSTM and GRU With a New Dataset for Named Entity Recognition in the Arabic Language. (arXiv:2304.03399v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03399","description":"<p>Named entity recognition (NER) is a natural language processing task (NLP),\nwhich aims to identify named entities and classify them like person, location,\norganization, etc. In the Arabic language, we can find a considerable size of\nunstructured data, and it needs to different preprocessing tool than languages\nlike (English, Russian, German...). From this point, we can note the importance\nof building a new structured dataset to solve the lack of structured data. In\nthis work, we use the BIOES format to tag the word, which allows us to handle\nthe nested name entity that consists of more than one sentence and define the\nstart and the end of the name. The dataset consists of more than thirty-six\nthousand records. In addition, this work proposes long short term memory (LSTM)\nunits and Gated Recurrent Units (GRU) for building the named entity recognition\nmodel in the Arabic language. The models give an approximately good result\n(80%) because LSTM and GRU models can find the relationships between the words\nof the sentence. Also, use a new library from Google, which is Trax and\nplatform Colab\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1\">Alaa Shaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldarf_A/0/1/0/all/0/1\">Alaa Aldarf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bessmertny_I/0/1/0/all/0/1\">Igor Bessmertny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAPOT: Creating Robust Dense Query Encoders using Post Training Contrastive Alignment. (arXiv:2304.03401v1 [cs.IR])","link":"http://arxiv.org/abs/2304.03401","description":"<p>The success of contextual word representations and advances in neural\ninformation retrieval have made dense vector-based retrieval a standard\napproach for passage and document ranking. While effective and efficient,\ndual-encoders are brittle to variations in query distributions and noisy\nqueries. Data augmentation can make models more robust but introduces overhead\nto training set generation and requires retraining and index regeneration. We\npresent Contrastive Alignment POst Training (CAPOT), a highly efficient\nfinetuning method that improves model robustness without requiring index\nregeneration, the training set optimization, or alteration. CAPOT enables\nrobust retrieval by freezing the document encoder while the query encoder\nlearns to align noisy queries with their unaltered root. We evaluate CAPOT\nnoisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval,\nfinding CAPOT has a similar impact as data augmentation with none of its\noverhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magnani_A/0/1/0/all/0/1\">Alessandro Magnani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Corpus-Scale Discovery of Selection Biases in News Coverage: Comparing What Sources Say About Entities as a Start. (arXiv:2304.03414v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03414","description":"<p>News sources undergo the process of selecting newsworthy information when\ncovering a certain topic. The process inevitably exhibits selection biases,\ni.e. news sources' typical patterns of choosing what information to include in\nnews coverage, due to their agenda differences. To understand the magnitude and\nimplications of selection biases, one must first discover (1) on what topics do\nsources typically have diverging definitions of \"newsworthy\" information, and\n(2) do the content selection patterns correlate with certain attributes of the\nnews sources, e.g. ideological leaning, etc.\n</p>\n<p>The goal of the paper is to investigate and discuss the challenges of\nbuilding scalable NLP systems for discovering patterns of media selection\nbiases directly from news content in massive-scale news corpora, without\nrelying on labeled data. To facilitate research in this domain, we propose and\nstudy a conceptual framework, where we compare how sources typically mention\ncertain controversial entities, and use such as indicators for the sources'\ncontent selection preferences. We empirically show the capabilities of the\nframework through a case study on NELA-2020, a corpus of 1.8M news articles in\nEnglish from 519 news sources worldwide. We demonstrate an unsupervised\nrepresentation learning method to capture the selection preferences for how\nsources typically mention controversial entities. Our experiments show that\nthat distributional divergence of such representations, when studied\ncollectively across entities and news sources, serve as good indicators for an\nindividual source's ideological leaning. We hope our findings will provide\ninsights for future research on media selection biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruno_W/0/1/0/all/0/1\">William Bruno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cleansing Jewel: A Neural Spelling Correction Model Built On Google OCR-ed Tibetan Manuscripts. (arXiv:2304.03427v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03427","description":"<p>Scholars in the humanities rely heavily on ancient manuscripts to study\nhistory, religion, and socio-political structures in the past. Many efforts\nhave been devoted to digitizing these precious manuscripts using OCR\ntechnology, but most manuscripts were blemished over the centuries so that an\nOptical Character Recognition (OCR) program cannot be expected to capture faded\ngraphs and stains on pages. This work presents a neural spelling correction\nmodel built on Google OCR-ed Tibetan Manuscripts to auto-correct OCR-ed noisy\noutput. This paper is divided into four sections: dataset, model architecture,\ntraining and analysis. First, we feature-engineered our raw Tibetan etext\ncorpus into two sets of structured data frames -- a set of paired toy data and\na set of paired real data. Then, we implemented a Confidence Score mechanism\ninto the Transformer architecture to perform spelling correction tasks.\nAccording to the Loss and Character Error Rate, our Transformer + Confidence\nscore mechanism architecture proves to be superior to Transformer, LSTM-2-LSTM\nand GRU-2-GRU architectures. Finally, to examine the robustness of our model,\nwe analyzed erroneous tokens, visualized Attention and Self-Attention heatmaps\nin our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1\">Queenie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. (arXiv:2304.03439v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03439","description":"<p>Harnessing logical reasoning ability is a comprehensive natural language\nunderstanding endeavor. With the release of Generative Pretrained Transformer 4\n(GPT-4), highlighted as \"advanced\" at reasoning tasks, we are eager to learn\nthe GPT-4 performance on various logical reasoning tasks. This report analyses\nmultiple logical reasoning datasets, with popular benchmarks like LogiQA and\nReClor, and newly-released datasets like AR-LSAT. We test the multi-choice\nreading comprehension and natural language inference tasks with benchmarks\nrequiring logical reasoning. We further construct a logical reasoning\nout-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4.\nWe also make a performance comparison between ChatGPT and GPT-4. Experiment\nresults show that ChatGPT performs significantly better than the RoBERTa\nfine-tuning method on most logical reasoning benchmarks. GPT-4 shows even\nhigher performance on our manual tests. Among benchmarks, ChatGPT and GPT-4 do\nrelatively well on well-known datasets like LogiQA and ReClor. However, the\nperformance drops significantly when handling newly released and\nout-of-distribution datasets. Logical reasoning remains challenging for ChatGPT\nand GPT-4, especially on out-of-distribution and natural language inference\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanmeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_R/0/1/0/all/0/1\">Ruoxi Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhiyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiji Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linking Representations with Multimodal Contrastive Learning. (arXiv:2304.03464v1 [cs.CV])","link":"http://arxiv.org/abs/2304.03464","description":"<p>Many applications require grouping instances contained in diverse document\ndatasets into classes. Most widely used methods do not employ deep learning and\ndo not exploit the inherently multimodal nature of documents. Notably, record\nlinkage is typically conceptualized as a string-matching problem. This study\ndevelops CLIPPINGS, (Contrastively Linking Pooled Pre-trained Embeddings), a\nmultimodal framework for record linkage. CLIPPINGS employs end-to-end training\nof symmetric vision and language bi-encoders, aligned through contrastive\nlanguage-image pre-training, to learn a metric space where the pooled\nimage-text representation for a given instance is close to representations in\nthe same class and distant from representations in different classes. At\ninference time, instances can be linked by retrieving their nearest neighbor\nfrom an offline exemplar embedding index or by clustering their\nrepresentations. The study examines two challenging applications: constructing\ncomprehensive supply chains for mid-20th century Japan through linking firm\nlevel financial records - with each firm name represented by its crop in the\ndocument image and the corresponding OCR - and detecting which image-caption\npairs in a massive corpus of historical U.S. newspapers came from the same\nunderlying photo wire source. CLIPPINGS outperforms widely used string matching\nmethods by a wide margin and also outperforms unimodal methods. Moreover, a\npurely self-supervised model trained on only image-OCR pairs also outperforms\npopular string-matching methods without requiring any labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Abhishek Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinmei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jheng_S/0/1/0/all/0/1\">Shao Yu Jheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dell_M/0/1/0/all/0/1\">Melissa Dell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Catalogue Generation for Literature Review: A Benchmark. (arXiv:2304.03512v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03512","description":"<p>Multi-document scientific summarization can extract and organize important\ninformation from an abundant collection of papers, arousing widespread\nattention recently. However, existing efforts focus on producing lengthy\noverviews lacking a clear and logical hierarchy. To alleviate this problem, we\npresent an atomic and challenging task named Hierarchical Catalogue Generation\nfor Literature Review (HiCatGLR), which aims to generate a hierarchical\ncatalogue for a review paper given various references. We carefully construct a\nnovel English Hierarchical Catalogues of Literature Reviews Dataset (HiCaD)\nwith 13.8k literature review catalogues and 120k reference papers, where we\nbenchmark diverse experiments via the end-to-end and pipeline methods. To\naccurately assess the model performance, we design evaluation metrics for\nsimilarity to ground truth from semantics and structure. Besides, our extensive\nanalyses verify the high quality of our dataset and the effectiveness of our\nevaluation metrics. Furthermore, we discuss potential directions for this task\nto motivate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiachong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yingsheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers. (arXiv:2304.03518v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03518","description":"<p>This paper describes our submission to Task 10 at SemEval 2023-Explainable\nDetection of Online Sexism (EDOS), divided into three subtasks. The recent rise\nin social media platforms has seen an increase in disproportionate levels of\nsexism experienced by women on social media platforms. This has made detecting\nand explaining online sexist content more important than ever to make social\nmedia safer and more accessible for women. Our approach consists of\nexperimenting and finetuning BERT-based models and using a Majority Voting\nensemble model that outperforms individual baseline model scores. Our system\nachieves a macro F1 score of 0.8392 for Task A, 0.6092 for Task B, and 0.4319\nfor Task C.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rallabandi_S/0/1/0/all/0/1\">Sriya Rallabandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Sanchit Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_P/0/1/0/all/0/1\">Pratinav Seth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Retrieval to Generation: Efficient and Effective Entity Set Expansion. (arXiv:2304.03531v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03531","description":"<p>Entity Set Expansion (ESE) is a critical task aiming to expand entities of\nthe target semantic class described by a small seed entity set. Most existing\nESE methods are retrieval-based frameworks that need to extract the contextual\nfeatures of entities and calculate the similarity between seed entities and\ncandidate entities. To achieve the two purposes, they should iteratively\ntraverse the corpus and the entity vocabulary provided in the datasets,\nresulting in poor efficiency and scalability. The experimental results indicate\nthat the time consumed by the retrieval-based ESE methods increases linearly\nwith entity vocabulary and corpus size. In this paper, we firstly propose a\ngenerative ESE framework, Generative Entity Set Expansion (GenExpan), which\nutilizes a generative pre-trained language model to accomplish ESE task.\nSpecifically, a prefix tree is employed to guarantee the validity of entity\ngeneration, and automatically generated class names are adopted to guide the\nmodel to generate target entities. Moreover, we propose Knowledge Calibration\nand Generative Ranking to further bridge the gap between generic knowledge of\nthe language model and the goal of ESE task. Experiments on publicly available\ndatasets show that GenExpan is efficient and effective. For efficiency,\nexpansion time consumed by GenExpan is independent of entity vocabulary and\ncorpus size, and GenExpan achieves an average 600% speedup compared to strong\nbaselines. For expansion performance, our framework outperforms previous\nstate-of-the-art ESE methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shulin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shirong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual Topic Modeling. (arXiv:2304.03544v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03544","description":"<p>Cross-lingual topic models have been prevalent for cross-lingual text\nanalysis by revealing aligned latent topics. However, most existing methods\nsuffer from producing repetitive topics that hinder further analysis and\nperformance decline caused by low-coverage dictionaries. In this paper, we\npropose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM).\nInstead of the direct alignment in previous work, we propose a topic alignment\nwith mutual information method. This works as a regularization to properly\nalign topics and prevent degenerate topic representations of words, which\nmitigates the repetitive topic issue. To address the low-coverage dictionary\nissue, we further propose a cross-lingual vocabulary linking method that finds\nmore linked cross-lingual words for topic alignment beyond the translations of\na given dictionary. Extensive experiments on English, Chinese, and Japanese\ndatasets demonstrate that our method outperforms state-of-the-art baselines,\nproducing more coherent, diverse, and well-aligned topics and showing better\ntransferability for cross-lingual classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaobao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinshuai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chaoqun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liangming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEMINI: Controlling the Sentence-level Writing Style for Abstractive Text Summarization. (arXiv:2304.03548v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03548","description":"<p>Human experts write summaries using different techniques, including rewriting\na sentence in the document or fusing multiple sentences to generate a summary\nsentence. These techniques are flexible and thus difficult to be imitated by\nany single method. To address this issue, we propose an adaptive model, GEMINI,\nthat integrates a rewriter and a fuser to mimic the sentence rewriting and\nfusion techniques, respectively. GEMINI adaptively chooses to rewrite a\nspecific document sentence or generate a summary sentence from scratch.\nExperiments demonstrate that our adaptive approach outperforms the pure\nabstractive and rewriting baselines on various benchmark datasets, especially\nwhen the dataset has a balanced distribution of styles. Interestingly,\nempirical results show that the human writing style of each summary sentence is\nconsistently predictable given its context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_G/0/1/0/all/0/1\">Guangsheng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zebin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArmanTTS single-speaker Persian dataset. (arXiv:2304.03585v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03585","description":"<p>TTS, or text-to-speech, is a complicated process that can be accomplished\nthrough appropriate modeling using deep learning methods. In order to implement\ndeep learning models, a suitable dataset is required. Since there is a scarce\namount of work done in this field for the Persian language, this paper will\nintroduce the single speaker dataset: ArmanTTS. We compared the characteristics\nof this dataset with those of various prevalent datasets to prove that ArmanTTS\nmeets the necessary standards for teaching a Persian text-to-speech conversion\nmodel. We also combined the Tacotron 2 and HiFi GAN to design a model that can\nreceive phonemes as input, with the output being the corresponding speech. 4.0\nvalue of MOS was obtained from real speech, 3.87 value was obtained by the\nvocoder prediction and 2.98 value was reached with the synthetic speech\ngenerated by the TTS model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shamgholi_M/0/1/0/all/0/1\">Mohammd Hasan Shamgholi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeedi_V/0/1/0/all/0/1\">Vahid Saeedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peymanfard_J/0/1/0/all/0/1\">Javad Peymanfard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhabib_L/0/1/0/all/0/1\">Leila Alhabib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeinali_H/0/1/0/all/0/1\">Hossein Zeinali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Automated Prompting: Are We Actually Doing Better?. (arXiv:2304.03609v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03609","description":"<p>Current literature demonstrates that Large Language Models (LLMs) are great\nfew-shot learners, and prompting significantly increases their performance on a\nrange of downstream tasks in a few-shot learning setting. An attempt to\nautomate human-led prompting followed, with some progress achieved. In\nparticular, subsequent work demonstrates automation can outperform fine-tuning\nin certain K-shot learning scenarios.\n</p>\n<p>In this paper, we revisit techniques for automated prompting on six different\ndownstream tasks and a larger range of K-shot learning settings. We find that\nautomated prompting does not consistently outperform simple manual prompts. Our\nwork suggests that, in addition to fine-tuning, manual prompts should be used\nas a baseline in this line of research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yulin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiren Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1\">Ilia Shumailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullins_R/0/1/0/all/0/1\">Robert Mullins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What does ChatGPT return about human values? Exploring value bias in ChatGPT using a descriptive value theory. (arXiv:2304.03612v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03612","description":"<p>There has been concern about ideological basis and possible discrimination in\ntext generated by Large Language Models (LLMs). We test possible value biases\nin ChatGPT using a psychological value theory. We designed a simple experiment\nin which we used a number of different probes derived from the Schwartz basic\nvalue theory (items from the revised Portrait Value Questionnaire, the value\ntype definitions, value names). We prompted ChatGPT via the OpenAI API\nrepeatedly to generate text and then analyzed the generated corpus for value\ncontent with a theory-driven value dictionary using a bag of words approach.\nOverall, we found little evidence of explicit value bias. The results showed\nsufficient construct and discriminant validity for the generated text in line\nwith the theoretical predictions of the psychological model, which suggests\nthat the value content was carried through into the outputs with high fidelity.\nWe saw some merging of socially oriented values, which may suggest that these\nvalues are less clearly differentiated at a linguistic level or alternatively,\nthis mixing may reflect underlying universal human motivations. We outline some\npossible applications of our findings for both applications of ChatGPT for\ncorporate usage and policy making as well as future research avenues. We also\nhighlight possible implications of this relatively high-fidelity replication of\nmotivational content using a linguistic model for the theorizing about human\nvalues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fischer_R/0/1/0/all/0/1\">Ronald Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luczak_Roesch_M/0/1/0/all/0/1\">Markus Luczak-Roesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karl_J/0/1/0/all/0/1\">Johannes A Karl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Theoretical Conditions and Empirical Failure of Bracket Counting on Long Sequences with Linear Recurrent Networks. (arXiv:2304.03639v1 [cs.LG])","link":"http://arxiv.org/abs/2304.03639","description":"<p>Previous work has established that RNNs with an unbounded activation function\nhave the capacity to count exactly. However, it has also been shown that RNNs\nare challenging to train effectively and generally do not learn exact counting\nbehaviour. In this paper, we focus on this problem by studying the simplest\npossible RNN, a linear single-cell network. We conduct a theoretical analysis\nof linear RNNs and identify conditions for the models to exhibit exact counting\nbehaviour. We provide a formal proof that these conditions are necessary and\nsufficient. We also conduct an empirical analysis using tasks involving a\nDyck-1-like Balanced Bracket language under two different settings. We observe\nthat linear RNNs generally do not meet the necessary and sufficient conditions\nfor counting behaviour when trained with the standard approach. We investigate\nhow varying the length of training sequences and utilising different target\nclasses impacts model behaviour during training and the ability of linear RNN\nmodels to effectively approximate the indicator conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+El_Naggar_N/0/1/0/all/0/1\">Nadine El-Naggar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1\">Pranava Madhyastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyde_T/0/1/0/all/0/1\">Tillman Weyde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BenCoref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference Annotations. (arXiv:2304.03682v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03682","description":"<p>Coreference Resolution is a well studied problem in NLP. While widely studied\nfor English and other resource-rich languages, research on coreference\nresolution in Bengali largely remains unexplored due to the absence of relevant\ndatasets. Bengali, being a low-resource language, exhibits greater\nmorphological richness compared to English. In this article, we introduce a new\ndataset, BenCoref, comprising coreference annotations for Bengali texts\ngathered from four distinct domains. This relatively small dataset contains\n5200 mention annotations forming 502 mention clusters within 48,569 tokens. We\ndescribe the process of creating this dataset and report performance of\nmultiple models trained using BenCoref. We anticipate that our work sheds some\nlight on the variations in coreference phenomena across multiple domains in\nBengali and encourages the development of additional resources for Bengali.\nFurthermore, we found poor crosslingual performance at zero-shot setting from\nEnglish, highlighting the need for more language-specific resources for this\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rohan_S/0/1/0/all/0/1\">Shadman Rohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Mojammel Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1\">Mohammad Mamun Or Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Nabeel Mohammed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Importance of Contrastive Loss in Multimodal Learning. (arXiv:2304.03717v1 [cs.LG])","link":"http://arxiv.org/abs/2304.03717","description":"<p>Recently, contrastive learning approaches (e.g., CLIP (Radford et al., 2021))\nhave received huge success in multimodal learning, where the model tries to\nminimize the distance between the representations of different views (e.g.,\nimage and its caption) of the same data point while keeping the representations\nof different data points away from each other. However, from a theoretical\nperspective, it is unclear how contrastive learning can learn the\nrepresentations from different views efficiently, especially when the data is\nnot isotropic. In this work, we analyze the training dynamics of a simple\nmultimodal contrastive learning model and show that contrastive pairs are\nimportant for the model to efficiently balance the learned representations. In\nparticular, we show that the positive pairs will drive the model to align the\nrepresentations at the cost of increasing the condition number, while the\nnegative pairs will reduce the condition number, keeping the learned\nrepresentations balanced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yunwei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Unified Language Checking. (arXiv:2304.03728v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03728","description":"<p>Despite recent concerns about undesirable behaviors generated by large\nlanguage models (LLMs), including non-factual, biased, and hateful language, we\nfind LLMs are inherent multi-task language checkers based on their latent\nrepresentations of natural and social knowledge. We present an interpretable,\nunified, language checking (UniLC) method for both human and machine-generated\nlanguage that aims to check if language input is factual and fair. While\nfairness and fact-checking tasks have been handled separately with dedicated\nmodels, we find that LLMs can achieve high performance on a combination of\nfact-checking, stereotype detection, and hate speech detection tasks with a\nsimple, few-shot, unified set of prompts. With the ``1/2-shot'' multi-task\nlanguage checking method proposed in this work, the GPT3.5-turbo model\noutperforms fully supervised baselines on several language tasks. The simple\napproach and results suggest that based on strong latent knowledge\nrepresentations, an LLM can be an adaptive and explainable tool for detecting\nmisinformation, stereotypes, and hate speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianhua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hongyin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaitskell_L/0/1/0/all/0/1\">Luc Gaitskell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartvigsen_T/0/1/0/all/0/1\">Thomas Hartvigsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xixin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Danny Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gated Mechanism Enhanced Multi-Task Learning for Dialog Routing. (arXiv:2304.03730v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03730","description":"<p>Currently, human-bot symbiosis dialog systems, e.g., pre- and after-sales in\nE-commerce, are ubiquitous, and the dialog routing component is essential to\nimprove the overall efficiency, reduce human resource cost, and enhance user\nexperience. Although most existing methods can fulfil this requirement, they\ncan only model single-source dialog data and cannot effectively capture the\nunderlying knowledge of relations among data and subtasks. In this paper, we\ninvestigate this important problem by thoroughly mining both the data-to-task\nand task-to-task knowledge among various kinds of dialog data. To achieve the\nabove targets, we propose a Gated Mechanism enhanced Multi-task Model (G3M),\nspecifically including a novel dialog encoder and two tailored gated mechanism\nmodules. The proposed method can play the role of hierarchical information\nfiltering and is non-invasive to existing dialog systems. Based on two datasets\ncollected from real world applications, extensive experimental results\ndemonstrate the effectiveness of our method, which achieves the\nstate-of-the-art performance by improving 8.7\\%/11.8\\% on RMSE metric and\n2.2\\%/4.4\\% on F1 metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhuoxuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Ke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shanshan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models. (arXiv:2304.03738v1 [cs.CY])","link":"http://arxiv.org/abs/2304.03738","description":"<p>As the capabilities of generative language models continue to advance, the\nimplications of biases ingrained within these models have garnered increasing\nattention from researchers, practitioners, and the broader public. This article\ninvestigates the challenges and risks associated with biases in large-scale\nlanguage models like ChatGPT. We discuss the origins of biases, stemming from,\namong others, the nature of training data, model specifications, algorithmic\nconstraints, product design, and policy decisions. We explore the ethical\nconcerns arising from the unintended consequences of biased model outputs. We\nfurther analyze the potential opportunities to mitigate biases, the\ninevitability of some biases, and the implications of deploying these models in\nvarious applications, such as virtual assistants, content generation, and\nchatbots. Finally, we review the current approaches to identify, quantify, and\nmitigate biases in language models, emphasizing the need for a\nmulti-disciplinary, collaborative effort to develop more equitable,\ntransparent, and responsible AI systems. This article aims to stimulate a\nthoughtful dialogue within the artificial intelligence community, encouraging\nresearchers and developers to reflect on the role of biases in generative\nlanguage models and the ongoing pursuit of ethical AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1\">Emilio Ferrara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering. (arXiv:2304.03754v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03754","description":"<p>Causal Video Question Answering (CVidQA) queries not only association or\ntemporal relations but also causal relations in a video. Existing question\nsynthesis methods pre-trained question generation (QG) systems on reading\ncomprehension datasets with text descriptions as inputs. However, QG models\nonly learn to ask association questions (e.g., ``what is someone doing...'')\nand result in inferior performance due to the poor transfer of association\nknowledge to CVidQA, which focuses on causal questions like ``why is someone\ndoing ...''. Observing this, we proposed to exploit causal knowledge to\ngenerate question-answer pairs, and proposed a novel framework, Causal\nKnowledge Extraction from Language Models (CaKE-LM), leveraging causal\ncommonsense knowledge from language models to tackle CVidQA. To extract\nknowledge from LMs, CaKE-LM generates causal questions containing two events\nwith one triggering another (e.g., ``score a goal'' triggers ``soccer player\nkicking ball'') by prompting LM with the action (soccer player kicking ball) to\nretrieve the intention (to score a goal). CaKE-LM significantly outperforms\nconventional methods by 4% to 6% of zero-shot CVidQA accuracy on NExT-QA and\nCausal-VidQA datasets. We also conduct comprehensive analyses and provide key\nfindings for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yulei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Influenza A Viral Host Using PSSM and Word Embeddings. (arXiv:2201.01140v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.01140","description":"<p>The rapid mutation of the influenza virus threatens public health.\nReassortment among viruses with different hosts can lead to a fatal pandemic.\nHowever, it is difficult to detect the original host of the virus during or\nafter an outbreak as influenza viruses can circulate between different species.\nTherefore, early and rapid detection of the viral host would help reduce the\nfurther spread of the virus. We use various machine learning models with\nfeatures derived from the position-specific scoring matrix (PSSM) and features\nlearned from word embedding and word encoding to infer the origin host of\nviruses. The results show that the performance of the PSSM-based model reaches\nthe MCC around 95%, and the F1 around 96%. The MCC obtained using the model\nwith word embedding is around 96%, and the F1 is around 97%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wojtczak_D/0/1/0/all/0/1\">Dominik Wojtczak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Pre-Trained Language Models for Cross-Cultural Differences in Values. (arXiv:2203.13722v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13722","description":"<p>Language embeds information about social, cultural, and political values\npeople hold. Prior work has explored social and potentially harmful biases\nencoded in Pre-Trained Language models (PTLMs). However, there has been no\nsystematic study investigating how values embedded in these models vary across\ncultures. In this paper, we introduce probes to study which values across\ncultures are embedded in these models, and whether they align with existing\ntheories and cross-cultural value surveys. We find that PTLMs capture\ndifferences in values across cultures, but those only weakly align with\nestablished value surveys. We discuss implications of using mis-aligned models\nin cross-cultural settings, as well as ways of aligning PTLMs with value\nsurveys.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Arnav Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaffee_L/0/1/0/all/0/1\">Lucie-Aim&#xe9;e Kaffee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making first order linear logic a generating grammar. (arXiv:2206.08955v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.08955","description":"<p>It is known that different categorial grammars have surface representation in\na fragment of first order multiplicative linear logic. We show that the\nfragment of interest is equivalent to the recently introduced {\\it extended\ntensor type calculus}. This provides the former not only with some alternative\nsyntax and intuitive geometric representation, but also with an intrinsic\ndeductive system, which has been absent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Slavnov_S/0/1/0/all/0/1\">Sergey Slavnov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SC-Ques: A Sentence Completion Question Dataset for English as a Second Language Learners. (arXiv:2206.12036v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.12036","description":"<p>Sentence completion (SC) questions present a sentence with one or more blanks\nthat need to be filled in, three to five possible words or phrases as options.\nSC questions are widely used for students learning English as a Second Language\n(ESL). In this paper, we present a large-scale SC dataset, \\textsc{SC-Ques},\nwhich is made up of 289,148 ESL SC questions from real-world standardized\nEnglish examinations. Furthermore, we build a comprehensive benchmark of\nautomatically solving the SC questions by training the large-scale pre-trained\nlanguage models on the proposed \\textsc{SC-Ques} dataset. We conduct detailed\nanalysis of the baseline models performance, limitations and trade-offs. The\ndata and our code are available for research purposes from:\n\\url{https://github.com/ai4ed/SC-Ques}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiongqiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yaying Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shuyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guimin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weiqi Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales. (arXiv:2211.01562v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01562","description":"<p>Neural language models (LMs) have achieved impressive results on various\nlanguage-based reasoning tasks by utilizing latent knowledge encoded in their\nown pretrained parameters. To make this reasoning process more explicit, recent\nworks retrieve a rationalizing LM's internal knowledge by training or prompting\nit to generate free-text rationales, which can be used to guide task\npredictions made by either the same LM or a separate reasoning LM. However,\nrationalizing LMs require expensive rationale annotation and/or computation,\nwithout any assurance that their generated rationales improve LM task\nperformance or faithfully reflect LM decision-making. In this paper, we propose\nPINTO, an LM pipeline that rationalizes via prompt-based learning, and learns\nto faithfully reason over rationales via counterfactual regularization. First,\nPINTO maps out a suitable reasoning process for the task input by prompting a\nfrozen rationalizing LM to generate a free-text rationale. Second, PINTO's\nreasoning LM is fine-tuned to solve the task using the generated rationale as\ncontext, while regularized to output less confident predictions when the\nrationale is perturbed. Across four datasets, we show that PINTO significantly\nimproves the generalization ability of the reasoning LM, yielding higher\nperformance on both in-distribution and out-of-distribution test sets. Also, we\nfind that PINTO's rationales are more faithful to its task predictions than\nthose generated by competitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Identification of Eviction Status from Electronic Health Record Notes. (arXiv:2212.02762v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.02762","description":"<p>Objective: Evictions are important social and behavioral determinants of\nhealth. Evictions are associated with a cascade of negative events that can\nlead to unemployment, housing insecurity/homelessness, long-term poverty, and\nmental health problems. In this study, we developed a natural language\nprocessing system to automatically detect eviction status from electronic\nhealth record (EHR) notes.\n</p>\n<p>Materials and Methods: We first defined eviction status (eviction presence\nand eviction period) and then annotated eviction status in 5000 EHR notes from\nthe Veterans Health Administration (VHA). We developed a novel model, KIRESH,\nthat has shown to substantially outperform other state-of-the-art models such\nas fine-tuning pre-trained language models like BioBERT and BioClinicalBERT.\nMoreover, we designed a novel prompt to further improve the model performance\nby using the intrinsic connection between the two sub-tasks of eviction\npresence and period prediction. Finally, we used the Temperature Scaling-based\nCalibration on our KIRESH-Prompt method to avoid over-confidence issues arising\nfrom the imbalance dataset.\n</p>\n<p>Results: KIRESH-Prompt substantially outperformed strong baseline models\nincluding fine-tuning the BioClinicalBERT model to achieve 0.74672 MCC, 0.71153\nMacro-F1, and 0.83396 Micro-F1 in predicting eviction period and 0.66827 MCC,\n0.62734 Macro-F1, and 0.7863 Micro-F1 in predicting eviction presence. We also\nconducted additional experiments on a benchmark social determinants of health\n(SBDH) dataset to demonstrate the generalizability of our methods.\n</p>\n<p>Conclusion and Future Work: KIRESH-Prompt has substantially improved eviction\nstatus classification. We plan to deploy KIRESH-Prompt to the VHA EHRs as an\neviction surveillance system to help address the US Veterans' housing\ninsecurity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zonghai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_J/0/1/0/all/0/1\">Jack Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weisong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_D/0/1/0/all/0/1\">David A. Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Druhl_E/0/1/0/all/0/1\">Emily Druhl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reisman_J/0/1/0/all/0/1\">Joel I Reisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal and Explainable Internet Meme Classification. (arXiv:2212.05612v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.05612","description":"<p>In the current context where online platforms have been effectively\nweaponized in a variety of geo-political events and social issues, Internet\nmemes make fair content moderation at scale even more difficult. Existing work\non meme classification and tracking has focused on black-box methods that do\nnot explicitly consider the semantics of the memes or the context of their\ncreation. In this paper, we pursue a modular and explainable architecture for\nInternet meme understanding. We design and implement multimodal classification\nmethods that perform example- and prototype-based reasoning over training\ncases, while leveraging both textual and visual SOTA models to represent the\nindividual cases. We study the relevance of our modular and explainable models\nin detecting harmful memes on two existing tasks: Hate Speech Detection and\nMisogyny Classification. We compare the performance between example- and\nprototype-based methods, and between text, vision, and multimodal models,\nacross different categories of harmfulness (e.g., stereotype and\nobjectification). We devise a user-friendly interface that facilitates the\ncomparative analysis of examples retrieved by all of our models for any given\nmeme, informing the community about the strengths and limitations of these\nexplainable methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Abhinav Kumar Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandlin_H/0/1/0/all/0/1\">H&#xf4;ng-&#xc2;n Sandlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sourati_Z/0/1/0/all/0/1\">Zhivar Sourati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luceri_L/0/1/0/all/0/1\">Luca Luceri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tommasini_R/0/1/0/all/0/1\">Riccardo Tommasini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mermoud_A/0/1/0/all/0/1\">Alain Mermoud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counteracts: Testing Stereotypical Representation in Pre-trained Language Models. (arXiv:2301.04347v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.04347","description":"<p>Recently, language models have demonstrated strong performance on various\nnatural language understanding tasks. Language models trained on large\nhuman-generated corpus encode not only a significant amount of human knowledge,\nbut also the human stereotype. As more and more downstream tasks have\nintegrated language models as part of the pipeline, it is necessary to\nunderstand the internal stereotypical representation in order to design the\nmethods for mitigating the negative effects. In this paper, we use\ncounterexamples to examine the internal stereotypical knowledge in pre-trained\nlanguage models (PLMs) that can lead to stereotypical preference. We mainly\nfocus on gender stereotypes, but the method can be extended to other types of\nstereotype. We evaluate 7 PLMs on 9 types of cloze-style prompt with different\ninformation and base knowledge. The results indicate that PLMs show a certain\namount of robustness against unrelated information and preference of shallow\nlinguistic cues, such as word position and syntactic structure, but a lack of\ninterpreting information by meaning. Such findings shed light on how to\ninteract with PLMs in a neutral approach for both finetuning and evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Damin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayz_J/0/1/0/all/0/1\">Julia Rayz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_R/0/1/0/all/0/1\">Romila Pradhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex QA and language models hybrid architectures, Survey. (arXiv:2302.09051v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.09051","description":"<p>This paper reviews the state-of-the-art of language models architectures and\nstrategies for \"complex\" question-answering (QA, CQA, CPS) with a focus on\nhybridization. Large Language Models (LLM) are good at leveraging public data\non standard problems but once you want to tackle more specific complex\nquestions or problems (e.g. How does the concept of personal freedom vary\nbetween different cultures ? What is the best mix of power generation methods\nto reduce climate change ?) you may need specific architecture, knowledge,\nskills, methods, sensitive data protection, explainability, human approval and\nversatile feedback... Recent projects like ChatGPT and GALACTICA have allowed\nnon-specialists to grasp the great potential as well as the equally strong\nlimitations of LLM in complex QA. In this paper, we start by reviewing required\nskills and evaluation techniques. We integrate findings from the robust\ncommunity edited research papers BIG, BLOOM and HELM which open source,\nbenchmark and analyze limits and challenges of LLM in terms of tasks complexity\nand strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) as\na baseline. We discuss some challenges associated with complex QA, including\ndomain adaptation, decomposition and efficient multi-step QA, long form and\nnon-factoid QA, safety and multi-sensitivity data protection, multimodal\nsearch, hallucinations, explainability and truthfulness, temporal reasoning. We\nanalyze current solutions and promising research trends, using elements such\nas: hybrid LLM architectural patterns, training and prompting strategies,\nactive human reinforcement learning supervised with AI, neuro-symbolic and\nstructured knowledge grounding, program synthesis, iterated decomposition and\nothers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daull_X/0/1/0/all/0/1\">Xavier Daull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellot_P/0/1/0/all/0/1\">Patrice Bellot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruno_E/0/1/0/all/0/1\">Emmanuel Bruno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_V/0/1/0/all/0/1\">Vincent Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murisasco_E/0/1/0/all/0/1\">Elisabeth Murisasco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction. (arXiv:2303.02468v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.02468","description":"<p>We study the influence of different activation functions in the output layer\nof deep neural network models for soft and hard label prediction in the\nlearning with disagreement task. In this task, the goal is to quantify the\namount of disagreement via predicting soft labels. To predict the soft labels,\nwe use BERT-based preprocessors and encoders and vary the activation function\nused in the output layer, while keeping other parameters constant. The soft\nlabels are then used for the hard label prediction. The activation functions\nconsidered are sigmoid as well as a step-function that is added to the model\npost-training and a sinusoidal activation function, which is introduced for the\nfirst time in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_P/0/1/0/all/0/1\">Peyman Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mehran Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Azzawi_S/0/1/0/all/0/1\">Sana Sabah Al-Azzawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_I/0/1/0/all/0/1\">Ignacio Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2303.05737","description":"<p>Automatic Speech Recognition (ASR) in medical contexts has the potential to\nsave time, cut costs, increase report accuracy, and reduce physician burnout.\nHowever, the healthcare industry has been slower to adopt this technology, in\npart due to the importance of avoiding medically-relevant transcription\nmistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR\nmetric that penalizes clinically-relevant mistakes more than others. We\ndemonstrate that this metric more closely aligns with clinician preferences on\nmedical sentences as compared to other metrics (WER, BLUE, METEOR, etc),\nsometimes by wide margins. We collect a benchmark of 18 clinician preferences\non 149 realistic medical sentences called the Clinician Transcript Preference\nbenchmark (CTP), demonstrate that CBERTScore more closely matches what\nclinicians prefer, and release the benchmark for the community to further\ndevelop clinically-aware ASR metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shor_J/0/1/0/all/0/1\">Joel Shor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bi_R/0/1/0/all/0/1\">Ruyue Agnes Bi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ibara_S/0/1/0/all/0/1\">Steven Ibara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goldenberg_R/0/1/0/all/0/1\">Roman Goldenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rivlin_E/0/1/0/all/0/1\">Ehud Rivlin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoXum: Cross-modal Visual and Textural Summarization of Videos. (arXiv:2303.12060v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.12060","description":"<p>Video summarization aims to distill the most important information from a\nsource video to produce either an abridged clip or a textual narrative.\nTraditionally, different methods have been proposed depending on whether the\noutput is a video or text, thus ignoring the correlation between the two\nsemantically related tasks of visual summarization and textual summarization.\nWe propose a new joint video and text summarization task. The goal is to\ngenerate both a shortened video clip along with the corresponding textual\nsummary from a long video, collectively referred to as a cross-modal summary.\nThe generated shortened video clip and text narratives should be semantically\nwell aligned. To this end, we first build a large-scale human-annotated dataset\n-- VideoXum (X refers to different modalities). The dataset is reannotated\nbased on ActivityNet. After we filter out the videos that do not meet the\nlength requirements, 14,001 long videos remain in our new dataset. Each video\nin our reannotated dataset has human-annotated video summaries and the\ncorresponding narrative summaries. We then design a novel end-to-end model --\nVTSUM-BILP to address the challenges of our proposed task. Moreover, we propose\na new metric called VT-CLIPScore to help evaluate the semantic consistency of\ncross-modality summary. The proposed model achieves promising performance on\nthis new task and establishes a benchmark for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jingyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_H/0/1/0/all/0/1\">Hang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Ming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_J/0/1/0/all/0/1\">Jenhao Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1\">Chiuman Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment. (arXiv:2303.16634v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.16634","description":"<p>The quality of texts generated by natural language generation (NLG) systems\nis hard to measure automatically. Conventional reference-based metrics, such as\nBLEU and ROUGE, have been shown to have relatively low correlation with human\njudgments, especially for tasks that require creativity and diversity. Recent\nstudies suggest using large language models (LLMs) as reference-free metrics\nfor NLG evaluation, which have the benefit of being applicable to new tasks\nthat lack human references. However, these LLM-based evaluators still have\nlower human correspondence than medium-size neural evaluators. In this work, we\npresent G-Eval, a framework of using large language models with\nchain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of\nNLG outputs. We experiment with two generation tasks, text summarization and\ndialogue generation. We show that G-Eval with GPT-4 as the backbone model\nachieves a Spearman correlation of 0.514 with human on summarization task,\noutperforming all previous methods by a large margin. We also propose\npreliminary analysis on the behavior of LLM-based evaluators, and highlight the\npotential issue of LLM-based evaluators having a bias towards the LLM-generated\ntexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1\">Dan Iter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02017","description":"<p>Large language models have revolutionized the field of artificial\nintelligence and have been used in various applications. Among these models,\nChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,\nit stands out as a powerful tool that has been widely adopted. ChatGPT has been\nsuccessfully applied in numerous areas, including chatbots, content generation,\nlanguage translation, personalized recommendations, and even medical diagnosis\nand treatment. Its success in these applications can be attributed to its\nability to generate human-like responses, understand natural language, and\nadapt to different contexts. Its versatility and accuracy make it a powerful\ntool for natural language processing (NLP). However, there are also limitations\nto ChatGPT, such as its tendency to produce biased responses and its potential\nto perpetuate harmful language patterns. This article provides a comprehensive\noverview of ChatGPT, its applications, advantages, and limitations.\nAdditionally, the paper emphasizes the importance of ethical considerations\nwhen using this robust tool in real-world scenarios. Finally, This paper\ncontributes to ongoing discussions surrounding artificial intelligence and its\nimpact on vision and NLP domains by providing insights into prompt engineering\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hariri_W/0/1/0/all/0/1\">Walid Hariri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments. (arXiv:2304.03047v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.03047","description":"<p>Vision-language navigation is a task that requires an agent to follow\ninstructions to navigate in environments. It becomes increasingly crucial in\nthe field of embodied AI, with potential applications in autonomous navigation,\nsearch and rescue, and human-robot interaction. In this paper, we propose to\naddress a more practical yet challenging counterpart setting - vision-language\nnavigation in continuous environments (VLN-CE). To develop a robust VLN-CE\nagent, we propose a new navigation framework, ETPNav, which focuses on two\ncritical skills: 1) the capability to abstract environments and generate\nlong-range navigation plans, and 2) the ability of obstacle-avoiding control in\ncontinuous environments. ETPNav performs online topological mapping of\nenvironments by self-organizing predicted waypoints along a traversed path,\nwithout prior environmental experience. It privileges the agent to break down\nthe navigation procedure into high-level planning and low-level control.\nConcurrently, ETPNav utilizes a transformer-based cross-modal planner to\ngenerate navigation plans based on topological maps and instructions. The plan\nis then performed through an obstacle-avoiding controller that leverages a\ntrial-and-error heuristic to prevent navigation from getting stuck in\nobstacles. Experimental results demonstrate the effectiveness of the proposed\nmethod. ETPNav yields more than 10% and 20% improvements over prior\nstate-of-the-art on R2R-CE and RxR-CE datasets, respectively. Our code is\navailable at https://github.com/MarSaKi/ETPNav.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Dong An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Keji He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Pareto Front of Multilingual Neural Machine Translation. (arXiv:2304.03216v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.03216","description":"<p>In this work, we study how the generalization performance of a given\ndirection changes with its sampling ratio in Multilingual Neural Machine\nTranslation (MNMT). By training over 200 multilingual models with various model\nsizes, directions, and total numbers of tasks, we find that scalarization leads\nto a multitask trade-off front that deviates from the traditional Pareto front\nwhen there exists data imbalance in the training corpus. That is, the\nperformance of certain translation directions does not improve with the\nincrease of its weight in the multi-task optimization objective, which poses a\ngreat challenge to improve the overall performance of all directions. Based on\nour observations, we propose the Double Power Law to predict the unique\nperformance trade-off front in MNMT, which is robust across various languages,\ndata adequacy, and the number of tasks. Finally, we formulate the sample ratio\nselection problem in MNMT as an optimization problem based on the Double Power\nLaw, which achieves better performance than temperature searching and gradient\nmanipulation methods using up to half of the total training budget in our\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large language models effectively leverage document-level context for literary translation, but critical errors persist. (arXiv:2304.03245v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.03245","description":"<p>Large language models (LLMs) are competitive with the state of the art on a\nwide range of sentence-level translation datasets. However, their ability to\ntranslate paragraphs and documents remains unexplored because evaluation in\nthese settings is costly and difficult. We show through a rigorous human\nevaluation that asking the Gpt-3.5 (text-davinci-003) LLM to translate an\nentire literary paragraph (e.g., from a novel) at once results in\nhigher-quality translations than standard sentence-by-sentence translation\nacross 18 linguistically-diverse language pairs (e.g., translating into and out\nof Japanese, Polish, and English). Our evaluation, which took approximately 350\nhours of effort for annotation and analysis, is conducted by hiring translators\nfluent in both the source and target language and asking them to provide both\nspan-level error annotations as well as preference judgments of which system's\ntranslations are better. We observe that discourse-level LLM translators commit\nfewer mistranslations, grammar errors, and stylistic inconsistencies than\nsentence-level approaches. With that said, critical errors still abound,\nincluding occasional content omissions, and a human translator's intervention\nremains necessary to ensure that the author's voice remains intact. We publicly\nrelease our dataset and error annotations to spur future research on evaluation\nof document-level literary translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karpinska_M/0/1/0/all/0/1\">Marzena Karpinska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}