{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-10T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The Casual Conversations v2 Dataset. (arXiv:2303.04838v1 [cs.CV])","link":"http://arxiv.org/abs/2303.04838","description":"<p>This paper introduces a new large consent-driven dataset aimed at assisting\nin the evaluation of algorithmic bias and robustness of computer vision and\naudio speech models in regards to 11 attributes that are self-provided or\nlabeled by trained annotators. The dataset includes 26,467 videos of 5,567\nunique paid participants, with an average of almost 5 videos per person,\nrecorded in Brazil, India, Indonesia, Mexico, Vietnam, Philippines, and the\nUSA, representing diverse demographic characteristics. The participants agreed\nfor their data to be used in assessing fairness of AI models and provided\nself-reported age, gender, language/dialect, disability status, physical\nadornments, physical attributes and geo-location information, while trained\nannotators labeled apparent skin tone using the Fitzpatrick Skin Type and Monk\nSkin Tone scales, and voice timbre. Annotators also labeled for different\nrecording setups and per-second activity annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Porgali_B/0/1/0/all/0/1\">Bilal Porgali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albiero_V/0/1/0/all/0/1\">V&#xed;tor Albiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryda_J/0/1/0/all/0/1\">Jordan Ryda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1\">Cristian Canton Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazirbas_C/0/1/0/all/0/1\">Caner Hazirbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexical Complexity Prediction: An Overview. (arXiv:2303.04851v1 [cs.CL])","link":"http://arxiv.org/abs/2303.04851","description":"<p>The occurrence of unknown words in texts significantly hinders reading\ncomprehension. To improve accessibility for specific target populations,\ncomputational modelling has been applied to identify complex words in texts and\nsubstitute them for simpler alternatives. In this paper, we present an overview\nof computational approaches to lexical complexity prediction focusing on the\nwork carried out on English data. We survey relevant approaches to this problem\nwhich include traditional machine learning classifiers (e.g. SVMs, logistic\nregression) and deep neural networks as well as a variety of features, such as\nthose inspired by literature in psycholinguistics as well as word frequency,\nword length, and many others. Furthermore, we introduce readers to past\ncompetitions and available datasets created on this topic. Finally, we include\nbrief sections on applications of lexical complexity prediction, such as\nreadability and text simplification, together with related studies on languages\nother than English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+North_K/0/1/0/all/0/1\">Kai North</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shardlow_M/0/1/0/all/0/1\">Matthew Shardlow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let's Get Personal: Personal Questions Improve SocialBot Performance in the Alexa Prize. (arXiv:2303.04953v1 [cs.CL])","link":"http://arxiv.org/abs/2303.04953","description":"<p>There has been an increased focus on creating conversational open-domain\ndialogue systems in the spoken dialogue community. Unlike traditional dialogue\nsystems, these conversational systems cannot assume any specific information\nneed or domain restrictions, i.e., the only inherent goal is to converse with\nthe user on an unknown set of topics. While massive improvements in Natural\nLanguage Understanding (NLU) and the growth of available knowledge resources\ncan partially support a robust conversation, these conversations generally lack\nthe rapport between two humans that know each other. We developed a robust\nopen-domain conversational system, Athena, that real Amazon Echo users access\nand evaluate at scale in the context of the Alexa Prize competition. We\nexperiment with methods intended to increase intimacy between Athena and the\nuser by heuristically developing a rule-based user model that personalizes both\nthe current and subsequent conversations and evaluating specific personal\nopinion question strategies in A/B studies. Our results show a statistically\nsignificant positive impact on perceived conversation quality and length when\nemploying these strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bowden_K/0/1/0/all/0/1\">Kevin K. Bowden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_M/0/1/0/all/0/1\">Marilyn Walker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Stage Coarse-to-Fine Contrastive Learning for Conversation Intent Induction. (arXiv:2303.05034v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05034","description":"<p>Intent recognition is critical for task-oriented dialogue systems. However,\nfor emerging domains and new services, it is difficult to accurately identify\nthe key intent of a conversation due to time-consuming data annotation and\ncomparatively poor model transferability. Therefore, the automatic induction of\ndialogue intention is very important for intelligent dialogue systems. This\npaper presents our solution to Track 2 of Intent Induction from Conversations\nfor Task-Oriented Dialogue at the Eleventh Dialogue System Technology Challenge\n(DSTC11). The essence of intention clustering lies in distinguishing the\nrepresentation of different dialogue utterances. The key to automatic intention\ninduction is that, for any given set of new data, the sentence representation\nobtained by the model can be well distinguished from different labels.\nTherefore, we propose a multi-stage coarse-to-fine contrastive learning model\ntraining scheme including unsupervised contrastive learning pre-training,\nsupervised contrastive learning pre-training, and fine-tuning with joint\ncontrastive learning and clustering to obtain a better dialogue utterance\nrepresentation model for the clustering task. In the released DSTC11 Track 2\nevaluation results, our proposed system ranked first on both of the two\nsubtasks of this Track.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Caiyuan Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Ya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jia-Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yongxin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guoping Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Language agnostic WER Standardization. (arXiv:2303.05046v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05046","description":"<p>Word error rate (WER) is a standard metric for the evaluation of Automated\nSpeech Recognition (ASR) systems. However, WER fails to provide a fair\nevaluation of human perceived quality in presence of spelling variations,\nabbreviations, or compound words arising out of agglutination. Multiple\nspelling variations might be acceptable based on locale/geography, alternative\nabbreviations, borrowed words, and transliteration of code-mixed words from a\nforeign language to the target language script. Similarly, in case of\nagglutination, often times the agglutinated, as well as the split forms, are\nacceptable. Previous work handled this problem by using manually identified\nnormalization pairs and applying them to both the transcription and the\nhypothesis before computing WER. In this paper, we propose an automatic WER\nnormalization system consisting of two modules: spelling normalization and\nsegmentation normalization. The proposed system is unsupervised and language\nagnostic, and therefore scalable. Experiments with ASR on 35K utterances across\nfour languages yielded an average WER reduction of 13.28%. Human judgements of\nthese automatically identified normalization pairs show that our WER-normalized\nevaluation is highly consistent with the perceived quality of ASR output.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guha_S/0/1/0/all/0/1\">Satarupa Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambavat_R/0/1/0/all/0/1\">Rahul Ambavat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankur Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1\">Rupeshkumar Mehta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction. (arXiv:2303.05063v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05063","description":"<p>Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated\nremarkable results in various natural language processing (NLP) tasks with\nin-context learning, which involves inference based on a few demonstration\nexamples. Despite their successes in NLP tasks, no investigation has been\nconducted to assess the ability of LLMs to perform document information\nextraction (DIE) using in-context learning. Applying LLMs to DIE poses two\nchallenges: the modality and task gap. To this end, we propose a simple but\neffective in-context learning framework called ICL-D3IE, which enables LLMs to\nperform DIE with different types of demonstration examples. Specifically, we\nextract the most difficult and distinct segments from hard training documents\nas hard demonstrations for benefiting all test instances. We design\ndemonstrations describing relationships that enable LLMs to understand\npositional relationships. We introduce formatting demonstrations for easy\nanswer extraction. Additionally, the framework improves diverse demonstrations\nby updating them iteratively. Our experiments on three widely used benchmark\ndatasets demonstrate that the ICL-D3IE framework enables GPT-3/ChatGPT to\nachieve superior performance when compared to previous pre-trained methods\nfine-tuned with full training in both the in-distribution (ID) setting and in\nthe out-of-distribution (OOD) setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiabang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Legibility of Visual Text Perturbations. (arXiv:2303.05077v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05077","description":"<p>Many adversarial attacks in NLP perturb inputs to produce visually similar\nstrings ('ergo' $\\rightarrow$ '$\\epsilon$rgo') which are legible to humans but\ndegrade model performance. Although preserving legibility is a necessary\ncondition for text perturbation, little work has been done to systematically\ncharacterize it; instead, legibility is typically loosely enforced via\nintuitions around the nature and extent of perturbations. Particularly, it is\nunclear to what extent can inputs be perturbed while preserving legibility, or\nhow to quantify the legibility of a perturbed string. In this work, we address\nthis gap by learning models that predict the legibility of a perturbed string,\nand rank candidate perturbations based on their legibility. To do so, we\ncollect and release \\dataset, a human-annotated dataset comprising the\nlegibility of visually perturbed text. Using this dataset, we build both text-\nand vision-based models which achieve up to $0.91$ F1 score in predicting\nwhether an input is legible, and an accuracy of $0.86$ in predicting which of\ntwo given perturbations is more legible. Additionally, we discover that legible\nperturbations from the \\dataset dataset are more effective at lowering the\nperformance of NLP models than best-known attack strategies, suggesting that\ncurrent models may be vulnerable to a broad range of perturbations beyond what\nis captured by existing visual attacks. Data, code, and models are available at\nhttps://github.com/dvsth/learning-legibility-2023.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seth_D/0/1/0/all/0/1\">Dev Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stureborg_R/0/1/0/all/0/1\">Rickard Stureborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1\">Danish Pruthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the relevance of traditional genres: a network analysis of fiction readers' preferences. (arXiv:2303.05080v1 [cs.SI])","link":"http://arxiv.org/abs/2303.05080","description":"<p>We investigate how well traditional fiction genres like Fantasy, Thriller,\nand Literature represent readers' preferences. Using user data from Goodreads\nwe construct a book network where two books are strongly linked if the same\npeople tend to read or enjoy them both. We then partition this network into\ncommunities of similar books and assign each a list of subjects from The Open\nLibrary to serve as a proxy for traditional genres. Our analysis reveals that\nthe network communities correspond to existing combinations of traditional\ngenres, but that the exact communities differ depending on whether we consider\nbooks that people read or books that people enjoy.\n</p>\n<p>In addition, we apply principal component analysis to the data and find that\nthe variance in the book communities is best explained by two factors: the\nmaturity/childishness and realism/fantastical nature of the books. We propose\nusing this maturity-realism plane as a coarse classification tool for stories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakal_T/0/1/0/all/0/1\">Taom Sakal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proulx_S/0/1/0/all/0/1\">Stephen Proulx</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Multi-View Fusion Mechanism For Chinese Relation Extraction. (arXiv:2303.05082v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05082","description":"<p>Recently, many studies incorporate external knowledge into character-level\nfeature based models to improve the performance of Chinese relation extraction.\nHowever, these methods tend to ignore the internal information of the Chinese\ncharacter and cannot filter out the noisy information of external knowledge. To\naddress these issues, we propose a mixture-of-view-experts framework (MoVE) to\ndynamically learn multi-view features for Chinese relation extraction. With\nboth the internal and external knowledge of Chinese characters, our framework\ncan better capture the semantic information of Chinese characters. To\ndemonstrate the effectiveness of the proposed framework, we conduct extensive\nexperiments on three real-world datasets in distinct domains. Experimental\nresults show consistent and significant superiority and robustness of our\nproposed framework. Our code and dataset will be released at:\nhttps://gitee.com/tmg-nudt/multi-view-of-expert-for-chineserelation-extraction\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1\">Bin Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Long Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jie Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Video Retrieval by Adaptive Margin. (arXiv:2303.05093v1 [cs.CV])","link":"http://arxiv.org/abs/2303.05093","description":"<p>Video retrieval is becoming increasingly important owing to the rapid\nemergence of videos on the Internet. The dominant paradigm for video retrieval\nlearns video-text representations by pushing the distance between the\nsimilarity of positive pairs and that of negative pairs apart from a fixed\nmargin. However, negative pairs used for training are sampled randomly, which\nindicates that the semantics between negative pairs may be related or even\nequivalent, while most methods still enforce dissimilar representations to\ndecrease their similarity. This phenomenon leads to inaccurate supervision and\npoor performance in learning video-text representations.\n</p>\n<p>While most video retrieval methods overlook that phenomenon, we propose an\nadaptive margin changed with the distance between positive and negative pairs\nto solve the aforementioned issue. First, we design the calculation framework\nof the adaptive margin, including the method of distance measurement and the\nfunction between the distance and the margin. Then, we explore a novel\nimplementation called \"Cross-Modal Generalized Self-Distillation\" (CMGSD),\nwhich can be built on the top of most video retrieval models with few\nmodifications. Notably, CMGSD adds few computational overheads at train time\nand adds no computational overhead at test time. Experimental results on three\nwidely used datasets demonstrate that the proposed method can yield\nsignificantly better performance than the corresponding backbone model, and it\noutperforms state-of-the-art methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Feng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhifan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenbin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yajuan Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+zhu_Y/0/1/0/all/0/1\">Yong zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESCL: Equivariant Self-Contrastive Learning for Sentence Representations. (arXiv:2303.05143v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05143","description":"<p>Previous contrastive learning methods for sentence representations often\nfocus on insensitive transformations to produce positive pairs, but neglect the\nrole of sensitive transformations that are harmful to semantic representations.\nTherefore, we propose an Equivariant Self-Contrastive Learning (ESCL) method to\nmake full use of sensitive transformations, which encourages the learned\nrepresentations to be sensitive to certain types of transformations with an\nadditional equivariant learning task. Meanwhile, in order to improve\npracticability and generality, ESCL simplifies the implementations of\ntraditional equivariant contrastive methods to share model parameters from the\nperspective of multi-task learning. We evaluate our ESCL on semantic textual\nsimilarity tasks. The proposed method achieves better results while using fewer\nlearning parameters compared to previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xue Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Junlan Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?. (arXiv:2303.05153v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05153","description":"<p>Neural document retrievers, including dense passage retrieval (DPR), have\noutperformed classical lexical-matching retrievers, such as BM25, when\nfine-tuned and tested on specific question-answering datasets. However, it has\nbeen shown that the existing dense retrievers do not generalize well not only\nout of domain but even in domain such as Wikipedia, especially when a named\nentity in a question is a dominant clue for retrieval. In this paper, we\npropose an approach toward in-domain generalization using the embeddings\ngenerated by the frozen language model trained with the entities in the domain.\nBy not fine-tuning, we explore the possibility that the rich knowledge\ncontained in a pretrained language model can be used for retrieval tasks. The\nproposed method outperforms conventional DPRs on entity-centric questions in\nWikipedia domain and achieves almost comparable performance to BM25 and\nstate-of-the-art SPAR model. We also show that the contextualized keys lead to\nstrong improvements compared to BM25 when the entity names consist of common\nwords. Our results demonstrate the feasibility of the zero-shot retrieval\nmethod for entity-centric questions of Wikipedia domain, where DPR has\nstruggled to perform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoshi_Y/0/1/0/all/0/1\">Yasuto Hoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyashita_D/0/1/0/all/0/1\">Daisuke Miyashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morioka_Y/0/1/0/all/0/1\">Yasuhiro Morioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1\">Youyang Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torii_O/0/1/0/all/0/1\">Osamu Torii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deguchi_J/0/1/0/all/0/1\">Jun Deguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\pi$-augmented pregroups and applications to linguistics. (arXiv:2303.05160v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05160","description":"<p>We enrich pregroups with a mapping which allows us to locally apply precyclic\npermutations to designated substrings. We prove a normalisation theorem for\nsuch algebraic structures and briefly formalise some known applications of\npregroups to the analysis of clitic pronouns in certain natural languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boboc_V/0/1/0/all/0/1\">Valentin Boboc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry of Language. (arXiv:2303.05208v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05208","description":"<p>In this article, we present a fresh perspective on language, combining ideas\nfrom various sources, but mixed in a new synthesis. As in the minimalist\nprogram, the question is whether we can formulate an elegant formalism, a\nuniversal grammar or a mechanism which explains significant aspects of the\nhuman faculty of language, which in turn can be considered a natural\ndisposition for the evolution and deployment of the diverse human languages. We\ndescribe such a mechanism, which differs from existing logical and grammatical\napproaches by its geometric nature. Our main contribution is to explore the\nassumption that sentence recognition takes place by forming chains of tokens\nrepresenting words, followed by matching these chains with pre-existing chains\nrepresenting grammatical word orders. The aligned chains of tokens give rise to\ntwo- and three-dimensional complexes. The resulting model gives an alternative\npresentation for subtle rules, traditionally formalized using categorial\ngrammar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feijs_L/0/1/0/all/0/1\">Loe Feijs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEAM: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading. (arXiv:2303.05221v1 [q-bio.NC])","link":"http://arxiv.org/abs/2303.05221","description":"<p>Models of eye-movement control during reading, developed largely within\npsychology, usually focus on visual, attentional, and motor processes but\nneglect post-lexical language processing; by contrast, models of sentence\ncomprehension processes, developed largely within psycholinguistics, generally\nfocus only on post-lexical language processes. We present a model that combines\nthese two research threads, by integrating eye-movement control and sentence\nprocessing. Developing such an integrated model is extremely challenging and\ncomputationally demanding, but such an integration is an important step toward\ncomplete mathematical models of natural language comprehension in reading. We\ncombine the SWIFT model of eye-movement control (Engbert et al., Psychological\nReview, 112, 2005, pp. 777-813) with key components of the Lewis and Vasishth\nsentence processing model (Lewis and Vasishth, Cognitive Science, 29, 2005, pp.\n375-419). This integration becomes possible, for the first time, due in part to\nrecent advances in successful parameter identification in dynamical models,\nwhich allows us to investigate profile log-likelihoods for individual model\nparameters. We present a fully implemented proof-of-concept model demonstrating\nhow such an integrated model can be achieved; our approach includes Bayesian\nmodel inference with Markov Chain Monte Carlo (MCMC) sampling as a key\ncomputational tool. The integrated model, SEAM, can successfully reproduce eye\nmovement patterns that arise due to similarity-based interference in reading.\nTo our knowledge, this is the first-ever integration of a complete process\nmodel of eye-movement control with linguistic dependency completion processes\nin sentence comprehension. In future work, this proof of concept model will\nneed to be evaluated using a comprehensive set of benchmark data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Rabe_M/0/1/0/all/0/1\">Maximilian M. Rabe</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Paape_D/0/1/0/all/0/1\">Dario Paape</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mertzen_D/0/1/0/all/0/1\">Daniela Mertzen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Vasishth_S/0/1/0/all/0/1\">Shravan Vasishth</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Engbert_R/0/1/0/all/0/1\">Ralf Engbert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can large language models build causal graphs?. (arXiv:2303.05279v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05279","description":"<p>Building causal graphs can be a laborious process. To ensure all relevant\ncausal pathways have been captured, researchers often have to discuss with\nclinicians and experts while also reviewing extensive relevant medical\nliterature. By encoding common and medical knowledge, large language models\n(LLMs) represent an opportunity to ease this process by automatically scoring\nedges (i.e., connections between two variables) in potential graphs. LLMs\nhowever have been shown to be brittle to the choice of probing words, context,\nand prompts that the user employs. In this work, we evaluate if LLMs can be a\nuseful tool in complementing causal graph development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Stephanie Long</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tibor Schuster</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Piche_A/0/1/0/all/0/1\">Alexandre Pich&#xe9;</a> (2,3) (1) <a href=\"http://arxiv.org/find/cs/1/au:+Medicine_D/0/1/0/all/0/1\">Department of Family Medicine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+University_M/0/1/0/all/0/1\">McGill University</a>, (2) <a href=\"http://arxiv.org/find/cs/1/au:+Mila/0/1/0/all/0/1\">Mila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montreal_U/0/1/0/all/0/1\">Universit&#xe9; de Montreal</a>, (3) <a href=\"http://arxiv.org/find/cs/1/au:+Research_S/0/1/0/all/0/1\">ServiceNow Research</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Stashing Quantization for Efficient Transformer Training. (arXiv:2303.05295v1 [cs.LG])","link":"http://arxiv.org/abs/2303.05295","description":"<p>Large Language Models (LLMs) have demonstrated impressive performance on a\nrange of Natural Language Processing (NLP) tasks. Unfortunately, the immense\namount of computations and memory accesses required for LLM training makes them\nprohibitively expensive in terms of hardware cost, and thus challenging to\ndeploy in use cases such as on-device learning. In this paper, motivated by the\nobservation that LLM training is memory-bound, we propose a novel dynamic\nquantization strategy, termed Dynamic Stashing Quantization (DSQ), that puts a\nspecial focus on reducing the memory operations, but also enjoys the other\nbenefits of low precision training, such as the reduced arithmetic cost. We\nconduct a thorough study on two translation tasks (trained-from-scratch) and\nthree classification tasks (fine-tuning). DSQ reduces the amount of arithmetic\noperations by $20.95\\times$ and the number of DRAM operations by $2.55\\times$\non IWSLT17 compared to the standard 16-bit fixed-point, which is widely used in\non-device learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1\">Daniel Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullins_R/0/1/0/all/0/1\">Robert Mullins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiren Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup for Visual Speech Translation and Recognition. (arXiv:2303.05309v1 [cs.CV])","link":"http://arxiv.org/abs/2303.05309","description":"<p>Multi-media communications facilitate global interaction among people.\nHowever, despite researchers exploring cross-lingual translation techniques\nsuch as machine translation and audio speech translation to overcome language\nbarriers, there is still a shortage of cross-lingual studies on visual speech.\nThis lack of research is mainly due to the absence of datasets containing\nvisual speech and translated text pairs. In this paper, we present\n\\textbf{AVMuST-TED}, the first dataset for \\textbf{A}udio-\\textbf{V}isual\n\\textbf{Mu}ltilingual \\textbf{S}peech \\textbf{T}ranslation, derived from\n\\textbf{TED} talks. Nonetheless, visual speech is not as distinguishable as\naudio speech, making it difficult to develop a mapping from source speech\nphonemes to the target language text. To address this issue, we propose\nMixSpeech, a cross-modality self-learning framework that utilizes audio speech\nto regularize the training of visual speech tasks. To further minimize the\ncross-modality gap and its impact on knowledge transfer, we suggest adopting\nmixed speech, which is created by interpolating audio and visual streams, along\nwith a curriculum learning strategy to adjust the mixing ratio as needed.\nMixSpeech enhances speech translation in noisy environments, improving BLEU\nscores for four languages on AVMuST-TED by +1.4 to +4.2. Moreover, it achieves\nstate-of-the-art performance in lip reading on CMLR (11.1\\%), LRS2 (25.5\\%),\nand LRS3 (28.0\\%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xize Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1\">Tao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rongjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zehan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huangdai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_A/0/1/0/all/0/1\">Aoxiong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Replacement as a Self-supervision for Fine-grained Vision-language Pre-training. (arXiv:2303.05313v1 [cs.CV])","link":"http://arxiv.org/abs/2303.05313","description":"<p>Fine-grained supervision based on object annotations has been widely used for\nvision and language pre-training (VLP). However, in real-world application\nscenarios, aligned multi-modal data is usually in the image-caption format,\nwhich only provides coarse-grained supervision. It is cost-expensive to collect\nobject annotations and build object annotation pre-extractor for different\nscenarios. In this paper, we propose a fine-grained self-supervision signal\nwithout object annotations from a replacement perspective. First, we propose a\nhomonym sentence rewriting (HSR) algorithm to provide token-level supervision.\nThe algorithm replaces a verb/noun/adjective/quantifier word of the caption\nwith its homonyms from WordNet. Correspondingly, we propose a replacement\nvision-language modeling (RVLM) framework to exploit the token-level\nsupervision. Two replaced modeling tasks, i.e., replaced language contrastive\n(RLC) and replaced language modeling (RLM), are proposed to learn the\nfine-grained alignment. Extensive experiments on several downstream tasks\ndemonstrate the superior performance of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lisai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhijian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yunpeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhonghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeing ChatGPT Through Students' Eyes: An Analysis of TikTok Data. (arXiv:2303.05349v1 [stat.AP])","link":"http://arxiv.org/abs/2303.05349","description":"<p>Advanced large language models like ChatGPT have gained considerable\nattention recently, including among students. However, while the debate on\nChatGPT in academia is making waves, more understanding is needed among\nlecturers and teachers on how students use and perceive ChatGPT. To address\nthis gap, we analyzed the content on ChatGPT available on TikTok in February\n2023. TikTok is a rapidly growing social media platform popular among\nindividuals under 30. Specifically, we analyzed the content of the 100 most\npopular videos in English tagged with #chatgpt, which collectively garnered\nover 250 million views. Most of the videos we studied promoted the use of\nChatGPT for tasks like writing essays or code. In addition, many videos\ndiscussed AI detectors, with a focus on how other tools can help to transform\nChatGPT output to fool these detectors. This also mirrors the discussion among\neducators on how to treat ChatGPT as lecturers and teachers in teaching and\ngrading. What is, however, missing from the analyzed clips on TikTok are videos\nthat discuss ChatGPT producing content that is nonsensical or unfaithful to the\ntraining data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Haensch_A/0/1/0/all/0/1\">Anna-Carolina Haensch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ball_S/0/1/0/all/0/1\">Sarah Ball</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Herklotz_M/0/1/0/all/0/1\">Markus Herklotz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kreuter_F/0/1/0/all/0/1\">Frauke Kreuter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering -- Example of ChatGPT. (arXiv:2303.05352v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05352","description":"<p>There has been a growing effort to replace hand extraction of data from\nresearch papers with automated data extraction based on natural language\nprocessing (NLP), language models (LMs), and recently, large language models\n(LLMs). Although these methods enable efficient extraction of data from large\nsets of research papers, they require a significant amount of up-front effort,\nexpertise, and coding. In this work we propose the ChatExtract method that can\nfully automate very accurate data extraction with essentially no initial effort\nor background using an advanced conversational LLM (or AI). ChatExtract\nconsists of a set of engineered prompts applied to a conversational LLM that\nboth identify sentences with data, extract data, and assure its correctness\nthrough a series of follow-up questions. These follow-up questions address a\ncritical challenge associated with LLMs - their tendency to provide factually\ninaccurate responses. ChatExtract can be applied with any conversational LLMs\nand yields very high quality data extraction. In tests on materials data we\nfind precision and recall both over 90% from the best conversational LLMs,\nlikely rivaling or exceeding human accuracy in many cases. We demonstrate that\nthe exceptional performance is enabled by the information retention in a\nconversational model combined with purposeful redundancy and introducing\nuncertainty through follow-up prompts. These results suggest that approaches\nsimilar to ChatExtract, due to their simplicity, transferability and accuracy\nare likely to replace other methods of data extraction in the near future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polak_M/0/1/0/all/0/1\">Maciej P. Polak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_D/0/1/0/all/0/1\">Dane Morgan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT is on the horizon: Could a large language model be all we need for Intelligent Transportation?. (arXiv:2303.05382v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05382","description":"<p>ChatGPT, developed by OpenAI, is one of the largest Large Language Models\n(LLM) with over 175 billion parameters. ChatGPT has demonstrated the impressive\ncapabilities of LLM, particularly in the field of natural language processing\n(NLP). With the emergence of the discussion and application of LLM in various\nresearch or engineering domains, it is time to envision how LLM may\nrevolutionize the way we approach intelligent transportation systems. This\npaper explores the future applications of LLM in addressing key transportation\nproblems. By leveraging LLM and a cross-modal encoder, an intelligent system\ncan handle traffic data from various modalities and execute transportation\noperations through a single LLM. NLP, combined with cross-modal processing, is\ninvestigated with its potential applications in transportation. To demonstrate\nthis potential, a smartphone-based crash report auto-generation and analysis\nframework is presented as a use case. Despite the potential benefits,\nchallenges related to data privacy, data quality, and model bias must be\nconsidered. Overall, the use of LLM in intelligent transport systems holds\npromise for more efficient, intelligent, and sustainable transportation systems\nthat improve the lives of people around the world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_O/0/1/0/all/0/1\">Ou Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdel_Aty_M/0/1/0/all/0/1\">Mohamed Abdel-Aty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongdong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shengxuan Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making a Computational Attorney. (arXiv:2303.05383v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05383","description":"<p>This \"blue sky idea\" paper outlines the opportunities and challenges in data\nmining and machine learning involving making a computational attorney -- an\nintelligent software agent capable of helping human lawyers with a wide range\nof complex high-level legal tasks such as drafting legal briefs for the\nprosecution or defense in court. In particular, we discuss what a ChatGPT-like\nLarge Legal Language Model (L$^3$M) can and cannot do today, which will inspire\nresearchers with promising short-term and long-term research objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dell Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schilder_F/0/1/0/all/0/1\">Frank Schilder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conrad_J/0/1/0/all/0/1\">Jack G. Conrad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makrehchi_M/0/1/0/all/0/1\">Masoud Makrehchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rickenbach_D/0/1/0/all/0/1\">David von Rickenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moulinier_I/0/1/0/all/0/1\">Isabelle Moulinier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Detection of Industry Sectors in Legal Articles Using Machine Learning Approaches. (arXiv:2303.05387v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05387","description":"<p>The ability to automatically identify industry sector coverage in articles on\nlegal developments, or any kind of news articles for that matter, can bring\nplentiful of benefits both to the readers and the content creators themselves.\nBy having articles tagged based on industry coverage, readers from all around\nthe world would be able to get to legal news that are specific to their region\nand professional industry. Simultaneously, writers would benefit from\nunderstanding which industries potentially lack coverage or which industries\nreaders are currently mostly interested in and thus, they would focus their\nwriting efforts towards more inclusive and relevant legal news coverage. In\nthis paper, a Machine Learning-powered industry analysis approach which\ncombined Natural Language Processing (NLP) with Statistical and Machine\nLearning (ML) techniques was investigated. A dataset consisting of over 1,700\nannotated legal articles was created for the identification of six industry\nsectors. Text and legal based features were extracted from the text. Both\ntraditional ML methods (e.g. gradient boosting machine algorithms, and\ndecision-tree based algorithms) and deep neural network (e.g. transformer\nmodels) were applied for performance comparison of predictive models. The\nsystem achieved promising results with area under the receiver operating\ncharacteristic curve scores above 0.90 and F-scores above 0.81 with respect to\nthe six industry sectors. The experimental results show that the suggested\nautomated industry analysis which employs ML techniques allows the processing\nof large collections of text data in an easy, efficient, and scalable way.\nTraditional ML methods perform better than deep neural networks when only a\nsmall and domain-specific training data is available for the study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hui Yang</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Hadjiantoni_S/0/1/0/all/0/1\">Stella Hadjiantoni</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yunfei Long</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Petraityte_R/0/1/0/all/0/1\">Ruta Petraityte</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Lausen_B/0/1/0/all/0/1\">Berthold Lausen</a> (1 and 4) ((1) Department of Mathematical Sciences, University of Essex, Wivenhoe Park, Colchester, CO43SQ, UK, (2) Mondaq Ltd, Bristol, UK, (3) School of Computer Science and Electronic Engineering, University of Essex, Wivenhoe Park, Colchester, CO43SQ, UK, (4) Institute of Medical Informatics, Biometry and Epidemiology, School of Medicine, Friedrich-Alexander University Erlangen-Nuremberg, Waldstr. 6, Erlangen, 91054, Germany)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"German BERT Model for Legal Named Entity Recognition. (arXiv:2303.05388v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05388","description":"<p>The use of BERT, one of the most popular language models, has led to\nimprovements in many Natural Language Processing (NLP) tasks. One such task is\nNamed Entity Recognition (NER) i.e. automatic identification of named entities\nsuch as location, person, organization, etc. from a given text. It is also an\nimportant base step for many NLP tasks such as information extraction and\nargumentation mining. Even though there is much research done on NER using BERT\nand other popular language models, the same is not explored in detail when it\ncomes to Legal NLP or Legal Tech. Legal NLP applies various NLP techniques such\nas sentence similarity or NER specifically on legal data. There are only a\nhandful of models for NER tasks using BERT language models, however, none of\nthese are aimed at legal documents in German. In this paper, we fine-tune a\npopular BERT language model trained on German data (German BERT) on a Legal\nEntity Recognition (LER) dataset. To make sure our model is not overfitting, we\nperformed a stratified 10-fold cross-validation. The results we achieve by\nfine-tuning German BERT on the LER dataset outperform the BiLSTM-CRF+ model\nused by the authors of the same LER dataset. Finally, we make the model openly\navailable via HuggingFace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darji_H/0/1/0/all/0/1\">Harshil Darji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrovic_J/0/1/0/all/0/1\">Jelena Mitrovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granitzer_M/0/1/0/all/0/1\">Michael Granitzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depression Detection Using Digital Traces on Social Media: A Knowledge-aware Deep Learning Approach. (arXiv:2303.05389v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05389","description":"<p>Depression is a common disease worldwide. It is difficult to diagnose and\ncontinues to be underdiagnosed. Because depressed patients constantly share\ntheir symptoms, major life events, and treatments on social media, researchers\nare turning to user-generated digital traces on social media for depression\ndetection. Such methods have distinct advantages in combating depression\nbecause they can facilitate innovative approaches to fight depression and\nalleviate its social and economic burden. However, most existing studies lack\neffective means to incorporate established medical domain knowledge in\ndepression detection or suffer from feature extraction difficulties that impede\ngreater performance. Following the design science research paradigm, we propose\na Deep Knowledge-aware Depression Detection (DKDD) framework to accurately\ndetect social media users at risk of depression and explain the critical\nfactors that contribute to such detection. Extensive empirical studies with\nreal-world data demonstrate that, by incorporating domain knowledge, our method\noutperforms existing state-of-the-art methods. Our work has significant\nimplications for IS research in knowledge-aware machine learning, digital\ntraces utilization, and NLP research in IS. Practically, by providing early\ndetection and explaining the critical factors, DKDD can supplement clinical\ndepression screening and enable large-scale evaluations of a population's\nmental health status.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disambiguation of Company names via Deep Recurrent Networks. (arXiv:2303.05391v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05391","description":"<p>Name Entity Disambiguation is the Natural Language Processing task of\nidentifying textual records corresponding to the same Named Entity, i.e.\nreal-world entities represented as a list of attributes (names, places,\norganisations, etc.). In this work, we face the task of disambiguating\ncompanies on the basis of their written names. We propose a Siamese LSTM\nNetwork approach to extract -- via supervised learning -- an embedding of\ncompany name strings in a (relatively) low dimensional vector space and use\nthis representation to identify pairs of company names that actually represent\nthe same company (i.e. the same Entity).\n</p>\n<p>Given that the manual labelling of string pairs is a rather onerous task, we\nanalyse how an Active Learning approach to prioritise the samples to be\nlabelled leads to a more efficient overall learning pipeline.\n</p>\n<p>With empirical investigations, we show that our proposed Siamese Network\noutperforms several benchmark approaches based on standard string matching\nalgorithms when enough labelled data are available. Moreover, we show that\nActive Learning prioritisation is indeed helpful when labelling resources are\nlimited, and let the learning models reach the out-of-sample performance\nsaturation with less labelled data with respect to standard (random) data\nlabelling approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basile_A/0/1/0/all/0/1\">Alessandro Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crupi_R/0/1/0/all/0/1\">Riccardo Crupi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasso_M/0/1/0/all/0/1\">Michele Grasso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercanti_A/0/1/0/all/0/1\">Alessandro Mercanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regoli_D/0/1/0/all/0/1\">Daniele Regoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarsi_S/0/1/0/all/0/1\">Simone Scarsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosentini_A/0/1/0/all/0/1\">Andrea Cosentini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatically Summarizing Evidence from Clinical Trials: A Prototype Highlighting Current Challenges. (arXiv:2303.05392v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05392","description":"<p>We present TrialsSummarizer, a system that aims to automatically summarize\nevidence presented in the set of randomized controlled trials most relevant to\na given query. Building on prior work, the system retrieves trial publications\nmatching a query specifying a combination of condition, intervention(s), and\noutcome(s), and ranks these according to sample size and estimated study\nquality. The top-k such studies are passed through a neural multi-document\nsummarization system, yielding a synopsis of these trials. We consider two\narchitectures: A standard sequence-to-sequence model based on BART and a\nmulti-headed architecture intended to provide greater transparency to\nend-users. Both models produce fluent and relevant summaries of evidence\nretrieved for queries, but their tendency to introduce unsupported statements\nrender them inappropriate for use in this domain at present. The proposed\narchitecture may help users verify outputs allowing users to trace generated\ntokens back to inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramprasad_S/0/1/0/all/0/1\">Sanjana Ramprasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McInerney_D/0/1/0/all/0/1\">Denis Jered McInerney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshal_I/0/1/0/all/0/1\">Iain J. Marshal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MathPrompter: Mathematical Reasoning using Large Language Models. (arXiv:2303.05398v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05398","description":"<p>Large Language Models (LLMs) have limited performance when solving arithmetic\nreasoning tasks and often provide incorrect answers. Unlike natural language\nunderstanding, math problems typically have a single correct answer, making the\ntask of generating accurate solutions more challenging for LLMs. To the best of\nour knowledge, we are not aware of any LLMs that indicate their level of\nconfidence in their responses which fuels a trust deficit in these models\nimpeding their adoption. To address this deficiency, we propose `MathPrompter',\na technique that improves performance of LLMs on arithmetic problems along with\nincreased reliance in the predictions. MathPrompter uses the Zero-shot\nchain-of-thought prompting technique to generate multiple Algebraic expressions\nor Python functions to solve the same math problem in different ways and\nthereby raise the confidence level in the output results. This is in contrast\nto other prompt based CoT methods, where there is no check on the validity of\nthe intermediate steps followed. Our technique improves over state-of-the-art\non the MultiArith dataset ($78.7\\%\\rightarrow92.5\\%$) evaluated using 175B\nparameter GPT-based LLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imani_S/0/1/0/all/0/1\">Shima Imani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Liang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_H/0/1/0/all/0/1\">Harsh Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-Based Learning for Thread Structure Prediction in Cybersecurity Forums. (arXiv:2303.05400v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05400","description":"<p>With recent trends indicating cyber crimes increasing in both frequency and\ncost, it is imperative to develop new methods that leverage data-rich hacker\nforums to assist in combating ever evolving cyber threats. Defining\ninteractions within these forums is critical as it facilitates identifying\nhighly skilled users, which can improve prediction of novel threats and future\ncyber attacks. We propose a method called Next Paragraph Prediction with\nInstructional Prompting (NPP-IP) to predict thread structures while grounded on\nthe context around posts. This is the first time to apply an instructional\nprompting approach to the cybersecurity domain. We evaluate our NPP-IP with the\nReddit dataset and Hacker Forums dataset that has posts and thread structures\nof real hacker forums' threads, and compare our method's performance with\nexisting methods. The experimental evaluation shows that our proposed method\ncan predict the thread structure significantly better than existing methods\nallowing for better social network prediction based on forum interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kashihara_K/0/1/0/all/0/1\">Kazuaki Kashihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1\">Kuntal Kumar Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trevino_R/0/1/0/all/0/1\">Robert P Trevino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Early Warning Signals of Social Instabilities in Twitter Data. (arXiv:2303.05401v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05401","description":"<p>The goal of this project is to create and study novel techniques to identify\nearly warning signals for socially disruptive events, like riots, wars, or\nrevolutions using only publicly available data on social media. Such techniques\nneed to be robust enough to work on real-time data: to achieve this goal we\npropose a topological approach together with more standard BERT models. Indeed,\ntopology-based algorithms, being provably stable against deformations and\nnoise, seem to work well in low-data regimes. The general idea is to build a\nbinary classifier that predicts if a given tweet is related to a disruptive\nevent or not. The results indicate that the persistent-gradient approach is\nstable and even more performant than deep-learning-based anomaly detection\nalgorithms. We also benchmark the generalisability of the methodology against\nout-of-samples tasks, with very promising results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shamsaddini_V/0/1/0/all/0/1\">Vahid Shamsaddini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirveslahti_H/0/1/0/all/0/1\">Henry Kirveslahti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reinauer_R/0/1/0/all/0/1\">Raphael Reinauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_W/0/1/0/all/0/1\">Wallyson Lemes de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caorsi_M/0/1/0/all/0/1\">Matteo Caorsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voutaz_E/0/1/0/all/0/1\">Etienne Voutaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"disco: a toolkit for Distributional Control of Generative Models. (arXiv:2303.05431v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05431","description":"<p>Pre-trained language models and other generative models have revolutionized\nNLP and beyond. However, these models tend to reproduce undesirable biases\npresent in their training data. Also, they may overlook patterns that are\nimportant but challenging to capture. To address these limitations, researchers\nhave introduced distributional control techniques. These techniques, not\nlimited to language, allow controlling the prevalence (i.e., expectations) of\nany features of interest in the model's outputs. Despite their potential, the\nwidespread adoption of these techniques has been hindered by the difficulty in\nadapting complex, disconnected code. Here, we present disco, an open-source\nPython library that brings these techniques to the broader public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kruszewski_G/0/1/0/all/0/1\">Germ&#xe1;n Kruszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozen_J/0/1/0/all/0/1\">Jos Rozen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dymetman_M/0/1/0/all/0/1\">Marc Dymetman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback. (arXiv:2303.05453v1 [cs.CL])","link":"http://arxiv.org/abs/2303.05453","description":"<p>Large language models (LLMs) are used to generate content for a wide range of\ntasks, and are set to reach a growing audience in coming years due to\nintegration in product interfaces like ChatGPT or search engines like Bing.\nThis intensifies the need to ensure that models are aligned with human\npreferences and do not produce unsafe, inaccurate or toxic outputs. While\nalignment techniques like reinforcement learning with human feedback (RLHF) and\nred-teaming can mitigate some safety concerns and improve model capabilities,\nit is unlikely that an aggregate fine-tuning process can adequately represent\nthe full range of users' preferences and values. Different people may\nlegitimately disagree on their preferences for language and conversational\nnorms, as well as on values or ideologies which guide their communication.\nPersonalising LLMs through micro-level preference learning processes may result\nin models that are better aligned with each user. However, there are several\nnormative challenges in defining the bounds of a societally-acceptable and safe\ndegree of personalisation. In this paper, we ask how, and in what ways, LLMs\nshould be personalised. First, we review literature on current paradigms for\naligning LLMs with human feedback, and identify issues including (i) a lack of\nclarity regarding what alignment means; (ii) a tendency of technology providers\nto prescribe definitions of inherently subjective preferences and values; and\n(iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in\nwho we are really aligning to. Second, we present a taxonomy of benefits and\nrisks associated with personalised LLMs, for individuals and society at large.\nFinally, we propose a three-tiered policy framework that allows users to\nexperience the benefits of personalised alignment, while restraining unsafe and\nundesirable LLM-behaviours within (supra-)national and organisational bounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertie Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Planning with Large Language Models for Code Generation. (arXiv:2303.05510v1 [cs.LG])","link":"http://arxiv.org/abs/2303.05510","description":"<p>Existing large language model-based code generation pipelines typically use\nbeam search or sampling algorithms during the decoding process. Although the\nprograms they generate achieve high token-matching-based scores, they often\nfail to compile or generate incorrect outputs. The main reason is that\nconventional Transformer decoding algorithms may not be the best choice for\ncode generation. In this work, we propose a novel Transformer decoding\nalgorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning\nalgorithm to do lookahead search and guide the Transformer to generate better\nprograms. Specifically, instead of simply optimizing the likelihood of the\ngenerated sequences, the Transformer makes use of a planner to generate\ncandidate programs and test them on public test cases. The Transformer can\ntherefore make more informed decisions and generate tokens that will eventually\nlead to higher-quality programs. We also design a mechanism that shares\ninformation between the Transformer and the planner to make our algorithm\ncomputationally efficient. We empirically evaluate our framework with several\nlarge language models as backbones on public coding challenge benchmarks,\nshowing that 1) it can generate programs that consistently achieve higher\nperformance compared with competing baseline methods; 2) it enables\ncontrollable code generation, such as concise codes and highly-commented codes\nby optimizing modified objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenfang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yikang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Event Extraction with Memory-based Loss Prediction Model. (arXiv:2112.03073v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.03073","description":"<p>Event extraction (EE) plays an important role in many industrial application\nscenarios, and high-quality EE methods require a large amount of manual\nannotation data to train supervised learning models. However, the cost of\nobtaining annotation data is very high, especially for annotation of domain\nevents, which requires the participation of experts from corresponding domain.\nSo we introduce active learning (AL) technology to reduce the cost of event\nannotation. But the existing AL methods have two main problems, which make them\nnot well used for event extraction. Firstly, the existing pool-based selection\nstrategies have limitations in terms of computational cost and sample validity.\nSecondly, the existing evaluation of sample importance lacks the use of local\nsample information. In this paper, we present a novel deep AL method for EE. We\npropose a batch-based selection strategy and a Memory-Based Loss Prediction\nmodel (MBLP) to select unlabeled samples efficiently. During the selection\nprocess, we use an internal-external sample loss ranking method to evaluate the\nsample importance by using local information. Finally, we propose a delayed\ntraining strategy to train the MBLP model. Extensive experiments are performed\non three domain datasets, and our method outperforms other state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Shirong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning for Monolingual End-to-End Automatic Speech Recognition. (arXiv:2112.09427v4 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2112.09427","description":"<p>Adapting Automatic Speech Recognition (ASR) models to new domains results in\na deterioration of performance on the original domain(s), a phenomenon called\nCatastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to\nnew accents, dialects, topics, etc. without suffering from CF, making them\nunable to be continually enhanced without storing all past data. Fortunately,\nContinual Learning (CL) methods, which aim to enable continual adaptation while\novercoming CF, can be used. In this paper, we implement an extensive number of\nCL methods for End-to-End ASR and test and compare their ability to extend a\nmonolingual Hybrid CTC-Transformer model across four new tasks. We find that\nthe best performing CL method closes the gap between the fine-tuned model\n(lower bound) and the model trained jointly on all tasks (upper bound) by more\nthan 40%, while requiring access to only 0.6% of the original data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Eeckt_S/0/1/0/all/0/1\">Steven Vander Eeckt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+hamme_H/0/1/0/all/0/1\">Hugo Van hamme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraphrasing Techniques for Maritime QA system. (arXiv:2203.10854v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10854","description":"<p>There has been an increasing interest in incorporating Artificial\nIntelligence (AI) into Defence and military systems to complement and augment\nhuman intelligence and capabilities. However, much work still needs to be done\ntoward achieving an effective human-machine partnership. This work is aimed at\nenhancing human-machine communications by developing a capability for\nautomatically translating human natural language into a machine-understandable\nlanguage (e.g., SQL queries). Techniques toward achieving this goal typically\ninvolve building a semantic parser trained on a very large amount of\nhigh-quality manually-annotated data. However, in many real-world Defence\nscenarios, it is not feasible to obtain such a large amount of training data.\nTo the best of our knowledge, there are few works trying to explore the\npossibility of training a semantic parser with limited manually-paraphrased\ndata, in other words, zero-shot. In this paper, we investigate how to exploit\nparaphrasing methods for the automated generation of large-scale training\ndatasets (in the form of paraphrased utterances and their corresponding logical\nforms in SQL format) and present our experimental results using real-world data\nin the maritime domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shiri_F/0/1/0/all/0/1\">Fatemeh Shiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_R/0/1/0/all/0/1\">Reza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Context Pattern Generation for Entity Set Expansion. (arXiv:2207.08087v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.08087","description":"<p>Entity Set Expansion (ESE) is a valuable task that aims to find entities of\nthe target semantic class described by given seed entities. Various Natural\nLanguage Processing (NLP) and Information Retrieval (IR) downstream\napplications have benefited from ESE due to its ability to discover knowledge.\nAlthough existing corpus-based ESE methods have achieved great progress, they\nstill rely on corpora with high-quality entity information annotated, because\nmost of them need to obtain the context patterns through the position of the\nentity in a sentence. Therefore, the quality of the given corpora and their\nentity annotation has become the bottleneck that limits the performance of such\nmethods. To overcome this dilemma and make the ESE models free from the\ndependence on entity annotation, our work aims to explore a new ESE paradigm,\nnamely corpus-independent ESE. Specifically, we devise a context pattern\ngeneration module that utilizes autoregressive language models (e.g., GPT-2) to\nautomatically generate high-quality context patterns for entities. In addition,\nwe propose the GAPA, a novel ESE framework that leverages the aforementioned\nGenerAted PAtterns to expand target entities. Extensive experiments and\ndetailed analyses on three widely used datasets demonstrate the effectiveness\nof our method. All the codes of our experiments are available at\nhttps://github.com/geekjuruo/GAPA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shulin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flat Multi-modal Interaction Transformer for Named Entity Recognition. (arXiv:2208.11039v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2208.11039","description":"<p>Multi-modal named entity recognition (MNER) aims at identifying entity spans\nand recognizing their categories in social media posts with the aid of images.\nHowever, in dominant MNER approaches, the interaction of different modalities\nis usually carried out through the alternation of self-attention and\ncross-attention or over-reliance on the gating machine, which results in\nimprecise and biased correspondence between fine-grained semantic units of text\nand image. To address this issue, we propose a Flat Multi-modal Interaction\nTransformer (FMIT) for MNER. Specifically, we first utilize noun phrases in\nsentences and general domain words to obtain visual cues. Then, we transform\nthe fine-grained semantic representation of the vision and text into a unified\nlattice structure and design a novel relative position encoding to match\ndifferent modalities in Transformer. Meanwhile, we propose to leverage entity\nboundary detection as an auxiliary task to alleviate visual bias. Experiments\nshow that our methods achieve the new state-of-the-art performance on two\nbenchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Junyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dixiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingjian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linearly Mapping from Image to Text Space. (arXiv:2209.15162v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.15162","description":"<p>The extent to which text-only language models (LMs) learn to represent\nfeatures of the non-linguistic world is an open question. Prior work has shown\nthat pretrained LMs can be taught to caption images when a vision model's\nparameters are optimized to encode images in the language space. We test a\nstronger hypothesis: that the conceptual representations learned by frozen\ntext-only models and vision-only models are similar enough that this can be\nachieved with a linear map. We show that the image representations from vision\nmodels can be transferred as continuous prompts to frozen LMs by training only\na single linear projection. Using these to prompt the LM achieves competitive\nperformance on captioning and visual question answering tasks compared to\nmodels that tune both the image encoder and text decoder (such as the MAGMA\nmodel). We compare three image encoders with increasing amounts of linguistic\nsupervision seen during pretraining: BEIT (no linguistic information),\nNF-ResNET (lexical category information), and CLIP (full natural language\ndescriptions). We find that all three encoders perform equally well at\ntransferring visual property information to the language model (e.g., whether\nan animal is large or small), but that image encoders pretrained with\nlinguistic supervision more saliently encode category information (e.g.,\ndistinguishing hippo vs. elephant) and thus perform significantly better on\nbenchmark language-and-vision tasks. Our results indicate that LMs encode\nconceptual information structurally similarly to vision-based models, even\nthose that are solely trained on images. Code is available here:\nhttps://github.com/jmerullo/limber\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merullo_J/0/1/0/all/0/1\">Jack Merullo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castricato_L/0/1/0/all/0/1\">Louis Castricato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViLPAct: A Benchmark for Compositional Generalization on Multimodal Human Activities. (arXiv:2210.05556v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.05556","description":"<p>We introduce ViLPAct, a novel vision-language benchmark for human activity\nplanning. It is designed for a task where embodied AI agents can reason and\nforecast future actions of humans based on video clips about their initial\nactivities and intents in text. The dataset consists of 2.9k videos from\n\\charades extended with intents via crowdsourcing, a multi-choice question test\nset, and four strong baselines. One of the baselines implements a neurosymbolic\napproach based on a multi-modal knowledge base (MKB), while the other ones are\ndeep generative models adapted from recent state-of-the-art (SOTA) methods.\nAccording to our extensive experiments, the key challenges are compositional\ngeneralization and effective use of information from both modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yaqing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yuecheng Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges in Explanation Quality Evaluation. (arXiv:2210.07126v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07126","description":"<p>While much research focused on producing explanations, it is still unclear\nhow the produced explanations' quality can be evaluated in a meaningful way.\nToday's predominant approach is to quantify explanations using proxy scores\nwhich compare explanations to (human-annotated) gold explanations. This\napproach assumes that explanations which reach higher proxy scores will also\nprovide a greater benefit to human users. In this paper, we present problems of\nthis approach. Concretely, we (i) formulate desired characteristics of\nexplanation quality, (ii) describe how current evaluation practices violate\nthem, and (iii) support our argumentation with initial evidence from a\ncrowdsourcing case study in which we investigate the explanation quality of\nstate-of-the-art explainable question answering systems. We find that proxy\nscores correlate poorly with human quality ratings and, additionally, become\nless expressive the more often they are used (i.e. following Goodhart's law).\nFinally, we propose guidelines to enable a meaningful evaluation of\nexplanations to drive the development of systems that provide tangible benefits\nto human users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1\">Peng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weight Averaging: A Simple Yet Effective Method to Overcome Catastrophic Forgetting in Automatic Speech Recognition. (arXiv:2210.15282v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2210.15282","description":"<p>Adapting a trained Automatic Speech Recognition (ASR) model to new tasks\nresults in catastrophic forgetting of old tasks, limiting the model's ability\nto learn continually and to be extended to new speakers, dialects, languages,\netc. Focusing on End-to-End ASR, in this paper, we propose a simple yet\neffective method to overcome catastrophic forgetting: weight averaging. By\nsimply taking the average of the previous and the adapted model, our method\nachieves high performance on both the old and new tasks. It can be further\nimproved by introducing a knowledge distillation loss during the adaptation. We\nillustrate the effectiveness of our method on both monolingual and multilingual\nASR. In both cases, our method strongly outperforms all baselines, even in its\nsimplest form.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Eeckt_S/0/1/0/all/0/1\">Steven Vander Eeckt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+hamme_H/0/1/0/all/0/1\">Hugo Van hamme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"kogito: A Commonsense Knowledge Inference Toolkit. (arXiv:2211.08451v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08451","description":"<p>In this paper, we present kogito, an open-source tool for generating\ncommonsense inferences about situations described in text. kogito provides an\nintuitive and extensible interface to interact with natural language generation\nmodels that can be used for hypothesizing commonsense knowledge inference from\na textual input. In particular, kogito offers several features for targeted,\nmulti-granularity knowledge generation. These include a standardized API for\ntraining and evaluating knowledge models, and generating and filtering\ninferences from them. We also include helper functions for converting natural\nlanguage texts into a format ingestible by knowledge models - intermediate\npipeline stages such as knowledge head extraction from text, heuristic and\nmodel-based knowledge head-relation matching, and an ability to define and use\ncustom knowledge relations. We make the code for kogito available at\nhttps://github.com/epfl-nlp/kogito along with thorough documentation at\nhttps://kogito.readthedocs.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ismayilzada_M/0/1/0/all/0/1\">Mete Ismayilzada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training. (arXiv:2301.02228v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2301.02228","description":"<p>In this paper, we consider enhancing medical visual-language pre-training\n(VLP) with domain-specific knowledge, by exploiting the paired image-text\nreports from the radiological daily practice. In particular, we make the\nfollowing contributions: First, unlike existing works that directly process the\nraw reports, we adopt a novel triplet extraction module to extract the\nmedical-related information, avoiding unnecessary complexity from language\ngrammar and enhancing the supervision signals; Second, we propose a novel\ntriplet encoding module with entity translation by querying a knowledge base,\nto exploit the rich domain knowledge in medical field, and implicitly build\nrelationships between medical entities in the language embedding space; Third,\nwe propose to use a Transformer-based fusion model for spatially aligning the\nentity description with visual signals at the image patch level, enabling the\nability for medical diagnosis; Fourth, we conduct thorough experiments to\nvalidate the effectiveness of our architecture, and benchmark on numerous\npublic benchmarks, e.g., ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax,\nCOVIDx CXR-2, COVID Rural, and EdemaSeverity. In both zero-shot and fine-tuning\nsettings, our model has demonstrated strong performance compared with the\nformer methods on disease classification and grounding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chaoyi Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoman Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A data science and machine learning approach to continuous analysis of Shakespeare's plays. (arXiv:2301.06024v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.06024","description":"<p>The availability of quantitative methods that can analyze text has provided\nnew ways of examining literature in a manner that was not available in the\npre-information era. Here we apply comprehensive machine learning analysis to\nthe work of William Shakespeare. The analysis shows clear change in style of\nwriting over time, with the most significant changes in the sentence length,\nfrequency of adjectives and adverbs, and the sentiments expressed in the text.\nApplying machine learning to make a stylometric prediction of the year of the\nplay shows a Pearson correlation of 0.71 between the actual and predicted year,\nindicating that Shakespeare's writing style as reflected by the quantitative\nmeasurements changed over time. Additionally, it shows that the stylometrics of\nsome of the plays is more similar to plays written either before or after the\nyear they were written. For instance, Romeo and Juliet is dated 1596, but is\nmore similar in stylometrics to plays written by Shakespeare after 1600. The\nsource code for the analysis is available for free download.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Swisher_C/0/1/0/all/0/1\">Charles Swisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_L/0/1/0/all/0/1\">Lior Shamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fillers in Spoken Language Understanding: Computational and Psycholinguistic Perspectives. (arXiv:2301.10761v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10761","description":"<p>Disfluencies (i.e. interruptions in the regular flow of speech), are\nubiquitous to spoken discourse. Fillers (\"uh\", \"um\") are disfluencies that\noccur the most frequently compared to other kinds of disfluencies. Yet, to the\nbest of our knowledge, there isn't a resource that brings together the research\nperspectives influencing Spoken Language Understanding (SLU) on these speech\nevents. This aim of this article is to survey a breadth of perspectives in a\nholistic way; i.e. from considering underlying (psycho)linguistic theory, to\ntheir annotation and consideration in Automatic Speech Recognition (ASR) and\nSLU systems, to lastly, their study from a generation standpoint. This article\naims to present the perspectives in an approachable way to the SLU and\nConversational AI community, and discuss moving forward, what we believe are\nthe trends and challenges in each area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinkar_T/0/1/0/all/0/1\">Tanvi Dinkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chlo&#xe9; Clavel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilescu_I/0/1/0/all/0/1\">Ioana Vasilescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex. (arXiv:2301.12868v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12868","description":"<p>Semantic parsing is a technique aimed at constructing a structured\nrepresentation of the meaning of a natural-language question. Recent\nadvancements in few-shot language models trained on code have demonstrated\nsuperior performance in generating these representations compared to\ntraditional unimodal language models, which are trained on downstream tasks.\nDespite these advancements, existing fine-tuned neural semantic parsers are\nsusceptible to adversarial attacks on natural-language inputs. While it has\nbeen established that the robustness of smaller semantic parsers can be\nenhanced through adversarial training, this approach is not feasible for large\nlanguage models in real-world scenarios, as it requires both substantial\ncomputational resources and expensive human annotation on in-domain semantic\nparsing data. This paper presents the first empirical study on the adversarial\nrobustness of a large prompt-based language model of code, \\codex. Our results\ndemonstrate that the state-of-the-art (SOTA) code-language models are\nvulnerable to carefully crafted adversarial examples. To address this\nchallenge, we propose methods for improving robustness without the need for\nsignificant amounts of labeled data or heavy computational resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yujin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiri_F/0/1/0/all/0/1\">Fatemeh Shiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex QA and language models hybrid architectures, Survey. (arXiv:2302.09051v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.09051","description":"<p>This paper reviews the state-of-the-art of hybrid language models\narchitectures and strategies for \"complex\" question-answering (QA, CQA, CPS).\nLarge Language Models (LLM) are good at leveraging public data on standard\nproblems but once you want to tackle more specific complex questions or\nproblems you may need specific architecture, knowledge, skills, methods,\nsensitive data protection, explainability, human approval and versatile\nfeedback... We identify key elements augmenting LLM to solve complex questions\nor problems. We extend findings from the robust community edited research\npapers BIG, BLOOM and HELM which open source, benchmark and analyze limits and\nchallenges of LLM in terms of tasks complexity and strict evaluation on\naccuracy (e.g. fairness, robustness, toxicity, ...). Recent projects like\nChatGPT and GALACTICA have allowed non-specialists to grasp the great potential\nas well as the equally strong limitations of language models in complex QA.\nHybridizing these models with different components could allow to overcome\nthese different limits and go much further. We discuss some challenges\nassociated with complex QA, including domain adaptation, decomposition and\nefficient multi-step QA, long form and non-factoid QA, safety and\nmulti-sensitivity data protection, multimodal search, hallucinations,\nexplainability and truthfulness, temproal reasoning. Therefore, we analyze\ncurrent solutions and promising research trends, using elements such as: hybrid\nLLM architectures, active human reinforcement learning supervised with AI,\nprompting adaptation, neuro-symbolic and structured knowledge grounding,\nprogram synthesis, iterated decomposition and others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daull_X/0/1/0/all/0/1\">Xavier Daull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellot_P/0/1/0/all/0/1\">Patrice Bellot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruno_E/0/1/0/all/0/1\">Emmanuel Bruno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_V/0/1/0/all/0/1\">Vincent Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murisasco_E/0/1/0/all/0/1\">Elisabeth Murisasco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask-guided BERT for Few Shot Text Classification. (arXiv:2302.10447v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.10447","description":"<p>Transformer-based language models have achieved significant success in\nvarious domains. However, the data-intensive nature of the transformer\narchitecture requires much labeled data, which is challenging in low-resource\nscenarios (i.e., few-shot learning (FSL)). The main challenge of FSL is the\ndifficulty of training robust models on small amounts of samples, which\nfrequently leads to overfitting. Here we present Mask-BERT, a simple and\nmodular framework to help BERT-based architectures tackle FSL. The proposed\napproach fundamentally differs from existing FSL strategies such as prompt\ntuning and meta-learning. The core idea is to selectively apply masks on text\ninputs and filter out irrelevant information, which guides the model to focus\non discriminative tokens that influence prediction results. In addition, to\nmake the text representations from different categories more separable and the\ntext representations from the same category more compact, we introduce a\ncontrastive learning loss function. Experimental results on public-domain\nbenchmark datasets demonstrate the effectiveness of Mask-BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wenxiong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haixing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoke Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuzhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongmin Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. (arXiv:2302.12813v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12813","description":"<p>Large language models (LLMs), such as ChatGPT, are able to generate\nhuman-like, fluent responses for many downstream tasks, e.g., task-oriented\ndialog and question answering. However, applying LLMs to real-world,\nmission-critical applications remains challenging mainly due to their tendency\nto generate hallucinations and their inability to use external knowledge. This\npaper proposes a LLM-Augmenter system, which augments a black-box LLM with a\nset of plug-and-play modules. Our system makes the LLM generate responses\ngrounded in external knowledge, e.g., stored in task-specific databases. It\nalso iteratively revises LLM prompts to improve model responses using feedback\ngenerated by utility functions, e.g., the factuality score of a LLM-generated\nresponse. The effectiveness of LLM-Augmenter is empirically validated on two\ntypes of scenarios, task-oriented dialog and open-domain question answering.\nLLM-Augmenter significantly reduces ChatGPT's hallucinations without\nsacrificing the fluency and informativeness of its responses. We make the\nsource code and models publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiuyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liden_L/0/1/0/all/0/1\">Lars Liden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Feasibility of ChatGPT for Event Extraction. (arXiv:2303.03836v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.03836","description":"<p>Event extraction is a fundamental task in natural language processing that\ninvolves identifying and extracting information about events mentioned in text.\nHowever, it is a challenging task due to the lack of annotated data, which is\nexpensive and time-consuming to obtain. The emergence of large language models\n(LLMs) such as ChatGPT provides an opportunity to solve language tasks with\nsimple prompts without the need for task-specific datasets and fine-tuning.\nWhile ChatGPT has demonstrated impressive results in tasks like machine\ntranslation, text summarization, and question answering, it presents challenges\nwhen used for complex tasks like event extraction. Unlike other tasks, event\nextraction requires the model to be provided with a complex set of instructions\ndefining all event types and their schemas. To explore the feasibility of\nChatGPT for event extraction and the challenges it poses, we conducted a series\nof experiments. Our results show that ChatGPT has, on average, only 51.04% of\nthe performance of a task-specific model such as EEQA in long-tail and complex\nscenarios. Our usability testing experiments indicate that ChatGPT is not\nrobust enough, and continuous refinement of the prompt does not lead to stable\nperformance improvements, which can result in a poor user experience. Besides,\nChatGPT is highly sensitive to different prompt styles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Challenging Benchmark for Low-Resource Learning. (arXiv:2303.03840v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.03840","description":"<p>With promising yet saturated results in high-resource settings, low-resource\ndatasets have gradually become popular benchmarks for evaluating the learning\nability of advanced neural networks (e.g., BigBench, superGLUE). Some models\neven surpass humans according to benchmark test results. However, we find that\nthere exists a set of hard examples in low-resource settings that challenge\nneural networks but are not well evaluated, which causes over-estimated\nperformance. We first give a theoretical analysis on which factors bring the\ndifficulty of low-resource learning. It then motivate us to propose a\nchallenging benchmark hardBench to better evaluate the learning ability, which\ncovers 11 datasets, including 3 computer vision (CV) datasets and 8 natural\nlanguage process (NLP) datasets. Experiments on a wide range of models show\nthat neural networks, even pre-trained language models, have sharp performance\ndrops on our benchmark, demonstrating the effectiveness on evaluating the\nweaknesses of neural networks. On NLP tasks, we surprisingly find that despite\nbetter results on traditional low-resource benchmarks, pre-trained networks,\ndoes not show performance improvements on our benchmarks. These results\ndemonstrate that there are still a large robustness gap between existing models\nand human-level performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELODIN: Naming Concepts in Embedding Spaces. (arXiv:2303.04001v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.04001","description":"<p>Despite recent advancements, the field of text-to-image synthesis still\nsuffers from lack of fine-grained control. Using only text, it remains\nchallenging to deal with issues such as concept coherence and concept\ncontamination. We propose a method to enhance control by generating specific\nconcepts that can be reused throughout multiple images, effectively expanding\nnatural language with new words that can be combined much like a painter's\npalette. Unlike previous contributions, our method does not copy visuals from\ninput data and can generate concepts through text alone. We perform a set of\ncomparisons that finds our method to be a significant improvement over\ntext-only prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mello_R/0/1/0/all/0/1\">Rodrigo Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calegario_F/0/1/0/all/0/1\">Filipe Calegario</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramalho_G/0/1/0/all/0/1\">Geber Ramalho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Risks of Stealing the Decoding Algorithms of Language Models. (arXiv:2303.04729v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.04729","description":"<p>A key component of generating text from modern language models (LM) is the\nselection and tuning of decoding algorithms. These algorithms determine how to\ngenerate text from the internal probability distribution generated by the LM.\nThe process of choosing a decoding algorithm and tuning its hyperparameters\ntakes significant time, manual effort, and computation, and it also requires\nextensive human evaluation. Therefore, the identity and hyperparameters of such\ndecoding algorithms are considered to be extremely valuable to their owners. In\nthis work, we show, for the first time, that an adversary with typical API\naccess to an LM can steal the type and hyperparameters of its decoding\nalgorithms at very low monetary costs. Our attack is effective against popular\nLMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the\nfeasibility of stealing such information with only a few dollars, e.g.,\n$\\$0.8$, $\\$1$, $\\$4$, and $\\$40$ for the four versions of GPT-3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naseh_A/0/1/0/all/0/1\">Ali Naseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houmansadr_A/0/1/0/all/0/1\">Amir Houmansadr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}