{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-11-08T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Leveraging Generative AI: Improving Software Metadata Classification with Generated Code-Comment Pairs. (arXiv:2311.03365v1 [cs.SE])","link":"http://arxiv.org/abs/2311.03365","description":"<p>In software development, code comments play a crucial role in enhancing code\ncomprehension and collaboration. This research paper addresses the challenge of\nobjectively classifying code comments as \"Useful\" or \"Not Useful.\" We propose a\nnovel solution that harnesses contextualized embeddings, particularly BERT, to\nautomate this classification process. We address this task by incorporating\ngenerated code and comment pairs. The initial dataset comprised 9048 pairs of\ncode and comments written in C, labeled as either Useful or Not Useful. To\naugment this dataset, we sourced an additional 739 lines of code-comment pairs\nand generated labels using a Large Language Model Architecture, specifically\nBERT. The primary objective was to build classification models that can\neffectively differentiate between useful and not useful code comments. Various\nmachine learning algorithms were employed, including Logistic Regression,\nDecision Tree, K-Nearest Neighbors (KNN), Support Vector Machine (SVM),\nGradient Boosting, Random Forest, and a Neural Network. Each algorithm was\nevaluated using precision, recall, and F1-score metrics, both with the original\nseed dataset and the augmented dataset. This study showcases the potential of\ngenerative AI for enhancing binary code comment quality classification models,\nproviding valuable insights for software developers and researchers in the\nfield of natural language processing and software engineering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1\">Samah Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_A/0/1/0/all/0/1\">Angel Deborah S</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Augmentations with R-drop for Classification of Tweets Self Reporting Covid-19. (arXiv:2311.03420v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03420","description":"<p>This paper presents models created for the Social Media Mining for Health\n2023 shared task. Our team addressed the first task, classifying tweets that\nself-report Covid-19 diagnosis. Our approach involves a classification model\nthat incorporates diverse textual augmentations and utilizes R-drop to augment\ndata and mitigate overfitting, boosting model efficacy. Our leading model,\nenhanced with R-drop and augmentations like synonym substitution, reserved\nwords, and back translations, outperforms the task mean and median scores. Our\nsystem achieves an impressive F1 score of 0.877 on the test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_S/0/1/0/all/0/1\">Sumam Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Exemplars as Clues to Retrieving from Large Associative Memory. (arXiv:2311.03498v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03498","description":"<p>Recently, large language models (LLMs) have made remarkable progress in\nnatural language processing. The most representative ability of LLMs is\nin-context learning (ICL), which enables LLMs to learn patterns from in-context\nexemplars without training. The performance of ICL greatly depends on the\nexemplars used. However, how to choose exemplars remains unclear due to the\nlack of understanding of how in-context learning works. In this paper, we\npresent a novel perspective on ICL by conceptualizing it as contextual\nretrieval from a model of associative memory. We establish a theoretical\nframework of ICL based on Hopfield Networks. Based on our framework, we look\ninto how in-context exemplars influence the performance of ICL and propose more\nefficient active exemplar selection. Our study sheds new light on the mechanism\nof ICL by connecting it to memory retrieval, with potential implications for\nadvancing the understanding of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiachen Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spoken Dialogue System for Medical Prescription Acquisition on Smartphone: Development, Corpus and Evaluation. (arXiv:2311.03510v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03510","description":"<p>Hospital information systems (HIS) have become an essential part of\nhealthcare institutions and now incorporate prescribing support software.\nPrescription support software allows for structured information capture, which\nimproves the safety, appropriateness and efficiency of prescriptions and\nreduces the number of adverse drug events (ADEs). However, such a system\nincreases the amount of time physicians spend at a computer entering\ninformation instead of providing medical care. In addition, any new visiting\nclinician must learn to manage complex interfaces since each HIS has its own\ninterfaces. In this paper, we present a natural language interface for\ne-prescribing software in the form of a spoken dialogue system accessible on a\nsmartphone. This system allows prescribers to record their prescriptions\nverbally, a form of interaction closer to their usual practice. The system\nextracts the formal representation of the prescription ready to be checked by\nthe prescribing software and uses the dialogue to request mandatory\ninformation, correct errors or warn of particular situations. Since, to the\nbest of our knowledge, there is no existing voice-based prescription dialogue\nsystem, we present the system developed in a low-resource environment, focusing\non dialogue modeling, semantic extraction and data augmentation. The system was\nevaluated in the wild with 55 participants. This evaluation showed that our\nsystem has an average prescription time of 66.15 seconds for physicians and\n35.64 seconds for other experts, and a task success rate of 76\\% for physicians\nand 72\\% for other experts. All evaluation data were recorded and annotated to\nform PxCorpus, the first spoken drug prescription corpus that has been made\nfully available to the community\n(\\url{https://doi.org/10.5281/zenodo.6524162}).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocabiyikoglu_A/0/1/0/all/0/1\">Ali Can Kocabiyikoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portet_F/0/1/0/all/0/1\">Fran&#xe7;ois Portet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babouchkine_J/0/1/0/all/0/1\">Jean-Marc Babouchkine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibert_P/0/1/0/all/0/1\">Prudence Gibert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanchon_H/0/1/0/all/0/1\">Herv&#xe9; Blanchon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavazzi_G/0/1/0/all/0/1\">Ga&#xeb;tan Gavazzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Uncertainty in Natural Language Explanations of Large Language Models. (arXiv:2311.03533v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03533","description":"<p>Large Language Models (LLMs) are increasingly used as powerful tools for\nseveral high-stakes natural language processing (NLP) applications. Recent\nprompting works claim to elicit intermediate reasoning steps and key tokens\nthat serve as proxy explanations for LLM predictions. However, there is no\ncertainty whether these explanations are reliable and reflect the LLMs\nbehavior. In this work, we make one of the first attempts at quantifying the\nuncertainty in explanations of LLMs. To this end, we propose two novel metrics\n-- $\\textit{Verbalized Uncertainty}$ and $\\textit{Probing Uncertainty}$ -- to\nquantify the uncertainty of generated explanations. While verbalized\nuncertainty involves prompting the LLM to express its confidence in its\nexplanations, probing uncertainty leverages sample and model perturbations as a\nmeans to quantify the uncertainty. Our empirical analysis of benchmark datasets\nreveals that verbalized uncertainty is not a reliable estimate of explanation\nconfidence. Further, we show that the probing uncertainty estimates are\ncorrelated with the faithfulness of an explanation, with lower uncertainty\ncorresponding to explanations with higher faithfulness. Our study provides\ninsights into the challenges and opportunities of quantifying uncertainty in\nLLM explanations, contributing to the broader discussion of the trustworthiness\nof foundation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanneru_S/0/1/0/all/0/1\">Sree Harsha Tanneru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Unlocks Emotions: Text-based Emotion Classification Dataset Auditing with Large Language Models. (arXiv:2311.03551v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03551","description":"<p>The lack of contextual information in text data can make the annotation\nprocess of text-based emotion classification datasets challenging. As a result,\nsuch datasets often contain labels that fail to consider all the relevant\nemotions in the vocabulary. This misalignment between text inputs and labels\ncan degrade the performance of machine learning models trained on top of them.\nAs re-annotating entire datasets is a costly and time-consuming task that\ncannot be done at scale, we propose to use the expressive capabilities of large\nlanguage models to synthesize additional context for input text to increase its\nalignment with the annotated emotional labels. In this work, we propose a\nformal definition of textual context to motivate a prompting strategy to\nenhance such contextual information. We provide both human and empirical\nevaluation to demonstrate the efficacy of the enhanced context. Our method\nimproves alignment between inputs and their human-annotated labels from both an\nempirical and human-evaluated standpoint.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Daniel Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kommineni_A/0/1/0/all/0/1\">Aditya Kommineni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshehri_M/0/1/0/all/0/1\">Mohammad Alshehri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_N/0/1/0/all/0/1\">Nilamadhab Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_V/0/1/0/all/0/1\">Vedant Modi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1\">Jonathan Gratch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Adversarial Datasets. (arXiv:2311.03566v1 [cs.LG])","link":"http://arxiv.org/abs/2311.03566","description":"<p>In the era of widespread public use of AI systems across various domains,\nensuring adversarial robustness has become increasingly vital to maintain\nsafety and prevent undesirable errors. Researchers have curated various\nadversarial datasets (through perturbations) for capturing model deficiencies\nthat cannot be revealed in standard benchmark datasets. However, little is\nknown about how these adversarial examples differ from the original data\npoints, and there is still no methodology to measure the intended and\nunintended consequences of those adversarial transformations. In this research,\nwe conducted a systematic survey of existing quantifiable metrics that describe\ntext instances in NLP tasks, among dimensions of difficulty, diversity, and\ndisagreement. We selected several current adversarial effect datasets and\ncompared the distributions between the original and their adversarial\ncounterparts. The results provide valuable insights into what makes these\ndatasets more challenging from a metrics perspective and whether they align\nwith underlying assumptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuanchen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Raoyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1\">Vijay Viswanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_T/0/1/0/all/0/1\">Tzu-Sheng Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dimensions of Online Conflict: Towards Modeling Agonism. (arXiv:2311.03584v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03584","description":"<p>Agonism plays a vital role in democratic dialogue by fostering diverse\nperspectives and robust discussions. Within the realm of online conflict there\nis another type: hateful antagonism, which undermines constructive dialogue.\nDetecting conflict online is central to platform moderation and monetization.\nIt is also vital for democratic dialogue, but only when it takes the form of\nagonism. To model these two types of conflict, we collected Twitter\nconversations related to trending controversial topics. We introduce a\ncomprehensive annotation schema for labelling different dimensions of conflict\nin the conversations, such as the source of conflict, the target, and the\nrhetorical strategies deployed. Using this schema, we annotated approximately\n4,000 conversations with multiple labels. We then trained both logistic\nregression and transformer-based models on the dataset, incorporating context\nfrom the conversation, including the number of participants and the structure\nof the interactions. Results show that contextual labels are helpful in\nidentifying conflict and make the models robust to variations in topic. Our\nresearch contributes a conceptualization of different dimensions of conflict, a\nrichly annotated dataset, and promising results that can contribute to content\nmoderation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Canute_M/0/1/0/all/0/1\">Matt Canute</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Mali Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+holtzclaw_h/0/1/0/all/0/1\">hannah holtzclaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lusoli_A/0/1/0/all/0/1\">Alberto Lusoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_P/0/1/0/all/0/1\">Philippa R Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandya_M/0/1/0/all/0/1\">Mugdha Pandya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taboada_M/0/1/0/all/0/1\">Maite Taboada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynard_D/0/1/0/all/0/1\">Diana Maynard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_W/0/1/0/all/0/1\">Wendy Hui Kyong Chun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STONYBOOK: A System and Resource for Large-Scale Analysis of Novels. (arXiv:2311.03614v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03614","description":"<p>Books have historically been the primary mechanism through which narratives\nare transmitted. We have developed a collection of resources for the\nlarge-scale analysis of novels, including: (1) an open source end-to-end NLP\nanalysis pipeline for the annotation of novels into a standard XML format, (2)\na collection of 49,207 distinct cleaned and annotated novels, and (3) a\ndatabase with an associated web interface for the large-scale aggregate\nanalysis of these literary works. We describe the major functionalities\nprovided in the annotation system along with their utilities. We present\nsamples of analysis artifacts from our website, such as visualizations of\ncharacter occurrences and interactions, similar books, representative\nvocabulary, part of speech statistics, and readability metrics. We also\ndescribe the use of the annotated format in qualitative and quantitative\nanalysis across large corpora of novels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pethe_C/0/1/0/all/0/1\">Charuta Pethe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Allen Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakar_R/0/1/0/all/0/1\">Rajesh Prabhakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pial_T/0/1/0/all/0/1\">Tanzir Pial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skiena_S/0/1/0/all/0/1\">Steven Skiena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GNAT: A General Narrative Alignment Tool. (arXiv:2311.03627v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03627","description":"<p>Algorithmic sequence alignment identifies similar segments shared between\npairs of documents, and is fundamental to many NLP tasks. But it is difficult\nto recognize similarities between distant versions of narratives such as\ntranslations and retellings, particularly for summaries and abridgements which\nare much shorter than the original novels.\n</p>\n<p>We develop a general approach to narrative alignment coupling the\nSmith-Waterman algorithm from bioinformatics with modern text similarity\nmetrics. We show that the background of alignment scores fits a Gumbel\ndistribution, enabling us to define rigorous p-values on the significance of\nany alignment. We apply and evaluate our general narrative alignment tool\n(GNAT) on four distinct problem domains differing greatly in both the relative\nand absolute length of documents, namely summary-to-book alignment, translated\nbook alignment, short story alignment, and plagiarism detection --\ndemonstrating the power and performance of our methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pial_T/0/1/0/all/0/1\">Tanzir Pial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skiena_S/0/1/0/all/0/1\">Steven Skiena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Innovation and Word Usage Patterns in Machine Learning. (arXiv:2311.03633v1 [cs.LG])","link":"http://arxiv.org/abs/2311.03633","description":"<p>In this study, we delve into the dynamic landscape of machine learning\nresearch evolution. Initially, through the utilization of Latent Dirichlet\nAllocation, we discern pivotal themes and fundamental concepts that have\nemerged within the realm of machine learning. Subsequently, we undertake a\ncomprehensive analysis to track the evolutionary trajectories of these\nidentified themes. To quantify the novelty and divergence of research\ncontributions, we employ the Kullback-Leibler Divergence metric. This\nstatistical measure serves as a proxy for ``surprise'', indicating the extent\nof differentiation between the content of academic papers and the subsequent\ndevelopments in research. By amalgamating these insights, we gain the ability\nto ascertain the pivotal roles played by prominent researchers and the\nsignificance of specific academic venues (periodicals and conferences) within\nthe machine learning domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borges_V/0/1/0/all/0/1\">V&#xed;tor Bandeira Borges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cajueiro_D/0/1/0/all/0/1\">Daniel Oliveira Cajueiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Linear Representation Hypothesis and the Geometry of Large Language Models. (arXiv:2311.03658v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03658","description":"<p>Informally, the 'linear representation hypothesis' is the idea that\nhigh-level concepts are represented linearly as directions in some\nrepresentation space. In this paper, we address two closely related questions:\nWhat does \"linear representation\" actually mean? And, how do we make sense of\ngeometric notions (e.g., cosine similarity or projection) in the representation\nspace? To answer these, we use the language of counterfactuals to give two\nformalizations of \"linear representation\", one in the output (word)\nrepresentation space, and one in the input (sentence) space. We then prove\nthese connect to linear probing and model steering, respectively. To make sense\nof geometric notions, we use the formalization to identify a particular\n(non-Euclidean) inner product that respects language structure in a sense we\nmake precise. Using this causal inner product, we show how to unify all notions\nof linear representation. In particular, this allows the construction of probes\nand steering vectors using counterfactual pairs. Experiments with LLaMA-2\ndemonstrate the existence of linear representations of concepts, the connection\nto interpretation and control, and the fundamental role of the choice of inner\nproduct.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kiho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_Y/0/1/0/all/0/1\">Yo Joong Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1\">Victor Veitch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalization of NLP Models: Notion and Causation. (arXiv:2311.03663v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03663","description":"<p>The NLP community typically relies on performance of a model on a held-out\ntest set to assess generalization. Performance drops observed in datasets\noutside of official test sets are generally attributed to\n\"out-of-distribution'' effects. Here, we explore the foundations of\ngeneralizability and study the various factors that affect it, articulating\ngeneralizability lessons from clinical studies. In clinical research\ngeneralizability depends on (a) internal validity of experiments to ensure\ncontrolled measurement of cause and effect, and (b) external validity or\ntransportability of the results to the wider population. We present the need to\nensure internal validity when building machine learning models in natural\nlanguage processing, especially where results may be impacted by spurious\ncorrelations in the data. We demonstrate how spurious factors, such as the\ndistance between entities in relation extraction tasks, can affect model\ninternal validity and in turn adversely impact generalization. We also offer\nguidance on how to analyze generalization failures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elangovan_A/0/1/0/all/0/1\">Aparna Elangovan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiayuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1\">Karin Verspoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CBSiMT: Mitigating Hallucination in Simultaneous Machine Translation with Weighted Prefix-to-Prefix Training. (arXiv:2311.03672v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03672","description":"<p>Simultaneous machine translation (SiMT) is a challenging task that requires\nstarting translation before the full source sentence is available.\nPrefix-to-prefix framework is often applied to SiMT, which learns to predict\ntarget tokens using only a partial source prefix. However, due to the word\norder difference between languages, misaligned prefix pairs would make SiMT\nmodels suffer from serious hallucination problems, i.e. target outputs that are\nunfaithful to source inputs. Such problems can not only produce target tokens\nthat are not supported by the source prefix, but also hinder generating the\ncorrect translation by receiving more source words. In this work, we propose a\nConfidence-Based Simultaneous Machine Translation (CBSiMT) framework, which\nuses model confidence to perceive hallucination tokens and mitigates their\nnegative impact with weighted prefix-to-prefix training. Specifically,\ntoken-level and sentence-level weights are calculated based on model confidence\nand acted on the loss function. We explicitly quantify the faithfulness of the\ngenerated target tokens using the token-level weight, and employ the\nsentence-level weight to alleviate the disturbance of sentence pairs with\nserious word order differences on the model. Experimental results on MuST-C\nEnglish-to-Chinese and WMT15 German-to-English SiMT tasks demonstrate that our\nmethod can consistently improve translation quality at most latency regimes,\nwith up to 2 BLEU scores improvement at low latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengge Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yanzhi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuhang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_J/0/1/0/all/0/1\">Jian Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuoying Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models. (arXiv:2311.03687v1 [cs.PF])","link":"http://arxiv.org/abs/2311.03687","description":"<p>Large Language Models (LLMs) have seen great advance in both academia and\nindustry, and their popularity results in numerous open-source frameworks and\ntechniques in accelerating LLM pre-training, fine-tuning, and inference.\nTraining and deploying LLMs are expensive as it requires considerable computing\nresources and memory, hence many efficient approaches have been developed for\nimproving system pipelines as well as operators. However, the runtime\nperformance can vary significantly across hardware and software stacks, which\nmakes it difficult to choose the best configuration. In this work, we aim to\nbenchmark the performance from both macro and micro perspectives. First, we\nbenchmark the end-to-end performance of pre-training, fine-tuning, and serving\nLLMs in different sizes , i.e., 7, 13, and 70 billion parameters (7B, 13B, and\n70B) on three 8-GPU platforms with and without individual optimization\ntechniques, including ZeRO, quantization, recomputation, FlashAttention. Then,\nwe dive deeper to provide a detailed runtime analysis of the sub-modules,\nincluding computing and communication operators in LLMs. For end users, our\nbenchmark and findings help better understand different optimization\ntechniques, training and inference frameworks, together with hardware platforms\nin choosing configurations for deploying LLMs. For researchers, our in-depth\nmodule-wise analyses discover potential opportunities for future work to\nfurther optimize the runtime performance of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longteng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xinglin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1\">Peijie Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Ruibo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Rui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1\">Qiong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shaohuai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaowen Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilingual Corpus Mining and Multistage Fine-Tuning for Improving Machine Translation of Lecture Transcripts. (arXiv:2311.03696v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03696","description":"<p>Lecture transcript translation helps learners understand online courses,\nhowever, building a high-quality lecture machine translation system lacks\npublicly available parallel corpora. To address this, we examine a framework\nfor parallel corpus mining, which provides a quick and effective way to mine a\nparallel corpus from publicly available lectures on Coursera. To create the\nparallel corpora, we propose a dynamic programming based sentence alignment\nalgorithm which leverages the cosine similarity of machine-translated\nsentences. The sentence alignment F1 score reaches 96%, which is higher than\nusing the BERTScore, LASER, or sentBERT methods. For both English--Japanese and\nEnglish--Chinese lecture translations, we extracted parallel corpora of\napproximately 50,000 lines and created development and test sets through manual\nfiltering for benchmarking translation performance. Through machine translation\nexperiments, we show that the mined corpora enhance the quality of lecture\ntranscript translation when used in conjunction with out-of-domain parallel\ncorpora via multistage fine-tuning. Furthermore, this study also suggests\nguidelines for gathering and cleaning corpora, mining parallel sentences,\ncleaning noise in the mined data, and creating high-quality evaluation splits.\nFor the sake of reproducibility, we have released the corpora as well as the\ncode to create them. The dataset is available at\nhttps://github.com/shyyhs/CourseraParallelCorpusMining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haiyue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_A/0/1/0/all/0/1\">Atsushi Fujita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM as an Art Director (LaDi): Using LLMs to improve Text-to-Media Generators. (arXiv:2311.03716v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03716","description":"<p>Recent advancements in text-to-image generation have revolutionized numerous\nfields, including art and cinema, by automating the generation of high-quality,\ncontext-aware images and video. However, the utility of these technologies is\noften limited by the inadequacy of text prompts in guiding the generator to\nproduce artistically coherent and subject-relevant images. In this paper, We\ndescribe the techniques that can be used to make Large Language Models (LLMs)\nact as Art Directors that enhance image and video generation. We describe our\nunified system for this called \"LaDi\". We explore how LaDi integrates multiple\ntechniques for augmenting the capabilities of text-to-image generators (T2Is)\nand text-to-video generators (T2Vs), with a focus on constrained decoding,\nintelligent prompting, fine-tuning, and retrieval. LaDi and these techniques\nare being used today in apps and platforms developed by Plai Labs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roush_A/0/1/0/all/0/1\">Allen Roush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakirov_E/0/1/0/all/0/1\">Emil Zakirov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirokov_A/0/1/0/all/0/1\">Artemiy Shirokov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lunina_P/0/1/0/all/0/1\">Polina Lunina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gane_J/0/1/0/all/0/1\">Jack Gane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duffy_A/0/1/0/all/0/1\">Alexander Duffy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basil_C/0/1/0/all/0/1\">Charlie Basil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitcomb_A/0/1/0/all/0/1\">Aber Whitcomb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benedetto_J/0/1/0/all/0/1\">Jim Benedetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeWolfe_C/0/1/0/all/0/1\">Chris DeWolfe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Large Language Models Attribution. (arXiv:2311.03731v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03731","description":"<p>Open-domain generative systems have gained significant attention in the field\nof conversational AI (e.g., generative search engines). This paper presents a\ncomprehensive review of the attribution mechanisms employed by these systems,\nparticularly large language models. Though attribution or citation improve the\nfactuality and verifiability, issues like ambiguous knowledge reservoirs,\ninherent biases, and the drawbacks of excessive attribution can hinder the\neffectiveness of these systems. The aim of this survey is to provide valuable\ninsights for researchers, aiding in the refinement of attribution methodologies\nto enhance the reliability and veracity of responses generated by open-domain\ngenerative systems. We believe that this field is still in its early stages;\nhence, we maintain a repository to keep track of ongoing studies at\nhttps://github.com/HITsz-TMG/awesome-llm-attributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongfang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zetian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinshuo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Aiguo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Learn for Few-shot Continual Active Learning. (arXiv:2311.03732v1 [cs.LG])","link":"http://arxiv.org/abs/2311.03732","description":"<p>Continual learning strives to ensure stability in solving previously seen\ntasks while demonstrating plasticity in a novel domain. Recent advances in CL\nare mostly confined to a supervised learning setting, especially in NLP domain.\nIn this work, we consider a few-shot continual active learning (CAL) setting\nwhere labeled data is inadequate, and unlabeled data is abundant but with a\nlimited annotation budget. We propose a simple but efficient method, called\nMeta-Continual Active Learning. Specifically, we employ meta-learning and\nexperience replay to address the trade-off between stability and plasticity. As\na result, it finds an optimal initialization that efficiently utilizes\nannotated information for fast adaptation while preventing catastrophic\nforgetting of past tasks. We conduct extensive experiments to validate the\neffectiveness of the proposed method and analyze the effect of various active\nlearning strategies and memory sample selection methods in a few-shot CAL\nsetup. Our experiment results demonstrate that random sampling is the best\ndefault strategy for both active learning and memory sample selection to solve\nfew-shot CAL problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1\">Stella Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Longxiang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning. (arXiv:2311.03734v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03734","description":"<p>Neural models, including large language models (LLMs), achieve superior\nperformance on multi-hop question-answering. To elicit reasoning capabilities\nfrom LLMs, recent works propose using the chain-of-thought (CoT) mechanism to\ngenerate both the reasoning chain and the answer, which enhances the model's\ncapabilities in conducting multi-hop reasoning. However, several challenges\nstill remain: such as struggling with inaccurate reasoning, hallucinations, and\nlack of interpretability. On the other hand, information extraction (IE)\nidentifies entities, relations, and events grounded to the text. The extracted\nstructured information can be easily interpreted by humans and machines\n(Grishman, 2019). In this work, we investigate constructing and leveraging\nextracted semantic structures (graphs) for multi-hop question answering,\nespecially the reasoning process. Empirical results and human evaluations show\nthat our framework: generates more faithful reasoning chains and substantially\nimproves the QA performance on two benchmark datasets. Moreover, the extracted\nstructures themselves naturally provide grounded explanations that are\npreferred by humans, as compared to the generated reasoning chains and\nsaliency-based explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruosen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xinya Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning. (arXiv:2311.03748v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03748","description":"<p>Unified Sequence Labeling that articulates different sequence labeling\nproblems such as Named Entity Recognition, Relation Extraction, Semantic Role\nLabeling, etc. in a generalized sequence-to-sequence format opens up the\nopportunity to make the maximum utilization of large language model knowledge\ntoward structured prediction. Unfortunately, this requires formatting them into\nspecialized augmented format unknown to the base pretrained language model\n(PLMs) necessitating finetuning to the target format. This significantly bounds\nits usefulness in data-limited settings where finetuning large models cannot\nproperly generalize to the target format. To address this challenge and\nleverage PLM knowledge effectively, we propose FISH-DIP, a sample-aware dynamic\nsparse finetuning strategy that selectively focuses on a fraction of\nparameters, informed by feedback from highly regressing examples, during the\nfine-tuning process. By leveraging the dynamism of sparsity, our approach\nmitigates the impact of well-learned samples and prioritizes underperforming\ninstances for improvement in generalization. Across five tasks of sequence\nlabeling, we demonstrate that FISH-DIP can smoothly optimize the model in low\nresource settings offering upto 40% performance improvements over full\nfine-tuning depending on target evaluation settings. Also, compared to\nin-context learning and other parameter-efficient fine-tuning approaches,\nFISH-DIP performs comparably or better, notably in extreme low-resource\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sarkar Snigdha Sarathi Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ranran Haoran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COOL: A Constraint Object-Oriented Logic Programming Language and its Neural-Symbolic Compilation System. (arXiv:2311.03753v1 [cs.AI])","link":"http://arxiv.org/abs/2311.03753","description":"<p>This paper explores the integration of neural networks with logic\nprogramming, addressing the longstanding challenges of combining the\ngeneralization and learning capabilities of neural networks with the precision\nof symbolic logic. Traditional attempts at this integration have been hampered\nby difficulties in initial data acquisition, the reliability of undertrained\nnetworks, and the complexity of reusing and augmenting trained models. To\novercome these issues, we introduce the COOL (Constraint Object-Oriented Logic)\nprogramming language, an innovative approach that seamlessly combines logical\nreasoning with neural network technologies. COOL is engineered to autonomously\nhandle data collection, mitigating the need for user-supplied initial data. It\nincorporates user prompts into the coding process to reduce the risks of\nundertraining and enhances the interaction among models throughout their\nlifecycle to promote the reuse and augmentation of networks. Furthermore, the\nfoundational principles and algorithms in COOL's design and its compilation\nsystem could provide valuable insights for future developments in programming\nlanguages and neural network architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jipeng Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Which is better? Exploring Prompting Strategy For LLM-based Metrics. (arXiv:2311.03754v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03754","description":"<p>This paper describes the DSBA submissions to the Prompting Large Language\nModels as Explainable Metrics shared task, where systems were submitted to two\ntracks: small and large summarization tracks. With advanced Large Language\nModels (LLMs) such as GPT-4, evaluating the quality of Natural Language\nGeneration (NLG) has become increasingly paramount. Traditional\nsimilarity-based metrics such as BLEU and ROUGE have shown to misalign with\nhuman evaluation and are ill-suited for open-ended generation tasks. To address\nthis issue, we explore the potential capability of LLM-based metrics,\nespecially leveraging open-source LLMs. In this study, wide range of prompts\nand prompting techniques are systematically analyzed with three approaches:\nprompting strategy, score aggregation, and explainability. Our research focuses\non formulating effective prompt templates, determining the granularity of NLG\nquality scores and assessing the impact of in-context examples on LLM-based\nevaluation. Furthermore, three aggregation strategies are compared to identify\nthe most reliable method for aggregating NLG quality scores. To examine\nexplainability, we devise a strategy that generates rationales for the scores\nand analyzes the characteristics of the explanation produced by the open-source\nLLMs. Extensive experiments provide insights regarding evaluation capabilities\nof open-source LLMs and suggest effective prompting strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joonghoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Saeran Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_K/0/1/0/all/0/1\">Kiyoon Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seung Hun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1\">Pilsung Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Mathematical Autoformalization. (arXiv:2311.03755v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03755","description":"<p>Autoformalization is the task of translating natural language materials into\nmachine-verifiable formalisations. Progress in autoformalization research is\nhindered by the lack of a sizeable dataset consisting of informal-formal pairs\nexpressing the same essence. Existing methods tend to circumvent this challenge\nby manually curating small corpora or using few-shot learning with large\nlanguage models. But these methods suffer from data scarcity and formal\nlanguage acquisition difficulty. In this work, we create $\\texttt{MMA}$, a\nlarge, flexible, multilingual, and multi-domain dataset of informal-formal\npairs, by using a language model to translate in the reverse direction, that\nis, from formal mathematical statements into corresponding informal ones.\nExperiments show that language models fine-tuned on $\\texttt{MMA}$ produce\n$16-18\\%$ of statements acceptable with minimal corrections on the\n$\\texttt{miniF2F}$ and $\\texttt{ProofNet}$ benchmarks, up from $0\\%$ with the\nbase model. We demonstrate that fine-tuning on multilingual formal data results\nin more capable autoformalization models even when deployed on monolingual\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Albert Q. Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamnik_M/0/1/0/all/0/1\">Mateja Jamnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender Inflected or Bias Inflicted: On Using Grammatical Gender Cues for Bias Evaluation in Machine Translation. (arXiv:2311.03767v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03767","description":"<p>Neural Machine Translation (NMT) models are state-of-the-art for machine\ntranslation. However, these models are known to have various social biases,\nespecially gender bias. Most of the work on evaluating gender bias in NMT has\nfocused primarily on English as the source language. For source languages\ndifferent from English, most of the studies use gender-neutral sentences to\nevaluate gender bias. However, practically, many sentences that we encounter do\nhave gender information. Therefore, it makes more sense to evaluate for bias\nusing such sentences. This allows us to determine if NMT models can identify\nthe correct gender based on the grammatical gender cues in the source sentence\nrather than relying on biased correlations with, say, occupation terms. To\ndemonstrate our point, in this work, we use Hindi as the source language and\nconstruct two sets of gender-specific sentences: OTSC-Hindi and WinoMT-Hindi\nthat we use to evaluate different Hindi-English (HI-EN) NMT systems\nautomatically for gender bias. Our work highlights the importance of\nconsidering the nature of language when designing such extrinsic bias\nevaluation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Pushpdeep Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensembling Textual and Structure-Based Models for Knowledge Graph Completion. (arXiv:2311.03780v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03780","description":"<p>We consider two popular approaches to Knowledge Graph Completion (KGC):\ntextual models that rely on textual entity descriptions, and structure-based\nmodels that exploit the connectivity structure of the Knowledge Graph (KG).\nPreliminary experiments show that these approaches have complementary\nstrengths: structure-based models perform well when the gold answer is easily\nreachable from the query head in the KG, while textual models exploit\ndescriptions to give good performance even when the gold answer is not\nreachable. In response, we explore ensembling as a way of combining the best of\nboth approaches. We propose a novel method for learning query-dependent\nensemble weights by using the distributions of scores assigned by individual\nmodels to all candidate entities. Our ensemble baseline achieves\nstate-of-the-art results on three standard KGC datasets, with up to 6.8 pt MRR\nand 8.3 pt Hits@1 gains over best individual models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nandi_A/0/1/0/all/0/1\">Ananjan Nandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_N/0/1/0/all/0/1\">Navdeep Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1\">Parag Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Representation Projection: Can We Transfer Factual Knowledge across Languages in Multilingual Language Models?. (arXiv:2311.03788v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03788","description":"<p>Multilingual pretrained language models serve as repositories of multilingual\nfactual knowledge. Nevertheless, a substantial performance gap of factual\nknowledge probing exists between high-resource languages and low-resource\nlanguages, suggesting limited implicit factual knowledge transfer across\nlanguages in multilingual pretrained language models. This paper investigates\nthe feasibility of explicitly transferring relatively rich factual knowledge\nfrom English to non-English languages. To accomplish this, we propose two\nparameter-free $\\textbf{L}$anguage $\\textbf{R}$epresentation\n$\\textbf{P}$rojection modules (LRP2). The first module converts non-English\nrepresentations into English-like equivalents, while the second module reverts\nEnglish-like representations back into representations of the corresponding\nnon-English language. Experimental results on the mLAMA dataset demonstrate\nthat LRP2 significantly improves factual knowledge retrieval accuracy and\nfacilitates knowledge transferability across diverse non-English languages. We\nfurther investigate the working mechanism of LRP2 from the perspectives of\nrepresentation space and cross-lingual knowledge neuron.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shaoyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junzhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Character-Level Bangla Text-to-IPA Transcription Using Transformer Architecture with Sequence Alignment. (arXiv:2311.03792v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03792","description":"<p>The International Phonetic Alphabet (IPA) is indispensable in language\nlearning and understanding, aiding users in accurate pronunciation and\ncomprehension. Additionally, it plays a pivotal role in speech therapy,\nlinguistic research, accurate transliteration, and the development of\ntext-to-speech systems, making it an essential tool across diverse fields.\nBangla being 7th as one of the widely used languages, gives rise to the need\nfor IPA in its domain. Its IPA mapping is too diverse to be captured manually\ngiving the need for Artificial Intelligence and Machine Learning in this field.\nIn this study, we have utilized a transformer-based sequence-to-sequence model\nat the letter and symbol level to get the IPA of each Bangla word as the\nvariation of IPA in association of different words is almost null. Our\ntransformer model only consisted of 8.5 million parameters with only a single\ndecoder and encoder layer. Additionally, to handle the punctuation marks and\nthe occurrence of foreign languages in the text, we have utilized manual\nmapping as the model won't be able to learn to separate them from Bangla words\nwhile decreasing our required computational resources. Finally, maintaining the\nrelative position of the sentence component IPAs and generation of the combined\nIPA has led us to achieve the top position with a word error rate of 0.10582 in\nthe public ranking of DataVerse Challenge - ITVerse 2023\n(https://www.kaggle.com/competitions/dataverse_2023/).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_J/0/1/0/all/0/1\">Jakir Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1\">Shrestha Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debnath_A/0/1/0/all/0/1\">Ameya Debnath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noisy Pair Corrector for Dense Retrieval. (arXiv:2311.03798v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03798","description":"<p>Most dense retrieval models contain an implicit assumption: the training\nquery-document pairs are exactly matched. Since it is expensive to annotate the\ncorpus manually, training pairs in real-world applications are usually\ncollected automatically, which inevitably introduces mismatched-pair noise. In\nthis paper, we explore an interesting and challenging problem in dense\nretrieval, how to train an effective model with mismatched-pair noise. To solve\nthis problem, we propose a novel approach called Noisy Pair Corrector (NPC),\nwhich consists of a detection module and a correction module. The detection\nmodule estimates noise pairs by calculating the perplexity between annotated\npositive and easy negative documents. The correction module utilizes an\nexponential moving average (EMA) model to provide a soft supervised signal,\naiding in mitigating the effects of noise. We conduct experiments on\ntext-retrieval benchmarks Natural Question and TriviaQA, code-search benchmarks\nStaQC and SO-DS. Experimental results show that NPC achieves excellent\nperformance in handling both synthetic and realistic noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking and Improving Multi-task Learning for End-to-end Speech Translation. (arXiv:2311.03810v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03810","description":"<p>Significant improvements in end-to-end speech translation (ST) have been\nachieved through the application of multi-task learning. However, the extent to\nwhich auxiliary tasks are highly consistent with the ST task, and how much this\napproach truly helps, have not been thoroughly studied. In this paper, we\ninvestigate the consistency between different tasks, considering different\ntimes and modules. We find that the textual encoder primarily facilitates\ncross-modal conversion, but the presence of noise in speech impedes the\nconsistency between text and speech representations. Furthermore, we propose an\nimproved multi-task learning (IMTL) approach for the ST task, which bridges the\nmodal gap by mitigating the difference in length and representation. We conduct\nexperiments on the MuST-C dataset. The results demonstrate that our method\nattains state-of-the-art results. Moreover, when additional data is used, we\nachieve the new SOTA result on MuST-C English to Spanish task with 20.8% of the\ntraining time required by the current SOTA method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversations in Galician: a Large Language Model for an Underrepresented Language. (arXiv:2311.03812v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03812","description":"<p>The recent proliferation of Large Conversation Language Models has\nhighlighted the economic significance of widespread access to this type of AI\ntechnologies in the current information age. Nevertheless, prevailing models\nhave primarily been trained on corpora consisting of documents written in\npopular languages. The dearth of such cutting-edge tools for low-resource\nlanguages further exacerbates their underrepresentation in the current economic\nlandscape, thereby impacting their native speakers. This paper introduces two\nnovel resources designed to enhance Natural Language Processing (NLP) for the\nGalician language. We present a Galician adaptation of the Alpaca dataset,\ncomprising 52,000 instructions and demonstrations. This dataset proves\ninvaluable for enhancing language models by fine-tuning them to more accurately\nadhere to provided instructions. Additionally, as a demonstration of the\ndataset utility, we fine-tuned LLaMA-7B to comprehend and respond in Galician,\na language not originally supported by the model, by following the Alpaca\nformat. This work contributes to the research on multilingual models tailored\nfor low-resource settings, a crucial endeavor in ensuring the inclusion of all\nlinguistic communities in the development of Large Language Models. Another\nnoteworthy aspect of this research is the exploration of how knowledge of a\nclosely related language, in this case, Portuguese, can assist in generating\ncoherent text when training resources are scarce. Both the Galician Alpaca\ndataset and Cabuxa-7B are publicly accessible on our Huggingface Hub, and we\nhave made the source code available to facilitate replication of this\nexperiment and encourage further advancements for underrepresented languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_E/0/1/0/all/0/1\">Eliseo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_A/0/1/0/all/0/1\">Anxo P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parapar_J/0/1/0/all/0/1\">Javier Parapar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OLaLa: Ontology Matching with Large Language Models. (arXiv:2311.03837v1 [cs.IR])","link":"http://arxiv.org/abs/2311.03837","description":"<p>Ontology (and more generally: Knowledge Graph) Matching is a challenging task\nwhere information in natural language is one of the most important signals to\nprocess. With the rise of Large Language Models, it is possible to incorporate\nthis knowledge in a better way into the matching pipeline. A number of\ndecisions still need to be taken, e.g., how to generate a prompt that is useful\nto the model, how information in the KG can be formulated in prompts, which\nLarge Language Model to choose, how to provide existing correspondences to the\nmodel, how to generate candidates, etc. In this paper, we present a prototype\nthat explores these questions by applying zero-shot and few-shot prompting with\nmultiple open Large Language Models to different tasks of the Ontology\nAlignment Evaluation Initiative (OAEI). We show that with only a handful of\nexamples and a well-designed prompt, it is possible to achieve results that are\nen par with supervised matching systems which use a much larger portion of the\nground truth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hertling_S/0/1/0/all/0/1\">Sven Hertling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspects of human memory and Large Language Models. (arXiv:2311.03839v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03839","description":"<p>Large Language Models (LLMs) are huge artificial neural networks which\nprimarily serve to generate text, but also provide a very sophisticated\nprobabilistic model of language use. Since generating a semantically consistent\ntext requires a form of effective memory, we investigate the memory properties\nof LLMs and find surprising similarities with key characteristics of human\nmemory. This result strongly suggests that the biological features of human\nmemory leave an imprint on the way that we structure our textual narratives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Janik_R/0/1/0/all/0/1\">Romuald A. Janik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Contrastive Learning of Sentence Embeddings. (arXiv:2311.03881v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03881","description":"<p>Recently, SimCSE has shown the feasibility of contrastive learning in\ntraining sentence embeddings and illustrates its expressiveness in spanning an\naligned and uniform embedding space. However, prior studies have shown that\ndense models could contain harmful parameters that affect the model\nperformance, and it is no wonder that SimCSE can as well be invented with such\nparameters. Driven by this, parameter sparsification is applied, where\nalignment and uniformity scores are used to measure the contribution of each\nparameter to the overall quality of sentence embeddings. Drawing from a\npreliminary study, we consider parameters with minimal contributions to be\ndetrimental, as their sparsification results in improved model performance. To\ndiscuss the ubiquity of detrimental parameters and remove them, more\nexperiments on the standard semantic textual similarity (STS) tasks and\ntransfer learning tasks are conducted, and the results show that the proposed\nsparsified SimCSE (SparseCSE) has excellent performance in comparison with\nSimCSE. Furthermore, through in-depth analysis, we establish the validity and\nstability of our sparsification method, showcasing that the embedding space\ngenerated by SparseCSE exhibits improved alignment compared to that produced by\nSimCSE. Importantly, the uniformity yet remains uncompromised.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_R/0/1/0/all/0/1\">Ruize An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iACOS: Advancing Implicit Sentiment Extraction with Informative and Adaptive Negative Examples. (arXiv:2311.03896v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03896","description":"<p>Aspect-based sentiment analysis (ABSA) have been extensively studied, but\nlittle light has been shed on the quadruple extraction consisting of four\nfundamental elements: aspects, categories, opinions and sentiments, especially\nwith implicit aspects and opinions. In this paper, we propose a new method\niACOS for extracting Implicit Aspects with Categories and Opinions with\nSentiments. First, iACOS appends two implicit tokens at the end of a text to\ncapture the context-aware representation of all tokens including implicit\naspects and opinions. Second, iACOS develops a sequence labeling model over the\ncontext-aware token representation to co-extract explicit and implicit aspects\nand opinions. Third, iACOS devises a multi-label classifier with a specialized\nmulti-head attention for discovering aspect-opinion pairs and predicting their\ncategories and sentiments simultaneously. Fourth, iACOS leverages informative\nand adaptive negative examples to jointly train the multi-label classifier and\nthe other two classifiers on categories and sentiments by multi-task learning.\nFinally, the experimental results show that iACOS significantly outperforms\nother quadruple extraction baselines according to the F1 score on two public\nbenchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiancai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jia-Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Lei Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhishang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Korean NLP Tasks with Linguistically Informed Subword Tokenization and Sub-character Decomposition. (arXiv:2311.03928v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03928","description":"<p>We introduce a morpheme-aware subword tokenization method that utilizes\nsub-character decomposition to address the challenges of applying Byte Pair\nEncoding (BPE) to Korean, a language characterized by its rich morphology and\nunique writing system. Our approach balances linguistic accuracy with\ncomputational efficiency in Pre-trained Language Models (PLMs). Our evaluations\nshow that this technique achieves good performances overall, notably improving\nresults in the syntactic task of NIKL-CoLA. This suggests that integrating\nmorpheme type information can enhance language models' syntactic and semantic\ncapabilities, indicating that adopting more linguistic insights can further\nimprove performance beyond standard morphological analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_T/0/1/0/all/0/1\">Taehee Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bongseok Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1\">Yoonseob Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis of Dialogue Repair in Voice Assistants. (arXiv:2311.03952v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03952","description":"<p>Spoken dialogue systems have transformed human-machine interaction by\nproviding real-time responses to queries. However, misunderstandings between\nthe user and system persist. This study explores the significance of\ninteractional language in dialogue repair between virtual assistants and users\nby analyzing interactions with Google Assistant and Siri, focusing on their\nutilization and response to the other-initiated repair strategy \"huh?\"\nprevalent in human-human interaction. Findings reveal several\nassistant-generated strategies but an inability to replicate human-like repair\nstrategies such as \"huh?\". English and Spanish user acceptability surveys show\ndifferences in users' repair strategy preferences and assistant usage, with\nboth similarities and disparities among the two surveyed languages. These\nresults shed light on inequalities between interactional language in\nhuman-human interaction and human-machine interaction, underscoring the need\nfor further research on the impact of interactional language in human-machine\ninteraction in English and beyond.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galbraith_M/0/1/0/all/0/1\">Matthew Galbraith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Expectation-Realization Model for Metaphor Detection. (arXiv:2311.03963v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03963","description":"<p>We propose a metaphor detection architecture that is structured around two\nmain modules: an expectation component that estimates representations of\nliteral word expectations given a context, and a realization component that\ncomputes representations of actual word meanings in context. The overall\narchitecture is trained to learn expectation-realization (ER) patterns that\ncharacterize metaphorical uses of words. When evaluated on three metaphor\ndatasets for within distribution, out of distribution, and novel metaphor\ngeneralization, the proposed method is shown to obtain results that are\ncompetitive or better than state-of-the art. Further increases in metaphor\ndetection accuracy are obtained through ensembling of ER models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uduehi_O/0/1/0/all/0/1\">Oseremen O. Uduehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bunescu_R/0/1/0/all/0/1\">Razvan C. Bunescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factoring Hate Speech: A New Annotation Framework to Study Hate Speech in Social Media. (arXiv:2311.03969v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03969","description":"<p>In this work we propose a novel annotation scheme which factors hate speech\ninto five separate discursive categories. To evaluate our scheme, we construct\na corpus of over 2.9M Twitter posts containing hateful expressions directed at\nJews, and annotate a sample dataset of 1,050 tweets. We present a statistical\nanalysis of the annotated dataset as well as discuss annotation examples, and\nconclude by discussing promising directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ron_G/0/1/0/all/0/1\">Gal Ron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levi_E/0/1/0/all/0/1\">Effi Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oshri_O/0/1/0/all/0/1\">Odelia Oshri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenhav_S/0/1/0/all/0/1\">Shaul R. Shenhav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Jiu-Jitsu Argumentation for Writing Peer Review Rebuttals. (arXiv:2311.03998v1 [cs.CL])","link":"http://arxiv.org/abs/2311.03998","description":"<p>In many domains of argumentation, people's arguments are driven by so-called\nattitude roots, i.e., underlying beliefs and world views, and their\ncorresponding attitude themes. Given the strength of these latent drivers of\narguments, recent work in psychology suggests that instead of directly\ncountering surface-level reasoning (e.g., falsifying given premises), one\nshould follow an argumentation style inspired by the Jiu-Jitsu 'soft' combat\nsystem (Hornsey and Fielding, 2017): first, identify an arguer's attitude roots\nand themes, and then choose a prototypical rebuttal that is aligned with those\ndrivers instead of invalidating those. In this work, we are the first to\nexplore Jiu-Jitsu argumentation for peer review by proposing the novel task of\nattitude and theme-guided rebuttal generation. To this end, we enrich an\nexisting dataset for discourse structure in peer reviews with attitude roots,\nattitude themes, and canonical rebuttals. To facilitate this process, we recast\nestablished annotation concepts from the domain of peer reviews (e.g., aspects\na review sentence is relating to) and train domain-specific models. We then\npropose strong rebuttal generation strategies, which we benchmark on our novel\ndataset for the task of end-to-end attitude and theme-guided rebuttal\ngeneration and two subtasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1\">Sukannya Purkayastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Reinforcement Learning for Automatic Disease Diagnosis. (arXiv:2004.14254v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2004.14254","description":"<p>Motivation: Disease diagnosis oriented dialogue system models the interactive\nconsultation procedure as Markov Decision Process and reinforcement learning\nalgorithms are used to solve the problem. Existing approaches usually employ a\nflat policy structure that treat all symptoms and diseases equally for action\nmaking. This strategy works well in the simple scenario when the action space\nis small, however, its efficiency will be challenged in the real environment.\nInspired by the offline consultation process, we propose to integrate a\nhierarchical policy structure of two levels into the dialogue systemfor policy\nlearning. The high-level policy consists of amastermodel that is responsible\nfor triggering a low-levelmodel, the lowlevel policy consists of several\nsymptom checkers and a disease classifier. The proposed policy structure is\ncapable to deal with diagnosis problem including large number of diseases and\nsymptoms.\n</p>\n<p>Results: Experimental results on three real-world datasets and a synthetic\ndataset demonstrate that our hierarchical framework achieves higher accuracy\nand symptom recall in disease diagnosis compared with existing systems. We\nconstruct a benchmark including datasets and implementation of existing\nalgorithms to encourage follow-up researches.\n</p>\n<p>Availability: The code and data is available from\nhttps://github.com/FudanDISC/DISCOpen-MedBox-DialoDiagnosis\n</p>\n<p>Contact: 21210980124@m.fudan.edu.cn\n</p>\n<p>Supplementary information: Supplementary data are available at Bioinformatics\nonline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Cheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kangenbei Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianlong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiajie Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CompRes: A Dataset for Narrative Structure in News. (arXiv:2007.04874v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.04874","description":"<p>This paper addresses the task of automatically detecting narrative structures\nin raw texts. Previous works have utilized the oral narrative theory by Labov\nand Waletzky to identify various narrative elements in personal stories texts.\nInstead, we direct our focus to news articles, motivated by their growing\nsocial impact as well as their role in creating and shaping public opinion.\n</p>\n<p>We introduce CompRes -- the first dataset for narrative structure in news\nmedia. We describe the process in which the dataset was constructed: first, we\ndesigned a new narrative annotation scheme, better suited for news media, by\nadapting elements from the narrative theory of Labov and Waletzky (Complication\nand Resolution) and adding a new narrative element of our own (Success); then,\nwe used that scheme to annotate a set of 29 English news articles (containing\n1,099 sentences) collected from news and partisan websites. We use the\nannotated dataset to train several supervised models to identify the different\nnarrative elements, achieving an $F_1$ score of up to 0.7. We conclude by\nsuggesting several promising directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levi_E/0/1/0/all/0/1\">Effi Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mor_G/0/1/0/all/0/1\">Guy Mor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenhav_S/0/1/0/all/0/1\">Shaul Shenhav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheafer_T/0/1/0/all/0/1\">Tamir Sheafer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Text Classification As Sub-Hierarchy Sequence Generation. (arXiv:2111.11104v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.11104","description":"<p>Hierarchical text classification (HTC) is essential for various real\napplications. However, HTC models are challenging to develop because they often\nrequire processing a large volume of documents and labels with hierarchical\ntaxonomy. Recent HTC models based on deep learning have attempted to\nincorporate hierarchy information into a model structure. Consequently, these\nmodels are challenging to implement when the model parameters increase for a\nlarge-scale hierarchy because the model structure depends on the hierarchy\nsize. To solve this problem, we formulate HTC as a sub-hierarchy sequence\ngeneration to incorporate hierarchy information into a target label sequence\ninstead of the model structure. Subsequently, we propose the Hierarchy DECoder\n(HiDEC), which decodes a text sequence into a sub-hierarchy sequence using\nrecursive hierarchy decoding, classifying all parents at the same level into\nchildren at once. In addition, HiDEC is trained to use hierarchical path\ninformation from a root to each leaf in a sub-hierarchy composed of the labels\nof a target document via an attention mechanism and hierarchy-aware masking.\nHiDEC achieved state-of-the-art performance with significantly fewer model\nparameters than existing models on benchmark datasets, such as RCV1-v2, NYT,\nand EURLEX57K.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1\">SangHun Im</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gibaeg Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1\">Heung-Seon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1\">Seongung Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghwan Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geodesic Multi-Modal Mixup for Robust Fine-Tuning. (arXiv:2203.03897v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03897","description":"<p>Pre-trained multi-modal models, such as CLIP, provide transferable embeddings\nand show promising results in diverse applications. However, the analysis of\nlearned multi-modal embeddings is relatively unexplored, and the embedding\ntransferability can be improved. In this work, we observe that CLIP holds\nseparated embedding subspaces for two different modalities, and then we\ninvestigate it through the lens of uniformity-alignment to measure the quality\nof learned representation. Both theoretically and empirically, we show that\nCLIP retains poor uniformity and alignment even after fine-tuning. Such a lack\nof alignment and uniformity might restrict the transferability and robustness\nof embeddings. To this end, we devise a new fine-tuning method for robust\nrepresentation equipping better alignment and uniformity. First, we propose a\nGeodesic Multi-Modal Mixup that mixes the embeddings of image and text to\ngenerate hard negative samples on the hypersphere. Then, we fine-tune the model\non hard negatives as well as original negatives and positives with contrastive\nloss. Based on the theoretical analysis about hardness guarantee and limiting\nbehavior, we justify the use of our method. Extensive experiments on retrieval,\ncalibration, few- or zero-shot classification (under distribution shift),\nembedding arithmetic, and image captioning further show that our method\nprovides transferable representations, enabling robust model adaptation on\ndiverse tasks. Code: https://github.com/changdaeoh/multimodal-mixup\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1\">Changdae Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+So_J/0/1/0/all/0/1\">Junhyuk So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1\">Hoyoon Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1\">YongTaek Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Minchul Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1\">Jong-June Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kyungwoo Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuro-Symbolic Causal Reasoning Meets Signaling Game for Emergent Semantic Communications. (arXiv:2210.12040v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.12040","description":"<p>Semantic communication (SC) aims to communicate reliably with minimal data\ntransfer while simultaneously providing seamless connectivity to heterogeneous\nservices and users. In this paper, a novel emergent SC (ESC) system framework\nis proposed and is composed of a signaling game for emergent language design\nand a neuro-symbolic (NeSy) artificial intelligence (AI) approach for causal\nreasoning. In order to design the language, the signaling game is solved using\nan alternating maximization between the communicating node's utilities. The\nemergent language helps create a context-aware transmit vocabulary (minimal\nsemantic representation) and aids the reasoning process (enabling\ngeneralization to unseen scenarios) by splitting complex messages into simpler\nreasoning tasks for the receiver. The causal description at the transmitter is\nthen modeled (a neural component) as a posterior distribution of the relevant\nattributes present in the data. Using the reconstructed causal state, the\nreceiver evaluates a set of logical formulas (symbolic part) to execute its\ntask. The nodes NeSy reasoning components are implemented by the recently\nproposed AI tool called Generative Flow Networks, and they are optimized for\nhigher semantic reliability. The ESC system is designed to enhance the novel\nmetrics of semantic information, reliability, distortion and similarity that\nare designed using rigorous algebraic properties from category theory thereby\ngeneralizing the metrics beyond Shannon's notion of uncertainty. Simulation\nresults validate the ability of ESC to communicate efficiently (with reduced\nbits) and achieve better semantic reliability than conventional wireless and\nstate-of-the-art systems that do not exploit causal reasoning capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1\">Christo Kurisummoottil Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1\">Walid Saad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Undesirable biases in NLP: Addressing challenges of measurement. (arXiv:2211.13709v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.13709","description":"<p>As Large Language Models and Natural Language Processing (NLP) technology\nrapidly develop and spread into daily life, it becomes crucial to anticipate\nhow their use could harm people. One problem that has received a lot of\nattention in recent years is that this technology has displayed harmful biases,\nfrom generating derogatory stereotypes to producing disparate outcomes for\ndifferent social groups. Although a lot of effort has been invested in\nassessing and mitigating these biases, our methods of measuring the biases of\nNLP models have serious problems and it is often unclear what they actually\nmeasure. In this paper, we provide an interdisciplinary approach to discussing\nthe issue of NLP model bias by adopting the lens of psychometrics -- a field\nspecialized in the measurement of concepts like bias that are not directly\nobservable. In particular, we will explore two central notions from\npsychometrics, the \\emph{construct validity} and the \\emph{reliability} of\nmeasurement tools, and discuss how they can be applied in the context of\nmeasuring model bias. Our goal is to provide NLP practitioners with\nmethodological tools for designing better bias measures, and to inspire them\nmore generally to explore tools from psychometrics when working on bias\nmeasurement tools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wal_O/0/1/0/all/0/1\">Oskar van der Wal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachmann_D/0/1/0/all/0/1\">Dominik Bachmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leidinger_A/0/1/0/all/0/1\">Alina Leidinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maanen_L/0/1/0/all/0/1\">Leendert van Maanen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuidema_W/0/1/0/all/0/1\">Willem Zuidema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_K/0/1/0/all/0/1\">Katrin Schulz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Diffusion for Language Generation. (arXiv:2212.09462v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09462","description":"<p>Diffusion models have achieved great success in modeling continuous data\nmodalities such as images, audio, and video, but have seen limited use in\ndiscrete domains such as language. Recent attempts to adapt diffusion to\nlanguage have presented diffusion as an alternative to existing pretrained\nlanguage models. We view diffusion and existing language models as\ncomplementary. We demonstrate that encoder-decoder language models can be\nutilized to efficiently learn high-quality language autoencoders. We then\ndemonstrate that continuous diffusion models can be learned in the latent space\nof the language autoencoder, enabling us to sample continuous latent\nrepresentations that can be decoded into natural language with the pretrained\ndecoder. We validate the effectiveness of our approach for unconditional,\nclass-conditional, and sequence-to-sequence language generation. We demonstrate\nacross multiple diverse data sets that our latent language diffusion models are\nsignificantly more effective than previous diffusion language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovelace_J/0/1/0/all/0/1\">Justin Lovelace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kishore_V/0/1/0/all/0/1\">Varsha Kishore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1\">Chao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhtman_E/0/1/0/all/0/1\">Eliot Shekhtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Break It Down: Evidence for Structural Compositionality in Neural Networks. (arXiv:2301.10884v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10884","description":"<p>Though modern neural networks have achieved impressive performance in both\nvision and language tasks, we know little about the functions that they\nimplement. One possibility is that neural networks implicitly break down\ncomplex tasks into subroutines, implement modular solutions to these\nsubroutines, and compose them into an overall solution to a task - a property\nwe term structural compositionality. Another possibility is that they may\nsimply learn to match new inputs to learned templates, eliding task\ndecomposition entirely. Here, we leverage model pruning techniques to\ninvestigate this question in both vision and language across a variety of\narchitectures, tasks, and pretraining regimens. Our results demonstrate that\nmodels often implement solutions to subroutines via modular subnetworks, which\ncan be ablated while maintaining the functionality of other subnetworks. This\nsuggests that neural networks may be able to learn compositionality, obviating\nthe need for specialized symbolic mechanisms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lepori_M/0/1/0/all/0/1\">Michael A. Lepori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1\">Thomas Serre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Competence-Based Analysis of Language Models. (arXiv:2303.00333v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.00333","description":"<p>Despite the recent success of large, pretrained neural language models (LLMs)\non a variety of prompting tasks, these models can be alarmingly brittle to\nsmall changes in inputs or application contexts. To better understand such\nbehavior and motivate the design of more robust LLMs, we provide a causal\nformulation of linguistic competence in the context of LLMs and propose a\ngeneral framework to study and measure LLM competence. Our framework, CALM\n(Competence-based Analysis of Language Models), establishes the first\nquantitative measure of LLM competence, which we study by damaging models'\ninternal representations of various linguistic properties in the course of\nperforming various tasks using causal probing and evaluating models' alignment\nunder these interventions with a given causal model. We also develop a novel\napproach for performing causal probing interventions using gradient-based\nadversarial attacks, which can target a broader range of properties and\nrepresentations than existing techniques. We carry out a case study of CALM\nusing these interventions to analyze BERT and RoBERTa's competence across a\nvariety of lexical inference tasks, showing that the CALM framework and\ncompetence metric can be valuable tools for explaining and predicting their\nbehavior across these tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davies_A/0/1/0/all/0/1\">Adam Davies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jize Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The AI Ghostwriter Effect: When Users Do Not Perceive Ownership of AI-Generated Text But Self-Declare as Authors. (arXiv:2303.03283v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2303.03283","description":"<p>Human-AI interaction in text production increases complexity in authorship.\nIn two empirical studies (n1 = 30 &amp; n2 = 96), we investigate authorship and\nownership in human-AI collaboration for personalized language generation. We\nshow an AI Ghostwriter Effect: Users do not consider themselves the owners and\nauthors of AI-generated text but refrain from publicly declaring AI authorship.\nPersonalization of AI-generated texts did not impact the AI Ghostwriter Effect,\nand higher levels of participants' influence on texts increased their sense of\nownership. Participants were more likely to attribute ownership to supposedly\nhuman ghostwriters than AI ghostwriters, resulting in a higher\nownership-authorship discrepancy for human ghostwriters. Rationalizations for\nauthorship in AI ghostwriters and human ghostwriters were similar. We discuss\nhow our findings relate to psychological ownership and human-AI interaction to\nlay the foundations for adapting authorship frameworks and user interfaces in\nAI in text-generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Draxler_F/0/1/0/all/0/1\">Fiona Draxler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werner_A/0/1/0/all/0/1\">Anna Werner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_F/0/1/0/all/0/1\">Florian Lehmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoppe_M/0/1/0/all/0/1\">Matthias Hoppe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_A/0/1/0/all/0/1\">Albrecht Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buschek_D/0/1/0/all/0/1\">Daniel Buschek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welsch_R/0/1/0/all/0/1\">Robin Welsch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Role of Attribute Context in Vision-Language Models for Object Recognition and Detection. (arXiv:2303.10093v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.10093","description":"<p>Vision-language alignment learned from image-caption pairs has been shown to\nbenefit tasks like object recognition and detection. Methods are mostly\nevaluated in terms of how well object class names are learned, but captions\nalso contain rich attribute context that should be considered when learning\nobject alignment. It is unclear how methods use this context in learning, as\nwell as whether models succeed when tasks require attribute and object\nunderstanding. To address this gap, we conduct extensive analysis of the role\nof attributes in vision-language models. We specifically measure model\nsensitivity to the presence and meaning of attribute context, gauging influence\non object embeddings through unsupervised phrase grounding and classification\nvia description methods. We further evaluate the utility of attribute context\nin training for open-vocabulary object detection, fine-grained text-region\nretrieval, and attribution tasks. Our results show that attribute context can\nbe wasted when learning alignment for detection, attribute meaning is not\nadequately considered in embeddings, and describing classes by only their\nattributes is ineffective. A viable strategy that we find to increase benefits\nfrom attributes is contrastive training with adjective-based negative captions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buettner_K/0/1/0/all/0/1\">Kyle Buettner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1\">Adriana Kovashka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.15714","description":"<p>Language models have been shown to perform remarkably well on a wide range of\nnatural language processing tasks. In this paper, we propose LEAP, a novel\nsystem that uses language models to perform multi-step logical reasoning and\nincorporates explicit planning into the inference procedure. Explicit planning\nenables the system to make more informed reasoning decisions at each step by\nlooking ahead into their future effects. Moreover, we propose a training\nstrategy that safeguards the planning process from being led astray by spurious\nfeatures. Our full system significantly outperforms other competing methods on\nmultiple standard datasets. When using small T5 models as its core selection\nand deduction components, our system performs competitively compared to GPT-3\ndespite having only about 1B parameters (i.e., 175 times smaller than GPT-3).\nWhen using GPT-3.5, it significantly outperforms chain-of-thought prompting on\nthe challenging PrOntoQA dataset. We have conducted extensive empirical studies\nto demonstrate that explicit planning plays a crucial role in the system's\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kangrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hongyuan Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic. (arXiv:2305.03353v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03353","description":"<p>Theory of Mind (ToM) is a critical component of intelligence but its\nassessment remains the subject of heated debates. Prior research applied human\nToM assessments to natural language processing models using either\nhuman-created standardized tests or rule-based templates. However, these\nmethods primarily focus on simplistic reasoning and require further validation.\nHere, we leverage dynamic epistemic logic to isolate a particular component of\nToM and to generate controlled problems. We also introduce new verbalization\ntechniques to express these problems in English natural language. Our findings\nindicate that some language model scaling (from 70M to 6B and 350M to 174B)\ndoes not consistently yield results better than random chance. While GPT-4\ndemonstrates superior epistemic reasoning capabilities, there is still room for\nimprovement. Our code and datasets are publicly available\n(https://huggingface.co/datasets/sileod/mindgames ,\nhttps://github.com/sileod/llm-theory-of-mind )\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sileo_D/0/1/0/all/0/1\">Damien Sileo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lernould_A/0/1/0/all/0/1\">Antoine Lernould</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QTSumm: Query-Focused Summarization over Tabular Data. (arXiv:2305.14303v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14303","description":"<p>People primarily consult tables to conduct data analysis or answer specific\nquestions. Text generation systems that can provide accurate table summaries\ntailored to users' information needs can facilitate more efficient access to\nrelevant data insights. Motivated by this, we define a new query-focused table\nsummarization task, where text generation models have to perform human-like\nreasoning and analysis over the given table to generate a tailored summary. We\nintroduce a new benchmark named QTSumm for this task, which contains 7,111\nhuman-annotated query-summary pairs over 2,934 tables covering diverse topics.\nWe investigate a set of strong baselines on QTSumm, including text generation,\ntable-to-text generation, and large language models. Experimental results and\nmanual analysis reveal that the new task presents significant challenges in\ntable-to-text generation for future research. Moreover, we propose a new\napproach named ReFactor, to retrieve and reason over query-relevant information\nfrom tabular data to generate several natural language facts. Experimental\nresults demonstrate that ReFactor can bring improvements to baselines by\nconcatenating the generated facts to the model input. Our data and code are\npublicly available at https://github.com/yale-nlp/QTSumm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yilun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhenting Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Linyong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_B/0/1/0/all/0/1\">Boyu Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Weijin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Simeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yumo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coverage-based Example Selection for In-Context Learning. (arXiv:2305.14907v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14907","description":"<p>In-context learning (ICL), the ability of large language models to perform\nnovel tasks by conditioning on a prompt with a few task examples, requires\nthese examples to be informative about the test instance. The standard approach\nof independently ranking and selecting the most similar examples selects\nredundant examples while omitting important information. In this work, we show\nthat BERTScore-Recall (BSR) selects better examples that demonstrate more of\nthe salient aspects, e.g. reasoning patterns, of the test input. We further\nextend BSR and many standard metrics to easily optimizable set-level metrics,\ngiving still better coverage of those salient aspects. On 15 datasets spanning\n6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric\nfor in-context example selection across the board, and (2) for compositional\ntasks, set selection using Set-BSR outperforms independent ranking by up to 17\npoints on average and, despite being training-free, surpasses methods that\nleverage task or LLM-specific training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shivanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Positional Encoding on Length Generalization in Transformers. (arXiv:2305.19466v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19466","description":"<p>Length generalization, the ability to generalize from small training context\nsizes to larger ones, is a critical challenge in the development of\nTransformer-based language models. Positional encoding (PE) has been identified\nas a major factor influencing length generalization, but the exact impact of\ndifferent PE schemes on extrapolation in downstream tasks remains unclear. In\nthis paper, we conduct a systematic empirical study comparing the length\ngeneralization performance of decoder-only Transformers with five different\nposition encoding approaches including Absolute Position Embedding (APE), T5's\nRelative PE, ALiBi, and Rotary, in addition to Transformers without positional\nencoding (NoPE). Our evaluation encompasses a battery of reasoning and\nmathematical tasks. Our findings reveal that the most commonly used positional\nencoding methods, such as ALiBi, Rotary, and APE, are not well suited for\nlength generalization in downstream tasks. More importantly, NoPE outperforms\nother explicit positional encoding methods while requiring no additional\ncomputation. We theoretically demonstrate that NoPE can represent both absolute\nand relative PEs, but when trained with SGD, it mostly resembles T5's relative\nPE attention patterns. Finally, we find that scratchpad is not always helpful\nto solve length generalization and its format highly impacts the model's\nperformance. Overall, our work suggests that explicit position embeddings are\nnot essential for decoder-only Transformers to generalize well to longer\nsequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kazemnejad_A/0/1/0/all/0/1\">Amirhossein Kazemnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1\">Inkit Padhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1\">Karthikeyan Natesan Ramamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Birth of a Transformer: A Memory Viewpoint. (arXiv:2306.00802v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2306.00802","description":"<p>Large language models based on transformers have achieved great empirical\nsuccesses. However, as they are deployed more widely, there is a growing need\nto better understand their internal mechanisms in order to make them more\nreliable. These models appear to store vast amounts of knowledge from their\ntraining data, and to adapt quickly to new information provided in their\ncontext or prompt. We study how transformers balance these two types of\nknowledge by considering a synthetic setup where tokens are generated from\neither global or context-specific bigram distributions. By a careful empirical\nanalysis of the training process on a simplified two-layer transformer, we\nillustrate the fast learning of global bigrams and the slower development of an\n\"induction head\" mechanism for the in-context bigrams. We highlight the role of\nweight matrices as associative memories, provide theoretical insights on how\ngradients enable their learning during training, and study the role of\ndata-distributional properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Bietti_A/0/1/0/all/0/1\">Alberto Bietti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cabannes_V/0/1/0/all/0/1\">Vivien Cabannes</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bouchacourt_D/0/1/0/all/0/1\">Diane Bouchacourt</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jegou_H/0/1/0/all/0/1\">Herve Jegou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bottou_L/0/1/0/all/0/1\">Leon Bottou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset. (arXiv:2307.04657v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.04657","description":"<p>In this paper, we introduce the BeaverTails dataset, aimed at fostering\nresearch on safety alignment in large language models (LLMs). This dataset\nuniquely separates annotations of helpfulness and harmlessness for\nquestion-answering pairs, thus offering distinct perspectives on these crucial\nattributes. In total, we have gathered safety meta-labels for 333,963\nquestion-answer (QA) pairs and 361,903 pairs of expert comparison data for both\nthe helpfulness and harmlessness metrics. We further showcase applications of\nBeaverTails in content moderation and reinforcement learning with human\nfeedback (RLHF), emphasizing its potential for practical safety measures in\nLLMs. We believe this dataset provides vital resources for the community,\ncontributing towards the safe development and deployment of LLMs. Our project\npage is available at the following URL:\nhttps://sites.google.com/view/pku-beavertails.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jiaming Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mickel Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Juntao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xuehai Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1\">Ce Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruiyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaodong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph. (arXiv:2307.07697v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.07697","description":"<p>Although large language models (LLMs) have achieved significant success in\nvarious tasks, they often struggle with hallucination problems, especially in\nscenarios requiring deep and responsible reasoning. These issues could be\npartially addressed by introducing external knowledge graphs (KG) in LLM\nreasoning. In this paper, we propose a new LLM-KG integrating paradigm\n``$\\hbox{LLM}\\otimes\\hbox{KG}$'' which treats the LLM as an agent to\ninteractively explore related entities and relations on KGs and perform\nreasoning based on the retrieved knowledge. We further implement this paradigm\nby introducing a new approach called Think-on-Graph (ToG), in which the LLM\nagent iteratively executes beam search on KG, discovers the most promising\nreasoning paths, and returns the most likely reasoning results. We use a number\nof well-designed experiments to examine and illustrate the following advantages\nof ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has\nthe ability of knowledge traceability and knowledge correctability by\nleveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible\nplug-and-play framework for different LLMs, KGs and prompting strategies\nwithout any additional training cost; 4) the performance of ToG with small LLM\nmodels could exceed large LLM such as GPT-4 in certain scenarios and this\nreduces the cost of LLM deployment and application. As a training-free method\nwith lower computational cost and better generality, ToG achieves overall SOTA\nin 6 out of 9 datasets where most previous SOTAs rely on additional training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiashuo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengjin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lumingyuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Saizhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1\">Lionel M. Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Heung-Yeung Shum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.07870","description":"<p>Large Language Models (LLMs) are often misleadingly recognized as having a\npersonality or a set of values. We argue that an LLM can be seen as a\nsuperposition of perspectives with different values and personality traits.\nLLMs exhibit context-dependent values and personality traits that change based\non the induced perspective (as opposed to humans, who tend to have more\ncoherent values and personality traits across contexts). We introduce the\nconcept of perspective controllability, which refers to a model's affordance to\nadopt various perspectives with differing values and personality traits. In our\nexperiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study\nhow exhibited values and personality traits change based on different\nperspectives. Through qualitative experiments, we show that LLMs express\ndifferent values when those are (implicitly or explicitly) implied in the\nprompt, and that LLMs express different values even when those are not\nobviously implied (demonstrating their context-dependent nature). We then\nconduct quantitative experiments to study the controllability of different\nmodels (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), the\neffectiveness of various methods for inducing perspectives, and the smoothness\nof the models' drivability. We conclude by examining the broader implications\nof our work and outline a variety of associated scientific questions. The\nproject website is available at\nhttps://sites.google.com/view/llm-superpositions .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1\">Grgur Kova&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawayama_M/0/1/0/all/0/1\">Masataka Sawayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1\">R&#xe9;my Portelas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1\">C&#xe9;dric Colas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dominey_P/0/1/0/all/0/1\">Peter Ford Dominey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Language Model Attacks with Perplexity. (arXiv:2308.14132v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.14132","description":"<p>A novel hack involving Large Language Models (LLMs) has emerged, exploiting\nadversarial suffixes to deceive models into generating perilous responses. Such\njailbreaks can trick LLMs into providing intricate instructions to a malicious\nuser for creating explosives, orchestrating a bank heist, or facilitating the\ncreation of offensive content. By evaluating the perplexity of queries with\nadversarial suffixes using an open-source LLM (GPT-2), we found that they have\nexceedingly high perplexity values. As we explored a broad range of regular\n(non-adversarial) prompt varieties, we concluded that false positives are a\nsignificant challenge for plain perplexity filtering. A Light-GBM trained on\nperplexity and token length resolved the false positives and correctly detected\nmost adversarial attacks in the test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alon_G/0/1/0/all/0/1\">Gabriel Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamfonas_M/0/1/0/all/0/1\">Michael Kamfonas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Language Models to Hallucinate Less with Synthetic Tasks. (arXiv:2310.06827v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.06827","description":"<p>Large language models (LLMs) frequently hallucinate on abstractive\nsummarization tasks such as document-based question-answering, meeting\nsummarization, and clinical report generation, even though all necessary\ninformation is included in context. However, optimizing LLMs to hallucinate\nless on these tasks is challenging, as hallucination is hard to efficiently\nevaluate at each optimization step. In this work, we show that reducing\nhallucination on a synthetic task can also reduce hallucination on real-world\ndownstream tasks. Our method, SynTra, first designs a synthetic task where\nhallucinations are easy to elicit and measure. It next optimizes the LLM's\nsystem message via prefix-tuning on the synthetic task, and finally transfers\nthe system message to realistic, hard-to-optimize tasks. Across three realistic\nabstractive summarization tasks, SynTra reduces hallucination for two\n13B-parameter LLMs using only a synthetic retrieval task for supervision. We\nalso find that optimizing the system message rather than the model weights can\nbe critical; fine-tuning the entire model on the synthetic task can\ncounterintuitively increase hallucination. Overall, SynTra demonstrates that\nthe extra flexibility of working with synthetic data can help mitigate\nundesired behaviors in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1\">Erik Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoes_C/0/1/0/all/0/1\">Clarisse Sim&#xf5;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1\">Varun Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1\">Arindam Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. (arXiv:2310.12798v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.12798","description":"<p>Language Models (LMs) have demonstrated impressive molecule understanding\nability on various 1D text-related tasks. However, they inherently lack 2D\ngraph perception - a critical ability of human professionals in comprehending\nmolecules' topological structures. To bridge this gap, we propose MolCA:\nMolecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal\nAdapter. MolCA enables an LM (e.g., Galactica) to understand both text- and\ngraph-based molecular contents via the cross-modal projector. Specifically, the\ncross-modal projector is implemented as a Q-Former to connect a graph encoder's\nrepresentation space and an LM's text space. Further, MolCA employs a uni-modal\nadapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks.\nUnlike previous studies that couple an LM with a graph encoder via cross-modal\ncontrastive learning, MolCA retains the LM's ability of open-ended text\ngeneration and augments it with 2D graph information. To showcase its\neffectiveness, we extensively benchmark MolCA on tasks of molecule captioning,\nIUPAC name prediction, and molecule-text retrieval, on which MolCA\nsignificantly outperforms the baselines. Our codes and checkpoints can be found\nat https://github.com/acharkq/MolCA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sihang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yanchen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating LLaMA Inference by Enabling Intermediate Layer Decoding via Instruction Tuning with LITE. (arXiv:2310.18581v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.18581","description":"<p>Large Language Models (LLMs) have achieved remarkable performance across a\nwide variety of natural language tasks; however, their large size makes their\ninference slow and computationally expensive. Focusing on this problem, we\npropose to instruction tune LLMs with additional explicit losses from the\nintermediate layers (LITE) and show that it enables these layers to acquire\n'good' generation ability without affecting the generation ability of the final\nlayer. We perform 'dynamic confidence-based early exiting' at token level from\nthe intermediate layers which improves the efficiency of text generation\nwithout compromising the quality of the generation. We conduct comprehensive\nexperiments by instruction tuning LLaMA-2 models on the Alpaca dataset and\nholistically evaluate on four different human-instruction test sets. We show\nthat dynamic early exiting achieves consistent and considerable inference\ncomputation cost improvements (37.86% for 7B and 46.35% for 13B model) while\nmaintaining the generation quality of the responses. We further conduct a\nthorough analysis of the results over several important aspects, such as\ncomparing the semantic similarity of the outputs and dissecting the efficiency\nimprovements by comparing the number of tokens generated in the output. In\nsummary, our work contributes to improving the efficiency of LLM inference\nwhile maintaining the generation quality, a crucial step en route to enabling\ntheir widespread adoption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1\">Agneet Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.20246","description":"<p>Existing research predominantly focuses on developing powerful language\nlearning models (LLMs) for mathematical reasoning within monolingual languages,\nwith few explorations in preserving efficacy in a multilingual context. To\nbridge this gap, this paper pioneers exploring and training powerful\nMultilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we\nconstruct the first multilingual math reasoning instruction dataset,\nMGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue\nof training data scarcity in xMR tasks. Based on the collected dataset, we\npropose different training strategies to build powerful xMR LLMs, named\nMathOctopus, notably outperform conventional open-source LLMs and exhibit\nsuperiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B\nreaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond\nremarkable results, we unearth several pivotal observations and insights from\nextensive experiments: (1) When extending the rejection sampling strategy to\nthe multilingual context, it proves effective for model performances, albeit\nlimited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT)\nacross multiple languages not only significantly enhances model performance\nmultilingually but also elevates their monolingual performance. This indicates\nthat crafting multilingual corpora can be regarded as a vital strategy for\nenhancing model performance in a specific language, especially in mathematical\nreasoning tasks. For instance, MathOctopus-7B improves its counterparts that\ntrained on English from 42.2% to 50.8% on GSM8K testset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zinan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Ning Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.01305","description":"<p>Large language models(LLMs) exhibit excellent performance across a variety of\ntasks, but they come with significant computational and storage costs.\nQuantizing these models is an effective way to alleviate this issue. However,\nexisting methods struggle to strike a balance between model accuracy and\nhardware efficiency. This is where we introduce AWEQ, a post-training method\nthat requires no additional training overhead. AWEQ excels in both\nultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization.\nThere is an observation that weight quantization is less challenging than\nactivation quantization. AWEQ transfers the difficulty of activation\nquantization to weights using channel equalization, achieving a balance between\nthe quantization difficulties of both, and thereby maximizing performance. We\nhave further refined the equalization method to mitigate quantization bias\nerror, ensuring the robustness of the model. Extensive experiments on popular\nmodels such as LLaMA and OPT demonstrate that AWEQ outperforms all existing\npost-training quantization methods for large models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baisong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haixiao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion. (arXiv:2311.01767v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.01767","description":"<p>Recent evaluations of Large Language Models (LLMs) have centered around\ntesting their zero-shot/few-shot capabilities for basic natural language tasks\nand their ability to translate instructions into tool APIs. However, the\nevaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal\ninstructions in a complex multi-modal environment has not been investigated. To\naddress this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark\nto assess LLMs' ability to create and edit PPT files based on user\ninstructions. It contains 279 multi-turn sessions covering diverse topics and\nhundreds of instructions involving multi-modal operations. We also propose the\nPPTX-Match Evaluation System that evaluates if LLMs finish the instruction\nbased on the prediction file rather than the label API sequence, thus it\nsupports various LLM-generated API sequences. We measure 3 closed LLMs and 6\nopen-source LLMs. The results show that GPT-4 outperforms other LLMs with\n75.1\\% accuracy in single-turn dialogue testing but faces challenges in\ncompleting entire sessions, achieving just 6\\% session accuracy. We find three\nmain error causes in our benchmark: error accumulation in the multi-turn\nsession, long PPT template processing, and multi-modality perception. These\npose great challenges for future LLM and agent systems. We release the data,\ncode, and evaluation system of PPTC at \\url{https://github.com/gydpku/PPTC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiduo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zekai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-training and Co-distillation for Quality Improvement and Compression of Language Models. (arXiv:2311.02849v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.02849","description":"<p>Knowledge Distillation (KD) compresses computationally expensive pre-trained\nlanguage models (PLMs) by transferring their knowledge to smaller models,\nallowing their use in resource-constrained or real-time settings. However, most\nsmaller models fail to surpass the performance of the original larger model,\nresulting in sacrificing performance to improve inference speed. To address\nthis issue, we propose Co-Training and Co-Distillation (CTCD), a novel\nframework that improves performance and inference speed together by co-training\ntwo models while mutually distilling knowledge. The CTCD framework successfully\nachieves this based on two significant findings: 1) Distilling knowledge from\nthe smaller model to the larger model during co-training improves the\nperformance of the larger model. 2) The enhanced performance of the larger\nmodel further boosts the performance of the smaller model. The CTCD framework\nshows promise as it can be combined with existing techniques like architecture\ndesign or data augmentation, replacing one-way KD methods, to achieve further\nperformance improvement. Extensive ablation studies demonstrate the\neffectiveness of CTCD, and the small model distilled by CTCD outperforms the\noriginal larger model by a significant margin of 1.66 on the GLUE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Rui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jongpil Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Davis Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_A/0/1/0/all/0/1\">Alexander Min</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges. (arXiv:2311.03287v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.03287","description":"<p>While GPT-4V(ision) impressively models both visual and textual information\nsimultaneously, it's hallucination behavior has not been systematically\nassessed. To bridge this gap, we introduce a new benchmark, namely, the Bias\nand Interference Challenges in Visual Language Models (Bingo). This benchmark\nis designed to evaluate and shed light on the two common types of\nhallucinations in visual language models: bias and interference. Here, bias\nrefers to the model's tendency to hallucinate certain types of responses,\npossibly due to imbalance in its training data. Interference pertains to\nscenarios where the judgment of GPT-4V(ision) can be disrupted due to how the\ntext prompt is phrased or how the input image is presented. We identify a\nnotable regional bias, whereby GPT-4V(ision) is better at interpreting Western\nimages or images with English writing compared to images from other countries\nor containing text in other languages. Moreover, GPT-4V(ision) is vulnerable to\nleading questions and is often confused when interpreting multiple images\ntogether. Popular mitigation approaches, such as self-correction and\nchain-of-thought reasoning, are not effective in resolving these challenges. We\nalso identified similar biases and interference vulnerabilities with LLaVA and\nBard. Our results characterize the hallucination challenges in GPT-4V(ision)\nand state-of-the-art visual-language models, and highlight the need for new\nsolutions. The Bingo benchmark is available at https://github.com/gzcch/Bingo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chenhang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shirley Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huaxiu Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-11-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}