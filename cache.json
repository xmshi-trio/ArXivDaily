{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-16T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Artificial Intelligence in Psychology Research. (arXiv:2302.07267v1 [cs.HC])","link":"http://arxiv.org/abs/2302.07267","description":"<p>Large Language Models have vastly grown in capabilities. One potential\napplication of such AI systems is to support data collection in the social\nsciences, where perfect experimental control is currently unfeasible and the\ncollection of large, representative datasets is generally expensive. In this\npaper, we re-replicate 14 studies from the Many Labs 2 replication project\n(Klein et al., 2018) with OpenAI's text-davinci-003 model, colloquially known\nas GPT3.5. For the 10 studies that we could analyse, we collected a total of\n10,136 responses, each of which was obtained by running GPT3.5 with the\ncorresponding study's survey inputted as text. We find that our GPT3.5-based\nsample replicates 30% of the original results as well as 30% of the Many Labs 2\nresults, although there is heterogeneity in both these numbers (as we replicate\nsome original findings that Many Labs 2 did not and vice versa). We also find\nthat unlike the corresponding human subjects, GPT3.5 answered some survey\nquestions with extreme homogeneity$\\unicode{x2013}$with zero variation in\ndifferent runs' responses$\\unicode{x2013}$raising concerns that a hypothetical\nAI-led future may in certain ways be subject to a diminished diversity of\nthought. Overall, while our results suggest that Large Language Model\npsychology studies are feasible, their findings should not be assumed to\nstraightforwardly generalise to the human case. Nevertheless, AI-based data\ncollection may eventually become a viable and economically relevant method in\nthe empirical social sciences, making the understanding of its capabilities and\napplications central.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_P/0/1/0/all/0/1\">Peter S. Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoenegger_P/0/1/0/all/0/1\">Philipp Schoenegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chongyang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Chat Assistants can Improve Conversations about Divisive Topics. (arXiv:2302.07268v1 [cs.HC])","link":"http://arxiv.org/abs/2302.07268","description":"<p>A rapidly increasing amount of human conversation occurs online. But\ndivisiveness and conflict can fester in text-based interactions on social media\nplatforms, in messaging apps, and on other digital forums. Such toxicity\nincreases polarization and, importantly, corrodes the capacity of diverse\nsocieties to develop efficient solutions to complex social problems that impact\neveryone. Scholars and civil society groups promote interventions that can make\ninterpersonal conversations less divisive or more productive in offline\nsettings, but scaling these efforts to the amount of discourse that occurs\nonline is extremely challenging. We present results of a large-scale experiment\nthat demonstrates how online conversations about divisive topics can be\nimproved with artificial intelligence tools. Specifically, we employ a large\nlanguage model to make real-time, evidence-based recommendations intended to\nimprove participants' perception of feeling understood in conversations. We\nfind that these interventions improve the reported quality of the conversation,\nreduce political divisiveness, and improve the tone, without systematically\nchanging the content of the conversation or moving people's policy attitudes.\nThese findings have important implications for future research on social media,\npolitical deliberation, and the growing community of scholars interested in the\nplace of artificial intelligence within computational social science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Argyle_L/0/1/0/all/0/1\">Lisa P. Argyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busby_E/0/1/0/all/0/1\">Ethan Busby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gubler_J/0/1/0/all/0/1\">Joshua Gubler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bail_C/0/1/0/all/0/1\">Chris Bail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howe_T/0/1/0/all/0/1\">Thomas Howe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rytting_C/0/1/0/all/0/1\">Christopher Rytting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wingate_D/0/1/0/all/0/1\">David Wingate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRESTLE: Toolkit for Reproducible Execution of Speech, Text and Language Experiments. (arXiv:2302.07322v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07322","description":"<p>The evidence is growing that machine and deep learning methods can learn the\nsubtle differences between the language produced by people with various forms\nof cognitive impairment such as dementia and cognitively healthy individuals.\nValuable public data repositories such as TalkBank have made it possible for\nresearchers in the computational community to join forces and learn from each\nother to make significant advances in this area. However, due to variability in\napproaches and data selection strategies used by various researchers, results\nobtained by different groups have been difficult to compare directly. In this\npaper, we present TRESTLE (\\textbf{T}oolkit for \\textbf{R}eproducible\n\\textbf{E}xecution of \\textbf{S}peech \\textbf{T}ext and \\textbf{L}anguage\n\\textbf{E}xperiments), an open source platform that focuses on two datasets\nfrom the TalkBank repository with dementia detection as an illustrative domain.\nSuccessfully deployed in the hackallenge (Hackathon/Challenge) of the\nInternational Workshop on Health Intelligence at AAAI 2022, TRESTLE provides a\nprecise digital blueprint of the data pre-processing and selection strategies\nthat can be reused via TRESTLE by other researchers seeking comparable results\nwith their peers and current state-of-the-art (SOTA) approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Trevor Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalowski_M/0/1/0/all/0/1\">Martin Michalowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakhomov_S/0/1/0/all/0/1\">Serguei Pakhomov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises. (arXiv:2302.07324v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07324","description":"<p>For many real-world applications, the user-generated inputs usually contain\nvarious noises due to speech recognition errors caused by linguistic\nvariations1 or typographical errors (typos). Thus, it is crucial to test model\nperformance on data with realistic input noises to ensure robustness and\nfairness. However, little study has been done to construct such benchmarks for\nChinese, where various language-specific input noises happen in the real world.\nIn order to fill this important gap, we construct READIN: a Chinese multi-task\nbenchmark with REalistic And Diverse Input Noises. READIN contains four diverse\ntasks and requests annotators to re-enter the original test data with two\ncommonly used Chinese input methods: Pinyin input and speech input. We designed\nour annotation pipeline to maximize diversity, for example by instructing the\nannotators to use diverse input method editors (IMEs) for keyboard noises and\nrecruiting speakers from diverse dialectical groups for speech noises. We\nexperiment with a series of strong pretrained language models as well as robust\ntraining methods, we find that these models often suffer significant\nperformance drops on READIN even with robustness methods like data\naugmentation. As the first large-scale attempt in creating a benchmark with\nnoises geared towards user-generated inputs, we believe that READIN serves as\nan important complement to existing Chinese NLP benchmarks. The source code and\ndataset can be obtained from https://github.com/thunlp/READIN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenglei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfa Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScatterShot: Interactive In-context Example Curation for Text Transformation. (arXiv:2302.07346v1 [cs.HC])","link":"http://arxiv.org/abs/2302.07346","description":"<p>The in-context learning capabilities of LLMs like GPT-3 allow annotators to\ncustomize an LLM to their specific tasks with a small number of examples.\nHowever, users tend to include only the most obvious patterns when crafting\nexamples, resulting in underspecified in-context functions that fall short on\nunseen cases. Further, it is hard to know when \"enough\" examples have been\nincluded even for known patterns. In this work, we present ScatterShot, an\ninteractive system for building high-quality demonstration sets for in-context\nlearning. ScatterShot iteratively slices unlabeled data into task-specific\npatterns, samples informative inputs from underexplored or not-yet-saturated\nslices in an active learning manner, and helps users label more efficiently\nwith the help of an LLM and the current example set. In simulation studies on\ntwo text perturbation scenarios, ScatterShot sampling improves the resulting\nfew-shot functions by 4-5 percentage points over random sampling, with less\nvariance as more examples are added. In a user study, ScatterShot greatly helps\nusers in covering different patterns in the input space and labeling in-context\nexamples more efficiently, resulting in better in-context learning and less\nuser effort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heer_J/0/1/0/all/0/1\">Jeffrey Heer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marco Tulio Ribeiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoBiasTest: Controllable Sentence Generation for Automated and Open-Ended Social Bias Testing in Language Models. (arXiv:2302.07371v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07371","description":"<p>Social bias in Pretrained Language Models (PLMs) affects text generation and\nother downstream NLP tasks. Existing bias testing methods rely predominantly on\nmanual templates or on expensive crowd-sourced data. We propose a novel\nAutoBiasTest method that automatically generates sentences for testing bias in\nPLMs, hence providing a flexible and low-cost alternative. Our approach uses\nanother PLM for generation and controls the generation of sentences by\nconditioning on social group and attribute terms. We show that generated\nsentences are natural and similar to human-produced content in terms of word\nlength and diversity. We illustrate that larger models used for generation\nproduce estimates of social bias with lower variance. We find that our bias\nscores are well correlated with manual templates, but AutoBiasTest highlights\nbiases not captured by these templates due to more diverse and realistic test\nsentences. By automating large-scale test sentence generation, we enable better\nestimation of underlying bias distributions\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocielnik_R/0/1/0/all/0/1\">Rafal Kocielnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1\">Shrimai Prabhumoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_V/0/1/0/all/0/1\">Vivian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_R/0/1/0/all/0/1\">R. Michael Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models. (arXiv:2302.07388v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07388","description":"<p>Pretrained large language models have become indispensable for solving\nvarious natural language processing (NLP) tasks. However, safely deploying them\nin real world applications is challenging because they generate toxic content.\nTo address this challenge, we propose two novel pretraining data augmentation\nstrategies that significantly reduce model toxicity without compromising its\nutility. Our two strategies are: (1) MEDA: adds raw toxicity score as meta-data\nto the pretraining samples, and (2) INST: adds instructions to those samples\nindicating their toxicity. Our results indicate that our best performing\nstrategy (INST) substantially reduces the toxicity probability up to 61% while\npreserving the accuracy on five benchmark NLP tasks as well as improving AUC\nscores on four bias detection tasks by 1.3%. We also demonstrate the\ngeneralizability of our techniques by scaling the number of training samples\nand the number of model parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1\">Shrimai Prabhumoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. (arXiv:2302.07452v1 [cs.IR])","link":"http://arxiv.org/abs/2302.07452","description":"<p>Various techniques have been developed in recent years to improve dense\nretrieval (DR), such as unsupervised contrastive learning and pseudo-query\ngeneration. Existing DRs, however, often suffer from effectiveness tradeoffs\nbetween supervised and zero-shot retrieval, which some argue was due to the\nlimited model capacity. We contradict this hypothesis and show that a\ngeneralizable DR can be trained to achieve high accuracy in both supervised and\nzero-shot retrieval without increasing model size. In particular, we\nsystematically examine the contrastive learning of DRs, under the framework of\nData Augmentation (DA). Our study shows that common DA practices such as query\naugmentation with generative models and pseudo-relevance label creation using a\ncross-encoder, are often inefficient and sub-optimal. We hence propose a new DA\napproach with diverse queries and sources of supervision to progressively train\na generalizable DR. As a result, DRAGON, our dense retriever trained with\ndiverse augmentation, is the first BERT-base-sized DR to achieve\nstate-of-the-art effectiveness in both supervised and zero-shot evaluations and\neven competes with models using more complex late interaction (ColBERTv2 and\nSPLADE++).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sheng-Chieh Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Capacity for Moral Self-Correction in Large Language Models. (arXiv:2302.07459v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07459","description":"<p>We test the hypothesis that language models trained with reinforcement\nlearning from human feedback (RLHF) have the capability to \"morally\nself-correct\" -- to avoid producing harmful outputs -- if instructed to do so.\nWe find strong evidence in support of this hypothesis across three different\nexperiments, each of which reveal different facets of moral self-correction. We\nfind that the capability for moral self-correction emerges at 22B model\nparameters, and typically improves with increasing model size and RLHF\ntraining. We believe that at this level of scale, language models obtain two\ncapabilities that they can use for moral self-correction: (1) they can follow\ninstructions and (2) they can learn complex normative concepts of harm like\nstereotyping, bias, and discrimination. As such, they can follow instructions\nto avoid certain kinds of morally harmful outputs. We believe our results are\ncause for cautious optimism regarding the ability to train language models to\nabide by ethical principles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1\">Deep Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1\">Amanda Askell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1\">Nicholas Schiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1\">Thomas Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukosiute_K/0/1/0/all/0/1\">Kamil&#x117; Luko&#x161;i&#x16b;t&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anna Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldie_A/0/1/0/all/0/1\">Anna Goldie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirhoseini_A/0/1/0/all/0/1\">Azalia Mirhoseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsson_C/0/1/0/all/0/1\">Catherine Olsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1\">Danny Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dustin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Johnson_E/0/1/0/all/0/1\">Eli Tran-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kernion_J/0/1/0/all/0/1\">Jackson Kernion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerr_J/0/1/0/all/0/1\">Jamie Kerr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Jared Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landau_J/0/1/0/all/0/1\">Joshua Landau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1\">Kamal Ndousse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Karina Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovitt_L/0/1/0/all/0/1\">Liane Lovitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellitto_M/0/1/0/all/0/1\">Michael Sellitto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhage_N/0/1/0/all/0/1\">Nelson Elhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercado_N/0/1/0/all/0/1\">Noemi Mercado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1\">Nova DasSarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_R/0/1/0/all/0/1\">Robert Lasenby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larson_R/0/1/0/all/0/1\">Robin Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ringer_S/0/1/0/all/0/1\">Sam Ringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Sandipan Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnston_S/0/1/0/all/0/1\">Scott Johnston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1\">Shauna Kravec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Showk_S/0/1/0/all/0/1\">Sheer El Showk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanham_T/0/1/0/all/0/1\">Tamera Lanham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telleen_Lawton_T/0/1/0/all/0/1\">Timothy Telleen-Lawton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henighan_T/0/1/0/all/0/1\">Tom Henighan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hume_T/0/1/0/all/0/1\">Tristan Hume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuntao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatfield_Dodds_Z/0/1/0/all/0/1\">Zac Hatfield-Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_B/0/1/0/all/0/1\">Ben Mann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amodei_D/0/1/0/all/0/1\">Dario Amodei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1\">Sam McCandlish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1\">Tom Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olah_C/0/1/0/all/0/1\">Christopher Olah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jack Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Envisioning the Next-Gen Document Reader. (arXiv:2302.07492v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07492","description":"<p>People read digital documents on a daily basis to share, exchange, and\nunderstand information in electronic settings. However, current document\nreaders create a static, isolated reading experience, which does not support\nusers' goals of gaining more knowledge and performing additional tasks through\ndocument interaction. In this work, we present our vision for the next-gen\ndocument reader that strives to enhance user understanding and create a more\nconnected, trustworthy information experience. We describe 18 NLP-powered\nfeatures to add to existing document readers and propose a novel plug-in\nmarketplace that allows users to further customize their reading experience, as\ndemonstrated through 3 exploratory UI prototypes available at\nhttps://github.com/catherinesyeh/nextgen-prototypes\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Catherine Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1\">Nedim Lipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word class representations spontaneously emerge in a deep neural network trained on next word prediction. (arXiv:2302.07588v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07588","description":"<p>How do humans learn language, and can the first language be learned at all?\nThese fundamental questions are still hotly debated. In contemporary\nlinguistics, there are two major schools of thought that give completely\nopposite answers. According to Chomsky's theory of universal grammar, language\ncannot be learned because children are not exposed to sufficient data in their\nlinguistic environment. In contrast, usage-based models of language assume a\nprofound relationship between language structure and language use. In\nparticular, contextual mental processing and mental representations are assumed\nto have the cognitive capacity to capture the complexity of actual language use\nat all levels. The prime example is syntax, i.e., the rules by which words are\nassembled into larger units such as sentences. Typically, syntactic rules are\nexpressed as sequences of word classes. However, it remains unclear whether\nword classes are innate, as implied by universal grammar, or whether they\nemerge during language acquisition, as suggested by usage-based approaches.\nHere, we address this issue from a machine learning and natural language\nprocessing perspective. In particular, we trained an artificial deep neural\nnetwork on predicting the next word, provided sequences of consecutive words as\ninput. Subsequently, we analyzed the emerging activation patterns in the hidden\nlayers of the neural network. Strikingly, we find that the internal\nrepresentations of nine-word input sequences cluster according to the word\nclass of the tenth word to be predicted as output, even though the neural\nnetwork did not receive any explicit information about syntactic rules or word\nclasses during training. This surprising result suggests, that also in the\nhuman brain, abstract representational categories such as word classes may\nnaturally emerge as a consequence of predictive coding and processing during\nlanguage acquisition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Surendra_K/0/1/0/all/0/1\">Kishore Surendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schilling_A/0/1/0/all/0/1\">Achim Schilling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoewer_P/0/1/0/all/0/1\">Paul Stoewer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauss_P/0/1/0/all/0/1\">Patrick Krauss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DP-BART for Privatized Text Rewriting under Local Differential Privacy. (arXiv:2302.07636v1 [cs.CR])","link":"http://arxiv.org/abs/2302.07636","description":"<p>Privatized text rewriting with local differential privacy (LDP) is a recent\napproach that enables sharing of sensitive textual documents while formally\nguaranteeing privacy protection to individuals. However, existing systems face\nseveral issues, such as formal mathematical flaws, unrealistic privacy\nguarantees, privatization of only individual words, as well as a lack of\ntransparency and reproducibility. In this paper, we propose a new system\n'DP-BART' that largely outperforms existing LDP systems. Our approach uses a\nnovel clipping method, iterative pruning, and further training of internal\nrepresentations which drastically reduces the amount of noise required for DP\nguarantees. We run experiments on five textual datasets of varying sizes,\nrewriting them at different privacy guarantees and evaluating the rewritten\ntexts on downstream text classification tasks. Finally, we thoroughly discuss\nthe privatized text rewriting approach and its limitations, including the\nproblem of the strict text adjacency constraint in the LDP paradigm that leads\nto the high noise requirement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Igamberdiev_T/0/1/0/all/0/1\">Timour Igamberdiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On graph-based reentrancy-free semantic parsing. (arXiv:2302.07679v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07679","description":"<p>We propose a novel graph-based approach for semantic parsing that resolves\ntwo problems observed in the literature: (1) seq2seq models fail on\ncompositional generalization tasks; (2) previous work using phrase structure\nparsers cannot cover all the semantic parses observed in treebanks. We prove\nthat both MAP inference and latent tag anchoring (required for\nweakly-supervised learning) are NP-hard problems. We propose two optimization\nalgorithms based on constraint smoothing and conditional gradient to\napproximately solve these inference problems. Experimentally, our approach\ndelivers state-of-the-art results on Geoquery, Scan and Clevr, both for i.i.d.\nsplits and for splits that test for compositional generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petit_A/0/1/0/all/0/1\">Alban Petit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corro_C/0/1/0/all/0/1\">Caio Corro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Enhanced Semantic Communication Receiver. (arXiv:2302.07727v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07727","description":"<p>In recent years, with the rapid development of deep learning and natural\nlanguage processing technologies, semantic communication has become a topic of\ngreat interest in the field of communication. Although existing deep learning\nbased semantic communication approaches have shown many advantages, they still\ndo not make sufficient use of prior knowledge. Moreover, most existing semantic\ncommunication methods focus on the semantic encoding at the transmitter side,\nwhile we believe that the semantic decoding capability of the receiver side\nshould also be concerned. In this paper, we propose a knowledge enhanced\nsemantic communication framework in which the receiver can more actively\nutilize the prior knowledge in the knowledge base for semantic reasoning and\ndecoding, without extra modifications to the neural network structure of the\ntransmitter. Specifically, we design a transformer-based knowledge extractor to\nfind relevant factual triples for the received noisy signal. Extensive\nsimulation results on the WebNLG dataset demonstrate that the proposed receiver\nyields superior performance on top of the knowledge graph enhanced decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianhang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhifeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honggang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AIDA: Legal Judgment Predictions for Non-Professional Fact Descriptions via Partial-and-Imbalanced Domain Adaptation. (arXiv:2302.07728v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07728","description":"<p>In this paper, we study the problem of legal domain adaptation problem from\nan imbalanced source domain to a partial target domain. The task aims to\nimprove legal judgment predictions for non-professional fact descriptions. We\nformulate this task as a partial-and-imbalanced domain adaptation problem.\nThough deep domain adaptation has achieved cutting-edge performance in many\nunsupervised domain adaptation tasks. However, due to the negative transfer of\nsamples in non-shared classes, it is hard for current domain adaptation model\nto solve the partial-and-imbalanced transfer problem. In this work, we explore\nlarge-scale non-shared but related classes data in the source domain with a\nhierarchy weighting adaptation to tackle this limitation. We propose to embed a\nnovel pArtial Imbalanced Domain Adaptation technique (AIDA) in the deep\nlearning model, which can jointly borrow sibling knowledge from non-shared\nclasses to shared classes in the source domain and further transfer the shared\nclasses knowledge from the source domain to the target domain. Experimental\nresults show that our model outperforms the state-of-the-art algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Guangyi Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinlong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jingzhi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiguo Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings. (arXiv:2302.07729v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07729","description":"<p>Nowadays many research articles are prefaced with research highlights to\nsummarize the main findings of the paper. Highlights not only help researchers\nprecisely and quickly identify the contributions of a paper, they also enhance\nthe discoverability of the article via search engines. We aim to automatically\nconstruct research highlights given certain segments of the research paper. We\nuse a pointer-generator network with coverage mechanism and a contextual\nembedding layer at the input that encodes the input tokens into SciBERT\nembeddings. We test our model on a benchmark dataset, CSPubSum and also present\nMixSub, a new multi-disciplinary corpus of papers for automatic research\nhighlight generation. For both CSPubSum and MixSub, we have observed that the\nproposed model achieves the best performance compared to related variants and\nother models proposed in the literature. On the CSPubSum data set, our model\nachieves the best performance when the input is only the abstract of a paper as\nopposed to other segments of the paper. It produces ROUGE-1, ROUGE-2 and\nROUGE-L F1-scores of 38.26, 14.26 and 35.51, respectively, METEOR F1-score of\n32.62, and BERTScore F1 of 86.65 which outperform all other baselines. On the\nnew MixSub data set, where only the abstract is the input, our proposed model\n(when trained on the whole training corpus without distinguishing between the\nsubject categories) achieves ROUGE-1, ROUGE-2 and ROUGE-L F1-scores of 31.78,\n9.76 and 29.3, respectively, METEOR F1-score of 24.00, and BERTScore F1 of\n85.25, outperforming other models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rehman_T/0/1/0/all/0/1\">Tohida Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_D/0/1/0/all/0/1\">Debarshi Kumar Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Samiran Chattopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhowmick_P/0/1/0/all/0/1\">Plaban Kumar Bhowmick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Partha Pratim Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer models: an introduction and catalog. (arXiv:2302.07730v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07730","description":"<p>In the past few years we have seen the meteoric appearance of dozens of\nmodels of the Transformer family, all of which have funny, but not\nself-explanatory, names. The goal of this paper is to offer a somewhat\ncomprehensive but simple catalog and classification of the most popular\nTransformer models. The paper also includes an introduction to the most\nimportant aspects and innovation in Transformer models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amatriain_X/0/1/0/all/0/1\">Xavier Amatriain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews on Social Media. (arXiv:2302.07731v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07731","description":"<p>Recent advances in generative models such as GPT may be used to fabricate\nindistinguishable fake customer reviews at a much lower cost, thus posing\nchallenges for social media platforms to detect these machine-generated fake\nreviews. We propose to leverage the high-quality elite restaurant reviews\nverified by Yelp to generate fake reviews from the OpenAI GPT review creator\nand ultimately fine-tune a GPT output detector to predict fake reviews that\nsignificantly outperforms existing solutions. We further apply the model to\npredict non-elite reviews and identify the patterns across several dimensions,\nsuch as review, user and restaurant characteristics, and writing style. We show\nthat social media platforms are continuously challenged by machine-generated\nfake reviews, although they may implement detection systems to filter out\nsuspicious reviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gambetti_A/0/1/0/all/0/1\">Alessandro Gambetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qiwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining text classifiers through progressive neighborhood approximation with realistic samples. (arXiv:2302.07733v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07733","description":"<p>The importance of neighborhood construction in local explanation methods has\nbeen already highlighted in the literature. And several attempts have been made\nto improve neighborhood quality for high-dimensional data, for example, texts,\nby adopting generative models. Although the generators produce more realistic\nsamples, the intuitive sampling approaches in the existing solutions leave the\nlatent space underexplored. To overcome this problem, our work, focusing on\nlocal model-agnostic explanations for text classifiers, proposes a progressive\napproximation approach that refines the neighborhood of a to-be-explained\ndecision with a careful two-stage interpolation using counterfactuals as\nlandmarks. We explicitly specify the two properties that should be satisfied by\ngenerative models, the reconstruction ability and the locality-preserving\nproperty, to guide the selection of generators for local explanation methods.\nMoreover, noticing the opacity of generative models during the study, we\npropose another method that implements progressive neighborhood approximation\nwith probability-based editions as an alternative to the generator-based\nsolution. The explanation results from both methods consist of word-level and\ninstance-level explanations benefiting from the realistic neighborhood. Through\nexhaustive experiments, we qualitatively and quantitatively demonstrate the\neffectiveness of the two proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimek_A/0/1/0/all/0/1\">Arthur Zimek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wunder_G/0/1/0/all/0/1\">Gerhard Wunder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Targeted Attack on GPT-Neo for the SATML Language Model Data Extraction Challenge. (arXiv:2302.07735v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07735","description":"<p>Previous work has shown that Large Language Models are susceptible to\nso-called data extraction attacks. This allows an attacker to extract a sample\nthat was contained in the training data, which has massive privacy\nimplications. The construction of data extraction attacks is challenging,\ncurrent attacks are quite inefficient, and there exists a significant gap in\nthe extraction capabilities of untargeted attacks and memorization. Thus,\ntargeted attacks are proposed, which identify if a given sample from the\ntraining data, is extractable from a model. In this work, we apply a targeted\ndata extraction attack to the SATML2023 Language Model Training Data Extraction\nChallenge. We apply a two-step approach. In the first step, we maximise the\nrecall of the model and are able to extract the suffix for 69% of the samples.\nIn the second step, we use a classifier-based Membership Inference Attack on\nthe generations. Our AutoSklearn classifier achieves a precision of 0.841. The\nfull approach reaches a score of 0.405 recall at a 10% false positive rate,\nwhich is an improvement of 34% over the baseline of 0.301.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Kaswan_A/0/1/0/all/0/1\">Ali Al-Kaswan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izadi_M/0/1/0/all/0/1\">Maliheh Izadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deursen_A/0/1/0/all/0/1\">Arie van Deursen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech. (arXiv:2302.07736v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07736","description":"<p>Recent studies have alarmed that many online hate speeches are implicit. With\nits subtle nature, the explainability of the detection of such hateful speech\nhas been a challenging problem. In this work, we examine whether ChatGPT can be\nused for providing natural language explanations (NLEs) for implicit hateful\nspeech detection. We design our prompt to elicit concise ChatGPT-generated NLEs\nand conduct user studies to evaluate their qualities by comparison with\nhuman-generated NLEs. We discuss the potential and limitations of ChatGPT in\nthe context of implicit hateful speech research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Haewoon Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1\">Jisun An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alloprof: a new French question-answer education dataset and its use in an information retrieval case study. (arXiv:2302.07738v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07738","description":"<p>Teachers and students are increasingly relying on online learning resources\nto supplement the ones provided in school. This increase in the breadth and\ndepth of available resources is a great thing for students, but only provided\nthey are able to find answers to their queries. Question-answering and\ninformation retrieval systems have benefited from public datasets to train and\nevaluate their algorithms, but most of these datasets have been in English text\nwritten by and for adults. We introduce a new public French question-answering\ndataset collected from Alloprof, a Quebec-based primary and high-school help\nwebsite, containing 29 349 questions and their explanations in a variety of\nschool subjects from 10 368 students, with more than half of the explanations\ncontaining links to other questions or some of the 2 596 reference pages on the\nwebsite. We also present a case study of this dataset in an information\nretrieval task. This dataset was collected on the Alloprof public forum, with\nall questions verified for their appropriateness and the explanations verified\nboth for their appropriateness and their relevance to the question. To predict\nrelevant documents, architectures using pre-trained BERT models were fine-tuned\nand evaluated. This dataset will allow researchers to develop\nquestion-answering, information retrieval and other algorithms specifically for\nthe French speaking education context. Furthermore, the range of language\nproficiency, images, mathematical symbols and spelling mistakes will\nnecessitate algorithms based on a multimodal comprehension. The case study we\npresent as a baseline shows an approach that relies on recent techniques\nprovides an acceptable performance level, but more work is necessary before it\ncan reliably be used and trusted in a production setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lefebvre_Brossard_A/0/1/0/all/0/1\">Antoine Lefebvre-Brossard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gazaille_S/0/1/0/all/0/1\">Stephane Gazaille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desmarais_M/0/1/0/all/0/1\">Michel C. Desmarais</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Learning Triplet Network with Adaptive Margins for Few-Shot Named Entity Recognition. (arXiv:2302.07739v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07739","description":"<p>Meta-learning methods have been widely used in few-shot named entity\nrecognition (NER), especially prototype-based methods. However, the Other(O)\nclass is difficult to be represented by a prototype vector because there are\ngenerally a large number of samples in the class that have miscellaneous\nsemantics. To solve the problem, we propose MeTNet, which generates prototype\nvectors for entity types only but not O-class. We design an improved triplet\nnetwork to map samples and prototype vectors into a low-dimensional space that\nis easier to be classified and propose an adaptive margin for each entity type.\nThe margin plays as a radius and controls a region with adaptive size in the\nlow-dimensional space. Based on the regions, we propose a new inference\nprocedure to predict the label of a query instance. We conduct extensive\nexperiments in both in-domain and cross-domain settings to show the superiority\nof MeTNet over other state-of-the-art methods. In particular, we release a\nChinese few-shot NER dataset FEW-COMM extracted from a well-known e-commerce\nplatform. To the best of our knowledge, this is the first Chinese few-shot NER\ndataset. All the datasets and codes are provided at\nhttps://github.com/hccngu/MeTNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chengcheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Renyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_J/0/1/0/all/0/1\">Jun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">FengJiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Ming Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xuezhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Team Triple-Check at Factify 2: Parameter-Efficient Large Foundation Models with Feature Representations for Multi-Modal Fact Verification. (arXiv:2302.07740v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07740","description":"<p>Multi-modal fact verification has become an important but challenging issue\non social media due to the mismatch between the text and images in the\nmisinformation of news content, which has been addressed by considering\ncross-modalities to identify the veracity of the news in recent years. In this\npaper, we propose the Pre-CoFactv2 framework with new parameter-efficient\nfoundation models for modeling fine-grained text and input embeddings with\nlightening parameters, multi-modal multi-type fusion for not only capturing\nrelations for the same and different modalities but also for different types\n(i.e., claim and document), and feature representations for explicitly\nproviding metadata for each sample. In addition, we introduce a unified\nensemble method to boost model performance by adjusting the importance of each\ntrained model with not only the weights but also the powers. Extensive\nexperiments show that Pre-CoFactv2 outperforms Pre-CoFact by a large margin and\nachieved new state-of-the-art results at the Factify challenge at AAAI 2023. We\nfurther illustrate model variations to verify the relative contributions of\ndifferent components. Our team won the first prize (F1-score: 81.82%) and we\nmade our code publicly available at\nhttps://github.com/wwweiwei/Pre-CoFactv2-AAAI-2023.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wei-Wei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hong-Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei-Yao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wen-Chih Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Whats New? Identifying the Unfolding of New Events in Narratives. (arXiv:2302.07748v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07748","description":"<p>Narratives include a rich source of events unfolding over time and context.\nAutomatic understanding of these events may provide a summarised comprehension\nof the narrative for further computation (such as reasoning). In this paper, we\nstudy the Information Status (IS) of the events and propose a novel challenging\ntask: the automatic identification of new events in a narrative. We define an\nevent as a triplet of subject, predicate, and object. The event is categorized\nas new with respect to the discourse context and whether it can be inferred\nthrough commonsense reasoning. We annotated a publicly available corpus of\nnarratives with the new events at sentence level using human annotators. We\npresent the annotation protocol and a study aiming at validating the quality of\nthe annotation and the difficulty of the task. We publish the annotated\ndataset, annotation materials, and machine learning baseline models for the\ntask of new event extraction for narrative understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1\">Seyed Mahed Mousavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_S/0/1/0/all/0/1\">Shohei Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roccabruna_G/0/1/0/all/0/1\">Gabriel Roccabruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshino_K/0/1/0/all/0/1\">Koichiro Yoshino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riccardi_G/0/1/0/all/0/1\">Giuseppe Riccardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the Instability of Fine-Tuning. (arXiv:2302.07778v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07778","description":"<p>Fine-tuning pre-trained language models on downstream tasks with varying\nrandom seeds has been shown to be unstable, especially on small datasets. Many\nprevious studies have investigated this instability and proposed methods to\nmitigate it. However, most studies only used the standard deviation of\nperformance scores (SD) as their measure, which is a narrow characterization of\ninstability. In this paper, we analyze SD and six other measures quantifying\ninstability at different levels of granularity. Moreover, we propose a\nsystematic framework to evaluate the validity of these measures. Finally, we\nanalyze the consistency and difference between different measures by\nreassessing existing instability mitigation methods. We hope our results will\ninform the development of better measurements of fine-tuning instability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yupei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dong Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmented Language Models: a Survey. (arXiv:2302.07842v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07842","description":"<p>This survey reviews works in which language models (LMs) are augmented with\nreasoning skills and the ability to use tools. The former is defined as\ndecomposing a potentially complex task into simpler subtasks while the latter\nconsists in calling external modules such as a code interpreter. LMs can\nleverage these augmentations separately or in combination via heuristics, or\nlearn to do so from demonstrations. While adhering to a standard missing tokens\nprediction objective, such augmented LMs can use various, possibly\nnon-parametric external modules to expand their context processing ability,\nthus departing from the pure language modeling paradigm. We therefore refer to\nthem as Augmented Language Models (ALMs). The missing token objective allows\nALMs to learn to reason, use tools, and even act, while still performing\nstandard natural language tasks and even outperforming most regular LMs on\nseveral benchmarks. In this work, after reviewing current advance in ALMs, we\nconclude that this new research direction has the potential to address common\nlimitations of traditional LMs such as interpretability, consistency, and\nscalability issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mialon_G/0/1/0/all/0/1\">Gr&#xe9;goire Mialon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1\">Roberto Dess&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomeli_M/0/1/0/all/0/1\">Maria Lomeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalmpantis_C/0/1/0/all/0/1\">Christoforos Nalmpantis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ram Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1\">Roberta Raileanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roziere_B/0/1/0/all/0/1\">Baptiste Rozi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_Yu_J/0/1/0/all/0/1\">Jane Dwivedi-Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grave_E/0/1/0/all/0/1\">Edouard Grave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NL2CMD: An Updated Workflow for Natural Language to Bash Commands Translation. (arXiv:2302.07845v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07845","description":"<p>Translating natural language into Bash Commands is an emerging research field\nthat has gained attention in recent years. Most efforts have focused on\nproducing more accurate translation models. To the best of our knowledge, only\ntwo datasets are available, with one based on the other. Both datasets involve\nscraping through known data sources (through platforms like stack overflow,\ncrowdsourcing, etc.) and hiring experts to validate and correct either the\nEnglish text or Bash Commands.\n</p>\n<p>This paper provides two contributions to research on synthesizing Bash\nCommands from scratch. First, we describe a state-of-the-art translation model\nused to generate Bash Commands from the corresponding English text. Second, we\nintroduce a new NL2CMD dataset that is automatically generated, involves\nminimal human intervention, and is over six times larger than prior datasets.\nSince the generation pipeline does not rely on existing Bash Commands, the\ndistribution and types of commands can be custom adjusted. Our empirical\nresults show how the scale and diversity of our dataset can offer unique\nopportunities for semantic parsing researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Quchen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhongwei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgaklis_M/0/1/0/all/0/1\">Marco Georgaklis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Jules White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1\">Douglas C. Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation. (arXiv:2302.07856v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07856","description":"<p>Large language models (LLMs) demonstrate remarkable machine translation (MT)\nabilities via prompting, even though they were not explicitly trained for this\ntask. However, even given the incredible quantities of data they are trained\non, LLMs can struggle to translate inputs with rare words, which are common in\nlow resource or domain transfer scenarios. We show that LLM prompting can\nprovide an effective solution for rare words as well, by using prior knowledge\nfrom bilingual dictionaries to provide control hints in the prompts. We propose\na novel method, DiPMT, that provides a set of possible translations for a\nsubset of the input words, thereby enabling fine-grained phrase-level prompted\ncontrol of the LLM. Extensive experiments show that DiPMT outperforms the\nbaseline both in low-resource MT, as well as for out-of-domain MT. We further\nprovide a qualitative analysis of the benefits and limitations of this\napproach, including the overall level of controllability that is achieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1\">Marjan Ghazvininejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1\">Hila Gonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Big Little Transformer Decoder. (arXiv:2302.07863v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07863","description":"<p>The recent emergence of Large Language Models based on the Transformer\narchitecture has enabled dramatic advancements in the field of Natural Language\nProcessing. However, these models have long inference latency, which limits\ntheir deployment, and which makes them prohibitively expensive for various\nreal-time applications. The inference latency is further exacerbated by\nautoregressive generative tasks, as models need to run iteratively to generate\ntokens sequentially without leveraging token-level parallelization. To address\nthis, we propose Big Little Decoder (BiLD), a framework that can improve\ninference efficiency and latency for a wide range of text generation\napplications. The BiLD framework contains two models with different sizes that\ncollaboratively generate text. The small model runs autoregressively to\ngenerate text with a low inference cost, and the large model is only invoked\noccasionally to refine the small model's inaccurate predictions in a\nnon-autoregressive manner. To coordinate the small and large models, BiLD\nintroduces two simple yet effective policies: (1) the fallback policy that\ndetermines when to hand control over to the large model; and (2) the rollback\npolicy that determines when the large model needs to review and correct the\nsmall model's inaccurate predictions. To evaluate our framework across\ndifferent tasks and models, we apply BiLD to various text generation scenarios\nencompassing machine translation on IWSLT 2017 De-En and WMT 2014 De-En,\nsummarization on CNN/DailyMail, and language modeling on WikiText-2. On an\nNVIDIA Titan Xp GPU, our framework achieves a speedup of up to 2.13x without\nany performance drop, and it achieves up to 2.38x speedup with only ~1 point\ndegradation. Furthermore, our framework is fully plug-and-play as it does not\nrequire any training or modifications to model architectures. Our code will be\nopen-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?. (arXiv:2302.07866v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07866","description":"<p>Compositionality is a pivotal property of symbolic reasoning. However, how\nwell recent neural models capture compositionality remains underexplored in the\nsymbolic reasoning tasks. This study empirically addresses this question by\nsystematically examining recently published pre-trained seq2seq models with a\ncarefully controlled dataset of multi-hop arithmetic symbolic reasoning. We\nintroduce a skill tree on compositionality in arithmetic symbolic reasoning\nthat defines the hierarchical levels of complexity along with three\ncompositionality dimensions: systematicity, productivity, and substitutivity.\nOur experiments revealed that among the three types of composition, the models\nstruggled most with systematicity, performing poorly even with relatively\nsimple compositions. That difficulty was not resolved even after training the\nmodels with intermediate reasoning steps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kudo_K/0/1/0/all/0/1\">Keito Kudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoki_Y/0/1/0/all/0/1\">Yoichi Aoki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuribayashi_T/0/1/0/all/0/1\">Tatsuki Kuribayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brassard_A/0/1/0/all/0/1\">Ana Brassard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1\">Masashi Yoshikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sub-Character Tokenization for Chinese Pretrained Language Models. (arXiv:2106.00400v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.00400","description":"<p>Tokenization is fundamental to pretrained language models (PLMs). Existing\ntokenization methods for Chinese PLMs typically treat each character as an\nindivisible token. However, they ignore the unique feature of the Chinese\nwriting system where additional linguistic information exists below the\ncharacter level, i.e., at the sub-character level. To utilize such information,\nwe propose sub-character (SubChar for short) tokenization. Specifically, we\nfirst encode the input text by converting each Chinese character into a short\nsequence based on its glyph or pronunciation, and then construct the vocabulary\nbased on the encoded text with sub-word segmentation. Experimental results show\nthat SubChar tokenizers have two main advantages over existing tokenizers: 1)\nThey can tokenize inputs into much shorter sequences, thus improving the\ncomputational efficiency. 2) Pronunciation-based SubChar tokenizers can encode\nChinese homophones into the same transliteration sequences and produce the same\ntokenization output, hence being robust to homophone typos. At the same time,\nmodels trained with SubChar tokenizers perform competitively on downstream\ntasks. We release our code and models at\nhttps://github.com/thunlp/SubCharTokenization to facilitate future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenglei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfa Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation. (arXiv:2106.05970v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.05970","description":"<p>Automatic evaluations for natural language generation (NLG) conventionally\nrely on token-level or embedding-level comparisons with text references. This\ndiffers from human language processing, for which visual imagination often\nimproves comprehension. In this work, we propose ImaginE, an imagination-based\nautomatic evaluation metric for natural language generation. With the help of\nStableDiffusion, a state-of-the-art text-to-image generator, we automatically\ngenerate an image as the embodied imagination for the text snippet and compute\nthe imagination similarity using contextual embeddings. Experiments spanning\nseveral text generation tasks demonstrate that adding machine-generated images\nwith our ImaginE displays great potential in introducing multi-modal\ninformation into NLG evaluation, and improves existing automatic metrics'\ncorrelations with human similarity judgments in both reference-based and\nreference-free evaluation scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1\">An Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning. (arXiv:2110.04429v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04429","description":"<p>Distantly supervised named entity recognition (DS-NER) efficiently reduces\nlabor costs but meanwhile intrinsically suffers from the label noise due to the\nstrong assumption of distant supervision. Typically, the wrongly labeled\ninstances comprise numbers of incomplete and inaccurate annotation noise, while\nmost prior denoising works are only concerned with one kind of noise and fail\nto fully explore useful information in the whole training set. To address this\nissue, we propose a robust learning paradigm named Self-Collaborative Denoising\nLearning (SCDL), which jointly trains two teacher-student networks in a\nmutually-beneficial manner to iteratively perform noisy label refinery. Each\nnetwork is designed to exploit reliable labels via self denoising, and two\nnetworks communicate with each other to explore unreliable annotations by\ncollaborative denoising. Extensive experimental results on five real-world\ndatasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Mengge Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information. (arXiv:2203.07893v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07893","description":"<p>We describe a simple and effective method (Spectral Attribute removaL; SAL)\nto remove private or guarded information from neural representations. Our\nmethod uses matrix decomposition to project the input representations into\ndirections with reduced covariance with the guarded information rather than\nmaximal covariance as factorization methods normally use. We begin with linear\ninformation removal and proceed to generalize our algorithm to the case of\nnonlinear information removal using kernels. Our experiments demonstrate that\nour algorithm retains better main task performance after removing the guarded\ninformation compared to previous work. In addition, our experiments demonstrate\nthat we need a relatively small amount of guarded attribute data to remove\ninformation about these attributes, which lowers the exposure to sensitive data\nand is more suitable for low-resource scenarios. Code is available at\nhttps://github.com/jasonshaoshun/SAL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Shun Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziser_Y/0/1/0/all/0/1\">Yftah Ziser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay B. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks. (arXiv:2204.02892v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02892","description":"<p>The field of Natural Language Processing has experienced a dramatic leap in\ncapabilities with the recent introduction of huge Language Models. Despite this\nsuccess, natural language problems that involve several compounded steps are\nstill practically unlearnable, even by the largest LMs. This complies with\nexperimental failures for end-to-end learning of composite problems that were\ndemonstrated in a variety of domains. An effective mitigation is to introduce\nintermediate supervision for solving sub-tasks of the compounded problem.\nRecently, several works have demonstrated high gains by taking a\nstraightforward approach for incorporating intermediate supervision in\ncompounded natural language problems: the sequence-to-sequence LM is fed with\nan augmented input, in which the decomposed tasks' labels are simply\nconcatenated to the original input. In this paper, we prove a positive learning\nresult that motivates these recent efforts. We show that when concatenating\nintermediate supervision to the input and training a sequence-to-sequence model\non this modified input, unlearnable composite problems can become learnable. We\nshow that this is true for any family of tasks which on the one hand, are\nunlearnable, and on the other hand, can be decomposed into a polynomial number\nof simple sub-tasks, each of which depends only on O(1) previous sub-task\nresults. Beyond motivating contemporary empirical efforts for incorporating\nintermediate supervision in sequence-to-sequence language models, our positive\ntheoretical result is the first of its kind in the landscape of results on the\nbenefits of intermediate supervision for neural-network learning: Until now,\nall theoretical results on the subject are negative, i.e., show cases where\nlearning is impossible without intermediate supervision, while our result is\npositive, showing that learning is facilitated in the presence of intermediate\nsupervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1\">Noam Wies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Multi-task Learning in Natural Language Processing: Regarding Task Relatedness and Training Methods. (arXiv:2204.03508v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03508","description":"<p>Multi-task learning (MTL) has become increasingly popular in natural language\nprocessing (NLP) because it improves the performance of related tasks by\nexploiting their commonalities and differences. Nevertheless, it is still not\nunderstood very well how multi-task learning can be implemented based on the\nrelatedness of training tasks. In this survey, we review recent advances of\nmulti-task learning methods in NLP, with the aim of summarizing them into two\ngeneral multi-task training methods based on their task relatedness: (i) joint\ntraining and (ii) multi-step training. We present examples in various NLP\ndownstream applications, summarize the task relationships and discuss future\ndirections of this promising topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mengxia Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhichun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Holistic Approach to Undesired Content Detection in the Real World. (arXiv:2208.03274v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.03274","description":"<p>We present a holistic approach to building a robust and useful natural\nlanguage classification system for real-world content moderation. The success\nof such a system relies on a chain of carefully designed and executed steps,\nincluding the design of content taxonomies and labeling instructions, data\nquality control, an active learning pipeline to capture rare events, and a\nvariety of methods to make the model robust and to avoid overfitting. Our\nmoderation system is trained to detect a broad set of categories of undesired\ncontent, including sexual content, hateful content, violence, self-harm, and\nharassment. This approach generalizes to a wide range of different content\ntaxonomies and can be used to create high-quality content classifiers that\noutperform off-the-shelf models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Markov_T/0/1/0/all/0/1\">Todor Markov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sandhini Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eloundou_T/0/1/0/all/0/1\">Tyna Eloundou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Teddy Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_S/0/1/0/all/0/1\">Steven Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Angela Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_L/0/1/0/all/0/1\">Lilian Weng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PANCETTA: Phoneme Aware Neural Completion to Elicit Tongue Twisters Automatically. (arXiv:2209.06275v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.06275","description":"<p>Tongue twisters are meaningful sentences that are difficult to pronounce. The\nprocess of automatically generating tongue twisters is challenging since the\ngenerated utterance must satisfy two conditions at once: phonetic difficulty\nand semantic meaning. Furthermore, phonetic difficulty is itself hard to\ncharacterize and is expressed in natural tongue twisters through a\nheterogeneous mix of phenomena such as alliteration and homophony. In this\npaper, we propose PANCETTA: Phoneme Aware Neural Completion to Elicit Tongue\nTwisters Automatically. We leverage phoneme representations to capture the\nnotion of phonetic difficulty, and we train language models to generate\noriginal tongue twisters on two proposed task settings. To do this, we curate a\ndataset called PANCETTA, consisting of existing English tongue twisters.\nThrough automatic and human evaluation, as well as qualitative analysis, we\nshow that PANCETTA generates novel, phonetically difficult, fluent, and\nsemantically meaningful tongue twisters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keh_S/0/1/0/all/0/1\">Sedrick Scott Keh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction and Applications of Billion-Scale Pre-trained Multimodal Business Knowledge Graph. (arXiv:2209.15214v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2209.15214","description":"<p>Business Knowledge Graphs (KGs) are important to many enterprises today,\nproviding factual knowledge and structured data that steer many products and\nmake them more intelligent. Despite their promising benefits, building business\nKG necessitates solving prohibitive issues of deficient structure and multiple\nmodalities. In this paper, we advance the understanding of the practical\nchallenges related to building KG in non-trivial real-world systems. We\nintroduce the process of building an open business knowledge graph (OpenBG)\nderived from a well-known enterprise, Alibaba Group. Specifically, we define a\ncore ontology to cover various abstract products and consumption demands, with\nfine-grained taxonomy and multimodal facts in deployed applications. OpenBG is\nan open business KG of unprecedented scale: 2.6 billion triples with more than\n88 million entities covering over 1 million core classes/concepts and 2,681\ntypes of relations. We release all the open resources (OpenBG benchmarks)\nderived from it for the community and report experimental results of KG-centric\ntasks. We also run up an online competition based on OpenBG benchmarks, and has\nattracted thousands of teams. We further pre-train OpenBG and apply it to many\nKG- enhanced downstream tasks in business scenarios, demonstrating the\neffectiveness of billion-scale multimodal knowledge for e-commerce. All the\nresources with codes have been released at\n\\url{https://github.com/OpenBGBenchmark/OpenBG}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zelin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hehong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Generalisation with Structured Reordering and Fertility Layers. (arXiv:2210.03183v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03183","description":"<p>Seq2seq models have been shown to struggle with compositional generalisation,\ni.e. generalising to new and potentially more complex structures than seen\nduring training. Taking inspiration from grammar-based models that excel at\ncompositional generalisation, we present a flexible end-to-end differentiable\nneural model that composes two structural operations: a fertility step, which\nwe introduce in this work, and a reordering step based on previous work (Wang\net al., 2021). To ensure differentiability, we use the expected value of each\nstep. Our model outperforms seq2seq models by a wide margin on challenging\ncompositional splits of realistic semantic parsing tasks that require\ngeneralisation to longer examples. It also compares favourably to other models\ntargeting compositional generalisation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lindemann_M/0/1/0/all/0/1\">Matthias Lindemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koller_A/0/1/0/all/0/1\">Alexander Koller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualize Before You Write: Imagination-Guided Open-Ended Text Generation. (arXiv:2210.03765v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03765","description":"<p>Recent advances in text-to-image synthesis make it possible to visualize\nmachine imaginations for a given context. On the other hand, when generating\ntext, human writers are gifted at creative visualization, which enhances their\nwritings by forming imaginations as blueprints before putting down the stories\nin words. Inspired by such a cognitive process, we ask the natural question of\nwhether we can endow machines with the same ability to utilize visual\ninformation and construct a general picture of the context to guide text\ngeneration. In this work, we propose iNLG that uses machine-generated images to\nguide language models in open-ended text generation. The experiments and\nanalyses demonstrate the effectiveness of iNLG on open-ended text generation\ntasks, including text completion, story generation, and concept-to-text\ngeneration in both few-shot and full-data scenarios. Both automatic metrics and\nhuman evaluations verify that the text snippets generated by our iNLG are\ncoherent and informative while displaying minor degeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1\">An Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting GPT-3 To Be Reliable. (arXiv:2210.09150v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09150","description":"<p>Large language models (LLMs) show impressive abilities via few-shot\nprompting. Commercialized APIs such as OpenAI GPT-3 further increase their use\nin real-world language applications. However, the crucial problem of how to\nimprove the reliability of GPT-3 is still under-explored. While reliability is\na broad and vaguely defined term, we decompose reliability into four main\nfacets that correspond to the existing framework of ML safety and are\nwell-recognized to be important: generalizability, social biases, calibration,\nand factuality. Our core contribution is to establish simple and effective\nprompts that improve GPT-3's reliability as it: 1) generalizes\nout-of-distribution, 2) balances demographic distribution and uses natural\nlanguage instructions to reduce social biases, 3) calibrates output\nprobabilities, and 4) updates the LLM's factual knowledge and reasoning chains.\nWith appropriate prompts, GPT-3 is more reliable than smaller-scale supervised\nmodels on all these facets. We release all processed datasets, evaluation\nscripts, and model predictions. Our systematic empirical study not only sheds\nnew insights on the reliability of prompting LLMs, but more importantly, our\nprompting strategies can help practitioners more reliably use LLMs like GPT-3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenglei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoKE: Prior Knowledge Enhanced Emotional Support Conversation with Latent Variable. (arXiv:2210.12640v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12640","description":"<p>Emotional support conversation (ESC) task can utilize various support\nstrategies to help people relieve emotional distress and overcome the problem\nthey face, which has attracted much attention in these years. However, most\nstate-of-the-art works rely heavily on external commonsense knowledge to infer\nthe mental state of the user in every dialogue round. Although effective, they\nmay suffer from significant human effort, knowledge update and domain change in\na long run. Therefore, in this article, we focus on exploring the task itself\nwithout using any external knowledge. We find all existing works ignore two\nsignificant characteristics of ESC. (a) Abundant prior knowledge exists in\nhistorical conversations, such as the responses to similar cases and the\ngeneral order of support strategies, which has a great reference value for\ncurrent conversation. (b) There is a one-to-many mapping relationship between\ncontext and support strategy, i.e.multiple strategies are reasonable for a\nsingle context. It lays a better foundation for the diversity of generations.\nTaking into account these two key factors, we propose Prior Knowledge Enhanced\nemotional support model with latent variable, PoKE. The proposed model fully\ntaps the potential of prior knowledge in terms of exemplars and strategy\nsequence and then utilizes a latent variable to model the one-to-many\nrelationship of strategy. Furthermore, we introduce a memory schema to\nincorporate the encoded knowledge into decoder. Experiment results on benchmark\ndataset show that our PoKE outperforms existing baselines on both automatic\nevaluation and human evaluation. Compared with the model using external\nknowledge, PoKE still can make a slight improvement in some metrics. Further\nexperiments prove that abundant prior knowledge is conducive to high-quality\nemotional support, and a well-learned latent variable is critical to the\ndiversity of generations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaohan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xuying Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yequan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?. (arXiv:2210.14699v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2210.14699","description":"<p>Language models are promising solutions for tackling increasing complex\nproblems. In software engineering, they recently attracted attention in code\nassistants, with programs automatically written in a given programming language\nfrom a programming task description in natural language. They have the\npotential to save time and effort when writing code. However, these systems are\ncurrently poorly understood, preventing them from being used optimally. In this\npaper, we investigate the various input parameters of two language models, and\nconduct a study to understand if variations of these input parameters (e.g.\nprogramming task description and the surrounding context, creativity of the\nlanguage model, number of generated solutions) can have a significant impact on\nthe quality of the generated programs. We design specific operators for varying\ninput parameters and apply them over two code assistants (Copilot and Codex)\nand two benchmarks representing algorithmic problems (HumanEval and LeetCode).\nOur results showed that varying the input parameters can significantly improve\nthe performance of language models. However, there is a tight dependency when\nvarying the temperature, the prompt and the number of generated solutions,\nmaking potentially hard for developers to properly control the parameters to\nobtain an optimal result. This work opens opportunities to propose (automated)\nstrategies for improving performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doderlein_J/0/1/0/all/0/1\">Jean-Baptiste D&#xf6;derlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acher_M/0/1/0/all/0/1\">Mathieu Acher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khelladi_D/0/1/0/all/0/1\">Djamel Eddine Khelladi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Combemale_B/0/1/0/all/0/1\">Benoit Combemale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. (arXiv:2211.10438v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.10438","description":"<p>Large language models (LLMs) show excellent performance but are compute- and\nmemory-intensive. Quantization can reduce memory and accelerate inference.\nHowever, for LLMs beyond 100 billion parameters, existing methods cannot\nmaintain accuracy or do not run efficiently on hardware. We propose\nSmoothQuant, a training-free, accuracy-preserving, and general-purpose\npost-training quantization (PTQ) solution to enable 8-bit weight, 8-bit\nactivation (W8A8) quantization for LLMs. Based on the fact that weights are\neasy to quantize while activations are not, SmoothQuant smooths the activation\noutliers by offline migrating the quantization difficulty from activations to\nweights with a mathematically equivalent transformation. SmoothQuant enables an\nINT8 quantization of both weights and activations for all the matrix\nmultiplications in LLMs, including OPT-175B, BLOOM-176B, GLM-130B, and MT-NLG\n530B. SmoothQuant has better hardware efficiency than existing techniques. We\ndemonstrate up to 1.56x speedup and 2x memory reduction for LLMs with\nnegligible loss in accuracy. We integrate SmoothQuant into FasterTransformer, a\nstate-of-the-art LLM serving framework, and achieve faster inference speed with\nhalf the number of GPUs compared to FP16, enabling the serving of a 530B LLM\nwithin a single node. Our work offers a turn-key solution that reduces hardware\ncosts and democratizes LLMs. Code is available at\nhttps://github.com/mit-han-lab/smoothquant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Guangxuan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Ji Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seznec_M/0/1/0/all/0/1\">Mickael Seznec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demouth_J/0/1/0/all/0/1\">Julien Demouth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraphrase Acquisition from Image Captions. (arXiv:2301.11030v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11030","description":"<p>We propose to use image captions from the Web as a previously underutilized\nresource for paraphrases (i.e., texts with the same \"message\") and to create\nand analyze a corresponding dataset. When an image is reused on the Web, an\noriginal caption is often assigned. We hypothesize that different captions for\nthe same image naturally form a set of mutual paraphrases. To demonstrate the\nsuitability of this idea, we analyze captions in the English Wikipedia, where\neditors frequently relabel the same image for different articles. The paper\nintroduces the underlying mining technology, the resulting Wikipedia-IPC\ndataset, and compares known paraphrase corpora with respect to their syntactic\nand semantic paraphrase similarity to our new resource. In this context, we\nintroduce characteristic maps along the two similarity dimensions to identify\nthe style of paraphrases coming from different sources. An annotation study\ndemonstrates the high reliability of the algorithmically determined\ncharacteristic maps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gohsen_M/0/1/0/all/0/1\">Marcel Gohsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagen_M/0/1/0/all/0/1\">Matthias Hagen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Protein Representation Learning via Knowledge Enhanced Primary Structure Modeling. (arXiv:2301.13154v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2301.13154","description":"<p>Protein representation learning has primarily benefited from the remarkable\ndevelopment of language models (LMs). Accordingly, pre-trained protein models\nalso suffer from a problem in LMs: a lack of factual knowledge. The recent\nsolution models the relationships between protein and associated knowledge\nterms as the knowledge encoding objective. However, it fails to explore the\nrelationships at a more granular level, i.e., the token level. To mitigate\nthis, we propose Knowledge-exploited Auto-encoder for Protein (KeAP), which\nperforms token-level knowledge graph exploration for protein representation\nlearning. In practice, non-masked amino acids iteratively query the associated\nknowledge tokens to extract and integrate helpful information for restoring\nmasked amino acids via attention. We show that KeAP can consistently outperform\nthe previous counterpart on 9 representative downstream applications, sometimes\nsurpassing it by large margins. These results suggest that KeAP provides an\nalternative yet effective way to perform knowledge enhanced protein\nrepresentation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yunxiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1\">Cheng Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing Radiograph Representation Learning with Masked Record Modeling. (arXiv:2301.13155v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2301.13155","description":"<p>Modern studies in radiograph representation learning rely on either\nself-supervision to encode invariant semantics or associated radiology reports\nto incorporate medical expertise, while the complementarity between them is\nbarely noticed. To explore this, we formulate the self- and report-completion\nas two complementary objectives and present a unified framework based on masked\nrecord modeling (MRM). In practice, MRM reconstructs masked image patches and\nmasked report tokens following a multi-task scheme to learn knowledge-enhanced\nsemantic representations. With MRM pre-training, we obtain pre-trained models\nthat can be well transferred to various radiography tasks. Specifically, we\nfind that MRM offers superior performance in label-efficient fine-tuning. For\ninstance, MRM achieves 88.5% mean AUC on CheXpert using 1% labeled data,\noutperforming previous R$^2$L methods with 100% labels. On NIH ChestX-ray, MRM\noutperforms the best performing counterpart by about 3% under small labeling\nratios. Besides, MRM surpasses self- and report-supervised pre-training in\nidentifying the pneumonia type and the pneumothorax area, sometimes by large\nmargins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_C/0/1/0/all/0/1\">Chenyu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nationality Bias in Text Generation. (arXiv:2302.02463v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.02463","description":"<p>Little attention is placed on analyzing nationality bias in language models,\nespecially when nationality is highly used as a factor in increasing the\nperformance of social NLP models. This paper examines how a text generation\nmodel, GPT-2, accentuates pre-existing societal biases about country-based\ndemonyms. We generate stories using GPT-2 for various nationalities and use\nsensitivity analysis to explore how the number of internet users and the\ncountry's economic status impacts the sentiment of the stories. To reduce the\npropagation of biases through large language models (LLM), we explore the\ndebiasing method of adversarial triggering. Our results show that GPT-2\ndemonstrates significant bias against countries with lower internet users, and\nadversarial triggering effectively reduces the same.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1\">Pranav Narayanan Venkit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_S/0/1/0/all/0/1\">Sanjana Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchanadikar_R/0/1/0/all/0/1\">Ruchi Panchanadikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1\">Shomir Wilson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAC: A unified framework boosting low resource automatic speech recognition. (arXiv:2302.03498v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03498","description":"<p>We propose a unified framework for low resource automatic speech recognition\ntasks named meta audio concatenation (MAC). It is easy to implement and can be\ncarried out in extremely low resource environments. Mathematically, we give a\nclear description of MAC framework from the perspective of bayesian sampling.\nIn this framework, we leverage a novel concatenative synthesis text-to-speech\nsystem to boost the low resource ASR task. By the concatenative synthesis\ntext-to-speech system, we can integrate language pronunciation rules and adjust\nthe TTS process. Furthermore, we propose a broad notion of meta audio set to\nmeet the modeling needs of different languages and different scenes when using\nthe system. Extensive experiments have demonstrated the great effectiveness of\nMAC on low resource ASR tasks. For CTC greedy search, CTC prefix, attention,\nand attention rescoring decode mode in Cantonese ASR task, Taiwanese ASR task,\nand Japanese ASR task the MAC method can reduce the CER by more than 15\\%.\nFurthermore, in the ASR task, MAC beats wav2vec2 (with fine-tuning) on common\nvoice datasets of Cantonese and gets really competitive results on common voice\ndatasets of Taiwanese and Japanese. Among them, it is worth mentioning that we\nachieve a \\textbf{10.9\\%} character error rate (CER) on the common voice\nCantonese ASR task, bringing about \\textbf{30\\%} relative improvement compared\nto the wav2vec2 (with fine-tuning).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_Z/0/1/0/all/0/1\">Zeping Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Q/0/1/0/all/0/1\">Qian Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+E_W/0/1/0/all/0/1\">Weinan E</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models. (arXiv:2302.05578v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.05578","description":"<p>Despite recent progress, it has been difficult to prevent semantic\nhallucinations in generative Large Language Models. One common solution to this\nis augmenting LLMs with a retrieval system and making sure that the generated\noutput is attributable to the retrieved information. Given this new added\nconstraint, it is plausible to expect that the overall quality of the output\nwill be affected, for example, in terms of fluency. Can scaling language models\nhelp?\n</p>\n<p>Here we examine the relationship between fluency and attribution in LLMs\nprompted with retrieved evidence in knowledge-heavy dialog settings. Our\nexperiments were implemented with a set of auto-metrics that are aligned with\nhuman preferences. They were used to evaluate a large set of generations,\nproduced under varying parameters of LLMs and supplied context.\n</p>\n<p>We show that larger models tend to do much better in both fluency and\nattribution, and that (naively) using top-k retrieval versus top-1 retrieval\nimproves attribution but hurts fluency. We next propose a recipe that could\nallow smaller models to both close the gap with larger models and preserve the\nbenefits of top-k retrieval while avoiding its drawbacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aksitov_R/0/1/0/all/0/1\">Renat Aksitov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chung-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1\">David Reitter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1\">Siamak Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yunhsuan Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a General-Purpose Natural Language Processing Task Solver?. (arXiv:2302.06476v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.06476","description":"<p>Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated the ability to perform a variety of natural language processing\n(NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently,\nthe debut of ChatGPT has drawn a great deal of attention from the natural\nlanguage processing (NLP) community due to the fact that it can generate\nhigh-quality responses to human input and self-correct previous mistakes based\non subsequent conversations. However, it is not yet known whether ChatGPT can\nserve as a generalist model that can perform many NLP tasks zero-shot. In this\nwork, we empirically analyze the zero-shot learning ability of ChatGPT by\nevaluating it on 20 popular NLP datasets covering 7 representative task\ncategories. With extensive empirical studies, we demonstrate both the\neffectiveness and limitations of the current version of ChatGPT. We find that\nChatGPT performs well on many tasks favoring reasoning capabilities (e.g.,\narithmetic reasoning) while it still faces challenges when solving specific\ntasks such as sequence tagging. We additionally provide in-depth analysis\nthrough qualitative case studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chengwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning Model Attribution Challenge. (arXiv:2302.06716v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.06716","description":"<p>We present the findings of the Machine Learning Model Attribution Challenge\nhttps://mlmac.io. Fine-tuned machine learning models may derive from other\ntrained models without obvious attribution characteristics. In this challenge,\nparticipants identify the publicly-available base models that underlie a set of\nanonymous, fine-tuned large language models (LLMs) using only textual output of\nthe models. Contestants aim to correctly attribute the most fine-tuned models,\nwith ties broken in the favor of contestants whose solutions use fewer calls to\nthe fine-tuned models' API. The most successful approaches were manual, as\nparticipants observed similarities between model outputs and developed\nattribution heuristics based on public documentation of the base models, though\nseveral teams also submitted automated, statistical solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merkhofer_E/0/1/0/all/0/1\">Elizabeth Merkhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_D/0/1/0/all/0/1\">Deepesh Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_H/0/1/0/all/0/1\">Hyrum S. Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manville_K/0/1/0/all/0/1\">Keith Manville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1\">Lily Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gante_J/0/1/0/all/0/1\">Jo&#xe3;o Gante</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}