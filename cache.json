{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-16T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Hope Speech Detection on Social Media Platforms. (arXiv:2212.07424v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07424","description":"<p>Since personal computers became widely available in the consumer market, the\namount of harmful content on the internet has significantly expanded. In simple\nterms, harmful content is anything online which causes a person distress or\nharm. It may include hate speech, violent content, threats, non-hope speech,\netc. The online content must be positive, uplifting and supportive. Over the\npast few years, many studies have focused on solving this problem through hate\nspeech detection, but very few focused on identifying hope speech. This paper\ndiscusses various machine learning approaches to identify a sentence as Hope\nSpeech, Non-Hope Speech, or a Neutral sentence. The dataset used in the study\ncontains English YouTube comments and is released as a part of the shared task\n\"EACL-2021: Hope Speech Detection for Equality, Diversity, and Inclusion\".\nInitially, the dataset obtained from the shared task had three classes: Hope\nSpeech, non-Hope speech, and not in English; however, upon deeper inspection,\nwe discovered that dataset relabeling is required. A group of undergraduates\nwas hired to help perform the entire dataset's relabeling task. We experimented\nwith conventional machine learning models (such as Na\\\"ive Bayes, logistic\nregression and support vector machine) and pre-trained models (such as BERT) on\nrelabeled data. According to the experimental results, the relabeled data has\nachieved a better accuracy for Hope speech identification than the original\ndata set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_P/0/1/0/all/0/1\">Pranjal Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandana_P/0/1/0/all/0/1\">Pasupuleti Chandana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemade_J/0/1/0/all/0/1\">Jagrut Nemade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shubham Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saumya_S/0/1/0/all/0/1\">Sunil Saumya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biradar_S/0/1/0/all/0/1\">Shankar Biradar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments. (arXiv:2212.07425v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07425","description":"<p>The spread of misinformation, propaganda, and flawed argumentation has been\namplified in the Internet era. Given the volume of data and the subtlety of\nidentifying violations of argumentation norms, supporting information analytics\ntasks, like content moderation, with trustworthy methods that can identify\nlogical fallacies is essential. In this paper, we formalize prior theoretical\nwork on logical fallacies into a comprehensive three-stage evaluation framework\nof detection, coarse-grained, and fine-grained classification. We adapt\nexisting evaluation datasets for each stage of the evaluation. We devise three\nfamilies of robust and explainable methods based on prototype reasoning,\ninstance-based reasoning, and knowledge injection. The methods are designed to\ncombine language models with background knowledge and explainable mechanisms.\nMoreover, we address data sparsity with strategies for data augmentation and\ncurriculum learning. Our three-stage framework natively consolidates prior\ndatasets and methods from existing tasks, like propaganda detection, serving as\nan overarching evaluation testbed. We extensively evaluate these methods on our\ndatasets, focusing on their robustness and explainability. Our results provide\ninsight into the strengths and weaknesses of the methods on different\ncomponents and fallacy classes, indicating that fallacy identification is a\nchallenging task that may require specialized forms of reasoning to capture\nvarious classes. We share our open-source code and data on GitHub to support\nfurther work on logical fallacy identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sourati_Z/0/1/0/all/0/1\">Zhivar Sourati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_V/0/1/0/all/0/1\">Vishnu Priya Prasanna Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_D/0/1/0/all/0/1\">Darshan Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawlani_H/0/1/0/all/0/1\">Himanshu Rawlani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandlin_H/0/1/0/all/0/1\">H&#xf4;ng-&#xc2;n Sandlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mermoud_A/0/1/0/all/0/1\">Alain Mermoud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Linguistically Informed Multi-Objective Pre-Training for Natural Language Inference. (arXiv:2212.07428v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07428","description":"<p>We introduce a linguistically enhanced combination of pre-training methods\nfor transformers. The pre-training objectives include POS-tagging, synset\nprediction based on semantic knowledge graphs, and parent prediction based on\ndependency parse trees. Our approach achieves competitive results on the\nNatural Language Inference task, compared to the state of the art. Specifically\nfor smaller models, the method results in a significant performance boost,\nemphasizing the fact that intelligent pre-training can make up for fewer\nparameters and help building more efficient models. Combining POS-tagging and\nsynset prediction yields the overall best results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pielka_M/0/1/0/all/0/1\">Maren Pielka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_S/0/1/0/all/0/1\">Svetlana Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pucknat_L/0/1/0/all/0/1\">Lisa Pucknat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifa_R/0/1/0/all/0/1\">Rafet Sifa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Multilingual Corpora for a Complex Named Entity Recognition and Classification Hierarchy using Wikipedia and DBpedia. (arXiv:2212.07429v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07429","description":"<p>With the ever-growing popularity of the field of NLP, the demand for datasets\nin low resourced-languages follows suit. Following a previously established\nframework, in this paper, we present the UNER dataset, a multilingual and\nhierarchical parallel corpus annotated for named-entities. We describe in\ndetail the developed procedure necessary to create this type of dataset in any\nlanguage available on Wikipedia with DBpedia information. The three-step\nprocedure extracts entities from Wikipedia articles, links them to DBpedia, and\nmaps the DBpedia sets of classes to the UNER labels. This is followed by a\npost-processing procedure that significantly increases the number of identified\nentities in the final results. The paper concludes with a statistical and\nqualitative analysis of the resulting dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alves_D/0/1/0/all/0/1\">Diego Alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_G/0/1/0/all/0/1\">Gaurish Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amaral_G/0/1/0/all/0/1\">Gabriel Amaral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuculo_T/0/1/0/all/0/1\">Tin Kuculo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadic_M/0/1/0/all/0/1\">Marko Tadi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Infinite Index: Information Retrieval on Generative Text-To-Image Models. (arXiv:2212.07476v1 [cs.IR])","link":"http://arxiv.org/abs/2212.07476","description":"<p>The text-to-image model Stable Diffusion has recently become very popular.\nOnly weeks after its open source release, millions are experimenting with image\ngeneration. This is due to its ease of use, since all it takes is a brief\ndescription of the desired image to \"prompt\" the generative model. Rarely do\nthe images generated for a new prompt immediately meet the user's expectations.\nUsually, an iterative refinement of the prompt (\"prompt engineering\") is\nnecessary for satisfying images. As a new perspective, we recast image prompt\nengineering as interactive image retrieval - on an \"infinite index\". Thereby, a\nprompt corresponds to a query and prompt engineering to query refinement.\nSelected image-prompt pairs allow direct relevance feedback, as the model can\nmodify an image for the refined prompt. This is a form of one-sided interactive\nretrieval, where the initiative is on the user side, whereas the server side\nremains stateless. In light of an extensive literature review, we develop these\nparallels in detail and apply the findings to a case study of a creative search\ntask on such a model. We note that the uncertainty in searching an infinite\nindex is virtually never-ending. We also discuss future research opportunities\nrelated to retrieval models specialized for generative models and interactive\ngenerative image retrieval. The application of IR technology, such as query\nreformulation and relevance feedback, will contribute to improved workflows\nwhen using generative models, while the notion of an infinite index raises new\nchallenges in IR research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deckers_N/0/1/0/all/0/1\">Niklas Deckers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frobe_M/0/1/0/all/0/1\">Maik Fr&#xf6;be</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiesel_J/0/1/0/all/0/1\">Johannes Kiesel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandolfo_G/0/1/0/all/0/1\">Gianluca Pandolfo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Intelligence for Health Message Generation: Theory, Method, and an Empirical Study Using Prompt Engineering. (arXiv:2212.07507v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07507","description":"<p>This study introduces and examines the potential of an AI system to generate\nhealth awareness messages. The topic of folic acid, a vitamin that is critical\nduring pregnancy, served as a test case. Using prompt engineering, we generated\nmessages that could be used to raise awareness and compared them to retweeted\nhuman-generated messages via computational and human evaluation methods. The\nsystem was easy to use and prolific, and computational analyses revealed that\nthe AI-generated messages were on par with human-generated ones in terms of\nsentiment, reading ease, and semantic content. Also, the human evaluation study\nshowed that AI-generated messages ranked higher in message quality and clarity.\nWe discuss the theoretical, practical, and ethical implications of these\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Sue Lim</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Schmalzle_R/0/1/0/all/0/1\">Ralf Schm&#xe4;lzle</a> (1) ((1) Michigan State University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language. (arXiv:2212.07525v1 [cs.LG])","link":"http://arxiv.org/abs/2212.07525","description":"<p>Current self-supervised learning algorithms are often modality-specific and\nrequire large amounts of computational resources. To address these issues, we\nincrease the training efficiency of data2vec, a learning objective that\ngeneralizes across several modalities. We do not encode masked tokens, use a\nfast convolutional decoder and amortize the effort to build teacher\nrepresentations. data2vec 2.0 benefits from the rich contextualized target\nrepresentations introduced in data2vec which enable a fast self-supervised\nlearner. Experiments on ImageNet-1K image classification show that data2vec 2.0\nmatches the accuracy of Masked Autoencoders in 16.4x lower pre-training time,\non Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x\nless time, and on GLUE natural language understanding it matches a retrained\nRoBERTa model in half the time. Trading some speed for accuracy results in\nImageNet-1K top-1 accuracy of 86.8\\% with a ViT-L model trained for 150 epochs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Arun Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relationship Between Online Harmful Behaviors and Social Network Message Writing Style. (arXiv:2212.07526v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07526","description":"<p>In this paper, we explore the relationship between an individual's writing\nstyle and the risk that they will engage in online harmful behaviors (such as\ncyberbullying). In particular, we consider whether measurable differences in\nwriting style relate to different personality types, as modeled by the Big-Five\npersonality traits and the Dark Triad traits, and can differentiate between\nusers who do or do not engage in harmful behaviors. We study messages from\nnearly 2,500 users from two online communities (Twitter and Reddit) and find\nthat we can measure significant personality differences between regular and\nharmful users from the writing style of as few as 100 tweets or 40 Reddit\nposts, aggregate these values to distinguish between healthy and harmful\ncommunities, and also use style attributes to predict which users will engage\nin harmful behaviors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Viera_T/0/1/0/all/0/1\">Talia Sanchez Viera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoury_R/0/1/0/all/0/1\">Richard Khoury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causes and Cures for Interference in Multilingual Translation. (arXiv:2212.07530v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07530","description":"<p>Multilingual machine translation models can benefit from synergy between\ndifferent language pairs, but also suffer from interference. While there is a\ngrowing number of sophisticated methods that aim to eliminate interference, our\nunderstanding of interference as a phenomenon is still limited. This work\nidentifies the main factors that contribute to interference in multilingual\nmachine translation. Through systematic experimentation, we find that\ninterference (or synergy) are primarily determined by model size, data size,\nand the proportion of each language pair within the total dataset. We observe\nthat substantial interference occurs mainly when the model is very small with\nrespect to the available training data, and that using standard transformer\nconfigurations with less than one billion parameters largely alleviates\ninterference and promotes synergy. Moreover, we show that tuning the sampling\ntemperature to control the proportion of each language pair in the data is key\nto balancing the amount of interference between low and high resource language\npairs effectively, and can lead to superior performance overall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1\">Uri Shaham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elbayad_M/0/1/0/all/0/1\">Maha Elbayad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1\">Vedanuj Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Natural Language Processing to Augment Structured Social Determinants of Health Data in the Electronic Health Record. (arXiv:2212.07538v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07538","description":"<p>Objective: Social Determinants of Health (SDOH) influence personal health\noutcomes and health systems interactions. Health systems capture SDOH\ninformation through structured data and unstructured clinical notes; however,\nclinical notes often contain a more comprehensive representation of several key\nSDOH. The objective of this work is to assess the SDOH information gain\nachievable by extracting structured semantic representations of SDOH from the\nclinical narrative and combining these extracted representations with available\nstructured data.\n</p>\n<p>Materials and Methods: We developed a natural language processing (NLP)\ninformation extraction model for SDOH that utilizes a deep learning entity and\nrelation extraction architecture. In an electronic health record (EHR) case\nstudy, we applied the SDOH extractor to a large existing clinical data set with\nover 200,000 patients and 400,000 notes and compared the extracted information\nwith available structured data.\n</p>\n<p>Results: The SDOH extractor achieved 0.86 F1 on a withheld test set. In the\nEHR case study, we found 19\\% of current tobacco users, 10\\% of drug users, and\n32\\% of homeless patients only include documentation of these risk factors in\nthe clinical narrative.\n</p>\n<p>Conclusions: Patients who are at-risk for negative health outcomes due to\nSDOH may be better served if health systems are able to identify SDOH risk\nfactors and associated social needs. Structured semantic representations of\ntext-encoded SDOH information can augment existing structured, and this more\ncomprehensive SDOH representation can assist health systems in identifying and\naddressing social needs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lybarger_K/0/1/0/all/0/1\">Kevin Lybarger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobbins_N/0/1/0/all/0/1\">Nicholas J Dobbins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_R/0/1/0/all/0/1\">Ritche Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Angad Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wedgeworth_P/0/1/0/all/0/1\">Patrick Wedgeworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozuner_O/0/1/0/all/0/1\">Ozlem Ozuner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Build-a-Bot: Teaching Conversational AI Using a Transformer-Based Intent Recognition and Question Answering Architecture. (arXiv:2212.07542v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07542","description":"<p>As artificial intelligence (AI) becomes a prominent part of modern life, AI\nliteracy is becoming important for all citizens, not just those in technology\ncareers. Previous research in AI education materials has largely focused on the\nintroduction of terminology as well as AI use cases and ethics, but few allow\nstudents to learn by creating their own machine learning models. Therefore,\nthere is a need for enriching AI educational tools with more adaptable and\nflexible platforms for interested educators with any level of technical\nexperience to utilize within their teaching material. As such, we propose the\ndevelopment of an open-source tool (Build-a-Bot) for students and teachers to\nnot only create their own transformer-based chatbots based on their own course\nmaterial, but also learn the fundamentals of AI through the model creation\nprocess. The primary concern of this paper is the creation of an interface for\nstudents to learn the principles of artificial intelligence by using a natural\nlanguage pipeline to train a customized model to answer questions based on\ntheir own school curriculums. The model uses contexts given by their\ninstructor, such as chapters of a textbook, to answer questions and is deployed\non an interactive chatbot/voice agent. The pipeline teaches students data\ncollection, data augmentation, intent recognition, and question answering by\nhaving them work through each of these processes while creating their AI agent,\ndiverging from previous chatbot work where students and teachers use the bots\nas black-boxes with no abilities for customization or the bots lack AI\ncapabilities, with the majority of dialogue scripts being rule-based. In\naddition, our tool is designed to make each step of this pipeline intuitive for\nstudents at a middle-school level. Further work primarily lies in providing our\ntool to schools and seeking student and teacher evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pearce_K/0/1/0/all/0/1\">Kate Pearce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alghowinem_S/0/1/0/all/0/1\">Sharifa Alghowinem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breazeal_C/0/1/0/all/0/1\">Cynthia Breazeal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Detection of Contextualized Embedding Bias with Application to Ideology. (arXiv:2212.07547v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07547","description":"<p>We propose a fully unsupervised method to detect bias in contextualized\nembeddings. The method leverages the assortative information latently encoded\nby social networks and combines orthogonality regularization, structured\nsparsity learning, and graph neural networks to find the embedding subspace\ncapturing this information. As a concrete example, we focus on the phenomenon\nof ideological bias: we introduce the concept of an ideological subspace, show\nhow it can be found by applying our method to online discussion forums, and\npresent techniques to probe it. Our experiments suggest that the ideological\nsubspace encodes abstract evaluative semantics and reflects changes in the\npolitical left-right spectrum during the presidency of Donald Trump.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_V/0/1/0/all/0/1\">Valentin Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1\">Janet B. Pierrehumbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReDDIT: Regret Detection and Domain Identification from Text. (arXiv:2212.07549v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07549","description":"<p>In this paper, we present a study of regret and its expression on social\nmedia platforms. Specifically, we present a novel dataset of Reddit texts that\nhave been classified into three classes: Regret by Action, Regret by Inaction,\nand No Regret. We then use this dataset to investigate the language used to\nexpress regret on Reddit and to identify the domains of text that are most\ncommonly associated with regret. Our findings show that Reddit users are most\nlikely to express regret for past actions, particularly in the domain of\nrelationships. We also found that deep learning models using GloVe embedding\noutperformed other models in all experiments, indicating the effectiveness of\nGloVe for representing the meaning and context of words in the domain of\nregret. Overall, our study provides valuable insights into the nature and\nprevalence of regret on social media, as well as the potential of deep learning\nand word embeddings for analyzing and understanding emotional language in\nonline text. These findings have implications for the development of natural\nlanguage processing algorithms and the design of social media platforms that\nsupport emotional expression and communication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balouchzahi_F/0/1/0/all/0/1\">Fazlourrahman Balouchzahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butt_S/0/1/0/all/0/1\">Sabur Butt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidorov_G/0/1/0/all/0/1\">Grigori Sidorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fixing MoE Over-Fitting on Low-Resource Languages in Multilingual Machine Translation. (arXiv:2212.07571v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07571","description":"<p>Sparsely gated Mixture of Experts (MoE) models have been shown to be a\ncompute-efficient method to scale model capacity for multilingual machine\ntranslation. However, for low-resource tasks, MoE models severely over-fit. We\nshow effective regularization strategies, namely dropout techniques for MoE\nlayers in EOM and FOM, Conditional MoE Routing and Curriculum Learning methods\nthat prevent over-fitting and improve the performance of MoE models on\nlow-resource tasks without adversely affecting high-resource tasks. On a\nmassively multilingual machine translation benchmark, our strategies result in\nabout +1 chrF++ improvement in very low resource language pairs. We perform an\nextensive analysis of the learned MoE routing to better understand the impact\nof our regularization methods and how we can improve them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elbayad_M/0/1/0/all/0/1\">Maha Elbayad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Anna Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking. (arXiv:2212.07617v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07617","description":"<p>Masked language modeling (MLM) has been widely used for pre-training\neffective bidirectional representations, but incurs substantial training costs.\nIn this paper, we propose a novel concept-based curriculum masking (CCM) method\nto efficiently pre-train a language model. CCM has two key differences from\nexisting curriculum learning approaches to effectively reflect the nature of\nMLM. First, we introduce a carefully-designed linguistic difficulty criterion\nthat evaluates the MLM difficulty of each token. Second, we construct a\ncurriculum that gradually masks words related to the previously masked words by\nretrieving a knowledge graph. Experimental results show that CCM significantly\nimproves pre-training efficiency. Specifically, the model trained with CCM\nshows comparative performance with the original BERT on the General Language\nUnderstanding Evaluation benchmark at half of the training cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mingyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jun-Hyung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kang-Min Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">SangKeun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient-based Intra-attention Pruning on Pre-trained Language Models. (arXiv:2212.07634v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07634","description":"<p>Pre-trained language models achieve superior performance, but they are\ncomputationally expensive due to their large size. Techniques such as pruning\nand knowledge distillation (KD) have been developed to reduce their size and\nlatency. In most structural pruning methods, the pruning units, such as\nattention heads and feed-forward hidden dimensions, only span a small model\nstructure space and limit the structures that the pruning algorithm can\nexplore. In this work, we propose Gradient-based Intra-attention pruning\n(GRAIN), which inspects fine intra-attention structures, and allows different\nheads to have different sizes. Intra-attention pruning greatly expands the\nsearching space of model structures and yields highly heterogeneous structures.\nWe further propose structure regularization to encourage generating more\nregular structures, which achieves higher speedups than heterogeneous ones. We\nalso integrate KD into the pruning process with a gradient separation strategy\nto reduce the interference of KD with the pruning process. GRAIN is evaluated\non a variety of tasks. Results show that it notably outperforms other methods\nat the same or similar model size. Even under extreme compression where only\n$3\\%$ weights in transformers remain, the pruned model is still competitive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improve Text Classification Accuracy with Intent Information. (arXiv:2212.07649v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07649","description":"<p>Text classification, a core component of task-oriented dialogue systems,\nattracts continuous research from both the research and industry community, and\nhas resulted in tremendous progress. However, existing method does not consider\nthe use of label information, which may weaken the performance of text\nclassification systems in some token-aware scenarios. To address the problem,\nin this paper, we introduce the use of label information as label embedding for\nthe task of text classification and achieve remarkable performance on benchmark\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yifeng Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Two Losses and Two Datasets Simultaneously to Improve TempoWiC Accuracy. (arXiv:2212.07669v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07669","description":"<p>WSD (Word Sense Disambiguation) is the task of identifying which sense of a\nword is meant in a sentence or other segment of text. Researchers have worked\non this task (e.g. Pustejovsky, 2002) for years but it's still a challenging\none even for SOTA (state-of-the-art) LMs (language models). The new dataset,\nTempoWiC introduced by Loureiro et al. (2022b) focuses on the fact that words\nchange over time. Their best baseline achieves 70.33% macro-F1. In this work,\nwe use two different losses simultaneously to train RoBERTa-based\nclassification models. We also improve our model by using another similar\ndataset to generalize better. Our best configuration beats their best baseline\nby 4.23% and reaches 74.56% macroF1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pirhadi_M/0/1/0/all/0/1\">Mohammad Javad Pirhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_M/0/1/0/all/0/1\">Motahhare Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eetemadi_S/0/1/0/all/0/1\">Sauleh Eetemadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization. (arXiv:2212.07672v1 [cs.CV])","link":"http://arxiv.org/abs/2212.07672","description":"<p>The goal of multimodal abstractive summarization (MAS) is to produce a\nconcise summary given the multimodal data (text and vision). Existing studies\non MAS mainly focus on how to effectively use the extracted visual features,\nhaving achieved impressive success on the high-resource English dataset.\nHowever, less attention has been paid to the quality of the visual features to\nthe summary, which may limit the model performance especially in the low- and\nzero-resource scenarios. In this paper, we propose to improve the summary\nquality through summary-oriented visual features. To this end, we devise two\nauxiliary tasks including \\emph{vision to summary task} and \\emph{masked image\nmodeling task}. Together with the main summarization task, we optimize the MAS\nmodel via the training objectives of all these tasks. By these means, the MAS\nmodel can be enhanced by capturing the summary-oriented visual features,\nthereby yielding more accurate summaries. Experiments on 44 languages, covering\nmid-high-, low-, and zero-resource scenarios, verify the effectiveness and\nsuperiority of the proposed approach, which achieves state-of-the-art\nperformance under all scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers learn in-context by gradient descent. (arXiv:2212.07677v1 [cs.LG])","link":"http://arxiv.org/abs/2212.07677","description":"<p>Transformers have become the state-of-the-art neural network architecture\nacross numerous domains of machine learning. This is partly due to their\ncelebrated ability to transfer and to learn in-context based on few examples.\nNevertheless, the mechanisms by which Transformers become in-context learners\nare not well understood and remain mostly an intuition. Here, we argue that\ntraining Transformers on auto-regressive tasks can be closely related to\nwell-known gradient-based meta-learning formulations. We start by providing a\nsimple weight construction that shows the equivalence of data transformations\ninduced by 1) a single linear self-attention layer and by 2) gradient-descent\n(GD) on a regression loss. Motivated by that construction, we show empirically\nthat when training self-attention-only Transformers on simple regression tasks\neither the models learned by GD and Transformers show great similarity or,\nremarkably, the weights found by optimization match the construction. Thus we\nshow how trained Transformers implement gradient descent in their forward pass.\nThis allows us, at least in the domain of regression problems, to\nmechanistically understand the inner workings of optimized Transformers that\nlearn in-context. Furthermore, we identify how Transformers surpass plain\ngradient descent by an iterative curvature correction and learn linear models\non deep data representations to solve non-linear regression tasks. Finally, we\ndiscuss intriguing parallels to a mechanism identified to be crucial for\nin-context learning termed induction-head (Olsson et al., 2022) and show how it\ncould be understood as a specific case of in-context learning by gradient\ndescent learning within Transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oswald_J/0/1/0/all/0/1\">Johannes von Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niklasson_E/0/1/0/all/0/1\">Eyvind Niklasson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Randazzo_E/0/1/0/all/0/1\">Ettore Randazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacramento_J/0/1/0/all/0/1\">Jo&#xe3;o Sacramento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordvintsev_A/0/1/0/all/0/1\">Alexander Mordvintsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1\">Andrey Zhmoginov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vladymyrov_M/0/1/0/all/0/1\">Max Vladymyrov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-based Disentanglement with Distant Supervision. (arXiv:2212.07699v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07699","description":"<p>Disentangled representation learning remains challenging as ground truth\nfactors of variation do not naturally exist. To address this, we present\nVocabulary Disentanglement Retrieval~(VDR), a simple yet effective\nretrieval-based disentanglement framework that leverages nature language as\ndistant supervision. Our approach is built upon the widely-used bi-encoder\narchitecture with disentanglement heads and is trained on data-text pairs that\nare readily available on the web or in existing datasets. This makes our\napproach task- and modality-agnostic with potential for a wide range of\ndownstream applications. We conduct experiments on 16 datasets in both\ntext-to-text and cross-modal scenarios and evaluate VDR in a zero-shot setting.\nWith the incorporation of disentanglement heads and a minor increase in\nparameters, VDR achieves significant improvements over the base retriever it is\nbuilt upon, with a 9% higher on NDCG@10 scores in zero-shot text-to-text\nretrieval and an average of 13% higher recall in cross-modal retrieval. In\ncomparison to other baselines, VDR outperforms them in most tasks, while also\nimproving explainability and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiawei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FreCDo: A Large Corpus for French Cross-Domain Dialect Identification. (arXiv:2212.07707v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07707","description":"<p>We present a novel corpus for French dialect identification comprising\n413,522 French text samples collected from public news websites in Belgium,\nCanada, France and Switzerland. To ensure an accurate estimation of the dialect\nidentification performance of models, we designed the corpus to eliminate\npotential biases related to topic, writing style, and publication source. More\nprecisely, the training, validation and test splits are collected from\ndifferent news websites, while searching for different keywords (topics). This\nleads to a French cross-domain (FreCDo) dialect identification task. We conduct\nexperiments with four competitive baselines, a fine-tuned CamemBERT model, an\nXGBoost based on fine-tuned CamemBERT features, a Support Vector Machines (SVM)\nclassifier based on fine-tuned CamemBERT features, and an SVM based on word\nn-grams. Aside from presenting quantitative results, we also make an analysis\nof the most discriminative features learned by CamemBERT. Our corpus is\navailable at https://github.com/MihaelaGaman/FreCDo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaman_M/0/1/0/all/0/1\">Mihaela Gaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chifu_A/0/1/0/all/0/1\">Adrian-Gabriel Chifu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domingues_W/0/1/0/all/0/1\">William Domingues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRIP: Triangular Document-level Pre-training for Multilingual Language Models. (arXiv:2212.07752v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07752","description":"<p>Despite the current success of multilingual pre-training, most prior works\nfocus on leveraging monolingual data or bilingual parallel data and overlooked\nthe value of trilingual parallel data. This paper presents \\textbf{Tri}angular\nDocument-level \\textbf{P}re-training (\\textbf{TRIP}), which is the first in the\nfield to extend the conventional monolingual and bilingual pre-training to a\ntrilingual setting by (i) \\textbf{Grafting} the same documents in two languages\ninto one mixed document, and (ii) predicting the remaining one language as the\nreference translation. Our experiments on document-level MT and cross-lingual\nabstractive summarization show that TRIP brings by up to 3.65 d-BLEU points and\n6.2 ROUGE-L points on three multilingual document-level machine translation\nbenchmarks and one cross-lingual abstractive summarization benchmark, including\nmultiple strong state-of-the-art (SOTA) scores. In-depth analysis indicates\nthat TRIP improves document-level machine translation and captures better\ndocument contexts in at least three characteristics: (i) tense consistency,\n(ii) noun consistency and (iii) conjunction presence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COLA: Improving Conversational Recommender Systems by Collaborative Augmentation. (arXiv:2212.07767v1 [cs.IR])","link":"http://arxiv.org/abs/2212.07767","description":"<p>Conversational recommender systems (CRS) aim to employ natural language\nconversations to suggest suitable products to users. Understanding user\npreferences for prospective items and learning efficient item representations\nare crucial for CRS. Despite various attempts, earlier studies mostly learned\nitem representations based on individual conversations, ignoring item\npopularity embodied among all others. Besides, they still need support in\nefficiently capturing user preferences since the information reflected in a\nsingle conversation is limited. Inspired by collaborative filtering, we propose\na collaborative augmentation (COLA) method to simultaneously improve both item\nrepresentation learning and user preference modeling to address these issues.\nWe construct an interactive user-item graph from all conversations, which\naugments item representations with user-aware information, i.e., item\npopularity. To improve user preference modeling, we retrieve similar\nconversations from the training corpus, where the involved items and attributes\nthat reflect the user's potential interests are used to augment the user\nrepresentation through gate control. Extensive experiments on two benchmark\ndatasets demonstrate the effectiveness of our method. Our code and data are\navailable at https://github.com/DongdingLin/COLA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dongding Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLAM: Selective Clarification for Ambiguous Questions with Large Language Models. (arXiv:2212.07769v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07769","description":"<p>State-of-the-art language models are often accurate on many\nquestion-answering benchmarks with well-defined questions. Yet, in real\nsettings questions are often unanswerable without asking the user for\nclarifying information. We show that current SotA models often do not ask the\nuser for clarification when presented with imprecise questions and instead\nprovide incorrect answers or \"hallucinate\". To address this, we introduce CLAM,\na framework that first uses the model to detect ambiguous questions, and if an\nambiguous question is detected, prompts the model to ask the user for\nclarification. Furthermore, we show how to construct a scalable and\ncost-effective automatic evaluation protocol using an oracle language model\nwith privileged information to provide clarifying information. We show that our\nmethod achieves a 20.15 percentage point accuracy improvement over SotA on a\nnovel ambiguous question-answering answering data set derived from TriviaQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuhn_L/0/1/0/all/0/1\">Lorenz Kuhn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1\">Sebastian Farquhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CREPE: Can Vision-Language Foundation Models Reason Compositionally?. (arXiv:2212.07796v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07796","description":"<p>A fundamental characteristic common to both human vision and natural language\nis their compositional nature. Yet, despite the performance gains contributed\nby large vision and language pretraining, we find that - across 6 architectures\ntrained with 4 algorithms on massive datasets - they exhibit little\ncompositionality. To arrive at this conclusion, we introduce a new\ncompositionality evaluation benchmark CREPE which measures two important\naspects of compositionality identified by cognitive science literature:\nsystematicity and productivity. To measure systematicity, CREPE consists of\nthree test datasets. The three test sets are designed to test models trained on\nthree of the popular training datasets: CC-12M, YFCC-15M, and LAION-400M. They\ncontain 385K, 385K, and 373K image-text pairs and 237K, 210K, and 178K hard\nnegative captions. To test productivity, CREPE contains 17K image-text pairs\nwith nine different complexities plus 246K hard negative captions with atomic,\nswapping, and negation foils. The datasets are generated by repurposing the\nVisual Genome scene graphs and region descriptions and applying handcrafted\ntemplates and GPT-3. For systematicity, we find that model performance\ndecreases consistently when novel compositions dominate the retrieval set, with\nRecall@1 dropping by up to 8%. For productivity, models' retrieval success\ndecays as complexity increases, frequently nearing random chance at high\ncomplexity. These results hold regardless of model and training dataset size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zixian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jerry Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gul_M/0/1/0/all/0/1\">Mustafa Omer Gul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1\">Mona Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_I/0/1/0/all/0/1\">Irena Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utilizing Background Knowledge for Robust Reasoning over Traffic Situations. (arXiv:2212.07798v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07798","description":"<p>Understanding novel situations in the traffic domain requires an intricate\ncombination of domain-specific and causal commonsense knowledge. Prior work has\nprovided sufficient perception-based modalities for traffic monitoring, in this\npaper, we focus on a complementary research aspect of Intelligent\nTransportation: traffic understanding. We scope our study to text-based methods\nand datasets given the abundant commonsense knowledge that can be extracted\nusing language models from large corpus and knowledge graphs. We adopt three\nknowledge-driven approaches for zero-shot QA over traffic situations, based on\nprior natural language inference methods, commonsense models with knowledge\ngraph self-supervision, and dense retriever-based models. We constructed two\ntext-based multiple-choice question answering sets: BDD-QA for evaluating\ncausal reasoning in the traffic domain and HDT-QA for measuring the possession\nof domain knowledge akin to human driving license tests. Among the methods,\nUnified-QA reaches the best performance on the BDD-QA dataset with the\nadaptation of multiple formats of question answers. Language models trained\nwith inference information and commonsense knowledge are also good at\npredicting the cause and effect in the traffic domain but perform badly at\nanswering human-driving QA sets. For such sets, DPR+Unified-QA performs the\nbest due to its efficient knowledge extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiarui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollaa_A/0/1/0/all/0/1\">Aravinda Kollaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oltramari_A/0/1/0/all/0/1\">Alessandro Oltramari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TeTIm-Eval: a novel curated evaluation data set for comparing text-to-image models. (arXiv:2212.07839v1 [cs.CV])","link":"http://arxiv.org/abs/2212.07839","description":"<p>Evaluating and comparing text-to-image models is a challenging problem.\nSignificant advances in the field have recently been made, piquing interest of\nvarious industrial sectors. As a consequence, a gold standard in the field\nshould cover a variety of tasks and application contexts. In this paper a novel\nevaluation approach is experimented, on the basis of: (i) a curated data set,\nmade by high-quality royalty-free image-text pairs, divided into ten\ncategories; (ii) a quantitative metric, the CLIP-score, (iii) a human\nevaluation task to distinguish, for a given text, the real and the generated\nimages. The proposed method has been applied to the most recent models, i.e.,\nDALLE2, Latent Diffusion, Stable Diffusion, GLIDE and Craiyon. Early\nexperimental results show that the accuracy of the human judgement is fully\ncoherent with the CLIP-score. The dataset has been made available to the\npublic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galatolo_F/0/1/0/all/0/1\">Federico A. Galatolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cimino_M/0/1/0/all/0/1\">Mario G. C. A. Cimino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cogotti_E/0/1/0/all/0/1\">Edoardo Cogotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers. (arXiv:2212.07841v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07841","description":"<p>Dense retrieval aims to map queries and passages into low-dimensional vector\nspace for efficient similarity measuring, showing promising effectiveness in\nvarious large-scale retrieval tasks. Since most existing methods commonly adopt\npre-trained Transformers (e.g. BERT) for parameter initialization, some work\nfocuses on proposing new pre-training tasks for compressing the useful semantic\ninformation from passages into dense vectors, achieving remarkable\nperformances. However, it is still challenging to effectively capture the rich\nsemantic information and relations about passages into the dense vectors via\none single particular pre-training task. In this work, we propose a multi-task\npre-trained model, MASTER, that unifies and integrates multiple pre-training\ntasks with different learning objectives under the bottlenecked masked\nautoencoder architecture. Concretely, MASTER utilizes a multi-decoder\narchitecture to integrate three types of pre-training tasks: corrupted passages\nrecovering, related passage recovering and PLMs outputs recovering. By\nincorporating a shared deep encoder, we construct a representation bottleneck\nin our architecture, compressing the abundant semantic information across tasks\ninto dense vectors. The first two types of tasks concentrate on capturing the\nsemantic information of passages and relationships among them within the\npre-training corpus. The third one can capture the knowledge beyond the corpus\nfrom external PLMs (e.g. GPT-2). Extensive experiments on several large-scale\npassage retrieval datasets have shown that our approach outperforms the\nprevious state-of-the-art dense retrieval methods. Our code and data are\npublicly released in https://github.com/microsoft/SimXNS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention as a guide for Simultaneous Speech Translation. (arXiv:2212.07850v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07850","description":"<p>The study of the attention mechanism has sparked interest in many fields,\nsuch as language modeling and machine translation. Although its patterns have\nbeen exploited to perform different tasks, from neural network understanding to\ntextual alignment, no previous work has analysed the encoder-decoder attention\nbehavior in speech translation (ST) nor used it to improve ST on a specific\ntask. In this paper, we fill this gap by proposing an attention-based policy\n(EDAtt) for simultaneous ST (SimulST) that is motivated by an analysis of the\nexisting attention relations between audio input and textual output. Its goal\nis to leverage the encoder-decoder attention scores to guide inference in real\ntime. Results on en-&gt;{de, es} show that the EDAtt policy achieves overall\nbetter results compared to the SimulST state of the art, especially in terms of\ncomputational-aware latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The effects of gender bias in word embeddings on depression prediction. (arXiv:2212.07852v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07852","description":"<p>Word embeddings are extensively used in various NLP problems as a\nstate-of-the-art semantic feature vector representation. Despite their success\non various tasks and domains, they might exhibit an undesired bias for\nstereotypical categories due to statistical and societal biases that exist in\nthe dataset they are trained on. In this study, we analyze the gender bias in\nfour different pre-trained word embeddings specifically for the depression\ncategory in the mental disorder domain. We use contextual and non-contextual\nembeddings that are trained on domain-independent as well as clinical\ndomain-specific data. We observe that embeddings carry bias for depression\ntowards different gender groups depending on the type of embeddings. Moreover,\nwe demonstrate that these undesired correlations are transferred to the\ndownstream task for depression phenotype recognition. We find that data\naugmentation by simply swapping gender words mitigates the bias significantly\nin the downstream task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sogancioglu_G/0/1/0/all/0/1\">Gizem Sogancioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaya_H/0/1/0/all/0/1\">Heysem Kaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effects of In-domain Corpus Size on pre-training BERT. (arXiv:2212.07914v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07914","description":"<p>Many prior language modeling efforts have shown that pre-training on an\nin-domain corpus can significantly improve performance on downstream\ndomain-specific NLP tasks. However, the difficulties associated with collecting\nenough in-domain data might discourage researchers from approaching this\npre-training task. In this paper, we conducted a series of experiments by\npre-training Bidirectional Encoder Representations from Transformers (BERT)\nwith different sizes of biomedical corpora. The results demonstrate that\npre-training on a relatively small amount of in-domain data (4GB) with limited\ntraining steps, can lead to better performance on downstream domain-specific\nNLP tasks compared with fine-tuning models pre-trained on general corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_C/0/1/0/all/0/1\">Chris Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheyuan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. (arXiv:2212.07919v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07919","description":"<p>Large language models show improved downstream task performance when prompted\nto generate step-by-step reasoning to justify their final answers. These\nreasoning steps greatly improve model interpretability and verification, but\nobjectively studying their correctness (independent of the final answer) is\ndifficult without reliable methods for automatic evaluation. We simply do not\nknow how often the stated reasoning steps actually support the final end task\npredictions. In this work, we present ROSCOE, a suite of interpretable,\nunsupervised automatic scores that improve and extend previous text generation\nevaluation metrics. To evaluate ROSCOE against baseline metrics, we design a\ntypology of reasoning errors and collect synthetic and human evaluation scores\non commonly used reasoning datasets. In contrast with existing metrics, ROSCOE\ncan measure semantic consistency, logicality, informativeness, fluency, and\nfactuality - among other traits - by leveraging properties of step-by-step\nrationales. We empirically verify the strength of our metrics on five human\nannotated and six programmatically perturbed diagnostics datasets - covering a\ndiverse set of tasks that require reasoning skills and show that ROSCOE can\nconsistently outperform baseline metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golovneva_O/0/1/0/all/0/1\">Olga Golovneva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Moya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poff_S/0/1/0/all/0/1\">Spencer Poff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corredor_M/0/1/0/all/0/1\">Martin Corredor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazel_Zarandi_M/0/1/0/all/0/1\">Maryam Fazel-Zarandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Learning for Joint Intent and Slot Labeling. (arXiv:2212.07922v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07922","description":"<p>It is expensive and difficult to obtain the large number of sentence-level\nintent and token-level slot label annotations required to train neural network\n(NN)-based Natural Language Understanding (NLU) components of task-oriented\ndialog systems, especially for the many real world tasks that have a large and\ngrowing number of intents and slot types. While zero shot learning approaches\nthat require no labeled examples -- only features and auxiliary information --\nhave been proposed only for slot labeling, we show that one can profitably\nperform joint zero-shot intent classification and slot labeling. We demonstrate\nthe value of capturing dependencies between intents and slots, and between\ndifferent slots in an utterance in the zero shot setting. We describe NN\narchitectures that translate between word and sentence embedding spaces, and\ndemonstrate that these modifications are required to enable zero shot learning\nfor this task. We show a substantial improvement over strong baselines and\nexplain the intuition behind each architectural modification through\nvisualizations and ablation studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gangadharaiah_R/0/1/0/all/0/1\">Rashmi Gangadharaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanaswamy_B/0/1/0/all/0/1\">Balakrishnan Narayanaswamy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Natural Language Processing to Predict Costume Core Vocabulary of Historical Artifacts. (arXiv:2212.07931v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07931","description":"<p>Historic dress artifacts are a valuable source for human studies. In\nparticular, they can provide important insights into the social aspects of\ntheir corresponding era. These insights are commonly drawn from garment\npictures as well as the accompanying descriptions and are usually stored in a\nstandardized and controlled vocabulary that accurately describes garments and\ncostume items, called the Costume Core Vocabulary. Building an accurate Costume\nCore from garment descriptions can be challenging because the historic garment\nitems are often donated, and the accompanying descriptions can be based on\nuntrained individuals and use a language common to the period of the items. In\nthis paper, we present an approach to use Natural Language Processing (NLP) to\nmap the free-form text descriptions of the historic items to that of the\ncontrolled vocabulary provided by the Costume Core. Despite the limited\ndataset, we were able to train an NLP model based on the Universal Sentence\nEncoder to perform this mapping with more than 90% test accuracy for a subset\nof the Costume Core vocabulary. We describe our methodology, design choices,\nand development of our approach, and show the feasibility of predicting the\nCostume Core for unseen descriptions. With more garment descriptions still\nbeing curated to be used for training, we expect to have higher accuracy for\nbetter generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muralikrishnan_M/0/1/0/all/0/1\">Madhuvanti Muralikrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilal_A/0/1/0/all/0/1\">Amr Hilal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1\">Chreston Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_Glaviana_D/0/1/0/all/0/1\">Dina Smith-Glaviana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually-augmented pretrained language models for NLP tasks without images. (arXiv:2212.07937v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07937","description":"<p>Although pre-trained language models (PLMs) have shown impressive performance\nby text-only self-supervised training, they are found lack of visual semantics\nor commonsense, e.g., sizes, shapes, and colors of commonplace objects.\nExisting solutions often rely on explicit images for visual knowledge\naugmentation (requiring time-consuming retrieval or generation), and they also\nconduct the augmentation for the whole input text, without considering whether\nit is actually needed in specific inputs or tasks. To address these issues, we\npropose a novel visually-augmented fine-tuning approach that can be generally\napplied to various PLMs or NLP tasks, without using any retrieved or generated\nimages, namely VAWI. Specifically, we first identify the visually-hungry words\n(VH-words) from input text via a token selector, where three different methods\nhave been proposed, including syntax-, attention- and learning-based\nstrategies. Then, we adopt a fixed CLIP text encoder to generate the\nvisually-augmented representations of these VH-words. As it has been\npre-trained by vision-language alignment task on the large-scale corpus, it is\ncapable of injecting visual semantics into the aligned text representations.\nFinally, the visually-augmented features will be fused and transformed into the\npre-designed visual prompts based on VH-words, which can be inserted into PLMs\nto enrich the visual semantics in word representations. We conduct extensive\nexperiments on ten NLP tasks, i.e., GLUE benchmark, CommonsenseQA, CommonGen,\nand SNLI-VE. Experimental results show that our approach can consistently\nimprove the performance of BERT, RoBERTa, BART, and T5 at different scales, and\noutperform several competitive baselines significantly. Our codes and data are\npublicly available at~\\url{https://github.com/RUCAIBox/VAWI}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hangyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RWEN-TTS: Relation-aware Word Encoding Network for Natural Text-to-Speech Synthesis. (arXiv:2212.07939v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07939","description":"<p>With the advent of deep learning, a huge number of text-to-speech (TTS)\nmodels which produce human-like speech have emerged. Recently, by introducing\nsyntactic and semantic information w.r.t the input text, various approaches\nhave been proposed to enrich the naturalness and expressiveness of TTS models.\nAlthough these strategies showed impressive results, they still have some\nlimitations in utilizing language information. First, most approaches only use\ngraph networks to utilize syntactic and semantic information without\nconsidering linguistic features. Second, most previous works do not explicitly\nconsider adjacent words when encoding syntactic and semantic information, even\nthough it is obvious that adjacent words are usually meaningful when encoding\nthe current word. To address these issues, we propose Relation-aware Word\nEncoding Network (RWEN), which effectively allows syntactic and semantic\ninformation based on two modules (i.e., Semantic-level Relation Encoding and\nAdjacent Word Relation Encoding). Experimental results show substantial\nimprovements compared to previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Shinhyeok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_H/0/1/0/all/0/1\">HyeongRae Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yoonseok Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_I/0/1/0/all/0/1\">Insoo Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation. (arXiv:2212.07981v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07981","description":"<p>Human evaluation is the foundation upon which the evaluation of both\nsummarization systems and automatic metrics rests. However, existing human\nevaluation protocols and benchmarks for summarization either exhibit low\ninter-annotator agreement or lack the scale needed to draw statistically\nsignificant conclusions, and an in-depth analysis of human evaluation is\nlacking. In this work, we address the shortcomings of existing summarization\nevaluation along the following axes: 1) We propose a modified summarization\nsalience protocol, Atomic Content Units (ACUs), which relies on fine-grained\nsemantic units and allows for high inter-annotator agreement. 2) We curate the\nRobust Summarization Evaluation (RoSE) benchmark, a large human evaluation\ndataset consisting of over 22k summary-level annotations over state-of-the-art\nsystems on three datasets. 3) We compare our ACU protocol with three other\nhuman evaluation protocols, underscoring potential confounding factors in\nevaluation setups. 4) We evaluate existing automatic metrics using the\ncollected human annotations across evaluation protocols and demonstrate how our\nbenchmark leads to more statistically stable and significant results.\nFurthermore, our findings have important implications for evaluating large\nlanguage models (LLMs), as we show that LLMs adjusted by human feedback (e.g.,\nGPT-3.5) may overfit unconstrained human evaluation, which is affected by the\nannotators' prior, input-agnostic preferences, calling for more robust,\ntargeted evaluation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yilun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Linyong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Ruilin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Simeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformers are Parameter-Efficient Audio-Visual Learners. (arXiv:2212.07983v1 [cs.CV])","link":"http://arxiv.org/abs/2212.07983","description":"<p>Vision transformers (ViTs) have achieved impressive results on various\ncomputer vision tasks in the last several years. In this work, we study the\ncapability of frozen ViTs, pretrained only on visual data, to generalize to\naudio-visual data without finetuning any of its original parameters. To do so,\nwe propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained\nViTs to audio-visual tasks by injecting a small number of trainable parameters\ninto every layer of a frozen ViT. To efficiently fuse visual and audio cues,\nour LAVISH adapter uses a small set of latent tokens, which form an attention\nbottleneck, thus, eliminating the quadratic cost of standard cross-attention.\nCompared to the existing modality-specific audio-visual methods, our approach\nachieves competitive or even better performance on various audio-visual tasks\nwhile using fewer tunable parameters and without relying on costly audio\npretraining or external audio encoders. Our code is available at\nhttps://genjib.github.io/project_page/LAVISH/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yan-Bo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yi-Lin Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-VALUE: A Framework for Cross-Dialectal English NLP. (arXiv:2212.08011v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08011","description":"<p>Dialect differences caused by regional, social, and economic barriers cause\nperformance discrepancies for many groups of users of language technology.\nFair, inclusive, and equitable language technology must critically be dialect\ninvariant, meaning that performance remains constant over dialectal shifts.\nCurrent English systems often fall significantly short of this ideal since they\nare designed and tested on a single dialect: Standard American English. We\nintroduce Multi-VALUE -- a suite of resources for evaluating and achieving\nEnglish dialect invariance. We build a controllable rule-based translation\nsystem spanning 50 English dialects and a total of 189 unique linguistic\nfeatures. Our translation maps Standard American English text to synthetic form\nof each dialect, which uses an upper-bound on the natural density of features\nin that dialect. First, we use this system to build stress tests for question\nanswering, machine translation, and semantic parsing tasks. Stress tests reveal\nsignificant performance disparities for leading models on non-standard\ndialects. Second, we use this system as a data augmentation technique to\nimprove the dialect robustness of existing systems. Finally, we partner with\nnative speakers of Chicano and Indian English to release new gold-standard\nvariants of the popular CoQA task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_W/0/1/0/all/0/1\">William Held</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models. (arXiv:2212.08037v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08037","description":"<p>Large language models (LLMs) have shown impressive results across a variety\nof tasks while requiring little or no direct supervision. Further, there is\nmounting evidence that LLMs may have potential in information-seeking\nscenarios. We believe the ability of an LLM to attribute the text that it\ngenerates is likely to be crucial for both system developers and users in this\nsetting. We propose and study Attributed QA as a key first step in the\ndevelopment of attributed LLMs. We develop a reproducable evaluation framework\nfor the task, using human annotations as a gold standard and a correlated\nautomatic metric that we show is suitable for development settings. We describe\nand benchmark a broad set of architectures for the task. Our contributions give\nsome concrete answers to two key questions (How to measure attribution?, and\nHow well do current state-of-the-art methods perform on attribution?), and give\nsome hints as to how to address a third key question (How to build LLMs with\nattribution?).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bohnet_B/0/1/0/all/0/1\">Bernd Bohnet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1\">Pat Verga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1\">Roee Aharoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andor_D/0/1/0/all/0/1\">Daniel Andor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_L/0/1/0/all/0/1\">Livio Baldini Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganchev_K/0/1/0/all/0/1\">Kuzman Ganchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1\">Jonathan Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Kai Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowski_T/0/1/0/all/0/1\">Tom Kwiatkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Ji Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Michael Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipanjan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_S/0/1/0/all/0/1\">Slav Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webster_K/0/1/0/all/0/1\">Kellie Webster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can REF output quality scores be assigned by AI? Experimental evidence. (arXiv:2212.08041v1 [cs.CY])","link":"http://arxiv.org/abs/2212.08041","description":"<p>This document describes strategies for using Artificial Intelligence (AI) to\npredict some journal article scores in future research assessment exercises.\nFive strategies have been assessed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thelwall_M/0/1/0/all/0/1\">Mike Thelwall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kousha_K/0/1/0/all/0/1\">Kayvan Kousha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdoli_M/0/1/0/all/0/1\">Mahshid Abdoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuart_E/0/1/0/all/0/1\">Emma Stuart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makita_M/0/1/0/all/0/1\">Meiko Makita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_P/0/1/0/all/0/1\">Paul Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levitt_J/0/1/0/all/0/1\">Jonathan Levitt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue. (arXiv:2212.08054v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08054","description":"<p>Modern virtual assistants use internal semantic parsing engines to convert\nuser utterances to actionable commands. However, prior work has demonstrated\nthat semantic parsing is a difficult multilingual transfer task with low\ntransfer efficiency compared to other tasks. In global markets such as India\nand Latin America, this is a critical issue as switching between languages is\nprevalent for bilingual users. In this work we dramatically improve the\nzero-shot performance of a multilingual and codeswitched semantic parsing\nsystem using two stages of multilingual alignment. First, we show that\nconstrastive alignment pretraining improves both English performance and\ntransfer efficiency. We then introduce a constrained optimization approach for\nhyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned\nMultilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and\n81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing\nbenchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Held_W/0/1/0/all/0/1\">William Held</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidey_C/0/1/0/all/0/1\">Christopher Hidey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">Eric Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rahul Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rushin Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units. (arXiv:2212.08055v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08055","description":"<p>Direct speech-to-speech translation (S2ST), in which all components can be\noptimized jointly, is advantageous over cascaded approaches to achieve fast\ninference with a simplified pipeline. We present a novel two-pass direct S2ST\narchitecture, {\\textit UnitY}, which first generates textual representations\nand predicts discrete acoustic units subsequently. We enhance the model\nperformance by subword prediction in the first-pass decoder, advanced two-pass\ndecoder architecture design and search strategy, and better training\nregularization. To leverage large amounts of unlabeled text data, we pre-train\nthe first-pass text decoder based on the self-supervised denoising\nauto-encoding task. Experimental evaluations on benchmark datasets at various\ndata scales demonstrate that UnitY outperforms a single-pass speech-to-unit\ntranslation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show\nthat the proposed methods boost the performance even when predicting\nspectrogram in the second pass. However, predicting discrete units achieves\n2.51x decoding speed-up compared to that case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popuri_S/0/1/0/all/0/1\">Sravya Popuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_I/0/1/0/all/0/1\">Ilia Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yu-An Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning. (arXiv:2212.08061v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08061","description":"<p>Generating a chain of thought (CoT) can increase large language model (LLM)\nperformance on a wide range of tasks. Zero-shot CoT evaluations, however, have\nbeen conducted primarily on logical tasks (e.g. arithmetic, commonsense QA). In\nthis paper, we perform a controlled evaluation of zero-shot CoT across two\nsensitive domains: harmful questions and stereotype benchmarks. We find that\nusing zero-shot CoT reasoning in a prompt can significantly increase a model's\nlikelihood to produce undesirable output. Without future advances in alignment\nor explicit mitigation instructions, zero-shot CoT should be avoided on tasks\nwhere models can make inferences about marginalized groups or harmful topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_O/0/1/0/all/0/1\">Omar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_W/0/1/0/all/0/1\">William Held</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_M/0/1/0/all/0/1\">Michael Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Definition and a Test for Human-Level Artificial Intelligence. (arXiv:2011.09410v5 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2011.09410","description":"<p>Despite recent advances of AI research in many application-specific domains,\nwe do not know how to build a human-level artificial intelligence (HLAI). We\nconjecture that learning from others' experience with the language is the\nessential characteristic that distinguishes human intelligence from the rest.\nHumans can update the action-value function with the verbal description as if\nthey experience states, actions, and corresponding rewards sequences firsthand.\nIn this paper, we present a classification of intelligence according to how\nindividual agents learn and propose a definition and a test for HLAI. The main\nidea is that language acquisition without explicit rewards can be a sufficient\ntest for HLAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Deokgun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondol_M/0/1/0/all/0/1\">Md Ashaduzzaman Rubel Mondol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pothula_A/0/1/0/all/0/1\">Aishwarya Pothula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mazharul Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Ideological Salience and Framing in Polarized Online Groups with Graph Neural Networks and Structured Sparsity. (arXiv:2104.08829v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08829","description":"<p>The increasing polarization of online political discourse calls for\ncomputational tools that automatically detect and monitor ideological divides\nin social media. We introduce a minimally supervised method that leverages the\nnetwork structure of online discussion forums, specifically Reddit, to detect\npolarized concepts. We model polarization along the dimensions of salience and\nframing, drawing upon insights from moral psychology. Our architecture combines\ngraph neural networks with structured sparsity learning and results in\nrepresentations for concepts and subreddits that capture temporal ideological\ndynamics such as right-wing and left-wing radicalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_V/0/1/0/all/0/1\">Valentin Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaowen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1\">Janet B. Pierrehumbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PALBERT: Teaching ALBERT to Ponder. (arXiv:2204.03276v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.03276","description":"<p>Currently, pre-trained models can be considered the default choice for a wide\nrange of NLP tasks. Despite their SoTA results, there is practical evidence\nthat these models may require a different number of computing layers for\ndifferent input sequences, since evaluating all layers leads to overconfidence\nin wrong predictions (namely overthinking). This problem can potentially be\nsolved by implementing adaptive computation time approaches, which were first\ndesigned to improve inference speed. Recently proposed PonderNet may be a\npromising solution for performing an early exit by treating the exit layer's\nindex as a latent variable. However, the originally proposed exit criterion,\nrelying on sampling from trained posterior distribution on the probability of\nexiting from the $i$-th layer, introduces major variance in exit layer indices,\nsignificantly reducing the resulting model's performance. In this paper, we\npropose improving PonderNet with a novel deterministic Q-exit criterion and a\nrevisited model architecture. We adapted the proposed mechanism to ALBERT and\nRoBERTa and compared it with recent methods for performing an early exit. We\nobserved that the proposed changes can be considered significant improvements\non the original PonderNet architecture and outperform PABEE on a wide range of\nGLUE tasks. In addition, we also performed an in-depth ablation study of the\nproposed architecture to further understand Lambda layers and their\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balagansky_N/0/1/0/all/0/1\">Nikita Balagansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavrilov_D/0/1/0/all/0/1\">Daniil Gavrilov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Tokenization Learning. (arXiv:2205.11443v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11443","description":"<p>In the presented study, we discover that the so-called \"transition freedom\"\nmetric appears superior for unsupervised tokenization purposes in comparison to\nstatistical metrics such as mutual information and conditional probability,\nproviding F-measure scores in range from 0.71 to 1.0 across explored\nmultilingual corpora. We find that different languages require different\noffshoots of that metric (such as derivative, variance, and \"peak values\") for\nsuccessful tokenization. Larger training corpora do not necessarily result in\nbetter tokenization quality, while compressing the models by eliminating\nstatistically weak evidence tends to improve performance. The proposed\nunsupervised tokenization technique provides quality better than or comparable\nto lexicon-based ones, depending on the language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolonin_A/0/1/0/all/0/1\">Anton Kolonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1\">Vignav Ramesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Label Errors by using Pre-Trained Language Models. (arXiv:2205.12702v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12702","description":"<p>We show that large pre-trained language models are inherently highly capable\nof identifying label errors in natural language datasets: simply examining\nout-of-sample data points in descending order of fine-tuned task loss\nsignificantly outperforms more complex error-detection mechanisms proposed in\nprevious work.\n</p>\n<p>To this end, we contribute a novel method for introducing realistic,\nhuman-originated label noise into existing crowdsourced datasets such as SNLI\nand TweetNLP. We show that this noise has similar properties to real,\nhand-verified label errors, and is harder to detect than existing synthetic\nnoise, creating challenges for model robustness. We argue that human-originated\nnoise is a better standard for evaluation than synthetic noise.\n</p>\n<p>Finally, we use crowdsourced verification to evaluate the detection of real\nerrors on IMDB, Amazon Reviews, and Recon, and confirm that pre-trained models\nperform at a 9-36% higher absolute Area Under the Precision-Recall Curve than\nexisting models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chong_D/0/1/0/all/0/1\">Derek Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jenny Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Scientific Creativity with Retrieval across Knowledge Domains. (arXiv:2206.01328v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2206.01328","description":"<p>Exposure to ideas in domains outside a scientist's own may benefit her in\nreformulating existing research problems in novel ways and discovering new\napplication domains for existing solution ideas. While improved performance in\nscholarly search engines can help scientists efficiently identify relevant\nadvances in domains they may already be familiar with, it may fall short of\nhelping them explore diverse ideas \\textit{outside} such domains. In this paper\nwe explore the design of systems aimed at augmenting the end-user ability in\ncross-domain exploration with flexible query specification. To this end, we\ndevelop an exploratory search system in which end-users can select a portion of\ntext core to their interest from a paper abstract and retrieve papers that have\na high similarity to the user-selected core aspect but differ in terms of\ndomains. Furthermore, end-users can `zoom in' to specific domain clusters to\nretrieve more papers from them and understand nuanced differences within the\nclusters. Our case studies with scientists uncover opportunities and design\nimplications for systems aimed at facilitating cross-domain exploration and\ninspiration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hyeonsu B. Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1\">Sheshera Mysore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kevin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Haw-Shiuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prein_T/0/1/0/all/0/1\">Thorben Prein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittur_A/0/1/0/all/0/1\">Aniket Kittur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olivetti_E/0/1/0/all/0/1\">Elsa Olivetti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndicSUPERB: A Speech Processing Universal Performance Benchmark for Indian languages. (arXiv:2208.11761v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.11761","description":"<p>A cornerstone in AI research has been the creation and adoption of\nstandardized training and test datasets to earmark the progress of\nstate-of-the-art models. A particularly successful example is the GLUE dataset\nfor training and evaluating Natural Language Understanding (NLU) models for\nEnglish. The large body of research around self-supervised BERT-based language\nmodels revolved around performance improvements on NLU tasks in GLUE. To\nevaluate language models in other languages, several language-specific GLUE\ndatasets were created. The area of speech language understanding (SLU) has\nfollowed a similar trajectory. The success of large self-supervised models such\nas wav2vec2 enable creation of speech models with relatively easy to access\nunlabelled data. These models can then be evaluated on SLU tasks, such as the\nSUPERB benchmark. In this work, we extend this to Indic languages by releasing\nthe IndicSUPERB benchmark. Specifically, we make the following three\ncontributions. (i) We collect Kathbath containing 1,684 hours of labelled\nspeech data across 12 Indian languages from 1,218 contributors located in 203\ndistricts in India. (ii) Using Kathbath, we create benchmarks across 6 speech\ntasks: Automatic Speech Recognition, Speaker Verification, Speaker\nIdentification (mono/multi), Language Identification, Query By Example, and\nKeyword Spotting for 12 languages. (iii) On the released benchmarks, we train\nand evaluate different self-supervised models alongside a commonly used\nbaseline FBANK. We show that language-specific fine-tuned models are more\naccurate than baseline on most of the tasks, including a large gap of 76\\% for\nthe Language Identification task. However, for speaker identification,\nself-supervised models trained on large datasets demonstrate an advantage. We\nhope IndicSUPERB contributes to the progress of developing speech language\nunderstanding models for Indian languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Javed_T/0/1/0/all/0/1\">Tahir Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhogale_K/0/1/0/all/0/1\">Kaushal Santosh Bhogale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_A/0/1/0/all/0/1\">Abhigyan Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations. (arXiv:2209.07562v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.07562","description":"<p>We present TwHIN-BERT, a multilingual language model trained on in-domain\ndata from the popular social network Twitter. TwHIN-BERT differs from prior\npre-trained language models as it is trained with not only text-based\nself-supervision, but also with a social objective based on the rich social\nengagements within a Twitter heterogeneous information network (TwHIN). Our\nmodel is trained on 7 billion tweets covering over 100 distinct languages\nproviding a valuable representation to model short, noisy, user-generated text.\nWe evaluate our model on a variety of multilingual social recommendation and\nsemantic understanding tasks and demonstrate significant metric improvement\nover established pre-trained language models. We will freely open-source\nTwHIN-BERT and our curated hashtag prediction and social engagement benchmark\ndatasets to the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkov_Y/0/1/0/all/0/1\">Yury Malkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florez_O/0/1/0/all/0/1\">Omar Florez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Serim Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McWilliams_B/0/1/0/all/0/1\">Brian McWilliams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1\">Ahmed El-Kishky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Generalized and Explainable Long-Range Context Representation for Dialogue Systems. (arXiv:2210.06282v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06282","description":"<p>Long-range context modeling is crucial to both dialogue understanding and\ngeneration. The most popular method for dialogue context representation is to\nconcatenate the last-$k$ previous utterances. However, this method may not be\nideal for conversations containing long-range dependencies. In this work, we\npropose DialoGX, a novel encoder-decoder based framework for conversational\nresponse generation with a generalized and explainable context representation\nthat can look beyond the last-$k$ utterances. Hence the method is adaptive to\nconversations with long-range dependencies. The main idea of our approach is to\nidentify and utilize the most relevant historical utterances instead of the\nlast-$k$ utterances in chronological order. We study the effectiveness of our\nproposed method on both dialogue generation (open-domain) and understanding\n(DST) tasks. DialoGX achieves comparable performance with the state-of-the-art\nmodels on DailyDialog dataset. We also observe performance gain in existing DST\nmodels with our proposed context representation strategy on MultiWOZ dataset.\nWe justify our context representation through the lens of psycholinguistics and\nshow that the relevance score of previous utterances agrees well with human\ncognition which makes DialoGX explainable as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Suvodip Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desarkar_M/0/1/0/all/0/1\">Maunendra Sankar Desarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srijith_P/0/1/0/all/0/1\">P. K. Srijith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Preference Learning for Storytelling via Contrastive Reinforcement Learning. (arXiv:2210.07792v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07792","description":"<p>Controlled automated story generation seeks to generate natural language\nstories satisfying constraints from natural language critiques or preferences.\nExisting methods to control for story preference utilize prompt engineering\nwhich is labor intensive and often inconsistent. They may also use\nlogit-manipulation methods which require annotated datasets to exist for the\ndesired attributes. To address these issues, we first train a contrastive\nbi-encoder model to align stories with corresponding human critiques, named\nCARP, building a general purpose preference model. This is subsequently used as\na reward function to fine-tune a generative language model via reinforcement\nlearning. However, simply fine-tuning a generative language model with a\ncontrastive reward model does not always reliably result in a story generation\nsystem capable of generating stories that meet user preferences. To increase\nstory generation robustness we further fine-tune the contrastive reward model\nusing a prompt-learning technique. A human participant study is then conducted\ncomparing generations from our full system, ablations, and two baselines. We\nshow that the full fine-tuning pipeline results in a story generator preferred\nover a LLM 20x as large as well as logit-based methods. This motivates the use\nof contrastive learning for general purpose human preference modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Castricato_L/0/1/0/all/0/1\">Louis Castricato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havrilla_A/0/1/0/all/0/1\">Alexander Havrilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matiana_S/0/1/0/all/0/1\">Shahbuland Matiana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pieler_M/0/1/0/all/0/1\">Michael Pieler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_A/0/1/0/all/0/1\">Anbang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_I/0/1/0/all/0/1\">Ian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frazier_S/0/1/0/all/0/1\">Spencer Frazier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous diffusion for categorical data. (arXiv:2211.15089v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15089","description":"<p>Diffusion models have quickly become the go-to paradigm for generative\nmodelling of perceptual signals (such as images and sound) through iterative\nrefinement. Their success hinges on the fact that the underlying physical\nphenomena are continuous. For inherently discrete and categorical data such as\nlanguage, various diffusion-inspired alternatives have been proposed. However,\nthe continuous nature of diffusion models conveys many benefits, and in this\nwork we endeavour to preserve it. We propose CDCD, a framework for modelling\ncategorical data with diffusion models that are continuous both in time and\ninput space. We demonstrate its efficacy on several language modelling tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dieleman_S/0/1/0/all/0/1\">Sander Dieleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sartran_L/0/1/0/all/0/1\">Laurent Sartran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roshannai_A/0/1/0/all/0/1\">Arman Roshannai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savinov_N/0/1/0/all/0/1\">Nikolay Savinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganin_Y/0/1/0/all/0/1\">Yaroslav Ganin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richemond_P/0/1/0/all/0/1\">Pierre H. Richemond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strudel_R/0/1/0/all/0/1\">Robin Strudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1\">Chris Dyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durkan_C/0/1/0/all/0/1\">Conor Durkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawthorne_C/0/1/0/all/0/1\">Curtis Hawthorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leblond_R/0/1/0/all/0/1\">R&#xe9;mi Leblond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grathwohl_W/0/1/0/all/0/1\">Will Grathwohl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_J/0/1/0/all/0/1\">Jonas Adler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastClass: A Time-Efficient Approach to Weakly-Supervised Text Classification. (arXiv:2212.05506v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05506","description":"<p>Weakly-supervised text classification aims to train a classifier using only\nclass descriptions and unlabeled data. Recent research shows that\nkeyword-driven methods can achieve state-of-the-art performance on various\ntasks. However, these methods not only rely on carefully-crafted class\ndescriptions to obtain class-specific keywords but also require substantial\namount of unlabeled data and takes a long time to train. This paper proposes\nFastClass, an efficient weakly-supervised classification approach. It uses\ndense text representation to retrieve class-relevant documents from external\nunlabeled corpus and selects an optimal subset to train a classifier. Compared\nto keyword-driven methods, our approach is less reliant on initial class\ndescriptions as it no longer needs to expand each class description into a set\nof class-specific keywords. Experiments on a wide range of classification tasks\nshow that the proposed approach frequently outperforms keyword-driven models in\nterms of classification accuracy and often enjoys orders-of-magnitude faster\ntraining speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tingyu Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposing a Recurrent Neural Network into Modules for Enabling Reusability and Replacement. (arXiv:2212.05970v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2212.05970","description":"<p>Can we take a recurrent neural network (RNN) trained to translate between\nlanguages and augment it to support a new natural language without retraining\nthe model from scratch? Can we fix the faulty behavior of the RNN by replacing\nportions associated with the faulty behavior? Recent works on decomposing a\nfully connected neural network (FCNN) and convolutional neural network (CNN)\ninto modules have shown the value of engineering deep models in this manner,\nwhich is standard in traditional SE but foreign for deep learning models.\nHowever, prior works focus on the image-based multiclass classification\nproblems and cannot be applied to RNN due to (a) different layer structures,\n(b) loop structures, (c) different types of input-output architectures, and (d)\nusage of both nonlinear and logistic activation functions. In this work, we\npropose the first approach to decompose an RNN into modules. We study different\ntypes of RNNs, i.e., Vanilla, LSTM, and GRU. Further, we show how such RNN\nmodules can be reused and replaced in various scenarios. We evaluate our\napproach against 5 canonical datasets (i.e., Math QA, Brown Corpus,\nWiki-toxicity, Clinc OOS, and Tatoeba) and 4 model variants for each dataset.\nWe found that decomposing a trained model has a small cost (Accuracy: -0.6%,\nBLEU score: +0.10%). Also, the decomposed modules can be reused and replaced\nwithout needing to retrain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imtiaz_S/0/1/0/all/0/1\">Sayem Mohammad Imtiaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batole_F/0/1/0/all/0/1\">Fraol Batole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Astha Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1\">Rangeet Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_B/0/1/0/all/0/1\">Breno Dantas Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1\">Hridesh Rajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards mapping the contemporary art world with ArtLM: an art-specific NLP model. (arXiv:2212.07127v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07127","description":"<p>With an increasing amount of data in the art world, discovering artists and\nartworks suitable to collectors' tastes becomes a challenge. It is no longer\nenough to use visual information, as contextual information about the artist\nhas become just as important in contemporary art. In this work, we present a\ngeneric Natural Language Processing framework (called ArtLM) to discover the\nconnections among contemporary artists based on their biographies. In this\napproach, we first continue to pre-train the existing general English language\nmodels with a large amount of unlabelled art-related data. We then fine-tune\nthis new pre-trained model with our biography pair dataset manually annotated\nby a team of professionals in the art industry. With extensive experiments, we\ndemonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and\noutperforms other baseline models. We also provide a visualisation and a\nqualitative analysis of the artist network built from ArtLM's outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qinkai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Mennaoui_M/0/1/0/all/0/1\">Mohamed El-Mennaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fosset_A/0/1/0/all/0/1\">Antoine Fosset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebei_A/0/1/0/all/0/1\">Amine Rebei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haoyang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBeirne_C/0/1/0/all/0/1\">Christy E&#xf3;in O&#x27;Beirne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shevchenko_S/0/1/0/all/0/1\">Sasha Shevchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenbaum_M/0/1/0/all/0/1\">Mathieu Rosenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-12-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}