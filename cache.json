{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-10-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Emergence of Shared Sensory-motor Graphical Language from Visual Input. (arXiv:2210.06468v1 [cs.AI])","link":"http://arxiv.org/abs/2210.06468","description":"<p>The framework of Language Games studies the emergence of languages in\npopulations of agents. Recent contributions relying on deep learning methods\nfocused on agents communicating via an idealized communication channel, where\nutterances produced by a speaker are directly perceived by a listener. This\ncomes in contrast with human communication, which instead relies on a\nsensory-motor channel, where motor commands produced by the speaker (e.g. vocal\nor gestural articulators) result in sensory effects perceived by the listener\n(e.g. audio or visual). Here, we investigate if agents can evolve a shared\nlanguage when they are equipped with a continuous sensory-motor system to\nproduce and perceive signs, e.g. drawings. To this end, we introduce the\nGraphical Referential Game (GREG) where a speaker must produce a graphical\nutterance to name a visual referent object consisting of combinations of MNIST\ndigits while a listener has to select the corresponding object among distractor\nreferents, given the produced message. The utterances are drawing images\nproduced using dynamical motor primitives combined with a sketching library. To\ntackle GREG we present CURVES: a multimodal contrastive deep learning mechanism\nthat represents the energy (alignment) between named referents and utterances\ngenerated through gradient ascent on the learned energy landscape. We, then,\npresent a set of experiments and metrics based on a systematic compositional\ndataset to evaluate the resulting language. We show that our method allows the\nemergence of a shared, graphical language with compositional properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lemesle_Y/0/1/0/all/0/1\">Yoann Lemesle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1\">Tristan Karch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1\">Romain Laroche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1\">Cl&#xe9;ment Moulin-Frier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models. (arXiv:2210.06475v1 [cs.LG])","link":"http://arxiv.org/abs/2210.06475","description":"<p>We introduce equi-tuning, a novel fine-tuning method that transforms\n(potentially non-equivariant) pretrained models into group equivariant models\nwhile incurring minimum $L_2$ loss between the feature representations of the\npretrained and the equivariant models. Large pretrained models can be\nequi-tuned for different groups to satisfy the needs of various downstream\ntasks. Equi-tuned models benefit from both group equivariance as an inductive\nbias and semantic priors from pretrained models. We provide applications of\nequi-tuning on three different tasks: image classification, compositional\ngeneralization in language, and fairness in natural language generation (NLG).\nWe also provide a novel group-theoretic definition for fairness in NLG. The\neffectiveness of this definition is shown by testing it against a standard\nempirical method of fairness in NLG. We provide experimental results for\nequi-tuning using a variety of pretrained models: Alexnet, Resnet, VGG, and\nDensenet for image classification; RNNs, GRUs, and LSTMs for compositional\ngeneralization; and GPT2 for fairness in NLG. We test these models on benchmark\ndatasets across all considered tasks to show the generality and effectiveness\nof the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Sourya Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattigeri_P/0/1/0/all/0/1\">Prasanna Sattigeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1\">Karthikeyan Natesan Ramamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenthamarakshan_V/0/1/0/all/0/1\">Vijil Chenthamarakshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1\">Kush R. Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_L/0/1/0/all/0/1\">Lav R. Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUMBot: Summarizing Context in Open-Domain Dialogue Systems. (arXiv:2210.06496v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06496","description":"<p>In this paper, we investigate the problem of including relevant information\nas context in open-domain dialogue systems. Most models struggle to identify\nand incorporate important knowledge from dialogues and simply use the entire\nturns as context, which increases the size of the input fed to the model with\nunnecessary information. Additionally, due to the input size limitation of a\nfew hundred tokens of large pre-trained models, regions of the history are not\nincluded and informative parts from the dialogue may be omitted. In order to\nsurpass this problem, we introduce a simple method that substitutes part of the\ncontext with a summary instead of the whole history, which increases the\nability of models to keep track of all the previous relevant information. We\nshow that the inclusion of a summary may improve the answer generation task and\ndiscuss some examples to further understand the system's weaknesses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_R/0/1/0/all/0/1\">Rui Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coheur_L/0/1/0/all/0/1\">Lu&#xed;sa Coheur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subword Segmental Language Modelling for Nguni Languages. (arXiv:2210.06525v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06525","description":"<p>Subwords have become the standard units of text in NLP, enabling efficient\nopen-vocabulary models. With algorithms like byte-pair encoding (BPE), subword\nsegmentation is viewed as a preprocessing step applied to the corpus before\ntraining. This can lead to sub-optimal segmentations for low-resource languages\nwith complex morphologies. We propose a subword segmental language model (SSLM)\nthat learns how to segment words while being trained for autoregressive\nlanguage modelling. By unifying subword segmentation and language modelling,\nour model learns subwords that optimise LM performance. We train our model on\nthe 4 Nguni languages of South Africa. These are low-resource agglutinative\nlanguages, so subword information is critical. As an LM, SSLM outperforms\nexisting approaches such as BPE-based models on average across the 4 languages.\nFurthermore, it outperforms standard subword segmenters on unsupervised\nmorphological segmentation. We also train our model as a word-level sequence\nmodel, resulting in an unsupervised morphological segmenter that outperforms\nexisting methods by a large margin for all 4 languages. Our results show that\nlearning subword segmentation is an effective alternative to existing subword\nsegmenters, enabling the model to discover morpheme-like subwords that improve\nits LM capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meyer_F/0/1/0/all/0/1\">Francois Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buys_J/0/1/0/all/0/1\">Jan Buys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual textual data: an approach through multiple factor analysis. (arXiv:2210.06527v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06527","description":"<p>This paper focuses on the analysis of open-ended questions answered in\ndifferent languages. Closed-ended questions, called contextual variables, are\nasked to all respondents in order to understand the relationships between the\nfree and the closed responses among the different samples since the latter\nassumably affect the word choices. We have developed \"Multiple Factor Analysis\non Generalized Aggregated Lexical Tables\" (MFA-GALT) to jointly study the\nopen-ended responses in different languages through the relationships between\nthe choice of words and the variables that drive this choice. MFA-GALT studies\nif variability among words is structured in the same way by variability among\nvariables, and inversely, from one sample to another. An application on an\ninternational satisfaction survey shows the easy-to-interpret results that are\nproposed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blechin_K/0/1/0/all/0/1\">Kostov Blechin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramon_A/0/1/0/all/0/1\">Alvarez-Esteban Ram&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monica_B/0/1/0/all/0/1\">B&#xe9;cue-Bertaut M&#xf3;nica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francois_H/0/1/0/all/0/1\">Husson Fran&#xe7;ois</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Developing a general-purpose clinical language inference model from a large corpus of clinical notes. (arXiv:2210.06566v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06566","description":"<p>Several biomedical language models have already been developed for clinical\nlanguage inference. However, these models typically utilize general\nvocabularies and are trained on relatively small clinical corpora. We sought to\nevaluate the impact of using a domain-specific vocabulary and a large clinical\ntraining corpus on the performance of these language models in clinical\nlanguage inference. We trained a Bidirectional Encoder Decoder from\nTransformers (BERT) model using a diverse, deidentified corpus of 75 million\ndeidentified clinical notes authored at the University of California, San\nFrancisco (UCSF). We evaluated this model on several clinical language\ninference benchmark tasks: clinical and temporal concept recognition, relation\nextraction and medical language inference. We also evaluated our model on two\ntasks using discharge summaries from UCSF: diagnostic code assignment and\ntherapeutic class inference. Our model performs at par with the best publicly\navailable biomedical language models of comparable sizes on the public\nbenchmark tasks, and is significantly better than these models in a\nwithin-system evaluation on the two tasks using UCSF data. The use of in-domain\nvocabulary appears to improve the encoding of longer documents. The use of\nlarge clinical corpora appears to enhance document encoding and inferential\naccuracy. However, further research is needed to improve abbreviation\nresolution, and numerical, temporal, and implicitly causal inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sushil_M/0/1/0/all/0/1\">Madhumita Sushil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludwig_D/0/1/0/all/0/1\">Dana Ludwig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butte_A/0/1/0/all/0/1\">Atul J. Butte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudrapatna_V/0/1/0/all/0/1\">Vivek A. Rudrapatna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DATScore: Evaluating Translation with Data Augmented Translations. (arXiv:2210.06576v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06576","description":"<p>The rapid development of large pretrained language models has revolutionized\nnot only the field of Natural Language Generation (NLG) but also its\nevaluation. Inspired by the recent work of BARTScore: a metric leveraging the\nBART language model to evaluate the quality of generated text from various\naspects, we introduce DATScore. DATScore uses data augmentation techniques to\nimprove the evaluation of machine translation. Our main finding is that\nintroducing data augmented translations of the source and reference texts is\ngreatly helpful in evaluating the quality of the generated translation. We also\npropose two novel score averaging and term weighting strategies to improve the\noriginal score computing process of BARTScore. Experimental results on WMT show\nthat DATScore correlates better with human meta-evaluations than the other\nrecent state-of-the-art metrics, especially for low-resource languages.\nAblation studies demonstrate the value added by our new scoring strategies.\nMoreover, we report in our extended experiments the performance of DATScore on\n3 NLG tasks other than translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eddine_M/0/1/0/all/0/1\">Moussa Kamal Eddine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_G/0/1/0/all/0/1\">Guokan Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Question Answering with Generation of NQ-like Questions. (arXiv:2210.06599v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06599","description":"<p>Question Answering (QA) systems require a large amount of annotated data\nwhich is costly and time-consuming to gather. Converting datasets of existing\nQA benchmarks are challenging due to different formats and complexities. To\naddress these issues, we propose an algorithm to automatically generate shorter\nquestions resembling day-to-day human communication in the Natural Questions\n(NQ) dataset from longer trivia questions in Quizbowl (QB) dataset by\nleveraging conversion in style among the datasets. This provides an automated\nway to generate more data for our QA systems. To ensure quality as well as\nquantity of data, we detect and remove ill-formed questions using a neural\nclassifier. We demonstrate that in a low resource setting, using the generated\ndata improves the QA performance over the baseline system on both NQ and QB\ndata. Our algorithm improves the scalability of training data while maintaining\nquality of data for QA systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_S/0/1/0/all/0/1\">Saptarashmi Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Shraman Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1\">Hao Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_A/0/1/0/all/0/1\">Abhranil Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Document-level Information Extraction via Imitation Learning. (arXiv:2210.06600v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06600","description":"<p>We present a novel iterative extraction (IterX) model for extracting complex\nrelations, or templates, i.e., N-tuples representing a mapping from named slots\nto spans of text contained within a document. Documents may support zero or\nmore instances of a template of any particular type, leading to the tasks of\nidentifying the templates in a document, and extracting each template's slot\nvalues. Our imitation learning approach relieves the need to use predefined\ntemplate orders to train an extractor and leads to state-of-the-art results on\ntwo established benchmarks -- 4-ary relation extraction on SciREX and template\nextraction on MUC-4 -- as well as a strong baseline on the new BETTER Granular\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunmo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gantt_W/0/1/0/all/0/1\">William Gantt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Weiwei Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tongfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Aaron Steven White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenCQA: Open-ended Question Answering with Charts. (arXiv:2210.06628v1 [cs.LG])","link":"http://arxiv.org/abs/2210.06628","description":"<p>Charts are very popular to analyze data and convey important insights. People\noften analyze visualizations to answer open-ended questions that require\nexplanatory answers. Answering such questions are often difficult and\ntime-consuming as it requires a lot of cognitive and perceptual efforts. To\naddress this challenge, we introduce a new task called OpenCQA, where the goal\nis to answer an open-ended question about a chart with descriptive texts. We\npresent the annotation process and an in-depth analysis of our dataset. We\nimplement and evaluate a set of baselines under three practical settings. In\nthe first setting, a chart and the accompanying article is provided as input to\nthe model. The second setting provides only the relevant paragraph(s) to the\nchart instead of the entire article, whereas the third setting requires the\nmodel to generate an answer solely based on the chart. Our analysis of the\nresults show that the top performing models generally produce fluent and\ncoherent text while they struggle to perform complex logical and arithmetic\nreasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kantharaj_S/0/1/0/all/0/1\">Shankar Kantharaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_X/0/1/0/all/0/1\">Xuan Long Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_R/0/1/0/all/0/1\">Rixie Tiffany Ko Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jia Qing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Enamul Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis. (arXiv:2210.06629v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06629","description":"<p>Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis\ntask which involves four elements from user-generated texts: aspect term,\naspect category, opinion term, and sentiment polarity. Most computational\napproaches focus on some of the ABSA sub-tasks such as tuple (aspect term,\nsentiment polarity) or triplet (aspect term, opinion term, sentiment polarity)\nextraction using either pipeline or joint modeling approaches. Recently,\ngenerative approaches have been proposed to extract all four elements as (one\nor more) quadruplets from text as a single task. In this work, we take a step\nfurther and propose a unified framework for solving ABSA, and the associated\nsub-tasks to improve the performance in few-shot scenarios. To this end, we\nfine-tune a T5 model with instructional prompts in a multi-task learning\nfashion covering all the sub-tasks, as well as the entire quadruple prediction\ntask. In experiments with multiple benchmark data sets, we show that the\nproposed multi-task prompting approach brings performance boost (by absolute\n$6.75$ F1) in the few-shot learning setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varia_S/0/1/0/all/0/1\">Siddharth Varia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halder_K/0/1/0/all/0/1\">Kishaloy Halder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vacareanu_R/0/1/0/all/0/1\">Robert Vacareanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1\">Miguel Ballesteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benajiba_Y/0/1/0/all/0/1\">Yassine Benajiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+John_N/0/1/0/all/0/1\">Neha Anna John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anubhai_R/0/1/0/all/0/1\">Rishita Anubhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Agnostic Multilingual Information Retrieval with Contrastive Learning. (arXiv:2210.06633v1 [cs.IR])","link":"http://arxiv.org/abs/2210.06633","description":"<p>Multilingual information retrieval is challenging due to the lack of training\ndatasets for many low-resource languages. We present an effective method by\nleveraging parallel and non-parallel corpora to improve the pretrained\nmultilingual language models' cross-lingual transfer ability for information\nretrieval. We design the semantic contrastive loss as regular contrastive\nlearning to improve the cross-lingual alignment of parallel sentence pairs, and\nwe propose a new contrastive loss, the language contrastive loss, to leverage\nboth parallel corpora and non-parallel corpora to further improve multilingual\nrepresentation learning. We train our model on an English information retrieval\ndataset, and test its zero-shot transfer ability to other languages. Our\nexperiment results show that our method brings significant improvement to prior\nwork on retrieval performance, while it requires much less computational\neffort. Our model can work well even with a small number of parallel corpora.\nAnd it can be used as an add-on module to any backbone and other tasks. Our\ncode is available at: https://github.com/xiyanghu/multilingualIR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiyang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinchi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1\">Peng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Deguang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunlun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiheng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The COVID That Wasn't: Counterfactual Journalism Using GPT. (arXiv:2210.06644v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06644","description":"<p>In this paper, we explore the use of large language models to assess human\ninterpretations of real world events. To do so, we use a language model trained\nprior to 2020 to artificially generate news articles concerning COVID-19 given\nthe headlines of actual articles written during the pandemic. We then compare\nstylistic qualities of our artificially generated corpus with a news corpus, in\nthis case 5,082 articles produced by CBC News between January 23 and May 5,\n2020. We find our artificially generated articles exhibits a considerably more\nnegative attitude towards COVID and a significantly lower reliance on\ngeopolitical framing. Our methods and results hold importance for researchers\nseeking to simulate large scale cultural processes via recent breakthroughs in\ntext generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_S/0/1/0/all/0/1\">Sil Hamilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piper_A/0/1/0/all/0/1\">Andrew Piper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-grounded Dialog State Tracking. (arXiv:2210.06656v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06656","description":"<p>Knowledge (including structured knowledge such as schema and ontology, and\nunstructured knowledge such as web corpus) is a critical part of dialog\nunderstanding, especially for unseen tasks and domains. Traditionally, such\ndomain-specific knowledge is encoded implicitly into model parameters for the\nexecution of downstream tasks, which makes training inefficient. In addition,\nsuch models are not easily transferable to new tasks with different schemas. In\nthis work, we propose to perform dialog state tracking grounded on knowledge\nencoded externally. We query relevant knowledge of various forms based on the\ndialog context where such information can ground the prediction of dialog\nstates. We demonstrate superior performance of our proposed method over strong\nbaselines, especially in the few-shot learning setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingqiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafran_I/0/1/0/all/0/1\">Izhak Shafran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafey_L/0/1/0/all/0/1\">Laurent El Shafey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltau_H/0/1/0/all/0/1\">Hagen Soltau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SubeventWriter: Iterative Sub-event Sequence Generation with Coherence Controller. (arXiv:2210.06694v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06694","description":"<p>In this paper, we propose a new task of sub-event generation for an unseen\nprocess to evaluate the understanding of the coherence of sub-event actions and\nobjects. To solve the problem, we design SubeventWriter, a sub-event sequence\ngeneration framework with a coherence controller. Given an unseen process, the\nframework can iteratively construct the sub-event sequence by generating one\nsub-event at each iteration. We also design a very effective coherence\ncontroller to decode more coherent sub-events. As our extensive experiments and\nanalysis indicate, SubeventWriter can generate more reliable and meaningful\nsub-event sequences for unseen processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_G/0/1/0/all/0/1\">Ginny Y. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1\">Simon See</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Reinforced User Simulator and Task-oriented Dialog System with Simplified Generative Architecture. (arXiv:2210.06706v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06706","description":"<p>Recently, there has been progress in supervised funetuning pretrained GPT-2\nto build end-to-end task-oriented dialog (TOD) systems. However, online\nreinforcement learning of a GPT-2 based dialog system (DS), together with a\nend-to-end user simulator (US), has not ever been explored. Moreover, a\ndrawback with existing GPT-2 based TOD systems is that they mostly employ the\nwhole dialog history as input, which brings inefficiencies in memory and\ncompute. In this paper, we first propose Simplified Generative Architectures\n(SGA) for DS and US respectively, both based on GPT-2 but using shortened\nhistory. Then, we successfully develop Jointly Reinforced US and DS, called\nSGA-JRUD. Our DS with the proposed SGA, when only supervised trained, achieves\nstate-of-the-art performance on MultiWOZ2.1 and is more compute-efficient in\nboth training and generation. Extensive experiments on MultiWOZ2.1 further show\nthe superiority of SGA-JRUD in both offline and online evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Junlan Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Categorizing Semantic Representations for Neural Machine Translation. (arXiv:2210.06709v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06709","description":"<p>Modern neural machine translation (NMT) models have achieved competitive\nperformance in standard benchmarks. However, they have recently been shown to\nsuffer limitation in compositional generalization, failing to effectively learn\nthe translation of atoms (e.g., words) and their semantic composition (e.g.,\nmodification) from seen compounds (e.g., phrases), and thus suffering from\nsignificantly weakened translation performance on unseen compounds during\ninference. We address this issue by introducing categorization to the source\ncontextualized representations. The main idea is to enhance generalization by\nreducing sparsity and overfitting, which is achieved by finding prototypes of\ntoken representations over the training set and integrating their embeddings\ninto the source encoding. Experiments on a dedicated MT dataset (i.e.,\nCoGnition) show that our method reduces compositional generalization error\nrates by 24\\% error reduction. In addition, our conceptually simple method\ngives consistently better results than the Transformer baseline on a range of\ngeneral MT datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yongjing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yafu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are few(1)-shot Table Reasoners. (arXiv:2210.06710v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06710","description":"<p>Recent literature has shown that large language models (LLMs) are generally\nexcellent few-shot reasoners to solve text reasoning tasks. However, the\ncapability of LLMs on table reasoning tasks is yet to be explored. In this\npaper, we aim at understanding how well LLMs can perform on these table tasks\nwith few-shot in-context learning. Specifically, we evaluate LLMs on popular\ntable QA and fact verification datasets like WikiTableQuestion, FetaQA,\nTabFact, and FEVEROUS and found that LLMs are really competent at complex\nreasoning over table structures. When combined with `chain of thoughts'\nprompting, GPT-3 is able to achieve very strong performance with only a 1-shot\ndemonstration. We further manually study the reasoning chains elicited from\nLLMs and found that these reasoning chains are highly consistent with the\n`ground truth' semantic form. We believe that our study opens new possibilities\nto employ LLMs on different table-based reasoning tasks under few-shot\nscenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-resource Neural Machine Translation with Cross-modal Alignment. (arXiv:2210.06716v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06716","description":"<p>How to achieve neural machine translation with limited parallel data?\nExisting techniques often rely on large-scale monolingual corpora, which is\nimpractical for some low-resource languages. In this paper, we turn to connect\nseveral low-resource languages to a particular high-resource one by additional\nvisual modality. Specifically, we propose a cross-modal contrastive learning\nmethod to learn a shared space for all languages, where both a coarse-grained\nsentence-level objective and a fine-grained token-level one are introduced.\nExperimental results and further analysis show that our method can effectively\nlearn the cross-modal and cross-lingual alignment with a small amount of\nimage-text pairs and achieves significant improvements over the text-only\nbaseline under both zero-shot and few-shot scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qingkai Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LIME: Weakly-Supervised Text Classification Without Seeds. (arXiv:2210.06720v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06720","description":"<p>In weakly-supervised text classification, only label names act as sources of\nsupervision. Predominant approaches to weakly-supervised text classification\nutilize a two-phase framework, where test samples are first assigned\npseudo-labels and are then used to train a neural text classifier. In most\nprevious work, the pseudo-labeling step is dependent on obtaining seed words\nthat best capture the relevance of each class label. We present LIME, a\nframework for weakly-supervised text classification that entirely replaces the\nbrittle seed-word generation process with entailment-based\npseudo-classification. We find that combining weakly-supervised classification\nand textual entailment mitigates shortcomings of both, resulting in a more\nstreamlined and effective classification pipeline. With just an off-the-shelf\ntextual entailment model, LIME outperforms recent baselines in\nweakly-supervised text classification and achieves state-of-the-art in 4\nbenchmarks. We open source our code at https://github.com/seongminp/LIME.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seongmin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihwa Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Relational Reasoning via Connection Subgraph Pretraining. (arXiv:2210.06722v1 [cs.LG])","link":"http://arxiv.org/abs/2210.06722","description":"<p>Few-shot knowledge graph (KG) completion task aims to perform inductive\nreasoning over the KG: given only a few support triplets of a new relation\n$\\bowtie$ (e.g., (chop,$\\bowtie$,kitchen), (read,$\\bowtie$,library), the goal\nis to predict the query triplets of the same unseen relation $\\bowtie$, e.g.,\n(sleep,$\\bowtie$,?). Current approaches cast the problem in a meta-learning\nframework, where the model needs to be first jointly trained over many training\nfew-shot tasks, each being defined by its own relation, so that\nlearning/prediction on the target few-shot task can be effective. However, in\nreal-world KGs, curating many training tasks is a challenging ad hoc process.\nHere we propose Connection Subgraph Reasoner (CSR), which can make predictions\nfor the target few-shot task directly without the need for pre-training on the\nhuman curated set of training tasks. The key to CSR is that we explicitly model\na shared connection subgraph between support and query triplets, as inspired by\nthe principle of eliminative induction. To adapt to specific KG, we design a\ncorresponding self-supervised pretraining scheme with the objective of\nreconstructing automatically sampled connection subgraphs. Our pretrained model\ncan then be directly applied to target few-shot tasks on without the need for\ntraining few-shot tasks. Extensive experiments on real KGs, including NELL,\nFB15K-237, and ConceptNet, demonstrate the effectiveness of our framework: we\nshow that even a learning-free implementation of CSR can already perform\ncompetitively to existing methods on target few-shot tasks; with pretraining,\nCSR can achieve significant gains of up to 52% on the more challenging\ninductive few-shot tasks where the entities are also unseen during\n(pre)training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Out-of-Domain Language Model Performance from Few Examples. (arXiv:2210.06725v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06725","description":"<p>While pretrained language models have exhibited impressive generalization\ncapabilities, they still behave unpredictably under certain domain shifts. In\nparticular, a model may learn a reasoning process on in-domain training data\nthat does not hold for out-of-domain test data. We address the task of\npredicting out-of-domain (OOD) performance in a few-shot fashion: given a few\ntarget-domain examples and a set of models with similar training performance,\ncan we understand how these models will perform on OOD test data? We benchmark\nthe performance on this task when looking at model accuracy on the few-shot\nexamples, then investigate how to incorporate analysis of the models' behavior\nusing feature attributions to better tackle this problem. Specifically, we\nexplore a set of \"factors\" designed to reveal model agreement with certain\npathological heuristics that may indicate worse generalization capabilities. On\ntextual entailment, paraphrase recognition, and a synthetic classification\ntask, we show that attribution-based factors can help rank relative model OOD\nperformance. However, accuracy on a few-shot test set is a surprisingly strong\nbaseline, particularly when the system designer does not have in-depth prior\nknowledge about the domain shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singhal_P/0/1/0/all/0/1\">Prasann Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forristal_J/0/1/0/all/0/1\">Jarad Forristal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explanations from Large Language Models Make Small Reasoners Better. (arXiv:2210.06726v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06726","description":"<p>Integrating free-text explanations to in-context learning of large language\nmodels (LLM) is shown to elicit strong reasoning capabilities along with\nreasonable explanations. In this paper, we consider the problem of leveraging\nthe explanations generated by LLM to improve the training of small reasoners,\nwhich are more favorable in real-production deployment due to their low cost.\nWe systematically explore three explanation generation approaches from LLM and\nutilize a multi-task learning framework to facilitate small models to acquire\nstrong reasoning power together with explanation generation capabilities.\nExperiments on multiple reasoning tasks show that our method can consistently\nand significantly outperform finetuning baselines across different settings,\nand even perform better than finetuning/prompting a 60x larger GPT-3 (175B)\nmodel by up to 9.5% in accuracy. As a side benefit, human evaluation further\nshows that our method can generate high-quality explanations to justify its\npredictions, moving towards the goal of explainable AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinlu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why self-attention is Natural for Sequence-to-Sequence Problems? A Perspective from Symmetries. (arXiv:2210.06741v1 [cs.LG])","link":"http://arxiv.org/abs/2210.06741","description":"<p>In this paper, we show that structures similar to self-attention are natural\nto learn many sequence-to-sequence problems from the perspective of symmetry.\nInspired by language processing applications, we study the orthogonal\nequivariance of seq2seq functions with knowledge, which are functions taking\ntwo inputs -- an input sequence and a ``knowledge'' -- and outputting another\nsequence. The knowledge consists of a set of vectors in the same embedding\nspace as the input sequence, containing the information of the language used to\nprocess the input sequence. We show that orthogonal equivariance in the\nembedding space is natural for seq2seq functions with knowledge, and under such\nequivariance the function must take the form close to the self-attention. This\nshows that network structures similar to self-attention are the right\nstructures to represent the target function of many seq2seq problems. The\nrepresentation can be further refined if a ``finite information principle'' is\nconsidered, or a permutation equivariance holds for the elements of the input\nsequence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1\">Lexing Ying</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shortcomings of Question Answering Based Factuality Frameworks for Error Localization. (arXiv:2210.06748v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06748","description":"<p>Despite recent progress in abstractive summarization, models often generate\nsummaries with factual errors. Numerous approaches to detect these errors have\nbeen proposed, the most popular of which are question answering (QA)-based\nfactuality metrics. These have been shown to work well at predicting\nsummary-level factuality and have potential to localize errors within\nsummaries, but this latter capability has not been systematically evaluated in\npast research. In this paper, we conduct the first such analysis and find that,\ncontrary to our expectations, QA-based frameworks fail to correctly identify\nerror spans in generated summaries and are outperformed by trivial exact match\nbaselines. Our analysis reveals a major reason for such poor localization:\nquestions generated by the QG module often inherit errors from non-factual\nsummaries which are then propagated further into downstream modules. Moreover,\neven human-in-the-loop question generation cannot easily offset these problems.\nOur experiments conclusively show that there exist fundamental issues with\nlocalization using the QA framework which cannot be fixed solely by stronger QA\nand QG models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamoi_R/0/1/0/all/0/1\">Ryo Kamoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Unintended Memorization in Language Models via Alternating Teaching. (arXiv:2210.06772v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06772","description":"<p>Recent research has shown that language models have a tendency to memorize\nrare or unique sequences in the training corpora which can thus leak sensitive\nattributes of user data. We employ a teacher-student framework and propose a\nnovel approach called alternating teaching to mitigate unintended memorization\nin sequential modeling. In our method, multiple teachers are trained on\ndisjoint training sets whose privacy one wishes to protect, and teachers'\npredictions supervise the training of a student model in an alternating manner\nat each time step. Experiments on LibriSpeech datasets show that the proposed\nmethod achieves superior privacy-preserving results than other counterparts. In\ncomparison with no prevention for unintended memorization, the overall utility\nloss is small when training records are sufficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuedong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_F/0/1/0/all/0/1\">Fuchun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re3: Generating Longer Stories With Recursive Reprompting and Revision. (arXiv:2210.06774v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06774","description":"<p>We consider the problem of automatically generating longer stories of over\ntwo thousand words. Compared to prior work on shorter stories, long-range plot\ncoherence and relevance are more central challenges here. We propose the\nRecursive Reprompting and Revision framework (Re3) to address these challenges\nby (a) prompting a general-purpose language model to construct a structured\noverarching plan, and (b) generating story passages by repeatedly injecting\ncontextual information from both the plan and current story state into a\nlanguage model prompt. We then revise by (c) reranking different continuations\nfor plot coherence and premise relevance, and finally (d) editing the best\ncontinuation for factual consistency. Compared to similar-length stories\ngenerated directly from the same base model, human evaluators judged\nsubstantially more of Re3's stories as having a coherent overarching plot (by\n14% absolute increase), and relevant to the given initial premise (by 20%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Closed-book Question Generation via Contrastive Learning. (arXiv:2210.06781v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06781","description":"<p>Question Generation (QG) is a fundamental NLP task for many downstream\napplications. Recent studies on open-book QG, where supportive question-context\npairs are provided to models, have achieved promising progress. However,\ngenerating natural questions under a more practical closed-book setting that\nlacks these supporting documents still remains a challenge. In this work, to\nlearn better representations from semantic information hidden in\nquestion-answer pairs under the closed-book setting, we propose a new QG model\nempowered by a contrastive learning module and an answer reconstruction module.\nWe present a new closed-book QA dataset -- WikiCQA involving abstractive long\nanswers collected from a wiki-style website. In the experiments, we validate\nthe proposed QG model on both public datasets and the new WikiCQA dataset.\nEmpirical results show that the proposed QG model outperforms baselines in both\nautomatic evaluation and human evaluation. In addition, we show how to leverage\nthe proposed model to improve existing closed-book QA systems. We observe that\nby pre-training a closed-book QA model on our generated synthetic QA pairs,\nsignificant QA improvement can be achieved on both seen and unseen datasets,\nwhich further demonstrates the effectiveness of our QG model for enhancing\nunsupervised and semi-supervised QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiangjue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiaying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caverlee_J/0/1/0/all/0/1\">James Caverlee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDW-ASL: A Dynamic System to Generate Large Scale Dataset for Continuous American Sign Language. (arXiv:2210.06791v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06791","description":"<p>Despite tremendous progress in natural language processing using deep\nlearning techniques in recent years, sign language production and comprehension\nhas advanced very little. One critical barrier is the lack of largescale\ndatasets available to the public due to the unbearable cost of labeled data\ngeneration. Efforts to provide public data for American Sign Language (ASL)\ncomprehension have yielded two datasets, comprising more than thousand video\nclips. These datasets are large enough to enable a meaningful start to deep\nlearning research on sign languages but are far too small to lead to any\nsolution that can be practically deployed. So far, there is still no suitable\ndataset for ASL production. We proposed a system that can generate large scale\nASL datasets for continuous ASL. It is suitable for general ASL processing and\nis particularly useful for ASL production. The continuous ASL dataset contains\nEnglish labeled human articulations in condensed body pose data formats. To\nbetter serve the research community, we are releasing the first version of our\nASL dataset, which contains 30k sentences, 416k words, a vocabulary of 18k\nwords, in a total of 104 hours. This is the largest continuous sign language\ndataset published to date in terms of video duration. We also describe a system\nthat can evolve and expand the dataset to incorporate better data processing\ntechniques and more contents when available. It is our hope that the release of\nthis ASL dataset and the sustainable dataset generation system to the public\nwill propel better deep-learning research in ASL natural language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yehong Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Long-tail Generalization with Likelihood Splits. (arXiv:2210.06799v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06799","description":"<p>In order to reliably process natural language, NLP systems must generalize to\nthe long tail of rare utterances. We propose a method to create challenging\nbenchmarks that require generalizing to the tail of the distribution by\nre-splitting existing datasets. We create 'Likelihood splits' where examples\nthat are assigned lower likelihood by a pre-trained language model (LM) are\nplaced in the test set, and more likely examples are in the training set. This\nsimple approach can be customized to construct meaningful train-test splits for\na wide range of tasks. Likelihood splits are more challenging than random\nsplits: relative error rates of state-of-the-art models on our splits increase\nby 59% for semantic parsing on Spider, 77% for natural language inference on\nSNLI, and 38% for yes/no question answering on BoolQ compared with the\ncorresponding random splits. Moreover, Likelihood splits create fairer\nbenchmarks than adversarial filtering; when the LM used to create the splits is\nused as the task model, our splits do not adversely penalize the LM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godbole_A/0/1/0/all/0/1\">Ameya Godbole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on Finding Spans. (arXiv:2210.06824v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06824","description":"<p>We present an empirical study on methods for span finding, the selection of\nconsecutive tokens in text for some downstream tasks. We focus on approaches\nthat can be employed in training end-to-end information extraction systems. We\nrecognize there is no silver bullet that can simply solve all downstream tasks\nwell without considering task properties and provide our observations to help\nwith design choices in the future: 1) tagging method usually yields a higher\nprecision while span enumeration and boundary prediction prefer a higher\nrecall; 2) span type information can benefit boundary prediction approach; 3)\nadditional contextualization does not help span finding in most cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Weiwei Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Boyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunmo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tongfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Annotation: Can Language Learners Contribute?. (arXiv:2210.06828v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06828","description":"<p>Researchers have traditionally recruited native speakers to provide\nannotations for the widely used benchmark datasets. But there are languages for\nwhich recruiting native speakers is difficult, and it would help to get\nlearners of those languages to annotate the data. In this paper, we investigate\nwhether language learners can contribute annotations to the benchmark datasets.\nIn a carefully controlled annotation experiment, we recruit 36 language\nlearners, provide two types of additional resources (dictionaries and\nmachine-translated sentences), and perform mini-tests to measure their language\nproficiency. We target three languages, English, Korean, and Indonesian, and\nfour NLP tasks, sentiment analysis, natural language inference, named entity\nrecognition, and machine reading comprehension. We find that language learners,\nespecially those with intermediate or advanced language proficiency, are able\nto provide fairly accurate labels with the help of additional resources.\nMoreover, we show that data annotation improves learners' language proficiency\nin terms of vocabulary and grammar. The implication of our findings is that\nbroadening the annotation task to include language learners can open up the\nopportunity to build benchmark datasets for languages for which it is difficult\nto recruit native speakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_H/0/1/0/all/0/1\">Haneul Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putri_R/0/1/0/all/0/1\">Rifki Afina Putri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Changyoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">So-Yeon Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble Creation via Anchored Regularization for Unsupervised Aspect Extraction. (arXiv:2210.06829v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06829","description":"<p>Aspect Based Sentiment Analysis is the most granular form of sentiment\nanalysis that can be performed on the documents / sentences. Besides delivering\nthe most insights at a finer grain, it also poses equally daunting challenges.\nOne of them being the shortage of labelled data. To bring in value right out of\nthe box for the text data being generated at a very fast pace in today's world,\nunsupervised aspect-based sentiment analysis allows us to generate insights\nwithout investing time or money in generating labels. From topic modelling\napproaches to recent deep learning-based aspect extraction models, this domain\nhas seen a lot of development. One of the models that we improve upon is ABAE\nthat reconstructs the sentences as a linear combination of aspect terms present\nin it, In this research we explore how we can use information from another\nunsupervised model to regularize ABAE, leading to better performance. We\ncontrast it with baseline rule based ensemble and show that the ensemble\nmethods work better than the individual models and the regularization based\nensemble performs better than the rule-based one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhandekar_P/0/1/0/all/0/1\">Pulah Dhandekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_M/0/1/0/all/0/1\">Manu Joseph</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of BioASQ 2022: The tenth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering. (arXiv:2210.06852v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06852","description":"<p>This paper presents an overview of the tenth edition of the BioASQ challenge\nin the context of the Conference and Labs of the Evaluation Forum (CLEF) 2022.\nBioASQ is an ongoing series of challenges that promotes advances in the domain\nof large-scale biomedical semantic indexing and question answering. In this\nedition, the challenge was composed of the three established tasks a, b, and\nSynergy, and a new task named DisTEMIST for automatic semantic annotation and\ngrounding of diseases from clinical content in Spanish, a key concept for\nsemantic indexing and search engines of literature and clinical records. This\nyear, BioASQ received more than 170 distinct systems from 38 teams in total for\nthe four different tasks of the challenge. As in previous years, the majority\nof the competing systems outperformed the strong baselines, indicating the\ncontinuous advancement of the state-of-the-art in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1\">Anastasios Nentidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsimpras_G/0/1/0/all/0/1\">Georgios Katsimpras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandorou_E/0/1/0/all/0/1\">Eirini Vandorou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1\">Anastasia Krithara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miranda_Escalada_A/0/1/0/all/0/1\">Antonio Miranda-Escalada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasco_L/0/1/0/all/0/1\">Luis Gasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krallinger_M/0/1/0/all/0/1\">Martin Krallinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CS-Insights: A System for Analyzing Computer Science Research. (arXiv:2210.06878v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06878","description":"<p>This paper presents CS-Insights, an interactive web application to analyze\ncomputer science publications from DBLP through multiple perspectives. The\ndedicated interfaces allow its users to identify trends in research activity,\nproductivity, accessibility, author's productivity, venues' statistics, topics\nof interest, and the impact of computer science research on other fields.\nCS-Insightsis publicly available, and its modular architecture can be easily\nadapted to domains other than computer science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kull_L/0/1/0/all/0/1\">Lennart K&#xfc;ll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Algorithms for Weighted Pushdown Automata. (arXiv:2210.06884v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06884","description":"<p>Weighted pushdown automata (WPDAs) are at the core of many natural language\nprocessing tasks, like syntax-based statistical machine translation and\ntransition-based dependency parsing. As most existing dynamic programming\nalgorithms are designed for context-free grammars (CFGs), algorithms for PDAs\noften resort to a PDA-to-CFG conversion. In this paper, we develop novel\nalgorithms that operate directly on WPDAs. Our algorithms are inspired by\nLang's algorithm, but use a more general definition of pushdown automaton and\neither reduce the space requirements by a factor of $|\\Gamma|$ (the size of the\nstack alphabet) or reduce the runtime by a factor of more than $|Q|$ (the\nnumber of states). When run on the same class of PDAs as Lang's algorithm, our\nalgorithm is both more space-efficient by a factor of $|\\Gamma|$ and more\ntime-efficient by a factor of $|Q| \\cdot |\\Gamma|$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Butoi_A/0/1/0/all/0/1\">Alexandra Butoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuSell_B/0/1/0/all/0/1\">Brian DuSell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Evaluation of the Plausibility and Faithfulness of Sentiment Analysis Explanations. (arXiv:2210.06916v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06916","description":"<p>Current Explainable AI (ExAI) methods, especially in the NLP field, are\nconducted on various datasets by employing different metrics to evaluate\nseveral aspects. The lack of a common evaluation framework is hindering the\nprogress tracking of such methods and their wider adoption. In this work,\ninspired by offline information retrieval, we propose different metrics and\ntechniques to evaluate the explainability of SA models from two angles. First,\nwe evaluate the strength of the extracted \"rationales\" in faithfully explaining\nthe predicted outcome. Second, we measure the agreement between ExAI methods\nand human judgment on a homegrown dataset1 to reflect on the rationales\nplausibility. Our conducted experiments comprise four dimensions: (1) the\nunderlying architectures of SA models, (2) the approach followed by the ExAI\nmethod, (3) the reasoning difficulty, and (4) the homogeneity of the\nground-truth rationales. We empirically demonstrate that anchors explanations\nare more aligned with the human judgment and can be more confident in\nextracting supporting rationales. As can be foreseen, the reasoning complexity\nof sentiment is shown to thwart ExAI methods from extracting supporting\nevidence. Moreover, a remarkable discrepancy is discerned between the results\nof different explainability methods on the various architectures suggesting the\nneed for consolidation to observe enhanced performance. Predominantly,\ntransformers are shown to exhibit better explainability than convolutional and\nrecurrent architectures. Our work paves the way towards designing more\ninterpretable NLP models and enabling a common evaluation ground for their\nrelative strengths and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zini_J/0/1/0/all/0/1\">Julia El Zini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_M/0/1/0/all/0/1\">Mohamad Mansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousi_B/0/1/0/all/0/1\">Basel Mousi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awad_M/0/1/0/all/0/1\">Mariette Awad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automotive Multilingual Fault Diagnosis. (arXiv:2210.06918v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06918","description":"<p>Automated fault diagnosis can facilitate diagnostics assistance, speedier\ntroubleshooting, and better-organised logistics. Currently, AI-based\nprognostics and health management in the automotive industry ignore the textual\ndescriptions of the experienced problems or symptoms. With this study, however,\nwe show that a multilingual pre-trained Transformer can effectively classify\nthe textual claims from a large company with vehicle fleets, despite the task's\nchallenging nature due to the 38 languages and 1,357 classes involved. Overall,\nwe report an accuracy of more than 80% for high-frequency classes and above 60%\nfor above-low-frequency classes, bringing novel evidence that multilingual\nclassification can benefit automotive troubleshooting management.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pavlopoulos_J/0/1/0/all/0/1\">John Pavlopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romell_A/0/1/0/all/0/1\">Alv Romell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curman_J/0/1/0/all/0/1\">Jacob Curman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinert_O/0/1/0/all/0/1\">Olof Steinert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindgren_T/0/1/0/all/0/1\">Tony Lindgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borg_M/0/1/0/all/0/1\">Markus Borg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Ambiguity, Grammaticality and Complexity Probes. (arXiv:2210.06928v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06928","description":"<p>It is unclear whether, how and where large pre-trained language models\ncapture subtle linguistic traits like ambiguity, grammaticality and sentence\ncomplexity. We present results of automatic classification of these traits and\ncompare their viability and patterns across representation types. We\ndemonstrate that template-based datasets with surface-level artifacts should\nnot be used for probing, careful comparisons with baselines should be done and\nthat t-SNE plots should not be used to determine the presence of a feature\namong dense vectors representations. We also show how features might be highly\nlocalized in the layers for these models and get lost in the upper layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Sunit Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zouhar_V/0/1/0/all/0/1\">Vil&#xe9;m Zouhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Explainability of Natural Language Processing Deep Models. (arXiv:2210.06929v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06929","description":"<p>While there has been a recent explosion of work on ExplainableAI ExAI on deep\nmodels that operate on imagery and tabular data, textual datasets present new\nchallenges to the ExAI community. Such challenges can be attributed to the lack\nof input structure in textual data, the use of word embeddings that add to the\nopacity of the models and the difficulty of the visualization of the inner\nworkings of deep models when they are trained on textual data.\n</p>\n<p>Lately, methods have been developed to address the aforementioned challenges\nand present satisfactory explanations on Natural Language Processing (NLP)\nmodels. However, such methods are yet to be studied in a comprehensive\nframework where common challenges are properly stated and rigorous evaluation\npractices and metrics are proposed. Motivated to democratize ExAI methods in\nthe NLP field, we present in this work a survey that studies model-agnostic as\nwell as model-specific explainability methods on NLP models. Such methods can\neither develop inherently interpretable NLP models or operate on pre-trained\nmodels in a post-hoc manner. We make this distinction and we further decompose\nthe methods into three categories according to what they explain: (1) word\nembeddings (input-level), (2) inner workings of NLP models (processing-level)\nand (3) models' decisions (output-level). We also detail the different\nevaluation approaches interpretability methods in the NLP field. Finally, we\npresent a case-study on the well-known neural machine translation in an\nappendix and we propose promising future research directions for ExAI in the\nNLP field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zini_J/0/1/0/all/0/1\">Julia El Zini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awad_M/0/1/0/all/0/1\">Mariette Awad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Zero Resource Speech Recognition Base on Self-Supervise Pre-Trained Acoustic Models. (arXiv:2210.06936v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06936","description":"<p>Labeled audio data is insufficient to build satisfying speech recognition\nsystems for most of the languages in the world. There have been some\nzero-resource methods trying to perform phoneme or word-level speech\nrecognition without labeled audio data of the target language, but the error\nrate of these methods is usually too high to be applied in real-world\nscenarios. Recently, the representation ability of self-supervise pre-trained\nmodels has been found to be extremely beneficial in zero-resource phoneme\nrecognition. As far as we are concerned, this paper is the first attempt to\nextend the use of pre-trained models into word-level zero-resource speech\nrecognition. This is done by fine-tuning the pre-trained models on IPA phoneme\ntranscriptions and decoding with a language model trained on extra texts.\nExperiments on Wav2vec 2.0 and HuBERT models show that this method can achieve\nless than 20% word error rate on some languages, and the average error rate on\n8 languages is 33.77%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suo_H/0/1/0/all/0/1\">Hongbin Suo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yulong Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differential Bias: On the Perceptibility of Stance Imbalance in Argumentation. (arXiv:2210.06970v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06970","description":"<p>Most research on natural language processing treats bias as an absolute\nconcept: Based on a (probably complex) algorithmic analysis, a sentence, an\narticle, or a text is classified as biased or not. Given the fact that for\nhumans the question of whether a text is biased can be difficult to answer or\nis answered contradictory, we ask whether an \"absolute bias classification\" is\na promising goal at all. We see the problem not in the complexity of\ninterpreting language phenomena but in the diversity of sociocultural\nbackgrounds of the readers, which cannot be handled uniformly: To decide\nwhether a text has crossed the proverbial line between non-biased and biased is\nsubjective. By asking \"Is text X more [less, equally] biased than text Y?\" we\npropose to analyze a simpler problem, which, by its construction, is rather\nindependent of standpoints, views, or sociocultural aspects. In such a model,\nbias becomes a preference relation that induces a partial ordering from least\nbiased to most biased texts without requiring a decision on where to draw the\nline. A prerequisite for this kind of bias model is the ability of humans to\nperceive relative bias differences in the first place. In our research, we\nselected a specific type of bias in argumentation, the stance bias, and\ndesigned a crowdsourcing study showing that differences in stance bias are\nperceptible when (light) support is provided through training or visual aid.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palomino_A/0/1/0/all/0/1\">Alonso Palomino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Khatib_K/0/1/0/all/0/1\">Khalid Al-Khatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tone prediction and orthographic conversion for Basaa. (arXiv:2210.06986v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06986","description":"<p>In this paper, we present a seq2seq approach for transliterating missionary\nBasaa orthographies into the official orthography. Our model uses pre-trained\nBasaa missionary and official orthography corpora using BERT. Since Basaa is a\nlow-resource language, we have decided to use the mT5 model for our project.\nBefore training our model, we pre-processed our corpora by eliminating\none-to-one correspondences between spellings and unifying characters variably\ncontaining either one to two characters into single-character form. Our best\nmT5 model achieved a CER equal to 12.6747 and a WER equal to 40.1012.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikitin_I/0/1/0/all/0/1\">Ilya Nikitin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_B/0/1/0/all/0/1\">Brian O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safonova_A/0/1/0/all/0/1\">Anastasia Safonova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Segmentation Approaches for Neural Machine Translation of Code-Switched Egyptian Arabic-English Text. (arXiv:2210.06990v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06990","description":"<p>Data sparsity is one of the main challenges posed by Code-switching (CS),\nwhich is further exacerbated in the case of morphologically rich languages. For\nthe task of Machine Translation (MT), morphological segmentation has proven\nsuccessful in alleviating data sparsity in monolingual contexts; however, it\nhas not been investigated for CS settings. In this paper, we study the\neffectiveness of different segmentation approaches on MT performance, covering\nmorphology-based and frequency-based segmentation techniques. We experiment on\nMT from code-switched Arabic-English to English. We provide detailed analysis,\nexamining a variety of conditions, such as data size and sentences with\ndifferent degrees in CS. Empirical results show that morphology-aware\nsegmenters perform the best in segmentation tasks but under-perform in MT.\nNevertheless, we find that the choice of the segmentation setup to use for MT\nis highly dependent on the data size. For extreme low-resource scenarios, a\ncombination of frequency and morphology-based segmentations is shown to perform\nthe best. For more resourced settings, such a combination does not bring\nsignificant improvements over the use of frequency-based segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaser_M/0/1/0/all/0/1\">Marwa Gaser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mager_M/0/1/0/all/0/1\">Manuel Mager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamed_I/0/1/0/all/0/1\">Injy Hamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdennadher_S/0/1/0/all/0/1\">Slim Abdennadher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DICTDIS: Dictionary Constrained Disambiguation for Improved NMT. (arXiv:2210.06996v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06996","description":"<p>Domain-specific neural machine translation (NMT) systems (e.g., in\neducational applications) are socially significant with the potential to help\nmake information accessible to a diverse set of users in multilingual\nsocieties. It is desirable that such NMT systems be lexically constrained and\ndraw from domain-specific dictionaries. Dictionaries could present multiple\ncandidate translations for a source words/phrases on account of the polysemous\nnature of words. The onus is then on the NMT model to choose the contextually\nmost appropriate candidate. Prior work has largely ignored this problem and\nfocused on the single candidate setting where the target word or phrase is\nreplaced by a single constraint. In this work we present DICTDIS, a lexically\nconstrained NMT system that disambiguates between multiple candidate\ntranslations derived from dictionaries. We achieve this by augmenting training\ndata with multiple dictionary candidates to actively encourage disambiguation\nduring training. We demonstrate the utility of DICTDIS via extensive\nexperiments on English-Hindi sentences in a variety of domains including news,\nfinance, medicine and engineering. We obtain superior disambiguation\nperformance on all domains with improved fluency in some domains of up to 4\nBLEU points, when compared with existing approaches for lexically constrained\nand unconstrained NMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1\">Ayush Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anonymizing Speech with Generative Adversarial Networks to Preserve Speaker Privacy. (arXiv:2210.07002v1 [cs.SD])","link":"http://arxiv.org/abs/2210.07002","description":"<p>In order to protect the privacy of speech data, speaker anonymization aims\nfor hiding the identity of a speaker by changing the voice in speech\nrecordings. This typically comes with a privacy-utility trade-off between\nprotection of individuals and usability of the data for downstream\napplications. One of the challenges in this context is to create non-existent\nvoices that sound as natural as possible.\n</p>\n<p>In this work, we propose to tackle this issue by generating speaker\nembeddings using a generative adversarial network with Wasserstein distance as\ncost function. By incorporating these artificial embeddings into a\nspeech-to-text-to-speech pipeline, we outperform previous approaches in terms\nof privacy and utility. According to standard objective metrics and human\nevaluation, our approach generates intelligible and content-preserving yet\nprivacy-protecting versions of the original recordings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meyer_S/0/1/0/all/0/1\">Sarina Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tilli_P/0/1/0/all/0/1\">Pascal Tilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denisov_P/0/1/0/all/0/1\">Pavel Denisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lux_F/0/1/0/all/0/1\">Florian Lux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_J/0/1/0/all/0/1\">Julia Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ComSearch: Equation Searching with Combinatorial Strategy for Solving Math Word Problems with Weak Supervision. (arXiv:2210.07017v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07017","description":"<p>Previous studies have introduced a weakly-supervised paradigm for solving\nmath word problems requiring only the answer value annotation. While these\nmethods search for correct value equation candidates as pseudo labels, they\nsearch among a narrow sub-space of the enormous equation space. To address this\nproblem, we propose a novel search algorithm with combinatorial strategy\n\\textbf{ComSearch}, which can compress the search space by excluding\nmathematically equivalent equations. The compression allows the searching\nalgorithm to enumerate all possible equations and obtain high-quality data. We\ninvestigate the noise in the pseudo labels that hold wrong mathematical logic,\nwhich we refer to as the \\textit{false-matching} problem, and propose a ranking\nmodel to denoise the pseudo labels. Our approach holds a flexible framework to\nutilize two existing supervised math word problem solvers to train pseudo\nlabels, and both achieve state-of-the-art performance in the weak supervision\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1\">Wenyu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianhao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CROP: Zero-shot Cross-lingual Named Entity Recognition with Multilingual Labeled Sequence Translation. (arXiv:2210.07022v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07022","description":"<p>Named entity recognition (NER) suffers from the scarcity of annotated\ntraining data, especially for low-resource languages without labeled data.\nCross-lingual NER has been proposed to alleviate this issue by transferring\nknowledge from high-resource languages to low-resource languages via aligned\ncross-lingual representations or machine translation results. However, the\nperformance of cross-lingual NER methods is severely affected by the\nunsatisfactory quality of translation or label projection. To address these\nproblems, we propose a Cross-lingual Entity Projection framework (CROP) to\nenable zero-shot cross-lingual NER with the help of a multilingual labeled\nsequence translation model. Specifically, the target sequence is first\ntranslated into the source language and then tagged by a source NER model. We\nfurther adopt a labeled sequence translation model to project the tagged\nsequence back to the target language and label the target raw sentence.\nUltimately, the whole pipeline is integrated into an end-to-end model by the\nway of self-training. Experimental results on two benchmarks demonstrate that\nour method substantially outperforms the previous strong baseline by a large\nmargin of +3~7 F1 scores and achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuwei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-based Connective Prediction Method for Fine-grained Implicit Discourse Relation Recognition. (arXiv:2210.07032v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07032","description":"<p>Due to the absence of connectives, implicit discourse relation recognition\n(IDRR) is still a challenging and crucial task in discourse analysis. Most of\nthe current work adopted multitask learning to aid IDRR through explicit\ndiscourse relation recognition (EDRR) or utilized dependencies between\ndiscourse relation labels to constrain model predictions. But these methods\nstill performed poorly on fine-grained IDRR and even utterly misidentified on\nmost of the few-shot discourse relation classes. To address these problems, we\npropose a novel Prompt-based Connective Prediction (PCP) method for IDRR. Our\nmethod instructs large-scale pre-trained models to use knowledge relevant to\ndiscourse relation and utilizes the strong correlation between connectives and\ndiscourse relation to help the model recognize implicit discourse relations.\nExperimental results show that our method surpasses the current\nstate-of-the-art model and achieves significant improvements on those\nfine-grained few-shot discourse relation. Moreover, our approach is able to be\ntransferred to EDRR and obtain acceptable results. Our code is released in\nhttps://github.com/zh-i9/PCP-for-IDRR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1\">Man Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanbin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Meirong Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spontaneous Emerging Preference in Two-tower Language Model. (arXiv:2210.07041v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07041","description":"<p>The ever-growing size of the foundation language model has brought\nsignificant performance gains in various types of downstream tasks. With the\nexistence of side-effects brought about by the large size of the foundation\nlanguage model such as deployment cost, availability issues, and environmental\ncost, there is some interest in exploring other possible directions, such as a\ndivide-and-conquer scheme. In this paper, we are asking a basic question: are\nlanguage processes naturally dividable? We study this problem with a simple\ntwo-tower language model setting, where two language models with identical\nconfigurations are trained side-by-side cooperatively. With this setting, we\ndiscover the spontaneous emerging preference phenomenon, where some of the\ntokens are consistently better predicted by one tower while others by another\ntower. This phenomenon is qualitatively stable, regardless of model\nconfiguration and type, suggesting this as an intrinsic property of natural\nlanguage. This study suggests that interesting properties of natural language\nare still waiting to be discovered, which may aid the future development of\nnatural language processing techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhengqi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toyoizumi_T/0/1/0/all/0/1\">Taro Toyoizumi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation. (arXiv:2210.07054v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07054","description":"<p>Sign language gloss translation aims to translate the sign glosses into\nspoken language texts, which is challenging due to the scarcity of labeled\ngloss-text parallel data. Back translation (BT), which generates\npseudo-parallel data by translating in-domain spoken language texts into sign\nglosses, has been applied to alleviate the data scarcity problem. However, the\nlack of large-scale high-quality domain spoken language text data limits the\neffect of BT. In this paper, to overcome the limitation, we propose a Prompt\nbased domain text Generation (PGEN) approach to produce the large-scale\nin-domain spoken language text data. Specifically, PGEN randomly concatenates\nsentences from the original in-domain spoken language text data as prompts to\ninduce a pre-trained language model (i.e., GPT-2) to generate spoken language\ntexts in a similar style. Experimental results on three benchmarks of sign\nlanguage gloss translation in varied languages demonstrate that BT with spoken\nlanguage texts generated by PGEN significantly outperforms the compared\nmethods. In addition, as the scale of spoken language texts generated by PGEN\nincreases, the BT technique can achieve further improvements, demonstrating the\neffectiveness of our approach. We release the code and data for facilitating\nfuture research in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jinhui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Open-World Lottery Ticket for Out-of-Domain Intent Classification. (arXiv:2210.07071v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07071","description":"<p>Most existing methods of Out-of-Domain (OOD) intent classification, which\nrely on extensive auxiliary OOD corpora or specific training paradigms, are\nunderdeveloped in the underlying principle that the models should have\ndifferentiated confidence in In- and Out-of-domain intent. In this work, we\ndemonstrate that calibrated subnetworks can be uncovered by pruning the\n(poor-calibrated) overparameterized model. Calibrated confidence provided by\nthe subnetwork can better distinguish In- and Out-of-domain. Furthermore, we\ntheoretically bring new insights into why temperature scaling can differentiate\nIn- and Out-of-Domain intent and empirically extend the Lottery Ticket\nHypothesis to the open-world setting. Extensive experiments on three real-world\ndatasets demonstrate our approach can establish consistent improvements\ncompared with a suite of competitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yunhua Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiju Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing. (arXiv:2210.07074v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07074","description":"<p>A bottleneck to developing Semantic Parsing (SP) models is the need for a\nlarge volume of human-labeled training data. Given the complexity and cost of\nhuman annotation for SP, labeled data is often scarce, particularly in\nmultilingual settings. Large Language Models (LLMs) excel at SP given only a\nfew examples, however LLMs are unsuitable for runtime systems which require low\nlatency. In this work, we propose CLASP, a simple method to improve\nlow-resource SP for moderate-sized models: we generate synthetic data from\nAlexaTM 20B to augment the training set for a model 40x smaller (500M\nparameters). We evaluate on two datasets in low-resource settings: English\nPIZZA, containing either 348 or 16 real examples, and mTOP cross-lingual\nzero-shot, where training data is available only in English, and the model must\ngeneralize to four new languages. On both datasets, we show significant\nimprovements over strong baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosenbaum_A/0/1/0/all/0/1\">Andy Rosenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltan_S/0/1/0/all/0/1\">Saleh Soltan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamza_W/0/1/0/all/0/1\">Wael Hamza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_A/0/1/0/all/0/1\">Amir Saffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damonte_M/0/1/0/all/0/1\">Macro Damonte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groves_I/0/1/0/all/0/1\">Isabel Groves</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query Expansion Using Contextual Clue Sampling with Language Models. (arXiv:2210.07093v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07093","description":"<p>Query expansion is an effective approach for mitigating vocabulary mismatch\nbetween queries and documents in information retrieval. One recent line of\nresearch uses language models to generate query-related contexts for expansion.\nAlong this line, we argue that expansion terms from these contexts should\nbalance two key aspects: diversity and relevance. The obvious way to increase\ndiversity is to sample multiple contexts from the language model. However, this\ncomes at the cost of relevance, because there is a well-known tendency of\nmodels to hallucinate incorrect or irrelevant contexts. To balance these two\nconsiderations, we propose a combination of an effective filtering strategy and\nfusion of the retrieved documents based on the generation probability of each\ncontext. Our lexical matching based approach achieves a similar top-5/top-20\nretrieval accuracy and higher top-100 accuracy compared with the\nwell-established dense retrieval model DPR, while reducing the index size by\nmore than 96%. For end-to-end QA, the reader model also benefits from our\nmethod and achieves the highest Exact-Match score against several competitive\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Context into Subword Vocabularies. (arXiv:2210.07095v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07095","description":"<p>Most current popular subword tokenizers are trained based on word frequency\nstatistics over a corpus, without considering information about co-occurrence\nor context. Nevertheless, the resulting vocabularies are used in language\nmodels' highly contextualized settings. We present SaGe, a tokenizer that\ntailors subwords for their downstream use by baking in the contextualized\nsignal at the vocabulary creation phase. We show that SaGe does a better job\nthan current widespread tokenizers in keeping token contexts cohesive, while\nnot incurring a large price in terms of encoding efficiency or domain\nrobustness. SaGe improves performance on English GLUE classification tasks as\nwell as on NER, and on Inference and NER in Turkish, demonstrating its\nrobustness to language properties such as morphological exponence and\nagglutination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yehezkel_S/0/1/0/all/0/1\">Shaked Yehezkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence. (arXiv:2210.07109v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07109","description":"<p>AI researchers have posited Dungeons and Dragons (D&amp;D) as a challenge problem\nto test systems on various language-related capabilities. In this paper, we\nframe D&amp;D specifically as a dialogue system challenge, where the tasks are to\nboth generate the next conversational turn in the game and predict the state of\nthe game given the dialogue history. We create a gameplay dataset consisting of\nnearly 900 games, with a total of 7,000 players, 800,000 dialogue turns,\n500,000 dice rolls, and 58 million words. We automatically annotate the data\nwith partial state information about the game play. We train a large language\nmodel (LM) to generate the next game turn, conditioning it on different\ninformation. The LM can respond as a particular character or as the player who\nruns the game--i.e., the Dungeon Master (DM). It is trained to produce dialogue\nthat is either in-character (roleplaying in the fictional world) or\nout-of-character (discussing rules or strategy). We perform a human evaluation\nto determine what factors make the generated output plausible and interesting.\nWe further perform an automatic evaluation to determine how well the model can\npredict the game state given the history and examine how well tracking the game\nstate improves its ability to produce plausible conversational output.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomar_G/0/1/0/all/0/1\">Gaurav Singh Tomar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Lara J. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailis_S/0/1/0/all/0/1\">Suma Bailis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1\">David Reitter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models. (arXiv:2210.07111v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07111","description":"<p>Recent work on tokenizer-free multilingual pretrained models show promising\nresults in improving cross-lingual transfer and reducing engineering overhead\n(Clark et al., 2022; Xue et al., 2022). However, these works mainly focus on\nreporting accuracy on a limited set of tasks and data settings, placing less\nemphasis on other important factors when tuning and deploying the models in\npractice, such as memory usage, inference speed, and fine-tuning data\nrobustness. We attempt to fill this gap by performing a comprehensive empirical\ncomparison of multilingual tokenizer-free and subword-based models considering\nthese various dimensions. Surprisingly, we find that subword-based models might\nstill be the most practical choice in many settings, achieving better\nperformance for lower inference latency and memory usage. Based on these\nresults, we encourage future work in tokenizer-free methods to consider these\nfactors when designing and evaluating new models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jimin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards End-to-End Open Conversational Machine Reading. (arXiv:2210.07113v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07113","description":"<p>In open-retrieval conversational machine reading (OR-CMR) task, machines are\nrequired to do multi-turn question answering given dialogue history and a\ntextual knowledge base. Existing works generally utilize two independent\nmodules to approach this problem's two successive sub-tasks: first with a\nhard-label decision making and second with a question generation aided by\nvarious entailment reasoning methods. Such usual cascaded modeling is\nvulnerable to error propagation and prevents the two sub-tasks from being\nconsistently optimized. In this work, we instead model OR-CMR as a unified\ntext-to-text task in a fully end-to-end style. Experiments on the OR-ShARC\ndataset show the effectiveness of our proposed end-to-end framework on both\nsub-tasks by a large margin, achieving new state-of-the-art results. Further\nablation studies support that our framework can generalize to different\nbackbone models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sizhe Zhou</a> (1, 2, 3), <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siru Ouyang</a> (1, 2, 3), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a> (1, 2, 3), <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a> (1, 2, 3) ((1) Department of Computer Science and Engineering, Shanghai Jiao Tong University, (2) Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, (3) MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How (Not) To Evaluate Explanation Quality. (arXiv:2210.07126v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07126","description":"<p>The importance of explainability is increasingly acknowledged in natural\nlanguage processing. However, it is still unclear how the quality of\nexplanations can be assessed effectively. The predominant approach is to\ncompare proxy scores (such as BLEU or explanation F1) evaluated against gold\nexplanations in the dataset. The assumption is that an increase of the proxy\nscore implies a higher utility of explanations to users. In this paper, we\nquestion this assumption. In particular, we (i) formulate desired\ncharacteristics of explanation quality that apply across tasks and domains,\n(ii) point out how current evaluation practices violate those characteristics,\nand (iii) propose actionable guidelines to overcome obstacles that limit\ntoday's evaluation of explanation quality and to enable the development of\nexplainable systems that provide tangible benefits for human users. We\nsubstantiate our theoretical claims (i.e., the lack of validity and temporal\ndecline of currently-used proxy scores) with empirical evidence from a\ncrowdsourcing case study in which we investigate the explanation quality of\nstate-of-the-art explainable question answering systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1\">Peng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models of Code are Few-Shot Commonsense Learners. (arXiv:2210.07128v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07128","description":"<p>We address the general task of structured commonsense reasoning: given a\nnatural language input, the goal is to generate a graph such as an event -- or\na reasoning-graph. To employ large language models (LMs) for this task,\nexisting approaches ``serialize'' the output graph as a flat list of nodes and\nedges. Although feasible, these serialized graphs strongly deviate from the\nnatural language corpora that LMs were pre-trained on, hindering LMs from\ngenerating them correctly. In this paper, we show that when we instead frame\nstructured commonsense reasoning tasks as code generation tasks, pre-trained\nLMs of code are better structured commonsense reasoners than LMs of natural\nlanguage, even when the downstream task does not involve source code at all. We\ndemonstrate our approach across three diverse structured commonsense reasoning\ntasks. In all these natural language tasks, we show that using our approach, a\ncode generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the\ntarget task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models. (arXiv:2210.07135v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07135","description":"<p>Multilingual models have been widely used for cross-lingual transfer to\nlow-resource languages. However, the performance on these languages is hindered\nby their underrepresentation in the pretraining data. To alleviate this\nproblem, we propose a novel multilingual training technique based on\nteacher-student knowledge distillation. In this setting, we utilize monolingual\nteacher models optimized for their language. We use those teachers along with\nbalanced (sub-sampled) data to distill the teachers' knowledge into a single\nmultilingual student. Our method outperforms standard training methods in\nlow-resource languages and retrains performance on high-resource languages\nwhile using the same amount of data. If applied widely, our approach can\nincrease the representation of low-resource languages in NLP systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Limisiewicz_T/0/1/0/all/0/1\">Tomasz Limisiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkin_D/0/1/0/all/0/1\">Dan Malkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning. (arXiv:2210.07138v1 [cs.AI])","link":"http://arxiv.org/abs/2210.07138","description":"<p>Multi-hop QA requires reasoning over multiple supporting facts to answer the\nquestion. However, the existing QA models always rely on shortcuts, e.g.,\nproviding the true answer by only one fact, rather than multi-hop reasoning,\nwhich is referred as $\\textit{disconnected reasoning}$ problem. To alleviate\nthis issue, we propose a novel counterfactual multihop QA, a causal-effect\napproach that enables to reduce the disconnected reasoning. It builds upon\nexplicitly modeling of causality: 1) the direct causal effects of disconnected\nreasoning and 2) the causal effect of true multi-hop reasoning from the total\ncausal effect. With the causal graph, a counterfactual inference is proposed to\ndisentangle the disconnected reasoning from the total causal effect, which\nprovides us a new perspective and technology to learn a QA model that exploits\nthe true multi-hop reasoning instead of shortcuts. Extensive experiments have\nconducted on the benchmark HotpotQA dataset, which demonstrate that the\nproposed method can achieve notable improvement on reducing disconnected\nreasoning. For example, our method achieves 5.8% higher points of its Supp$_s$\nscore on HotpotQA through true multihop reasoning. The code is available at\nsupplementary material.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wangzhen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Q/0/1/0/all/0/1\">Qinkang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Hanjiang Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Translating between Ancient Chinese and Contemporary Chinese with Limited Aligned Corpora. (arXiv:1803.01557v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1803.01557","description":"<p>The Chinese language has evolved a lot during the long-term development.\nTherefore, native speakers now have trouble in reading sentences written in\nancient Chinese. In this paper, we propose to build an end-to-end neural model\nto automatically translate between ancient and contemporary Chinese. However,\nthe existing ancient-contemporary Chinese parallel corpora are not aligned at\nthe sentence level and sentence-aligned corpora are limited, which makes it\ndifficult to train the model. To build the sentence level parallel training\ndata for the model, we propose an unsupervised algorithm that constructs\nsentence-aligned ancient-contemporary pairs by using the fact that the aligned\nsentence pair shares many of the tokens. Based on the aligned corpus, we\npropose an end-to-end neural model with copying mechanism and local attention\nto translate between ancient and contemporary Chinese. Experiments show that\nthe proposed unsupervised algorithm achieves 99.4% F1 score for sentence\nalignment, and the translation model achieves 26.95 BLEU from ancient to\ncontemporary, and 36.34 BLEU from contemporary to ancient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qi Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic modeling of rational communication with conditionals. (arXiv:2105.05502v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.05502","description":"<p>While a large body of work has scrutinized the meaning of conditional\nsentences, considerably less attention has been paid to formal models of their\npragmatic use and interpretation. Here, we take a probabilistic approach to\npragmatic reasoning about indicative conditionals which flexibly integrates\ngradient beliefs about richly structured world states. We model listeners'\nupdate of their prior beliefs about the causal structure of the world and the\njoint probabilities of the consequent and antecedent based on assumptions about\nthe speaker's utterance production protocol. We show that, when supplied with\nnatural contextual assumptions, our model uniformly explains a number of\ninferences attested in the literature, including epistemic inferences,\nconditional perfection and the dependency between antecedent and consequent of\na conditional. We argue that this approach also helps explain three puzzles\nintroduced by Douven (2012) about updating with conditionals: depending on the\nutterance context, the listener's belief in the antecedent may increase,\ndecrease or remain unchanged.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grusdt_B/0/1/0/all/0/1\">Britta Grusdt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassiter_D/0/1/0/all/0/1\">Daniel Lassiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franke_M/0/1/0/all/0/1\">Michael Franke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Graphical Conventions in a Visual Communication Game. (arXiv:2111.14210v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.14210","description":"<p>Humans communicate with graphical sketches apart from symbolic languages.\nPrimarily focusing on the latter, recent studies of emergent communication\noverlook the sketches; they do not account for the evolution process through\nwhich symbolic sign systems emerge in the trade-off between iconicity and\nsymbolicity. In this work, we take the very first step to model and simulate\nthis process via two neural agents playing a visual communication game; the\nsender communicates with the receiver by sketching on a canvas. We devise a\nnovel reinforcement learning method such that agents are evolved jointly\ntowards successful communication and abstract graphical conventions. To inspect\nthe emerged conventions, we define three fundamental properties -- iconicity,\nsymbolicity, and semanticity -- and design evaluation methods accordingly. Our\nexperimental results under different controls are consistent with the\nobservation in studies of human graphical conventions. Of note, we find that\nevolved sketches can preserve the continuum of semantics under proper\nenvironmental pressures. More interestingly, co-evolved agents can switch\nbetween conventionalized and iconic communication based on their familiarity\nwith referents. We hope the present research can pave the path for studying\nemergent communication with the modality of sketches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shuwen Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sirui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lifeng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Linking: Grounding Event Mentions to Wikipedia. (arXiv:2112.07888v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07888","description":"<p>Comprehending an article requires understanding its constituent events.\nHowever, the context where an event is mentioned often lacks the details of\nthis event. A question arises: how can the reader obtain more knowledge about\nthis particular event in addition to what is provided by the local context in\nthe article?\n</p>\n<p>This work defines Event Linking, a new natural language understanding task at\nthe event level. Event linking tries to link an event mention appearing in an\narticle to the most appropriate Wikipedia page. This page is expected to\nprovide rich knowledge about what the event mention refers to. To standardize\nthe research in this new direction, we contribute in four-fold. First, this is\nthe first work in the community that formally defines Event Linking task.\nSecond, we collect a dataset for this new task. Specifically, we automatically\ngather training set from Wikipedia, and then create two evaluation sets: one\nfrom the Wikipedia domain, reporting the in-domain performance, and a second\nfrom the real-world news domain, to evaluate out-of-domain performance. Third,\nwe retrain and evaluate two state-of-the-art (SOTA) entity linking models,\nshowing the challenges of event linking, and we propose an event-specific\nlinking system EVELINK to set a competitive result for the new task. Fourth, we\nconduct a detailed and insightful analysis to help understand the task and the\nlimitation of the current model. Overall, as our analysis shows, Event Linking\nis a considerably challenging and essential task requiring more effort from the\ncommunity. Data and code are available here:\nhttps://github.com/CogComp/event-linking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaodong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nitish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptBERT: Improving BERT Sentence Embeddings with Prompts. (arXiv:2201.04337v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.04337","description":"<p>We propose PromptBERT, a novel contrastive learning method for learning\nbetter sentence representation. We firstly analyze the drawback of current\nsentence embedding from original BERT and find that it is mainly due to the\nstatic token embedding bias and ineffective BERT layers. Then we propose the\nfirst prompt-based sentence embeddings method and discuss two prompt\nrepresenting methods and three prompt searching methods to make BERT achieve\nbetter sentence embeddings. Moreover, we propose a novel unsupervised training\nobjective by the technology of template denoising, which substantially shortens\nthe performance gap between the supervised and unsupervised settings. Extensive\nexperiments show the effectiveness of our method. Compared to SimCSE,\nPromptBert achieves 2.29 and 2.58 points of improvement based on BERT and\nRoBERTa in the unsupervised setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Ting Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Deqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haizhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Denvy Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers. (arXiv:2202.06690v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06690","description":"<p>The applications of conversational agents for scientific disciplines (as\nexpert domains) are understudied due to the lack of dialogue data to train such\nagents. While most data collection frameworks, such as Amazon Mechanical Turk,\nfoster data collection for generic domains by connecting crowd workers and task\ndesigners, these frameworks are not much optimized for data collection in\nexpert domains. Scientists are rarely present in these frameworks due to their\nlimited time budget. Therefore, we introduce a novel framework to collect\ndialogues between scientists as domain experts on scientific papers. Our\nframework lets scientists present their scientific papers as groundings for\ndialogues and participate in dialogue they like its paper title. We use our\nframework to collect a novel argumentative dialogue dataset, ArgSciChat. It\nconsists of 498 messages collected from 41 dialogues on 20 scientific papers.\nAlongside extensive analysis on ArgSciChat, we evaluate a recent conversational\nagent on our dataset. Experimental results show that this agent poorly performs\non ArgSciChat, motivating further research on argumentative scientific agents.\nWe release our framework and the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruggeri_F/0/1/0/all/0/1\">Federico Ruggeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesgar_M/0/1/0/all/0/1\">Mohsen Mesgar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VScript: Controllable Script Generation with Visual Presentation. (arXiv:2203.00314v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00314","description":"<p>In order to offer a customized script tool and inspire professional\nscriptwriters, we present VScript. It is a controllable pipeline that generates\ncomplete scripts, including dialogues and scene descriptions, as well as\npresents visually using video retrieval. With an interactive interface, our\nsystem allows users to select genres and input starting words that control the\ntheme and development of the generated script. We adopt a hierarchical\nstructure, which first generates the plot, then the script and its visual\npresentation. A novel approach is also introduced to plot-guided dialogue\ngeneration by treating it as an inverse dialogue summarization. The experiment\nresults show that our approach outperforms the baselines on both automatic and\nhuman evaluations, especially in genre control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_I/0/1/0/all/0/1\">I-Tsun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1\">Etsuko Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Min Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linearizing Transformer with Key-Value Memory. (arXiv:2203.12644v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12644","description":"<p>Efficient transformer variants with linear time complexity have been\ndeveloped to mitigate the quadratic computational overhead of the vanilla\ntransformer. Among them are low-rank projection methods such as Linformer and\nkernel-based Transformers. Despite their unique merits, they usually suffer\nfrom a performance drop comparing with the vanilla transformer on many sequence\ngeneration tasks, and often fail to obtain computation gain when the generation\nis short. We propose MemSizer, an approach towards closing the performance gap\nwhile improving the efficiency even with short generation. It projects the\nsource sequences into lower dimension representations like Linformer, while\nenjoying efficient recurrent-style incremental computation similar to\nkernel-based transformers. This yields linear computation time and constant\nmemory complexity at inference time. MemSizer also employs a lightweight\nmulti-head mechanism which renders the computation as light as a single-head\nmodel. We demonstrate that MemSizer provides an improved balance between\nefficiency and accuracy over the vanilla transformer and other efficient\ntransformer variants in three typical sequence generation tasks, including\nmachine translation, abstractive text summarization, and language modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space. (arXiv:2203.14680v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.14680","description":"<p>Transformer-based language models (LMs) are at the core of modern NLP, but\ntheir internal prediction construction process is opaque and largely not\nunderstood. In this work, we make a substantial step towards unveiling this\nunderlying prediction process, by reverse-engineering the operation of the\nfeed-forward network (FFN) layers, one of the building blocks of transformer\nmodels. We view the token representation as a changing distribution over the\nvocabulary, and the output from each FFN layer as an additive update to that\ndistribution. Then, we analyze the FFN updates in the vocabulary space, showing\nthat each update can be decomposed to sub-updates corresponding to single FFN\nparameter vectors, each promoting concepts that are often human-interpretable.\nWe then leverage these findings for controlling LM predictions, where we reduce\nthe toxicity of GPT2 by almost 50%, and for improving computation efficiency\nwith a simple early exit rule, saving 20% of computation on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kevin Ro Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASR data augmentation using cross-lingual multi-speaker TTS and cross-lingual voice conversion. (arXiv:2204.00618v4 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2204.00618","description":"<p>We explore cross-lingual multi-speaker speech synthesis and cross-lingual\nvoice conversion applied to data augmentation for automatic speech recognition\n(ASR) systems. Through extensive experiments, we show that our approach permits\nthe application of speech synthesis and voice conversion to improve ASR systems\non a target language using only one target-language speaker during model\ntraining. We managed to close the gap between ASR models trained with\nsynthesized versus human speech compared to other works that use many speakers.\nFinally, we show that it is possible to obtain promising ASR training results\nwith our data augmentation method using only a single real speaker in a target\nlanguage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shulby_C/0/1/0/all/0/1\">Christopher Shulby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korolev_A/0/1/0/all/0/1\">Alexander Korolev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soares_A/0/1/0/all/0/1\">Anderson da Silva Soares</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aluisio_S/0/1/0/all/0/1\">Sandra Alu&#xed;sio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Antonelli Ponti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation. (arXiv:2204.06674v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06674","description":"<p>Recent improvements in KG-to-text generation are due to additional auxiliary\npre-training tasks designed to give the fine-tune task a boost in performance.\nThese tasks require extensive computational resources while only suggesting\nmarginal improvements. Here, we demonstrate that by fusing graph-aware elements\ninto existing pre-trained language models, we are able to outperform\nstate-of-the-art models and close the gap imposed by additional pre-training\ntasks. We do so by proposing a mask structure to capture neighborhood\ninformation and a novel type encoder that adds a bias to the graph-attention\nweights depending on the connection type. Experiments on two KG-to-text\nbenchmark datasets show our models are competitive while involving fewer\nparameters and no additional pre-training tasks. By formulating the problem as\na framework, we can interchange the various proposed components and begin\ninterpreting KG-to-text generative models based on the topological and type\ninformation found in a graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colas_A/0/1/0/all/0/1\">Anthony Colas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvandipour_M/0/1/0/all/0/1\">Mehrdad Alvandipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daisy Zhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08790","description":"<p>Learning visual representations from natural language supervision has\nrecently shown great promise in a number of pioneering works. In general, these\nlanguage-augmented visual models demonstrate strong transferability to a\nvariety of datasets and tasks. However, it remains challenging to evaluate the\ntransferablity of these models due to the lack of easy-to-use evaluation\ntoolkits and public benchmarks. To tackle this, we build ELEVATER (Evaluation\nof Language-augmented Visual Task-level Transfer), the first benchmark and\ntoolkit for evaluating(pre-trained) language-augmented visual models. ELEVATER\nis composed of three components. (i) Datasets. As downstream evaluation suites,\nit consists of 20 image classification datasets and 35 object detection\ndatasets, each of which is augmented with external knowledge. (ii) Toolkit. An\nautomatic hyper-parameter tuning toolkit is developed to facilitate model\nevaluation on downstream tasks. (iii) Metrics. A variety of evaluation metrics\nare used to measure sample-efficiency (zero-shot and few-shot) and\nparameter-efficiency (linear probing and full model fine-tuning). ELEVATER is a\nplatform for Computer Vision in the Wild (CVinW), and is publicly released at\nat https://computer-vision-in-the-wild.github.io/ELEVATER/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haotian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aneja_J/0/1/0/all/0/1\">Jyoti Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Ping Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Houdong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LM-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models. (arXiv:2204.12130v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.12130","description":"<p>The opaque nature and unexplained behavior of transformer-based language\nmodels (LMs) have spurred a wide interest in interpreting their predictions.\nHowever, current interpretation methods mostly focus on probing models from\noutside, executing behavioral tests, and analyzing salience input features,\nwhile the internal prediction construction process is largely not understood.\nIn this work, we introduce LM-Debugger, an interactive debugger tool for\ntransformer-based LMs, which provides a fine-grained interpretation of the\nmodel's internal prediction process, as well as a powerful framework for\nintervening in LM behavior. For its backbone, LM-Debugger relies on a recent\nmethod that interprets the inner token representations and their updates by the\nfeed-forward layers in the vocabulary space. We demonstrate the utility of\nLM-Debugger for single-prediction debugging, by inspecting the internal\ndisambiguation process done by GPT2. Moreover, we show how easily LM-Debugger\nallows to shift model behavior in a direction of the user's choice, by\nidentifying a few vectors in the network and inducing effective interventions\nto the prediction process. We release LM-Debugger as an open-source tool and a\ndemo over GPT2 models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dar_G/0/1/0/all/0/1\">Guy Dar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roit_P/0/1/0/all/0/1\">Paul Roit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadde_S/0/1/0/all/0/1\">Shoval Sadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlain_M/0/1/0/all/0/1\">Micah Shlain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamir_B/0/1/0/all/0/1\">Bar Tamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Cross-Lingual Lexical Knowledge from Multilingual Sentence Encoders. (arXiv:2205.00267v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00267","description":"<p>Pretrained multilingual language models (LMs) can be successfully transformed\ninto multilingual sentence encoders (SEs; e.g., LaBSE, xMPNet) via additional\nfine-tuning or model distillation with parallel data. However, it remains\nunclear how to best leverage them to represent sub-sentence lexical items\n(i.e., words and phrases) in cross-lingual lexical tasks. In this work, we\nprobe SEs for the amount of cross-lingual lexical knowledge stored in their\nparameters, and compare them against the original multilingual LMs. We also\ndevise a simple yet efficient method for exposing the cross-lingual lexical\nknowledge by means of additional fine-tuning through inexpensive contrastive\nlearning that requires only a small amount of word translation pairs. Using\nbilingual lexical induction (BLI), cross-lingual lexical semantic similarity,\nand cross-lingual entity linking as lexical probing tasks, we report\nsubstantial gains on standard benchmarks (e.g., +10 Precision@1 points in BLI).\nThe results indicate that the SEs such as LaBSE can be 'rewired' into effective\ncross-lingual lexical encoders via the contrastive learning procedure, and that\nthey contain more cross-lingual lexical knowledge than what 'meets the eye'\nwhen they are used as off-the-shelf SEs. This way, we also provide an effective\ntool for harnessing 'covert' multilingual lexical knowledge hidden in\nmultilingual sentence encoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Global and Local Hierarchies for Hierarchical Text Classification. (arXiv:2205.02613v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02613","description":"<p>Hierarchical text classification aims to leverage label hierarchy in\nmulti-label text classification. Existing methods encode label hierarchy in a\nglobal view, where label hierarchy is treated as the static hierarchical\nstructure containing all labels. Since global hierarchy is static and\nirrelevant to text samples, it makes these methods hard to exploit hierarchical\ninformation. Contrary to global hierarchy, local hierarchy as a structured\nlabels hierarchy corresponding to each text sample. It is dynamic and relevant\nto text samples, which is ignored in previous methods.To exploit global and\nlocal hierarchies,we propose Hierarchy-guided BERT with Global and Local\nhierarchies (HBGL), which utilizes the large-scale parameters and prior\nlanguage knowledge of BERT to model both global and local\nhierarchies.Moreover,HBGL avoids the intentional fusion of semantic and\nhierarchical modules by directly modeling semantic and hierarchical information\nwith BERT.Compared with the state-of-the-art method HGCLR,our method achieves\nsignificant improvement on three benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Ting Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Deqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Leilei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhongzhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qinghong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning. (arXiv:2205.03401v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03401","description":"<p>Does prompting a large language model (LLM) like GPT-3 with explanations\nimprove in-context learning? We study this question on two NLP tasks that\ninvolve reasoning over text, namely question answering and natural language\ninference. We test the performance of four LLMs on three textual reasoning\ndatasets using prompts that include explanations in multiple different styles.\nFor these tasks, we find that including explanations in the prompts for OPT,\nGPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to\nmoderate accuracy improvements over standard few-show learning. However,\ntext-davinci-002 is able to benefit more substantially.\n</p>\n<p>We further show that explanations generated by the LLMs may not entail the\nmodels' predictions nor be factually grounded in the input, even on simple\ntasks with extractive explanations. However, these flawed explanations can\nstill be useful as a way to verify LLMs' predictions post-hoc. Through analysis\nin our three settings, we show that explanations judged by humans to be\ngood--logically consistent with the input and the prediction--more likely\ncooccur with accurate predictions. Following these observations, we train\ncalibrators using automatically extracted scores that assess the reliability of\nexplanations, allowing us to improve performance post-hoc across all of our\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Modeling of Multi-Domain Multi-Device ASR Systems. (arXiv:2205.06655v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.06655","description":"<p>Modern Automatic Speech Recognition (ASR) systems often use a portfolio of\ndomain-specific models in order to get high accuracy for distinct user\nutterance types across different devices. In this paper, we propose an\ninnovative approach that integrates the different per-domain per-device models\ninto a unified model, using a combination of domain embedding, domain experts,\nmixture of experts and adversarial training. We run careful ablation studies to\nshow the benefit of each of these innovations in contributing to the accuracy\nof the overall unified model. Experiments show that our proposed unified\nmodeling approach actually outperforms the carefully tuned per-domain models,\ngiving relative gains of up to 10% over a baseline model with negligible\nincrease in the number of parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1\">Soumyajit Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1\">Swayambhu Nath Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padi_B/0/1/0/all/0/1\">Bharat Padi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_A/0/1/0/all/0/1\">Arunasish Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilgi_R/0/1/0/all/0/1\">Raghavendra Bilgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arsikere_H/0/1/0/all/0/1\">Harish Arsikere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shalini Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasamurthy_A/0/1/0/all/0/1\">Ajay Srinivasamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_S/0/1/0/all/0/1\">Sri Garimella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DDXPlus: A New Dataset For Automatic Medical Diagnosis. (arXiv:2205.09148v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09148","description":"<p>There has been a rapidly growing interest in Automatic Symptom Detection\n(ASD) and Automatic Diagnosis (AD) systems in the machine learning research\nliterature, aiming to assist doctors in telemedicine services. These systems\nare designed to interact with patients, collect evidence about their symptoms\nand relevant antecedents, and possibly make predictions about the underlying\ndiseases. Doctors would review the interactions, including the evidence and the\npredictions, collect if necessary additional information from patients, before\ndeciding on next steps. Despite recent progress in this area, an important\npiece of doctors' interactions with patients is missing in the design of these\nsystems, namely the differential diagnosis. Its absence is largely due to the\nlack of datasets that include such information for models to train on. In this\nwork, we present a large-scale synthetic dataset of roughly 1.3 million\npatients that includes a differential diagnosis, along with the ground truth\npathology, symptoms and antecedents for each patient. Unlike existing datasets\nwhich only contain binary symptoms and antecedents, this dataset also contains\ncategorical and multi-choice symptoms and antecedents useful for efficient data\ncollection. Moreover, some symptoms are organized in a hierarchy, making it\npossible to design systems able to interact with patients in a logical way. As\na proof-of-concept, we extend two existing AD and ASD systems to incorporate\nthe differential diagnosis, and provide empirical evidence that using\ndifferentials as training signals is essential for the efficiency of such\nsystems or for helping doctors better understand the reasoning of those\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tchango_A/0/1/0/all/0/1\">Arsene Fansi Tchango</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rishab Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_J/0/1/0/all/0/1\">Julien Martel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosn_J/0/1/0/all/0/1\">Joumana Ghosn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation. (arXiv:2205.09921v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09921","description":"<p>Relative positional embeddings (RPE) have received considerable attention\nsince RPEs effectively model the relative distance among tokens and enable\nlength extrapolation. We propose KERPLE, a framework that generalizes relative\nposition embedding for extrapolation by kernelizing positional differences. We\nachieve this goal using conditionally positive definite (CPD) kernels, a class\nof functions known for generalizing distance metrics. To maintain the inner\nproduct interpretation of self-attention, we show that a CPD kernel can be\ntransformed into a PD kernel by adding a constant offset. This offset is\nimplicitly absorbed in the Softmax normalization during self-attention. The\ndiversity of CPD kernels allows us to derive various RPEs that enable length\nextrapolation in a principled way. Experiments demonstrate that the logarithmic\nvariant achieves excellent extrapolation performance on three large language\nmodeling datasets. Our implementation and pretrained checkpoints are released\nat https://github.com/chijames/KERPLE.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_T/0/1/0/all/0/1\">Ta-Chung Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1\">Ting-Han Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramadge_P/0/1/0/all/0/1\">Peter J. Ramadge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander I. Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Byte Fusion for Neural Machine Translation. (arXiv:2205.11490v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11490","description":"<p>Subword tokenization schemes are the dominant technique used in current NLP\nmodels. However, such schemes can be rigid and tokenizers built on one corpus\ndo not adapt well to other parallel corpora. It has also been observed that in\nmultilingual corpora, subword tokenization schemes over-segment low-resource\nlanguages leading to a drop in translation performance. A simple alternative to\nsubword tokenizers is byte-based methods i.e. tokenization into byte sequences\nusing encoding schemes such as UTF-8. Byte tokens often represent inputs at a\nsub-character granularity i.e. one character can be represented by a sequence\nof multiple byte tokens. This results in byte sequences that are significantly\nlonger than character sequences. Enforcing aggregation of local information in\nthe lower layers can guide the model to build higher-level semantic\ninformation. We propose a Local Byte Fusion (LOBEF) method for byte-based\nmachine translation -- utilizing byte $n$-gram and word boundaries -- to\naggregate local semantic information. Extensive experiments on multilingual\ntranslation, zero-shot cross-lingual transfer, and domain adaptation reveal a\nconsistent improvement over traditional byte-based models and even over subword\ntechniques. Further analysis also indicates that our byte-based models are\nparameter-efficient and can be trained faster than subword models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sreedhar_M/0/1/0/all/0/1\">Makesh Narsimhan Sreedhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiangpeng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaskEval: Weighted MLM-Based Evaluation for Text Summarization and Simplification. (arXiv:2205.12394v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12394","description":"<p>In text summarization and simplification, system outputs must be evaluated\nalong multiple dimensions such as relevance, factual consistency, fluency, and\ngrammaticality, and a wide range of possible outputs could be of high quality.\nThese properties make the development of an adaptable, reference-less\nevaluation metric both necessary and challenging. We introduce MaskEval, a\nreference-less metric for text summarization and simplification that operates\nby performing masked language modeling (MLM) on the concatenation of the\ncandidate and the source texts. It features an attention-like weighting\nmechanism to modulate the relative importance of each MLM step, which crucially\nallows it to be adapted to evaluate different quality dimensions. We\ndemonstrate its effectiveness on English summarization and simplification in\nterms of correlations with human judgments, and explore transfer scenarios\nbetween the two tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Lu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1\">Rachel Bawden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT. (arXiv:2205.12399v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.12399","description":"<p>We combine the capacity of sparsely gated Mixture-of-Experts (MoE) with the\nspeed and stability of linear, mixing transformations to design the Sparse\nMixer encoder model. Sparse Mixer slightly outperforms (&lt;1%) BERT on GLUE and\nSuperGLUE, but more importantly trains 65% faster and runs inference 61%\nfaster. We also present a faster variant, prosaically named Fast Sparse Mixer,\nthat marginally underperforms BERT on SuperGLUE, but trains and runs nearly\ntwice as fast. We justify the design of these two models by carefully ablating\nthrough various mixing mechanisms, MoE configurations and hyperparameters.\nSparse Mixer overcomes many of the latency and stability concerns of MoE models\nand offers the prospect of serving sparse student models, without resorting to\ndistilling them to dense variants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Helpfulness and Fairness of Task-Oriented Dialogue Systems. (arXiv:2205.12554v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12554","description":"<p>Goal-oriented dialogue systems aim to help users achieve certain goals.\nTherefore, how humans perceive their helpfulness is important. However, neither\nthe human-perceived helpfulness of goal-oriented dialogue systems nor its\nfairness implication has been well studied. In this paper, we study\ncomputational measurements of helpfulness. We first formally define a dialogue\nresponse as helpful if it is relevant &amp; coherent, useful, and informative to a\nquery. Then, we collect human annotations for the helpfulness of dialogue\nresponses based on our definition and build a classifier to automatically\ndetermine the helpfulness of a response. We further propose to use the\nhelpfulness level of a dialogue system towards different user queries to\nmeasure the fairness of a dialogue system. Experiments with state-of-the-art\ndialogue systems under three information-seeking scenarios reveal that existing\nsystems tend to be more helpful for questions regarding concepts from\nhighly-developed countries than less-developed countries, uncovering potential\nfairness concerns underlying the current goal-oriented dialogue systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perturbation Augmentation for Fairer NLP. (arXiv:2205.12586v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12586","description":"<p>Unwanted and often harmful social biases are becoming ever more salient in\nNLP research, affecting both models and datasets. In this work, we ask whether\ntraining on demographically perturbed data leads to fairer language models. We\ncollect a large dataset of human annotated text perturbations and train a\nneural perturbation model, which we show outperforms heuristic alternatives. We\nfind that (i) language models (LMs) pre-trained on demographically perturbed\ncorpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE\ndatasets exhibit less demographic bias on downstream tasks, and (iii) fairness\nimprovements do not come at the expense of performance on downstream tasks.\nLastly, we discuss outstanding questions about how best to evaluate the\n(un)fairness of large language models. We hope that this exploration of neural\ndemographic perturbation will help drive more improvement towards fairer NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rebecca Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_C/0/1/0/all/0/1\">Candace Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_J/0/1/0/all/0/1\">Jude Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1\">Eric Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuro-Symbolic Procedural Planning with Commonsense Prompting. (arXiv:2206.02928v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.02928","description":"<p>Procedural planning aims to implement complex high-level goals by\ndecomposition into sequential simpler low-level steps. Although procedural\nplanning is a basic skill set for humans in daily life, it remains a challenge\nfor large language models (LLMs) that lack a deep understanding of the\ncause-effect relations in procedures. Previous methods require manual exemplars\nto acquire procedural planning knowledge from LLMs in the zero-shot setting.\nHowever, such elicited pre-trained knowledge in LLMs induces spurious\ncorrelations between goals and steps, which impair the model generalization to\nunseen tasks. In contrast, this paper proposes a neuro-symbolic procedural\nPLANner (PLAN) that elicits procedural planning knowledge from the LLMs with\ncommonsense-infused prompting. To mitigate spurious goal-step correlations, we\nuse symbolic program executors on the latent procedural representations to\nformalize prompts from commonsense knowledge bases as a causal intervention\ntoward the Structural Causal Model. Both automatic and human evaluations on\nWikiHow and RobotHow show the superiority of PLAN on procedural planning\nwithout further training or manual exemplars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Weixi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Generate Prompts for Dialogue Generation through Reinforcement Learning. (arXiv:2206.03931v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.03931","description":"<p>Much literature has shown that prompt-based learning is an efficient method\nto make use of the large pre-trained language model. Recent works also exhibit\nthe possibility of steering a chatbot's output by plugging in an appropriate\nprompt. Gradient-based methods are often used to perturb the prompts. However,\nsome language models are not even available to the public. In this work, we\nfirst explored the combination of prompting and reinforcement learning (RL) to\nsteer models' generation without accessing any of the models' parameters.\nSecond, to reduce the training effort and enhance the generalizability to the\nunseen task, we apply multi-task learning to make the model learn to generalize\nto new tasks better. The experiment results show that our proposed method can\nsuccessfully control several state-of-the-art (SOTA) dialogue models without\naccessing their parameters. Furthermore, the model demonstrates the strong\nability to quickly adapt to an unseen task in fewer steps than the baseline\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hsuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_P/0/1/0/all/0/1\">Pohan Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shih-Cheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_C/0/1/0/all/0/1\">Chung Ho Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahay_S/0/1/0/all/0/1\">Saurav Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shang-Tse Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL. (arXiv:2206.09674v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.09674","description":"<p>Reinforcement learning (RL) in long horizon and sparse reward tasks is\nnotoriously difficult and requires a lot of training steps. A standard solution\nto speed up the process is to leverage additional reward signals, shaping it to\nbetter guide the learning process. In the context of language-conditioned RL,\nthe abstraction and generalisation properties of the language input provide\nopportunities for more efficient ways of shaping the reward. In this paper, we\nleverage this idea and propose an automated reward shaping method where the\nagent extracts auxiliary objectives from the general language goal. These\nauxiliary objectives use a question generation (QG) and question answering (QA)\nsystem: they consist of questions leading the agent to try to reconstruct\npartial information about the global goal using its own trajectory. When it\nsucceeds, it receives an intrinsic reward proportional to its confidence in its\nanswer. This incentivizes the agent to generate trajectories which\nunambiguously explain various aspects of the general language goal. Our\nexperimental study shows that this approach, which does not require engineer\nintervention to design the auxiliary objectives, improves sample efficiency by\neffectively directing exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carta_T/0/1/0/all/0/1\">Thomas Carta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1\">Olivier Sigaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focus-Driven Contrastive Learniang for Medical Question Summarization. (arXiv:2209.00484v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.00484","description":"<p>Automatic medical question summarization can significantly help the system to\nunderstand consumer health questions and retrieve correct answers. The Seq2Seq\nmodel based on maximum likelihood estimation (MLE) has been applied in this\ntask, which faces two general problems: the model can not capture well question\nfocus and and the traditional MLE strategy lacks the ability to understand\nsentence-level semantics. To alleviate these problems, we propose a novel\nquestion focus-driven contrastive learning framework (QFCL). Specially, we\npropose an easy and effective approach to generate hard negative samples based\non the question focus, and exploit contrastive learning at both encoder and\ndecoder to obtain better sentence level representations. On three medical\nbenchmark datasets, our proposed model achieves new state-of-the-art results,\nand obtains a performance gain of 5.33, 12.85 and 3.81 points over the baseline\nBART model on three datasets respectively. Further human judgement and detailed\nanalysis prove that our QFCL model learns better sentence representations with\nthe ability to distinguish different sentence meanings, and generates\nhigh-quality summaries by capturing question focus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shuai Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining Large Pre-Trained Language Models for Machine Translation: What You Don't Know About It. (arXiv:2209.07417v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.07417","description":"<p>Pre-trained language models (PLMs) often take advantage of the monolingual\nand multilingual dataset that is freely available online to acquire general or\nmixed domain knowledge before deployment into specific tasks. Extra-large PLMs\n(xLPLMs) are proposed very recently to claim supreme performances over\nsmaller-sized PLMs such as in machine translation (MT) tasks. These xLPLMs\ninclude Meta-AI's wmt21-dense-24-wide-en-X (2021) and NLLB (2022). In this\nwork, we examine if xLPLMs are absolutely superior to smaller-sized PLMs in\nfine-tuning toward domain-specific MTs. We use two different in-domain data of\ndifferent sizes: commercial automotive in-house data and clinical shared task\ndata from the ClinSpEn2022 challenge at WMT2022. We choose popular Marian\nHelsinki as smaller sized PLM and two massive-sized Mega-Transformers from\nMeta-AI as xLPLMs.\n</p>\n<p>Our experimental investigation shows that 1) on smaller-sized in-domain\ncommercial automotive data, xLPLM wmt21-dense-24-wide-en-X indeed shows much\nbetter evaluation scores using SacreBLEU and hLEPOR metrics than smaller-sized\nMarian, even though its score increase rate is lower than Marian after\nfine-tuning; 2) on relatively larger-size well prepared clinical data\nfine-tuning, the xLPLM NLLB tends to lose its advantage over smaller-sized\nMarian on two sub-tasks (clinical terms and ontology concepts) using ClinSpEn\noffered metrics METEOR, COMET, and ROUGE-L, and totally lost to Marian on\nTask-1 (clinical cases) on all official metrics including SacreBLEU and BLEU;\n3) metrics do not always agree with each other on the same tasks using the same\nmodel outputs; 4) clinic-Marian ranked No.2 on Task- 1 (via SACREBLEU/BLEU) and\nTask-3 (via METEOR and ROUGE) among all submissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1\">Gleb Erofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorokina_I/0/1/0/all/0/1\">Irina Sorokina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1\">Serge Gladkoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango. (arXiv:2209.07686v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.07686","description":"<p>The past decade has witnessed dramatic gains in natural language processing\nand an unprecedented scaling of large language models. These developments have\nbeen accelerated by the advent of few-shot techniques such as chain of thought\n(CoT) prompting. Specifically, CoT pushes the performance of large language\nmodels in a few-shot setup by augmenting the prompts with intermediate steps.\nDespite impressive results across various tasks, the reasons behind their\nsuccess have not been explored. This work uses counterfactual prompting to\ndevelop a deeper understanding of CoT-based few-shot prompting mechanisms in\nlarge language models. We first systematically identify and define the key\ncomponents of a prompt: symbols, patterns, and text. Then, we devise and\nconduct an exhaustive set of experiments across four different tasks, by\nquerying the model with counterfactual prompts where only one of these\ncomponents is altered. Our experiments across three models (PaLM, GPT-3, and\nCODEX) reveal several surprising findings and brings into question the\nconventional wisdom around few-shot prompting. First, the presence of factual\npatterns in a prompt is practically immaterial to the success of CoT. Second,\nour results conclude that the primary role of intermediate steps may not be to\nfacilitate learning how to solve a task. The intermediate steps are rather a\nbeacon for the model to realize what symbols to replicate in the output to form\na factual answer. Further, text imbues patterns with commonsense knowledge and\nmeaning. Our empirical and qualitative analysis reveals that a symbiotic\nrelationship between text and patterns explains the success of few-shot\nprompting: text helps extract commonsense from the question to help patterns,\nand patterns enforce task understanding and direct text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdanbakhsh_A/0/1/0/all/0/1\">Amir Yazdanbakhsh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Interpretable Latent Dialogue Actions With Less Supervision. (arXiv:2209.11128v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.11128","description":"<p>We present a novel architecture for explainable modeling of task-oriented\ndialogues with discrete latent variables to represent dialogue actions. Our\nmodel is based on variational recurrent neural networks (VRNN) and requires no\nexplicit annotation of semantic information. Unlike previous works, our\napproach models the system and user turns separately and performs database\nquery modeling, which makes the model applicable to task-oriented dialogues\nwhile producing easily interpretable action latent variables. We show that our\nmodel outperforms previous approaches with less supervision in terms of\nperplexity and BLEU on three datasets, and we propose a way to measure dialogue\nsuccess without the need for expert annotation. Finally, we propose a novel way\nto explain semantics of the latent variables with respect to system actions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hudecek_V/0/1/0/all/0/1\">Vojt&#x11b;ch Hude&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaPrompting: Learning to Learn Better Prompts. (arXiv:2209.11486v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.11486","description":"<p>Prompting method is regarded as one of the crucial progress for few-shot\nnature language processing. Recent research on prompting moves from discrete\ntokens based ``hard prompts'' to continuous ``soft prompts'', which employ\nlearnable vectors as pseudo prompt tokens and achieve better performance.\nThough showing promising prospects, these soft-prompting methods are observed\nto rely heavily on good initialization to take effect. Unfortunately, obtaining\na perfect initialization for soft prompts requires understanding of inner\nlanguage models working and elaborate design, which is no easy task and has to\nrestart from scratch for each new task. To remedy this, we propose a\ngeneralized soft prompting method called MetaPrompting, which adopts the\nwell-recognized model-agnostic meta-learning algorithm to automatically find\nbetter prompt initialization that facilitates fast adaptation to new prompting\ntasks.Extensive experiments show MetaPrompting tackles soft prompt\ninitialization problem and brings significant improvement on four different\ndatasets (over 6 points improvement in accuracy for 1-shot setting), achieving\nnew state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yutai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hongyuan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinghao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Family Adapters for Multilingual Neural Machine Translation. (arXiv:2209.15236v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.15236","description":"<p>Massively multilingual models pretrained on abundant corpora with\nself-supervision achieve state-of-the-art results in a wide range of natural\nlanguage processing tasks. In machine translation, multilingual pretrained\nmodels are often fine-tuned on parallel data from one or multiple language\npairs. Multilingual fine-tuning improves performance on medium- and\nlow-resource languages but requires modifying the entire model and can be\nprohibitively expensive. Training a new set of adapters on each language pair\nor training a single set of adapters on all language pairs while keeping the\npretrained model's parameters frozen has been proposed as a parameter-efficient\nalternative. However, the former do not permit any sharing between languages,\nwhile the latter share parameters for all languages and have to deal with\nnegative interference. In this paper, we propose training language-family\nadapters on top of a pretrained multilingual model to facilitate cross-lingual\ntransfer. Our model consistently outperforms other adapter-based approaches. We\nalso demonstrate that language-family adapters provide an effective method to\ntranslate to languages unseen during pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chronopoulou_A/0/1/0/all/0/1\">Alexandra Chronopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stojanovski_D/0/1/0/all/0/1\">Dario Stojanovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot ICD Coding. (arXiv:2210.03304v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03304","description":"<p>Automatic International Classification of Diseases (ICD) coding aims to\nassign multiple ICD codes to a medical note with average length of 3,000+\ntokens. This task is challenging due to a high-dimensional space of multi-label\nassignment (tens of thousands of ICD codes) and the long-tail challenge: only a\nfew codes (common diseases) are frequently assigned while most codes (rare\ndiseases) are infrequently assigned. This study addresses the long-tail\nchallenge by adapting a prompt-based fine-tuning technique with label\nsemantics, which has been shown to be effective under few-shot setting. To\nfurther enhance the performance in medical domain, we propose a\nknowledge-enhanced longformer by injecting three domain-specific knowledge:\nhierarchy, synonym, and abbreviation with additional pretraining using\ncontrastive learning. Experiments on MIMIC-III-full, a benchmark dataset of\ncode assignment, show that our proposed method outperforms previous\nstate-of-the-art method in 14.5% in marco F1 (from 10.3 to 11.8, P&lt;0.001). To\nfurther test our model on few-shot setting, we created a new rare diseases\ncoding dataset, MIMIC-III-rare50, on which our model improves marco F1 from\n17.1 to 30.4 and micro F1 from 17.2 to 32.6 compared to previous method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shufan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_B/0/1/0/all/0/1\">Bhanu Pratap Singh Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1\">Avijit Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metaphorical Paraphrase Generation: Feeding Metaphorical Language Models with Literal Texts. (arXiv:2210.04756v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04756","description":"<p>This study presents a new approach to metaphorical paraphrase generation by\nmasking literal tokens of literal sentences and unmasking them with\nmetaphorical language models. Unlike similar studies, the proposed algorithm\ndoes not only focus on verbs but also on nouns and adjectives. Despite the fact\nthat the transfer rate for the former is the highest (56%), the transfer of the\nlatter is feasible (24% and 31%). Human evaluation showed that our\nsystem-generated metaphors are considered more creative and metaphorical than\nhuman-generated ones while when using our transferred metaphors for data\naugmentation improves the state of the art in metaphorical sentence\nclassification by 3% in F1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ottolina_G/0/1/0/all/0/1\">Giorgio Ottolina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlopoulos_J/0/1/0/all/0/1\">John Pavlopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What the DAAM: Interpreting Stable Diffusion Using Cross Attention. (arXiv:2210.04885v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.04885","description":"<p>Large-scale diffusion neural networks represent a substantial milestone in\ntext-to-image generation, with some performing similar to real photographs in\nhuman evaluation. However, they remain poorly understood, lacking\nexplainability and interpretability analyses, largely due to their proprietary,\nclosed-source nature. In this paper, to shine some much-needed light on\ntext-to-image diffusion models, we perform a text-image attribution analysis on\nStable Diffusion, a recently open-sourced large diffusion model. To produce\npixel-level attribution maps, we propose DAAM, a novel method based on\nupscaling and aggregating cross-attention activations in the latent denoising\nsubnetwork. We support its correctness by evaluating its unsupervised semantic\nsegmentation quality on its own generated imagery, compared to supervised\nsegmentation models. We show that DAAM performs strongly on COCO\ncaption-generated images, achieving an mIoU of 61.0, and it outperforms\nsupervised models on open-vocabulary segmentation, for an mIoU of 51.5. We\nfurther find that certain parts of speech, like punctuation and conjunctions,\ninfluence the generated imagery most, which agrees with the prior literature,\nwhile determiners and numerals the least, suggesting poor numeracy. To our\nknowledge, we are the first to propose and study word-pixel attribution for\ninterpreting large-scale diffusion models. Our code and data are at\nhttps://github.com/castorini/daam.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Raphael Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Akshat Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhiying Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Gefei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1\">Karun Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ture_F/0/1/0/all/0/1\">Ferhan Ture</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Locate Visual Answer in Video Corpus Using Question. (arXiv:2210.05423v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.05423","description":"<p>We introduce a new task, named video corpus visual answer localization\n(VCVAL), which aims to locate the visual answer in a large collection of\nuntrimmed, unsegmented instructional videos using a natural language question.\nThis task requires a range of skills - the interaction between vision and\nlanguage, video retrieval, passage comprehension, and visual answer\nlocalization. In this paper, we propose a cross-modal contrastive global-span\n(CCGS) method for the VCVAL, jointly training the video corpus retrieval and\nvisual answer localization subtasks. More precisely, we first enhance the video\nquestion-answer semantic by adding element-wise visual information into the\npre-trained language model, and then design a novel global-span predictor\nthrough fusion information to locate the visual answer point. The global-span\ncontrastive learning is adopted to sort the span point from the positive and\nnegative samples with the global-span matrix. We have reconstructed a dataset\nnamed MedVidCQA, on which the VCVAL task is benchmarked. Experimental results\nshow that the proposed method outperforms other competitive methods both in the\nvideo corpus retrieval and visual answer localization subtasks. Most\nimportantly, we perform detailed analyses on extensive experiments, paving a\nnew path for understanding the instructional videos, which ushers in further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers generalize differently from information stored in context vs in weights. (arXiv:2210.05675v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05675","description":"<p>Transformer models can use two fundamentally different kinds of information:\ninformation stored in weights during training, and information provided\n``in-context'' at inference time. In this work, we show that transformers\nexhibit different inductive biases in how they represent and generalize from\nthe information in these two sources. In particular, we characterize whether\nthey generalize via parsimonious rules (rule-based generalization) or via\ndirect comparison with observed examples (exemplar-based generalization). This\nis of important practical consequence, as it informs whether to encode\ninformation in weights or in context, depending on how we want models to use\nthat information. In transformers trained on controlled stimuli, we find that\ngeneralization from weights is more rule-based whereas generalization from\ncontext is largely exemplar-based. In contrast, we find that in transformers\npre-trained on natural language, in-context learning is significantly\nrule-based, with larger models showing more rule-basedness. We hypothesise that\nrule-based generalization from in-context information might be an emergent\nconsequence of large-scale training on language, which has sparse rule-like\nstructure. Using controlled stimuli, we verify that transformers pretrained on\ndata containing sparse rule-like structure exhibit more rule-based\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stephanie C.Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junkyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaran_D/0/1/0/all/0/1\">Dharshan Kumaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew K. Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Language Maps for Robot Navigation. (arXiv:2210.05714v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2210.05714","description":"<p>Grounding language to the visual observations of a navigating agent can be\nperformed using off-the-shelf visual-language models pretrained on\nInternet-scale data (e.g., image captions). While this is useful for matching\nimages to natural language descriptions of object goals, it remains disjoint\nfrom the process of mapping the environment, so that it lacks the spatial\nprecision of classic geometric maps. To address this problem, we propose\nVLMaps, a spatial map representation that directly fuses pretrained\nvisual-language features with a 3D reconstruction of the physical world. VLMaps\ncan be autonomously built from video feed on robots using standard exploration\napproaches and enables natural language indexing of the map without additional\nlabeled data. Specifically, when combined with large language models (LLMs),\nVLMaps can be used to (i) translate natural language commands into a sequence\nof open-vocabulary navigation goals (which, beyond prior work, can be spatial\nby construction, e.g., \"in between the sofa and TV\" or \"three meters to the\nright of the chair\") directly localized in the map, and (ii) can be shared\namong multiple robots with different embodiments to generate new obstacle maps\non-the-fly (by using a list of obstacle categories). Extensive experiments\ncarried out in simulated and real world environments show that VLMaps enable\nnavigation according to more complex language instructions than existing\nmethods. Videos are available at https://vlmaps.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenguang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mees_O/0/1/0/all/0/1\">Oier Mees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vote'n'Rank: Revision of Benchmarking with Social Choice Theory. (arXiv:2210.05769v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.05769","description":"<p>The development of state-of-the-art systems in different applied areas of\nmachine learning (ML) is driven by benchmarks, which have shaped the paradigm\nof evaluating generalisation capabilities from multiple perspectives. Although\nthe paradigm is shifting towards more fine-grained evaluation across diverse\ntasks, the delicate question of how to aggregate the performances has received\nparticular interest in the community. In general, benchmarks follow the\nunspoken utilitarian principles, where the systems are ranked based on their\nmean average score over task-specific metrics. Such aggregation procedure has\nbeen viewed as a sub-optimal evaluation protocol, which may have created the\nillusion of progress. This paper proposes Vote'n'Rank, a framework for ranking\nsystems in multi-task benchmarks under the principles of the social choice\ntheory. We demonstrate that our approach can be efficiently utilised to draw\nnew insights on benchmarking in several ML sub-fields and identify the\nbest-performing systems in research and development case studies. The\nVote'n'Rank's procedures are more robust than the mean average while being able\nto handle missing performance scores and determine conditions under which the\nsystem becomes the winner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rofin_M/0/1/0/all/0/1\">Mark Rofin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florinskiy_M/0/1/0/all/0/1\">Mikhail Florinskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravchenko_A/0/1/0/all/0/1\">Andrey Kravchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutubalina_E/0/1/0/all/0/1\">Elena Tutubalina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavrina_T/0/1/0/all/0/1\">Tatiana Shavrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karabekyan_D/0/1/0/all/0/1\">Daniel Karabekyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate-CLIPper: Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features. (arXiv:2210.05916v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05916","description":"<p>Hateful memes are a growing menace on social media. While the image and its\ncorresponding text in a meme are related, they do not necessarily convey the\nsame meaning when viewed individually. Hence, detecting hateful memes requires\ncareful consideration of both visual and textual information. Multimodal\npre-training can be beneficial for this task because it effectively captures\nthe relationship between the image and the text by representing them in a\nsimilar feature space. Furthermore, it is essential to model the interactions\nbetween the image and text features through intermediate fusion. Most existing\nmethods either employ multimodal pre-training or intermediate fusion, but not\nboth. In this work, we propose the Hate-CLIPper architecture, which explicitly\nmodels the cross-modal interactions between the image and text representations\nobtained using Contrastive Language-Image Pre-training (CLIP) encoders via a\nfeature interaction matrix (FIM). A simple classifier based on the FIM\nrepresentation is able to achieve state-of-the-art performance on the Hateful\nMemes Challenge (HMC) dataset with an AUROC of 85.8, which even surpasses the\nhuman performance of 82.65. Experiments on other meme datasets such as\nPropaganda Memes and TamilMemes also demonstrate the generalizability of the\nproposed approach. Finally, we analyze the interpretability of the FIM\nrepresentation and show that cross-modal interactions can indeed facilitate the\nlearning of meaningful concepts. The code for this work is available at\nhttps://github.com/gokulkarthik/hateclipper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gokul Karthik Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandakumar_K/0/1/0/all/0/1\">Karthik Nandakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summary on the ISCSLP 2022 Chinese-English Code-Switching ASR Challenge. (arXiv:2210.06091v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06091","description":"<p>Code-switching automatic speech recognition becomes one of the most\nchallenging and the most valuable scenarios of automatic speech recognition,\ndue to the code-switching phenomenon between multilingual language and the\nfrequent occurrence of code-switching phenomenon in daily life. The ISCSLP 2022\nChinese-English Code-Switching Automatic Speech Recognition (CSASR) Challenge\naims to promote the development of code-switching automatic speech recognition.\nThe ISCSLP 2022 CSASR challenge provided two training sets, TAL_CSASR corpus\nand MagicData-RAMC corpus, a development and a test set for participants, which\nare used for CSASR model training and evaluation. Along with the challenge, we\nalso provide the baseline system performance for reference. As a result, more\nthan 40 teams participated in this challenge, and the winner team achieved\n16.70% Mixture Error Rate (MER) performance on the test set and has achieved\n9.8% MER absolute improvement compared with the baseline system. In this paper,\nwe will describe the datasets, the associated baselines system and the\nrequirements, and summarize the CSASR challenge results and major techniques\nand tricks used in the submitted systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuhao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinfeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Runyan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gaofeng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yonghong Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Radiology Report Generation Systems by Removing Hallucinated References to Non-existent Priors. (arXiv:2210.06340v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06340","description":"<p>Current deep learning models trained to generate radiology reports from chest\nradiographs are capable of producing clinically accurate, clear, and actionable\ntext that can advance patient care. However, such systems all succumb to the\nsame problem: making hallucinated references to non-existent prior reports.\nSuch hallucinations occur because these models are trained on datasets of\nreal-world patient reports that inherently refer to priors. To this end, we\npropose two methods to remove references to priors in radiology reports: (1) a\nGPT-3-based few-shot approach to rewrite medical reports without references to\npriors; and (2) a BioBERT-based token classification approach to directly\nremove words referring to priors. We use the aforementioned approaches to\nmodify MIMIC-CXR, a publicly available dataset of chest X-rays and their\nassociated free-text radiology reports; we then retrain CXR-RePaiR, a radiology\nreport generation system, on the adapted MIMIC-CXR dataset. We find that our\nre-trained model--which we call CXR-ReDonE--outperforms previous report\ngeneration methods on clinical metrics, achieving an average BERTScore of\n0.2351 (2.57% absolute improvement). We expect our approach to be broadly\nvaluable in enabling current radiology report generation systems to be more\ndirectly integrated into clinical pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1\">Vignav Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_N/0/1/0/all/0/1\">Nathan Andrew Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GMP*: Well-Tuned Global Magnitude Pruning Can Outperform Most BERT-Pruning Methods. (arXiv:2210.06384v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06384","description":"<p>We revisit the performance of the classic gradual magnitude pruning (GMP)\nbaseline for large language models, focusing on the classic BERT benchmark on\nvarious popular tasks. Despite existing evidence in the literature that GMP\nperforms poorly, we show that a simple and general variant, which we call GMP*,\ncan match and sometimes outperform more complex state-of-the-art methods. Our\nresults provide a simple yet strong baseline for future work, highlight the\nimportance of parameter tuning for baselines, and even improve the performance\nof the state-of-the-art second-order pruning method in this setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurtic_E/0/1/0/all/0/1\">Eldar Kurtic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relational Graph Convolutional Neural Networks for Multihop Reasoning: A Comparative Study. (arXiv:2210.06418v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06418","description":"<p>Multihop Question Answering is a complex Natural Language Processing task\nthat requires multiple steps of reasoning to find the correct answer to a given\nquestion. Previous research has explored the use of models based on Graph\nNeural Networks for tackling this task. Various architectures have been\nproposed, including Relational Graph Convolutional Networks (RGCN). For these\nmany node types and relations between them have been introduced, such as simple\nentity co-occurrences, modelling coreferences, or \"reasoning paths\" from\nquestions to answers via intermediary entities. Nevertheless, a thoughtful\nanalysis on which relations, node types, embeddings and architecture are the\nmost beneficial for this task is still missing. In this paper we explore a\nnumber of RGCN-based Multihop QA models, graph relations, and node embeddings,\nand empirically explore the influence of each on Multihop QA performance on the\nWikiHop dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Staliunaite_I/0/1/0/all/0/1\">Ieva Stali&#x16b;nait&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorinski_P/0/1/0/all/0/1\">Philip John Gorinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iacobacci_I/0/1/0/all/0/1\">Ignacio Iacobacci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfoCSE: Information-aggregated Contrastive Learning of Sentence Embeddings. (arXiv:2210.06432v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06432","description":"<p>Contrastive learning has been extensively studied in sentence embedding\nlearning, which assumes that the embeddings of different views of the same\nsentence are closer. The constraint brought by this assumption is weak, and a\ngood sentence representation should also be able to reconstruct the original\nsentence fragments. Therefore, this paper proposes an information-aggregated\ncontrastive learning framework for learning unsupervised sentence embeddings,\ntermed InfoCSE. InfoCSE forces the representation of [CLS] positions to\naggregate denser sentence information by introducing an additional Masked\nlanguage model task and a well-designed network. We evaluate the proposed\nInfoCSE on several benchmark datasets w.r.t the semantic text similarity (STS)\ntask. Experimental results show that InfoCSE outperforms SimCSE by an average\nSpearman correlation of 2.60% on BERT-base, and 1.77% on BERT-large, achieving\nstate-of-the-art results among unsupervised sentence representation learning\nmethods. Our code are available at\nhttps://github.com/caskcsg/sentemb/tree/main/InfoCSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chaochen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zijia Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}