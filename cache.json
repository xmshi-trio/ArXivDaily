{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-24T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Named Entity Recognition Based Automatic Generation of Research Highlights. (arXiv:2303.12795v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12795","description":"<p>A scientific paper is traditionally prefaced by an abstract that summarizes\nthe paper. Recently, research highlights that focus on the main findings of the\npaper have emerged as a complementary summary in addition to an abstract.\nHowever, highlights are not yet as common as abstracts, and are absent in many\npapers. In this paper, we aim to automatically generate research highlights\nusing different sections of a research paper as input. We investigate whether\nthe use of named entity recognition on the input improves the quality of the\ngenerated highlights. In particular, we have used two deep learning-based\nmodels: the first is a pointer-generator network, and the second augments the\nfirst model with coverage mechanism. We then augment each of the above models\nwith named entity recognition features. The proposed method can be used to\nproduce highlights for papers with missing highlights. Our experiments show\nthat adding named entity information improves the performance of the deep\nlearning-based summarizers in terms of ROUGE, METEOR and BERTScore measures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rehman_T/0/1/0/all/0/1\">Tohida Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_D/0/1/0/all/0/1\">Debarshi Kumar Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_P/0/1/0/all/0/1\">Prasenjit Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Samiran Chattopadhyay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis of Abstractive Text Summarization Using Pre-trained Models. (arXiv:2303.12796v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12796","description":"<p>People nowadays use search engines like Google, Yahoo, and Bing to find\ninformation on the Internet. Due to explosion in data, it is helpful for users\nif they are provided relevant summaries of the search results rather than just\nlinks to webpages. Text summarization has become a vital approach to help\nconsumers swiftly grasp vast amounts of information.In this paper, different\npre-trained models for text summarization are evaluated on different datasets.\nSpecifically, we have used three different pre-trained models, namely,\ngoogle/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have\nconsidered three different datasets, namely, CNN-dailymail, SAMSum and BillSum\nto get the output from the above three models. The pre-trained models are\ncompared over these different datasets, each of 2000 examples, through ROUGH\nand BLEU metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rehman_T/0/1/0/all/0/1\">Tohida Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Suchandan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_D/0/1/0/all/0/1\">Debarshi Kumar Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Samiran Chattopadhyay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Features matching using natural language processing. (arXiv:2303.12804v1 [cs.DB])","link":"http://arxiv.org/abs/2303.12804","description":"<p>The feature matching is a basic step in matching different datasets. This\narticle proposes shows a new hybrid model of a pretrained Natural Language\nProcessing (NLP) based model called BERT used in parallel with a statistical\nmodel based on Jaccard similarity to measure the similarity between list of\nfeatures from two different datasets. This reduces the time required to search\nfor correlations or manually match each feature from one dataset to another.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khilji_M/0/1/0/all/0/1\">Muhammad Danial Khilji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PACO: Provocation Involving Action, Culture, and Oppression. (arXiv:2303.12808v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12808","description":"<p>In India, people identify with a particular group based on certain attributes\nsuch as religion. The same religious groups are often provoked against each\nother. Previous studies show the role of provocation in increasing tensions\nbetween India's two prominent religious groups: Hindus and Muslims. With the\nadvent of the Internet, such provocation also surfaced on social media\nplatforms such as WhatsApp.\n</p>\n<p>By leveraging an existing dataset of Indian WhatsApp posts, we identified\nthree categories of provoking sentences against Indian Muslims. Further, we\nlabeled 7,000 sentences for three provocation categories and called this\ndataset PACO. We leveraged PACO to train a model that can identify provoking\nsentences from a WhatsApp post. Our best model is fine-tuned RoBERTa and\nachieved a 0.851 average AUC score over five-fold cross-validation.\nAutomatically identifying provoking sentences could stop provoking text from\nreaching out to the masses, and can prevent possible discrimination or violence\nagainst the target religious group.\n</p>\n<p>Further, we studied the provocative speech through a pragmatic lens, by\nidentifying the dialog acts and impoliteness super-strategies used against the\nreligious group.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_V/0/1/0/all/0/1\">Vaibhav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Ganning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Munindar P. Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning Skills of LLMs. (arXiv:2303.12810v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12810","description":"<p>The potential of large language models (LLMs) to reason like humans has been\na highly contested topic in Machine Learning communities. However, the\nreasoning abilities of humans are multifaceted and can be seen in various\nforms, including analogical, spatial and moral reasoning, among others. This\nfact raises the question whether LLMs can perform equally well across all these\ndifferent domains. This research work aims to investigate the performance of\nLLMs on different reasoning tasks by conducting experiments that directly use\nor draw inspirations from existing datasets on analogical and spatial\nreasoning. Additionally, to evaluate the ability of LLMs to reason like human,\ntheir performance is evaluted on more open-ended, natural language questions.\nMy findings indicate that LLMs excel at analogical and moral reasoning, yet\nstruggle to perform as proficiently on spatial reasoning tasks. I believe these\nexperiments are crucial for informing the future development of LLMs,\nparticularly in contexts that require diverse reasoning proficiencies. By\nshedding light on the reasoning abilities of LLMs, this study aims to push\nforward our understanding of how they can better emulate the cognitive\nabilities of humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Shrivats Agrawal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])","link":"http://arxiv.org/abs/2303.12816","description":"<p>Knowledge graph embedding (KGE) that maps entities and relations into vector\nrepresentations is essential for downstream tasks. Conventional KGE methods\nrequire relatively high-dimensional entity representations to preserve the\nstructural information of knowledge graph, but lead to oversized model\nparameters. Recent methods reduce model parameters by adopting low-dimensional\nentity representations, while developing techniques (e.g., knowledge\ndistillation) to compensate for the reduced dimension. However, such operations\nproduce degraded model accuracy and limited reduction of model parameters.\nSpecifically, we view the concatenation of all entity representations as an\nembedding layer, and then conventional KGE methods that adopt high-dimensional\nentity representations equal to enlarging the width of the embedding layer to\ngain expressiveness. To achieve parameter efficiency without sacrificing\naccuracy, we instead increase the depth and propose a deeper embedding network\nfor entity representations, i.e., a narrow embedding layer and a multi-layer\ndimension lifting network (LiftNet). Experiments on three public datasets show\nthat the proposed method (implemented based on TransE and DistMult) with\n4-dimensional entity representations achieves more accurate link prediction\nresults than counterpart parameter-efficient KGE methods and strong KGE\nbaselines, including TransE and DistMult with 512-dimensional entity\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_B/0/1/0/all/0/1\">Borui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Longxiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_T/0/1/0/all/0/1\">Tom Luan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salient Span Masking for Temporal Understanding. (arXiv:2303.12860v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12860","description":"<p>Salient Span Masking (SSM) has shown itself to be an effective strategy to\nimprove closed-book question answering performance. SSM extends general masked\nlanguage model pretraining by creating additional unsupervised training\nsentences that mask a single entity or date span, thus oversampling factual\ninformation. Despite the success of this paradigm, the span types and sampling\nstrategies are relatively arbitrary and not widely studied for other tasks.\nThus, we investigate SSM from the perspective of temporal tasks, where learning\na good representation of various temporal expressions is important. To that\nend, we introduce Temporal Span Masking (TSM) intermediate training. First, we\nfind that SSM alone improves the downstream performance on three temporal tasks\nby an avg. +5.8 points. Further, we are able to achieve additional improvements\n(avg. +0.29 points) by adding the TSM task. These comprise the new best\nreported results on the targeted tasks. Our analysis suggests that the\neffectiveness of SSM stems from the sentences chosen in the training data\nrather than the mask choice: sentences with entities frequently also contain\ntemporal expressions. Nonetheless, the additional targeted spans of TSM can\nstill improve performance, especially in a zero-shot context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1\">Jeremy R. Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aditi Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JaCoText: A Pretrained Model for Java Code-Text Generation. (arXiv:2303.12869v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12869","description":"<p>Pretrained transformer-based models have shown high performance in natural\nlanguage generation task. However, a new wave of interest has surged: automatic\nprogramming language generation. This task consists of translating natural\nlanguage instructions to a programming code. Despite the fact that well-known\npretrained models on language generation have achieved good performance in\nlearning programming languages, effort is still needed in automatic code\ngeneration. In this paper, we introduce JaCoText, a model based on Transformers\nneural network. It aims to generate java source code from natural language\ntext. JaCoText leverages advantages of both natural language and code\ngeneration models. More specifically, we study some findings from the state of\nthe art and use them to (1) initialize our model from powerful pretrained\nmodels, (2) explore additional pretraining on our java dataset, (3) carry out\nexperiments combining the unimodal and bimodal data in the training, and (4)\nscale the input and output length during the fine-tuning of the model.\nConducted experiments on CONCODE dataset show that JaCoText achieves new\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Espejel_J/0/1/0/all/0/1\">Jessica L&#xf3;pez Espejel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alassan_M/0/1/0/all/0/1\">Mahaman Sanoussi Yahaya Alassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahhane_W/0/1/0/all/0/1\">Walid Dahhane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ettifouri_E/0/1/0/all/0/1\">El Hassane Ettifouri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification. (arXiv:2303.12892v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12892","description":"<p>In recent years, Transformer-based models such as the Switch Transformer have\nachieved remarkable results in natural language processing tasks. However,\nthese models are often too complex and require extensive pre-training, which\nlimits their effectiveness for small clinical text classification tasks with\nlimited data. In this study, we propose a simplified Switch Transformer\nframework and train it from scratch on a small French clinical text\nclassification dataset at CHU Sainte-Justine hospital. Our results demonstrate\nthat the simplified small-scale Transformer models outperform pre-trained\nBERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT.\nAdditionally, using a mixture of expert mechanisms from the Switch Transformer\nhelps capture diverse patterns; hence, the proposed approach achieves better\nresults than a conventional Transformer with the self-attention mechanism.\nFinally, our proposed framework achieves an accuracy of 87\\%, precision at\n87\\%, and recall at 85\\%, compared to the third-best pre-trained BERT-based\nmodel, FlauBERT, which achieved an accuracy of 84\\%, precision at 84\\%, and\nrecall at 84\\%. However, Switch Transformers have limitations, including a\ngeneralization gap and sharp minima. We compare it with a multi-layer\nperceptron neural network for small French clinical narratives classification\nand show that the latter outperforms all other models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thanh-Dung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouvet_P/0/1/0/all/0/1\">Philippe Jouvet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noumeir_R/0/1/0/all/0/1\">Rita Noumeir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding the Generalization of Medical Text-to-SQL Models and Datasets. (arXiv:2303.12898v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12898","description":"<p>Electronic medical records (EMRs) are stored in relational databases. It can\nbe challenging to access the required information if the user is unfamiliar\nwith the database schema or general database fundamentals. Hence, researchers\nhave explored text-to-SQL generation methods that provide healthcare\nprofessionals direct access to EMR data without needing a database expert.\nHowever, currently available datasets have been essentially \"solved\" with\nstate-of-the-art models achieving accuracy greater than or near 90%. In this\npaper, we show that there is still a long way to go before solving text-to-SQL\ngeneration in the medical domain. To show this, we create new splits of the\nexisting medical text-to-SQL dataset MIMICSQL that better measure the\ngeneralizability of the resulting models. We evaluate state-of-the-art language\nmodels on our new split showing substantial drops in performance with accuracy\ndropping from up to 92% to 28%, thus showing substantial room for improvement.\nMoreover, we introduce a novel data augmentation approach to improve the\ngeneralizability of the language models. Overall, this paper is the first step\ntowards developing more robust text-to-SQL models in the medical\ndomain.\\footnote{The dataset and code will be released upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tarbell_R/0/1/0/all/0/1\">Richard Tarbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_K/0/1/0/all/0/1\">Kim-Kwang Raymond Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietrich_G/0/1/0/all/0/1\">Glenn Dietrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rios_A/0/1/0/all/0/1\">Anthony Rios</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Generalizability of Deep Contextualized Language Representations For Text Classification. (arXiv:2303.12936v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12936","description":"<p>This study evaluates the robustness of two state-of-the-art deep contextual\nlanguage representations, ELMo and DistilBERT, on supervised learning of binary\nprotest news classification and sentiment analysis of product reviews. A\n\"cross-context\" setting is enabled using test sets that are distinct from the\ntraining data. Specifically, in the news classification task, the models are\ndeveloped on local news from India and tested on the local news from China. In\nthe sentiment analysis task, the models are trained on movie reviews and tested\non customer reviews. This comparison is aimed at exploring the limits of the\nrepresentative power of today's Natural Language Processing systems on the path\nto the systems that are generalizable to real-life scenarios. The models are\nfine-tuned and fed into a Feed-Forward Neural Network and a Bidirectional Long\nShort Term Memory network. Multinomial Naive Bayes and Linear Support Vector\nMachine are used as traditional baselines. The results show that, in binary\ntext classification, DistilBERT is significantly better than ELMo on\ngeneralizing to the cross-context setting. ELMo is observed to be significantly\nmore robust to the cross-context test data than both baselines. On the other\nhand, the baselines performed comparably well to ELMo when the training and\ntest data are subsets of the same corpus (no cross-context). DistilBERT is also\nfound to be 30% smaller and 83% faster than ELMo. The results suggest that\nDistilBERT can transfer generic semantic knowledge to other domains better than\nELMo. DistilBERT is also favorable in incorporating into real-life systems for\nit requires a smaller computational training budget. When generalization is not\nthe utmost preference and test domain is similar to the training domain, the\ntraditional ML algorithms can still be considered as more economic alternatives\nto deep language representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buyukoz_B/0/1/0/all/0/1\">Berfu Buyukoz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT A Good Keyphrase Generator? A Preliminary Study. (arXiv:2303.13001v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13001","description":"<p>The emergence of ChatGPT has recently garnered significant attention from the\ncomputational linguistics community. To demonstrate its capabilities as a\nkeyphrase generator, we conduct a preliminary evaluation of ChatGPT for the\nkeyphrase generation task. We evaluate its performance in various aspects,\nincluding keyphrase generation prompts, keyphrase generation diversity,\nmulti-domain keyphrase generation, and long document understanding. Our\nevaluation is based on six benchmark datasets, and we adopt the prompt\nsuggested by OpenAI while extending it to six candidate prompts. We find that\nChatGPT performs exceptionally well on all six candidate prompts, with minor\nperformance differences observed across the datasets. Based on our findings, we\nconclude that ChatGPT has great potential for keyphrase generation. Moreover,\nwe discover that ChatGPT still faces challenges when it comes to generating\nabsent keyphrases. Meanwhile, in the final section, we also present some\nlimitations and future expansions of this report.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Songfang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shilong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huafeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GesGPT: Speech Gesture Synthesis With Text Parsing from GPT. (arXiv:2303.13013v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13013","description":"<p>Gesture synthesis has gained significant attention as a critical research\narea, focusing on producing contextually appropriate and natural gestures\ncorresponding to speech or textual input. Although deep learning-based\napproaches have achieved remarkable progress, they often overlook the rich\nsemantic information present in the text, leading to less expressive and\nmeaningful gestures. We propose GesGPT, a novel approach to gesture generation\nthat leverages the semantic analysis capabilities of Large Language Models\n(LLMs), such as GPT. By capitalizing on the strengths of LLMs for text\nanalysis, we design prompts to extract gesture-related information from textual\ninput. Our method entails developing prompt principles that transform gesture\ngeneration into an intention classification problem based on GPT, and utilizing\na curated gesture library and integration module to produce semantically rich\nco-speech gestures. Experimental results demonstrate that GesGPT effectively\ngenerates contextually appropriate and expressive gestures, offering a new\nperspective on semantic co-speech gesture generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Nan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zeyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuwu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_D/0/1/0/all/0/1\">Dongdong Weng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13035","description":"<p>Electronic health records (EHRs) store an extensive array of patient\ninformation, encompassing medical histories, diagnoses, treatments, and test\noutcomes. These records are crucial for enabling healthcare providers to make\nwell-informed decisions regarding patient care. Summarizing clinical notes\nfurther assists healthcare professionals in pinpointing potential health risks\nand making better-informed decisions. This process contributes to reducing\nerrors and enhancing patient outcomes by ensuring providers have access to the\nmost pertinent and current patient data. Recent research has shown that\nincorporating prompts with large language models (LLMs) substantially boosts\nthe efficacy of summarization tasks. However, we show that this approach also\nleads to increased output variance, resulting in notably divergent outputs even\nwhen prompts share similar meanings. To tackle this challenge, we introduce a\nmodel-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft\nprompts to diminish variance while preserving the advantages of prompt-based\nsummarization. Experimental findings on multiple clinical note tasks and LLMs\nindicate that our method not only bolsters performance but also effectively\ncurbs variance for various LLMs, providing a more uniform and dependable\nsolution for summarizing vital medical information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yu-Neng Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-Augmented Classification with Decoupled Representation. (arXiv:2303.13065v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13065","description":"<p>Pretrained language models (PLMs) have shown marvelous improvements across\nvarious NLP tasks. Most Chinese PLMs simply treat an input text as a sequence\nof characters, and completely ignore word information. Although Whole Word\nMasking can alleviate this, the semantics in words is still not well\nrepresented. In this paper, we revisit the segmentation granularity of Chinese\nPLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both\ncharacters and words. To achieve this, we design objective functions for\nlearning both character and word-level representations. We conduct extensive\nexperiments on various Chinese NLP tasks to evaluate existing PLMs as well as\nthe proposed MigBERT. Experimental results show that MigBERT achieves new SOTA\nperformance on all these tasks. Further analysis demonstrates that words are\nsemantically richer than characters. More interestingly, we show that MigBERT\nalso works with Japanese. Our code has been released\nhere~\\footnote{\\url{https://github.com/xnliang98/MigBERT}} and you can download\nour model here~\\footnote{\\url{https://huggingface.co/xnliang/MigBERT-large/}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinnian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaqi Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1\">Chao Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Universal Transformer: block reusing with adaptor in Transformer for automatic speech recognit. (arXiv:2303.13072v1 [cs.SD])","link":"http://arxiv.org/abs/2303.13072","description":"<p>Transformer-based models have recently made significant achievements in the\napplication of end-to-end (E2E) automatic speech recognition (ASR). It is\npossible to deploy the E2E ASR system on smart devices with the help of\nTransformer-based models. While these models still have the disadvantage of\nrequiring a large number of model parameters. To overcome the drawback of\nuniversal Transformer models for the application of ASR on edge devices, we\npropose a solution that can reuse the block in Transformer models for the\noccasion of the small footprint ASR system, which meets the objective of\naccommodating resource limitations without compromising recognition accuracy.\nSpecifically, we design a novel block-reusing strategy for speech Transformer\n(BRST) to enhance the effectiveness of parameters and propose an adapter module\n(ADM) that can produce a compact and adaptable model with only a few additional\ntrainable parameters accompanying each reusing block. We conducted an\nexperiment with the proposed method on the public AISHELL-1 corpus, and the\nresults show that the proposed approach achieves the character error rate (CER)\nof 9.3%/6.63% with only 7.6M/8.3M parameters without and with the ADM,\nrespectively. In addition, we also make a deeper analysis to show the effect of\nADM in the general block-reusing method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinfeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer. (arXiv:2303.13099v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13099","description":"<p>In Task Oriented Dialogue (TOD) system, detecting and inducing new intents\nare two main challenges to apply the system in the real world. In this paper,\nwe suggest the semantic multi-view model to resolve these two challenges: (1)\nSBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue\ndomain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized\nsemantic. MDB feeds diverse dialogue datasets to the model at once to tackle\nthe multi-domain problem by learning the multiple domain knowledge. We\nintroduce a novel method PGT, which employs the Siamese network to fine-tune\nthe model with a clustering method directly.Our model can learn how to cluster\ndialogue utterances by using PGT. Experimental results demonstrate that our\nmulti-view model with MDB and PGT significantly improves the Open Intent\nInduction performance compared to baseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koh_H/0/1/0/all/0/1\">Hyukhun Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyun_H/0/1/0/all/0/1\">Haesung Pyun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nakyeong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Explanation for the Phase Transition in Large Language Models with List Decoding. (arXiv:2303.13112v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13112","description":"<p>Various recent experimental results show that large language models (LLM)\nexhibit emergent abilities that are not present in small models. System\nperformance is greatly improved after passing a certain critical threshold of\nscale. In this letter, we provide a simple explanation for such a phase\ntransition phenomenon. For this, we model an LLM as a sequence-to-sequence\nrandom function. Instead of using instant generation at each step, we use a\nlist decoder that keeps a list of candidate sequences at each step and defers\nthe generation of the output sequence at the end. We show that there is a\ncritical threshold such that the expected number of erroneous candidate\nsequences remains bounded when an LLM is below the threshold, and it grows\nexponentially when an LLM is above the threshold. Such a threshold is related\nto the basic reproduction number in a contagious disease.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Cheng-Shang Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness-guided Few-shot Prompting for Large Language Models. (arXiv:2303.13217v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13217","description":"<p>Large language models have demonstrated surprising ability to perform\nin-context learning, i.e., these models can be directly applied to solve\nnumerous downstream tasks by conditioning on a prompt constructed by a few\ninput-output examples. However, prior research has shown that in-context\nlearning can suffer from high instability due to variations in training\nexamples, example order, and prompt formats. Therefore, the construction of an\nappropriate prompt is essential for improving the performance of in-context\nlearning. In this paper, we revisit this problem from the view of predictive\nbias. Specifically, we introduce a metric to evaluate the predictive bias of a\nfixed prompt against labels or a given attributes. Then we empirically show\nthat prompts with higher bias always lead to unsatisfactory predictive quality.\nBased on this observation, we propose a novel search strategy based on the\ngreedy search to identify the near-optimal prompt for improving the performance\nof in-context learning. We perform comprehensive experiments with\nstate-of-the-art mainstream models such as GPT-3 on various downstream tasks.\nOur results indicate that our method can enhance the model's in-context\nlearning performance in an effective and interpretable manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Huan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1\">Yatao Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bingzhe Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Sparse Retrievers and Rerankers using Adapters. (arXiv:2303.13220v1 [cs.IR])","link":"http://arxiv.org/abs/2303.13220","description":"<p>Parameter-Efficient transfer learning with Adapters have been studied in\nNatural Language Processing (NLP) as an alternative to full fine-tuning.\nAdapters are memory-efficient and scale well with downstream tasks by training\nsmall bottle-neck layers added between transformer layers while keeping the\nlarge pretrained language model (PLMs) frozen. In spite of showing promising\nresults in NLP, these methods are under-explored in Information Retrieval.\nWhile previous studies have only experimented with dense retriever or in a\ncross lingual retrieval scenario, in this paper we aim to complete the picture\non the use of adapters in IR. First, we study adapters for SPLADE, a sparse\nretriever, for which adapters not only retain the efficiency and effectiveness\notherwise achieved by finetuning, but are memory-efficient and orders of\nmagnitude lighter to train. We observe that Adapters-SPLADE not only optimizes\njust 2\\% of training parameters, but outperforms fully fine-tuned counterpart\nand existing parameter-efficient dense IR models on IR benchmark datasets.\nSecondly, we address domain adaptation of neural retrieval thanks to adapters\non cross-domain BEIR datasets and TripClick. Finally, we also consider\nknowledge sharing between rerankers and first stage rankers. Overall, our study\ncomplete the examination of adapters for neural IR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pal_V/0/1/0/all/0/1\">Vaishali Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassance_C/0/1/0/all/0/1\">Carlos Lassance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dejean_H/0/1/0/all/0/1\">Herv&#xe9; D&#xe9;jean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual-Language Prompt Tuning with Knowledge-guided Context Optimization. (arXiv:2303.13283v1 [cs.CV])","link":"http://arxiv.org/abs/2303.13283","description":"<p>Prompt tuning is an effective way to adapt the pre-trained visual-language\nmodel (VLM) to the downstream task using task-related textual tokens.\nRepresentative CoOp-based work combines the learnable textual tokens with the\nclass tokens to obtain specific textual knowledge. However, the specific\ntextual knowledge is the worse generalization to the unseen classes because it\nforgets the essential general textual knowledge having a strong generalization\nability. To tackle this issue, we introduce a novel Knowledge-guided Context\nOptimization (KgCoOp) to enhance the generalization ability of the learnable\nprompt for unseen classes. The key insight of KgCoOp is that forgetting about\nessential knowledge can be alleviated by reducing the discrepancy between the\nlearnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the\ndiscrepancy between the textual embeddings generated by learned prompts and the\nhand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can\nmake a discriminative prompt for both seen and unseen tasks. Extensive\nevaluation of several benchmarks demonstrates that the proposed\nKnowledge-guided Context Optimization is an efficient method for prompt tuning,\n\\emph{i.e.,} achieves better performance with less training time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Hantao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering. (arXiv:2303.13284v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13284","description":"<p>In this work, we present an end-to-end Knowledge Graph Question Answering\n(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text\npre-trained language model. The model takes a question in natural language as\ninput and produces a simpler form of the intended SPARQL query. In the simpler\nform, the model does not directly produce entity and relation IDs. Instead, it\nproduces corresponding entity and relation labels. The labels are grounded to\nKG entity and relation IDs in a subsequent step. To further improve the\nresults, we instruct the model to produce a truncated version of the KG\nembedding for each entity. The truncated KG embedding enables a finer search\nfor disambiguation purposes. We find that T5 is able to learn the truncated KG\nembeddings without any change of loss function, improving KGQA performance. As\na result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata\ndatasets on end-to-end KGQA over Wikidata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debayan Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_P/0/1/0/all/0/1\">Pranav Ajit Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SwissBERT: The Multilingual Language Model for Switzerland. (arXiv:2303.13310v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13310","description":"<p>We present SwissBERT, a masked language model created specifically for\nprocessing Switzerland-related text. SwissBERT is a pre-trained model that we\nadapted to news articles written in the national languages of Switzerland --\nGerman, French, Italian, and Romansh. We evaluate SwissBERT on natural language\nunderstanding tasks related to Switzerland and find that it tends to outperform\nprevious models on these tasks, especially when processing contemporary news\nand/or Romansh Grischun. Since SwissBERT uses language adapters, it may be\nextended to Swiss German dialects in future work. The model and our open-source\ncode are publicly released at https://github.com/ZurichNLP/swissbert.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vamvas_J/0/1/0/all/0/1\">Jannis Vamvas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graen_J/0/1/0/all/0/1\">Johannes Gra&#xeb;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Foundation Models for Clinical Text Analysis. (arXiv:2303.13314v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13314","description":"<p>Infectious diseases are a significant public health concern globally, and\nextracting relevant information from scientific literature can facilitate the\ndevelopment of effective prevention and treatment strategies. However, the\nlarge amount of clinical data available presents a challenge for information\nextraction. To address this challenge, this study proposes a natural language\nprocessing (NLP) framework that uses a pre-trained transformer model fine-tuned\non task-specific data to extract key information related to infectious diseases\nfrom free-text clinical data. The proposed framework includes three components:\na data layer for preparing datasets from clinical texts, a foundation model\nlayer for entity extraction, and an assessment layer for performance analysis.\nThe results of the evaluation indicate that the proposed method outperforms\nstandard methods, and leveraging prior knowledge through the pre-trained\ntransformer model makes it useful for investigating other infectious diseases\nin the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1\">Shaina Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashir_S/0/1/0/all/0/1\">Syed Raza Bashir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph. (arXiv:2303.13351v1 [cs.DL])","link":"http://arxiv.org/abs/2303.13351","description":"<p>In this work we create a question answering dataset over the DBLP scholarly\nknowledge graph (KG). DBLP is an on-line reference for bibliographic\ninformation on major computer science publications that indexes over 4.4\nmillion publications published by more than 2.2 million authors. Our dataset\nconsists of 10,000 question answer pairs with the corresponding SPARQL queries\nwhich can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD\nis the largest scholarly question answering dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debayan Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awale_S/0/1/0/all/0/1\">Sushil Awale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revealing Weaknesses of Vietnamese Language Models Through Unanswerable Questions in Machine Reading Comprehension. (arXiv:2303.13355v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13355","description":"<p>Although the curse of multilinguality significantly restricts the language\nabilities of multilingual models in monolingual settings, researchers now still\nhave to rely on multilingual models to develop state-of-the-art systems in\nVietnamese Machine Reading Comprehension. This difficulty in researching is\nbecause of the limited number of high-quality works in developing Vietnamese\nlanguage models. In order to encourage more work in this research field, we\npresent a comprehensive analysis of language weaknesses and strengths of\ncurrent Vietnamese monolingual models using the downstream task of Machine\nReading Comprehension. From the analysis results, we suggest new directions for\ndeveloping Vietnamese language models. Besides this main contribution, we also\nsuccessfully reveal the existence of artifacts in Vietnamese Machine Reading\nComprehension benchmarks and suggest an urgent need for new high-quality\nbenchmarks to track the progress of Vietnamese Machine Reading Comprehension.\nMoreover, we also introduced a minor but valuable modification to the process\nof annotating unanswerable questions for Machine Reading Comprehension from\nprevious work. Our proposed modification helps improve the quality of\nunanswerable questions to a higher level of difficulty for Machine Reading\nComprehension systems to solve.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Quoc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_P/0/1/0/all/0/1\">Phong Nguyen-Thuan Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the Scalable Evaluation of Cooperativeness in Language Models. (arXiv:2303.13360v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13360","description":"<p>It is likely that AI systems driven by pre-trained language models (PLMs)\nwill increasingly be used to assist humans in high-stakes interactions with\nother agents, such as negotiation or conflict resolution. Consistent with the\ngoals of Cooperative AI \\citep{dafoe_open_2020}, we wish to understand and\nshape the multi-agent behaviors of PLMs in a pro-social manner. An important\nfirst step is the evaluation of model behaviour across diverse cooperation\nproblems. Since desired behaviour in an interaction depends upon precise\ngame-theoretic structure, we focus on generating scenarios with particular\nstructures with both crowdworkers and a language model. Our work proceeds as\nfollows. First, we discuss key methodological issues in the generation of\nscenarios corresponding to particular game-theoretic structures. Second, we\nemploy both crowdworkers and a language model to generate such scenarios. We\nfind that the quality of generations tends to be mediocre in both cases. We\nadditionally get both crowdworkers and a language model to judge whether given\nscenarios align with their intended game-theoretic structure, finding mixed\nresults depending on the game. Third, we provide a dataset of scenario based on\nour data generated. We provide both quantitative and qualitative evaluations of\nUnifiedQA and GPT-3 on this dataset. We find that instruct-tuned models tend to\nact in a way that could be perceived as cooperative when scaled up, while other\nmodels seemed to have flat scaling trends.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Alan Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riche_M/0/1/0/all/0/1\">Maxime Rich&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_J/0/1/0/all/0/1\">Jesse Clifton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reevaluating Data Partitioning for Emotion Detection in EmoWOZ. (arXiv:2303.13364v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13364","description":"<p>This paper focuses on the EmoWoz dataset, an extension of MultiWOZ that\nprovides emotion labels for the dialogues. MultiWOZ was partitioned initially\nfor another purpose, resulting in a distributional shift when considering the\nnew purpose of emotion recognition. The emotion tags in EmoWoz are highly\nimbalanced and unevenly distributed across the partitions, which causes\nsub-optimal performance and poor comparison of models. We propose a stratified\nsampling scheme based on emotion tags to address this issue, improve the\ndataset's distribution, and reduce dataset shift. We also introduce a special\ntechnique to handle conversation (sequential) data with many emotional tags.\nUsing our proposed sampling method, models built upon EmoWoz can perform\nbetter, making it a more reliable resource for training conversational agents\nwith emotional intelligence. We recommend that future researchers use this new\npartitioning to ensure consistent and accurate performance evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mostafavi_M/0/1/0/all/0/1\">Moeen Mostafavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porter_M/0/1/0/all/0/1\">Michael D. Porter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Requirement Formalisation using Natural Language Processing and Machine Learning: A Systematic Review. (arXiv:2303.13365v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13365","description":"<p>Improvement of software development methodologies attracts developers to\nautomatic Requirement Formalisation (RF) in the Requirement Engineering (RE)\nfield. The potential advantages by applying Natural Language Processing (NLP)\nand Machine Learning (ML) in reducing the ambiguity and incompleteness of\nrequirement written in natural languages is reported in different studies. The\ngoal of this paper is to survey and classify existing work on NLP and ML for\nRF, identifying challenges in this domain and providing promising future\nresearch directions. To achieve this, we conducted a systematic literature\nreview to outline the current state-of-the-art of NLP and ML techniques in RF\nby selecting 257 papers from common used libraries. The search result is\nfiltered by defining inclusion and exclusion criteria and 47 relevant studies\nbetween 2012 and 2022 are selected. We found that heuristic NLP approaches are\nthe most common NLP techniques used for automatic RF, primary operating on\nstructured and semi-structured data. This study also revealed that Deep\nLearning (DL) technique are not widely used, instead classical ML techniques\nare predominant in the surveyed studies. More importantly, we identified the\ndifficulty of comparing the performance of different approaches due to the lack\nof standard benchmark cases for RF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolahdouz_Rahimi_S/0/1/0/all/0/1\">Shekoufeh Kolahdouz-Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lano_K/0/1/0/all/0/1\">Kevin Lano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT and a New Academic Reality: AI-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing. (arXiv:2303.13367v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13367","description":"<p>This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,\nwhich uses natural language processing to fulfill text-based user requests\n(i.e., a chatbot). The history and principles behind ChatGPT and similar models\nare discussed. This technology is then discussed in relation to its potential\nimpact on academia and scholarly research and publishing. ChatGPT is seen as a\npotential model for the automated preparation of essays and other types of\nscholarly manuscripts. Potential ethical issues that could arise with the\nemergence of large language models like GPT-3, the underlying technology behind\nChatGPT, and its usage by academics and researchers, are discussed and situated\nwithin the context of broader advancements in artificial intelligence, machine\nlearning, and natural language processing for research and scholarly\npublishing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lund_B/0/1/0/all/0/1\">Brady Lund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannuru_N/0/1/0/all/0/1\">Nishith Reddy Mannuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_B/0/1/0/all/0/1\">Bing Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimray_S/0/1/0/all/0/1\">Somipam Shimray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning ClimateBert transformer with ClimaText for the disclosure analysis of climate-related financial risks. (arXiv:2303.13373v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13373","description":"<p>In recent years there has been a growing demand from financial agents,\nespecially from particular and institutional investors, for companies to report\non climate-related financial risks. A vast amount of information, in text\nformat, can be expected to be disclosed in the short term by firms in order to\nidentify these types of risks in their financial and non financial reports,\nparticularly in response to the growing regulation that is being passed on the\nmatter. To this end, this paper applies state-of-the-art NLP techniques to\nachieve the detection of climate change in text corpora. We use transfer\nlearning to fine-tune two transformer models, BERT and ClimateBert -a recently\npublished DistillRoBERTa-based model that has been specifically tailored for\nclimate text classification-. These two algorithms are based on the transformer\narchitecture which enables learning the contextual relationships between words\nin a text. We carry out the fine-tuning process of both models on the novel\nClima-Text database, consisting of data collected from Wikipedia, 10K Files\nReports and web-based claims. Our text classification model obtained from the\nClimateBert fine-tuning process on ClimaText, outperforms the models created\nwith BERT and the current state-of-the-art transformer in this particular\nproblem. Our study is the first one to implement on the ClimaText database the\nrecently published ClimateBert algorithm. Based on our results, it can be said\nthat ClimateBert fine-tuned on ClimaText is an outstanding tool within the NLP\npre-trained transformer models that may and should be used by investors,\ninstitutional agents and companies themselves to monitor the disclosure of\nclimate risk in financial reports. In addition, our transfer learning\nmethodology is cheap in computational terms, thus allowing any organization to\nperform it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garrido_Merchan_E/0/1/0/all/0/1\">Eduardo C. Garrido-Merch&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Barthe_C/0/1/0/all/0/1\">Cristina Gonz&#xe1;lez-Barthe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaca_M/0/1/0/all/0/1\">Mar&#xed;a Coronado Vaca</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capabilities of GPT-4 on Medical Challenge Problems. (arXiv:2303.13375v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13375","description":"<p>Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation across various domains, including\nmedicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art\nLLM, on medical competency examinations and benchmark datasets. GPT-4 is a\ngeneral-purpose model that is not specialized for medical problems through\ntraining or engineered to solve clinical tasks. Our analysis covers two sets of\nofficial practice materials for the USMLE, a three-step examination program\nused to assess clinical competency and grant licensure in the United States. We\nalso evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond\nmeasuring model performance, experiments were conducted to investigate the\ninfluence of test questions containing both text and images on model\nperformance, probe for memorization of content during training, and study\nprobability calibration, which is of critical importance in high-stakes\napplications like medicine. Our results show that GPT-4, without any\nspecialized prompt crafting, exceeds the passing score on USMLE by over 20\npoints and outperforms earlier general-purpose models (GPT-3.5) as well as\nmodels specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned\nversion of Flan-PaLM 540B). In addition, GPT-4 is significantly better\ncalibrated than GPT-3.5, demonstrating a much-improved ability to predict the\nlikelihood that its answers are correct. We also explore the behavior of the\nmodel qualitatively through a case study that shows the ability of GPT-4 to\nexplain medical reasoning, personalize explanations to students, and\ninteractively craft new counterfactual scenarios around a medical case.\nImplications of the findings are discussed for potential uses of GPT-4 in\nmedical education, assessment, and clinical practice, with appropriate\nattention to challenges of accuracy and safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nori_H/0/1/0/all/0/1\">Harsha Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_N/0/1/0/all/0/1\">Nicholas King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKinney_S/0/1/0/all/0/1\">Scott Mayer McKinney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carignan_D/0/1/0/all/0/1\">Dean Carignan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical and Ethical Challenges of Large Language Models in Education: A Systematic Literature Review. (arXiv:2303.13379v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13379","description":"<p>Educational technology innovations that have been developed based on large\nlanguage models (LLMs) have shown the potential to automate the laborious\nprocess of generating and analysing textual content. While various innovations\nhave been developed to automate a range of educational tasks (e.g., question\ngeneration, feedback provision, and essay grading), there are concerns\nregarding the practicality and ethicality of these innovations. Such concerns\nmay hinder future research and the adoption of LLMs-based innovations in\nauthentic educational contexts. To address this, we conducted a systematic\nliterature review of 118 peer-reviewed papers published since 2017 to pinpoint\nthe current state of research on using LLMs to automate and support educational\ntasks. The practical and ethical challenges of LLMs-based innovations were also\nidentified by assessing their technological readiness, model performance,\nreplicability, system transparency, privacy, equality, and beneficence. The\nfindings were summarised into three recommendations for future studies,\nincluding updating existing innovations with state-of-the-art models (e.g.,\nGPT-3), embracing the initiative of open-sourcing models/systems, and adopting\na human-centred approach throughout the developmental process. These\nrecommendations could support future research to develop practical and ethical\ninnovations for supporting diverse educational tasks and benefiting students,\nteachers, and institutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Lixiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1\">Lele Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Linxuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Maldonado_R/0/1/0/all/0/1\">Roberto Martinez-Maldonado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueqiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasevic_D/0/1/0/all/0/1\">Dragan Ga&#x161;evi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Zero-Shot Domain Transfer with Text-to-Text Models. (arXiv:2303.13386v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13386","description":"<p>Label scarcity is a bottleneck for improving task performance in specialised\ndomains. We propose a novel compositional transfer learning framework (DoT5 -\ndomain compositional zero-shot T5) for zero-shot domain transfer. Without\naccess to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of\nunlabelled in-domain free text) and task knowledge (from task training on more\nreadily available general-domain data) in a multi-task manner. To improve the\ntransferability of task training, we design a strategy named NLGU: we\nsimultaneously train NLG for in-domain label-to-data generation which enables\ndata augmentation for self-finetuning and NLU for label prediction. We evaluate\nDoT5 on the biomedical domain and the resource-lean subdomain of radiology,\nfocusing on NLI, text summarisation and embedding learning. DoT5 demonstrates\nthe effectiveness of compositional transfer learning through multi-task\nlearning. In particular, DoT5 outperforms the current SOTA in zero-shot\ntransfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with\nablations and a case study demonstrating its ability to solve challenging NLI\nexamples requiring in-domain expertise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianchu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1\">Fernando P&#xe9;rez-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyland_S/0/1/0/all/0/1\">Stephanie L. Hyland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. (arXiv:2303.13408v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13408","description":"<p>To detect the deployment of large language models for malicious use cases\n(e.g., fake content creation or academic plagiarism), several approaches have\nrecently been proposed for identifying AI-generated text via watermarks or\nstatistical irregularities. How robust are these detection algorithms to\nparaphrases of AI-generated text? To stress test these detectors, we first\ntrain an 11B parameter paraphrase generation model (DIPPER) that can paraphrase\nparagraphs, optionally leveraging surrounding text (e.g., user-written prompts)\nas context. DIPPER also uses scalar knobs to control the amount of lexical\ndiversity and reordering in the paraphrases. Paraphrasing text generated by\nthree large language models (including GPT3.5-davinci-003) with DIPPER\nsuccessfully evades several detectors, including watermarking, GPTZero,\nDetectGPT, and OpenAI's text classifier. For example, DIPPER drops the\ndetection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false\npositive rate of 1%), without appreciably modifying the input semantics. To\nincrease the robustness of AI-generated text detection to paraphrase attacks,\nwe introduce a simple defense that relies on retrieving semantically-similar\ngenerations and must be maintained by a language model API provider. Given a\ncandidate text, our algorithm searches a database of sequences previously\ngenerated by the API, looking for sequences that match the candidate text\nwithin a certain threshold. We empirically verify our defense using a database\nof 15M generations from a fine-tuned T5-XXL model and find that it can detect\n80% to 97% of paraphrased generations across different settings, while only\nclassifying 1% of human-written sequences as AI-generated. We will open source\nour code, model and data for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yixiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpinska_M/0/1/0/all/0/1\">Marzena Karpinska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development and validation of a natural language processing algorithm to pseudonymize documents in the context of a clinical data warehouse. (arXiv:2303.13451v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13451","description":"<p>The objective of this study is to address the critical issue of\nde-identification of clinical reports in order to allow access to data for\nresearch purposes, while ensuring patient privacy. The study highlights the\ndifficulties faced in sharing tools and resources in this domain and presents\nthe experience of the Greater Paris University Hospitals (AP-HP) in\nimplementing a systematic pseudonymization of text documents from its Clinical\nData Warehouse. We annotated a corpus of clinical documents according to 12\ntypes of identifying entities, and built a hybrid system, merging the results\nof a deep learning model as well as manual rules. Our results show an overall\nperformance of 0.99 of F1-score. We discuss implementation choices and present\nexperiments to better understand the effort involved in such a task, including\ndataset size, document types, language models, or rule addition. We share\nguidelines and code under a 3-Clause BSD license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tannier_X/0/1/0/all/0/1\">Xavier Tannier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wajsburt_P/0/1/0/all/0/1\">Perceval Wajsb&#xfc;rt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calliger_A/0/1/0/all/0/1\">Alice Calliger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dura_B/0/1/0/all/0/1\">Basile Dura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchet_A/0/1/0/all/0/1\">Alexandre Mouchet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilka_M/0/1/0/all/0/1\">Martin Hilka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bey_R/0/1/0/all/0/1\">Romain Bey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoBIT: A Contrastive Bi-directional Image-Text Generation Model. (arXiv:2303.13455v1 [cs.CV])","link":"http://arxiv.org/abs/2303.13455","description":"<p>The field of vision and language has witnessed a proliferation of pre-trained\nfoundation models. Most existing methods are independently pre-trained with\ncontrastive objective like CLIP, image-to-text generative objective like PaLI,\nor text-to-image generative objective like Parti. However, the three objectives\ncan be pre-trained on the same data, image-text pairs, and intuitively they\ncomplement each other as contrasting provides global alignment capacity and\ngeneration grants fine-grained understanding. In this work, we present a\nContrastive Bi-directional Image-Text generation model (CoBIT), which attempts\nto unify the three pre-training objectives in one framework. Specifically,\nCoBIT employs a novel unicoder-decoder structure, consisting of an image\nunicoder, a text unicoder and a cross-modal decoder. The image/text unicoders\ncan switch between encoding and decoding in different tasks, enabling\nflexibility and shared knowledge that benefits both image-to-text and\ntext-to-image generations. CoBIT achieves superior performance in image\nunderstanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE)\nand text-based content creation, particularly in zero-shot scenarios. For\ninstance, 82.7% in zero-shot ImageNet classification, 9.37 FID score in\nzero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhecan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"W2KPE: Keyphrase Extraction with Word-Word Relation. (arXiv:2303.13463v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13463","description":"<p>This paper describes our submission to ICASSP 2023 MUG Challenge Track 4,\nKeyphrase Extraction, which aims to extract keyphrases most relevant to the\nconference theme from conference materials. We model the challenge as a\nsingle-class Named Entity Recognition task and developed techniques for better\nperformance on the challenge: For the data preprocessing, we encode the split\nkeyphrases after word segmentation. In addition, we increase the amount of\ninput information that the model can accept at one time by fusing multiple\npreprocessed sentences into one segment. We replace the loss function with the\nmulti-class focal loss to address the sparseness of keyphrases. Besides, we\nscore each appearance of keyphrases and add an extra output layer to fit the\nscore to rank keyphrases. Exhaustive evaluations are performed to find the best\ncombination of the word segmentation tool, the pre-trained embedding model, and\nthe corresponding hyperparameters. With these proposals, we scored 45.04 on the\nfinal test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shichen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep RL with Hierarchical Action Exploration for Dialogue Generation. (arXiv:2303.13465v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13465","description":"<p>Conventionally, since the natural language action space is astronomical,\napproximate dynamic programming applied to dialogue generation involves policy\nimprovement with action sampling. However, such a practice is inefficient for\nreinforcement learning (RL) because the eligible (high action value) responses\nare very sparse, and the greedy policy sustained by the random sampling is\nflabby. This paper shows that the performance of dialogue policy positively\ncorrelated with sampling size by theoretical and experimental. We introduce a\nnovel dual-granularity Q-function to alleviate this limitation by exploring the\nmost promising response category to intervene in the sampling. It extracts the\nactions following the grained hierarchy, which can achieve the optimum with\nfewer policy iterations. Our approach learns in the way of offline RL from\nmultiple reward functions designed to recognize human emotional details.\nEmpirical studies demonstrate that our algorithm outperforms the baseline\nmethods. Further verification presents that ours can generate responses with\nhigher expected rewards and controllability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_I/0/1/0/all/0/1\">Itsugun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_R/0/1/0/all/0/1\">Ryota Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanase_Y/0/1/0/all/0/1\">Yusaku Yanase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1\">Hiroaki Saito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Physical Rehabilitation Exercise Information from Clinical Notes: a Comparison of Rule-Based and Machine Learning Natural Language Processing Techniques. (arXiv:2303.13466v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13466","description":"<p>Physical rehabilitation plays a crucial role in the recovery process of\npost-stroke patients. By personalizing therapies for patients leveraging\npredictive modeling and electronic health records (EHRs), healthcare providers\ncan make the rehabilitation process more efficient. Before predictive modeling\ncan provide decision support for the assignment of treatment plans, automated\nmethods are necessary to extract physical rehabilitation exercise information\nfrom unstructured EHRs. We introduce a rule-based natural language processing\nalgorithm to annotate therapeutic procedures for stroke patients and compare it\nto several small machine learning models. We find that our algorithm\noutperforms these models in extracting half of the concepts where sufficient\ndata is available, and individual exercise descriptions can be assigned binary\nlabels with an f-score of no less than 0.75 per concept. More research needs to\nbe done before these algorithms can be deployed on unlabeled documents, but\ncurrent progress gives promise to the potential of precision rehabilitation\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaffran_S/0/1/0/all/0/1\">Stephen W. Shaffran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Fengyi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1\">Parker E. Denny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldhahwani_B/0/1/0/all/0/1\">Bayan M. Aldhahwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bove_A/0/1/0/all/0/1\">Allyn Bove</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visweswaran_S/0/1/0/all/0/1\">Shyam Visweswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanshan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Semantic Text Similarity to rank Hypernyms of Financial Terms. (arXiv:2303.13475v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13475","description":"<p>Over the years, there has been a paradigm shift in how users access financial\nservices. With the advancement of digitalization more users have been\npreferring the online mode of performing financial activities. This has led to\nthe generation of a huge volume of financial content. Most investors prefer to\ngo through these contents before making decisions. Every industry has terms\nthat are specific to the domain it operates in. Banking and Financial Services\nare not an exception to this. In order to fully comprehend these contents, one\nneeds to have a thorough understanding of the financial terms. Getting a basic\nidea about a term becomes easy when it is explained with the help of the broad\ncategory to which it belongs. This broad category is referred to as hypernym.\nFor example, \"bond\" is a hypernym of the financial term \"alternative\ndebenture\". In this paper, we propose a system capable of extracting and\nranking hypernyms for a given financial term. The system has been trained with\nfinancial text corpora obtained from various sources like DBpedia [4],\nInvestopedia, Financial Industry Business Ontology (FIBO), prospectus and so\non. Embeddings of these terms have been extracted using FinBERT [3], FinISH [1]\nand fine-tuned using SentenceBERT [54]. A novel approach has been used to\naugment the training set with negative samples. It uses the hierarchy present\nin FIBO. Finally, we benchmark the system performance with that of the existing\nones. We establish that it performs better than the existing ones and is also\nscalable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sohom Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ankush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naskar_S/0/1/0/all/0/1\">Sudip Kumar Naskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning and Verification of Task Structure in Instructional Videos. (arXiv:2303.13519v1 [cs.CV])","link":"http://arxiv.org/abs/2303.13519","description":"<p>Given the enormous number of instructional videos available online, learning\na diverse array of multi-step task models from videos is an appealing goal. We\nintroduce a new pre-trained video model, VideoTaskformer, focused on\nrepresenting the semantics and structure of instructional videos. We pre-train\nVideoTaskformer using a simple and effective objective: predicting weakly\nsupervised textual labels for steps that are randomly masked out from an\ninstructional video (masked step modeling). Compared to prior work which learns\nstep representations locally, our approach involves learning them globally,\nleveraging video of the entire surrounding task as context. From these learned\nrepresentations, we can verify if an unseen video correctly executes a given\ntask, as well as forecast which steps are likely to be taken after a given\nstep. We introduce two new benchmarks for detecting mistakes in instructional\nvideos, to verify if there is an anomalous step and if steps are executed in\nthe right order. We also introduce a long-term forecasting benchmark, where the\ngoal is to predict long-range future steps from a given step. Our method\noutperforms previous baselines on these tasks, and we believe the tasks will be\na valuable way for the community to measure the quality of step\nrepresentations. Additionally, we evaluate VideoTaskformer on 3 existing\nbenchmarks -- procedural activity recognition, step classification, and step\nforecasting -- and demonstrate on each that our method outperforms existing\nbaselines and achieves new state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_M/0/1/0/all/0/1\">Medhini Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_S/0/1/0/all/0/1\">Sean Bell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Offensive Language and Hate Speech Detection for Danish. (arXiv:1908.04531v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1908.04531","description":"<p>The presence of offensive language on social media platforms and the\nimplications this poses is becoming a major concern in modern society. Given\nthe enormous amount of content created every day, automatic methods are\nrequired to detect and deal with this type of content. Until now, most of the\nresearch has focused on solving the problem for the English language, while the\nproblem is multilingual.\n</p>\n<p>We construct a Danish dataset containing user-generated comments from\n\\textit{Reddit} and \\textit{Facebook}. It contains user generated comments from\nvarious social media platforms, and to our knowledge, it is the first of its\nkind. Our dataset is annotated to capture various types and target of offensive\nlanguage. We develop four automatic classification systems, each designed to\nwork for both the English and the Danish language. In the detection of\noffensive language in English, the best performing system achieves a macro\naveraged F1-score of $0.74$, and the best performing system for Danish achieves\na macro averaged F1-score of $0.70$. In the detection of whether or not an\noffensive post is targeted, the best performing system for English achieves a\nmacro averaged F1-score of $0.62$, while the best performing system for Danish\nachieves a macro averaged F1-score of $0.73$. Finally, in the detection of the\ntarget type in a targeted offensive post, the best performing system for\nEnglish achieves a macro averaged F1-score of $0.56$, and the best performing\nsystem for Danish achieves a macro averaged F1-score of $0.63$.\n</p>\n<p>Our work for both the English and the Danish language captures the type and\ntargets of offensive language, and present automatic methods for detecting\ndifferent kinds of offensive language such as hate speech and cyberbullying.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sigurbergsson_G/0/1/0/all/0/1\">Gudbjartur Ingi Sigurbergsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminating Between Similar Nordic Languages. (arXiv:2012.06431v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.06431","description":"<p>Automatic language identification is a challenging problem. Discriminating\nbetween closely related languages is especially difficult. This paper presents\na machine learning approach for automatic language identification for the\nNordic languages, which often suffer miscategorisation by existing\nstate-of-the-art tools. Concretely we will focus on discrimination between six\nNordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokm{\\aa}l),\nFaroese and Icelandic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haas_R/0/1/0/all/0/1\">Ren&#xe9; Haas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focusing on Potential Named Entities During Active Label Acquisition. (arXiv:2111.03837v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.03837","description":"<p>Named entity recognition (NER) aims to identify mentions of named entities in\nan unstructured text and classify them into predefined named entity classes.\nWhile deep learning-based pre-trained language models help to achieve good\npredictive performances in NER, many domain-specific NER applications still\ncall for a substantial amount of labeled data. Active learning (AL), a general\nframework for the label acquisition problem, has been used for NER tasks to\nminimize the annotation cost without sacrificing model performance. However,\nthe heavily imbalanced class distribution of tokens introduces challenges in\ndesigning effective AL querying methods for NER. We propose several AL sentence\nquery evaluation functions that pay more attention to potential positive\ntokens, and evaluate these proposed functions with both sentence-based and\ntoken-based cost evaluation strategies. We also propose a better data-driven\nnormalization approach to penalize sentences that are too long or too short.\nOur experiments on three datasets from different domains reveal that the\nproposed approach reduces the number of annotated tokens while achieving better\nor comparable prediction performance with conventional methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sapci_A/0/1/0/all/0/1\">Ali Osman Berk Sapci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tastan_O/0/1/0/all/0/1\">Oznur Tastan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeniterzi_R/0/1/0/all/0/1\">Reyyan Yeniterzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptDA: Label-guided Data Augmentation for Prompt-based Few-shot Learners. (arXiv:2205.09229v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09229","description":"<p>Recent advances in large pre-trained language models (PLMs) lead to\nimpressive gains in natural language understanding (NLU) tasks with\ntask-specific fine-tuning. However, directly fine-tuning PLMs heavily relies on\nsufficient labeled training instances, which are usually hard to obtain.\nPrompt-based tuning on PLMs has shown to be powerful for various downstream\nfew-shot tasks. Existing works studying prompt-based tuning for few-shot NLU\ntasks mainly focus on deriving proper label words with a verbalizer or\ngenerating prompt templates to elicit semantics from PLMs. In addition,\nconventional data augmentation strategies such as synonym substitution, though\nwidely adopted in low-resource scenarios, only bring marginal improvements for\nprompt-based few-shot learning. Thus, an important research question arises:\nhow to design effective data augmentation methods for prompt-based few-shot\ntuning? To this end, considering the label semantics are essential in\nprompt-based tuning, we propose a novel label-guided data augmentation\nframework PromptDA, which exploits the enriched label semantic information for\ndata augmentation. Extensive experiment results on few-shot text classification\ntasks demonstrate the superior performance of the proposed framework by\neffectively leveraging label semantics and data augmentation for natural\nlanguage understanding. Our code is available at\nhttps://github.com/canyuchen/PromptDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Canyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1\">Kai Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-lingual Evaluation of Code Generation Models. (arXiv:2210.14868v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.14868","description":"<p>We present new benchmarks on evaluation code generation models: MBXP and\nMultilingual HumanEval, and MathQA-X. These datasets cover over 10 programming\nlanguages and are generated using a scalable conversion framework that\ntranspiles prompts and test cases from the original Python datasets into the\ncorresponding data in the target language. Using these benchmarks, we are able\nto assess the performance of code generation models in a multi-lingual fashion,\nand discovered generalization ability of language models on out-of-domain\nlanguages, advantages of multi-lingual models over mono-lingual, the ability of\nfew-shot prompting to teach the model new languages, and zero-shot translation\nabilities even on mono-lingual settings. Furthermore, we use our code\ngeneration model to perform large-scale bootstrapping to obtain synthetic\ncanonical solutions in several languages, which can be used for other\ncode-related evaluations such as code insertion, robustness, or summarization\ntasks. Overall, our benchmarks represents a significant step towards a deeper\nunderstanding of language models' code generation abilities. We publicly\nrelease our code and datasets at https://github.com/amazon-research/mxeval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Athiwaratkun_B/0/1/0/all/0/1\">Ben Athiwaratkun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouda_S/0/1/0/all/0/1\">Sanjay Krishna Gouda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuchen Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Ming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_M/0/1/0/all/0/1\">Mingyue Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonugondla_S/0/1/0/all/0/1\">Sujan Kumar Gonugondla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Hantian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fulton_N/0/1/0/all/0/1\">Nathan Fulton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farahani_A/0/1/0/all/0/1\">Arash Farahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Siddhartha Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giaquinto_R/0/1/0/all/0/1\">Robert Giaquinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Haifeng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanathan_M/0/1/0/all/0/1\">Murali Krishna Ramanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallapati_R/0/1/0/all/0/1\">Ramesh Nallapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Baishakhi Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1\">Parminder Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Sudipta Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Severity Assessment of Dysarthric speech by using Self-supervised Model with Multi-task Learning. (arXiv:2210.15387v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15387","description":"<p>Automatic assessment of dysarthric speech is essential for sustained\ntreatments and rehabilitation. However, obtaining atypical speech is\nchallenging, often leading to data scarcity issues. To tackle the problem, we\npropose a novel automatic severity assessment method for dysarthric speech,\nusing the self-supervised model in conjunction with multi-task learning.\nWav2vec 2.0 XLS-R is jointly trained for two different tasks: severity\nclassification and auxiliary automatic speech recognition (ASR). For the\nbaseline experiments, we employ hand-crafted acoustic features and machine\nlearning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean\ndysarthric speech QoLT database, our model outperforms the traditional baseline\nmethods, with a relative percentage increase of 1.25% for F1-score. In\naddition, the proposed model surpasses the model trained without ASR head,\nachieving 10.61% relative percentage improvements. Furthermore, we present how\nmulti-task learning affects the severity classification performance by\nanalyzing the latent representations and regularization effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeo_E/0/1/0/all/0/1\">Eun Jung Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Kwanghee Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_M/0/1/0/all/0/1\">Minhwa Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings. (arXiv:2210.16848v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16848","description":"<p>Although contextualized embeddings generated from large-scale pre-trained\nmodels perform well in many tasks, traditional static embeddings (e.g.,\nSkip-gram, Word2Vec) still play an important role in low-resource and\nlightweight settings due to their low computational cost, ease of deployment,\nand stability. In this paper, we aim to improve word embeddings by 1)\nincorporating more contextual information from existing pre-trained models into\nthe Skip-gram framework, which we call Context-to-Vec; 2) proposing a\npost-processing retrofitting method for static embeddings independent of\ntraining by employing priori synonym knowledge and weighted vector\ndistribution. Through extrinsic and intrinsic tasks, our methods are well\nproven to outperform the baselines by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiangbin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yile Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guojiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction. (arXiv:2301.09209v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2301.09209","description":"<p>We study object interaction anticipation in egocentric videos. This task\nrequires an understanding of the spatiotemporal context formed by past actions\non objects, coined action context. We propose TransFusion, a multimodal\ntransformer-based architecture. It exploits the representational power of\nlanguage by summarising the action context. TransFusion leverages pre-trained\nimage captioning and vision-language models to extract the action context from\npast video frames. This action context together with the next video frame is\nprocessed by the multimodal fusion module to forecast the next object\ninteraction. Our model enables more efficient end-to-end learning. The large\npre-trained language models add common sense and a generalisation capability.\nExperiments on Ego4D and EPIC-KITCHENS-100 show the effectiveness of our\nmultimodal fusion model. They also highlight the benefits of using\nlanguage-based context summaries in a task where vision seems to suffice. Our\nmethod outperforms state-of-the-art approaches by 40.4% in relative terms in\noverall mAP on the Ego4D test set. We validate the effectiveness of TransFusion\nvia experiments on EPIC-KITCHENS-100. Video and code are available at:\nhttps://eth-ait.github.io/transfusion-proj/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasca_R/0/1/0/all/0/1\">Razvan-George Pasca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavryushin_A/0/1/0/all/0/1\">Alexey Gavryushin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1\">Yen-Ling Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memotion 3: Dataset on Sentiment and Emotion Analysis of Codemixed Hindi-English Memes. (arXiv:2303.09892v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.09892","description":"<p>Memes are the new-age conveyance mechanism for humor on social media sites.\nMemes often include an image and some text. Memes can be used to promote\ndisinformation or hatred, thus it is crucial to investigate in details. We\nintroduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other\nprevalent datasets in the domain, including prior iterations of Memotion,\nMemotion 3 introduces Hindi-English Codemixed memes while prior works in the\narea were limited to only the English memes. We describe the Memotion task, the\ndata collection and the dataset creation methodologies. We also provide a\nbaseline for the task. The baseline code and dataset will be made available at\nhttps://github.com/Shreyashm16/Memotion-3.0\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shreyash Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suryavardan_S/0/1/0/all/0/1\">S Suryavardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1\">Parth Patwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_M/0/1/0/all/0/1\">Megha Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rani_A/0/1/0/all/0/1\">Anku Rani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reganti_A/0/1/0/all/0/1\">Aishwarya Reganti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Amitava Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinnakotla_M/0/1/0/all/0/1\">Manoj Chinnakotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Srijan Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"cTBL: Augmenting Large Language Models for Conversational Tables. (arXiv:2303.12024v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.12024","description":"<p>An open challenge in multimodal conversational AI requires augmenting large\nlanguage models with information from textual and non-textual sources for\nmulti-turn dialogue. To address this problem, this paper introduces\nConversational Tables (cTBL), a three-step encoder-decoder approach to retrieve\ntabular information and generate dialogue responses grounded on the retrieved\ninformation. cTBL uses Transformer encoder embeddings for Dense Table Retrieval\nand obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over\nsparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs\ntabular knowledge retrieval using both encoder and decoder models, resulting in\nup to 46% relative improvement in ROUGE scores and better human evaluation for\nresponse generation on HyrbiDialogue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundar_A/0/1/0/all/0/1\">Anirudh S Sundar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_L/0/1/0/all/0/1\">Larry Heck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-Based Decoding for Task Oriented Semantic Parsing. (arXiv:2109.04587v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2109.04587","description":"<p>The dominant paradigm for semantic parsing in recent years is to formulate\nparsing as a sequence-to-sequence task, generating predictions with\nauto-regressive sequence decoders. In this work, we explore an alternative\nparadigm. We formulate semantic parsing as a dependency parsing task, applying\ngraph-based decoding techniques developed for syntactic parsing. We compare\nvarious decoding techniques given the same pre-trained Transformer encoder on\nthe TOP dataset, including settings where training data is limited or contains\nonly partially-annotated examples. We find that our graph-based approach is\ncompetitive with sequence decoders on the standard setting, and offers\nsignificant improvements in data efficiency and settings where\npartially-annotated data is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1\">Jeremy R. Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nanjiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupat_P/0/1/0/all/0/1\">Panupong Pasupat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Luheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1\">Peter Shaw</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}