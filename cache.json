{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-08T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Witscript 2: A System for Generating Improvised Jokes Without Wordplay. (arXiv:2302.03036v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03036","description":"<p>A previous paper presented Witscript, a system for generating conversational\njokes that rely on wordplay. This paper extends that work by presenting\nWitscript 2, which uses a large language model to generate conversational jokes\nthat rely on common sense instead of wordplay. Like Witscript, Witscript 2 is\nbased on joke-writing algorithms created by an expert comedy writer. Human\nevaluators judged Witscript 2's responses to input sentences to be jokes 46% of\nthe time, compared to 70% of the time for human-written responses. This is\nevidence that Witscript 2 represents another step toward giving a chatbot a\nhumanlike sense of humor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toplyn_J/0/1/0/all/0/1\">Joe Toplyn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient and Flexible Topic Modeling using Pretrained Embeddings and Bag of Sentences. (arXiv:2302.03106v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03106","description":"<p>Pre-trained language models have led to a new state-of-the-art in many NLP\ntasks. However, for topic modeling, statistical generative models such as LDA\nare still prevalent, which do not easily allow incorporating contextual word\nvectors. They might yield topics that do not align very well with human\njudgment. In this work, we propose a novel topic modeling and inference\nalgorithm. We suggest a bag of sentences (BoS) approach using sentences as the\nunit of analysis. We leverage pre-trained sentence embeddings by combining\ngenerative process models with clustering. We derive a fast inference algorithm\nbased on expectation maximization, hard assignments, and an annealing process.\nOur evaluation shows that our method yields state-of-the art results with\nrelatively little computational demands. Our methods is more flexible compared\nto prior works leveraging word embeddings, since it provides the possibility to\ncustomize topic-document distributions using priors. Code is at\n\\url{https://github.com/JohnTailor/BertSenClu}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1\">Johannes Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Gloss Augmentation for Improving Arabic Target Sense Verification. (arXiv:2302.03126v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03126","description":"<p>Arabic language lacks semantic datasets and sense inventories. The most\ncommon semantically-labeled dataset for Arabic is the ArabGlossBERT, a\nrelatively small dataset that consists of 167K context-gloss pairs (about 60K\npositive and 107K negative pairs), collected from Arabic dictionaries. This\npaper presents an enrichment to the ArabGlossBERT dataset, by augmenting it\nusing (Arabic-English-Arabic) machine back-translation. Augmentation increased\nthe dataset size to 352K pairs (149K positive and 203K negative pairs). We\nmeasure the impact of augmentation using different data configurations to\nfine-tune BERT on target sense verification (TSV) task. Overall, the accuracy\nranges between 78% to 84% for different data configurations. Although our\napproach performed at par with the baseline, we did observe some improvements\nfor some POS tags in some experiments. Furthermore, our fine-tuned models are\ntrained on a larger dataset covering larger vocabulary and contexts. We provide\nan in-depth analysis of the accuracy for each part-of-speech (POS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malaysha_S/0/1/0/all/0/1\">Sanad Malaysha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarrar_M/0/1/0/all/0/1\">Mustafa Jarrar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalilia_M/0/1/0/all/0/1\">Mohammed Khalilia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Techniques to Improve Neural Math Word Problem Solvers. (arXiv:2302.03145v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03145","description":"<p>Developing automatic Math Word Problem (MWP) solvers is a challenging task\nthat demands the ability of understanding and mathematical reasoning over the\nnatural language. Recent neural-based approaches mainly encode the problem text\nusing a language model and decode a mathematical expression over quantities and\noperators iteratively. Note the problem text of a MWP consists of a context\npart and a question part, a recent work finds these neural solvers may only\nperform shallow pattern matching between the context text and the golden\nexpression, where question text is not well used. Meanwhile, existing decoding\nprocesses fail to enforce the mathematical laws into the design, where the\nrepresentations for mathematical equivalent expressions are different. To\naddress these two issues, we propose a new encoder-decoder architecture that\nfully leverages the question text and preserves step-wise commutative law.\nBesides generating quantity embeddings, our encoder further encodes the\nquestion text and uses it to guide the decoding process. At each step, our\ndecoder uses Deep Sets to compute expression representations so that these\nembeddings are invariant under any permutation of quantities. Experiments on\nfour established benchmarks demonstrate that our framework outperforms\nstate-of-the-art neural MWP solvers, showing the effectiveness of our\ntechniques. We also conduct a detailed analysis of the results to show the\nlimitations of our approach and further discuss the potential future work. Code\nis available at https://github.com/sophistz/Question-Aware-Deductive-MWP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youyuan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's about Time: Rethinking Evaluation on Rumor Detection Benchmarks using Chronological Splits. (arXiv:2302.03147v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03147","description":"<p>New events emerge over time influencing the topics of rumors in social media.\nCurrent rumor detection benchmarks use random splits as training, development\nand test sets which typically results in topical overlaps. Consequently, models\ntrained on random splits may not perform well on rumor classification on\npreviously unseen topics due to the temporal concept drift. In this paper, we\nprovide a re-evaluation of classification models on four popular rumor\ndetection benchmarks considering chronological instead of random splits. Our\nexperimental results show that the use of random splits can significantly\noverestimate predictive performance across all datasets and models. Therefore,\nwe suggest that rumor detection models should always be evaluated using\nchronological splits for minimizing topical overlaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yida Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1\">Kalina Bontcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Protecting Language Generation Models via Invisible Watermarking. (arXiv:2302.03162v1 [cs.CR])","link":"http://arxiv.org/abs/2302.03162","description":"<p>Language generation models have been an increasingly powerful enabler for\nmany applications. Many such models offer free or affordable API access, which\nmakes them potentially vulnerable to model extraction attacks through\ndistillation. To protect intellectual property (IP) and ensure fair use of\nthese models, various techniques such as lexical watermarking and synonym\nreplacement have been proposed. However, these methods can be nullified by\nobvious countermeasures such as \"synonym randomization\". To address this issue,\nwe propose GINSEW, a novel method to protect text generation models from being\nstolen through distillation. The key idea of our method is to inject secret\nsignals into the probability vector of the decoding steps for each target\ntoken. We can then detect the secret message by probing a suspect model to tell\nif it is distilled from the protected one. Experimental results show that\nGINSEW can effectively identify instances of IP infringement with minimal\nimpact on the generation quality of protected APIs. Our method demonstrates an\nabsolute improvement of 19 to 29 points on mean average precision (mAP) in\ndetecting suspects compared to previous methods against watermark removal\nattacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuandong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Selection for Language Models via Importance Resampling. (arXiv:2302.03169v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03169","description":"<p>Selecting a suitable training dataset is crucial for both general-domain\n(e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We\nformalize this data selection problem as selecting a subset of a large raw\nunlabeled dataset to match a desired target distribution, given some unlabeled\ntarget samples. Due to the large scale and dimensionality of the raw text data,\nexisting methods use simple heuristics to select data that are similar to a\nhigh-quality reference corpus (e.g., Wikipedia), or leverage experts to\nmanually curate data. Instead, we extend the classic importance resampling\napproach used in low-dimensions for LM data selection. Crucially, we work in a\nreduced feature space to make importance weight estimation tractable over the\nspace of text. To determine an appropriate feature space, we first show that KL\nreduction, a data metric that measures the proximity between selected data and\nthe target in a feature space, has high correlation with average accuracy on 8\ndownstream tasks (r=0.89) when computed with simple n-gram features. From this\nobservation, we present Data Selection with Importance Resampling (DSIR), an\nefficient and scalable algorithm that estimates importance weights in a reduced\nfeature space (e.g., n-gram features in our instantiation) and selects data\nwith importance resampling according to these weights. When training\ngeneral-domain models (target is Wikipedia + books), DSIR improves over random\nselection and heuristic filtering baselines by 2--2.5% on the GLUE benchmark.\nWhen performing continued pretraining towards a specific domain, DSIR performs\ncomparably to expert curated data across 8 target distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santurkar_S/0/1/0/all/0/1\">Shibani Santurkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Topic Framing via Masked Language Modeling. (arXiv:2302.03183v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03183","description":"<p>Differential framing of issues can lead to divergent world views on important\nissues. This is especially true in domains where the information presented can\nreach a large audience, such as traditional and social media. Scalable and\nreliable measurement of such differential framing is an important first step in\naddressing them. In this work, based on the intuition that framing affects the\ntone and word choices in written language, we propose a framework for modeling\nthe differential framing of issues through masked token prediction via\nlarge-scale fine-tuned language models (LMs). Specifically, we explore three\nkey factors for our framework: 1) prompt generation methods for the masked\ntoken prediction; 2) methods for normalizing the output of fine-tuned LMs; 3)\nrobustness to the choice of pre-trained LMs used for fine-tuning. Through\nexperiments on a dataset of articles from traditional media outlets covering\nfive diverse and politically polarized topics, we show that our framework can\ncapture differential framing of these topics with high reliability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaobo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Weicheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UDApter -- Efficient Domain Adaptation Using Adapters. (arXiv:2302.03194v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03194","description":"<p>We propose two methods to make unsupervised domain adaptation (UDA) more\nparameter efficient using adapters, small bottleneck layers interspersed with\nevery layer of the large-scale pre-trained language model (PLM). The first\nmethod deconstructs UDA into a two-step process: first by adding a domain\nadapter to learn domain-invariant information and then by adding a task adapter\nthat uses domain-invariant information to learn task representations in the\nsource domain. The second method jointly learns a supervised classifier while\nreducing the divergence measure. Compared to strong baselines, our simple\nmethods perform well in natural language inference (MNLI) and the cross-domain\nsentiment classification task. We even outperform unsupervised domain\nadaptation methods such as DANN and DSN in sentiment classification, and we are\nwithin 0.85% F1 for natural language inference task, by fine-tuning only a\nfraction of the full model parameters. We release our code at\nhttps://github.com/declare-lab/UDAPTER\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malik_B/0/1/0/all/0/1\">Bhavitvya Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_A/0/1/0/all/0/1\">Abhinav Ramesh Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Benefits of Training Expert Language Models over Instruction Tuning. (arXiv:2302.03202v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03202","description":"<p>Recently, Language Models (LMs) instruction-tuned on multiple tasks, also\nknown as multitask-prompted fine-tuning (MT), have shown the capability to\ngeneralize to unseen tasks. Previous work has shown that scaling the number of\ntraining tasks is the key component in making stronger MT LMs. In this work, we\nreport an unexpected finding that an expert LM fine-tuned on just a single task\ncan outperform an MT LM trained with 300+ different tasks on 11 different\nunseen datasets and on 13 datasets of the BIG-bench benchmark by a mean\naccuracy of 3.20% and 1.29%, respectively. This finding casts doubt on the\npreviously held belief that simply scaling the number of tasks makes stronger\nMT LMs. Leveraging this finding, we further show that this distributed approach\nof training a separate expert LM per training task instead of a single MT LM\nfor zero-shot inference possesses many benefits including (1) avoiding negative\ntask transfer that often occurs during instruction tuning, (2) being able to\ncontinually learn new tasks without having to re-train on previous tasks to\navoid catastrophic forgetting, and (3) showing compositional capabilities when\nmerging individual experts together. The code is available at\nhttps://github.com/joeljang/ELM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungone Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logeswaran_L/0/1/0/all/0/1\">Lajanugen Logeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Moontae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyungjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An entity-guided text summarization framework with relational heterogeneous graph neural network. (arXiv:2302.03205v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03205","description":"<p>Two crucial issues for text summarization to generate faithful summaries are\nto make use of knowledge beyond text and to make use of cross-sentence\nrelations in text. Intuitive ways for the two issues are Knowledge Graph (KG)\nand Graph Neural Network (GNN) respectively. Entities are semantic units in\ntext and in KG. This paper focuses on both issues by leveraging entities\nmentioned in text to connect GNN and KG for summarization. Firstly, entities\nare leveraged to construct a sentence-entity graph with weighted multi-type\nedges to model sentence relations, and a relational heterogeneous GNN for\nsummarization is proposed to calculate node encodings. Secondly, entities are\nleveraged to link the graph to KG to collect knowledge. Thirdly, entities guide\na two-step summarization framework defining a multi-task selector to select\nsalient sentences and entities, and using an entity-focused abstractor to\ncompress the sentences. GNN is connected with KG by constructing\nsentence-entity graphs where entity-entity edges are built based on KG,\ninitializing entity embeddings on KG, and training entity embeddings using\nentity-entity edges. The relational heterogeneous GNN utilizes both edge\nweights and edge types in GNN to calculate graphs with weighted multi-type\nedges. Experiments show the proposed method outperforms extractive baselines\nincluding the HGNN-based HGNNSum and abstractive baselines including the\nentity-driven SENECA on CNN/DM, and outperforms most baselines on NYT50.\nExperiments on sub-datasets show the density of sentence-entity edges greatly\ninfluences the performance of the proposed method. The greater the density, the\nbetter the performance. Ablations show effectiveness of the method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingqiang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bringing the State-of-the-Art to Customers: A Neural Agent Assistant Framework for Customer Service Support. (arXiv:2302.03222v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03222","description":"<p>Building Agent Assistants that can help improve customer service support\nrequires inputs from industry users and their customers, as well as knowledge\nabout state-of-the-art Natural Language Processing (NLP) technology. We combine\nexpertise from academia and industry to bridge the gap and build\ntask/domain-specific Neural Agent Assistants (NAA) with three high-level\ncomponents for: (1) Intent Identification, (2) Context Retrieval, and (3)\nResponse Generation. In this paper, we outline the pipeline of the NAA's core\nsystem and also present three case studies in which three industry partners\nsuccessfully adapt the framework to find solutions to their unique challenges.\nOur findings suggest that a collaborative process is instrumental in spurring\nthe development of emerging NLP models for Conversational AI tasks in industry.\nThe full reference implementation code and results are available at\n\\url{https://github.com/VectorInstitute/NAA}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Obadinma_S/0/1/0/all/0/1\">Stephen Obadinma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattak_F/0/1/0/all/0/1\">Faiza Khan Khattak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shirley Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidhom_T/0/1/0/all/0/1\">Tania Sidhom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_E/0/1/0/all/0/1\">Elaine Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_S/0/1/0/all/0/1\">Sean Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1\">Jingcheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Au_W/0/1/0/all/0/1\">Winnie Au</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munim_A/0/1/0/all/0/1\">Alif Munim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaskar_K/0/1/0/all/0/1\">Karthik Raja K. Bhaskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_B/0/1/0/all/0/1\">Bencheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_I/0/1/0/all/0/1\">Iris Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_W/0/1/0/all/0/1\">Waqar Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Erin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishola_B/0/1/0/all/0/1\">Bukola Ishola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Michael Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanner_G/0/1/0/all/0/1\">Griffin Tanner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiah_Y/0/1/0/all/0/1\">Yu-Jia Shiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sean X. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apponsah_K/0/1/0/all/0/1\">Kwesi P. Apponsah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1\">Kanishk Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narain_J/0/1/0/all/0/1\">Jaswinder Narain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandya_D/0/1/0/all/0/1\">Deval Pandya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolatabadi_E/0/1/0/all/0/1\">Elham Dolatabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning of Language Models. (arXiv:2302.03241v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03241","description":"<p>Language models (LMs) have been instrumental for the rapid advance of natural\nlanguage processing. This paper studies continual learning of LMs, in\nparticular, continual domain-adaptive pre-training (or continual DAP-training).\nExisting research has shown that further pre-training an LM using a domain\ncorpus to adapt the LM to the domain can improve the end-task performance in\nthe domain. This paper proposes a novel method to continually DAP-train an LM\nwith a sequence of unlabeled domain corpora to adapt the LM to these domains to\nimprove their end-task performances. The key novelty of our method is a\nsoft-masking mechanism that directly controls the update to the LM. A novel\nproxy is also proposed to preserve the general knowledge in the original LM.\nAdditionally, it contrasts the representations of the previously learned domain\nknowledge (including the general knowledge in the pre-trained LM) and the\nknowledge from the current full network to achieve knowledge integration. The\nmethod not only overcomes catastrophic forgetting, but also achieves knowledge\ntransfer to improve end-task performances. Empirical evaluation demonstrates\nthe effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zixuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yijia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haowei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konishi_T/0/1/0/all/0/1\">Tatsuya Konishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyuhak Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLACES: Prompting Language Models for Social Conversation Synthesis. (arXiv:2302.03269v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03269","description":"<p>Collecting high quality conversational data can be very expensive for most\napplications and infeasible for others due to privacy, ethical, or similar\nconcerns. A promising direction to tackle this problem is to generate synthetic\ndialogues by prompting large language models. In this work, we use a small set\nof expert-written conversations as in-context examples to synthesize a social\nconversation dataset using prompting. We perform several thorough evaluations\nof our synthetic conversations compared to human-collected conversations. This\nincludes various dimensions of conversation quality with human evaluation\ndirectly on the synthesized conversations, and interactive human evaluation of\nchatbots fine-tuned on the synthetically generated dataset. We additionally\ndemonstrate that this prompting approach is generalizable to multi-party\nconversations, providing potential to create new synthetic data for multi-party\ntasks. Our synthetic multi-party conversations were rated more favorably across\nall measured dimensions compared to conversation excerpts sampled from a\nhuman-collected multi-party dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maximillian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenbaum_A/0/1/0/all/0/1\">Andy Rosenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoWS: Automated Weak Supervision Framework for Text Classification. (arXiv:2302.03297v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03297","description":"<p>Creating large, good quality labeled data has become one of the major\nbottlenecks for developing machine learning applications. Multiple techniques\nhave been developed to either decrease the dependence of labeled data\n(zero/few-shot learning, weak supervision) or to improve the efficiency of\nlabeling process (active learning). Among those, Weak Supervision has been\nshown to reduce labeling costs by employing hand crafted labeling functions\ndesigned by domain experts. We propose AutoWS -- a novel framework for\nincreasing the efficiency of weak supervision process while decreasing the\ndependency on domain experts. Our method requires a small set of labeled\nexamples per label class and automatically creates a set of labeling functions\nto assign noisy labels to numerous unlabeled data. Noisy labels can then be\naggregated into probabilistic labels used by a downstream discriminative\nclassifier. Our framework is fully automatic and requires no hyper-parameter\nspecification by users. We compare our approach with different state-of-the-art\nwork on weak supervision and noisy training. Experimental results show that our\nmethod outperforms competitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bohra_A/0/1/0/all/0/1\">Abhinav Bohra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khatwani_D/0/1/0/all/0/1\">Devashish Khatwani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Manner of Execution from Partial Corrections. (arXiv:2302.03338v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03338","description":"<p>Some actions must be executed in different ways depending on the context. For\nexample, wiping away marker requires vigorous force while wiping away almonds\nrequires more gentle force. In this paper we provide a model where an agent\nlearns which manner of action execution to use in which context, drawing on\nevidence from trial and error and verbal corrections when it makes a mistake\n(e.g., ``no, gently''). The learner starts out with a domain model that lacks\nthe concepts denoted by the words in the teacher's feedback; both the words\ndescribing the context (e.g., marker) and the adverbs like ``gently''. We show\nthat through the the semantics of coherence, our agent can perform the symbol\ngrounding that's necessary for exploiting the teacher's feedback so as to solve\nits domain-level planning problem: to perform its actions in the current\ncontext in the right way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Appelgren_M/0/1/0/all/0/1\">Mattias Appelgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lascarides_A/0/1/0/all/0/1\">Alex Lascarides</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study. (arXiv:2302.03341v1 [cs.DL])","link":"http://arxiv.org/abs/2302.03341","description":"<p>Due to the exponential growth of scientific publications on the Web, there is\na pressing need to tag each paper with fine-grained topics so that researchers\ncan track their interested fields of study rather than drowning in the whole\nliterature. Scientific literature tagging is beyond a pure multi-label text\nclassification task because papers on the Web are prevalently accompanied by\nmetadata information such as venues, authors, and references, which may serve\nas additional signals to infer relevant tags. Although there have been studies\nmaking use of metadata in academic paper classification, their focus is often\nrestricted to one or two scientific fields (e.g., computer science and\nbiomedicine) and to one specific model. In this work, we systematically study\nthe effect of metadata on scientific literature tagging across 19 fields. We\nselect three representative multi-label classifiers (i.e., a bag-of-words\nmodel, a sequence-based model, and a pre-trained language model) and explore\ntheir performance change in scientific literature tagging when metadata are fed\nto the classifiers as additional features. We observe some ubiquitous patterns\nof metadata's effects across all fields (e.g., venues are consistently\nbeneficial to paper tagging in almost all cases), as well as some unique\npatterns in fields other than computer science and biomedicine, which are not\nexplored in previous studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Bowen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What do Language Models know about word senses? Zero-Shot WSD with Language Models and Domain Inventories. (arXiv:2302.03353v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03353","description":"<p>Language Models are the core for almost any Natural Language Processing\nsystem nowadays. One of their particularities is their contextualized\nrepresentations, a game changer feature when a disambiguation between word\nsenses is necessary. In this paper we aim to explore to what extent language\nmodels are capable of discerning among senses at inference time. We performed\nthis analysis by prompting commonly used Languages Models such as BERT or\nRoBERTa to perform the task of Word Sense Disambiguation (WSD). We leverage the\nrelation between word senses and domains, and cast WSD as a textual entailment\nproblem, where the different hypothesis refer to the domains of the word\nsenses. Our results show that this approach is indeed effective, close to\nsupervised systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sainz_O/0/1/0/all/0/1\">Oscar Sainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacalle_O/0/1/0/all/0/1\">Oier Lopez de Lacalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigau_G/0/1/0/all/0/1\">German Rigau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-Aware Dual Co-Attention Network for Fake News Detection. (arXiv:2302.03475v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03475","description":"<p>Fake news and misinformation spread rapidly on the Internet. How to identify\nit and how to interpret the identification results have become important\nissues. In this paper, we propose a Dual Co-Attention Network (Dual-CAN) for\nfake news detection, which takes news content, social media replies, and\nexternal knowledge into consideration. Our experimental results support that\nthe proposed Dual-CAN outperforms current representative models in two\nbenchmark datasets. We further make in-depth discussions by comparing how\nmodels work in both datasets with empirical analysis of attention weights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sin-Han Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chung-Chi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hen-Hsen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsin-Hsi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning. (arXiv:2302.03488v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03488","description":"<p>Practical natural language processing (NLP) tasks are commonly long-tailed\nwith noisy labels. Those problems challenge the generalization and robustness\nof complex models such as Deep Neural Networks (DNNs). Some commonly used\nresampling techniques, such as oversampling or undersampling, could easily lead\nto overfitting. It is growing popular to learn the data weights leveraging a\nsmall amount of metadata. Besides, recent studies have shown the advantages of\nself-supervised pre-training, particularly to the under-represented data. In\nthis work, we propose a general framework to handle the problem of both\nlong-tail and noisy labels. The model is adapted to the domain of problems in a\ncontrastive learning manner. The re-weighting module is a feed-forward network\nthat learns explicit weighting functions and adapts weights according to\nmetadata. The framework further adapts weights of terms in the loss function\nthrough a combination of the polynomial expansion of cross-entropy loss and\nfocal loss. Our extensive experiments show that the proposed framework\nconsistently outperforms baseline methods. Lastly, our sensitive analysis\nemphasizes the capability of the proposed framework to handle the long-tailed\nproblem and mitigate the negative impact of noisy labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_S/0/1/0/all/0/1\">Sunyi Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zheng Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing for Policymaking. (arXiv:2302.03490v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03490","description":"<p>Language is the medium for many political activities, from campaigns to news\nreports. Natural language processing (NLP) uses computational tools to parse\ntext into key information that is needed for policymaking. In this chapter, we\nintroduce common methods of NLP, including text classification, topic modeling,\nevent extraction, and text scaling. We then overview how these methods can be\nused for policymaking through four major applications including data collection\nfor evidence-based policymaking, interpretation of political decisions, policy\ncommunication, and investigation of policy effects. Finally, we highlight some\npotential limitations and ethical concerns when using NLP for policymaking.\n</p>\n<p>This text is from Chapter 7 (pages 141-162) of the Handbook of Computational\nSocial Science for Policy (2023). Open Access on Springer:\nhttps://doi.org/10.1007/978-3-031-16624-2\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Translation Quality Evaluation on Low Resource Languages from Large Language Models. (arXiv:2302.03491v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03491","description":"<p>Learned metrics such as BLEURT have in recent years become widely employed to\nevaluate the quality of machine translation systems. Training such metrics\nrequires data which can be expensive and difficult to acquire, particularly for\nlower-resource languages. We show how knowledge can be distilled from Large\nLanguage Models (LLMs) to improve upon such learned metrics without requiring\nhuman annotators, by creating synthetic datasets which can be mixed into\nexisting datasets, requiring only a corpus of text in the target language. We\nshow that the performance of a BLEURT-like model on lower resource languages\ncan be improved in this way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohtashami_A/0/1/0/all/0/1\">Amirkeivan Mohtashami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verzetti_M/0/1/0/all/0/1\">Mauro Verzetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubenstein_P/0/1/0/all/0/1\">Paul K. Rubenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03494","description":"<p>Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,\nwhich is the focus of this study. Ten categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis on YouTube Smart Phone Unboxing Video Reviews in Sri Lanka. (arXiv:2302.03496v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03496","description":"<p>Product-related reviews are based on users' experiences that are mostly\nshared on videos in YouTube. It is the second most popular website globally in\n2021. People prefer to watch videos on recently released products prior to\npurchasing, in order to gather overall feedback and make worthy decisions.\nThese videos are created by vloggers who are enthusiastic about technical\nmaterials and feedback is usually placed by experienced users of the product or\nits brand. Analyzing the sentiment of the user reviews gives useful insights\ninto the product in general. This study is focused on three smartphone reviews,\nnamely, Apple iPhone 13, Google Pixel 6, and Samsung Galaxy S21 which were\nreleased in 2021. VADER, which is a lexicon and rule-based sentiment analysis\ntool was used to classify each comment to its appropriate positive or negative\norientation. All three smartphones show a positive sentiment from the users'\nperspective and iPhone 13 has the highest number of positive reviews. The\nresulting models have been tested using N\\\"aive Bayes, Decision Tree, and\nSupport Vector Machine. Among these three classifiers, Support Vector Machine\nshows higher accuracies and F1-scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sally_S/0/1/0/all/0/1\">Sherina Sally</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAMP: A unified framework boosting low resource automatic speech recognition. (arXiv:2302.03498v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03498","description":"<p>We propose a novel text-to-speech (TTS) data augmentation framework for low\nresource automatic speech recognition (ASR) tasks, named phoneme audio mix up\n(PAMP). The PAMP method is highly interpretable and can incorporate prior\nknowledge of pronunciation rules. Furthermore, PAMP can be easily deployed in\nalmost any language, extremely for low resource ASR tasks. Extensive\nexperiments have demonstrated the great effectiveness of PAMP on low resource\nASR tasks: we achieve a \\textbf{10.84\\%} character error rate (CER) on the\ncommon voice Cantonese ASR task, bringing a great relative improvement of about\n\\textbf{30\\%} compared to the previous state-of-the-art which was achieved by\nfine-tuning the wav2vec2 pretrained model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_Z/0/1/0/all/0/1\">Zeping Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Q/0/1/0/all/0/1\">Qian Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+E_W/0/1/0/all/0/1\">Weinan E</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Data Augmentation for Code Generation Tasks. (arXiv:2302.03499v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03499","description":"<p>Advances in natural language processing, such as transfer learning from\npre-trained language models, have impacted how models are trained for\nprogramming language tasks too. Previous research primarily explored code\npre-training and expanded it through multi-modality and multi-tasking, yet the\ndata for downstream tasks remain modest in size. Focusing on data utilization\nfor downstream tasks, we propose and adapt augmentation methods that yield\nconsistent improvements in code translation and summarization by up to 6.9% and\n7.5% respectively. Further analysis suggests that our methods work orthogonally\nand show benefits in output code style and numeric consistency. We also discuss\ntest data imperfections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pinzhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampouras_G/0/1/0/all/0/1\">Gerasimos Lampouras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Learning Siamese Network for Few-Shot Text Classification. (arXiv:2302.03507v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03507","description":"<p>Few-shot learning has been used to tackle the problem of label scarcity in\ntext classification, of which meta-learning based methods have shown to be\neffective, such as the prototypical networks (PROTO). Despite the success of\nPROTO, there still exist three main problems: (1) ignore the randomness of the\nsampled support sets when computing prototype vectors; (2) disregard the\nimportance of labeled samples; (3) construct meta-tasks in a purely random\nmanner. In this paper, we propose a Meta-Learning Siamese Network, namely,\nMeta-SN, to address these issues. Specifically, instead of computing prototype\nvectors from the sampled support sets, Meta-SN utilizes external knowledge\n(e.g. class names and descriptive texts) for class labels, which is encoded as\nthe low-dimensional embeddings of prototype vectors. In addition, Meta-SN\npresents a novel sampling strategy for constructing meta-tasks, which gives\nhigher sampling probabilities to hard-to-classify samples. Extensive\nexperiments are conducted on six benchmark datasets to show the clear\nsuperiority of Meta-SN over other state-of-the-art models. For reproducibility,\nall the datasets and codes are provided at https://github.com/hccngu/Meta-SN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chengcheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingnan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Ming Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aoying Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cluster-Level Contrastive Learning for Emotion Recognition in Conversations. (arXiv:2302.03508v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03508","description":"<p>A key challenge for Emotion Recognition in Conversations (ERC) is to\ndistinguish semantically similar emotions. Some works utilise Supervised\nContrastive Learning (SCL) which uses categorical emotion labels as supervision\nsignals and contrasts in high-dimensional semantic space. However, categorical\nlabels fail to provide quantitative information between emotions. ERC is also\nnot equally dependent on all embedded features in the semantic space, which\nmakes the high-dimensional SCL inefficient. To address these issues, we propose\na novel low-dimensional Supervised Cluster-level Contrastive Learning (SCCL)\nmethod, which first reduces the high-dimensional SCL space to a\nthree-dimensional affect representation space Valence-Arousal-Dominance (VAD),\nthen performs cluster-level contrastive learning to incorporate measurable\nemotion prototypes. To help modelling the dialogue and enriching the context,\nwe leverage the pre-trained knowledge adapters to infuse linguistic and factual\nknowledge. Experiments show that our method achieves new state-of-the-art\nresults with 69.81% on IEMOCAP, 65.7% on MELD, and 62.51% on DailyDialog\ndatasets. The analysis also proves that the VAD space is not only suitable for\nERC but also interpretable, with VAD prototypes enhancing its performance and\nstabilising the training of SCCL. In addition, the pre-trained knowledge\nadapters benefit the performance of the utterance encoder and SCCL. Our code is\navailable at: https://github.com/SteveKGYang/SCCL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhuzali_H/0/1/0/all/0/1\">Hassan Alhuzali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Arabic Named Entity Recognition: Past, Recent Advances, and Future Trends. (arXiv:2302.03512v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03512","description":"<p>As more and more Arabic texts emerged on the Internet, extracting important\ninformation from these Arabic texts is especially useful. As a fundamental\ntechnology, Named entity recognition (NER) serves as the core component in\ninformation extraction technology, while also playing a critical role in many\nother Natural Language Processing (NLP) systems, such as question answering and\nknowledge graph building. In this paper, we provide a comprehensive review of\nthe development of Arabic NER, especially the recent advances in deep learning\nand pre-trained language model. Specifically, we first introduce the background\nof Arabic NER, including the characteristics of Arabic and existing resources\nfor Arabic NER. Then, we systematically review the development of Arabic NER\nmethods. Traditional Arabic NER systems focus on feature engineering and\ndesigning domain-specific rules. In recent years, deep learning methods achieve\nsignificant progress by representing texts via continuous vector\nrepresentations. With the growth of pre-trained language model, Arabic NER\nyields better performance. Finally, we conclude the method gap between Arabic\nNER and NER methods from other languages, which helps outline future directions\nfor Arabic NER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yingjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1\">Qingrong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zechang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1\">Baoxing Huai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficiently Upgrading Multilingual Machine Translation Models to Support More Languages. (arXiv:2302.03528v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03528","description":"<p>With multilingual machine translation (MMT) models continuing to grow in size\nand number of supported languages, it is natural to reuse and upgrade existing\nmodels to save computation as data becomes available in more languages.\nHowever, adding new languages requires updating the vocabulary, which\ncomplicates the reuse of embeddings. The question of how to reuse existing\nmodels while also making architectural changes to provide capacity for both old\nand new languages has also not been closely studied. In this work, we introduce\nthree techniques that help speed up effective learning of the new languages and\nalleviate catastrophic forgetting despite vocabulary and architecture\nmismatches. Our results show that by (1) carefully initializing the network,\n(2) applying learning rate scaling, and (3) performing data up-sampling, it is\npossible to exceed the performance of a same-sized baseline model with 30%\ncomputation and recover the performance of a larger model trained from scratch\nwith over 50% reduction in computation. Furthermore, our analysis reveals that\nthe introduced techniques help learn the new directions more effectively and\nalleviate catastrophic forgetting at the same time. We hope our work will guide\nresearch into more efficient approaches to growing languages for these MMT\nmodels and ultimately maximize the reuse of existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Simeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elbayad_M/0/1/0/all/0/1\">Maha Elbayad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Anna Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CALaMo: a Constructionist Assessment of Language Models. (arXiv:2302.03589v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03589","description":"<p>This paper presents a novel framework for evaluating Neural Language Models'\nlinguistic abilities using a constructionist approach. Not only is the\nusage-based model in line with the underlying stochastic philosophy of neural\narchitectures, but it also allows the linguist to keep meaning as a determinant\nfactor in the analysis. We outline the framework and present two possible\nscenarios for its application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pannitto_L/0/1/0/all/0/1\">Ludovica Pannitto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herbelot_A/0/1/0/all/0/1\">Aur&#xe9;lie Herbelot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploitation and exploration in text evolution. Quantifying planning and translation flows during writing. (arXiv:2302.03645v1 [cs.CL])","link":"http://arxiv.org/abs/2302.03645","description":"<p>Writing is a complex process at the center of much of modern human activity.\nDespite it appears to be a linear process, writing conceals many highly\nnon-linear processes. Previous research has focused on three phases of writing:\nplanning, translation and transcription, and revision. While research has shown\nthese are non-linear, they are often treated linearly when measured. Here, we\nintroduce measures to detect and quantify subcycles of planning (exploration)\nand translation (exploitation) during the writing process. We apply these to a\nnovel dataset that recorded the creation of a text in all its phases, from\nearly attempts to the finishing touches on a final version. This dataset comes\nfrom a series of writing workshops in which, through innovative versioning\nsoftware, we were able to record all the steps in the construction of a text.\nMore than 60 junior researchers in science wrote a scientific essay intended\nfor a general readership. We recorded each essay as a writing cloud, defined as\na complex topological structure capturing the history of the essay itself.\nThrough this unique dataset of writing clouds, we expose a representation of\nthe writing process that quantifies its complexity and the writer's efforts\nthroughout the draft and through time. Interestingly, this representation\nhighlights the phases of \"translation flow\", where authors improve existing\nideas, and exploration, where creative deviations appear as the writer returns\nto the planning phase. These turning points between translation and exploration\nbecome rarer as the writing process progresses and the author approaches the\nfinal version. Our results and the new measures introduced have the potential\nto foster the discussion about the non-linear nature of writing and support the\ndevelopment of tools that can support more creative and impactful writing\nprocesses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sardo_D/0/1/0/all/0/1\">Donald Ruggiero Lo Sardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gravino_P/0/1/0/all/0/1\">Pietro Gravino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuskley_C/0/1/0/all/0/1\">Christine Cuskley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loreto_V/0/1/0/all/0/1\">Vittorio Loreto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery. (arXiv:2302.03668v1 [cs.LG])","link":"http://arxiv.org/abs/2302.03668","description":"<p>The strength of modern generative models lies in their ability to be\ncontrolled through text-based prompts. Typical \"hard\" prompts are made from\ninterpretable words and tokens, and must be hand-crafted by humans. There are\nalso \"soft\" prompts, which consist of continuous feature vectors. These can be\ndiscovered using powerful optimization methods, but they cannot be easily\ninterpreted, re-used across models, or plugged into a text-based interface.\n</p>\n<p>We describe an approach to robustly optimize hard text prompts through\nefficient gradient-based optimization. Our approach automatically generates\nhard text-based prompts for both text-to-image and text-to-text applications.\nIn the text-to-image setting, the method creates hard prompts for diffusion\nmodels, allowing API users to easily generate, discover, and mix and match\nimage concepts without prior knowledge on how to prompt the model. In the\ntext-to-text setting, we show that hard prompts can be automatically discovered\nthat are effective in tuning LMs for classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Neel Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchenbauer_J/0/1/0/all/0/1\">John Kirchenbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auditing Gender Presentation Differences in Text-to-Image Models. (arXiv:2302.03675v1 [cs.CV])","link":"http://arxiv.org/abs/2302.03675","description":"<p>Text-to-image models, which can generate high-quality images based on textual\ninput, have recently enabled various content-creation tools. Despite\nsignificantly affecting a wide range of downstream applications, the\ndistributions of these generated images are still not fully understood,\nespecially when it comes to the potential stereotypical attributes of different\ngenders. In this work, we propose a paradigm (Gender Presentation Differences)\nthat utilizes fine-grained self-presentation attributes to study how gender is\npresented differently in text-to-image models. By probing gender indicators in\nthe input text (e.g., \"a woman\" or \"a man\"), we quantify the frequency\ndifferences of presentation-centric attributes (e.g., \"a shirt\" and \"a dress\")\nthrough human annotation and introduce a novel metric: GEP. Furthermore, we\npropose an automatic method to estimate such differences. The automatic GEP\nmetric based on our approach yields a higher correlation with human annotations\nthan that based on existing CLIP scores, consistently across three\nstate-of-the-art text-to-image models. Finally, we demonstrate the\ngeneralization ability of our metrics in the context of gender stereotypes\nrelated to occupations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turk_G/0/1/0/all/0/1\">Greg Turk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morality, Machines and the Interpretation Problem: A Value-based, Wittgensteinian Approach to Building Moral Agents. (arXiv:2103.02728v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2103.02728","description":"<p>We present what we call the Interpretation Problem, whereby any rule in\nsymbolic form is open to infinite interpretation in ways that we might\ndisapprove of and argue that any attempt to build morality into machines is\nsubject to it. We show how the Interpretation Problem in Artificial\nIntelligence is an illustration of Wittgenstein's general claim that no rule\ncan contain the criteria for its own application, and that the risks created by\nthis problem escalate in proportion to the degree to which to machine is\ncausally connected to the world, in what we call the Law of Interpretative\nExposure. Using game theory, we attempt to define the structure of normative\nspaces and argue that any rule-following within a normative space is guided by\nvalues that are external to that space and which cannot themselves be\nrepresented as rules. In light of this, we categorise the types of mistakes an\nartificial moral agent could make into Mistakes of Intention and Instrumental\nMistakes, and we propose ways of building morality into machines by getting\nthem to interpret the rules we give in accordance with these external values,\nthrough explicit moral reasoning, the Show, not Tell paradigm, the adjustment\nof causal power and structure of the agent, and relational values, with the\nultimate aim that the machine develop a virtuous character and that the impact\nof the Interpretation Problem is minimised.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Badea_C/0/1/0/all/0/1\">Cosmin Badea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artus_G/0/1/0/all/0/1\">Gregory Artus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Comparison of Pre-training Language Models. (arXiv:2106.11483v8 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.11483","description":"<p>Recently, the development of pre-trained language models has brought natural\nlanguage processing (NLP) tasks to the new state-of-the-art. In this paper we\nexplore the efficiency of various pre-trained language models. We pre-train a\nlist of transformer-based models with the same amount of text and the same\ntraining steps. The experimental results shows that the most improvement upon\nthe origin BERT is adding the RNN-layer to capture more contextual information\nfor short text understanding. But the conclusion is: There are no remarkable\nimprovement for short text understanding for similar BERT structures.\nData-centric method[12] can achieve better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaQA: Combining Expert Agents for Multi-Skill Question Answering. (arXiv:2112.01922v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.01922","description":"<p>The recent explosion of question answering (QA) datasets and models has\nincreased the interest in the generalization of models across multiple domains\nand formats by either training on multiple datasets or by combining multiple\nmodels. Despite the promising results of multi-dataset models, some domains or\nQA formats may require specific architectures, and thus the adaptability of\nthese models might be limited. In addition, current approaches for combining\nmodels disregard cues such as question-answer compatibility. In this work, we\npropose to combine expert agents with a novel, flexible, and training-efficient\narchitecture that considers questions, answer predictions, and\nanswer-prediction confidence scores to select the best answer among a list of\nanswer candidates. Through quantitative and qualitative experiments we show\nthat our model i) creates a collaboration between agents that outperforms\nprevious multi-agent and multi-dataset approaches in both in-domain and\nout-of-domain scenarios, ii) is highly data-efficient to train, and iii) can be\nadapted to any QA format. We release our code and a dataset of answer\npredictions from expert agents for 16 QA datasets to foster future developments\nof multi-agent systems on https://github.com/UKPLab/MetaQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puerto_H/0/1/0/all/0/1\">Haritz Puerto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_G/0/1/0/all/0/1\">G&#xf6;zde G&#xfc;l &#x15e;ahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advances in Neural Text Generation: A Task-Agnostic Survey. (arXiv:2203.03047v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03047","description":"<p>In recent years much effort has been devoted to applying neural models to the\ntask of natural language generation. The challenge is to generate natural\nhuman-like text, and to control the generation process. This paper presents a\ntask-agnostic survey of recent advances in neural text generation. These\nadvances have been achieved by numerous developments, which we group under the\nfollowing four headings: data construction, neural frameworks, training and\ninference strategies, and evaluation metrics. Finally we discuss the future\ndirections for the development of neural text generation including neural\npipelines and exploiting back-ground knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yucheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages. (arXiv:2203.08388v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08388","description":"<p>While there has been a recent burgeoning of applications at the intersection\nof natural and programming languages, such as code generation and code\nsummarization, these applications are usually English-centric. This creates a\nbarrier for program developers who are not proficient in English. To mitigate\nthis gap in technology development across languages, we propose a multilingual\ndataset, MCoNaLa, to benchmark code generation from natural language commands\nextending beyond English. Modeled off of the methodology from the English\nCode/Natural Language Challenge (CoNaLa) dataset, we annotated a total of 896\nNL-code pairs in three languages: Spanish, Japanese, and Russian. We present a\nquantitative evaluation of performance on the MCoNaLa dataset by testing with\nstate-of-the-art code generation systems. While the difficulties vary across\nthese three languages, all systems lag significantly behind their English\ncounterparts, revealing the challenges in adapting code generation to new\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuenca_G/0/1/0/all/0/1\">Grace Cuenca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions. (arXiv:2205.00415v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00415","description":"<p>In recent years, progress in NLU has been driven by benchmarks. These\nbenchmarks are typically collected by crowdsourcing, where annotators write\nexamples based on annotation instructions crafted by dataset creators. In this\nwork, we hypothesize that annotators pick up on patterns in the crowdsourcing\ninstructions, which bias them to write many similar examples that are then\nover-represented in the collected data. We study this form of bias, termed\ninstruction bias, in 14 recent NLU benchmarks, showing that instruction\nexamples often exhibit concrete patterns, which are propagated by crowdworkers\nto the collected data. This extends previous work (Geva et al., 2019) and\nraises a new concern of whether we are modeling the dataset creator's\ninstructions, rather than the task. Through a series of experiments, we show\nthat, indeed, instruction bias can lead to overestimation of model performance,\nand that models struggle to generalize beyond biases originating in the\ncrowdsourcing instructions. We further analyze the influence of instruction\nbias in terms of pattern frequency and model size, and derive concrete\nrecommendations for creating future NLU benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Non-monotonic Self-terminating Language Model. (arXiv:2210.00660v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.00660","description":"<p>Recent large-scale neural autoregressive sequence models have shown\nimpressive performances on a variety of natural language generation tasks.\nHowever, their generated sequences often exhibit degenerate properties such as\nnon-termination, undesirable repetition, and premature termination, when\ngenerated with decoding algorithms such as greedy search, beam search, top-$k$\nsampling, and nucleus sampling. In this paper, we focus on the problem of\nnon-terminating sequences resulting from an incomplete decoding algorithm. We\nfirst define an incomplete probable decoding algorithm which includes greedy\nsearch, top-$k$ sampling, and nucleus sampling, beyond the incomplete decoding\nalgorithm originally put forward by Welleck et al. (2020). We then propose a\nnon-monotonic self-terminating language model, which significantly relaxes the\nconstraint of monotonically increasing termination probability in the\noriginally proposed self-terminating language model by Welleck et al. (2020),\nto address the issue of non-terminating sequences when using incomplete\nprobable decoding algorithms. We prove that our proposed model prevents\nnon-terminating sequences when using not only incomplete probable decoding\nalgorithms but also beam search. We empirically validate our model on sequence\ncompletion tasks with various architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eugene Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Cheolhyoung Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation. (arXiv:2210.07054v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07054","description":"<p>Sign language gloss translation aims to translate the sign glosses into\nspoken language texts, which is challenging due to the scarcity of labeled\ngloss-text parallel data. Back translation (BT), which generates\npseudo-parallel data by translating in-domain spoken language texts into sign\nglosses, has been applied to alleviate the data scarcity problem. However, the\nlack of large-scale high-quality domain spoken language text data limits the\neffect of BT. In this paper, to overcome the limitation, we propose a Prompt\nbased domain text Generation (PGEN) approach to produce the large-scale\nin-domain spoken language text data. Specifically, PGEN randomly concatenates\nsentences from the original in-domain spoken language text data as prompts to\ninduce a pre-trained language model (i.e., GPT-2) to generate spoken language\ntexts in a similar style. Experimental results on three benchmarks of sign\nlanguage gloss translation in varied languages demonstrate that BT with spoken\nlanguage texts generated by PGEN significantly outperforms the compared\nmethods. In addition, as the scale of spoken language texts generated by PGEN\nincreases, the BT technique can achieve further improvements, demonstrating the\neffectiveness of our approach. We release the code and data for facilitating\nfuture research in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jinhui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FADO: Feedback-Aware Double COntrolling Network for Emotional Support Conversation. (arXiv:2211.00250v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.00250","description":"<p>Emotional Support Conversation (ESConv) aims to reduce help-seekers'emotional\ndistress with the supportive strategy and response. It is essential for the\nsupporter to select an appropriate strategy with the feedback of the\nhelp-seeker (e.g., emotion change during dialog turns, etc) in ESConv. However,\nprevious methods mainly focus on the dialog history to select the strategy and\nignore the help-seeker's feedback, leading to the wrong and user-irrelevant\nstrategy prediction. In addition, these approaches only model the\ncontext-to-strategy flow and pay less attention to the strategy-to-context flow\nthat can focus on the strategy-related context for generating the\nstrategy-constrain response. In this paper, we propose a Feedback-Aware Double\nCOntrolling Network (FADO) to make a strategy schedule and generate the\nsupportive response. The core module in FADO consists of a dual-level feedback\nstrategy selector and a double control reader. Specifically, the dual-level\nfeedback strategy selector leverages the turn-level and conversation-level\nfeedback to encourage or penalize strategies. The double control reader\nconstructs the novel strategy-to-context flow for generating the\nstrategy-constrain response. Furthermore, a strategy dictionary is designed to\nenrich the semantic information of the strategy and improve the quality of\nstrategy-constrain response. Experimental results on ESConv show that the\nproposed FADO has achieved the state-of-the-art performance in terms of both\nstrategy selection and response generation. Our code is available at\nhttps://github.com/Thedatababbler/FADO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Ziyuan Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunpeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting Neural Machine Translation with Translation Memories. (arXiv:2301.05380v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.05380","description":"<p>Improving machine translation (MT) systems with translation memories (TMs) is\nof great interest to practitioners in the MT community. However, previous\napproaches require either a significant update of the model architecture and/or\nadditional training efforts to make the models well-behaved when TMs are taken\nas additional input. In this paper, we present a simple but effective method to\nintroduce TMs into neural machine translation (NMT) systems. Specifically, we\ntreat TMs as prompts to the NMT model at test time, but leave the training\nprocess unchanged. The result is a slight update of an existing NMT system,\nwhich can be implemented in a few hours by anyone who is familiar with NMT.\nExperimental results on several datasets demonstrate that our system\nsignificantly outperforms strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reheman_A/0/1/0/all/0/1\">Abudurexiti Reheman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yingfeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Di Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProKD: An Unsupervised Prototypical Knowledge Distillation Network for Zero-Resource Cross-Lingual Named Entity Recognition. (arXiv:2301.08855v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.08855","description":"<p>For named entity recognition (NER) in zero-resource languages, utilizing\nknowledge distillation methods to transfer language-independent knowledge from\nthe rich-resource source languages to zero-resource languages is an effective\nmeans. Typically, these approaches adopt a teacher-student architecture, where\nthe teacher network is trained in the source language, and the student network\nseeks to learn knowledge from the teacher network and is expected to perform\nwell in the target language. Despite the impressive performance achieved by\nthese methods, we argue that they have two limitations. Firstly, the teacher\nnetwork fails to effectively learn language-independent knowledge shared across\nlanguages due to the differences in the feature distribution between the source\nand target languages. Secondly, the student network acquires all of its\nknowledge from the teacher network and ignores the learning of target\nlanguage-specific knowledge. Undesirably, these limitations would hinder the\nmodel's performance in the target language. This paper proposes an unsupervised\nprototype knowledge distillation network (ProKD) to address these issues.\nSpecifically, ProKD presents a contrastive learning-based prototype alignment\nmethod to achieve class feature alignment by adjusting the distance among\nprototypes in the source and target languages, boosting the teacher network's\ncapacity to acquire language-independent knowledge. In addition, ProKD\nintroduces a prototypical self-training method to learn the intrinsic structure\nof the language by retraining the student network on the target data using\nsamples' distance information from prototypes, thereby enhancing the student\nnetwork's ability to acquire language-specific knowledge. Extensive experiments\non three benchmark cross-lingual NER datasets demonstrate the effectiveness of\nour approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1\">Ling Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chunming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guanghui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jihong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N-Gram Nearest Neighbor Machine Translation. (arXiv:2301.12866v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12866","description":"<p>Nearest neighbor machine translation augments the Autoregressive\nTranslation~(AT) with $k$-nearest-neighbor retrieval, by comparing the\nsimilarity between the token-level context representations of the target tokens\nin the query and the datastore. However, the token-level representation may\nintroduce noise when translating ambiguous words, or fail to provide accurate\nretrieval results when the representation generated by the model contains\nindistinguishable context information, e.g., Non-Autoregressive\nTranslation~(NAT) models. In this paper, we propose a novel $n$-gram nearest\nneighbor retrieval method that is model agnostic and applicable to both AT and\nNAT models. Specifically, we concatenate the adjacent $n$-gram hidden\nrepresentations as the key, while the tuple of corresponding target tokens is\nthe value. In inference, we propose tailored decoding algorithms for AT and NAT\nmodels respectively. We demonstrate that the proposed method consistently\noutperforms the token-level method on both AT and NAT models as well on general\nas on domain adaptation translation tasks. On domain adaptation, the proposed\nmethod brings $1.03$ and $2.76$ improvements regarding the average BLEU score\non AT and NAT models respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_R/0/1/0/all/0/1\">Rui Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Understanding Word-level Textual Adversarial Attack via n-gram Frequency Descend. (arXiv:2302.02568v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.02568","description":"<p>Word-level textual adversarial attacks have achieved striking performance in\nfooling natural language processing models. However, the fundamental questions\nof why these attacks are effective, and the intrinsic properties of the\nadversarial examples (AEs), are still not well understood. This work attempts\nto interpret textual attacks through the lens of $n$-gram frequency.\nSpecifically, it is revealed that existing word-level attacks exhibit a strong\ntendency toward generation of examples with $n$-gram frequency descend\n($n$-FD). Intuitively, this finding suggests a natural way to improve model\nrobustness by training the model on the $n$-FD examples. To verify this idea,\nwe devise a model-agnostic and gradient-free AE generation approach that relies\nsolely on the $n$-gram frequency information, and further integrate it into the\nrecently proposed convex hull framework for adversarial training. Surprisingly,\nthe resultant method performs quite similarly to the original gradient-based\nmethod in terms of model robustness. These findings provide a\nhuman-understandable perspective for interpreting word-level textual\nadversarial attacks, and a new direction to improve model robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1\">Ning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}