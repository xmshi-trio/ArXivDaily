{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-10-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Continuum of Generation Tasks for Investigating Length Bias and Degenerate Repetition. (arXiv:2210.10817v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10817","description":"<p>Language models suffer from various degenerate behaviors. These differ\nbetween tasks: machine translation (MT) exhibits length bias, while tasks like\nstory generation exhibit excessive repetition. Recent work has attributed the\ndifference to task constrainedness, but evidence for this claim has always\ninvolved many confounding variables. To study this question directly, we\nintroduce a new experimental framework that allows us to smoothly vary task\nconstrainedness, from MT at one end to fully open-ended generation at the\nother, while keeping all other aspects fixed. We find that: (1) repetition\ndecreases smoothly with constrainedness, explaining the difference in\nrepetition across tasks; (2) length bias surprisingly also decreases with\nconstrainedness, suggesting some other cause for the difference in length bias;\n(3) across the board, these problems affect the mode, not the whole\ndistribution; (4) the differences cannot be attributed to a change in the\nentropy of the distribution, since another method of changing the entropy,\nlabel smoothing, does not produce the same effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riley_D/0/1/0/all/0/1\">Darcey Riley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VTC: Improving Video-Text Retrieval with User Comments. (arXiv:2210.10820v1 [cs.CV])","link":"http://arxiv.org/abs/2210.10820","description":"<p>Multi-modal retrieval is an important problem for many applications, such as\nrecommendation and search. Current benchmarks and even datasets are often\nmanually constructed and consist of mostly clean samples where all modalities\nare well-correlated with the content. Thus, current video-text retrieval\nliterature largely focuses on video titles or audio transcripts, while ignoring\nuser comments, since users often tend to discuss topics only vaguely related to\nthe video. Despite the ubiquity of user comments online, there is currently no\nmulti-modal representation learning datasets that includes comments. In this\npaper, we a) introduce a new dataset of videos, titles and comments; b) present\nan attention-based mechanism that allows the model to learn from sometimes\nirrelevant data such as comments; c) show that by using comments, our method is\nable to learn better, more contextualised, representations for image, video and\naudio representations. Project page: https://unitaryai.github.io/vtc-paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanu_L/0/1/0/all/0/1\">Laura Hanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thewlis_J/0/1/0/all/0/1\">James Thewlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuki M. Asano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rupprecht_C/0/1/0/all/0/1\">Christian Rupprecht</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Hate Speech Varies by Target Identity: A Computational Analysis. (arXiv:2210.10839v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10839","description":"<p>This paper investigates how hate speech varies in systematic ways according\nto the identities it targets. Across multiple hate speech datasets annotated\nfor targeted identities, we find that classifiers trained on hate speech\ntargeting specific identity groups struggle to generalize to other targeted\nidentities. This provides empirical evidence for differences in hate speech by\ntarget identity; we then investigate which patterns structure this variation.\nWe find that the targeted demographic category (e.g. gender/sexuality or\nrace/ethnicity) appears to have a greater effect on the language of hate speech\nthan does the relative social power of the targeted identity group. We also\nfind that words associated with hate speech targeting specific identities often\nrelate to stereotypes, histories of oppression, current social movements, and\nother social contexts specific to identities. These experiments suggest the\nimportance of considering targeted identity, as well as the social contexts\nassociated with these identities, in automated hate speech classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoder_M/0/1/0/all/0/1\">Michael Miller Yoder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_L/0/1/0/all/0/1\">Lynnette Hui Xian Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">David West Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carley_K/0/1/0/all/0/1\">Kathleen M. Carley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models. (arXiv:2210.10841v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10841","description":"<p>Prompt learning is a new learning paradigm which reformulates downstream\ntasks as similar pretraining tasks on pretrained models by leveraging textual\nprompts. Recent works have demonstrated that prompt learning is particularly\nuseful for few-shot learning, where there is limited training data. Depending\non the granularity of prompts, those methods can be roughly divided into\ntask-level prompting and instance-level prompting. Task-level prompting methods\nlearn one universal prompt for all input samples, which is efficient but\nineffective to capture subtle differences among different classes.\nInstance-level prompting methods learn a specific prompt for each input, though\neffective but inefficient. In this work, we develop a novel prototype-based\nprompt learning method to overcome the above limitations. In particular, we\nfocus on few-shot image recognition tasks on pretrained vision-language models\n(PVLMs) and develop a method of prompting through prototype (PTP), where we\ndefine $K$ image prototypes and $K$ prompt prototypes. In PTP, the image\nprototype represents a centroid of a certain image cluster in the latent space\nand a prompt prototype is defined as a soft prompt in the continuous space. The\nsimilarity between a query image and an image prototype determines how much\nthis prediction relies on the corresponding prompt prototype. Hence, in PTP,\nsimilar images will utilize similar prompting ways. Through extensive\nexperiments on seven real-world benchmarks, we show that PTP is an effective\nmethod to leverage the latent knowledge and adaptive to various PVLMs.\nMoreover, through detailed analysis, we discuss pros and cons for prompt\nlearning and parameter-efficient fine-tuning under the context of few-shot\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hongliang Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingcheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Turn Debate Doesn't Help Humans Answer Hard Reading Comprehension Questions. (arXiv:2210.10860v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10860","description":"<p>The use of language-model-based question-answering systems to aid humans in\ncompleting difficult tasks is limited, in part, by the unreliability of the\ntext these systems generate. Using hard multiple-choice reading comprehension\nquestions as a testbed, we assess whether presenting humans with arguments for\ntwo competing answer options, where one is correct and the other is incorrect,\nallows human judges to perform more accurately, even when one of the arguments\nis unreliable and deceptive. If this is helpful, we may be able to increase our\njustified trust in language-model-based systems by asking them to produce these\narguments where needed. Previous research has shown that just a single turn of\narguments in this format is not helpful to humans. However, as debate settings\nare characterized by a back-and-forth dialogue, we follow up on previous\nresults to test whether adding a second round of counter-arguments is helpful\nto humans. We find that, regardless of whether they have access to arguments or\nnot, humans perform similarly on our task. These findings suggest that, in the\ncase of answering reading comprehension questions, debate is not a helpful\nformat.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1\">Alicia Parrish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1\">Harsh Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nangia_N/0/1/0/all/0/1\">Nikita Nangia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1\">Vishakh Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saimbhi_A/0/1/0/all/0/1\">Amanpreet Singh Saimbhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation. (arXiv:2210.10861v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10861","description":"<p>Question answering (QA) has recently shown impressive results for answering\nquestions from customized domains. Yet, a common challenge is to adapt QA\nmodels to an unseen target domain. In this paper, we propose a novel\nself-supervised framework called QADA for QA domain adaptation. QADA introduces\na novel data augmentation pipeline used to augment training QA samples.\nDifferent from existing methods, we enrich the samples via hidden space\naugmentation. For questions, we introduce multi-hop synonyms and sample\naugmented token embeddings with Dirichlet distributions. For contexts, we\ndevelop an augmentation method which learns to drop context spans via a custom\nattentive sampling strategy. Additionally, contrastive learning is integrated\nin the proposed self-supervised adaptation framework QADA. Unlike existing\napproaches, we generate pseudo labels and propose to train the model via a\nnovel attention-based contrastive adaptation method. The attention weights are\nused to build informative features for discrepancy estimation that helps the QA\nmodel separate answers and generalize across source and target domains. To the\nbest of our knowledge, our work is the first to leverage hidden space\naugmentation and attention-based contrastive adaptation for self-supervised\ndomain adaptation in QA. Our evaluation shows that QADA achieves considerable\nimprovements on multiple target datasets over state-of-the-art baselines in QA\ndomain adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhenrui Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Huimin Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kratzwald_B/0/1/0/all/0/1\">Bernhard Kratzwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feuerriegel_S/0/1/0/all/0/1\">Stefan Feuerriegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"G-Augment: Searching For The Meta-Structure Of Data Augmentation Policies For ASR. (arXiv:2210.10879v1 [cs.LG])","link":"http://arxiv.org/abs/2210.10879","description":"<p>Data augmentation is a ubiquitous technique used to provide robustness to\nautomatic speech recognition (ASR) training. However, even as so much of the\nASR training process has become automated and more \"end-to-end\", the data\naugmentation policy (what augmentation functions to use, and how to apply them)\nremains hand-crafted. We present Graph-Augment, a technique to define the\naugmentation space as directed acyclic graphs (DAGs) and search over this space\nto optimize the augmentation policy itself. We show that given the same\ncomputational budget, policies produced by G-Augment are able to perform better\nthan SpecAugment policies obtained by random search on fine-tuning tasks on\nCHiME-6 and AMI. G-Augment is also able to establish a new state-of-the-art ASR\nperformance on the CHiME-6 evaluation set (30.7% WER). We further demonstrate\nthat G-Augment policies show better transfer properties across warm-start to\ncold-start training and model size compared to random-searched SpecAugment\npolicies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gary Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1\">Ekin D.Cubuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shuyang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_R/0/1/0/all/0/1\">Ron J. Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_P/0/1/0/all/0/1\">Pedro J. Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Daniel S. Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A baseline revisited: Pushing the limits of multi-segment models for context-aware translation. (arXiv:2210.10906v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10906","description":"<p>This paper addresses the task of contextual translation using multi-segment\nmodels. Specifically we show that increasing model capacity further pushes the\nlimits of this approach and that deeper models are more suited to capture\ncontext dependencies. Furthermore, improvements observed with larger models can\nbe transferred to smaller models using knowledge distillation. Our experiments\nshow that this approach achieves competitive performance across several\nlanguages and benchmarks, without additional language-specific tuning and task\nspecific architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majumde_S/0/1/0/all/0/1\">Suvodeep Majumde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauly_S/0/1/0/all/0/1\">Stanislas Lauly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadejde_M/0/1/0/all/0/1\">Maria Nadejde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_G/0/1/0/all/0/1\">Georgiana Dinu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prophet Attention: Predicting Attention with Future Attention for Improved Image Captioning. (arXiv:2210.10914v1 [cs.CV])","link":"http://arxiv.org/abs/2210.10914","description":"<p>Recently, attention based models have been used extensively in many\nsequence-to-sequence learning systems. Especially for image captioning, the\nattention based models are expected to ground correct image regions with proper\ngenerated words. However, for each time step in the decoding process, the\nattention based models usually use the hidden state of the current input to\nattend to the image regions. Under this setting, these attention models have a\n\"deviated focus\" problem that they calculate the attention weights based on\nprevious words instead of the one to be generated, impairing the performance of\nboth grounding and captioning. In this paper, we propose the Prophet Attention,\nsimilar to the form of self-supervision. In the training stage, this module\nutilizes the future information to calculate the \"ideal\" attention weights\ntowards image regions. These calculated \"ideal\" weights are further used to\nregularize the \"deviated\" attention. In this manner, image regions are grounded\nwith the correct words. The proposed Prophet Attention can be easily\nincorporated into existing image captioning models to improve their performance\nof both grounding and captioning. The experiments on the Flickr30k Entities and\nthe MSCOCO datasets show that the proposed Prophet Attention consistently\noutperforms baselines in both automatic metrics and human evaluations. It is\nworth noticing that we set new state-of-the-arts on the two benchmark datasets\nand achieve the 1st place on the leaderboard of the online MSCOCO benchmark in\nterms of the default ranking score, i.e., CIDEr-c40.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuewei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Document Selection for Efficient Encoder Pretraining. (arXiv:2210.10951v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10951","description":"<p>Building pretrained language models is considered expensive and\ndata-intensive, but must we increase dataset size to achieve better\nperformance? We propose an alternative to larger training sets by automatically\nidentifying smaller yet domain-representative subsets. We extend Cynical Data\nSelection, a statistical sentence scoring method that conditions on a\nrepresentative target domain corpus. As an example, we treat the OntoNotes\ncorpus as a target domain and pretrain a RoBERTa-like encoder from a cynically\nselected subset of the Pile. On both perplexity and across several downstream\ntasks in the target domain, it consistently outperforms random selection with\n20x less data, 3x fewer training iterations, and 2x less estimated cloud\ncompute cost, validating the recipe of automatic document selection for LM\npretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yukun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1\">Patrick Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MBTI Personality Prediction for Fictional Characters Using Movie Scripts. (arXiv:2210.10994v1 [cs.AI])","link":"http://arxiv.org/abs/2210.10994","description":"<p>An NLP model that understands stories should be able to understand the\ncharacters in them. To support the development of neural models for this\npurpose, we construct a benchmark, Story2Personality. The task is to predict a\nmovie character's MBTI or Big 5 personality types based on the narratives of\nthe character. Experiments show that our task is challenging for the existing\ntext classification models, as none is able to largely outperform random\nguesses. We further proposed a multi-view model for personality prediction\nusing both verbal and non-verbal descriptions, which gives improvement compared\nto using only verbal descriptions. The uniqueness and challenges in our dataset\ncall for the development of narrative comprehension techniques from the\nperspective of understanding characters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sang_Y/0/1/0/all/0/1\">Yisi Sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_X/0/1/0/all/0/1\">Xiangyang Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanton_J/0/1/0/all/0/1\">Jeffrey Stanton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Chinese Spelling Check by Character Pronunciation Prediction: The Effects of Adaptivity and Granularity. (arXiv:2210.10996v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10996","description":"<p>Chinese spelling check (CSC) is a fundamental NLP task that detects and\ncorrects spelling errors in Chinese texts. As most of these spelling errors are\ncaused by phonetic similarity, effectively modeling the pronunciation of\nChinese characters is a key factor for CSC. In this paper, we consider\nintroducing an auxiliary task of Chinese pronunciation prediction (CPP) to\nimprove CSC, and, for the first time, systematically discuss the adaptivity and\ngranularity of this auxiliary task. We propose SCOPE which builds on top of a\nshared encoder two parallel decoders, one for the primary CSC task and the\nother for a fine-grained auxiliary CPP task, with a novel adaptive weighting\nscheme to balance the two tasks. In addition, we design a delicate iterative\ncorrection strategy for further improvements during inference. Empirical\nevaluation shows that SCOPE achieves new state-of-the-art on three CSC\nbenchmarks, demonstrating the effectiveness and superiority of the auxiliary\nCPP task. Comprehensive ablation studies further verify the positive effects of\nadaptivity and granularity of the task. Code and data used in this paper are\npublicly available at https://github.com/jiahaozhenbang/SCOPE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhendong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junbo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanyan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongdong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-trained Sentence Embeddings for Implicit Discourse Relation Classification. (arXiv:2210.11005v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11005","description":"<p>Implicit discourse relations bind smaller linguistic units into coherent\ntexts. Automatic sense prediction for implicit relations is hard, because it\nrequires understanding the semantics of the linked arguments. Furthermore,\nannotated datasets contain relatively few labeled examples, due to the scale of\nthe phenomenon: on average each discourse relation encompasses several dozen\nwords. In this paper, we explore the utility of pre-trained sentence embeddings\nas base representations in a neural network for implicit discourse relation\nsense classification. We present a series of experiments using both supervised\nend-to-end trained models and pre-trained sentence encoding techniques -\nSkipThought, Sent2vec and Infersent. The pre-trained embeddings are competitive\nwith the end-to-end model, and the approaches are complementary, with combined\nmodels yielding significant performance improvements on two of the three\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balusu_M/0/1/0/all/0/1\">Murali Raghu Babu Balusu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Granularity Optimization for Non-Autoregressive Translation. (arXiv:2210.11017v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11017","description":"<p>Despite low latency, non-autoregressive machine translation (NAT) suffers\nsevere performance deterioration due to the naive independence assumption. This\nassumption is further strengthened by cross-entropy loss, which encourages a\nstrict match between the hypothesis and the reference token by token. To\nalleviate this issue, we propose multi-granularity optimization for NAT, which\ncollects model behaviors on translation segments of various granularities and\nintegrates feedback for backpropagation. Experiments on four WMT benchmarks\nshow that the proposed method significantly outperforms the baseline models\ntrained with cross-entropy loss, and achieves the best performance on WMT'16\nEn-Ro and highly competitive results on WMT'14 En-De for fully\nnon-autoregressive translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yafu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yongjing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Out-of-Distribution Detection in Natural Language Understanding via Implicit Layer Ensemble. (arXiv:2210.11034v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11034","description":"<p>Out-of-distribution (OOD) detection aims to discern outliers from the\nintended data distribution, which is crucial to maintaining high reliability\nand a good user experience. Most recent studies in OOD detection utilize the\ninformation from a single representation that resides in the penultimate layer\nto determine whether the input is anomalous or not. Although such a method is\nstraightforward, the potential of diverse information in the intermediate\nlayers is overlooked. In this paper, we propose a novel framework based on\ncontrastive learning that encourages intermediate features to learn\nlayer-specialized representations and assembles them implicitly into a single\nrepresentation to absorb rich information in the pre-trained language model.\nExtensive experiments in various intent classification and OOD datasets\ndemonstrate that our approach is significantly more effective than other works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunsoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Choonghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewook Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-goo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots. (arXiv:2210.11060v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11060","description":"<p>This paper introduces Doc2Bot, a novel dataset for building machines that\nhelp users seek information via conversations. This is of particular interest\nfor companies and organizations that own a large number of manuals or\ninstruction books. Despite its potential, the nature of our task poses several\nchallenges: (1) documents contain various structures that hinder the ability of\nmachines to comprehend, and (2) user information needs are often\nunderspecified. Compared to prior datasets that either focus on a single\nstructural type or overlook the role of questioning to uncover user needs, the\nDoc2Bot dataset is developed to target such challenges systematically. Our\ndataset contains over 100,000 turns based on Chinese documents from five\ndomains, larger than any prior document-grounded dialog dataset for information\nseeking. We propose three tasks in Doc2Bot: (1) dialog state tracking to track\nuser intentions, (2) dialog policy learning to plan system actions and\ncontents, and (3) response generation which generates responses based on the\noutputs of the dialog policy. Baseline methods based on the latest deep\nlearning models are presented, indicating that our proposed tasks are\nchallenging and worthy of further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Haomin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yeqin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cam-Tu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MovieCLIP: Visual Scene Recognition in Movies. (arXiv:2210.11065v1 [cs.CV])","link":"http://arxiv.org/abs/2210.11065","description":"<p>Longform media such as movies have complex narrative structures, with events\nspanning a rich variety of ambient visual scenes. Domain specific challenges\nassociated with visual scenes in movies include transitions, person coverage,\nand a wide array of real-life and fictional scenarios. Existing visual scene\ndatasets in movies have limited taxonomies and don't consider the visual scene\ntransition within movie clips. In this work, we address the problem of visual\nscene recognition in movies by first automatically curating a new and extensive\nmovie-centric taxonomy of 179 scene labels derived from movie scripts and\nauxiliary web-based video datasets. Instead of manual annotations which can be\nexpensive, we use CLIP to weakly label 1.12 million shots from 32K movie clips\nbased on our proposed taxonomy. We provide baseline visual models trained on\nthe weakly labeled dataset called MovieCLIP and evaluate them on an independent\ndataset verified by human raters. We show that leveraging features from models\npretrained on MovieCLIP benefits downstream tasks such as multi-label scene and\ngenre classification of web videos and movie trailers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bose_D/0/1/0/all/0/1\">Digbalay Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebbar_R/0/1/0/all/0/1\">Rajat Hebbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somandepalli_K/0/1/0/all/0/1\">Krishna Somandepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_McLaughlin_K/0/1/0/all/0/1\">Kree Cole-McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huisheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Apple of Sodom: Hidden Backdoors in Superior Sentence Embeddings via Contrastive Learning. (arXiv:2210.11082v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11082","description":"<p>This paper finds that contrastive learning can produce superior sentence\nembeddings for pre-trained models but is also vulnerable to backdoor attacks.\nWe present the first backdoor attack framework, BadCSE, for state-of-the-art\nsentence embeddings under supervised and unsupervised learning settings. The\nattack manipulates the construction of positive and negative pairs so that the\nbackdoored samples have a similar embedding with the target sample (targeted\nattack) or the negative embedding of its clean version (non-targeted attack).\nBy injecting the backdoor in sentence embeddings, BadCSE is resistant against\ndownstream fine-tuning. We evaluate BadCSE on both STS tasks and other\ndownstream tasks. The supervised non-targeted attack obtains a performance\ndegradation of 194.86%, and the targeted attack maps the backdoored samples to\nthe target embedding with a 97.70% success rate while maintaining the model\nutility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_B/0/1/0/all/0/1\">Baisong Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1\">Shengfang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiqing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1\">Qingni Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghai Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Execution Time Program Verification With Tight Bounds. (arXiv:2210.11105v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11105","description":"<p>This paper presents a proof system for reasoning about execution time bounds\nfor a core imperative programming language. Proof systems are defined for three\ndifferent scenarios: approximations of the worst-case execution time, exact\ntime reasoning, and less pessimistic execution time estimation using amortized\nanalysis. We define a Hoare logic for the three cases and prove its soundness\nwith respect to an annotated cost-aware operational semantics. Finally, we\ndefine a verification conditions (VC) generator that generates the goals needed\nto prove program correctness, cost, and termination. Those goals are then sent\nto the Easycrypt toolset for validation. The practicality of the proof system\nis demonstrated with an implementation in OCaml of the different modules needed\nto apply it to example programs. Our case studies are motivated by real-time\nand cryptographic software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Ana Carolina Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_M/0/1/0/all/0/1\">Manuel Barbosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florido_M/0/1/0/all/0/1\">Mario Florido</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation. (arXiv:2210.11109v1 [cs.CV])","link":"http://arxiv.org/abs/2210.11109","description":"<p>Image-to-text tasks, such as open-ended image captioning and controllable\nimage description, have received extensive attention for decades. Here, we\nfurther advance this line of work by presenting Visual Spatial Description\n(VSD), a new perspective for image-to-text toward spatial semantics. Given an\nimage and two objects inside it, VSD aims to produce one description focusing\non the spatial perspective between the two objects. Accordingly, we manually\nannotate a dataset to facilitate the investigation of the newly-introduced task\nand build several benchmark encoder-decoder models by using VL-BART and VL-T5\nas backbones. In addition, we investigate pipeline and joint end-to-end\narchitectures for incorporating visual spatial relationship classification\n(VSRC) information into our model. Finally, we conduct experiments on our\nbenchmark dataset to evaluate all our models. Results show that our models are\nimpressive, providing accurate and human-like spatial-oriented text\ndescriptions. Meanwhile, VSRC has great potential for VSD, and the joint\nend-to-end architecture is the better choice for their integration. We make the\ndataset and codes public for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jianguo Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhichao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yueheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training Language Models with Deterministic Factual Knowledge. (arXiv:2210.11165v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11165","description":"<p>Previous works show that Pre-trained Language Models (PLMs) can capture\nfactual knowledge. However, some analyses reveal that PLMs fail to perform it\nrobustly, e.g., being sensitive to the changes of prompts when extracting\nfactual knowledge. To mitigate this issue, we propose to let PLMs learn the\ndeterministic relationship between the remaining context and the masked\ncontent. The deterministic relationship ensures that the masked factual content\ncan be deterministically inferable based on the existing clues in the context.\nThat would provide more stable patterns for PLMs to capture factual knowledge\nthan randomly masking. Two pre-training tasks are further introduced to\nmotivate PLMs to rely on the deterministic relationship when filling masks.\nSpecifically, we use an external Knowledge Base (KB) to identify deterministic\nrelationships and continuously pre-train PLMs with the proposed methods. The\nfactual knowledge probing experiments indicate that the continuously\npre-trained PLMs achieve better robustness in factual knowledge capturing.\nFurther experiments on question-answering datasets show that trying to learn a\ndeterministic relationship with the proposed methods can also help other\nknowledge-intensive tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chengjie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhenzhou Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wait-info Policy: Balancing Source and Target at Information Level for Simultaneous Machine Translation. (arXiv:2210.11220v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11220","description":"<p>Simultaneous machine translation (SiMT) outputs the translation while\nreceiving the source inputs, and hence needs to balance the received source\ninformation and translated target information to make a reasonable decision\nbetween waiting for inputs or outputting translation. Previous methods always\nbalance source and target information at the token level, either directly\nwaiting for a fixed number of tokens or adjusting the waiting based on the\ncurrent token. In this paper, we propose a Wait-info Policy to balance source\nand target at the information level. We first quantify the amount of\ninformation contained in each token, named info. Then during simultaneous\ntranslation, the decision of waiting or outputting is made based on the\ncomparison results between the total info of previous target outputs and\nreceived source inputs. Experiments show that our method outperforms strong\nbaselines under and achieves better balance via the proposed info.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shoutao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-driven Visual Object Recognition based on Knowledge Graphs. (arXiv:2210.11233v1 [cs.AI])","link":"http://arxiv.org/abs/2210.11233","description":"<p>Current deep learning methods for object recognition are purely data-driven\nand require a large number of training samples to achieve good results. Due to\ntheir sole dependence on image data, these methods tend to fail when confronted\nwith new environments where even small deviations occur. Human perception,\nhowever, has proven to be significantly more robust to such distribution\nshifts. It is assumed that their ability to deal with unknown scenarios is\nbased on extensive incorporation of contextual knowledge. Context can be based\neither on object co-occurrences in a scene or on memory of experience. In\naccordance with the human visual cortex which uses context to form different\nobject representations for a seen image, we propose an approach that enhances\ndeep learning methods by using external contextual knowledge encoded in a\nknowledge graph. Therefore, we extract different contextual views from a\ngeneric knowledge graph, transform the views into vector space and infuse it\ninto a DNN. We conduct a series of experiments to investigate the impact of\ndifferent contextual views on the learned object representations for the same\nimage dataset. The experimental results provide evidence that the contextual\nviews influence the image representations in the DNN differently and therefore\nlead to different predictions for the same images. We also show that context\nhelps to strengthen the robustness of object recognition models for\nout-of-distribution images, usually occurring in transfer learning tasks or\nreal-world scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1\">Sebastian Monka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1\">Lavdim Halilaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1\">Achim Rettinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Semantic Relation Generation. (arXiv:2210.11253v1 [cs.CV])","link":"http://arxiv.org/abs/2210.11253","description":"<p>Scene graphs provide structured semantic understanding beyond images. For\ndownstream tasks, such as image retrieval, visual question answering, visual\nrelationship detection, and even autonomous vehicle technology, scene graphs\ncan not only distil complex image information but also correct the bias of\nvisual models using semantic-level relations, which has broad application\nprospects. However, the heavy labour cost of constructing graph annotations may\nhinder the application of PSG in practical scenarios. Inspired by the\nobservation that people usually identify the subject and object first and then\ndetermine the relationship between them, we proposed to decouple the scene\ngraphs generation task into two sub-tasks: 1) an image segmentation task to\npick up the qualified objects. 2) a restricted auto-regressive text generation\ntask to generate the relation between given objects. Therefore, in this work,\nwe introduce image semantic relation generation (ISRG), a simple but effective\nimage-to-text model, which achieved 31 points on the OpenPSG dataset and\noutperforms strong baselines respectively by 16 points (ResNet-50) and 5 points\n(CLIP).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Mingzhe Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evidence > Intuition: Transferability Estimation for Encoder Selection. (arXiv:2210.11255v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11255","description":"<p>With the increase in availability of large pre-trained language models (LMs)\nin Natural Language Processing (NLP), it becomes critical to assess their fit\nfor a specific target task a priori - as fine-tuning the entire space of\navailable LMs is computationally prohibitive and unsustainable. However,\nencoder transferability estimation has received little to no attention in NLP.\nIn this paper, we propose to generate quantitative evidence to predict which\nLM, out of a pool of models, will perform best on a target task without having\nto fine-tune all candidates. We provide a comprehensive study on LM ranking for\n10 NLP tasks spanning the two fundamental problem types of classification and\nstructured prediction. We adopt the state-of-the-art Logarithm of Maximum\nEvidence (LogME) measure from Computer Vision (CV) and find that it positively\ncorrelates with final LM performance in 94% of the setups. In the first study\nof its kind, we further compare transferability measures with the de facto\nstandard of human practitioner ranking, finding that evidence from quantitative\nmetrics is more robust than pure intuition and can help identify unexpected LM\ncandidates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bassignana_E/0/1/0/all/0/1\">Elisa Bassignana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Eberstein_M/0/1/0/all/0/1\">Max M&#xfc;ller-Eberstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers. (arXiv:2210.11265v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11265","description":"<p>This paper presents ReasonFormer, a unified reasoning framework for mirroring\nthe modular and compositional reasoning process of humans in complex decision\nmaking. Inspired by dual-process theory in cognitive science, the\nrepresentation module (automatic thinking) and reasoning modules (controlled\nthinking) are disentangled to capture different levels of cognition. Upon the\ntop of the representation module, the pre-trained reasoning modules are modular\nand expertise in specific and fundamental reasoning skills (e.g., logic, simple\nQA, etc). To mimic the controlled compositional thinking process, different\nreasoning modules are dynamically activated and composed in both parallel and\ncascaded manners to control what reasoning skills are activated and how deep\nthe reasoning process will be reached to solve the current problems. The\nunified reasoning framework solves multiple tasks with a single model,and is\ntrained and inferred in an end-to-end manner. Evaluated on 11 datasets\nrequiring different reasoning skills and complexity, ReasonFormer demonstrates\nsubstantial performance boosts, revealing the compositional reasoning ability.\nFew-shot experiments exhibit better generalization ability by learning to\ncompose pre-trained skills for new tasks with limited data,and decoupling the\nrepresentation module and the reasoning modules. Further analysis shows the\nmodularity of reasoning modules as different tasks activate distinct reasoning\nskills at different reasoning depths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tingting Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chin-Yew Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogUSR: Complex Dialogue Utterance Splitting and Reformulation for Multiple Intent Detection. (arXiv:2210.11279v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11279","description":"<p>While interacting with chatbots, users may elicit multiple intents in a\nsingle dialogue utterance. Instead of training a dedicated multi-intent\ndetection model, we propose DialogUSR, a dialogue utterance splitting and\nreformulation task that first splits multi-intent user query into several\nsingle-intent sub-queries and then recovers all the coreferred and omitted\ninformation in the sub-queries. DialogUSR can serve as a plug-in and\ndomain-agnostic module that empowers the multi-intent detection for the\ndeployed chatbots with minimal efforts. We collect a high-quality naturally\noccurring dataset that covers 23 domains with a multi-step crowd-souring\nprocedure. To benchmark the proposed dataset, we propose multiple action-based\ngenerative models that involve end-to-end and two-stage training, and conduct\nin-depth analyses on the pros and cons of the proposed baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Haoran Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_Z/0/1/0/all/0/1\">Zheng Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zizhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">He Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Binghuai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuemin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts. (arXiv:2210.11292v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11292","description":"<p>Prompt tuning is a parameter-efficient tuning (PETuning) method for utilizing\npre-trained models (PTMs) that simply prepends a soft prompt to the input and\nonly optimizes the prompt to adapt PTMs to downstream tasks. Although it is\nparameter- and deployment-efficient, its performance still lags behind other\nstate-of-the-art PETuning methods. Besides, the training cost of prompt tuning\nis not significantly reduced due to the back-propagation through the entire\nmodel. Through empirical analyses, we shed some light on the lagging\nperformance of prompt tuning and recognize a trade-off between the propagation\ndistance from label signals to the inserted prompt and the influence of the\nprompt on model outputs. Further, we present Late Prompt Tuning (LPT) that\ninserts a late prompt into an intermediate layer of the PTM instead of the\ninput layer or all layers. The late prompt is obtained by a neural prompt\ngenerator conditioned on the hidden states before the prompt insertion layer\nand therefore is instance-dependent. Through extensive experimental results\nacross various tasks and PTMs, we show that LPT can achieve competitive\nperformance to full model tuning and other PETuning methods under both\nfull-data and few-shot scenarios while possessing faster training speed and\nlower memory cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The University of Edinburgh's Submission to the WMT22 Code-Mixing Shared Task (MixMT). (arXiv:2210.11309v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11309","description":"<p>The University of Edinburgh participated in the WMT22 shared task on\ncode-mixed translation. This consists of two subtasks: i) generating code-mixed\nHindi/English (Hinglish) text generation from parallel Hindi and English\nsentences and ii) machine translation from Hinglish to English. As both\nsubtasks are considered low-resource, we focused our efforts on careful data\ngeneration and curation, especially the use of backtranslation from monolingual\nresources. For subtask 1 we explored the effects of constrained decoding on\nEnglish and transliterated subwords in order to produce Hinglish. For subtask\n2, we investigated different pretraining techniques, namely comparing simple\ninitialisation from existing machine translation models and aligned\naugmentation. For both subtasks, we found that our baseline systems worked\nbest. Our systems for both subtasks were one of the overall top-performing\nsubmissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirefu_F/0/1/0/all/0/1\">Faheem Kirefu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_V/0/1/0/all/0/1\">Vivek Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pinzhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burchell_L/0/1/0/all/0/1\">Laurie Burchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Efficient Strategies for Expanding Hate Speech Detection into Under-Resourced Languages. (arXiv:2210.11359v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11359","description":"<p>Hate speech is a global phenomenon, but most hate speech datasets so far\nfocus on English-language content. This hinders the development of more\neffective hate speech detection models in hundreds of languages spoken by\nbillions across the world. More data is needed, but annotating hateful content\nis expensive, time-consuming and potentially harmful to annotators. To mitigate\nthese issues, we explore data-efficient strategies for expanding hate speech\ndetection into under-resourced languages. In a series of experiments with mono-\nand multilingual models across five non-English languages, we find that 1) a\nsmall amount of target-language fine-tuning data is needed to achieve strong\nperformance, 2) the benefits of using more such data decrease exponentially,\nand 3) initial fine-tuning on readily-available English data can partially\nsubstitute target-language data and improve model generalisability. Based on\nthese findings, we formulate actionable recommendations for hate speech\ndetection in low-resource language settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nozza_D/0/1/0/all/0/1\">Debora Nozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meeting Decision Tracker: Making Meeting Minutes with De-Contextualized Utterances. (arXiv:2210.11374v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11374","description":"<p>Meetings are a universal process to make decisions in business and project\ncollaboration. The capability to automatically itemize the decisions in daily\nmeetings allows for extensive tracking of past discussions. To that end, we\ndeveloped Meeting Decision Tracker, a prototype system to construct decision\nitems comprising decision utterance detector (DUD) and decision utterance\nrewriter (DUR). We show that DUR makes a sizable contribution to improving the\nuser experience by dealing with utterance collapse in natural conversation. An\nintroduction video of our system is also available at\nhttps://youtu.be/TG1pJJo0Iqo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_S/0/1/0/all/0/1\">Shumpei Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_P/0/1/0/all/0/1\">Pham Viet Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tsungwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Tien Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transcending Scaling Laws with 0.1% Extra Compute. (arXiv:2210.11399v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11399","description":"<p>Scaling language models improves performance but comes with significant\ncomputational costs. This paper proposes UL2R, a method that substantially\nimproves existing language models and their scaling curves with a relatively\ntiny amount of extra compute. The key idea is to continue training a\nstate-of-the-art large language model (e.g., PaLM) on a few more steps with\nUL2's mixture-of-denoiser objective. We show that, with almost negligible extra\ncomputational costs and no new sources of data, we are able to substantially\nimprove the scaling properties of large language models on downstream metrics.\nIn this paper, we continue training PaLM with UL2R, introducing a new set of\nmodels at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B\nscale, we show an approximately 2x computational savings rate where U-PaLM\nachieves the same performance as the final PaLM 540B model at around half its\ncomputational budget (i.e., saving $\\sim$4.4 million TPUv4 hours). We further\nshow that this improved scaling curve leads to 'emergent abilities' on\nchallenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM\non some tasks or demonstrates better quality at much smaller scale (62B as\nopposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many\nfew-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question\nanswering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual\ntasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide\nqualitative examples showing the new capabilities of U-PaLM for single and\nmulti-span infilling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1\">David R. So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1\">Siamak Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huaixiu Steven Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jinfeng Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_S/0/1/0/all/0/1\">Slav Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Instruction-Finetuned Language Models. (arXiv:2210.11416v1 [cs.LG])","link":"http://arxiv.org/abs/2210.11416","description":"<p>Finetuning language models on a collection of datasets phrased as\ninstructions has been shown to improve model performance and generalization to\nunseen tasks. In this paper we explore instruction finetuning with a particular\nfocus on (1) scaling the number of tasks, (2) scaling the model size, and (3)\nfinetuning on chain-of-thought data. We find that instruction finetuning with\nthe above aspects dramatically improves performance on a variety of model\nclasses (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and\nevaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For\ninstance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM\n540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves\nstate-of-the-art performance on several benchmarks, such as 75.2% on five-shot\nMMLU. We also publicly release Flan-T5 checkpoints, which achieve strong\nfew-shot performance even compared to much larger models, such as PaLM 62B.\nOverall, instruction finetuning is a general method for improving the\nperformance and usability of pretrained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Eric Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1\">Siddhartha Brahma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhuyun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzgun_M/0/1/0/all/0/1\">Mirac Suzgun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1\">Gaurav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_S/0/1/0/all/0/1\">Slav Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed H. Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1\">Jeff Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_J/0/1/0/all/0/1\">Jacob Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario. (arXiv:2210.11431v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11431","description":"<p>People can acquire knowledge in an unsupervised manner by reading, and\ncompose the knowledge to make novel combinations. In this paper, we investigate\nwhether pretrained language models can perform compositional generalization in\na realistic setting: recipe generation. We design the counterfactual recipe\ngeneration task, which asks models to modify a base recipe according to the\nchange of an ingredient. This task requires compositional generalization at two\nlevels: the surface level of incorporating the new ingredient into the base\nrecipe, and the deeper level of adjusting actions related to the changing\ningredient. We collect a large-scale recipe dataset in Chinese for models to\nlearn culinary knowledge, and a subset of action-level fine-grained annotations\nfor evaluation. We finetune pretrained language models on the recipe corpus,\nand use unsupervised counterfactual generation methods to generate modified\nrecipes. Results show that existing models have difficulties in modifying the\ningredients while preserving the original text style, and often miss actions\nthat need to be adjusted. Although pretrained language models can generate\nfluent recipe texts, they fail to truly learn and use the culinary knowledge in\na compositional way. Code and data are available at\nhttps://github.com/xxxiaol/counterfactual-recipe-generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yansong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jizhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chengang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Choose Your Lenses: Flaws in Gender Bias Evaluation. (arXiv:2210.11471v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11471","description":"<p>Considerable efforts to measure and mitigate gender bias in recent years have\nled to the introduction of an abundance of tasks, datasets, and metrics used in\nthis vein. In this position paper, we assess the current paradigm of gender\nbias evaluation and identify several flaws in it. First, we highlight the\nimportance of extrinsic bias metrics that measure how a model's performance on\nsome task is affected by gender, as opposed to intrinsic evaluations of model\nrepresentations, which are less strongly connected to specific harms to people\ninteracting with systems. We find that only a few extrinsic metrics are\nmeasured in most studies, although more can be measured. Second, we find that\ndatasets and metrics are often coupled, and discuss how their coupling hinders\nthe ability to obtain reliable conclusions, and how one may decouple them. We\nthen investigate how the choice of the dataset and its composition, as well as\nthe choice of the metric, affect bias measurement, finding significant\nvariations across each of them. Finally, we propose several guidelines for more\nreliable gender bias evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orgad_H/0/1/0/all/0/1\">Hadas Orgad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogue-adaptive Language Model Pre-training From Quality Estimation. (arXiv:2009.04984v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.04984","description":"<p>Pre-trained language models (PrLMs) have achieved great success on a wide\nrange of natural language processing tasks by virtue of the universal language\nrepresentation ability obtained by self-supervised learning on a large corpus.\nThese models are pre-trained on standard plain texts with general language\nmodel (LM) training objectives, which would be insufficient to model\ndialogue-exclusive attributes like specificity and informativeness reflected in\nthese tasks that are not explicitly captured by the pre-trained universal\nlanguage representations. In this work, we propose dialogue-adaptive\npre-training objectives (DAPO) derived from quality estimation to simulate\ndialogue-specific features, namely coherence, specificity, and informativeness.\nAs the foundation for model pre-training, we synthesize a new dialogue corpus\nand build our training set with two unsupervised methods: 1) coherence-oriented\ncontext corruption, including utterance ordering, insertion, and replacement,\nto help the model capture the coherence inside the dialogue contexts; and 2)\nspecificity-oriented automatic rescoring, which encourages the model to measure\nthe quality of the synthesized data for dialogue-adaptive pre-training by\nconsidering specificity and informativeness. Experimental results on widely\nused open-domain response selection and quality estimation benchmarks show that\nDAPO significantly improves the baseline models and achieves state-of-the-art\nperformance on the MuTual leaderboard, verifying the effectiveness of\nestimating quality evaluation factors into pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rewriting Meaningful Sentences via Conditional BERT Sampling and an application on fooling text classifiers. (arXiv:2010.11869v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.11869","description":"<p>Most adversarial attack methods that are designed to deceive a text\nclassifier change the text classifier's prediction by modifying a few words or\ncharacters. Few try to attack classifiers by rewriting a whole sentence, due to\nthe difficulties inherent in sentence-level rephrasing as well as the problem\nof setting the criteria for legitimate rewriting.\n</p>\n<p>In this paper, we explore the problem of creating adversarial examples with\nsentence-level rewriting. We design a new sampling method, named\nParaphraseSampler, to efficiently rewrite the original sentence in multiple\nways. Then we propose a new criteria for modification, called a sentence-level\nthreaten model. This criteria allows for both word- and sentence-level changes,\nand can be adjusted independently in two dimensions: semantic similarity and\ngrammatical quality. Experimental results show that many of these rewritten\nsentences are misclassified by the classifier. On all 6 datasets, our\nParaphraseSampler achieves a better attack success rate than our baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_I/0/1/0/all/0/1\">Ivan Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1\">Kalyan Veeramachaneni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R&R: Metric-guided Adversarial Sentence Generation. (arXiv:2104.08453v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08453","description":"<p>Adversarial examples are helpful for analyzing and improving the robustness\nof text classifiers. Generating high-quality adversarial examples is a\nchallenging task as it requires generating fluent adversarial sentences that\nare semantically similar to the original sentences and preserve the original\nlabels, while causing the classifier to misclassify them. Existing methods\nprioritize misclassification by maximizing each perturbation's effectiveness at\nmisleading a text classifier; thus, the generated adversarial examples fall\nshort in terms of fluency and similarity. In this paper, we propose a rewrite\nand rollback (R&amp;R) framework for adversarial attack. It improves the quality of\nadversarial examples by optimizing a critique score which combines the fluency,\nsimilarity, and misclassification metrics. R&amp;R generates high-quality\nadversarial examples by allowing exploration of perturbations that do not have\nimmediate impact on the misclassification metric but can improve fluency and\nsimilarity metrics. We evaluate our method on 5 representative datasets and 3\nclassifier architectures. Our method outperforms current state-of-the-art in\nattack success rate by +16.2%, +12.8%, and +14.0% on the classifiers\nrespectively. Code is available at https://github.com/DAI-Lab/fibber\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuesta_Infante_A/0/1/0/all/0/1\">Alfredo Cuesta-Infante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berti_Equille_L/0/1/0/all/0/1\">Laure Berti-Equille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1\">Kalyan Veeramachaneni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Neural Networks for Natural Language Processing: A Survey. (arXiv:2106.06090v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06090","description":"<p>Deep learning has become the dominant approach in coping with various tasks\nin Natural LanguageProcessing (NLP). Although text inputs are typically\nrepresented as a sequence of tokens, there isa rich variety of NLP problems\nthat can be best expressed with a graph structure. As a result, thereis a surge\nof interests in developing new deep learning techniques on graphs for a large\nnumberof NLP tasks. In this survey, we present a comprehensive overview onGraph\nNeural Networks(GNNs) for Natural Language Processing. We propose a new\ntaxonomy of GNNs for NLP, whichsystematically organizes existing research of\nGNNs for NLP along three axes: graph construction,graph representation\nlearning, and graph based encoder-decoder models. We further introducea large\nnumber of NLP applications that are exploiting the power of GNNs and summarize\nthecorresponding benchmark datasets, evaluation metrics, and open-source codes.\nFinally, we discussvarious outstanding challenges for making the full use of\nGNNs for NLP as well as future researchdirections. To the best of our\nknowledge, this is the first comprehensive overview of Graph NeuralNetworks for\nNatural Language Processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1\">Kai Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaojie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hanning Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shucheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Attention for Automatic Chest X-ray Report Generation. (arXiv:2106.06965v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06965","description":"<p>Recently, chest X-ray report generation, which aims to automatically generate\ndescriptions of given chest X-ray images, has received growing research\ninterests. The key challenge of chest X-ray report generation is to accurately\ncapture and describe the abnormal regions. In most cases, the normal regions\ndominate the entire chest X-ray image, and the corresponding descriptions of\nthese normal regions dominate the final report. Due to such data bias,\nlearning-based models may fail to attend to abnormal regions. In this work, to\neffectively capture and describe abnormal regions, we propose the Contrastive\nAttention (CA) model. Instead of solely focusing on the current input image,\nthe CA model compares the current input image with normal images to distill the\ncontrastive information. The acquired contrastive information can better\nrepresent the visual features of abnormal regions. According to the experiments\non the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into\nseveral existing models can boost their performance across most metrics. In\naddition, according to the analysis, the CA model can help existing models\nbetter attend to the abnormal regions and provide more accurate descriptions\nwhich are crucial for an interpretable diagnosis. Specifically, we achieve the\nstate-of-the-art results on the two public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuewei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Changchang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Exploration of Pre-training Language Models. (arXiv:2106.11483v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.11483","description":"<p>Recently, the development of pre-trained language models has brought natural\nlanguage processing (NLP) tasks to the new state-of-the-art. In this paper we\nexplore the efficiency of various pre-trained language models. We pre-train a\nlist of transformer-based models with the same amount of text and the same\ntraining steps. The experimental results shows that the most improvement upon\nthe origin BERT is adding the RNN-layer to capture more contextual information\nfor short text understanding. But there are no remarkable improvement for short\ntext understanding for similar BERT structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Jargon: Combining Extraction and Generation for Definition Modeling. (arXiv:2111.07267v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07267","description":"<p>Can machines know what twin prime is? From the composition of this phrase,\nmachines may guess twin prime is a certain kind of prime, but it is still\ndifficult to deduce exactly what twin stands for without additional knowledge.\nHere, twin prime is a jargon - a specialized term used by experts in a\nparticular field. Explaining jargon is challenging since it usually requires\ndomain knowledge to understand. Recently, there is an increasing interest in\nextracting and generating definitions of words automatically. However, existing\napproaches, either extraction or generation, perform poorly on jargon. In this\npaper, we propose to combine extraction and generation for jargon definition\nmodeling: first extract self- and correlative definitional information of\ntarget jargon from the Web and then generate the final definitions by\nincorporating the extracted definitional information. Our framework is\nremarkably simple but effective: experiments demonstrate our method can\ngenerate high-quality definitions for jargon and outperform state-of-the-art\nmodels significantly, e.g., BLEU score from 8.76 to 22.66 and human-annotated\nscore from 2.34 to 4.04.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1\">Hanyin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwu_W/0/1/0/all/0/1\">Wen-mei Hwu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding Arithmetic of Multimodal Queries for Image Retrieval. (arXiv:2112.03162v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03162","description":"<p>Latent text representations exhibit geometric regularities, such as the\nfamous analogy: queen is to king what woman is to man. Such structured semantic\nrelations were not demonstrated on image representations. Recent works aiming\nat bridging this semantic gap embed images and text into a multimodal space,\nenabling the transfer of text-defined transformations to the image modality. We\nintroduce the SIMAT dataset to evaluate the task of Image Retrieval with\nMultimodal queries. SIMAT contains 6k images and 18k textual transformation\nqueries that aim at either replacing scene elements or changing pairwise\nrelationships between scene elements. The goal is to retrieve an image\nconsistent with the (source image, text transformation) query. We use an\nimage/text matching oracle (OSCAR) to assess whether the image transformation\nis successful. The SIMAT dataset will be publicly available. We use SIMAT to\nevaluate the geometric properties of multimodal embedding spaces trained with\nan image/text matching objective, like CLIP. We show that vanilla CLIP\nembeddings are not very well suited to transform images with delta vectors, but\nthat a simple finetuning on the COCO dataset can bring dramatic improvements.\nWe also study whether it is beneficial to leverage pretrained universal\nsentence encoders (FastText, LASER and LaBSE).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Couairon_G/0/1/0/all/0/1\">Guillaume Couairon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Roof-Transformer: Divided and Joined Understanding with Knowledge Enhancement. (arXiv:2112.06736v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06736","description":"<p>Recent work on enhancing BERT-based language representation models with\nknowledge graphs (KGs) and knowledge bases (KBs) has yielded promising results\non multiple NLP tasks. State-of-the-art approaches typically integrate the\noriginal input sentences with KG triples and feed the combined representation\ninto a BERT model. However, as the sequence length of a BERT model is limited,\nsuch a framework supports little knowledge other than the original input\nsentences and is thus forced to discard some knowledge. This problem is\nespecially severe for downstream tasks for which the input is a long paragraph\nor even a document, such as QA or reading comprehension tasks. We address this\nproblem with Roof-Transformer, a model with two underlying BERTs and a fusion\nlayer on top. One underlying BERT encodes the knowledge resources and the other\none encodes the original input sentences, and the fusion layer integrates the\ntwo resultant encodings. Experimental results on a QA task and the GLUE\nbenchmark attest the effectiveness of the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wei-Lin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Cheng-En Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wei-Yun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Yes-Yes-Yes: Proactive Data Collection for ACL Rolling Review and Beyond. (arXiv:2201.11443v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11443","description":"<p>The shift towards publicly available text sources has enabled language\nprocessing at unprecedented scale, yet leaves under-serviced the domains where\npublic and openly licensed data is scarce. Proactively collecting text data for\nresearch is a viable strategy to address this scarcity, but lacks systematic\nmethodology taking into account the many ethical, legal and\nconfidentiality-related aspects of data collection. Our work presents a case\nstudy on proactive data collection in peer review -- a challenging and\nunder-resourced NLP domain. We outline ethical and legal desiderata for\nproactive data collection and introduce \"Yes-Yes-Yes\", the first donation-based\npeer reviewing data collection workflow that meets these requirements. We\nreport on the implementation of Yes-Yes-Yes at ACL Rolling Review and\nempirically study the implications of proactive data collection for the dataset\nsize and the biases induced by the donation behavior on the peer reviewing\nplatform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dycke_N/0/1/0/all/0/1\">Nils Dycke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1\">Ilia Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiFSMN: Binary Neural Network for Keyword Spotting. (arXiv:2202.06483v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06483","description":"<p>The deep neural networks, such as the Deep-FSMN, have been widely studied for\nkeyword spotting (KWS) applications. However, computational resources for these\nnetworks are significantly constrained since they usually run on-call on edge\ndevices. In this paper, we present BiFSMN, an accurate and extreme-efficient\nbinary neural network for KWS. We first construct a High-frequency Enhancement\nDistillation scheme for the binarization-aware training, which emphasizes the\nhigh-frequency information from the full-precision network's representation\nthat is more crucial for the optimization of the binarized network. Then, to\nallow the instant and adaptive accuracy-efficiency trade-offs at runtime, we\nalso propose a Thinnable Binarization Architecture to further liberate the\nacceleration potential of the binarized network from the topology perspective.\nMoreover, we implement a Fast Bitwise Computation Kernel for BiFSMN on ARMv8\ndevices which fully utilizes registers and increases instruction throughput to\npush the limit of deployment efficiency. Extensive experiments show that BiFSMN\noutperforms existing binarization methods by convincing margins on various\ndatasets and is even comparable with the full-precision counterpart (e.g., less\nthan 3% drop on Speech Commands V1-12). We highlight that benefiting from the\nthinnable architecture and the optimized 1-bit implementation, BiFSMN can\nachieve an impressive 22.3x speedup and 15.5x storage-saving on real-world edge\nhardware. Our code is released at https://github.com/htqin/BiFSMN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Haotong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xudong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\rm{C {\\small IS}}^2$: A Simplified Commonsense Inference Evaluation for Story Prose. (arXiv:2202.07880v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07880","description":"<p>Transformers have been showing near-human performance on a variety of tasks,\nbut they are not without their limitations. We discuss the issue of conflating\nresults of transformers that are instructed to do multiple tasks\nsimultaneously. In particular, we focus on the domain of commonsense reasoning\nwithin story prose, which we call contextual commonsense inference (CCI). We\nlook at the GLUCOSE (Mostafazadeh et al. 2020) dataset and task for predicting\nimplicit commonsense inferences between story sentences. Since the GLUCOSE task\nsimultaneously generates sentences and predicts the CCI relation, there is a\nconflation in the results. Is the model really measuring CCI or is its ability\nto generate grammatical text carrying the results? In this paper, we introduce\nthe task contextual commonsense inference in sentence selection ($\\rm{C {\\small\nIS}}^2$), a simplified task that avoids conflation by eliminating language\ngeneration altogether. Our findings emphasize the necessity of future work to\ndisentangle language generation from the desired NLP tasks at hand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bryan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Lara J. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?. (arXiv:2202.12837v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12837","description":"<p>Large language models (LMs) are able to in-context learn -- perform a new\ntask via inference alone by conditioning on a few input-label pairs\n(demonstrations) and making predictions for new inputs. However, there has been\nlittle understanding of how the model learns and which aspects of the\ndemonstrations contribute to end task performance. In this paper, we show that\nground truth demonstrations are in fact not required -- randomly replacing\nlabels in the demonstrations barely hurts performance on a range of\nclassification and multi-choce tasks, consistently over 12 different models\nincluding GPT-3. Instead, we find that other aspects of the demonstrations are\nthe key drivers of end task performance, including the fact that they provide a\nfew examples of (1) the label space, (2) the distribution of the input text,\nand (3) the overall format of the sequence. Together, our analysis provides a\nnew way of understanding how and why in-context learning works, while opening\nup new questions about how much can be learned from large language models\nthrough inference alone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xinxi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Conformer Based Acoustic Model for Robust Automatic Speech Recognition. (arXiv:2203.00725v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.00725","description":"<p>This study addresses robust automatic speech recognition (ASR) by introducing\na Conformer-based acoustic model. The proposed model builds on the wide\nresidual bi-directional long short-term memory network (WRBN) with\nutterance-wise dropout and iterative speaker adaptation, but employs a\nConformer encoder instead of the recurrent network. The Conformer encoder uses\na convolution-augmented attention mechanism for acoustic modeling. The proposed\nsystem is evaluated on the monaural ASR task of the CHiME-4 corpus. Coupled\nwith utterance-wise normalization and speaker adaptation, our model achieves\n$6.25\\%$ word error rate, which outperforms WRBN by $8.4\\%$ relatively. In\naddition, the proposed Conformer-based model is $18.3\\%$ smaller in model size\nand reduces total training time by $79.6\\%$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yufeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">DeLiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning. (arXiv:2203.02053v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.02053","description":"<p>We present modality gap, an intriguing geometric phenomenon of the\nrepresentation space of multi-modal models. Specifically, we show that\ndifferent data modalities (e.g. images and text) are embedded at arm's length\nin their shared representation in multi-modal models such as CLIP. Our\nsystematic analysis demonstrates that this gap is caused by a combination of\nmodel initialization and contrastive learning optimization. In model\ninitialization, we show empirically and theoretically that the representation\nof a common deep neural network is restricted to a narrow cone. As a\nconsequence, in a multi-modal model with two encoders, the representations of\nthe two modalities are clearly apart when the model is initialized. During\noptimization, contrastive learning keeps the different modalities separate by a\ncertain distance, which is influenced by the temperature parameter in the loss\nfunction. Our experiments further demonstrate that varying the modality gap\ndistance has a significant impact in improving the model's downstream zero-shot\nclassification performance and fairness. Our code and data are available at\nhttps://modalitygap.readthedocs.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Weixin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yongchan Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Serena Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling. (arXiv:2204.08152v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08152","description":"<p>Multi-turn dialogue modeling as a challenging branch of natural language\nunderstanding (NLU), aims to build representations for machines to understand\nhuman dialogues, which provides a solid foundation for multiple downstream\ntasks. Recent studies of dialogue modeling commonly employ pre-trained language\nmodels (PrLMs) to encode the dialogue history as successive tokens, which is\ninsufficient in capturing the temporal characteristics of dialogues. Therefore,\nwe propose Bidirectional Information Decoupling Network (BiDeN) as a universal\ndialogue encoder, which explicitly incorporates both the past and future\ncontexts and can be generalized to a wide range of dialogue-related tasks.\nExperimental results on datasets of different downstream tasks demonstrate the\nuniversality and effectiveness of our BiDeN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring Implicit Relations in Complex Questions with Language Models. (arXiv:2204.13778v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13778","description":"<p>A prominent challenge for modern language understanding systems is the\nability to answer implicit reasoning questions, where the required reasoning\nsteps for answering the question are not mentioned in the text explicitly. In\nthis work, we investigate why current models struggle with implicit reasoning\nquestion answering (QA) tasks, by decoupling inference of reasoning steps from\ntheir execution. We define a new task of implicit relation inference and\nconstruct a benchmark, IMPLICITRELATIONS, where given a question, a model\nshould output a list of concept-relation pairs, where the relations describe\nthe implicit reasoning steps required for answering the question. Using\nIMPLICITRELATIONS, we evaluate models from the GPT-3 family and find that,\nwhile these models struggle on the implicit reasoning QA task, they often\nsucceed at inferring implicit relations. This suggests that the challenge in\nimplicit reasoning questions does not stem from the need to plan a reasoning\nstrategy alone, but to do it while also retrieving and reasoning over relevant\ninformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katz_U/0/1/0/all/0/1\">Uri Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CiteSum: Citation Text-guided Scientific Extreme Summarization and Domain Adaptation with Limited Supervision. (arXiv:2205.06207v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.06207","description":"<p>Scientific extreme summarization (TLDR) aims to form ultra-short summaries of\nscientific papers. Previous efforts on curating scientific TLDR datasets failed\nto scale up due to the heavy human annotation and domain expertise required. In\nthis paper, we propose a simple yet effective approach to automatically\nextracting TLDR summaries for scientific papers from their citation texts.\nBased on the proposed approach, we create a new benchmark CiteSum without human\nannotation, which is around 30 times larger than the previous human-curated\ndataset SciTLDR. We conduct a comprehensive analysis of CiteSum, examining its\ndata characteristics and establishing strong baselines. We further demonstrate\nthe usefulness of CiteSum by adapting models pre-trained on CiteSum (named\nCITES) to new tasks and domains with limited supervision. For scientific\nextreme summarization, CITES outperforms most fully-supervised methods on\nSciTLDR without any fine-tuning and obtains state-of-the-art results with only\n128 examples. For news extreme summarization, CITES achieves significant gains\non XSum over its base model (not pre-trained on CiteSum), e.g., +7.2 ROUGE-1\nzero-shot performance and state-of-the-art few-shot performance. For news\nheadline generation, CITES performs the best among unsupervised and zero-shot\nmethods on Gigaword. Our dataset and code can be found at\nhttps://github.com/morningmoni/CiteSum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training Transformer Models with Sentence-Level Objectives for Answer Sentence Selection. (arXiv:2205.10455v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10455","description":"<p>An important task for designing QA systems is answer sentence selection\n(AS2): selecting the sentence containing (or constituting) the answer to a\nquestion from a set of retrieved relevant documents. In this paper, we propose\nthree novel sentence-level transformer pre-training objectives that incorporate\nparagraph-level semantics within and across documents, to improve the\nperformance of transformers for AS2, and mitigate the requirement of large\nlabeled datasets. Specifically, the model is tasked to predict whether: (i) two\nsentences are extracted from the same paragraph, (ii) a given sentence is\nextracted from a given paragraph, and (iii) two paragraphs are extracted from\nthe same document. Our experiments on three public and one industrial AS2\ndatasets demonstrate the empirical superiority of our pre-trained transformers\nover baseline models such as RoBERTa and ELECTRA for AS2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liello_L/0/1/0/all/0/1\">Luca Di Liello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Siddhant Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEER: Descriptive Knowledge Graph for Explaining Entity Relationships. (arXiv:2205.10479v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10479","description":"<p>We propose DEER (Descriptive Knowledge Graph for Explaining Entity\nRelationships) - an open and informative form of modeling entity relationships.\nIn DEER, relationships between entities are represented by free-text relation\ndescriptions. For instance, the relationship between entities of machine\nlearning and algorithm can be represented as ``Machine learning explores the\nstudy and construction of algorithms that can learn from and make predictions\non data.'' To construct DEER, we propose a self-supervised learning method to\nextract relation descriptions with the analysis of dependency patterns and\ngenerate relation descriptions with a transformer-based relation description\nsynthesizing model, where no human labeling is required. Experiments\ndemonstrate that our system can extract and generate high-quality relation\ndescriptions for explaining entity relationships. The results suggest that we\ncan build an open and informative knowledge graph without human annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kerui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwu_W/0/1/0/all/0/1\">Wen-mei Hwu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relphormer: Relational Graph Transformer for Knowledge Graph Representations. (arXiv:2205.10852v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10852","description":"<p>Transformers have achieved remarkable performance in widespread fields,\nincluding natural language processing, computer vision and graph mining.\nHowever, vanilla Transformer architectures have not yielded promising\nimprovements in the Knowledge Graph (KG) representations, where the\ntranslational distance paradigm dominates this area. Note that vanilla\nTransformer architectures struggle to capture the intrinsically heterogeneous\nsemantic and structural information of knowledge graphs. To this end, we\npropose a new variant of Transformer for knowledge graph representations dubbed\nRelphormer. Specifically, we introduce Triple2Seq which can dynamically sample\ncontextualized sub-graph sequences as the input to alleviate the heterogeneity\nissue. We propose a novel structure-enhanced self-attention mechanism to encode\nthe relational information and keep the globally semantic information among\nsub-graphs. Moreover, we propose masked knowledge modeling as a new paradigm\nfor knowledge graph representation learning. We apply Relphormer to three\ntasks, namely, knowledge graph completion, KG-based question answering and\nKG-based recommendation for evaluation. Experimental results show that\nRelphormer can obtain better performance on benchmark datasets compared with\nbaselines. Code is available in https://github.com/zjunlp/Relphormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models. (arXiv:2205.11432v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11432","description":"<p>Current Natural Language Inference (NLI) models achieve impressive results,\nsometimes outperforming humans when evaluating on in-distribution test sets.\nHowever, as these models are known to learn from annotation artefacts and\ndataset biases, it is unclear to what extent the models are learning the task\nof NLI instead of learning from shallow heuristics in their training data. We\naddress this issue by introducing a logical reasoning framework for NLI,\ncreating highly transparent model decisions that are based on logical rules.\nUnlike prior work, we show that improved interpretability can be achieved\nwithout decreasing the predictive accuracy. We almost fully retain performance\non SNLI, while also identifying the exact hypothesis spans that are responsible\nfor each model prediction. Using the e-SNLI human explanations, we verify that\nour model makes sensible decisions at a span level, despite not using any span\nlabels during training. We can further improve model performance and span-level\ndecisions by using the e-SNLI explanations during training. Finally, our model\nis more robust in a reduced data setting. When training with only 1,000\nexamples, out-of-distribution performance improves on the MNLI matched and\nmismatched validation sets by 13% and 16% relative to the baseline. Training\nwith fewer observations yields further improvements, both in-distribution and\nout-of-distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stacey_J/0/1/0/all/0/1\">Joe Stacey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubossarsky_H/0/1/0/all/0/1\">Haim Dubossarsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Garden-Path Traversal in GPT-2. (arXiv:2205.12302v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12302","description":"<p>In recent years, large-scale transformer decoders such as the GPT-x family of\nmodels have become increasingly popular. Studies examining the behavior of\nthese models tend to focus only on the output of the language modeling head and\navoid analysis of the internal states of the transformer decoder. In this\nstudy, we present a collection of methods to analyze the hidden states of GPT-2\nand use the model's navigation of garden path sentences as a case study. To\nenable this, we compile the largest currently available dataset of garden path\nsentences. We show that Manhattan distances and cosine similarities provide\nmore reliable insights compared to established surprisal methods that analyze\nnext-token probabilities computed by a language modeling head. Using these\nmethods, we find that negating tokens have minimal impacts on the model's\nrepresentations for unambiguous forms of sentences with ambiguity solely over\nwhat the object of a verb is, but have a more substantial impact of\nrepresentations for unambiguous sentences whose ambiguity would stem from the\nvoice of a verb. Further, we find that analyzing the decoder model's hidden\nstates reveals periods of ambiguity that might conclude in a garden path effect\nbut happen not to, whereas surprisal analyses routinely miss this detail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jurayj_W/0/1/0/all/0/1\">William Jurayj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudman_W/0/1/0/all/0/1\">William Rudman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Large Pre-Trained Language Models Leaking Your Personal Information?. (arXiv:2205.12628v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12628","description":"<p>Are Large Pre-Trained Language Models Leaking Your Personal Information? In\nthis paper, we analyze whether Pre-Trained Language Models (PLMs) are prone to\nleaking personal information. Specifically, we query PLMs for email addresses\nwith contexts of the email address or prompts containing the owner's name. We\nfind that PLMs do leak personal information due to memorization. However, since\nthe models are weak at association, the risk of specific personal information\nbeing extracted by attackers is low. We hope this work could help the community\nto better understand the privacy risk of PLMs and bring new insights to make\nPLMs safe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1\">Hanyin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Adversarial Attack on Vision-Language Pre-training Models. (arXiv:2206.09391v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.09391","description":"<p>While vision-language pre-training model (VLP) has shown revolutionary\nimprovements on various vision-language (V+L) tasks, the studies regarding its\nadversarial robustness remain largely unexplored. This paper studied the\nadversarial attack on popular VLP models and V+L tasks. First, we analyzed the\nperformance of adversarial attacks under different settings. By examining the\ninfluence of different perturbed objects and attack targets, we concluded some\nkey observations as guidance on both designing strong multimodal adversarial\nattack and constructing robust VLP models. Second, we proposed a novel\nmultimodal attack method on the VLP models called Collaborative Multimodal\nAdversarial Attack (Co-Attack), which collectively carries out the attacks on\nthe image modality and the text modality. Experimental results demonstrated\nthat the proposed method achieves improved attack performances on different V+L\ndownstream tasks and VLP models. The analysis observations and novel attack\nmethod hopefully provide new understanding into the adversarial robustness of\nVLP models, so as to contribute their safe and reliable deployment in more\nreal-world scenarios. Code is available at\nhttps://github.com/adversarial-for-goodness/Co-Attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1\">Qi Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1\">Jitao Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Competence-based Multimodal Curriculum Learning for Medical Report Generation. (arXiv:2206.14579v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14579","description":"<p>Medical report generation task, which targets to produce long and coherent\ndescriptions of medical images, has attracted growing research interests\nrecently. Different from the general image captioning tasks, medical report\ngeneration is more challenging for data-driven neural models. This is mainly\ndue to 1) the serious data bias and 2) the limited medical data. To alleviate\nthe data bias and make best use of available data, we propose a\nCompetence-based Multimodal Curriculum Learning framework (CMCL). Specifically,\nCMCL simulates the learning process of radiologists and optimizes the model in\na step by step manner. Firstly, CMCL estimates the difficulty of each training\ninstance and evaluates the competence of current model; Secondly, CMCL selects\nthe most suitable batch of training instances considering current model\ncompetence. By iterating above two steps, CMCL can gradually improve the\nmodel's performance. The experiments on the public IU-Xray and MIMIC-CXR\ndatasets show that CMCL can be incorporated into existing models to improve\ntheir performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuewei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepGen: Diverse Search Ad Generation and Real-Time Customization. (arXiv:2208.03438v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.03438","description":"<p>We present DeepGen, a system deployed at web scale for automatically creating\nsponsored search advertisements (ads) for BingAds customers. We leverage\nstate-of-the-art natural language generation (NLG) models to generate fluent\nads from advertiser's web pages in an abstractive fashion and solve practical\nissues such as factuality and inference speed. In addition, our system creates\na customized ad in real-time in response to the user's search query, therefore\nhighlighting different aspects of the same product based on what the user is\nlooking for. To achieve this, our system generates a diverse choice of smaller\npieces of the ad ahead of time and, at query time, selects the most relevant\nones to be stitched into a complete ad. We improve generation diversity by\ntraining a controllable NLG model to generate multiple ads for the same web\npage highlighting different selling points. Our system design further improves\ndiversity horizontally by first running an ensemble of generation models\ntrained with different objectives and then using a diversity sampling algorithm\nto pick a diverse subset of generation results for online selection.\nExperimental results show the effectiveness of our proposed system design. Our\nsystem is currently deployed in production, serving ${\\sim}4\\%$ of global ads\nserved in Bing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golobokov_K/0/1/0/all/0/1\">Konstantin Golobokov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Junyi Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_V/0/1/0/all/0/1\">Victor Ye Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_M/0/1/0/all/0/1\">Mandy Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_B/0/1/0/all/0/1\">Bingyu Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yulan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TexPrax: A Messaging Application for Ethical, Real-time Data Collection and Annotation. (arXiv:2208.07846v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.07846","description":"<p>Collecting and annotating task-oriented dialog data is difficult, especially\nfor highly specific domains that require expert knowledge. At the same time,\ninformal communication channels such as instant messengers are increasingly\nbeing used at work. This has led to a lot of work-relevant information that is\ndisseminated through those channels and needs to be post-processed manually by\nthe employees. To alleviate this problem, we present TexPrax, a messaging\nsystem to collect and annotate problems, causes, and solutions that occur in\nwork-related chats. TexPrax uses a chatbot to directly engage the employees to\nprovide lightweight annotations on their conversation and ease their\ndocumentation work. To comply with data privacy and security regulations, we\nuse an end-to-end message encryption and give our users full control over their\ndata which has various advantages over conventional annotation tools. We\nevaluate TexPrax in a user-study with German factory employees who ask their\ncolleagues for solutions on problems that arise during their daily work.\nOverall, we collect 202 task-oriented German dialogues containing 1,027\nsentences with sentence-level expert annotations. Our data analysis also\nreveals that real-world conversations frequently contain instances with\ncode-switching, varying abbreviations for the same entity, and dialects which\nNLP systems should be able to handle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stangier_L/0/1/0/all/0/1\">Lorenz Stangier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Ji-Ung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Marvin M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frick_N/0/1/0/all/0/1\">Nicholas Frick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metternich_J/0/1/0/all/0/1\">Joachim Metternich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UKP-SQuARE v2: Explainability and Adversarial Attacks for Trustworthy QA. (arXiv:2208.09316v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.09316","description":"<p>Question Answering (QA) systems are increasingly deployed in applications\nwhere they support real-world decisions. However, state-of-the-art models rely\non deep neural networks, which are difficult to interpret by humans. Inherently\ninterpretable models or post hoc explainability methods can help users to\ncomprehend how a model arrives at its prediction and, if successful, increase\ntheir trust in the system. Furthermore, researchers can leverage these insights\nto develop new methods that are more accurate and less biased. In this paper,\nwe introduce SQuARE v2, the new version of SQuARE, to provide an explainability\ninfrastructure for comparing models based on methods such as saliency maps and\ngraph-based explanations. While saliency maps are useful to inspect the\nimportance of each input token for the model's prediction, graph-based\nexplanations from external Knowledge Graphs enable the users to verify the\nreasoning behind the model prediction. In addition, we provide multiple\nadversarial attacks to compare the robustness of QA models. With these\nexplainability methods and adversarial attacks, we aim to ease the research on\ntrustworthy QA models. SQuARE is available on https://square.ukp-lab.de.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_R/0/1/0/all/0/1\">Rachneet Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puerto_H/0/1/0/all/0/1\">Haritz Puerto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_T/0/1/0/all/0/1\">Tim Baumg&#xe4;rtner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariverdian_S/0/1/0/all/0/1\">Sewin Tariverdian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadi_H/0/1/0/all/0/1\">Hossain Shaikh Saadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IDIAPers @ Causal News Corpus 2022: Extracting Cause-Effect-Signal Triplets via Pre-trained Autoregressive Language Model. (arXiv:2209.03891v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.03891","description":"<p>In this paper, we describe our shared task submissions for Subtask 2 in\nCASE-2022, Event Causality Identification with Casual News Corpus. The\nchallenge focused on the automatic detection of all cause-effect-signal spans\npresent in the sentence from news-media. We detect cause-effect-signal spans in\na sentence using T5 -- a pre-trained autoregressive language model. We\niteratively identify all cause-effect-signal span triplets, always conditioning\nthe prediction of the next triplet on the previously predicted ones. To predict\nthe triplet itself, we consider different causal relationships such as\ncause$\\rightarrow$effect$\\rightarrow$signal. Each triplet component is\ngenerated via a language model conditioned on the sentence, the previous parts\nof the current triplet, and previously predicted triplets. Despite training on\nan extremely small dataset of 160 samples, our approach achieved competitive\nperformance, being placed second in the competition. Furthermore, we show that\nassuming either cause$\\rightarrow$effect or effect$\\rightarrow$cause order\nachieves similar results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Muskaan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villatoro_Tello_E/0/1/0/all/0/1\">Esa&#xfa; Villatoro-Tello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burdisso_S/0/1/0/all/0/1\">Sergio Burdisso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answering Numerical Reasoning Questions in Table-Text Hybrid Contents with Graph-based Encoder and Tree-based Decoder. (arXiv:2209.07692v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.07692","description":"<p>In the real-world question answering scenarios, hybrid form combining both\ntabular and textual contents has attracted more and more attention, among which\nnumerical reasoning problem is one of the most typical and challenging\nproblems. Existing methods usually adopt encoder-decoder framework to represent\nhybrid contents and generate answers. However, it can not capture the rich\nrelationship among numerical value, table schema, and text information on the\nencoder side. The decoder uses a simple predefined operator classifier which is\nnot flexible enough to handle numerical reasoning processes with diverse\nexpressions. To address these problems, this paper proposes a\n\\textbf{Re}lational \\textbf{G}raph enhanced \\textbf{H}ybrid table-text\n\\textbf{N}umerical reasoning model with \\textbf{T}ree decoder\n(\\textbf{RegHNT}). It models the numerical question answering over table-text\nhybrid contents as an expression tree generation task. Moreover, we propose a\nnovel relational graph modeling method, which models alignment between\nquestions, tables, and paragraphs. We validated our model on the publicly\navailable table-text hybrid QA benchmark (TAT-QA). The proposed RegHNT\nsignificantly outperform the baseline model and achieve state-of-the-art\nresults. We openly released the source code and data at\nhttps://github.com/lfy79001/RegHNT (2022-05-05).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_F/0/1/0/all/0/1\">Fangyu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shizhu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction and Applications of Billion-Scale Pre-trained Multimodal Business Knowledge Graph. (arXiv:2209.15214v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2209.15214","description":"<p>Business Knowledge Graphs (KGs) are important to many enterprises today,\nproviding factual knowledge and structured data that steer many products and\nmake them more intelligent. Despite their promising benefits, building business\nKG necessitates solving prohibitive issues of deficient structure and multiple\nmodalities. In this paper, we advance the understanding of the practical\nchallenges related to building KG in non-trivial real-world systems. We\nintroduce the process of building an open business knowledge graph (OpenBG)\nderived from a well-known enterprise, Alibaba Group. Specifically, we define a\ncore ontology to cover various abstract products and consumption demands, with\nfine-grained taxonomy and multimodal facts in deployed applications. OpenBG is\nan open business KG of unprecedented scale: 2.6 billion triples with more than\n88 million entities covering over 1 million core classes/concepts and 2,681\ntypes of relations. We release all the open resources (OpenBG benchmarks)\nderived from it for the community and report experimental results of KG-centric\ntasks. We also run up an online competition based on OpenBG benchmarks, and has\nattracted thousands of teams. We further pre-train OpenBG and apply it to many\nKG- enhanced downstream tasks in business scenarios, demonstrating the\neffectiveness of billion-scale multimodal knowledge for e-commerce. All the\nresources with codes have been released at\n\\url{https://github.com/OpenBGBenchmark/OpenBG}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zelin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hehong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploration of A Self-Supervised Speech Model: A Study on Emotional Corpora. (arXiv:2210.02595v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2210.02595","description":"<p>Self-supervised speech models have grown fast during the past few years and\nhave proven feasible for use in various downstream tasks. Some recent work has\nstarted to look at the characteristics of these models, yet many concerns have\nnot been fully addressed. In this work, we conduct a study on emotional corpora\nto explore a popular self-supervised model -- wav2vec 2.0. Via a set of\nquantitative analysis, we mainly demonstrate that: 1) wav2vec 2.0 appears to\ndiscard paralinguistic information that is less useful for word recognition\npurposes; 2) for emotion recognition, representations from the middle layer\nalone perform as well as those derived from layer averaging, while the final\nlayer results in the worst performance in some cases; 3) current\nself-supervised models may not be the optimal solution for downstream tasks\nthat make use of non-lexical features. Our work provides novel findings that\nwill aid future research in this area and theoretical basis for the use of\nexisting models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuanchao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohamied_Y/0/1/0/all/0/1\">Yumnah Mohamied</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_C/0/1/0/all/0/1\">Catherine Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text. (arXiv:2210.02928v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02928","description":"<p>While language Models store a massive amount of world knowledge implicitly in\ntheir parameters, even very large models often fail to encode information about\nrare entities and events, while incurring huge computational costs. Recently,\nretrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated\nworld knowledge into language generation by leveraging an external\nnon-parametric index and have demonstrated impressive performance with\nconstrained model sizes. However, these methods are restricted to retrieving\nonly textual knowledge, neglecting the ubiquitous amount of knowledge in other\nmodalities like images -- much of which contains information not covered by any\ntext. To address this limitation, we propose the first Multimodal\nRetrieval-Augmented Transformer (MuRAG), which accesses an external\nnon-parametric multimodal memory to augment language generation. MuRAG is\npre-trained with a mixture of large-scale image-text and text-only corpora\nusing a joint contrastive and generative loss. We perform experiments on two\ndifferent datasets that require retrieving and reasoning over both images and\ntext to answer a given query: WebQA, and MultimodalQA. Our results show that\nMuRAG achieves state-of-the-art accuracy, outperforming existing models by\n10-20\\% absolute on both datasets and under both distractor and full-wiki\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1\">Pat Verga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair Modeling. (arXiv:2210.05261v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05261","description":"<p>Transformer-based models have achieved great success on sentence pair\nmodeling tasks, such as answer selection and natural language inference (NLI).\nThese models generally perform cross-attention over input pairs, leading to\nprohibitive computational costs. Recent studies propose dual-encoder and late\ninteraction architectures for faster computation. However, the balance between\nthe expressive of cross-attention and computation speedup still needs better\ncoordinated. To this end, this paper introduces a novel paradigm MixEncoder for\nefficient sentence pair modeling. MixEncoder involves a light-weight\ncross-attention mechanism. It conducts query encoding only once while modeling\nthe query-candidate interaction in parallel. Extensive experiments conducted on\nfour tasks demonstrate that our MixEncoder can speed up sentence pairing by\nover 113x while achieving comparable performance as the more expensive\ncross-attention models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuanhang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+qi_s/0/1/0/all/0/1\">shiyi qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Cuiyun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuanyi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anonymizing Speech with Generative Adversarial Networks to Preserve Speaker Privacy. (arXiv:2210.07002v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2210.07002","description":"<p>In order to protect the privacy of speech data, speaker anonymization aims\nfor hiding the identity of a speaker by changing the voice in speech\nrecordings. This typically comes with a privacy-utility trade-off between\nprotection of individuals and usability of the data for downstream\napplications. One of the challenges in this context is to create non-existent\nvoices that sound as natural as possible.\n</p>\n<p>In this work, we propose to tackle this issue by generating speaker\nembeddings using a generative adversarial network with Wasserstein distance as\ncost function. By incorporating these artificial embeddings into a\nspeech-to-text-to-speech pipeline, we outperform previous approaches in terms\nof privacy and utility. According to standard objective metrics and human\nevaluation, our approach generates intelligible and content-preserving yet\nprivacy-protecting versions of the original recordings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meyer_S/0/1/0/all/0/1\">Sarina Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tilli_P/0/1/0/all/0/1\">Pascal Tilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denisov_P/0/1/0/all/0/1\">Pavel Denisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lux_F/0/1/0/all/0/1\">Florian Lux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_J/0/1/0/all/0/1\">Julia Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving generalizability of distilled self-supervised speech processing models under distorted settings. (arXiv:2210.07978v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2210.07978","description":"<p>Self-supervised learned (SSL) speech pre-trained models perform well across\nvarious speech processing tasks. Distilled versions of SSL models have been\ndeveloped to match the needs of on-device speech applications. Though having\nsimilar performance as original SSL models, distilled counterparts suffer from\nperformance degradation even more than their original versions in distorted\nenvironments. This paper proposes to apply Cross-Distortion Mapping and Domain\nAdversarial Training to SSL models during knowledge distillation to alleviate\nthe performance gap caused by the domain mismatch problem. Results show\nconsistent performance improvements under both in- and out-of-domain distorted\nsetups for different downstream tasks while keeping efficient model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Po Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yu-Kuan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_T/0/1/0/all/0/1\">Tsu-Yuan Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_F/0/1/0/all/0/1\">Fabian Ritter Gutierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan-Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_L/0/1/0/all/0/1\">Liang-Hsuan Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models for Multi-label Propaganda Detection. (arXiv:2210.08209v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08209","description":"<p>The spread of propaganda through the internet has increased drastically over\nthe past years. Lately, propaganda detection has started gaining importance\nbecause of the negative impact it has on society. In this work, we describe our\napproach for the WANLP 2022 shared task which handles the task of propaganda\ndetection in a multi-label setting. The task demands the model to label the\ngiven text as having one or more types of propaganda techniques. There are a\ntotal of 21 propaganda techniques to be detected. We show that an ensemble of\nfive models performs the best on the task, scoring a micro-F1 score of 59.73%.\nWe also conduct comprehensive ablations and propose various future directions\nfor this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chavan_T/0/1/0/all/0/1\">Tanmay Chavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kane_A/0/1/0/all/0/1\">Aditya Kane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Character-Centric Story Visualization via Visual Planning and Token Alignment. (arXiv:2210.08465v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.08465","description":"<p>Story visualization advances the traditional text-to-image generation by\nenabling multiple image generation based on a complete story. This task\nrequires machines to 1) understand long text inputs and 2) produce a globally\nconsistent image sequence that illustrates the contents of the story. A key\nchallenge of consistent story visualization is to preserve characters that are\nessential in stories. To tackle the challenge, we propose to adapt a recent\nwork that augments Vector-Quantized Variational Autoencoders (VQ-VAE) with a\ntext-tovisual-token (transformer) architecture. Specifically, we modify the\ntext-to-visual-token module with a two-stage framework: 1) character token\nplanning model that predicts the visual tokens for characters only; 2) visual\ntoken completion model that generates the remaining visual token sequence,\nwhich is sent to VQ-VAE for finalizing image generations. To encourage\ncharacters to appear in the images, we further train the two-stage framework\nwith a character-token alignment objective. Extensive experiments and\nevaluations demonstrate that the proposed method excels at preserving\ncharacters and can produce higher quality image sequences compared with the\nstrong baselines. Codes can be found in https://github.com/sairin1202/VP-CSV\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rujun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Te-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakayama_H/0/1/0/all/0/1\">Hideki Nakayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Some Languages are More Equal than Others: Probing Deeper into the Linguistic Disparity in the NLP World. (arXiv:2210.08523v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08523","description":"<p>Linguistic disparity in the NLP world is a problem that has been widely\nacknowledged recently. However, different facets of this problem, or the\nreasons behind this disparity are seldom discussed within the NLP community.\nThis paper provides a comprehensive analysis of the disparity that exists\nwithin the languages of the world. We show that simply categorising languages\nconsidering data availability may not be always correct. Using an existing\nlanguage categorisation based on speaker population and vitality, we analyse\nthe distribution of language data resources, amount of NLP/CL research,\ninclusion in multilingual web-based platforms and the inclusion in pre-trained\nmultilingual models. We show that many languages do not get covered in these\nresources or platforms, and even within the languages belonging to the same\nlanguage group, there is wide disparity. We analyse the impact of family,\ngeographical location, GDP and the speaker population of languages and provide\npossible reasons for this disparity, along with some suggestions to overcome\nthe same.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1\">Surangika Ranathunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELASTIC: Numerical Reasoning with Adaptive Symbolic Compiler. (arXiv:2210.10105v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2210.10105","description":"<p>Numerical reasoning over text is a challenging task of Artificial\nIntelligence (AI), requiring reading comprehension and numerical reasoning\nabilities. Previous approaches use numerical reasoning programs to represent\nthe reasoning process. However, most works do not separate the generation of\noperators and operands, which are key components of a numerical reasoning\nprogram, thus limiting their ability to generate such programs for complicated\ntasks. In this paper, we introduce the numEricaL reASoning with adapTive\nsymbolIc Compiler (ELASTIC) model, which is constituted of the RoBERTa as the\nEncoder and a Compiler with four modules: Reasoning Manager, Operator\nGenerator, Operands Generator, and Memory Register. ELASTIC is robust when\nconducting complicated reasoning. Also, it is domain agnostic by supporting the\nexpansion of diverse operators without caring about the number of operands it\ncontains. Experiments show that ELASTIC achieves 68.96 and 65.21 of execution\naccuracy and program accuracy on the FinQA dataset and 83.00 program accuracy\non the MathQA dataset, outperforming previous state-of-the-art models\nsignificantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moshfeghi_Y/0/1/0/all/0/1\">Yashar Moshfeghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightEA: A Scalable, Robust, and Interpretable Entity Alignment Framework via Three-view Label Propagation. (arXiv:2210.10436v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2210.10436","description":"<p>Entity Alignment (EA) aims to find equivalent entity pairs between KGs, which\nis the core step of bridging and integrating multi-source KGs. In this paper,\nwe argue that existing GNN-based EA methods inherit the inborn defects from\ntheir neural network lineage: weak scalability and poor interpretability.\nInspired by recent studies, we reinvent the Label Propagation algorithm to\neffectively run on KGs and propose a non-neural EA framework -- LightEA,\nconsisting of three efficient components: (i) Random Orthogonal Label\nGeneration, (ii) Three-view Label Propagation, and (iii) Sparse Sinkhorn\nIteration. According to the extensive experiments on public datasets, LightEA\nhas impressive scalability, robustness, and interpretability. With a mere tenth\nof time consumption, LightEA achieves comparable results to state-of-the-art\nmethods across all datasets and even surpasses them on many.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanbin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1\">Man Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective. (arXiv:2210.10488v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10488","description":"<p>Two interlocking research questions of growing interest and importance in\nprivacy research are Authorship Attribution (AA) and Authorship Obfuscation\n(AO). Given an artifact, especially a text t in question, an AA solution aims\nto accurately attribute t to its true author out of many candidate authors\nwhile an AO solution aims to modify t to hide its true authorship.\nTraditionally, the notion of authorship and its accompanying privacy concern is\nonly toward human authors. However, in recent years, due to the explosive\nadvancements in Neural Text Generation (NTG) techniques in NLP, capable of\nsynthesizing human-quality open-ended texts (so-called \"neural texts\"), one has\nto now consider authorships by humans, machines, or their combination. Due to\nthe implications and potential threats of neural texts when used maliciously,\nit has become critical to understand the limitations of traditional AA/AO\nsolutions and develop novel AA/AO solutions in dealing with neural texts. In\nthis survey, therefore, we make a comprehensive review of recent literature on\nthe attribution and obfuscation of neural text authorship from a Data Mining\nperspective, and share our view on their limitations and promising research\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uchendu_A/0/1/0/all/0/1\">Adaku Uchendu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thai Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Separating Grains from the Chaff: Using Data Filtering to Improve Multilingual Translation for Low-Resourced African Languages. (arXiv:2210.10692v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10692","description":"<p>We participated in the WMT 2022 Large-Scale Machine Translation Evaluation\nfor the African Languages Shared Task. This work describes our approach, which\nis based on filtering the given noisy data using a sentence-pair classifier\nthat was built by fine-tuning a pre-trained language model. To train the\nclassifier, we obtain positive samples (i.e. high-quality parallel sentences)\nfrom a gold-standard curated dataset and extract negative samples (i.e.\nlow-quality parallel sentences) from automatically aligned parallel data by\nchoosing sentences with low alignment scores. Our final machine translation\nmodel was then trained on filtered data, instead of the entire noisy dataset.\nWe empirically validate our approach by evaluating on two common datasets and\nshow that data filtering generally improves overall translation quality, in\nsome cases even significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beukman_M/0/1/0/all/0/1\">Michael Beukman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba O. Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asiko_E/0/1/0/all/0/1\">Everlyn Asiko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeyemi_M/0/1/0/all/0/1\">Mofetoluwa Adeyemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousuf_O/0/1/0/all/0/1\">Oreen Yousuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwadabe_T/0/1/0/all/0/1\">Tajuddeen Rabiu Gwadabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}