{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2024-01-30T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks. (arXiv:2401.15170v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15170","description":"<p>Qualitative coding, or content analysis, extracts meaning from text to\ndiscern quantitative patterns across a corpus of texts. Recently, advances in\nthe interpretive abilities of large language models (LLMs) offer potential for\nautomating the coding process (applying category labels to texts), thereby\nenabling human researchers to concentrate on more creative research aspects,\nwhile delegating these interpretive tasks to AI. Our case study comprises a set\nof socio-historical codes on dense, paragraph-long passages representative of a\nhumanistic study. We show that GPT-4 is capable of human-equivalent\ninterpretations, whereas GPT-3.5 is not. Compared to our human-derived gold\nstandard, GPT-4 delivers excellent intercoder reliability (Cohen's $\\kappa \\geq\n0.79$) for 3 of 9 codes, and substantial reliability ($\\kappa \\geq 0.6$) for 8\nof 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes\n($mean(\\kappa) = 0.34$; $max(\\kappa) = 0.55$). Importantly, we find that coding\nfidelity improves considerably when the LLM is prompted to give rationale\njustifying its coding decisions (chain-of-thought reasoning). We present these\nand other findings along with a set of best practices for adapting traditional\ncodebooks for LLMs. Our results indicate that for certain codebooks,\nstate-of-the-art LLMs are already adept at large-scale content analysis.\nFurthermore, they suggest the next generation of models will likely render AI\ncoding a viable option for a majority of codebooks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dunivin_Z/0/1/0/all/0/1\">Zackary Okun Dunivin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection. (arXiv:2401.15222v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15222","description":"<p>Background: The semantics of entities extracted from a clinical text can be\ndramatically altered by modifiers, including entity negation, uncertainty,\nconditionality, severity, and subject. Existing models for determining\nmodifiers of clinical entities involve regular expression or features weights\nthat are trained independently for each modifier.\n</p>\n<p>Methods: We develop and evaluate a multi-task transformer architecture design\nwhere modifiers are learned and predicted jointly using the publicly available\nSemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that\ncontains modifiers shared with SemEval as well as novel modifiers specific for\nOUD. We evaluate the effectiveness of our multi-task learning approach versus\npreviously published systems and assess the feasibility of transfer learning\nfor clinical entity modifiers when only a portion of clinical modifiers are\nshared.\n</p>\n<p>Results: Our approach achieved state-of-the-art results on the ShARe corpus\nfrom SemEval 2015 Task 14, showing an increase of 1.1% on weighted accuracy,\n1.7% on unweighted accuracy, and 10% on micro F1 scores.\n</p>\n<p>Conclusions: We show that learned weights from our shared model can be\neffectively transferred to a new partially matched data set, validating the use\nof transfer learning for clinical text modifiers\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almudaifer_A/0/1/0/all/0/1\">Abdullateef I. Almudaifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+O%60Leary_T/0/1/0/all/0/1\">Tobias O`Leary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Covington_W/0/1/0/all/0/1\">Whitney Covington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hairston_J/0/1/0/all/0/1\">JaMor Hairston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deitch_Z/0/1/0/all/0/1\">Zachary Deitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Ankit Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carroll_C/0/1/0/all/0/1\">Caleb M. Carroll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crisan_E/0/1/0/all/0/1\">Estera Crisan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradford_W/0/1/0/all/0/1\">William Bradford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walter_L/0/1/0/all/0/1\">Lauren Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellen_E/0/1/0/all/0/1\">Eaton Ellen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1\">Sue S. Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osborne_J/0/1/0/all/0/1\">John D. Osborne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlearning Reveals the Influential Training Data of Language Models. (arXiv:2401.15241v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15241","description":"<p>In order to enhance the performance of language models while mitigating the\nrisks of generating harmful content, it is crucial to identify which training\ndataset affects the model's outputs. Ideally, we can measure the influence of\neach dataset by removing it from training; however, it is prohibitively\nexpensive to retrain a model multiple times. This paper presents UnTrac, which\nestimates the influence of a training dataset by unlearning it from the trained\nmodel. UnTrac is extremely simple; each training dataset is unlearned by\ngradient ascent, and we evaluate how much the model's predictions change after\nunlearning. We empirically examine if our methods can assess the influence of\npretraining datasets on generating toxic, biased, and untruthful content.\nExperimental results demonstrate that our method estimates their influence much\nmore accurately than existing methods while requiring neither excessive memory\nspace nor multiple model checkpoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Isonuma_M/0/1/0/all/0/1\">Masaru Isonuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models. (arXiv:2401.15269v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15269","description":"<p>Recent proprietary large language models (LLMs), such as GPT-4, have achieved\na milestone in tackling diverse challenges in the biomedical domain, ranging\nfrom multiple-choice questions to long-form generations. To address challenges\nthat still cannot be handled with the encoded knowledge of LLMs, various\nretrieval-augmented generation (RAG) methods have been developed by searching\ndocuments from the knowledge corpus and appending them unconditionally or\nselectively to the input of LLMs for generation. However, when applying\nexisting methods to different domain-specific problems, poor generalization\nbecomes apparent, leading to fetching incorrect documents or making inaccurate\njudgments. In this paper, we introduce Self-BioRAG, a framework reliable for\nbiomedical text that specializes in generating explanations, retrieving\ndomain-specific documents, and self-reflecting generated responses. We utilize\n84k filtered biomedical instruction sets to train Self-BioRAG that can assess\nits generated explanations with customized reflective tokens. Our work proves\nthat domain-specific components, such as a retriever, domain-related document\ncorpus, and instruction sets are necessary for adhering to domain-related\ninstructions. Using three major medical question-answering benchmark datasets,\nexperimental results of Self-BioRAG demonstrate significant performance gains\nby achieving a 7.2% absolute improvement on average over the state-of-the-art\nopen-foundation model with a parameter size of 7B or less. Overall, we analyze\nthat Self-BioRAG finds the clues in the question, retrieves relevant documents\nif needed, and understands how to answer with information from retrieved\ndocuments and encoded knowledge as a medical expert does. We release our data\nand code for training our framework components and model weights (7B and 13B)\nto enhance capabilities in biomedical and clinical domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Minbyul Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1\">Jiwoong Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Mujeen Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How We Refute Claims: Automatic Fact-Checking through Flaw Identification and Explanation. (arXiv:2401.15312v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15312","description":"<p>Automated fact-checking is a crucial task in the governance of internet\ncontent. Although various studies utilize advanced models to tackle this issue,\na significant gap persists in addressing complex real-world rumors and\ndeceptive claims. To address this challenge, this paper explores the novel task\nof flaw-oriented fact-checking, including aspect generation and flaw\nidentification. We also introduce RefuteClaim, a new framework designed\nspecifically for this task. Given the absence of an existing dataset, we\npresent FlawCheck, a dataset created by extracting and transforming insights\nfrom expert reviews into relevant aspects and identified flaws. The\nexperimental results underscore the efficacy of RefuteClaim, particularly in\nclassifying and elucidating false claims.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kao_W/0/1/0/all/0/1\">Wei-Yu Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_A/0/1/0/all/0/1\">An-Zi Yen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNSEE: Unsupervised Non-contrastive Sentence Embeddings. (arXiv:2401.15316v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15316","description":"<p>We present UNSEE: Unsupervised Non-Contrastive Sentence Embeddings, a novel\napproach that outperforms SimCSE in the Massive Text Embedding benchmark. Our\nexploration begins by addressing the challenge of representation collapse, a\nphenomenon observed when contrastive objectives in SimCSE are replaced with\nnon-contrastive objectives. To counter this issue, we propose a straightforward\nsolution known as the target network, effectively mitigating representation\ncollapse. The introduction of the target network allows us to leverage\nnon-contrastive objectives, maintaining training stability while achieving\nperformance improvements comparable to contrastive objectives. Our method has\nachieved peak performance in non-contrastive sentence embeddings through\nmeticulous fine-tuning and optimization. This comprehensive effort has yielded\nsuperior sentence representation models, showcasing the effectiveness of our\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cagatan_O/0/1/0/all/0/1\">&#xd6;mer Veysel &#xc7;a&#x11f;atan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance. (arXiv:2401.15328v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15328","description":"<p>Large language models (LLMs) have exhibited an array of reasoning\ncapabilities but face challenges like error propagation and hallucination,\nparticularly in specialised areas like finance, where data is heterogeneous,\nand precision is paramount. We explore the potential of language model\naugmentation with external tools to mitigate these limitations and offload\ncertain reasoning steps to external tools that are more suited for the task,\ninstead of solely depending on the LLM's inherent abilities. More concretely,\nusing financial domain question-answering datasets, we apply supervised\nfine-tuning on a LLaMA-2 13B Chat model to act both as a 'task router' and\n'task solver'. The 'task router' dynamically directs a question to either be\nanswered internally by the LLM or externally via the right tool from the tool\nset. Our tool-equipped SFT model, Raven, demonstrates an improvement of 35.2%\nand 5.06% over the base model and SFT-only baselines, respectively, and is\nhighly competitive with strong GPT-3.5 results. To the best of our knowledge,\nour work is the first that investigates tool augmentation of language models\nfor the finance domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theuma_A/0/1/0/all/0/1\">Adrian Theuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1\">Ehsan Shareghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey of Compression Algorithms for Language Models. (arXiv:2401.15347v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15347","description":"<p>How can we compress language models without sacrificing accuracy? The number\nof compression algorithms for language models is rapidly growing to benefit\nfrom remarkable advances of recent language models without side effects due to\nthe gigantic size of language models, such as increased carbon emissions and\nexpensive maintenance fees. While numerous compression algorithms have shown\nremarkable progress in compressing language models, it ironically becomes\nchallenging to capture emerging trends and identify the fundamental concepts\nunderlying them due to the excessive number of algorithms. In this paper, we\nsurvey and summarize diverse compression algorithms including pruning,\nquantization, knowledge distillation, low-rank approximation, parameter\nsharing, and efficient architecture design. We not only summarize the overall\ntrend of diverse compression algorithms but also select representative\nalgorithms and provide in-depth analyses of them. We discuss the value of each\ncategory of compression algorithms, and the desired properties of low-cost\ncompression algorithms which have a significant impact due to the emergence of\nlarge language models. Finally, we introduce promising future research topics\nbased on our survey results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seungcheol Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaehyeon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sojin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_U/0/1/0/all/0/1\">U Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Neural Topic Models: Methods, Applications, and Challenges. (arXiv:2401.15351v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15351","description":"<p>Topic models have been prevalent for decades to discover latent topics and\ninfer topic proportions of documents in an unsupervised fashion. They have been\nwidely used in various applications like text analysis and context\nrecommendation. Recently, the rise of neural networks has facilitated the\nemergence of a new research field -- Neural Topic Models (NTMs). Different from\nconventional topic models, NTMs directly optimize parameters without requiring\nmodel-specific derivations. This endows NTMs with better scalability and\nflexibility, resulting in significant research attention and plentiful new\nmethods and applications. In this paper, we present a comprehensive survey on\nneural topic models concerning methods, applications, and challenges.\nSpecifically, we systematically organize current NTM methods according to their\nnetwork structures and introduce the NTMs for various scenarios like short\ntexts and cross-lingual documents. We also discuss a wide range of popular\napplications built on NTMs. Finally, we highlight the challenges confronted by\nNTMs to inspire future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaobao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Importance-Aware Data Augmentation for Document-Level Neural Machine Translation. (arXiv:2401.15360v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15360","description":"<p>Document-level neural machine translation (DocNMT) aims to generate\ntranslations that are both coherent and cohesive, in contrast to its\nsentence-level counterpart. However, due to its longer input length and limited\navailability of training data, DocNMT often faces the challenge of data\nsparsity. To overcome this issue, we propose a novel Importance-Aware Data\nAugmentation (IADA) algorithm for DocNMT that augments the training data based\non token importance information estimated by the norm of hidden states and\ntraining gradients. We conduct comprehensive experiments on three widely-used\nDocNMT benchmarks. Our empirical results show that our proposed IADA\noutperforms strong DocNMT baselines as well as several data augmentation\napproaches, with statistical significance on both sentence-level and\ndocument-level BLEU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_G/0/1/0/all/0/1\">George Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LegalDuet: Learning Effective Representations for Legal Judgment Prediction through a Dual-View Legal Clue Reasoning. (arXiv:2401.15371v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15371","description":"<p>Most existing Legal Judgment Prediction (LJP) models focus on discovering the\nlegal triggers in the criminal fact description. However, in real-world\nscenarios, a professional judge not only needs to assimilate the law case\nexperience that thrives on past sentenced legal judgments but also depends on\nthe professional legal grounded reasoning that learned from professional legal\nknowledge. In this paper, we propose a LegalDuet model, which pretrains\nlanguage models to learn a tailored embedding space for making legal judgments.\nIt proposes a dual-view legal clue reasoning mechanism, which derives from two\nreasoning chains of judges: 1) Law Case Reasoning, which makes legal judgments\naccording to the judgment experiences learned from analogy/confusing legal\ncases; 2) Legal Ground Reasoning, which lies in matching the legal clues\nbetween criminal cases and legal decisions. Our experiments show that LegalDuet\nachieves state-of-the-art performance on the CAIL2018 dataset and outperforms\nbaselines with about 4% improvements on average. Our dual-view reasoning based\npretraining can capture critical legal clues to learn a tailored embedding\nspace to distinguish criminal cases. It reduces LegalDuet's uncertainty during\nprediction and brings pretraining advances to the confusing/low frequent\ncharges. All codes are available at https://github.com/NEUIR/LegalDuet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liner Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Ge Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuang-hua Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15378","description":"<p>There exist challenges in learning and understanding religions as the\npresence of complexity and depth of religious doctrines and teachings. Chatbots\nas question-answering systems can help in solving these challenges. LLM\nchatbots use NLP techniques to establish connections between topics and\naccurately respond to complex questions. These capabilities make it perfect to\nbe used in enlightenment on religion as a question answering chatbot. However,\nLLMs also have a tendency to generate false information, known as\nhallucination. The responses of the chatbots can include content that insults\npersonal religious beliefs, interfaith conflicts, and controversial or\nsensitive topics. It needs to avoid such cases without promoting hate speech or\noffending certain groups of people or their beliefs. This study uses a vector\ndatabase-based Retrieval Augmented Generation (RAG) approach to enhance the\naccuracy and transparency of LLMs. Our question-answering system is called as\n\"MufassirQAS\". We created a vector database with several open-access books that\ninclude Turkish context. These are Turkish translations, and interpretations on\nIslam. We worked on creating system prompts with care, ensuring they provide\ninstructions that prevent harmful, offensive, or disrespectful responses. We\nalso tested the MufassirQAS and ChatGPT with sensitive questions. We got better\nperformance with our system. Study and enhancements are still in progress.\nResults and future works are given.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alan_A/0/1/0/all/0/1\">Ahmet Yusuf Alan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaarslan_E/0/1/0/all/0/1\">Enis Karaarslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aydin_O/0/1/0/all/0/1\">Omer Aydin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Event Extraction from Speech with Contextual Clues. (arXiv:2401.15385v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15385","description":"<p>While text-based event extraction has been an active research area and has\nseen successful application in many domains, extracting semantic events from\nspeech directly is an under-explored problem. In this paper, we introduce the\nSpeech Event Extraction (SpeechEE) task and construct three synthetic training\nsets and one human-spoken test set. Compared to event extraction from text,\nSpeechEE poses greater challenges mainly due to complex speech signals that are\ncontinuous and have no word boundaries. Additionally, unlike perceptible sound\nevents, semantic events are more subtle and require a deeper understanding. To\ntackle these challenges, we introduce a sequence-to-structure generation\nparadigm that can produce events from speech signals in an end-to-end manner,\ntogether with a conditioned generation method that utilizes speech recognition\ntranscripts as the contextual clue. We further propose to represent events with\na flat format to make outputs more natural language-like. Our experimental\nresults show that our method brings significant improvements on all datasets,\nachieving a maximum F1 gain of 10.7%. The code and datasets are released on\nhttps://github.com/jodie-kang/SpeechEE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jingqi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongtong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guitao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries. (arXiv:2401.15391v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15391","description":"<p>Retrieval-augmented generation (RAG) augments large language models (LLM) by\nretrieving relevant knowledge, showing promising potential in mitigating LLM\nhallucinations and enhancing response quality, thereby facilitating the great\nadoption of LLMs in practice. However, we find that existing RAG systems are\ninadequate in answering multi-hop queries, which require retrieving and\nreasoning over multiple pieces of supporting evidence. Furthermore, to our\nknowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.\nIn this paper, we develop a novel dataset, MultiHop-RAG, which consists of a\nknowledge base, a large collection of multi-hop queries, their ground-truth\nanswers, and the associated supporting evidence. We detail the procedure of\nbuilding the dataset, utilizing an English news article dataset as the\nunderlying RAG knowledge base. We demonstrate the benchmarking utility of\nMultiHop-RAG in two experiments. The first experiment compares different\nembedding models for retrieving evidence for multi-hop queries. In the second\nexperiment, we examine the capabilities of various state-of-the-art LLMs,\nincluding GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop\nqueries given the evidence. Both experiments reveal that existing RAG methods\nperform unsatisfactorily in retrieving and answering multi-hop queries. We hope\nMultiHop-RAG will be a valuable resource for the community in developing\neffective RAG systems, thereby facilitating greater adoption of LLMs in\npractice. The MultiHop-RAG and implemented RAG system is publicly available at\nhttps://github.com/yixuantt/MultiHop-RAG/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yixuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantics of Multiword Expressions in Transformer-Based Models: A Survey. (arXiv:2401.15393v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15393","description":"<p>Multiword expressions (MWEs) are composed of multiple words and exhibit\nvariable degrees of compositionality. As such, their meanings are notoriously\ndifficult to model, and it is unclear to what extent this issue affects\ntransformer architectures. Addressing this gap, we provide the first in-depth\nsurvey of MWE processing with transformer models. We overall find that they\ncapture MWE semantics inconsistently, as shown by reliance on surface patterns\nand memorized information. MWE meaning is also strongly localized,\npredominantly in early layers of the architecture. Representations benefit from\nspecific linguistic properties, such as lower semantic idiosyncrasy and\nambiguity of target expressions. Our findings overall question the ability of\ntransformer models to robustly capture fine-grained semantics. Furthermore, we\nhighlight the need for more directly comparable evaluation setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miletic_F/0/1/0/all/0/1\">Filip Mileti&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walde_S/0/1/0/all/0/1\">Sabine Schulte im Walde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indexing Portuguese NLP Resources with PT-Pump-Up. (arXiv:2401.15400v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15400","description":"<p>The recent advances in natural language processing (NLP) are linked to\ntraining processes that require vast amounts of corpora. Access to this data is\ncommonly not a trivial process due to resource dispersion and the need to\nmaintain these infrastructures online and up-to-date. New developments in NLP\nare often compromised due to the scarcity of data or lack of a shared\nrepository that works as an entry point to the community. This is especially\ntrue in low and mid-resource languages, such as Portuguese, which lack data and\nproper resource management infrastructures. In this work, we propose\nPT-Pump-Up, a set of tools that aim to reduce resource dispersion and improve\nthe accessibility to Portuguese NLP resources. Our proposal is divided into\nfour software components: a) a web platform to list the available resources; b)\na client-side Python package to simplify the loading of Portuguese NLP\nresources; c) an administrative Python package to manage the platform and d) a\npublic GitHub repository to foster future collaboration and contributions. All\nfour components are accessible using: https://linktr.ee/pt_pump_up\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almeida_R/0/1/0/all/0/1\">R&#xfa;ben Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_R/0/1/0/all/0/1\">Ricardo Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorge_A/0/1/0/all/0/1\">Al&#xed;pio Jorge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunes_S/0/1/0/all/0/1\">S&#xe9;rgio Nunes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Data Augmentation in Large Model Era. (arXiv:2401.15422v1 [cs.LG])","link":"http://arxiv.org/abs/2401.15422","description":"<p>Large models, encompassing large language and diffusion models, have shown\nexceptional promise in approximating human-level intelligence, garnering\nsignificant interest from both academic and industrial spheres. However, the\ntraining of these large models necessitates vast quantities of high-quality\ndata, and with continuous updates to these models, the existing reservoir of\nhigh-quality data may soon be depleted. This challenge has catalyzed a surge in\nresearch focused on data augmentation methods. Leveraging large models, these\ndata augmentation techniques have outperformed traditional approaches. This\npaper offers an exhaustive review of large model-driven data augmentation\nmethods, adopting a comprehensive perspective. We begin by establishing a\nclassification of relevant studies into three main categories: image\naugmentation, text augmentation, and paired data augmentation. Following this,\nwe delve into various data post-processing techniques pertinent to large\nmodel-based data augmentation. Our discussion then expands to encompass the\narray of applications for these data augmentation methods within natural\nlanguage processing, computer vision, and audio signal processing. We proceed\nto evaluate the successes and limitations of large model-based data\naugmentation across different scenarios. Concluding our review, we highlight\nprospective challenges and avenues for future exploration in the field of data\naugmentation. Our objective is to furnish researchers with critical insights,\nultimately contributing to the advancement of more sophisticated large models.\nWe consistently maintain the related open-source materials at:\nhttps://github.com/MLGroup-JLU/LLM-data-aug-survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chenlu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training and Diagnosing Knowledge Base Completion Models. (arXiv:2401.15439v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15439","description":"<p>In this work, we introduce and analyze an approach to knowledge transfer from\none collection of facts to another without the need for entity or relation\nmatching. The method works for both canonicalized knowledge bases and\nuncanonicalized or open knowledge bases, i.e., knowledge bases where more than\none copy of a real-world entity or relation may exist. The main contribution is\na method that can make use of large-scale pre-training on facts, which were\ncollected from unstructured text, to improve predictions on structured data\nfrom a specific domain. The introduced method is most impactful on small\ndatasets such as ReVerb20k, where a 6% absolute increase of mean reciprocal\nrank and 65% relative decrease of mean rank over the previously best method was\nachieved, despite not relying on large pre-trained models like Bert. To\nunderstand the obtained pre-trained models better, we then introduce a novel\ndataset for the analysis of pre-trained models for Open Knowledge Base\nCompletion, called Doge (Diagnostics of Open knowledge Graph Embeddings). It\nconsists of 6 subsets and is designed to measure multiple properties of a\npre-trained model: robustness against synonyms, ability to perform deductive\nreasoning, presence of gender stereotypes, consistency with reverse relations,\nand coverage of different areas of general knowledge. Using the introduced\ndataset, we show that the existing OKBC models lack consistency in the presence\nof synonyms and inverse relations and are unable to perform deductive\nreasoning. Moreover, their predictions often align with gender stereotypes,\nwhich persist even when presented with counterevidence. We additionally\ninvestigate the role of pre-trained word embeddings and demonstrate that\navoiding biased word embeddings is not a sufficient measure to prevent biased\nbehavior of OKBC models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocijan_V/0/1/0/all/0/1\">Vid Kocijan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_M/0/1/0/all/0/1\">Myeongjun Erik Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation. (arXiv:2401.15449v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15449","description":"<p>We evaluate the ability of Large Language Models (LLMs) to discern and\nexpress their internal knowledge state, a key factor in countering factual\nhallucination and ensuring reliable application of LLMs. We observe a robust\nself-awareness of internal knowledge state in LLMs, evidenced by over 85%\naccuracy in knowledge probing. However, LLMs often fail to express their\ninternal knowledge during generation, leading to factual hallucinations. We\ndevelop an automated hallucination annotation tool, Dreamcatcher, which merges\nknowledge probing and consistency checking methods to rank factual preference\ndata. Using knowledge preference as reward, We propose a Reinforcement Learning\nfrom Knowledge Feedback (RLKF) training framework, leveraging reinforcement\nlearning to enhance the factuality and honesty of LLMs. Our experiments across\nmultiple models show that RLKF training effectively enhances the ability of\nmodels to utilize their internal knowledge state, boosting performance in a\nvariety of knowledge-based and honesty-related tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuxin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhuoyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geographic Adaptation of Pretrained Language Models. (arXiv:2203.08565v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08565","description":"<p>While pretrained language models (PLMs) have been shown to possess a plethora\nof linguistic knowledge, the existing body of research has largely neglected\nextralinguistic knowledge, which is generally difficult to obtain by\npretraining on text alone. Here, we contribute to closing this gap by examining\ngeolinguistic knowledge, i.e., knowledge about geographic variation in\nlanguage. We introduce geoadaptation, an intermediate training step that\ncouples language modeling with geolocation prediction in a multi-task learning\nsetup. We geoadapt four PLMs, covering language groups from three geographic\nareas, and evaluate them on five different tasks: fine-tuned (i.e., supervised)\ngeolocation prediction, zero-shot (i.e., unsupervised) geolocation prediction,\nfine-tuned language identification, zero-shot language identification, and\nzero-shot prediction of dialect features. Geoadaptation is very successful at\ninjecting geolinguistic knowledge into the PLMs: the geoadapted PLMs\nconsistently outperform PLMs adapted using only language modeling (by\nespecially wide margins on zero-shot prediction tasks), and we obtain new\nstate-of-the-art results on two benchmarks for geolocation prediction and\nlanguage identification. Furthermore, we show that the effectiveness of\ngeoadaptation stems from its ability to geographically retrofit the\nrepresentation space of the PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_V/0/1/0/all/0/1\">Valentin Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ljubesic_N/0/1/0/all/0/1\">Nikola Ljube&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1\">Janet B. Pierrehumbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MiniDisc: Minimal Distillation Schedule for Language Model Compression. (arXiv:2205.14570v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14570","description":"<p>Recent studies have uncovered that language model distillation is less\neffective when facing a large capacity gap between the teacher and the student,\nand introduced teacher assistant-based distillation to bridge the gap. As a\nconnection, the scale and the performance of the teacher assistant is of vital\nimportance to bring the knowledge from the teacher to the student. However,\nexisting teacher assistant-based methods require maximally many trials before\nscheduling an optimal teacher assistant. To this end, we propose a minimal\ndistillation schedule (MiniDisc) for scheduling the optimal teacher assistant\nin minimally one trial. In particular, motivated by the finding that the\nperformance of the student is positively correlated to the scale-performance\ntradeoff of the teacher assistant, MiniDisc is designed with a\n$\\lambda$-tradeoff to measure the optimality of the teacher assistant without\ntrial distillation to the student. MiniDisc then can schedule the optimal\nteacher assistant with the best $\\lambda$-tradeoff in a sandwich framework.\nMiniDisc is evaluated with an extensive set of experiments on GLUE.\nExperimental results demonstrate the improved efficiency our MiniDisc compared\nto several state-of-the-art baselines. We further apply MiniDisc to a language\nmodel with billions of parameters and show its scalability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiahao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles. (arXiv:2207.01079v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.01079","description":"<p>A crucial component in the curation of KB for a scientific domain (e.g.,\nmaterials science, foods &amp; nutrition, fuels) is information extraction from\ntables in the domain's published research articles. To facilitate research in\nthis direction, we define a novel NLP task of extracting compositions of\nmaterials (e.g., glasses) from tables in materials science papers. The task\ninvolves solving several challenges in concert, such as tables that mention\ncompositions have highly varying structures; text in captions and full paper\nneeds to be incorporated along with data in tables; and regular languages for\nnumbers, chemical compounds and composition expressions must be integrated into\nthe model. We release a training dataset comprising 4,408 distantly supervised\ntables, along with 1,475 manually annotated dev and test tables. We also\npresent a strong baseline DISCOMAT, that combines multiple graph neural\nnetworks with several task-specific regular expressions, features, and\nconstraints. We show that DISCOMAT outperforms recent table processing\narchitectures by significant margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanishq Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohd Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khatsuriya_D/0/1/0/all/0/1\">Devanshi Khatsuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hira_K/0/1/0/all/0/1\">Kausik Hira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">N. M. Anoop Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Relation between Sensitivity and Accuracy in In-context Learning. (arXiv:2209.07661v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.07661","description":"<p>In-context learning (ICL) suffers from oversensitivity to the prompt, making\nit unreliable in real-world scenarios. We study the sensitivity of ICL with\nrespect to multiple perturbation types. First, we find that label bias obscures\nthe true sensitivity, and therefore prior work may have significantly\nunderestimated ICL sensitivity. Second, we observe a strong negative\ncorrelation between ICL sensitivity and accuracy: predictions sensitive to\nperturbations are less likely to be correct. Motivated by these findings, we\npropose \\textsc{SenSel}, a few-shot selective prediction method that abstains\nfrom sensitive predictions. Experiments on ten classification datasets show\nthat \\textsc{SenSel} consistently outperforms two commonly used\nconfidence-based and entropy-based baselines on abstention decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graphemic Normalization of the Perso-Arabic Script. (arXiv:2210.12273v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12273","description":"<p>Since its original appearance in 1991, the Perso-Arabic script representation\nin Unicode has grown from 169 to over 440 atomic isolated characters spread\nover several code pages representing standard letters, various diacritics and\npunctuation for the original Arabic and numerous other regional orthographic\ntraditions. This paper documents the challenges that Perso-Arabic presents\nbeyond the best-documented languages, such as Arabic and Persian, building on\nearlier work by the expert community. We particularly focus on the situation in\nnatural language processing (NLP), which is affected by multiple, often\nneglected, issues such as the use of visually ambiguous yet canonically\nnonequivalent letters and the mixing of letters from different orthographies.\nAmong the contributing conflating factors are the lack of input methods, the\ninstability of modern orthographies, insufficient literacy, and loss or lack of\northographic tradition. We evaluate the effects of script normalization on\neight languages from diverse language families in the Perso-Arabic script\ndiaspora on machine translation and statistical language modeling tasks. Our\nresults indicate statistically significant improvements in performance in most\nconditions for all the languages considered when normalization is applied. We\nargue that better understanding and representation of Perso-Arabic script\nvariation within regional orthographic traditions, where those are present, is\ncrucial for further progress of modern computational NLP techniques especially\nfor languages with a paucity of resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doctor_R/0/1/0/all/0/1\">Raiomond Doctor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutkin_A/0/1/0/all/0/1\">Alexander Gutkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johny_C/0/1/0/all/0/1\">Cibu Johny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roark_B/0/1/0/all/0/1\">Brian Roark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sproat_R/0/1/0/all/0/1\">Richard Sproat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compressing Transformer-based self-supervised models for speech processing. (arXiv:2211.09949v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.09949","description":"<p>Despite the success of Transformers in self- supervised learning with\napplications to various downstream tasks, the computational cost of training\nand inference remains a major challenge for applying these models to a wide\nspectrum of devices. Several isolated attempts have been made to compress\nTransformers, but the settings and metrics are different across studies.\nTrade-off at various compression rates are also largely missing in prior work,\nmaking it difficult to compare compression techniques. In this work, we aim to\nprovide context for the isolated results, studying several commonly used\ncompression techniques, including weight pruning, head pruning, low-rank\napproximation, and knowledge distillation. We report trade- off at various\ncompression rate, including wall-clock time, the number of parameters, and the\nnumber of multiply-accumulate operations. Our results show that compared to\nrecent approaches, basic compression techniques are strong baselines. We\nfurther present several applications of our results, revealing properties of\nTransformers, such as the significance of diagonal attention heads. In\naddition, our results lead to a simple combination of compression techniques\nthat improves trade-off over recent approaches. We hope the results would\npromote more diverse comparisons among model compression techniques and promote\nthe use of model compression as a tool for analyzing models. Our code of\ncompressing speech self-supervised model is available at\nhttps://github.com/nervjack2/Speech-SSL-Compression/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tzu-Quan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tsung-Huan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chun-Yao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuang-Ming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Tzu-hsun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. (arXiv:2301.12132v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12132","description":"<p>Large pretrained language models are widely used in downstream NLP tasks via\ntask-specific fine-tuning, but such procedures can be costly. Recently,\nParameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task\nperformance while updating much fewer parameters than full model fine-tuning\n(FFT). However, it is non-trivial to make informed design choices on the PEFT\nconfigurations, such as their architecture, the number of tunable parameters,\nand even the layers in which the PEFT modules are inserted. Consequently, it is\nhighly likely that the current, manually designed configurations are suboptimal\nin terms of their performance-efficiency trade-off. Inspired by advances in\nneural architecture search, we propose AutoPEFT for automatic PEFT\nconfiguration selection: we first design an expressive configuration search\nspace with multiple representative PEFT modules as building blocks. Using\nmulti-objective Bayesian optimisation in a low-cost setup, we then discover a\nPareto-optimal set of configurations with strong performance-cost trade-offs\nacross different numbers of parameters that are also highly transferable across\ndifferent tasks. Empirically, on GLUE and SuperGLUE tasks, we show that\nAutoPEFT-discovered configurations significantly outperform existing PEFT\nmethods and are on par or better than FFT without incurring substantial\ntraining efficiency costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Han Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xingchen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Reddit Users with Depression Using a Hybrid Neural Network SBERT-CNN. (arXiv:2302.02759v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.02759","description":"<p>Depression is a widespread mental health issue, affecting an estimated 3.8%\nof the global population. It is also one of the main contributors to disability\nworldwide. Recently it is becoming popular for individuals to use social media\nplatforms (e.g., Reddit) to express their difficulties and health issues (e.g.,\ndepression) and seek support from other users in online communities. It opens\ngreat opportunities to automatically identify social media users with\ndepression by parsing millions of posts for potential interventions. Deep\nlearning methods have begun to dominate in the field of machine learning and\nnatural language processing (NLP) because of their ease of use, efficient\nprocessing, and state-of-the-art results on many NLP tasks. In this work, we\npropose a hybrid deep learning model which combines a pretrained sentence BERT\n(SBERT) and convolutional neural network (CNN) to detect individuals with\ndepression with their Reddit posts. The sentence BERT is used to learn the\nmeaningful representation of semantic information in each post. CNN enables the\nfurther transformation of those embeddings and the temporal identification of\nbehavioral patterns of users. We trained and evaluated the model performance to\nidentify Reddit users with depression by utilizing the Self-reported Mental\nHealth Diagnoses (SMHD) data. The hybrid deep learning model achieved an\naccuracy of 0.86 and an F1 score of 0.86 and outperformed the state-of-the-art\ndocumented result (F1 score of 0.79) by other machine learning models in the\nliterature. The results show the feasibility of the hybrid model to identify\nindividuals with depression. Although the hybrid model is validated to detect\ndepression with Reddit posts, it can be easily tuned and applied to other text\nclassification tasks and different clinical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ren Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Sunyang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_N/0/1/0/all/0/1\">Nansu Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Ming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models. (arXiv:2302.04914v2 [cond-mat.mtrl-sci] UPDATED)","link":"http://arxiv.org/abs/2302.04914","description":"<p>Accurate and comprehensive material databases extracted from research papers\nare critical for materials science and engineering but require significant\nhuman effort to develop. In this paper we present a simple method of extracting\nmaterials data from full texts of research papers suitable for quickly\ndeveloping modest-sized databases. The method requires minimal to no coding,\nprior knowledge about the extracted property, or model training, and provides\nhigh recall and almost perfect precision in the resultant database. The method\nis fully automated except for one human-assisted step, which typically requires\njust a few hours of human labor. The method builds on top of natural language\nprocessing and large general language models but can work with almost any such\nmodel. The language models GPT-3/3.5, bart and DeBERTaV3 are evaluated here for\ncomparison. We provide a detailed detailed analysis of the methods performance\nin extracting bulk modulus data, obtaining up to 90% precision at 96% recall,\ndepending on the amount of human effort involved. We then demonstrate the\nmethods broader effectiveness by developing a database of critical cooling\nrates for metallic glasses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Polak_M/0/1/0/all/0/1\">Maciej P. Polak</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Modi_S/0/1/0/all/0/1\">Shrey Modi</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Latosinska_A/0/1/0/all/0/1\">Anna Latosinska</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zhang_J/0/1/0/all/0/1\">Jinming Zhang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Wang_C/0/1/0/all/0/1\">Ching-Wen Wang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Wang_S/0/1/0/all/0/1\">Shanonan Wang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Hazra_A/0/1/0/all/0/1\">Ayan Deep Hazra</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Morgan_D/0/1/0/all/0/1\">Dane Morgan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Like a Good Nearest Neighbor: Practical Content Moderation and Text Classification. (arXiv:2302.08957v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08957","description":"<p>Few-shot text classification systems have impressive capabilities but are\ninfeasible to deploy and use reliably due to their dependence on prompting and\nbillion-parameter language models. SetFit (Tunstall et al., 2022) is a recent,\npractical approach that fine-tunes a Sentence Transformer under a contrastive\nlearning paradigm and achieves similar results to more unwieldy systems.\nInexpensive text classification is important for addressing the problem of\ndomain drift in all classification tasks, and especially in detecting harmful\ncontent, which plagues social media platforms. Here, we propose Like a Good\nNearest Neighbor (LaGoNN), a modification to SetFit that introduces no\nlearnable parameters but alters input text with information from its nearest\nneighbor, for example, the label and text, in the training data, making novel\ndata appear similar to an instance on which the model was optimized. LaGoNN is\neffective at flagging undesirable content and text classification, and improves\nthe performance of SetFit. To demonstrate the value of LaGoNN, we conduct a\nthorough study of text classification systems in the context of content\nmoderation under four label distributions, and in general and multilingual\nclassification settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bates_L/0/1/0/all/0/1\">Luke Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01295","description":"<p>Cross-lingual transfer of language models trained on high-resource languages\nlike English has been widely studied for many NLP tasks, but focus on\nconversational tasks has been rather limited. This is partly due to the high\ncost of obtaining non-English conversational data, which results in limited\ncoverage. In this work, we introduce XSGD for cross-lingual alignment\npretraining, a parallel and large-scale multilingual conversation dataset that\nwe created by translating the English-only Schema-Guided Dialogue (SGD) dataset\n(Rastogi et al., 2020) into 105 other languages. XSGD contains approximately\n330k utterances per language. To facilitate aligned cross-lingual\nrepresentations, we develop an efficient prompt-tuning-based method for\nlearning alignment prompts. We also investigate two different classifiers:\nNLI-based and vanilla classifiers, and test cross-lingual capability enabled by\nthe aligned prompts. We evaluate our model's cross-lingual generalization\ncapabilities on two conversation tasks: slot-filling and intent classification.\nOur results demonstrate the strong and efficient modeling ability of NLI-based\nclassifiers and the large cross-lingual transfer improvements achieved by our\naligned prompts, particularly in few-shot settings. In addition, we highlight\nthe nice results of our approach compared to LLMs such as text-davinci-003 and\nChatGPT in both zero-shot and few-shot settings. While LLMs exhibit impressive\nperformance in English, their cross-lingual capabilities in other languages,\nparticularly low-resource languages, are limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1\">Lifu Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions. (arXiv:2304.14402v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14402","description":"<p>Large language models (LLMs) with instruction fine-tuning demonstrate\nsuperior generative capabilities. However, these models are resource-intensive.\nTo alleviate this issue, we explore distilling knowledge from instruction-tuned\nLLMs into much smaller ones. To this end, we carefully develop a large set of\n2.58M instructions based on both existing and newly-generated instructions. In\naddition to being sizable, we design our instructions to cover a broad set of\ntopics to ensure diversity. Extensive analysis of our instruction dataset\nconfirms its diversity, and we generate responses for these instructions using\ngpt-3.5-turbo. Leveraging these instructions, we fine-tune a diverse herd of\nmodels, collectively referred to as LaMini-LM, which includes models from both\nthe encoder-decoder and decoder-only families, with varying sizes. We evaluate\nthe performance of our models using automatic metrics on 15 different natural\nlanguage processing (NLP) benchmarks, as well as through human assessment. The\nresults demonstrate that our proposed LaMini-LM models are comparable to\ncompetitive baselines, while being much smaller in size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waheed_A/0/1/0/all/0/1\">Abdul Waheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Building the Federated GPT: Federated Instruction Tuning. (arXiv:2305.05644v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05644","description":"<p>While \"instruction-tuned\" generative large language models (LLMs) have\ndemonstrated an impressive ability to generalize to new tasks, the training\nphases heavily rely on large amounts of diverse and high-quality instruction\ndata (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data,\nespecially when it comes to human-written data, can pose significant challenges\nboth in terms of cost and accessibility. Moreover, concerns related to privacy\ncan further limit access to such data, making the process of obtaining it a\ncomplex and nuanced undertaking. Consequently, this hinders the generality of\nthe tuned models and may restrict their effectiveness in certain contexts. To\ntackle this issue, our study introduces a new approach called Federated\nInstruction Tuning (FedIT), which leverages federated learning (FL) as the\nlearning framework for the instruction tuning of LLMs. This marks the first\nexploration of FL-based instruction tuning for LLMs. This is especially\nimportant since text data is predominantly generated by end users. Therefore,\nit is imperative to design and adapt FL approaches to effectively leverage\nthese users' diverse instructions stored on local devices, while preserving\nprivacy and ensuring data security. In the current paper, by conducting widely\nused GPT-4 auto-evaluation, we demonstrate that by exploiting the heterogeneous\nand diverse sets of instructions on the client's end with the proposed\nframework FedIT, we improved the performance of LLMs compared to centralized\ntraining with only limited local instructions. Further, in this paper, we\ndeveloped a Github repository named Shepherd. This repository offers a\nfoundational framework for exploring federated fine-tuning of LLMs using\nheterogeneous instructions across diverse categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahidian_S/0/1/0/all/0/1\">Saeed Vahidian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_M/0/1/0/all/0/1\">Martin Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yufan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiran Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples. (arXiv:2305.07984v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07984","description":"<p>Detecting negatives (such as non-entailment relationships, unanswerable\nquestions, and false claims) is an important and challenging aspect of many\nnatural language understanding tasks. Though manually collecting challenging\nnegative examples can help models detect them, it is both costly and\ndomain-specific. In this work, we propose Self-labeled Counterfactuals for\nExtrapolating to Negative Examples (SCENE), an automatic method for\nsynthesizing training data that greatly improves models' ability to detect\nchallenging negative examples. In contrast with standard data augmentation,\nwhich synthesizes new examples for existing labels, SCENE can synthesize\nnegative examples zero-shot from only positive ones. Given a positive example,\nSCENE perturbs it with a mask infilling model, then determines whether the\nresulting example is negative based on a self-training heuristic. With access\nto only answerable training examples, SCENE can close 69.6% of the performance\ngap on SQuAD 2.0, a dataset where half of the evaluation examples are\nunanswerable, compared to a model trained on SQuAD 2.0. Our method also extends\nto boolean question answering and recognizing textual entailment, and improves\ngeneralization from SQuAD to ACE-whQA, an out-of-domain extractive QA\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Deqing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godbole_A/0/1/0/all/0/1\">Ameya Godbole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Leverage External Knowledge to Extend Clinical Insight Beyond Language Boundaries. (arXiv:2305.10163v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10163","description":"<p>Objectives: Large Language Models (LLMs) such as ChatGPT and Med-PaLM have\nexcelled in various medical question-answering tasks. However, these\nEnglish-centric models encounter challenges in non-English clinical settings,\nprimarily due to limited clinical knowledge in respective languages, a\nconsequence of imbalanced training corpora. We systematically evaluate LLMs in\nthe Chinese medical context and develop a novel in-context learning framework\nto enhance their performance.\n</p>\n<p>Materials and Methods: The latest China National Medical Licensing\nExamination (CNMLE-2022) served as the benchmark. We collected 53 medical books\nand 381,149 medical questions to construct the medical knowledge base and\nquestion bank. The proposed Knowledge and Few-shot Enhancement In-context\nLearning (KFE) framework leverages the in-context learning ability of LLMs to\nintegrate diverse external clinical knowledge sources. We evaluated KFE with\nChatGPT(GPT3.5), GPT4, Baichuan2-7b, and Baichuan2-13B in CNMLE-2022 and\nfurther investigated the effectiveness of different pathways for incorporating\nLLMs with medical knowledge from seven distinct perspectives.\n</p>\n<p>Results: Directly applying ChatGPT failed to qualify for the CNMLE-2022 at a\nscore of 51. Cooperated with the KFE framework, the LLMs with varying sizes\nyielded consistent and significant improvements. The ChatGPT's performance\nsurged to 70.04 and GPT-4 achieved the highest score of 82.59. This surpasses\nthe qualification threshold (60) and exceeds the average human score of 68.70,\naffirming the effectiveness and robustness of the framework. It also enabled a\nsmaller Baichuan2-13B to pass the examination, showcasing the great potential\nin low-resource settings. This study shed light on the optimal practices to\nenhance the capabilities of LLMs in non-English medical scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiageng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhaopeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach. (arXiv:2305.11789v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11789","description":"<p>Humans work together to solve common problems by having discussions,\nexplaining, and agreeing or disagreeing with each other. Similarly, if a system\ncan have discussions with humans when solving tasks, it can improve the\nsystem's performance and reliability. In previous research on explainability,\nit has only been possible for the system to make predictions and for humans to\nask questions about them rather than having a mutual exchange of opinions. This\nresearch aims to create a dataset and computational framework for systems that\ndiscuss and refine their predictions through dialogue. Through experiments, we\nshow that the proposed system can have beneficial discussions with humans\nimproving the accuracy by up to 25 points in the natural language inference\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_M/0/1/0/all/0/1\">Masahiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EnCore: Fine-Grained Entity Typing by Pre-Training Entity Encoders on Coreference Chains. (arXiv:2305.12924v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12924","description":"<p>Entity typing is the task of assigning semantic types to the entities that\nare mentioned in a text. In the case of fine-grained entity typing (FET), a\nlarge set of candidate type labels is considered. Since obtaining sufficient\namounts of manual annotations is then prohibitively expensive, FET models are\ntypically trained using distant supervision. In this paper, we propose to\nimprove on this process by pre-training an entity encoder such that embeddings\nof coreferring entities are more similar to each other than to the embeddings\nof other entities. The main problem with this strategy, which helps to explain\nwhy it has not previously been considered, is that predicted coreference links\nare often too noisy. We show that this problem can be addressed by using a\nsimple trick: we only consider coreference links that are predicted by two\ndifferent off-the-shelf systems. With this prudent use of coreference links,\nour pre-training strategy allows us to improve the state-of-the-art in\nbenchmarks on fine-grained entity typing, as well as traditional entity\nextraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mtumbuka_F/0/1/0/all/0/1\">Frank Mtumbuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1\">Steven Schockaert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CEO: Corpus-based Open-Domain Event Ontology Induction. (arXiv:2305.13521v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13521","description":"<p>Existing event-centric NLP models often only apply to the pre-defined\nontology, which significantly restricts their generalization capabilities. This\npaper presents CEO, a novel Corpus-based Event Ontology induction model to\nrelax the restriction imposed by pre-defined event ontologies. Without direct\nsupervision, CEO leverages distant supervision from available summary datasets\nto detect corpus-wise salient events and exploits external event knowledge to\nforce events within a short distance to have close embeddings. Experiments on\nthree popular event datasets show that the schema induced by CEO has better\ncoverage and higher accuracy than previous methods. Moreover, CEO is the first\nevent ontology induction model that can induce a hierarchical event ontology\nwith meaningful names on eleven open-domain corpora, making the induced schema\nmore trustworthy and easier to be further curated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Nan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mPLM-Sim: Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models. (arXiv:2305.13684v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13684","description":"<p>Recent multilingual pretrained language models (mPLMs) have been shown to\nencode strong language-specific signals, which are not explicitly provided\nduring pretraining. It remains an open question whether it is feasible to\nemploy mPLMs to measure language similarity, and subsequently use the\nsimilarity results to select source languages for boosting cross-lingual\ntransfer. To investigate this, we propose mPLMSim, a language similarity\nmeasure that induces the similarities across languages from mPLMs using\nmulti-parallel corpora. Our study shows that mPLM-Sim exhibits moderately high\ncorrelations with linguistic similarity measures, such as lexicostatistics,\ngenealogical language family, and geographical sprachbund. We also conduct a\ncase study on languages with low correlation and observe that mPLM-Sim yields\nmore accurate similarity results. Additionally, we find that similarity results\nvary across different mPLMs and different layers within an mPLM. We further\ninvestigate whether mPLMSim is effective for zero-shot cross-lingual transfer\nby conducting experiments on both low-level syntactic tasks and high-level\nsemantic tasks. The experimental results demonstrate that mPLM-Sim is capable\nof selecting better source languages than linguistic measures, resulting in a\n1%-2% improvement in zero-shot cross-lingual transfer performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Peiqin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chengzhi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvGQR: Generative Query Reformulation for Conversational Search. (arXiv:2305.15645v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2305.15645","description":"<p>In conversational search, the user's real search intent for the current turn\nis dependent on the previous conversation history. It is challenging to\ndetermine a good search query from the whole conversation context. To avoid the\nexpensive re-training of the query encoder, most existing methods try to learn\na rewriting model to de-contextualize the current query by mimicking the manual\nquery rewriting. However, manually rewritten queries are not always the best\nsearch queries. Training a rewriting model on them would limit the model's\nability to produce good search queries. Another useful hint is the potential\nanswer to the question. In this paper, we propose ConvGQR, a new framework to\nreformulate conversational queries based on generative pre-trained language\nmodels (PLMs), one for query rewriting and another for generating potential\nanswers. By combining both, ConvGQR can produce better search queries. In\naddition, to relate query reformulation to retrieval performance, we propose a\nknowledge infusion mechanism to optimize both query reformulation and\nretrieval. Extensive experiments on four conversational search datasets\ndemonstrate the effectiveness of ConvGQR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_F/0/1/0/all/0/1\">Fengran Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1\">Kelong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yihong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaiyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Give Me More Details: Improving Fact-Checking with Latent Retrieval. (arXiv:2305.16128v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16128","description":"<p>Evidence plays a crucial role in automated fact-checking. When verifying\nreal-world claims, existing fact-checking systems either assume the evidence\nsentences are given or use the search snippets returned by the search engine.\nSuch methods ignore the challenges of collecting evidence and may not provide\nsufficient information to verify real-world claims. Aiming at building a better\nfact-checking system, we propose to incorporate full text from source documents\nas evidence and introduce two enriched datasets. The first one is a\nmultilingual dataset, while the second one is monolingual (English). We further\ndevelop a latent variable model to jointly extract evidence sentences from\ndocuments and perform claim verification. Experiments indicate that including\nsource documents can provide sufficient contextual clues even when gold\nevidence sentences are not annotated. The proposed system is able to achieve\nsignificant improvements upon best-reported models under different settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junzhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the Robustness of NLP Models to Domain Shifts. (arXiv:2306.00168v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00168","description":"<p>Existing research on Domain Robustness (DR) suffers from disparate setups,\nlack of task variety, and scarce research on recent capabilities such as\nfew-shot learning. Furthermore, we claim that the common practice of measuring\nDR might further obscure the picture. Current research focuses on challenge\nsets and relies solely on the Source Drop (SD): Using the source in-domain\nperformance as a reference point for degradation. However, the Target Drop\n(TD), which measures degradation from the target in-domain performance, should\nbe used as a complementary point of view. In this study, we developed a\nbenchmark comprised of seven NLP tasks, including classification, QA, and\ngeneration. Our benchmark focuses on natural topical domain shifts and enables\nmeasuring both the SD and the TD. Our comprehensive study, involving over\n14,000 domain shifts across 18 fine-tuned and few-shot models, shows that both\nmodel types suffer from drops upon domain shifts. While fine-tuned models excel\nin-domain, few-shot LLMs often surpass them cross-domain, showing better\nrobustness. In addition, we found that a large SD can be explained by shifting\nto a harder domain rather than by a genuine DR challenge. Thus, the TD is a\nmore reliable metric for assessing DR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calderon_N/0/1/0/all/0/1\">Nitay Calderon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porat_N/0/1/0/all/0/1\">Naveh Porat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1\">Eyal Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chapanin_A/0/1/0/all/0/1\">Alexander Chapanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gekhman_Z/0/1/0/all/0/1\">Zorik Gekhman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oved_N/0/1/0/all/0/1\">Nadav Oved</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalumov_V/0/1/0/all/0/1\">Vitaly Shalumov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Corrections of Zipf's and Heaps' Laws Derived from Hapax Rate Models. (arXiv:2307.12896v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.12896","description":"<p>The article introduces corrections to Zipf's and Heaps' laws based on\nsystematic models of the proportion of hapaxes, i.e., words that occur once.\nThe derivation rests on two assumptions: The first one is the standard urn\nmodel which predicts that marginal frequency distributions for shorter texts\nlook as if word tokens were sampled blindly from a given longer text. The\nsecond assumption posits that the hapax rate is a simple function of the text\nlength. Four such functions are discussed: the constant model, the Davis model,\nthe linear model, and the logistic model. It is shown that the logistic model\nyields the best fit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Debowski_L/0/1/0/all/0/1\">&#x141;ukasz D&#x119;bowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data. (arXiv:2307.14385v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.14385","description":"<p>Advances in large language models (LLMs) have empowered a variety of\napplications. However, there is still a significant gap in research when it\ncomes to understanding and enhancing the capabilities of LLMs in the field of\nmental health. In this work, we present a comprehensive evaluation of multiple\nLLMs on various mental health prediction tasks via online text data, including\nAlpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of\nexperiments, covering zero-shot prompting, few-shot prompting, and instruction\nfine-tuning. The results indicate a promising yet limited performance of LLMs\nwith zero-shot and few-shot prompt designs for mental health tasks. More\nimportantly, our experiments show that instruction finetuning can significantly\nboost the performance of LLMs for all tasks simultaneously. Our best-finetuned\nmodels, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of\nGPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of\nGPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the\nstate-of-the-art task-specific language model. We also conduct an exploratory\ncase study on LLMs' capability on mental health reasoning tasks, illustrating\nthe promising capability of certain models such as GPT-4. We summarize our\nfindings into a set of action guidelines for potential methods to enhance LLMs'\ncapability for mental health tasks. Meanwhile, we also emphasize the important\nlimitations before achieving deployability in real-world mental health\nsettings, such as known racial and gender bias. We highlight the important\nethical risks accompanying this line of research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuhai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuanzhe Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1\">Saadia Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendler_J/0/1/0/all/0/1\">James Hendler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1\">Marzyeh Ghassemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1\">Anind K. Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the meanings of function words from grounded language using a visual question answering model. (arXiv:2308.08628v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.08628","description":"<p>Interpreting a seemingly-simple function word like \"or\", \"behind\", or \"more\"\ncan require logical, numerical, and relational reasoning. How are such words\nlearned by children? Prior acquisition theories have often relied on positing a\nfoundation of innate knowledge. Yet recent neural-network based visual question\nanswering models apparently can learn to use function words as part of\nanswering questions about complex visual scenes. In this paper, we study what\nthese models learn about function words, in the hope of better understanding\nhow the meanings of these words can be learnt by both models and children. We\nshow that recurrent models trained on visually grounded language learn gradient\nsemantics for function words requiring spacial and numerical reasoning.\nFurthermore, we find that these models can learn the meanings of logical\nconnectives \"and\" and \"or\" without any prior knowledge of logical reasoning, as\nwell as early evidence that they are sensitive to alternative expressions when\ninterpreting language. Finally, we show that word learning difficulty is\ndependent on frequency in models' input. Our findings offer proof-of-concept\nevidence that it is possible to learn the nuanced interpretations of function\nwords in visually grounded context by using non-symbolic general statistical\nlearning algorithms, without any prior knowledge of linguistic meaning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Portelance_E/0/1/0/all/0/1\">Eva Portelance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_M/0/1/0/all/0/1\">Michael C. Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation. (arXiv:2308.10335v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.10335","description":"<p>Recently, the large language models (LLMs) have shown extraordinary ability\nin understanding natural language and generating programming code. It has been\na common practice of software engineers to consult LLMs when encountering\ncoding questions. Although efforts have been made to avoid syntax errors and\nalign the code with the intended semantics, the reliability and robustness of\nthe code generationfrom LLMs have not yet been thoroughly studied. The\nexecutable code is not equivalent to the reliable and robust code, especially\nin the context of real-world software development. The misuse of APIs in the\ngenerated code could lead to severe problem, such as resource leaks, program\ncrashes. To make things worse, the users of LLM code generation services are\nactually the developers that are most vulnerable to these code that seems right\n-- They are always novice developers that are not familiar with the APIs that\nLLMs generate code for them. Therefore, they could hardly tell the misuse in\nthe code generated by LLMs, which further facilitates the incorrect code\napplied in real-world software. Existing code evaluation benchmark and datasets\nfocus on crafting small tasks such as programming questions in coding\ninterviews, which however deviates from the problem that developers would ask\nLLM for real-world coding help. To fill the missing piece, in this work, we\npropose a dataset RobustAPI for evaluating the reliability and robustness of\ncode generated by LLMs. We collect 1208 coding questions from StackOverflow on\n24 representative Java APIs. We summarize thecommon misuse patterns of these\nAPIs and evaluate them oncurrent popular LLMs. The evaluation results show that\nevenfor GPT-4, 62% of the generated code contains API misuses,which would cause\nunexpected consequences if the code isintroduced into real-world software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Li Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification. (arXiv:2309.04174v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.04174","description":"<p>Prompt-based classification adapts tasks to a cloze question format utilizing\nthe [MASK] token and the filled tokens are then mapped to labels through\npre-defined verbalizers. Recent studies have explored the use of verbalizer\nembeddings to reduce labor in this process. However, all existing studies\nrequire a tuning process for either the pre-trained models or additional\ntrainable embeddings. Meanwhile, the distance between high-dimensional\nverbalizer embeddings should not be measured by Euclidean distance due to the\npotential for non-linear manifolds in the representation space. In this study,\nwe propose a tuning-free manifold-based space re-embedding method called\nLocally Linear Embedding with Intra-class Neighborhood Constraint (LLE-INC) for\nverbalizer embeddings, which preserves local properties within the same class\nas guidance for classification. Experimental results indicate that even without\ntuning any parameters, our LLE-INC is on par with automated verbalizers with\nparameter tuning. And with the parameter updating, our approach further\nenhances prompt-based tuning by up to 3.2%. Furthermore, experiments with the\nLLaMA-7B&amp;13B indicate that LLE-INC is an efficient tuning-free classification\napproach for the hyper-scale language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haochun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sendong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_N/0/1/0/all/0/1\">Nuwa Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Muzhen Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking through the learning plateaus of in-context learning in Transformer. (arXiv:2309.06054v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2309.06054","description":"<p>In-context learning, i.e., learning from context examples, is an impressive\nability of Transformer. Training Transformers to possess this in-context\nlearning skill is computationally intensive due to the occurrence of learning\nplateaus, which are periods within the training process where there is minimal\nor no enhancement in the model's in-context learning capability. To study the\nmechanism behind the learning plateaus, we conceptually seperate a component\nwithin the model's internal representation that is exclusively affected by the\nmodel's weights. We call this the \"weights component\", and the remainder is\nidentified as the \"context component\". By conducting meticulous and controlled\nexperiments on synthetic tasks, we note that the persistence of learning\nplateaus correlates with compromised functionality of the weights component.\nRecognizing the impaired performance of the weights component as a fundamental\nbehavior drives learning plateaus, we have developed three strategies to\nexpedite the learning of Transformers. The effectiveness of these strategies is\nfurther confirmed in natural language processing tasks. In conclusion, our\nresearch demonstrates the feasibility of cultivating a powerful in-context\nlearning ability within AI systems in an eco-friendly manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Hallucinations and Off-target Machine Translation with Source-Contrastive and Language-Contrastive Decoding. (arXiv:2309.07098v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.07098","description":"<p>Hallucinations and off-target translation remain unsolved problems in MT,\nespecially for low-resource languages and massively multilingual models. In\nthis paper, we introduce two related methods to mitigate these failure cases\nwith a modified decoding objective, without either requiring retraining or\nexternal models. In source-contrastive decoding, we search for a translation\nthat is probable given the correct input, but improbable given a random input\nsegment. In language-contrastive decoding, we search for a translation that is\nprobable, but improbable given the wrong language indicator token. Experiments\non the massively multilingual models M2M-100 (418M) and SMaLL-100 show that\nthese methods suppress hallucinations and off-target translations, reducing the\nnumber of translations with segment-level chrF2 below 10 by 67-83% on average,\nand the number of translations with oscillatory hallucinations by 75-92% on\naverage, across 57 tested translation directions. In a proof of concept on\nout-of-English translation, we also show that we can suppress off-target\ntranslations with large language models. We release our source code at\nhttps://github.com/ZurichNLP/ContraDecode.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vamvas_J/0/1/0/all/0/1\">Jannis Vamvas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1\">Alireza Mohammadshahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Media of Langue: The dictionary that visualizes Inter-Lingual Semantic Network/Space. (arXiv:2309.08609v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.08609","description":"<p>This paper introduces \"Media of Langue,\" a novel dictionary visualizing\nInter-lingual semantic network/space. Our proposed Inter-lingual semantic\nnetwork/space is formed solely from the accumulation of translation practices\nbetween two or more language systems, in contrast to existing semantic\nnetworks/spaces that explicitly use \"intra\"-lingual relations. By visualizing\nthis network/space for humans, an Inter-lingual dictionary can be realized that\npoints to the semantic place of many words at once with a chain of mutual\ntranslation, which also contains the functions of existing dictionaries such as\nbilingual and synonym dictionaries. We implemented and published this interface\nas a web application, focusing on seven language pairs. In this paper, we first\ndescribe Inter-lingual semantic network/space with its basic features and the\nway to develop it from bilingual corpora, then details the design of \"Media of\nLangue,\" with a quick analysis and illustrative examples of use cases. Our\nwebsite is www.media-of-langue.org. A demonstration video is available at\nhttps://youtu.be/98lXuX4yjsU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muramoto_G/0/1/0/all/0/1\">Goki Muramoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_A/0/1/0/all/0/1\">Atsuki Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyama_T/0/1/0/all/0/1\">Takayoshi Koyama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards LLM-guided Causal Explainability for Black-box Text Classifiers. (arXiv:2309.13340v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.13340","description":"<p>With the advent of larger and more complex deep learning models, such as in\nNatural Language Processing (NLP), model qualities like explainability and\ninterpretability, albeit highly desirable, are becoming harder challenges to\ntackle and solve. For example, state-of-the-art models in text classification\nare black-box by design. Although standard explanation methods provide some\ndegree of explainability, these are mostly correlation-based methods and do not\nprovide much insight into the model. The alternative of causal explainability\nis more desirable to achieve but extremely challenging in NLP due to a variety\nof reasons. Inspired by recent endeavors to utilize Large Language Models\n(LLMs) as experts, in this work, we aim to leverage the instruction-following\nand textual understanding capabilities of recent state-of-the-art LLMs to\nfacilitate causal explainability via counterfactual explanation generation for\nblack-box text classifiers. To do this, we propose a three-step pipeline via\nwhich, we use an off-the-shelf LLM to: (1) identify the latent or unobserved\nfeatures in the input text, (2) identify the input features associated with the\nlatent features, and finally (3) use the identified input features to generate\na counterfactual explanation. We experiment with our pipeline on multiple NLP\ntext classification datasets, with several recent LLMs, and present interesting\nand promising findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Amrita Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1\">Raha Moraffah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garland_J/0/1/0/all/0/1\">Joshua Garland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syllable-level lyrics generation from melody exploiting character-level language model. (arXiv:2310.00863v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.00863","description":"<p>The generation of lyrics tightly connected to accompanying melodies involves\nestablishing a mapping between musical notes and syllables of lyrics. This\nprocess requires a deep understanding of music constraints and semantic\npatterns at syllable-level, word-level, and sentence-level semantic meanings.\nHowever, pre-trained language models specifically designed at the syllable\nlevel are publicly unavailable. To solve these challenging issues, we propose\nto exploit fine-tuning character-level language models for syllable-level\nlyrics generation from symbolic melody. In particular, our method endeavors to\nincorporate linguistic knowledge of the language model into the beam search\nprocess of a syllable-level Transformer generator network. Additionally, by\nexploring ChatGPT-based evaluation for generated lyrics, along with human\nsubjective evaluation, we demonstrate that our approach enhances the coherence\nand correctness of the generated lyrics, eliminating the need to train\nexpensive new language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasocki_K/0/1/0/all/0/1\">Karol Lasocki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takasu_A/0/1/0/all/0/1\">Atsuhiro Takasu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. (arXiv:2310.01801v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.01801","description":"<p>In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Suyu Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource Languages Jailbreak GPT-4. (arXiv:2310.02446v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.02446","description":"<p>AI safety training and red-teaming of large language models (LLMs) are\nmeasures to mitigate the generation of unsafe content. Our work exposes the\ninherent cross-lingual vulnerability of these safety mechanisms, resulting from\nthe linguistic inequality of safety training data, by successfully\ncircumventing GPT-4's safeguard through translating unsafe English inputs into\nlow-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe\ntranslated inputs and provides actionable items that can get the users towards\ntheir harmful goals 79% of the time, which is on par with or even surpassing\nstate-of-the-art jailbreaking attacks. Other high-/mid-resource languages have\nsignificantly lower attack success rate, which suggests that the cross-lingual\nvulnerability mainly applies to low-resource languages. Previously, limited\ntraining on low-resource languages primarily affects speakers of those\nlanguages, causing technological disparities. However, our work highlights a\ncrucial shift: this deficiency now poses a risk to all LLMs users. Publicly\navailable translation APIs enable anyone to exploit LLMs' safety\nvulnerabilities. Therefore, our work calls for a more holistic red-teaming\nefforts to develop robust multilingual safeguards with wide language coverage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1\">Zheng-Xin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menghini_C/0/1/0/all/0/1\">Cristina Menghini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.07276","description":"<p>Recent advancements in biological research leverage the integration of\nmolecules, proteins, and natural language to enhance drug discovery. However,\ncurrent models exhibit several limitations, such as the generation of invalid\nmolecular SMILES, underutilization of contextual information, and equal\ntreatment of structured and unstructured knowledge. To address these issues, we\npropose $\\mathbf{BioT5}$, a comprehensive pre-training framework that enriches\ncross-modal integration in biology with chemical knowledge and natural language\nassociations. $\\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular\nrepresentations and extracts knowledge from the surrounding context of\nbio-entities in unstructured biological literature. Furthermore,\n$\\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge,\nleading to more effective utilization of information. After fine-tuning, BioT5\nshows superior performance across a wide range of tasks, demonstrating its\nstrong capability of capturing underlying relations and properties of\nbio-entities. Our code is available at\n$\\href{https://github.com/QizhiPei/BioT5}{Github}$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_Q/0/1/0/all/0/1\">Qizhi Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinhua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kehan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kaiyuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstructing Materials Tetrahedron: Challenges in Materials Information Extraction. (arXiv:2310.08383v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.08383","description":"<p>Discovery of new materials has a documented history of propelling human\nprogress for centuries and more. The behaviour of a material is a function of\nits composition, structure, and properties, which further depend on its\nprocessing and testing conditions. Recent developments in deep learning and\nnatural language processing have enabled information extraction at scale from\npublished literature such as peer-reviewed publications, books, and patents.\nHowever, this information is spread in multiple formats, such as tables, text,\nand images, and with little or no uniformity in reporting style giving rise to\nseveral machine learning challenges. Here, we discuss, quantify, and document\nthese challenges in automated information extraction (IE) from materials\nscience literature towards the creation of a large materials science knowledge\nbase. Specifically, we focus on IE from text and tables and outline several\nchallenges with examples. We hope the present work inspires researchers to\naddress the challenges in a coherent fashion, providing a fillip to IE towards\na materials knowledge base.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hira_K/0/1/0/all/0/1\">Kausik Hira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohd Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_D/0/1/0/all/0/1\">Dhruvil Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">N M Anoop Krishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Establishing Vocabulary Tests as a Benchmark for Evaluating Large Language Models. (arXiv:2310.14703v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.14703","description":"<p>Vocabulary tests, once a cornerstone of language modeling evaluation, have\nbeen largely overlooked in the current landscape of Large Language Models\n(LLMs) like Llama, Mistral, and GPT. While most LLM evaluation benchmarks focus\non specific tasks or domain-specific knowledge, they often neglect the\nfundamental linguistic aspects of language understanding and production. In\nthis paper, we advocate for the revival of vocabulary tests as a valuable tool\nfor assessing LLM performance. We evaluate seven LLMs using two vocabulary test\nformats across two languages and uncover surprising gaps in their lexical\nknowledge. These findings shed light on the intricacies of LLM word\nrepresentations, their learning mechanisms, and performance variations across\nmodels and languages. Moreover, the ability to automatically generate and\nperform vocabulary tests offers new opportunities to expand the approach and\nprovide a more complete picture of LLMs' language skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_G/0/1/0/all/0/1\">Gonzalo Mart&#xed;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conde_J/0/1/0/all/0/1\">Javier Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merino_Gomez_E/0/1/0/all/0/1\">Elena Merino-G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bermudez_Margaretto_B/0/1/0/all/0/1\">Beatriz Berm&#xfa;dez-Margaretto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_J/0/1/0/all/0/1\">Jos&#xe9; Alberto Hern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reviriego_P/0/1/0/all/0/1\">Pedro Reviriego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brysbaert_M/0/1/0/all/0/1\">Marc Brysbaert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Open Source Data Contamination Report for Large Language Models. (arXiv:2310.17589v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.17589","description":"<p>Data contamination in model evaluation has become increasingly prevalent with\nthe growing popularity of large language models. It allows models to \"cheat\"\nvia memorisation instead of displaying true capabilities. Therefore,\ncontamination analysis has become an crucial part of reliable model evaluation\nto validate results. However, existing contamination analysis is usually\nconducted internally by large language model developers and often lacks\ntransparency and completeness. This paper presents an extensive data\ncontamination report for over 15 popular large language models across six\npopular multiple-choice QA benchmarks. We also introduce an open-source\npipeline that enables the community to perform contamination analysis on\ncustomised data and models. Our experiments reveal varying contamination levels\nranging from 1\\% to 45\\% across benchmarks, with the contamination degree\nincreasing rapidly over time. Performance analysis of large language models\nindicates that data contamination does not necessarily lead to increased model\nmetrics: while significant accuracy boosts of up to 14\\% and 7\\% are observed\non contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is\nnoted on contaminated MMLU. We also find larger models seem able to gain more\nadvantages than smaller models on contaminated test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yucheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.01927","description":"<p>Linear Recurrence has proven to be a powerful tool for modeling long\nsequences efficiently. In this work, we show that existing models fail to take\nfull advantage of its potential. Motivated by this finding, we develop\nGateLoop, a foundational sequence model that generalizes linear recurrent\nmodels such as S4, S5, LRU and RetNet, by employing data-controlled state\ntransitions. Utilizing this theoretical advance, GateLoop empirically\noutperforms existing models for auto-regressive language modeling. Our method\ncomes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \\log_{2} l)$\nparallel mode making use of highly optimized associative scan implementations.\nFurthermore, we derive an $O(l^2)$ surrogate attention mode, revealing\nremarkable implications for Transformer and recently proposed architectures.\nSpecifically, we prove that our approach can be interpreted as providing\ndata-controlled relative-positional information to Attention. While many\nexisting models solely rely on data-controlled cumulative sums for context\naggregation, our findings suggest that incorporating data-controlled complex\ncumulative products may be a crucial step towards more powerful sequence\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katsch_T/0/1/0/all/0/1\">Tobias Katsch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction. (arXiv:2311.04507v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.04507","description":"<p>Emotion recognition is a crucial task for human conversation understanding.\nIt becomes more challenging with the notion of multimodal data, e.g., language,\nvoice, and facial expressions. As a typical solution, the global- and the local\ncontext information are exploited to predict the emotional label for every\nsingle sentence, i.e., utterance, in the dialogue. Specifically, the global\nrepresentation could be captured via modeling of cross-modal interactions at\nthe conversation level. The local one is often inferred using the temporal\ninformation of speakers or emotional shifts, which neglects vital factors at\nthe utterance level. Additionally, most existing approaches take fused features\nof multiple modalities in an unified input without leveraging modality-specific\nrepresentations. Motivating from these problems, we propose the Relational\nTemporal Graph Neural Network with Auxiliary Cross-Modality Interaction\n(CORECT), an novel neural network framework that effectively captures\nconversation-level cross-modality interactions and utterance-level temporal\ndependencies with the modality-specific manner for conversation understanding.\nExtensive experiments demonstrate the effectiveness of CORECT via its\nstate-of-the-art results on the IEMOCAP and CMU-MOSEI datasets for the\nmultimodal ERC task Implementation available at:\nhttps://github.com/leson502/CORECT\\_EMNLP2023\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cam-Van Thi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_A/0/1/0/all/0/1\">Anh-Tuan Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">The-Son Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kieu_H/0/1/0/all/0/1\">Hai-Dang Kieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc-Trong Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs. (arXiv:2311.04892v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.04892","description":"<p>Recent works have showcased the ability of LLMs to embody diverse personas in\ntheir responses, exemplified by prompts like 'You are Yoda. Explain the Theory\nof Relativity.' While this ability allows personalization of LLMs and enables\nhuman behavior simulation, its effect on LLMs' capabilities remains unclear. To\nfill this gap, we present the first extensive study of the unintended\nside-effects of persona assignment on the ability of LLMs to perform basic\nreasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse\npersonas (e.g. an Asian person) spanning 5 socio-demographic groups. Our\nexperiments unveil that LLMs harbor deep rooted bias against various\nsocio-demographics underneath a veneer of fairness. While they overtly reject\nstereotypes when explicitly asked ('Are Black people less skilled at\nmathematics?'), they manifest stereotypical and erroneous presumptions when\nasked to answer questions while adopting a persona. These can be observed as\nabstentions in responses, e.g., 'As a Black person, I can't answer this\nquestion as it requires math knowledge', and generally result in a substantial\nperformance drop. Our experiments with ChatGPT-3.5 show that this bias is\nubiquitous - 80% of our personas demonstrate bias; it is significant - some\ndatasets show performance drops of 70%+; and can be especially harmful for\ncertain groups - some personas suffer statistically significant drops on 80%+\nof the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with\nGPT-4-Turbo showing the least but still a problematic amount of bias (evident\nin 42% of the personas). Further analysis shows that these persona-induced\nerrors can be hard-to-discern and hard-to-avoid. Our findings serve as a\ncautionary tale that the practice of assigning personas to LLMs - a trend on\nthe rise - can surface their deep-rooted biases and have unforeseeable and\ndetrimental side-effects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shashank Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_V/0/1/0/all/0/1\">Vaishnavi Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual Question Answering. (arXiv:2311.07536v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.07536","description":"<p>The emergence of multimodal large models (MLMs) has significantly advanced\nthe field of visual understanding, offering remarkable capabilities in the\nrealm of visual question answering (VQA). Yet, the true challenge lies in the\ndomain of knowledge-intensive VQA tasks, which necessitate not just recognition\nof visual elements, but also a deep comprehension of the visual information in\nconjunction with a vast repository of learned knowledge. To uncover such\ncapabilities of MLMs, particularly the newly introduced GPT-4V, we provide an\nin-depth evaluation from three perspectives: 1) Commonsense Knowledge, which\nassesses how well models can understand visual cues and connect to general\nknowledge; 2) Fine-grained World Knowledge, which tests the model's skill in\nreasoning out specific knowledge from images, showcasing their proficiency\nacross various specialized fields; 3) Comprehensive Knowledge with\nDecision-making Rationales, which examines model's capability to provide\nlogical explanations for its inference, facilitating a deeper analysis from the\ninterpretability perspective. Extensive experiments indicate that GPT-4V\nachieves SOTA performance on above three tasks. Interestingly, we find that: a)\nGPT-4V demonstrates enhanced reasoning and explanation when using composite\nimages as few-shot; b) GPT-4V produces severe hallucinations when dealing with\nworld knowledge, highlighting the future need for advancements in this research\ndirection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chenyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Construction in Power Distribution Networks. (arXiv:2311.08724v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.08724","description":"<p>In this paper, we propose a method for knowledge graph construction in power\ndistribution networks. This method leverages entity features, which involve\ntheir semantic, phonetic, and syntactic characteristics, in both the knowledge\ngraph of distribution network and the dispatching texts. An enhanced model\nbased on Convolutional Neural Network, is utilized for effectively matching\ndispatch text entities with those in the knowledge graph. The effectiveness of\nthis model is evaluated through experiments in real-world power distribution\ndispatch scenarios. The results indicate that, compared with the baselines, the\nproposed model excels in linking a variety of entity types, demonstrating high\noverall accuracy in power distribution knowledge graph construction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Che Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sizhe Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization. (arXiv:2311.09335v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.09335","description":"<p>Despite the remarkable performance of generative large language models (LLMs)\non abstractive summarization, they face two significant challenges: their\nconsiderable size and tendency to hallucinate. Hallucinations are concerning\nbecause they erode reliability and raise safety issues. Pruning is a technique\nthat reduces model size by removing redundant weights, enabling more efficient\nsparse inference. Pruned models yield downstream task performance comparable to\nthe original, making them ideal alternatives when operating on a limited\nbudget. However, the effect that pruning has upon hallucinations in abstractive\nsummarization with LLMs has yet to be explored. In this paper, we provide an\nextensive empirical study across five summarization datasets, two\nstate-of-the-art pruning methods, and five instruction-tuned LLMs.\nSurprisingly, we find that hallucinations from pruned LLMs are less prevalent\nthan the original models. Our analysis suggests that pruned models tend to\ndepend more on the source document for summary generation. This leads to a\nhigher lexical overlap between the generated summary and the source document,\nwhich could be a reason for the reduction in hallucination risk.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chrysostomou_G/0/1/0/all/0/1\">George Chrysostomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhixue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1\">Miles Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Fault Analysis in Substations Based on Knowledge Graphs. (arXiv:2311.13708v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.13708","description":"<p>To address the challenge of identifying hidden danger in substations from\nunstructured text, a novel dynamic analysis method is proposed. We first\nextract relevant information from the unstructured text, and then leverages a\nflexible distributed search engine built on Elastic-Search to handle the data.\nFollowing this, the hidden Markov model is employed to train the data within\nthe engine. The Viterbi algorithm is integrated to decipher the hidden state\nsequences, facilitating the segmentation and labeling of entities related to\nhidden dangers. The final step involves using the Neo4j graph database to\ndynamically create a knowledge graph that visualizes hidden dangers in the\nsubstation. The effectiveness of the proposed method is demonstrated through a\ncase analysis from a specific substation with hidden dangers revealed in the\ntext records.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sizhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hui Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Time Signature Determination for New Scores Using Lyrics for Latent Rhythmic Structure. (arXiv:2311.15480v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.15480","description":"<p>There has recently been a sharp increase in interest in Artificial\nIntelligence-Generated Content (AIGC). Despite this, musical components such as\ntime signatures have not been studied sufficiently to form an algorithmic\ndetermination approach for new compositions, especially lyrical songs. This is\nlikely because of the neglect of musical details, which is critical for\nconstructing a robust framework. Specifically, time signatures establish the\nfundamental rhythmic structure for almost all aspects of a song, including the\nphrases and notes. In this paper, we propose a novel approach that only uses\nlyrics as input to automatically generate a fitting time signature for lyrical\nsongs and uncover the latent rhythmic structure utilizing explainable machine\nlearning models. In particular, we devise multiple methods that are associated\nwith discovering lyrical patterns and creating new features that simultaneously\ncontain lyrical, rhythmic, and statistical information. In this approach, the\nbest of our experimental results reveal a 97.6% F1 score and a 0.996 Area Under\nthe Curve (AUC) of the Receiver Operating Characteristic (ROC) score. In\nconclusion, our research directly generates time signatures from lyrics\nautomatically for new scores utilizing machine learning, which is an innovative\nidea that approaches an understudied component of musicology and therefore\ncontributes significantly to the future of Artificial Intelligence (AI) music\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1\">Callie C. Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_D/0/1/0/all/0/1\">Duoduo Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guessford_J/0/1/0/all/0/1\">Jesse Guessford</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Fault Characteristics Evaluation in Power Grid. (arXiv:2311.16522v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.16522","description":"<p>To enhance the intelligence degree in operation and maintenance, a novel\nmethod for fault detection in power grids is proposed. The proposed GNN-based\napproach first identifies fault nodes through a specialized feature extraction\nmethod coupled with a knowledge graph. By incorporating temporal data, the\nmethod leverages the status of nodes from preceding and subsequent time periods\nto help current fault detection. To validate the effectiveness of the node\nfeatures, a correlation analysis of the output features from each node was\nconducted. The results from experiments show that this method can accurately\nlocate fault nodes in simulation scenarios with a remarkable accuracy.\nAdditionally, the graph neural network based feature modeling allows for a\nqualitative examination of how faults spread across nodes, which provides\nvaluable insights for analyzing fault nodes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1\">Hao Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Si Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuanfu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Che Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sizhe Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Functional Differentiation in JAX. (arXiv:2311.18727v2 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2311.18727","description":"<p>We extend JAX with the capability to automatically differentiate higher-order\nfunctions (functionals and operators). By representing functions as a\ngeneralization of arrays, we seamlessly use JAX's existing primitive system to\nimplement higher-order functions. We present a set of primitive operators that\nserve as foundational building blocks for constructing several key types of\nfunctionals. For every introduced primitive operator, we derive and implement\nboth linearization and transposition rules, aligning with JAX's internal\nprotocols for forward and reverse mode automatic differentiation. This\nenhancement allows for functional differentiation in the same syntax\ntraditionally use for functions. The resulting functional gradients are\nthemselves functions ready to be invoked in python. We showcase this tool's\nefficacy and simplicity through applications where functional derivatives are\nindispensable. The source code of this work is released at\nhttps://github.com/sail-sg/autofd .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Min Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs. (arXiv:2312.03731v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.03731","description":"<p>Graphs can inherently model interconnected objects on the Web, thereby\nfacilitating a series of Web applications, such as web analyzing and content\nrecommendation. Recently, Graph Neural Networks (GNNs) have emerged as a\nmainstream technique for graph representation learning. However, their efficacy\nwithin an end-to-end supervised framework is significantly tied to the\navailabilityof task-specific labels. To mitigate labeling costs and enhance\nrobustness in few-shot settings, pre-training on self-supervised tasks has\nemerged as a promising method, while prompting has been proposed to further\nnarrow the objective gap between pretext and downstream tasks. Although there\nhas been some initial exploration of prompt-based learning on graphs, they\nprimarily leverage a single pretext task, resulting in a limited subset of\ngeneral knowledge that could be learned from the pre-training data. Hence, in\nthis paper, we propose MultiGPrompt, a novel multi-task pre-training and\nprompting framework to exploit multiple pretext tasks for more comprehensive\npre-trained knowledge. First, in pre-training, we design a set of pretext\ntokens to synergize multiple pretext tasks. Second, we propose a dual-prompt\nmechanism consisting of composed and open prompts to leverage task-specific and\nglobal pre-training knowledge, to guide downstream tasks in few-shot settings.\nFinally, we conduct extensive experiments on six public datasets to evaluate\nand analyze MultiGPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xingtong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinming Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints. (arXiv:2312.03905v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2312.03905","description":"<p>Neuro-symbolic AI bridges the gap between purely symbolic and neural\napproaches to learning. This often requires maximizing the likelihood of a\nsymbolic constraint w.r.t the neural network's output distribution. Such output\ndistributions are typically assumed to be fully-factorized. This limits the\napplicability of neuro-symbolic learning to the more expressive autoregressive\ndistributions, e.g., transformers. Under such distributions, computing the\nlikelihood of even simple constraints is #P-hard. Instead of attempting to\nenforce the constraint on the entire output distribution, we propose to do so\non a random, local approximation thereof. More precisely, we optimize the\nlikelihood of the constraint under a pseudolikelihood-based approximation\ncentered around a model sample. Our approximation is factorized, allowing the\nreuse of solutions to sub-problems, a main tenet for efficiently computing\nneuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of\nthe likelihood, exhibiting low entropy and KL-divergence around the model\nsample. We evaluate our approach on Sudoku and shortest-path prediction cast as\nautoregressive generation, and observe that we greatly improve upon the base\nmodel's ability to predict logically-consistent outputs. We also evaluate on\nthe task of detoxifying large language models. Using a simple constraint\ndisallowing a list of toxic words, we are able to steer the model's outputs\naway from toxic generations, achieving SoTA detoxification compared to previous\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_K/0/1/0/all/0/1\">Kareem Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1\">Guy Van den Broeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Enhanced Aspect-Level Sentiment Analysis. (arXiv:2312.10048v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.10048","description":"<p>In this paper, we propose a novel method to enhance sentiment analysis by\naddressing the challenge of context-specific word meanings. It combines the\nadvantages of a BERT model with a knowledge graph based synonym data. This\nsynergy leverages a dynamic attention mechanism to develop a knowledge-driven\nstate vector. For classifying sentiments linked to specific aspects, the\napproach constructs a memory bank integrating positional data. The data are\nthen analyzed using a DCGRU to pinpoint sentiment characteristics related to\nspecific aspect terms. Experiments on three widely used datasets demonstrate\nthe superior performance of our method in sentiment classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1\">Kavita Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1\">Ritu Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Sunita Iyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEILP: Time Prediction over Knowledge Graphs via Logical Reasoning. (arXiv:2312.15816v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.15816","description":"<p>Conventional embedding-based models approach event time prediction in\ntemporal knowledge graphs (TKGs) as a ranking problem. However, they often fall\nshort in capturing essential temporal relationships such as order and distance.\nIn this paper, we propose TEILP, a logical reasoning framework that naturally\nintegrates such temporal elements into knowledge graph predictions. We first\nconvert TKGs into a temporal event knowledge graph (TEKG) which has a more\nexplicit representation of time in term of nodes of the graph. The TEKG equips\nus to develop a differentiable random walk approach to time prediction.\nFinally, we introduce conditional probability density functions, associated\nwith the logical rules involving the query interval, using which we arrive at\nthe time prediction. We compare TEILP with state-of-the-art methods on five\nbenchmark datasets. We show that our model achieves a significant improvement\nover baselines while providing interpretable explanations. In particular, we\nconsider several scenarios where training samples are limited, event types are\nimbalanced, and forecasting the time of future events based on only past events\nis desired. In all these cases, TEILP outperforms state-of-the-art methods in\nterms of robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_S/0/1/0/all/0/1\">Siheng Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Payani_A/0/1/0/all/0/1\">Ali Payani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerce_J/0/1/0/all/0/1\">James C Kerce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fekri_F/0/1/0/all/0/1\">Faramarz Fekri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rule-Guided Joint Embedding Learning over Knowledge Graphs. (arXiv:2401.02968v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.02968","description":"<p>Recent studies focus on embedding learning over knowledge graphs, which map\nentities and relations in knowledge graphs into low-dimensional vector spaces.\nWhile existing models mainly consider the aspect of graph structure, there\nexists a wealth of contextual and literal information that can be utilized for\nmore effective embedding learning. This paper introduces a novel model that\nincorporates both contextual and literal information into entity and relation\nembeddings by utilizing graph convolutional networks. Specifically, for\ncontextual information, we assess its significance through confidence and\nrelatedness metrics. In addition, a unique rule-based method is developed to\ncalculate the confidence metric, and the relatedness metric is derived from the\nliteral information's representations. We validate our model performance with\nthorough experiments on two established benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qisong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Ji Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Sijia Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Neng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Classification Based on Knowledge Graphs and Improved Attention Mechanism. (arXiv:2401.03591v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.03591","description":"<p>To resolve the semantic ambiguity in texts, we propose a model, which\ninnovatively combines a knowledge graph with an improved attention mechanism.\nAn existing knowledge base is utilized to enrich the text with relevant\ncontextual concepts. The model operates at both character and word levels to\ndeepen its understanding by integrating the concepts. We first adopt\ninformation gain to select import words. Then an encoder-decoder framework is\nused to encode the text along with the related concepts. The local attention\nmechanism adjusts the weight of each concept, reducing the influence of\nirrelevant or noisy concepts during classification. We improve the calculation\nformula for attention scores in the local self-attention mechanism, ensuring\nthat words with different frequencies of occurrence in the text receive higher\nattention scores. Finally, the model employs a Bi-directional Gated Recurrent\nUnit (Bi-GRU), which is effective in feature extraction from texts for improved\nclassification accuracy. Its performance is demonstrated on datasets such as\nAGNews, Ohsumed, and TagMyNews, achieving accuracy of 75.1%, 58.7%, and 68.5%\nrespectively, showing its effectiveness in classifying tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chenwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Parsing for Question Answering over Knowledge Graphs. (arXiv:2401.06772v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.06772","description":"<p>In this paper, we introduce a novel method with graph-to-segment mapping for\nquestion answering over knowledge graphs, which helps understanding question\nutterances. This method centers on semantic parsing, a key approach for\ninterpreting these utterances. The challenges lie in comprehending implicit\nentities, relationships, and complex constraints like time, ordinality, and\naggregation within questions, contextualized by the knowledge graph. Our\nframework employs a combination of rule-based and neural-based techniques to\nparse and construct highly accurate and comprehensive semantic segment\nsequences. These sequences form semantic query graphs, effectively representing\nquestion utterances. We approach question semantic parsing as a sequence\ngeneration task, utilizing an encoder-decoder neural network to transform\nnatural language questions into semantic segments. Moreover, to enhance the\nparsing of implicit entities and relations, we incorporate a graph neural\nnetwork that leverages the context of the knowledge graph to better understand\nquestion representations. Our experimental evaluations on two datasets\ndemonstrate the effectiveness and superior performance of our model in semantic\nparsing for question answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Sijia Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qisong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Driven Recommendation System Algorithm. (arXiv:2401.10244v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2401.10244","description":"<p>In this paper, we propose a novel graph neural network-based recommendation\nmodel called KGLN, which leverages Knowledge Graph (KG) information to enhance\nthe accuracy and effectiveness of personalized recommendations. We first use a\nsingle-layer neural network to merge individual node features in the graph, and\nthen adjust the aggregation weights of neighboring entities by incorporating\ninfluence factors. The model evolves from a single layer to multiple layers\nthrough iteration, enabling entities to access extensive multi-order associated\nentity information. The final step involves integrating features of entities\nand users to produce a recommendation score. The model performance was\nevaluated by comparing its effects on various aggregation methods and influence\nfactors. In tests over the MovieLen-1M and Book-Crossing datasets, KGLN shows\nan Area Under the ROC curve (AUC) improvement of 0.3% to 5.9% and 1.1% to 8.2%,\nrespectively, which is better than existing benchmark methods like LibFM,\nDeepFM, Wide&amp;Deep, and RippleNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Siwei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2401.10711","description":"<p>Video Question Answering (VideoQA) aims to answer natural language questions\nbased on the information observed in videos. Despite the recent success of\nLarge Multimodal Models (LMMs) in image-language understanding and reasoning,\nthey deal with VideoQA insufficiently by simply taking uniformly sampled frames\nas visual inputs, which ignores question-relevant visual clues. Moreover, there\nare no human annotations for question-critical timestamps in existing VideoQA\ndatasets. In light of this, we propose a novel weakly supervised framework to\nenforce the LMMs to reason out the answers with question-critical moments as\nvisual inputs. Specifically, we fuse the question and answer pairs as event\ndescriptions to find multiple keyframes as target moments, which will be\npseudo-labels. With these pseudo-labels as additionally weak supervision, we\ndevise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG\nlearns multiple Gaussian functions to characterize the temporal structure of\nthe video, and sample question-critical frames as positive moments to be the\nvisual inputs of LMMs. Extensive experiments on several VideoQA benchmarks\nverify the effectiveness of our framework, and we achieve substantial\nimprovements compared to previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Chenghang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yixuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Weifeng Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment. (arXiv:2401.10768v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.10768","description":"<p>While Large Language Models (LLMs) have proven to be exceptional on a variety\nof tasks after alignment, they may still produce responses that contradict the\ncontext or world knowledge confidently, a phenomenon known as\n``hallucination''. In this paper, we demonstrate that reducing the\ninconsistency between the external knowledge encapsulated in the training data\nand the intrinsic knowledge inherited in the pretraining corpus could mitigate\nhallucination in alignment. Specifically, we introduce a novel knowledge\nconsistent alignment (KCA) approach, which involves automatically formulating\nexaminations based on external knowledge for accessing the comprehension of\nLLMs. For data encompassing knowledge inconsistency, KCA implements several\nsimple yet efficient strategies for processing. We illustrate the superior\nperformance of the proposed KCA approach in mitigating hallucinations across\nsix benchmarks using LLMs of different backbones and scales. Furthermore, we\nconfirm the correlation between knowledge inconsistency and hallucination,\nsignifying the effectiveness of reducing knowledge inconsistency in alleviating\nhallucinations. Our code, model weights, and data are public at\n\\url{https://github.com/fanqiwan/KCA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1\">Fanqi Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinting Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Location Sensitive Embedding for Knowledge Graph Reasoning. (arXiv:2401.10893v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2401.10893","description":"<p>Embedding methods transform the knowledge graph into a continuous,\nlow-dimensional space, facilitating inference and completion tasks. Existing\nmethods are mainly divided into two types: translational distance models and\nsemantic matching models. A key challenge in translational distance models is\ntheir inability to effectively differentiate between 'head' and 'tail' entities\nin graphs. To address this problem, a novel location-sensitive embedding (LSE)\nmethod has been developed. LSE innovatively modifies the head entity using\nrelation-specific mappings, conceptualizing relations as linear transformations\nrather than mere translations. The theoretical foundations of LSE, including\nits representational capabilities and its connections to existing models, have\nbeen thoroughly examined. A more streamlined variant, LSE-d, which employs a\ndiagonal matrix for transformations to enhance practical efficiency, is also\nproposed. Experiments conducted on four large-scale KG datasets for link\nprediction show that LSEd either outperforms or is competitive with\nstate-of-the-art related works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Deepak Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishaan_A/0/1/0/all/0/1\">Anjali Ishaan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.11864","description":"<p>This work addresses the challenge of democratizing advanced Large Language\nModels (LLMs) by compressing their mathematical reasoning capabilities into\nsub-billion parameter Small Language Models (SLMs) without compromising\nperformance. We introduce Equation-of-Thought Distillation (EoTD), a novel\ntechnique that encapsulates the reasoning process into equation-based\nrepresentations to construct an EoTD dataset for fine-tuning SLMs.\nAdditionally, we propose the Ensemble Thoughts Distillation (ETD) framework to\nenhance the reasoning performance of SLMs. This involves creating a reasoning\ndataset with multiple thought processes, including Chain-of-Thought (CoT),\nProgram-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for\nfine-tuning. Our experimental findings demonstrate that EoTD significantly\nboosts the reasoning abilities of SLMs, while ETD enables these models to\nachieve state-of-the-art reasoning performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xunyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Can Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Misgendering and Assuming Gender in Machine Translation when Working with Low-Resource Languages. (arXiv:2401.13165v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.13165","description":"<p>This chapter focuses on gender-related errors in machine translation (MT) in\nthe context of low-resource languages. We begin by explaining what low-resource\nlanguages are, examining the inseparable social and computational factors that\ncreate such linguistic hierarchies. We demonstrate through a case study of our\nmother tongue Bengali, a global language spoken by almost 300 million people\nbut still classified as low-resource, how gender is assumed and inferred in\ntranslations to and from the high(est)-resource English when no such\ninformation is provided in source texts. We discuss the postcolonial and\nsocietal impacts of such errors leading to linguistic erasure and\nrepresentational harms, and conclude by discussing potential solutions towards\nuplifting languages by providing them more agency in MT conversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sourojit Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Srishti Chatterjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can AI Assistants Know What They Don't Know?. (arXiv:2401.13275v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.13275","description":"<p>Recently, AI assistants based on large language models (LLMs) show surprising\nperformance in many tasks, such as dialogue, solving math problems, writing\ncode, and using tools. Although LLMs possess intensive world knowledge, they\nstill make factual errors when facing some knowledge intensive tasks, like\nopen-domain question answering. These untruthful responses from the AI\nassistant may cause significant risks in practical applications. We believe\nthat an AI assistant's refusal to answer questions it does not know is a\ncrucial method for reducing hallucinations and making the assistant truthful.\nTherefore, in this paper, we ask the question \"Can AI assistants know what they\ndon't know and express them through natural language?\" To answer this question,\nwe construct a model-specific \"I don't know\" (Idk) dataset for an assistant,\nwhich contains its known and unknown questions, based on existing open-domain\nquestion answering datasets. Then we align the assistant with its corresponding\nIdk dataset and observe whether it can refuse to answer its unknown questions\nafter alignment. Experimental results show that after alignment with Idk\ndatasets, the assistant can refuse to answer most its unknown questions. For\nquestions they attempt to answer, the accuracy is significantly higher than\nbefore the alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qinyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhangyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shimin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhengfu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Malaysian Language Model Based on Mistral for Enhanced Local Language Understanding. (arXiv:2401.13565v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.13565","description":"<p>In this paper, we present significant advancements in the pretraining of\nMistral 7B, a large-scale language model, using a dataset of 32.6 GB,\nequivalent to 1.1 billion tokens. We explore the impact of extending the\ncontext length, releasing models with context lengths of 4096 and 32768 tokens,\nand further refining performance with a specialized 16384 context length\ninstruction-tuned model, we called it Malaysian Mistral.\n</p>\n<p>Our experiments demonstrate the efficacy of continue pretraining and the\ninfluence of extended context lengths on Mistral 7B's language understanding\ncapabilities. Additionally, we release a model specifically tuned with a 16384\ncontext length instruction, showcasing its potential for capturing nuanced\nlanguage intricacies.\n</p>\n<p>Furthermore, our research contributes to the benchmarking of Malaysian\nMistral against prominent language models, including ChatGPT3.5 and Claude 2.\nWe present compelling results indicating Malaysian Mistral's superior\nperformance on Tatabahasa (Malay grammar) test set, particularly when\nfine-tuned with instructions.\n</p>\n<p>All models released at\nhttps://huggingface.co/collections/mesolitica/malaysian-mistral-7b-6528f2ec825f4bba46c1700c\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zolkepli_H/0/1/0/all/0/1\">Husein Zolkepli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razak_A/0/1/0/all/0/1\">Aisyah Razak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adha_K/0/1/0/all/0/1\">Kamarul Adha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazhan_A/0/1/0/all/0/1\">Ariff Nazhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2401.13802","description":"<p>Large Language Models (LLMs) have demonstrated remarkable success in various\nnatural language processing and software engineering tasks, such as code\ngeneration. The LLMs are mainly utilized in the prompt-based zero/few-shot\nparadigm to guide the model in accomplishing the task. GPT-based models are one\nof the popular ones studied for tasks such as code comment generation or test\ngeneration. These tasks are `generative' tasks. However, there is limited\nresearch on the usage of LLMs for `non-generative' tasks such as classification\nusing the prompt-based paradigm. In this preliminary exploratory study, we\ninvestigated the applicability of LLMs for Code Clone Detection (CCD), a\nnon-generative task. By building a mono-lingual and cross-lingual CCD dataset\nderived from CodeNet, we first investigated two different prompts using ChatGPT\nto detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot\nsetting. We then conducted an analysis to understand the strengths and\nweaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language\nCCD attaining an F1-score of 0.877 and achieves comparable performance to fully\nfine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the\nprompt and the difficulty level of the problems has an impact on the\nperformance of ChatGPT. Finally we provide insights and future directions based\non our initial analysis\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khajezade_M/0/1/0/all/0/1\">Mohamad Khajezade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie JW Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1\">Fatemeh Hendijani Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Perez_G/0/1/0/all/0/1\">Gema Rodr&#xed;guez-P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1\">Mohamed Sami Shehata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.13919","description":"<p>The advancement of large language models (LLMs) leads to a new era marked by\nthe development of autonomous applications in the real world, which drives\ninnovation in the creation of advanced web-based agents. Existing web agents\ntypically only handle one input modality and are evaluated only in simplified\nweb simulators or static web snapshots, greatly limiting their applicability in\nreal-world scenarios. To bridge this gap, we introduce WebVoyager, an\ninnovative Large Multimodal Model (LMM) powered web agent that can complete\nuser instructions end-to-end by interacting with real-world websites. Moreover,\nwe propose a new evaluation protocol for web agents to address the challenges\nof automatic evaluation of open-ended web agent tasks, leveraging the robust\nmultimodal comprehension capabilities of GPT-4V. We create a new benchmark by\ngathering real-world tasks from 15 widely used websites to evaluate our agents.\nWe show that WebVoyager achieves a 55.7% task success rate, significantly\nsurpassing the performance of both GPT-4 (All Tools) and the WebVoyager\n(text-only) setups, underscoring the exceptional capability of WebVoyager in\npractical applications. We found that our proposed automatic evaluation\nachieves 85.3% agreement with human judgment, paving the way for further\ndevelopment of web agents in a real-world setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhenzhong Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction. (arXiv:2401.14166v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.14166","description":"<p>As a novel and effective fine-tuning paradigm based on large-scale\npre-trained language models (PLMs), prompt-tuning aims to reduce the gap\nbetween downstream tasks and pre-training objectives. While prompt-tuning has\nyielded continuous advancements in various tasks, such an approach still\nremains a persistent defect: prompt-tuning methods fail to generalize to\nspecific few-shot patterns. From the perspective of distribution analyses, we\ndisclose that the intrinsic issues behind the phenomenon are the\nover-multitudinous conceptual knowledge contained in PLMs and the abridged\nknowledge for target downstream domains, which jointly result in that PLMs\nmis-locate the knowledge distributions corresponding to the target domains in\nthe universal knowledge embedding space. To this end, we intuitively explore to\napproximate the unabridged target domains of downstream tasks in a debiased\nmanner, and then abstract such domains to generate discriminative prompts,\nthereby providing the de-ambiguous guidance for PLMs. Guided by such an\nintuition, we propose a simple yet effective approach, namely BayesPrompt, to\nlearn prompts that contain the domain discriminative information against the\ninterference from domain-irrelevant knowledge. BayesPrompt primitively\nleverages known distributions to approximate the debiased factual distributions\nof target domains and further uniformly samples certain representative features\nfrom the approximated distributions to generate the ultimate prompts for PLMs.\nWe provide theoretical insights with the connection to domain adaptation.\nEmpirically, our method achieves state-of-the-art performance on benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangmeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1\">Fei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yifan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1\">Wenwen Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changwen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement. (arXiv:2401.14215v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.14215","description":"<p>Memorizing and utilizing speakers' personas is a common practice for response\ngeneration in long-term conversations. Yet, human-authored datasets often\nprovide uninformative persona sentences that hinder response quality. This\npaper presents a novel framework that leverages commonsense-based persona\nexpansion to address such issues in long-term conversation. While prior work\nfocuses on not producing personas that contradict others, we focus on\ntransforming contradictory personas into sentences that contain rich speaker\ninformation, by refining them based on their contextual backgrounds with\ndesigned strategies. As the pioneer of persona expansion in multi-session\nsettings, our framework facilitates better response generation via human-like\npersona refinement. The supplementary video of our work is available at\nhttps://caffeine-15bbf.web.app/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hana Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_K/0/1/0/all/0/1\">Kai Tzu-iunn Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seoyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongha Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1\">Jinyoung Yeo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaLLaM -- Malaysia Large Language Model. (arXiv:2401.14680v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.14680","description":"<p>Addressing the gap in Large Language Model pretrained from scratch with\nMalaysian context, We trained models with 1.1 billion, 3 billion, and 5 billion\nparameters on a substantial 349GB dataset, equivalent to 90 billion tokens\nbased on our pretrained Byte Pair Encoding (BPE) tokenizer for a single epoch.\nMaLLaM contributes to enhanced natural language understanding and generation\ntasks in the Malay language. Although trained on a smaller dataset of 90\nbillion tokens, our instruction-tuned MaLLaM models perform competitively. When\ncompared to ChatGPT3.5 and Malaysian Mistral, MaLLaM's instruction-tuned models\ndemonstrate notable proficiency, underscoring the effectiveness of our approach\nin capturing and understanding the nuances of the Malaysian language. MaLLaM\nmodels mark a significant contribution to the field, providing comprehensive\nlanguage representations grounded in Malaysian context. This endeavor aims to\npave the way for enhanced natural language understanding and generation tasks\nspecific to the linguistic nuances present in Malaysia. We discuss the training\nmethodology, dataset composition, and the potential impact of MaLLaM in\nadvancing the capabilities of large language models within the context of the\nMalay language.\n</p>\n<p>All models released at\nhttps://huggingface.co/collections/mesolitica/mallam-6577b59d1e0b436ae75f930f\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zolkepli_H/0/1/0/all/0/1\">Husein Zolkepli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razak_A/0/1/0/all/0/1\">Aisyah Razak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adha_K/0/1/0/all/0/1\">Kamarul Adha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazhan_A/0/1/0/all/0/1\">Ariff Nazhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Power of Noise: Redefining Retrieval for RAG Systems. (arXiv:2401.14887v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2401.14887","description":"<p>Retrieval-Augmented Generation (RAG) systems represent a significant\nadvancement over traditional Large Language Models (LLMs). RAG systems enhance\ntheir generation ability by incorporating external data retrieved through an\nInformation Retrieval (IR) phase, overcoming the limitations of standard LLMs,\nwhich are restricted to their pre-trained knowledge and limited context window.\nMost research in this area has predominantly concentrated on the generative\naspect of LLMs within RAG systems. Our study fills this gap by thoroughly and\ncritically analyzing the influence of IR components on RAG systems. This paper\nanalyzes which characteristics a retriever should possess for an effective\nRAG's prompt formulation, focusing on the type of documents that should be\nretrieved. We evaluate various elements, such as the relevance of the documents\nto the prompt, their position, and the number included in the context. Our\nfindings reveal, among other insights, that including irrelevant documents can\nunexpectedly enhance performance by more than 30% in accuracy, contradicting\nour initial assumption of diminished quality. These results underscore the need\nfor developing specialized strategies to integrate retrieval with language\ngeneration models, thereby laying the groundwork for future research in this\nfield.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cuconasu_F/0/1/0/all/0/1\">Florin Cuconasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trappolini_G/0/1/0/all/0/1\">Giovanni Trappolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siciliano_F/0/1/0/all/0/1\">Federico Siciliano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filice_S/0/1/0/all/0/1\">Simone Filice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campagnano_C/0/1/0/all/0/1\">Cesare Campagnano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maarek_Y/0/1/0/all/0/1\">Yoelle Maarek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonellotto_N/0/1/0/all/0/1\">Nicola Tonellotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Fabrizio Silvestri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2024-01-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}