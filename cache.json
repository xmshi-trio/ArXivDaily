{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-09T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Can Offline Reinforcement Learning Help Natural Language Understanding?. (arXiv:2212.03864v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03864","description":"<p>Pre-training has been a useful method for learning implicit transferable\nknowledge and it shows the benefit of offering complementary features across\ndifferent modalities. Recent work mainly focuses on the modalities such as\nimage and text, for example, studies show that visual features learned from\nimages can help visual-grounded language understanding. In this paper, we\nconsider investigating the potential connection between offline reinforcement\nlearning (RL) and language modeling (LM). Intuitively, RL and LM are similar in\npredicting the next states based on the current and previous states, which rely\non both local and long-range dependency across states. To validate such an\nassumption, we pre-trained different offline RL tasks using Transformer and\nthen evaluate these models on various language-related tasks. Experimental\nresults show that our RL pre-trained models can give close performance compared\nwith the models using the LM training objective, showing that there exist\ncommon useful features across these two modalities. To further explore the\npotential relationship, we investigate some factors such as Markov property and\nthe sequential nature of RL trajectory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yile Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Donglin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Training With Scientific Text Improves Educational Question Generation. (arXiv:2212.03869v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03869","description":"<p>With the boom of digital educational materials and scalable e-learning\nsystems, the potential for realising AI-assisted personalised learning has\nskyrocketed. In this landscape, the automatic generation of educational\nquestions will play a key role, enabling scalable self-assessment when a global\npopulation is manoeuvring their personalised learning journeys. We develop\nEduQG, a novel educational question generation model built by adapting a large\nlanguage model. Our initial experiments demonstrate that EduQG can produce\nsuperior educational questions by pre-training on scientific text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muse_H/0/1/0/all/0/1\">Hamze Muse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulathwela_S/0/1/0/all/0/1\">Sahan Bulathwela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TweetDrought: A Deep-Learning Drought Impacts Recognizer based on Twitter Data. (arXiv:2212.04001v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04001","description":"<p>Acquiring a better understanding of drought impacts becomes increasingly\nvital under a warming climate. Traditional drought indices describe mainly\nbiophysical variables and not impacts on social, economic, and environmental\nsystems. We utilized natural language processing and bidirectional encoder\nrepresentation from Transformers (BERT) based transfer learning to fine-tune\nthe model on the data from the news-based Drought Impact Report (DIR) and then\napply it to recognize seven types of drought impacts based on the filtered\nTwitter data from the United States. Our model achieved a satisfying macro-F1\nscore of 0.89 on the DIR test set. The model was then applied to California\ntweets and validated with keyword-based labels. The macro-F1 score was 0.58.\nHowever, due to the limitation of keywords, we also spot-checked tweets with\ncontroversial labels. 83.5% of BERT labels were correct compared to the keyword\nlabels. Overall, the fine-tuned BERT-based recognizer provided proper\npredictions and valuable information on drought impacts. The interpretation and\nanalysis of the model were consistent with experiential domain expertise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schilder_F/0/1/0/all/0/1\">Frank Schilder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kelly Helm Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_M/0/1/0/all/0/1\">Michael J. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harms_S/0/1/0/all/0/1\">Sherri Harms</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadesse_T/0/1/0/all/0/1\">Tsegaye Tadesse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demystifying Prompts in Language Models via Perplexity Estimation. (arXiv:2212.04037v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04037","description":"<p>Language models can be prompted to perform a wide variety of zero- and\nfew-shot learning problems. However, performance varies significantly with the\nchoice of prompt, and we do not yet understand why this happens or how to pick\nthe best prompts. In this work, we analyze the factors that contribute to this\nvariance and establish a new empirical hypothesis: the performance of a prompt\nis coupled with the extent to which the model is familiar with the language it\ncontains. Over a wide range of tasks, we show that the lower the perplexity of\nthe prompt is, the better the prompt is able to perform the task. As a result,\nwe devise a method for creating prompts: (1) automatically extend a small seed\nset of manually written prompts by paraphrasing using GPT3 and backtranslation\nand (2) choose the lowest perplexity prompts to get significant gains in\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1\">Hila Gonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Srini Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blevins_T/0/1/0/all/0/1\">Terra Blevins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Dub Movies via Hierarchical Prosody Models. (arXiv:2212.04054v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04054","description":"<p>Given a piece of text, a video clip and a reference audio, the movie dubbing\n(also known as visual voice clone V2C) task aims to generate speeches that\nmatch the speaker's emotion presented in the video using the desired speaker\nvoice as reference. V2C is more challenging than conventional text-to-speech\ntasks as it additionally requires the generated speech to exactly match the\nvarying emotions and speaking speed presented in the video. Unlike previous\nworks, we propose a novel movie dubbing architecture to tackle these problems\nvia hierarchical prosody modelling, which bridges the visual information to\ncorresponding speech prosody from three aspects: lip, face, and scene.\nSpecifically, we align lip movement to the speech duration, and convey facial\nexpression to speech energy and pitch via attention mechanism based on valence\nand arousal representations inspired by recent psychology findings. Moreover,\nwe design an emotion booster to capture the atmosphere from global video\nscenes. All these embeddings together are used to generate mel-spectrogram and\nthen convert to speech waves via existing vocoder. Extensive experimental\nresults on the Chem and V2C benchmark datasets demonstrate the favorable\nperformance of the proposed method. The source code and trained models will be\nreleased to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1\">Gaoxiang Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zhengjun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Glyph Phonetic Information for Chinese Spell Checking: What Works and What's Next. (arXiv:2212.04068v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04068","description":"<p>While pre-trained Chinese language models have demonstrated impressive\nperformance on a wide range of NLP tasks, the Chinese Spell Checking (CSC) task\nremains a challenge. Previous research has explored using information such as\nglyphs and phonetics to improve the ability to distinguish misspelled\ncharacters, with good results. However, the generalization ability of these\nmodels is not well understood: it is unclear whether they incorporate\nglyph-phonetic information and, if so, whether this information is fully\nutilized. In this paper, we aim to better understand the role of glyph-phonetic\ninformation in the CSC task and suggest directions for improvement.\nAdditionally, we propose a new, more challenging, and practical setting for\ntesting the generalizability of CSC models. All code is made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanjun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey on Multi-hop Machine Reading Comprehension Datasets and Metrics. (arXiv:2212.04070v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04070","description":"<p>Multi-hop Machine reading comprehension is a challenging task with aim of\nanswering a question based on disjoint pieces of information across the\ndifferent passages. The evaluation metrics and datasets are a vital part of\nmulti-hop MRC because it is not possible to train and evaluate models without\nthem, also, the proposed challenges by datasets often are an important\nmotivation for improving the existing models. Due to increasing attention to\nthis field, it is necessary and worth reviewing them in detail. This study aims\nto present a comprehensive survey on recent advances in multi-hop MRC\nevaluation metrics and datasets. In this regard, first, the multi-hop MRC\nproblem definition will be presented, then the evaluation metrics based on\ntheir multi-hop aspect will be investigated. Also, 15 multi-hop datasets have\nbeen reviewed in detail from 2017 to 2022, and a comprehensive analysis has\nbeen prepared at the end. Finally, open issues in this field have been\ndiscussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1\">Azade Mohammadi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_R/0/1/0/all/0/1\">Reza Ramezani</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Baraani_A/0/1/0/all/0/1\">Ahmad Baraani</a> (3) ((1) Candidate student in University of Isfahan, (2) Assistant Professor in University of Isfahan, (3) Professor of Computer Engineering in University of Isfahan)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches. (arXiv:2212.04072v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04072","description":"<p>Machine reading comprehension (MRC) is a long-standing topic in natural\nlanguage processing (NLP). The MRC task aims to answer a question based on the\ngiven context. Recently studies focus on multi-hop MRC which is a more\nchallenging extension of MRC, which to answer a question some disjoint pieces\nof information across the context are required. Due to the complexity and\nimportance of multi-hop MRC, a large number of studies have been focused on\nthis topic in recent years, therefore, it is necessary and worth reviewing the\nrelated literature. This study aims to investigate recent advances in the\nmulti-hop MRC approaches based on 31 studies from 2018 to 2022. In this regard,\nfirst, the multi-hop MRC problem definition will be introduced, then 31 models\nwill be reviewed in detail with a strong focus on their multi-hop aspects. They\nalso will be categorized based on their main techniques. Finally, a fine-grain\ncomprehensive comparison of the models and techniques will be presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1\">Azade Mohammadi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_R/0/1/0/all/0/1\">Reza Ramezani</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Baraani_A/0/1/0/all/0/1\">Ahmad Baraani</a> (3) ((1) Ph.D student in University of Isfahan, (2) Assistant Professor in University of Isfahan, (3) Professor of Computer Engineering in University of Isfahan)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models. (arXiv:2212.04088v1 [cs.AI])","link":"http://arxiv.org/abs/2212.04088","description":"<p>This study focuses on embodied agents that can follow natural language\ninstructions to complete complex tasks in a visually-perceived environment.\nExisting methods rely on a large amount of (instruction, gold trajectory) pairs\nto learn a good policy. The high data cost and poor sample efficiency prevents\nthe development of versatile agents that are capable of many tasks and can\nlearn new tasks quickly. In this work, we propose a novel method, LLM-Planner,\nthat harnesses the power of large language models (LLMs) such as GPT-3 to do\nfew-shot planning for embodied agents. We further propose a simple but\neffective way to enhance LLMs with physical grounding to generate plans that\nare grounded in the current environment. Experiments on the ALFRED dataset show\nthat our method can achieve very competitive few-shot performance, even\noutperforming several recent baselines that are trained using the full training\ndata despite using less than 0.5% of paired training data. Existing methods can\nbarely complete any task successfully under the same few-shot setting. Our work\nopens the door for developing versatile and sample-efficient embodied agents\nthat can quickly learn many tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chan Hee Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaman Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_C/0/1/0/all/0/1\">Clayton Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1\">Brian M. Sadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Editing Models with Task Arithmetic. (arXiv:2212.04089v1 [cs.LG])","link":"http://arxiv.org/abs/2212.04089","description":"<p>Changing how pre-trained models behave -- e.g., improving their performance\non a downstream task or mitigating biases learned during pre-training -- is a\ncommon practice when developing machine learning systems. In this work, we\npropose a new paradigm for steering the behavior of neural networks, centered\naround \\textit{task vectors}. A task vector specifies a direction in the weight\nspace of a pre-trained model, such that movement in that direction improves\nperformance on the task. We build task vectors by subtracting the weights of a\npre-trained model from the weights of the same model after fine-tuning on a\ntask. We show that these task vectors can be modified and combined together\nthrough arithmetic operations such as negation and addition, and the behavior\nof the resulting model is steered accordingly. Negating a task vector decreases\nperformance on the target task, with little change in model behavior on control\ntasks. Moreover, adding task vectors together can improve performance on\nmultiple tasks at once. Finally, when tasks are linked by an analogy\nrelationship of the form ``A is to B as C is to D\", combining task vectors from\nthree of the tasks can improve performance on the fourth, even when no data\nfrom the fourth task is used for training. Overall, our experiments with\nseveral models, modalities and tasks show that task arithmetic is a simple,\nefficient and effective way of editing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marco Tulio Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Successive Prompting for Decomposing Complex Questions. (arXiv:2212.04092v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04092","description":"<p>Answering complex questions that require making latent decisions is a\nchallenging task, especially when limited supervision is available. Recent\nworks leverage the capabilities of large language models (LMs) to perform\ncomplex question answering in a few-shot setting by demonstrating how to output\nintermediate rationalizations while solving the complex question in a single\npass. We introduce ``Successive Prompting'', where we iteratively break down a\ncomplex task into a simple task, solve it, and then repeat the process until we\nget the final solution. Successive prompting decouples the supervision for\ndecomposing complex questions from the supervision for answering simple\nquestions, allowing us to (1) have multiple opportunities to query in-context\nexamples at each reasoning step (2) learn question decomposition separately\nfrom question answering, including using synthetic data, and (3) use bespoke\n(fine-tuned) components for reasoning steps where a large LM does not perform\nwell. The intermediate supervision is typically manually written, which can be\nexpensive to collect. We introduce a way to generate a synthetic dataset which\ncan be used to bootstrap a model's ability to decompose and answer intermediate\nquestions. Our best model (with successive prompting) achieves an improvement\nof ~5% absolute F1 on a few-shot version of the DROP dataset when compared with\na state-of-the-art model with the same supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dua_D/0/1/0/all/0/1\">Dheeru Dua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shivanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogCC: Large-Scale Multi-Modal Dialogue Dataset. (arXiv:2212.04119v1 [cs.CV])","link":"http://arxiv.org/abs/2212.04119","description":"<p>As sharing images in an instant message is a crucial factor, there has been\nactive research on learning a image-text multi-modal dialogue model. However,\ntraining a well-generalized multi-modal dialogue model is challenging because\nexisting multi-modal dialogue datasets contain a small number of data, limited\ntopics, and a restricted variety of images per dialogue. In this paper, we\npresent a multi-modal dialogue dataset creation pipeline that involves matching\nlarge-scale images to dialogues based on CLIP similarity. Using this automatic\npipeline, we propose a large-scale multi-modal dialogue dataset, DialogCC,\nwhich covers diverse real-world topics and various images per dialogue. With\nextensive experiments, we demonstrate that training a multi-modal dialogue\nmodel with our dataset can improve generalization performance. Additionally,\nexisting models trained with our dataset achieve state-of-the-art performance\non image and text retrieval tasks. The source code and the dataset will be\nreleased after publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Young-Jun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1\">Byungsoo Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Han-Gyu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Ho-Jin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Discrete Genres: Mapping News Items onto a Multidimensional Framework of Genre Cues. (arXiv:2212.04185v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04185","description":"<p>In the contemporary media landscape, with the vast and diverse supply of\nnews, it is increasingly challenging to study such an enormous amount of items\nwithout a standardized framework. Although attempts have been made to organize\nand compare news items on the basis of news values, news genres receive little\nattention, especially the genres in a news consumer's perception. Yet,\nperceived news genres serve as an essential component in exploring how news has\ndeveloped, as well as a precondition for understanding media effects. We\napproach this concept by conceptualizing and operationalizing a non-discrete\nframework for mapping news items in terms of genre cues. As a starting point,\nwe propose a preliminary set of dimensions consisting of \"factuality\" and\n\"formality\". To automatically analyze a large amount of news items, we deliver\ntwo computational models for predicting news sentences in terms of the said two\ndimensions. Such predictions could then be used for locating news items within\nour framework. This proposed approach that positions news items upon a\nmultidimensional grid helps in deepening our insight into the evolving nature\nof news genres.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zilin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welbers_K/0/1/0/all/0/1\">Kasper Welbers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vermeer_S/0/1/0/all/0/1\">Susan Vermeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trilling_D/0/1/0/all/0/1\">Damian Trilling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DC-MBR: Distributional Cooling for Minimum Bayesian Risk Decoding. (arXiv:2212.04205v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04205","description":"<p>Minimum Bayesian Risk Decoding (MBR) emerges as a promising decoding\nalgorithm in Neural Machine Translation. However, MBR performs poorly with\nlabel smoothing, which is surprising as label smoothing provides decent\nimprovement with beam search and improves generality in various tasks. In this\nwork, we show that the issue arises from the un-consistency of label smoothing\non the token-level and sequence-level distributions. We demonstrate that even\nthough label smoothing only causes a slight change in the token-level, the\nsequence-level distribution is highly skewed. We coin the issue\n\\emph{distributional over-smoothness}. To address this issue, we propose a\nsimple and effective method, Distributional Cooling MBR (DC-MBR), which\nmanipulates the entropy of output distributions by tuning down the Softmax\ntemperature. We theoretically prove the equivalence between pre-tuning label\nsmoothing factor and distributional cooling. Experiments on NMT benchmarks\nvalidate that distributional cooling improves MBR's efficiency and\neffectiveness in various settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scientific Paper Extractive Summarization Enhanced by Citation Graphs. (arXiv:2212.04214v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04214","description":"<p>In a citation graph, adjacent paper nodes share related scientific terms and\ntopics. The graph thus conveys unique structure information of document-level\nrelatedness that can be utilized in the paper summarization task, for exploring\nbeyond the intra-document information. In this work, we focus on leveraging\ncitation graphs to improve scientific paper extractive summarization under\ndifferent settings. We first propose a Multi-granularity Unsupervised\nSummarization model (MUS) as a simple and low-cost solution to the task. MUS\nfinetunes a pre-trained encoder model on the citation graph by link prediction\ntasks. Then, the abstract sentences are extracted from the corresponding paper\nconsidering multi-granularity information. Preliminary results demonstrate that\ncitation graph is helpful even in a simple unsupervised framework. Motivated by\nthis, we next propose a Graph-based Supervised Summarization model (GSS) to\nachieve more accurate results on the task when large-scale labeled data are\navailable. Apart from employing the link prediction as an auxiliary task, GSS\nintroduces a gated sentence encoder and a graph information fusion module to\ntake advantage of the graph information to polish the sentence representation.\nExperiments on a public benchmark dataset show that MUS and GSS bring\nsubstantial improvements over the prior state-of-the-art model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Neural Correlates of Linguistic Structure Building: Comments on Kazanina & Tavano (2022). (arXiv:2212.04219v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04219","description":"<p>A recent perspective paper by Kazanina &amp; Tavano (referred to as the KT\nperspective in the following) argues how neural oscillations cannot provide a\npotential neural correlate for syntactic structure building. The view that\nneural oscillations can provide a potential neural correlate for syntactic\nstructure building is largely attributed to a study by Ding, Melloni, Zhang,\nTian, and Poeppel in 2016 (referred to as the DMZTP study).\n</p>\n<p>The KT perspective is thought provoking, but has severe misinterpretations\nabout the arguments in DMZTP and other studies, and contains contradictory\nconclusions in different parts of the perspective, making it impossible to\nunderstand the position of the authors. In the following, I summarize a few\nmisinterpretations and inconsistent arguments in the KT perspective, and put\nforward a few suggestions for future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nai Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level Natural Language Explanations. (arXiv:2212.04231v1 [cs.CV])","link":"http://arxiv.org/abs/2212.04231","description":"<p>Natural language explanations promise to offer intuitively understandable\nexplanations of a neural network's decision process in complex vision-language\ntasks, as pursued in recent VL-NLE models. While current models offer\nimpressive performance on task accuracy and explanation plausibility, they\nsuffer from a range of issues: Some models feature a modular design where the\nexplanation generation module is poorly integrated with a separate module for\ntask-answer prediction, employ backbone models trained on limited sets of\ntasks, or incorporate ad hoc solutions to increase performance on single\ndatasets. We propose to evade these limitations by applying recent advances in\nlarge-scale multi-task pretraining of generative Transformer models to the\nproblem of VL-NLE tasks. Our approach outperforms recent models by a large\nmargin, with human annotators preferring the generated explanations over the\nground truth in two out of three evaluated datasets. As a novel challenge in\nVL-NLE research, we propose the problem of multi-task VL-NLE and show that\njointly training on multiple tasks can increase the explanation quality. We\ndiscuss the ethical implications of high-quality NLE generation and other\nissues in recent VL-NLE research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pluster_B/0/1/0/all/0/1\">Bj&#xf6;rn Pl&#xfc;ster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambsdorf_J/0/1/0/all/0/1\">Jakob Ambsdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braach_L/0/1/0/all/0/1\">Lukas Braach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Momentum Calibration for Text Generation. (arXiv:2212.04257v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04257","description":"<p>The input and output of most text generation tasks can be transformed to two\nsequences of tokens and they can be modeled using sequence-to-sequence learning\nmodeling tools such as Transformers. These models are usually trained by\nmaximizing the likelihood the output text sequence and assumes the input\nsequence and all gold preceding tokens are given during training, while during\ninference the model suffers from the exposure bias problem (i.e., it only has\naccess to its previously predicted tokens rather gold tokens during beam\nsearch). In this paper, we propose MoCa ({\\bf Mo}mentum {\\bf Ca}libration) for\ntext generation. MoCa is an online method that dynamically generates slowly\nevolving (but consistent) samples using a momentum moving average generator\nwith beam search and MoCa learns to align its model scores of these samples\nwith their actual qualities. Experiments on four text generation datasets\n(i.e., CNN/DailyMail, XSum, SAMSum and Gigaword) show MoCa consistently\nimproves strong pre-trained transformers using vanilla fine-tuning and we\nachieve the state-of-the-art results on CNN/DailyMail and SAMSum datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si-Qing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wayne Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConsistTL: Modeling Consistency in Transfer Learning for Low-Resource Neural Machine Translation. (arXiv:2212.04262v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04262","description":"<p>Transfer learning is a simple and powerful method that can be used to boost\nmodel performance of low-resource neural machine translation (NMT). Existing\ntransfer learning methods for NMT are static, which simply transfer knowledge\nfrom a parent model to a child model once via parameter initialization. In this\npaper, we propose a novel transfer learning method for NMT, namely ConsistTL,\nwhich can continuously transfer knowledge from the parent model during the\ntraining of the child model. Specifically, for each training instance of the\nchild model, ConsistTL constructs the semantically-equivalent instance for the\nparent model and encourages prediction consistency between the parent and child\nfor this instance, which is equivalent to the child model learning each\ninstance under the guidance of the parent model. Experimental results on five\nlow-resource NMT tasks demonstrate that ConsistTL results in significant\nimprovements over strong transfer learning baselines, with a gain up to 1.7\nBLEU over the existing back-translation model on the widely-used WMT17\nTurkish-English benchmark. Further analysis reveals that ConsistTL can improve\nthe inference calibration of the child model. Code and scripts are freely\navailable at https://github.com/NLP2CT/ConsistTL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaocong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Modality-level Explainable Framework for Misinformation Checking in Social Networks. (arXiv:2212.04272v1 [cs.LG])","link":"http://arxiv.org/abs/2212.04272","description":"<p>The widespread of false information is a rising concern worldwide with\ncritical social impact, inspiring the emergence of fact-checking organizations\nto mitigate misinformation dissemination. However, human-driven verification\nleads to a time-consuming task and a bottleneck to have checked trustworthy\ninformation at the same pace they emerge. Since misinformation relates not only\nto the content itself but also to other social features, this paper addresses\nautomatic misinformation checking in social networks from a multimodal\nperspective. Moreover, as simply naming a piece of news as incorrect may not\nconvince the citizen and, even worse, strengthen confirmation bias, the\nproposal is a modality-level explainable-prone misinformation classifier\nframework. Our framework comprises a misinformation classifier assisted by\nexplainable methods to generate modality-oriented explainable inferences.\nPreliminary findings show that the misinformation classifier does benefit from\nmultimodal information encoding and the modality-oriented explainable mechanism\nincreases both inferences' interpretability and completeness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lourenco_V/0/1/0/all/0/1\">V&#xed;tor Louren&#xe7;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paes_A/0/1/0/all/0/1\">Aline Paes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lie detection algorithms attract few users but vastly increase accusation rates. (arXiv:2212.04277v1 [econ.GN])","link":"http://arxiv.org/abs/2212.04277","description":"<p>People are not very good at detecting lies, which may explain why they\nrefrain from accusing others of lying, given the social costs attached to false\naccusations - both for the accuser and the accused. Here we consider how this\nsocial balance might be disrupted by the availability of lie-detection\nalgorithms powered by Artificial Intelligence. Will people elect to use lie\ndetection algorithms that perform better than humans, and if so, will they show\nless restraint in their accusations? We built a machine learning classifier\nwhose accuracy (67\\%) was significantly better than human accuracy (50\\%) in a\nlie-detection task and conducted an incentivized lie-detection experiment in\nwhich we measured participants' propensity to use the algorithm, as well as the\nimpact of that use on accusation rates. We find that the few people (33\\%) who\nelect to use the algorithm drastically increase their accusation rates (from\n25\\% in the baseline condition up to 86% when the algorithm flags a statement\nas a lie). They make more false accusations (18pp increase), but at the same\ntime, the probability of a lie remaining undetected is much lower in this group\n(36pp decrease). We consider individual motivations for using lie detection\nalgorithms and the social implications of these algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/econ/1/au:+Schenk_A/0/1/0/all/0/1\">Alicia von Schenk</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Klockmann_V/0/1/0/all/0/1\">Victor Klockmann</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Bonnefon_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Bonnefon</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Rahwan_I/0/1/0/all/0/1\">Iyad Rahwan</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Kobis_N/0/1/0/all/0/1\">Nils K&#xf6;bis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Montague semantics and modifier consistency measurement in neural language models. (arXiv:2212.04310v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04310","description":"<p>The recent dominance of distributional language representation models has\nelicited a variety of questions regarding their capabilities and intrinsic\nproperties, one of which is the manifestation of compositional phenomena in\nnatural language, which has significant implications towards explainability and\nsafety/fairness in the use of such models. While most current research on\ncompositionality has been directed towards improving performance of the\nrepresentations on similarity tasks, this work proposes a methodology for\nmeasuring the presence of compositional behaviour in contemporary language\nmodels related to adjectival modifier phenomena in adjective-noun phrases. Our\nresults show that current neural language models do not behave consistently\naccording to the linguistic theories with regard to the evaluated intersective\nproperty, but on the other hand, the differences between adjective categories\nare noticeable in single adjective interactions, indicating that such\ndifferences are encoded in individual word representations, but they do not\ntransfer generally in the expected way to the compositions. This raises the\nquestion of whether current language models are not capable of capturing the\ntrue underlying distributional properties of language, or whether linguistic\ntheories from Montagovian tradition do not hold to distributional scrutiny.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_D/0/1/0/all/0/1\">Danilo S. Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manino_E/0/1/0/all/0/1\">Edoardo Manino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozanova_J/0/1/0/all/0/1\">Julia Rozanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordeiro_L/0/1/0/all/0/1\">Lucas Cordeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers. (arXiv:2212.04325v1 [eess.AS])","link":"http://arxiv.org/abs/2212.04325","description":"<p>Recently, RNN-Transducers have achieved remarkable results on various\nautomatic speech recognition tasks. However, lattice-free sequence\ndiscriminative training methods, which obtain superior performance in hybrid\nmodes, are rarely investigated in RNN-Transducers. In this work, we propose\nthree lattice-free training objectives, namely lattice-free maximum mutual\ninformation, lattice-free segment-level minimum Bayes risk, and lattice-free\nminimum Bayes risk, which are used for the final posterior output of the\nphoneme-based neural transducer with a limited context dependency. Compared to\ncriteria using N-best lists, lattice-free methods eliminate the decoding step\nfor hypotheses generation during training, which leads to more efficient\ntraining. Experimental results show that lattice-free methods gain up to 6.5%\nrelative improvement in word error rate compared to a sequence-level\ncross-entropy trained model. Compared to the N-best-list based minimum Bayes\nrisk objectives, lattice-free methods gain 40% - 70% relative training time\nspeedup with a small degradation in performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Zijian Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit causality in GPT-2: a case study. (arXiv:2212.04348v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04348","description":"<p>This case study investigates the extent to which a language model (GPT-2) is\nable to capture native speakers' intuitions about implicit causality in a\nsentence completion task. We first reproduce earlier results (showing lower\nsurprisal values for pronouns that are congruent with either the subject or\nobject, depending on which one corresponds to the implicit causality bias of\nthe verb), and then examine the effects of gender and verb frequency on model\nperformance. Our second study examines the reasoning ability of GPT-2: is the\nmodel able to produce more sensible motivations for why the subject VERBed the\nobject if the verbs have stronger causality biases? We also developed a\nmethodology to avoid human raters being biased by obscenities and disfluencies\ngenerated by the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_H/0/1/0/all/0/1\">Hien Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lentz_T/0/1/0/all/0/1\">Tomas O. Lentz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miltenburg_E/0/1/0/all/0/1\">Emiel van Miltenburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Speech Recognition via Large-Scale Weak Supervision. (arXiv:2212.04356v1 [eess.AS])","link":"http://arxiv.org/abs/2212.04356","description":"<p>We study the capabilities of speech processing systems trained simply to\npredict large amounts of transcripts of audio on the internet. When scaled to\n680,000 hours of multilingual and multitask supervision, the resulting models\ngeneralize well to standard benchmarks and are often competitive with prior\nfully supervised results but in a zero-shot transfer setting without the need\nfor any fine-tuning. When compared to humans, the models approach their\naccuracy and robustness. We are releasing models and inference code to serve as\na foundation for further work on robust speech processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Radford_A/0/1/0/all/0/1\">Alec Radford</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Jong Wook Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_T/0/1/0/all/0/1\">Tao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brockman_G/0/1/0/all/0/1\">Greg Brockman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McLeavey_C/0/1/0/all/0/1\">Christine McLeavey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sutskever_I/0/1/0/all/0/1\">Ilya Sutskever</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVBert: Topo-Metric Map Pre-training for Language-guided Navigation. (arXiv:2212.04385v1 [cs.CV])","link":"http://arxiv.org/abs/2212.04385","description":"<p>Existing approaches for vision-and-language navigation (VLN) are mainly based\non cross-modal reasoning over discrete views. However, this scheme may hamper\nan agent's spatial and numerical reasoning because of incomplete objects within\na single view and duplicate observations across views. A potential solution is\nmapping discrete views into a unified birds's-eye view, which can aggregate\npartial and duplicate observations. Existing metric maps could achieve this\ngoal, but they suffer from less expressive semantics (e.g. usually predefined\nlabels) and limited map size, which weakens an agent's language grounding and\nlong-term planning ability. Inspired by the robotics community, we introduce\nhybrid topo-metric maps into VLN, where a topological map is used for long-term\nplanning and a metric map for short-term reasoning. Beyond mapping with more\nexpressive deep features, we further design a pre-training framework via the\nhybrid map to learn language-informed map representations, which enhances\ncross-modal grounding and facilitates the final language-guided navigation\ngoal. Extensive experiments demonstrate the effectiveness of the map-based\nroute for VLN, and the proposed method sets the new state-of-the-art on three\nVLN benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Dong An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models. (arXiv:2212.04408v1 [cs.CV])","link":"http://arxiv.org/abs/2212.04408","description":"<p>Generalist models, which are capable of performing diverse multi-modal tasks\nin a task-agnostic way within a single model, have been explored recently.\nBeing, hopefully, an alternative to approaching general-purpose AI, existing\ngeneralist models are still at an early stage, where modality and task coverage\nis limited. To empower multi-modal task-scaling and speed up this line of\nresearch, we release a generalist model learning system, OFASys, built on top\nof a declarative task interface named multi-modal instruction. At the core of\nOFASys is the idea of decoupling multi-modal task representations from the\nunderlying model implementations. In OFASys, a task involving multiple\nmodalities can be defined declaratively even with just a single line of code.\nThe system automatically generates task plans from such instructions for\ntraining and inference. It also facilitates multi-task training for diverse\nmulti-modal workloads. As a starting point, we provide presets of 7 different\nmodalities and 23 highly-diverse example tasks in OFASys, with which we also\ndevelop a first-in-kind, single model, OFA+, that can handle text, image,\nspeech, video, and motion data. The single OFA+ model achieves 95% performance\nin average with only 16% parameters of 15 task-finetuned models, showcasing the\nperformance reliability of multi-modal task-scaling provided by OFASys.\nAvailable at https://github.com/OFA-Sys/OFASys\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinze Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_K/0/1/0/all/0/1\">Kai Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaohuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Sinan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zeyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Shuai Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wenbin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning. (arXiv:2105.03654v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.03654","description":"<p>Recent advances in Named Entity Recognition (NER) show that document-level\ncontexts can significantly improve model performance. In many application\nscenarios, however, such contexts are not available. In this paper, we propose\nto find external contexts of a sentence by retrieving and selecting a set of\nsemantically relevant texts through a search engine, with the original sentence\nas the query. We find empirically that the contextual representations computed\non the retrieval-based input view, constructed through the concatenation of a\nsentence and its external contexts, can achieve significantly improved\nperformance compared to the original input view based only on the sentence.\nFurthermore, we can improve the model performance of both input views by\nCooperative Learning, a training method that encourages the two input views to\nproduce similar contextual representations or output label distributions.\nExperiments show that our approach can achieve new state-of-the-art performance\non 8 NER data sets across 5 domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1\">Nguyen Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confounds and Overestimations in Fake Review Detection: Experimentally Controlling for Product-Ownership and Data-Origin. (arXiv:2110.15130v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15130","description":"<p>The popularity of online shopping is steadily increasing. At the same time,\nfake product reviewsare published widely and have the potential to affect\nconsumer purchasing behavior. In response,previous work has developed automated\nmethods for the detection of deceptive product reviews.However, studies vary\nconsiderably in terms of classification performance, and many use data\nthatcontain potential confounds, which makes it difficult to determine their\nvalidity. Two possibleconfounds are data-origin (i.e., the dataset is composed\nof more than one source) and productownership (i.e., reviews written by\nindividuals who own or do not own the reviewed product). Inthe present study,\nwe investigate the effect of both confounds for fake review detection. Using\nanexperimental design, we manipulate data-origin, product ownership, review\npolarity, and veracity.Supervised learning analysis suggests that review\nveracity (60.26 - 69.87%) is somewhat detectablebut reviews additionally\nconfounded with product-ownership (66.19 - 74.17%), or with data-origin(84.44 -\n86.94%) are easier to classify. Review veracity is most easily classified if\nconfounded withproduct-ownership and data-origin combined (87.78 - 88.12%),\nsuggesting overestimations of thetrue performance in other work. These findings\nare moderated by review polarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soldner_F/0/1/0/all/0/1\">Felix Soldner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_B/0/1/0/all/0/1\">Bennett Kleinberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Shane Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning for Few-Shot Text Generation Adaptation. (arXiv:2111.11030v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.11030","description":"<p>Controlling the generative model to adapt a new domain with limited samples\nis a difficult challenge and it is receiving increasing attention. Recently,\nmethods based on meta-learning have shown promising results for few-shot domain\nadaptation. However, meta-learning-based methods usually suffer from the\nproblem of overfitting, which results in a lack of diversity in the generated\ntexts. To avoid this problem, in this study, a novel framework based on\nreinforcement learning (RL) is proposed. In this framework, to increase the\nsample utilization of RL and decrease its sample requirement, maximum\nlikelihood estimation learning is incorporated into the RL process. When there\nare only a few in-domain samples available, experimental results on five target\ndomains in two few-shot configurations show that this framework performs better\nthan baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pengsen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jinqiao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiamiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_P/0/1/0/all/0/1\">Peng Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Context-Word Biases in Lexical Semantic Datasets. (arXiv:2112.06733v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06733","description":"<p>State-of-the-art pretrained contextualized models (PCM) eg. BERT use tasks\nsuch as WiC and WSD to evaluate their word-in-context representations. This\ninherently assumes that performance in these tasks reflect how well a model\nrepresents the coupled word and context semantics. We question this assumption\nby presenting the first quantitative analysis on the context-word interaction\nbeing tested in major contextual lexical semantic tasks. To achieve this, we\nrun probing baselines on masked input, and propose measures to calculate and\nvisualize the degree of context or word biases in existing datasets. The\nanalysis was performed on both models and humans. Our findings demonstrate that\nmodels are usually not being tested for word-in-context semantics in the same\nway as humans are in these tasks, which helps us better understand the\nmodel-human gap. Specifically, to PCMs, most existing datasets fall into the\nextreme ends (the retrieval-based tasks exhibit strong target word bias while\nWiC-style tasks and WSD show strong context bias); In comparison, humans are\nless biased and achieve much better performance when both word and context are\navailable than with masked input. We recommend our framework for understanding\nand controlling these biases for model interpretation and future task design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianchu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_D/0/1/0/all/0/1\">Diana McCarthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Know Where You're Going: Meta-Learning for Parameter-Efficient Fine-Tuning. (arXiv:2205.12453v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12453","description":"<p>A recent family of techniques, dubbed lightweight fine-tuning methods,\nfacilitates parameter-efficient transfer learning by updating only a small set\nof additional parameters while keeping the parameters of the pretrained\nlanguage model frozen. While proven to be an effective method, there are no\nexisting studies on if and how such knowledge of the downstream fine-tuning\napproach should affect the pretraining stage. In this work, we show that taking\nthe ultimate choice of fine-tuning method into consideration boosts the\nperformance of parameter-efficient fine-tuning. By relying on\noptimization-based meta-learning using MAML with certain modifications for our\ndistinct purpose, we prime the pretrained model specifically for\nparameter-efficient fine-tuning, resulting in gains of up to 1.7 points on\ncross-lingual NER fine-tuning. Our ablation settings and analyses further\nreveal that the tweaks we introduce in MAML are crucial for the attained gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gheini_M/0/1/0/all/0/1\">Mozhdeh Gheini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuezhe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Memory Transformer. (arXiv:2207.06881v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.06881","description":"<p>Transformer-based models show their effectiveness across multiple domains and\ntasks. The self-attention allows to combine information from all sequence\nelements into context-aware representations. However, global and local\ninformation has to be stored mostly in the same element-wise representations.\nMoreover, the length of an input sequence is limited by quadratic computational\ncomplexity of self-attention.\n</p>\n<p>In this work, we propose and study a memory-augmented segment-level recurrent\nTransformer (RMT). Memory allows to store and process local and global\ninformation as well as to pass information between segments of the long\nsequence with the help of recurrence.\n</p>\n<p>We implement a memory mechanism with no changes to Transformer model by\nadding special memory tokens to the input or output sequence. Then the model is\ntrained to control both memory operations and sequence representations\nprocessing.\n</p>\n<p>Results of experiments show that RMT performs on par with the Transformer-XL\non language modeling for smaller memory sizes and outperforms it for tasks that\nrequire longer sequence processing. We show that adding memory tokens to Tr-XL\nis able to improve its performance. This makes Recurrent Memory Transformer a\npromising architecture for applications that require learning of long-term\ndependencies and general purpose in memory processing, such as algorithmic\ntasks and reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bulatov_A/0/1/0/all/0/1\">Aydar Bulatov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuratov_Y/0/1/0/all/0/1\">Yuri Kuratov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail S. Burtsev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What the DAAM: Interpreting Stable Diffusion Using Cross Attention. (arXiv:2210.04885v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.04885","description":"<p>Large-scale diffusion neural networks represent a substantial milestone in\ntext-to-image generation, but they remain poorly understood, lacking\ninterpretability analyses. In this paper, we perform a text-image attribution\nanalysis on Stable Diffusion, a recently open-sourced model. To produce\npixel-level attribution maps, we upscale and aggregate cross-attention\nword-pixel scores in the denoising subnetwork, naming our method DAAM. We\nevaluate its correctness by testing its semantic segmentation ability on nouns,\nas well as its generalized attribution quality on all parts of speech, rated by\nhumans. We then apply DAAM to study the role of syntax in the pixel space,\ncharacterizing head--dependent heat map interaction patterns for ten common\ndependency relations. Finally, we study several semantic phenomena using DAAM,\nwith a focus on feature entanglement, where we find that cohyponyms worsen\ngeneration quality and descriptive adjectives attend too broadly. To our\nknowledge, we are the first to interpret large diffusion models from a\nvisuolinguistic perspective, which enables future lines of research. Our code\nis at https://github.com/castorini/daam.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Raphael Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Akshat Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhiying Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Gefei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1\">Karun Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ture_F/0/1/0/all/0/1\">Ferhan Ture</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-3-driven pedagogical agents for training children's curious question-asking skills. (arXiv:2211.14228v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.14228","description":"<p>Students' ability to ask curious questions is a crucial skill that improves\ntheir learning processes. To train this skill, previous research has used a\nconversational agent that propose specific cues to prompt children's curiosity\nduring learning. Despite showing pedagogical efficiency, this method is still\nlimited since it relies on generating the said prompts by hand for each\neducational resource, which can be a very long and costly process. In this\ncontext, we leverage the advances in the natural language processing field and\nexplore using a large language model (GPT-3) to automate the generation of this\nagent's curiosity-prompting cues to help children ask more and deeper\nquestions. We then used this study to investigate a different\ncuriosity-prompting behavior for the agent. The study was conducted with 75\nstudents aged between 9 and 10. They either interacted with a hand-crafted\nconversational agent that proposes \"closed\" manually-extracted cues leading to\npredefined questions, a GPT-3-driven one that proposes the same type of cues,\nor a GPT-3-driven one that proposes \"open\" cues that can lead to several\npossible questions. Results showed a similar question-asking performance\nbetween children who had the two \"closed\" agents, but a significantly better\none for participants with the \"open\" agent. Our first results suggest the\nvalidity of using GPT-3 to facilitate the implementation of\ncuriosity-stimulating learning technologies. In a second step, we also show\nthat GPT-3 can be efficient in proposing the relevant open cues that leave\nchildren with more autonomy to express their curiosity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelghani_R/0/1/0/all/0/1\">Rania Abdelghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yen-Hsiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauzeon_H/0/1/0/all/0/1\">H&#xe9;l&#xe8;ne Sauz&#xe9;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event knowledge in large language models: the gap between the impossible and the unlikely. (arXiv:2212.01488v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01488","description":"<p>People constantly use language to learn about the world. Computational\nlinguists have capitalized on this fact to build large language models (LLMs)\nthat acquire co-occurrence-based knowledge from language corpora. LLMs achieve\nimpressive performance on many tasks, but the robustness of their world\nknowledge has been questioned. Here, we ask: do LLMs acquire generalized\nknowledge about real-world events? Using curated sets of minimal sentence pairs\n(n=1215), we tested whether LLMs are more likely to generate plausible event\ndescriptions compared to their implausible counterparts. We found that LLMs\nsystematically distinguish possible and impossible events (The teacher bought\nthe laptop vs. The laptop bought the teacher) but fall short of human\nperformance when distinguishing likely and unlikely events (The nanny tutored\nthe boy vs. The boy tutored the nanny). In follow-up analyses, we show that (i)\nLLM scores are driven by both plausibility and surface-level sentence features,\n(ii) LLMs generalize well across syntactic sentence variants (active vs\npassive) but less well across semantic sentence variants (synonymous\nsentences), (iii) some, but not all LLM deviations from ground-truth labels\nalign with crowdsourced human judgments, and (iv) explicit event plausibility\ninformation emerges in middle LLM layers and remains high thereafter. Overall,\nour analyses reveal a gap in LLMs' event knowledge, highlighting their\nlimitations as generalized knowledge bases. We conclude by speculating that the\ndifferential performance on impossible vs. unlikely events is not a temporary\nsetback but an inherent property of LLMs, reflecting a fundamental difference\nbetween linguistic knowledge and world knowledge in intelligent systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kauf_C/0/1/0/all/0/1\">Carina Kauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanova_A/0/1/0/all/0/1\">Anna A. Ivanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambelli_G/0/1/0/all/0/1\">Giulia Rambelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1\">Emmanuele Chersoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_J/0/1/0/all/0/1\">Jingyuan S. She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_Z/0/1/0/all/0/1\">Zawad Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1\">Evelina Fedorenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1\">Alessandro Lenci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generative Approach for Script Event Prediction via Contrastive Fine-tuning. (arXiv:2212.03496v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.03496","description":"<p>Script event prediction aims to predict the subsequent event given the\ncontext. This requires the capability to infer the correlations between events.\nRecent works have attempted to improve event correlation reasoning by using\npretrained language models and incorporating external knowledge~(e.g.,\ndiscourse relations). Though promising results have been achieved, some\nchallenges still remain. First, the pretrained language models adopted by\ncurrent works ignore event-level knowledge, resulting in an inability to\ncapture the correlations between events well. Second, modeling correlations\nbetween events with discourse relations is limited because it can only capture\nexplicit correlations between events with discourse markers, and cannot capture\nmany implicit correlations. To this end, we propose a novel generative approach\nfor this task, in which a pretrained language model is fine-tuned with an\nevent-centric pretraining objective and predicts the next event within a\ngenerative paradigm. Specifically, we first introduce a novel event-level blank\ninfilling strategy as the learning objective to inject event-level knowledge\ninto the pretrained language model, and then design a likelihood-based\ncontrastive loss for fine-tuning the generative model. Instead of using an\nadditional prediction layer, we perform prediction by using sequence\nlikelihoods generated by the generative model. Our approach models correlations\nbetween events in a soft way without any external knowledge. The\nlikelihood-based prediction eliminates the need to use additional networks to\nmake predictions and is somewhat interpretable since it scores each word in the\nevent. Experimental results on the multi-choice narrative cloze~(MCNC) task\ndemonstrate that our approach achieves better results than other\nstate-of-the-art baselines. Our code will be available at\nhttps://github.com/zhufq00/mcnc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_X/0/1/0/all/0/1\">Xin Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks. (arXiv:2212.03613v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.03613","description":"<p>Recently, domain-specific PLMs have been proposed to boost the task\nperformance of specific domains (e.g., biomedical and computer science) by\ncontinuing to pre-train general PLMs with domain-specific corpora. However,\nthis Domain-Adaptive Pre-Training (DAPT; Gururangan et al. (2020)) tends to\nforget the previous general knowledge acquired by general PLMs, which leads to\na catastrophic forgetting phenomenon and sub-optimal performance. To alleviate\nthis problem, we propose a new framework of General Memory Augmented\nPre-trained Language Model (G-MAP), which augments the domain-specific PLM by a\nmemory representation built from the frozen general PLM without losing any\ngeneral knowledge. Specifically, we propose a new memory-augmented layer, and\nbased on it, different augmented strategies are explored to build the memory\nrepresentation and then adaptively fuse it into the domain-specific PLM. We\ndemonstrate the effectiveness of G-MAP on various domains (biomedical and\ncomputer science publications, news, and reviews) and different kinds (text\nclassification, QA, NER) of tasks, and the extensive results show that the\nproposed G-MAP can achieve SOTA results on all tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhongwei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-12-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}