{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-12T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models. (arXiv:2301.04213v1 [cs.LG])","link":"http://arxiv.org/abs/2301.04213","description":"<p>Language models are known to learn a great quantity of factual information\nduring pretraining, and recent work localizes this information to specific\nmodel weights like mid-layer MLP weights (Meng et al., 2022). In this paper, we\nfind that we can change how a fact is stored in a model by editing weights that\nare in a different location than where existing methods suggest that the fact\nis stored. This is surprising because we would expect that localizing facts to\nspecific parameters in models would tell us where to manipulate knowledge in\nmodels, and this assumption has motivated past work on model editing methods.\nSpecifically, we show that localization conclusions from representation\ndenoising (also known as Causal Tracing) do not provide any insight into which\nmodel MLP layer would be best to edit in order to override an existing stored\nfact with a new one. This finding raises questions about how past work relies\non Causal Tracing to select which model layers to edit (Meng et al., 2022).\nNext, to better understand the discrepancy between representation denoising and\nweight editing, we develop several variants of the editing problem that appear\nmore and more like representation denoising in their design and objective.\nExperiments show that, for one of our editing problems, editing performance\ndoes relate to localization results from representation denoising, but we find\nthat which layer we edit is a far better predictor of performance. Our results\nsuggest, counterintuitively, that better mechanistic understanding of how\npretrained language models work may not always translate to insights about how\nto best change their behavior. Code is available at:\nhttps://github.com/google/belief-localization\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Been Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghandeharioun_A/0/1/0/all/0/1\">Asma Ghandeharioun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User-Centered Security in Natural Language Processing. (arXiv:2301.04230v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04230","description":"<p>This dissertation proposes a framework of user-centered security in Natural\nLanguage Processing (NLP), and demonstrates how it can improve the\naccessibility of related research. Accordingly, it focuses on two security\ndomains within NLP with great public interest. First, that of author profiling,\nwhich can be employed to compromise online privacy through invasive inferences.\nWithout access and detailed insight into these models' predictions, there is no\nreasonable heuristic by which Internet users might defend themselves from such\ninferences. Secondly, that of cyberbullying detection, which by default\npresupposes a centralized implementation; i.e., content moderation across\nsocial platforms. As access to appropriate data is restricted, and the nature\nof the task rapidly evolves (both through lexical variation, and cultural\nshifts), the effectiveness of its classifiers is greatly diminished and thereby\noften misrepresented.\n</p>\n<p>Under the proposed framework, we predominantly investigate the use of\nadversarial attacks on language; i.e., changing a given input (generating\nadversarial samples) such that a given model does not function as intended.\nThese attacks form a common thread between our user-centered security problems;\nthey are highly relevant for privacy-preserving obfuscation methods against\nauthor profiling, and adversarial samples might also prove useful to assess the\ninfluence of lexical variation and augmentation on cyberbullying detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Emmery_C/0/1/0/all/0/1\">Chris Emmery</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Hateful Discussions on Reddit using Graph Transformer Networks and Communal Context. (arXiv:2301.04248v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04248","description":"<p>We propose a system to predict harmful discussions on social media platforms.\nOur solution uses contextual deep language models and proposes the novel idea\nof integrating state-of-the-art Graph Transformer Networks to analyze all\nconversations that follow an initial post. This framework also supports\nadapting to future comments as the conversation unfolds. In addition, we study\nwhether a community-specific analysis of hate speech leads to more effective\ndetection of hateful discussions. We evaluate our approach on 333,487 Reddit\ndiscussions from various communities. We find that community-specific modeling\nimproves performance two-fold and that models which capture wider-discussion\ncontext improve accuracy by 28\\% (35\\% for the most hateful content) compared\nto limited context models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hebert_L/0/1/0/all/0/1\">Liam Hebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golab_L/0/1/0/all/0/1\">Lukasz Golab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1\">Robin Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClimaBench: A Benchmark Dataset For Climate Change Text Understanding in English. (arXiv:2301.04253v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04253","description":"<p>The topic of Climate Change (CC) has received limited attention in NLP\ndespite its real world urgency. Activists and policy-makers need NLP tools in\norder to effectively process the vast and rapidly growing textual data produced\non CC. Their utility, however, primarily depends on whether the current\nstate-of-the-art models can generalize across various tasks in the CC domain.\nIn order to address this gap, we introduce Climate Change Benchmark\n(ClimaBench), a benchmark collection of existing disparate datasets for\nevaluating model performance across a diverse set of CC NLU tasks\nsystematically. Further, we enhance the benchmark by releasing two large-scale\nlabelled text classification and question-answering datasets curated from\npublicly available environmental disclosures. Lastly, we provide an analysis of\nseveral generic and CC-oriented models answering whether fine-tuning on domain\ntext offers any improvements across these tasks. We hope this work provides a\nstandard assessment tool for research on CC text data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laud_T/0/1/0/all/0/1\">Tanmay Laud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spokoyny_D/0/1/0/all/0/1\">Daniel Spokoyny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corringham_T/0/1/0/all/0/1\">Tom Corringham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Modal Geographic Pre-Training Method. (arXiv:2301.04283v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04283","description":"<p>As a core task in location-based services (LBS) (e.g., navigation maps),\nquery and point of interest (POI) matching connects users' intent with\nreal-world geographic information. Recently, pre-trained models (PTMs) have\nmade advancements in many natural language processing (NLP) tasks. Generic\ntext-based PTMs do not have enough geographic knowledge for query-POI matching.\nTo overcome this limitation, related literature attempts to employ\ndomain-adaptive pre-training based on geo-related corpus. However, a query\ngenerally contains mentions of multiple geographic objects, such as nearby\nroads and regions of interest (ROIs). The geographic context (GC), i.e., these\ndiverse geographic objects and their relationships, is therefore pivotal to\nretrieving the most relevant POI. Single-modal PTMs can barely make use of the\nimportant GC and therefore have limited performance. In this work, we propose a\nnovel query-POI matching method Multi-modal Geographic language model (MGeo),\nwhich comprises a geographic encoder and a multi-modal interaction module. MGeo\nrepresents GC as a new modality and is able to fully extract multi-modal\ncorrelations for accurate query-POI matching. Besides, there is no publicly\navailable benchmark for this topic. In order to facilitate further research, we\nbuild a new open-source large-scale benchmark Geographic TExtual Similarity\n(GeoTES). The POIs come from an open-source geographic information system\n(GIS). The queries are manually generated by annotators to prevent privacy\nissues. Compared with several strong baselines, the extensive experiment\nresults and detailed ablation analyses on GeoTES demonstrate that our proposed\nmulti-modal pre-training method can significantly improve the query-POI\nmatching capability of generic PTMs, even when the queries' GC is not provided.\nOur code and dataset are publicly available at\nhttps://github.com/PhantomGrapes/MGeo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Ruixue Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word-Graph2vec: An efficient word embedding approach on word co-occurrence graph using random walk sampling. (arXiv:2301.04312v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04312","description":"<p>Word embedding has become ubiquitous and is widely used in various text\nmining and natural language processing (NLP) tasks, such as information\nretrieval, semantic analysis, and machine translation, among many others.\nUnfortunately, it is prohibitively expensive to train the word embedding in a\nrelatively large corpus. We propose a graph-based word embedding algorithm,\ncalled Word-Graph2vec, which converts the large corpus into a word\nco-occurrence graph, then takes the word sequence samples from this graph by\nrandomly traveling and trains the word embedding on this sampling corpus in the\nend. We posit that because of the stable vocabulary, relative idioms, and fixed\nexpressions in English, the size and density of the word co-occurrence graph\nchange slightly with the increase in the training corpus. So that\nWord-Graph2vec has stable runtime on the large scale data set, and its\nperformance advantage becomes more and more obvious with the growth of the\ntraining corpus. Extensive experiments conducted on real-world datasets show\nthat the proposed algorithm outperforms traditional Skip-Gram by four-five\ntimes in terms of efficiency, while the error generated by the random walk\nsampling is small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanzhe Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Learning for Large Vocabulary On-Device ASR. (arXiv:2301.04327v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04327","description":"<p>Dual learning is a paradigm for semi-supervised machine learning that seeks\nto leverage unsupervised data by solving two opposite tasks at once. In this\nscheme, each model is used to generate pseudo-labels for unlabeled examples\nthat are used to train the other model. Dual learning has seen some use in\nspeech processing by pairing ASR and TTS as dual tasks. However, these results\nmostly address only the case of using unpaired examples to compensate for very\nsmall supervised datasets, and mostly on large, non-streaming models. Dual\nlearning has not yet been proven effective for using unsupervised data to\nimprove realistic on-device streaming models that are already trained on large\nsupervised corpora. We provide this missing piece though an analysis of an\non-device-sized streaming conformer trained on the entirety of Librispeech,\nshowing relative WER improvements of 10.7%/5.2% without an LM and 11.7%/16.4%\nwith an LM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peyser_C/0/1/0/all/0/1\">Cal Peyser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1\">Michael Picheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topics in Contextualised Attention Embeddings. (arXiv:2301.04339v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04339","description":"<p>Contextualised word vectors obtained via pre-trained language models encode a\nvariety of knowledge that has already been exploited in applications.\nComplementary to these language models are probabilistic topic models that\nlearn thematic patterns from the text. Recent work has demonstrated that\nconducting clustering on the word-level contextual representations from a\nlanguage model emulates word clusters that are discovered in latent topics of\nwords from Latent Dirichlet Allocation. The important question is how such\ntopical word clusters are automatically formed, through clustering, in the\nlanguage model when it has not been explicitly designed to model latent topics.\nTo address this question, we design different probe experiments. Using BERT and\nDistilBERT, we find that the attention framework plays a key role in modelling\nsuch word topic clusters. We strongly believe that our work paves way for\nfurther research into the relationships between probabilistic topic models and\npre-trained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Talebpour_M/0/1/0/all/0/1\">Mozhgan Talebpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrera_A/0/1/0/all/0/1\">Alba Garcia Seco de Herrera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jameel_S/0/1/0/all/0/1\">Shoaib Jameel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counteracts: Testing Stereotypical Representation in Pre-trained Language Models. (arXiv:2301.04347v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04347","description":"<p>Language models have demonstrated strong performance on various natural\nlanguage understanding tasks. Similar to humans, language models could also\nhave their own bias that is learned from the training data. As more and more\ndownstream tasks integrate language models as part of the pipeline, it is\nnecessary to understand the internal stereotypical representation and the\nmethods to mitigate the negative effects. In this paper, we proposed a simple\nmethod to test the internal stereotypical representation in pre-trained\nlanguage models using counterexamples. We mainly focused on gender bias, but\nthe method can be extended to other types of bias. We evaluated models on 9\ndifferent cloze-style prompts consisting of knowledge and base prompts. Our\nresults indicate that pre-trained language models show a certain amount of\nrobustness when using unrelated knowledge, and prefer shallow linguistic cues,\nsuch as word position and syntactic structure, to alter the internal\nstereotypical representation. Such findings shed light on how to manipulate\nlanguage models in a neutral approach for both finetuning and evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Damin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Inverse Cloze Task for Knowledge-based Visual Question Answering. (arXiv:2301.04366v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04366","description":"<p>We present a new pre-training method, Multimodal Inverse Cloze Task, for\nKnowledge-based Visual Question Answering about named Entities (KVQAE). KVQAE\nis a recently introduced task that consists in answering questions about named\nentities grounded in a visual context using a Knowledge Base. Therefore, the\ninteraction between the modalities is paramount to retrieve information and\nmust be captured with complex fusion models. As these models require a lot of\ntraining data, we design this pre-training task from existing work in textual\nQuestion Answering. It consists in considering a sentence as a pseudo-question\nand its context as a pseudo-relevant passage and is extended by considering\nimages near texts in multimodal documents. Our method is applicable to\ndifferent neural network architectures and leads to a 9% relative-MRR and 15%\nrelative-F1 gain for retrieval and reading comprehension, respectively, over a\nno-pre-training baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lerner_P/0/1/0/all/0/1\">Paul Lerner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferret_O/0/1/0/all/0/1\">Olivier Ferret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guinaudeau_C/0/1/0/all/0/1\">Camille Guinaudeau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceive and predict: self-supervised speech representation based loss functions for speech enhancement. (arXiv:2301.04388v1 [cs.SD])","link":"http://arxiv.org/abs/2301.04388","description":"<p>Recent work in the domain of speech enhancement has explored the use of\nself-supervised speech representations to aid in the training of neural speech\nenhancement models. However, much of this work focuses on using the deepest or\nfinal outputs of self supervised speech representation models, rather than the\nearlier feature encodings. The use of self supervised representations in such a\nway is often not fully motivated. In this work it is shown that the distance\nbetween the feature encodings of clean and noisy speech correlate strongly with\npsychoacoustically motivated measures of speech quality and intelligibility, as\nwell as with human Mean Opinion Score (MOS) ratings. Experiments using this\ndistance as a loss function are performed and improved performance over the use\nof STFT spectrogram distance based loss as well as other common loss functions\nfrom speech enhancement literature is demonstrated using objective measures\nsuch as perceptual evaluation of speech quality (PESQ) and short-time objective\nintelligibility (STOI).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Close_G/0/1/0/all/0/1\">George Close</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravenscroft_W/0/1/0/all/0/1\">William Ravenscroft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hain_T/0/1/0/all/0/1\">Thomas Hain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goetze_S/0/1/0/all/0/1\">Stefan Goetze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT as Knowledge Worker: A Zero-Shot Evaluation of (AI)CPA Capabilities. (arXiv:2301.04408v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04408","description":"<p>The global economy is increasingly dependent on knowledge workers to meet the\nneeds of public and private organizations. While there is no single definition\nof knowledge work, organizations and industry groups still attempt to measure\nindividuals' capability to engage in it. The most comprehensive assessment of\ncapability readiness for professional knowledge workers is the Uniform CPA\nExamination developed by the American Institute of Certified Public Accountants\n(AICPA). In this paper, we experimentally evaluate OpenAI's `text-davinci-003`\nand prior versions of GPT on both a sample Regulation (REG) exam and an\nassessment of over 200 multiple-choice questions based on the AICPA Blueprints\nfor legal, financial, accounting, technology, and ethical tasks. First, we find\nthat `text-davinci-003` achieves a correct rate of 14.4% on a sample REG exam\nsection, significantly underperforming human capabilities on quantitative\nreasoning in zero-shot prompts. Second, `text-davinci-003` appears to be\napproaching human-level performance on the Remembering &amp; Understanding and\nApplication skill levels in the Exam absent calculation. For best prompt and\nparameters, the model answers 57.6% of questions correctly, significantly\nbetter than the 25% guessing rate, and its top two answers are correct 82.1% of\nthe time, indicating strong non-entailment. Finally, we find that recent\ngenerations of GPT-3 demonstrate material improvements on this assessment,\nrising from 30% for `text-davinci-001` to 57% for `text-davinci-003`. These\nfindings strongly suggest that large language models have the potential to\ntransform the quality and efficiency of future knowledge work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bommarito_J/0/1/0/all/0/1\">Jillian Bommarito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bommarito_M/0/1/0/all/0/1\">Michael Bommarito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_D/0/1/0/all/0/1\">Daniel Martin Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_J/0/1/0/all/0/1\">Jessica Katz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Entity and Relation Extraction from Unified to Language-specific Training. (arXiv:2301.04434v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04434","description":"<p>Entity and relation extraction is a key task in information extraction, where\nthe output can be used for downstream NLP tasks. Existing approaches for entity\nand relation extraction tasks mainly focus on the English corpora and ignore\nother languages. Thus, it is critical to improving performance in a\nmultilingual setting. Meanwhile, multilingual training is usually used to boost\ncross-lingual performance by transferring knowledge from languages (e.g.,\nhigh-resource) to other (e.g., low-resource) languages. However, language\ninterference usually exists in multilingual tasks as the model parameters are\nshared among all languages. In this paper, we propose a two-stage multilingual\ntraining method and a joint model called Multilingual Entity and Relation\nExtraction framework (mERE) to mitigate language interference across languages.\nSpecifically, we randomly concatenate sentences in different languages to train\na Language-universal Aggregator (LA), which narrows the distance of embedding\nrepresentations by obtaining the unified language representation. Then, we\nseparate parameters to mitigate interference via tuning a Language-specific\nSwitcher (LS), which includes several independent sub-modules to refine the\nlanguage-specific feature representation. After that, to enhance the relational\ntriple extraction, the sentence representations concatenated with the relation\nfeature are used to recognize the entities. Extensive experimental results show\nthat our method outperforms both the monolingual and multilingual baseline\nmethods. Besides, we also perform detailed analysis to show that mERE is\nlightweight but effective on relational triple extraction and mERE{} is easy to\ntransfer to other backbone models of multi-field tasks, which further\ndemonstrates the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tongliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1\">Ying Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaqi Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Longtao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diving Deep into Modes of Fact Hallucinations in Dialogue Systems. (arXiv:2301.04449v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04449","description":"<p>Knowledge Graph(KG) grounded conversations often use large pre-trained models\nand usually suffer from fact hallucination. Frequently entities with no\nreferences in knowledge sources and conversation history are introduced into\nresponses, thus hindering the flow of the conversation -- existing work attempt\nto overcome this issue by tweaking the training procedure or using a multi-step\nrefining method. However, minimal effort is put into constructing an\nentity-level hallucination detection system, which would provide fine-grained\nsignals that control fallacious content while generating responses. As a first\nstep to address this issue, we dive deep to identify various modes of\nhallucination in KG-grounded chatbots through human feedback analysis.\nSecondly, we propose a series of perturbation strategies to create a synthetic\ndataset named FADE (FActual Dialogue Hallucination DEtection Dataset). Finally,\nwe conduct comprehensive data analyses and create multiple baseline models for\nhallucination detection to compare against human-verified data and already\nestablished benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Souvik Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sougata Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_R/0/1/0/all/0/1\">Rohini K. Srihari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deteksi Depresi dan Kecemasan Pengguna Twitter Menggunakan Bidirectional LSTM. (arXiv:2301.04521v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04521","description":"<p>The most common mental disorders experienced by a person in daily life are\ndepression and anxiety. Social stigma makes people with depression and anxiety\nneglected by their surroundings. Therefore, they turn to social media like\nTwitter for support. Detecting users with potential depression and anxiety\ndisorders through textual data is not easy because they do not explicitly\ndiscuss their mental state. It takes a model that can identify potential users\nwho experience depression and anxiety on textual data to get treatment earlier.\nText classification techniques can achieve this. One approach that can be used\nis LSTM as an RNN architecture development in dealing with vanishing gradient\nproblems. Standard LSTM does not capture enough information because it can only\nread sentences from one direction. Meanwhile, Bidirectional LSTM (BiLSTM) is a\ntwo-way LSTM that can capture information without ignoring the context and\nmeaning of a sentence. The proposed BiLSTM model is higher than all traditional\nmachine learning models and standard LSTMs. Based on the test results, the\nhighest accuracy obtained by BiLSTM reached 94.12%. This study has succeeded in\ndeveloping a model for the detection of depression and anxiety in Twitter\nusers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1\">Kuncahyo Setyo Nugroho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbar_I/0/1/0/all/0/1\">Ismail Akbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suksmawati_A/0/1/0/all/0/1\">Affi Nizar Suksmawati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Istiadi/0/1/0/all/0/1\">Istiadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Role of Interactive Visualization in Explaining (Large) NLP Models: from Data to Inference. (arXiv:2301.04528v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04528","description":"<p>With a constant increase of learned parameters, modern neural language models\nbecome increasingly more powerful. Yet, explaining these complex model's\nbehavior remains a widely unsolved problem. In this paper, we discuss the role\ninteractive visualization can play in explaining NLP models (XNLP). We motivate\nthe use of visualization in relation to target users and common NLP pipelines.\nWe also present several use cases to provide concrete examples on XNLP with\nvisualization. Finally, we point out an extensive list of research\nopportunities in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brath_R/0/1/0/all/0/1\">Richard Brath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keim_D/0/1/0/all/0/1\">Daniel Keim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knittel_J/0/1/0/all/0/1\">Johannes Knittel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shimei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommerauer_P/0/1/0/all/0/1\">Pia Sommerauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Learning for Cross-Target Stance Detection by Aggregating Multimodal Embeddings. (arXiv:2301.04535v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04535","description":"<p>Despite the increasing popularity of the stance detection task, existing\napproaches are predominantly limited to using the textual content of social\nmedia posts for the classification, overlooking the social nature of the task.\nThe stance detection task becomes particularly challenging in cross-target\nclassification scenarios, where even in few-shot training settings the model\nneeds to predict the stance towards new targets for which the model has only\nseen few relevant samples during training. To address the cross-target stance\ndetection in social media by leveraging the social nature of the task, we\nintroduce CT-TN, a novel model that aggregates multimodal embeddings derived\nfrom both textual and network features of the data. We conduct experiments in a\nfew-shot cross-target scenario on six different combinations of\nsource-destination target pairs. By comparing CT-TN with state-of-the-art\ncross-target stance detection models, we demonstrate the effectiveness of our\nmodel by achieving average performance improvements ranging from 11% to 21%\nacross different baseline models. Experiments with different numbers of shots\nshow that CT-TN can outperform other models after seeing 300 instances of the\ndestination target. Further, ablation experiments demonstrate the positive\ncontribution of each of the components of CT-TN towards the final performance.\nWe further analyse the network interactions between social media users, which\nreveal the potential of using social features for cross-target stance\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khiabani_P/0/1/0/all/0/1\">Parisa Jamadi Khiabani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing. (arXiv:2301.04558v1 [cs.CV])","link":"http://arxiv.org/abs/2301.04558","description":"<p>Self-supervised learning in vision-language processing exploits semantic\nalignment between imaging and text modalities. Prior work in biomedical VLP has\nmostly relied on the alignment of single image and report pairs even though\nclinical notes commonly refer to prior images. This does not only introduce\npoor alignment between the modalities but also a missed opportunity to exploit\nrich self-supervision through existing temporal content in the data. In this\nwork, we explicitly account for prior images and reports when available during\nboth training and fine-tuning. Our approach, named BioViL-T, uses a\nCNN-Transformer hybrid multi-image encoder trained jointly with a text model.\nIt is designed to be versatile to arising challenges such as pose variations\nand missing input images across time. The resulting model excels on downstream\ntasks both in single- and multi-image setups, achieving state-of-the-art\nperformance on (I) progression classification, (II) phrase grounding, and (III)\nreport generation, whilst offering consistent improvements on disease\nclassification and sentence-similarity tasks. We release a novel multi-modal\ntemporal benchmark dataset, MS-CXR-T, to quantify the quality of\nvision-language representations in terms of temporal semantics. Our\nexperimental results show the advantages of incorporating prior images and\nreports to make most use of the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyland_S/0/1/0/all/0/1\">Stephanie Hyland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianchu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1\">Fernando Perez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilse_M/0/1/0/all/0/1\">Maximilian Ilse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1\">Harshita Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouzid_K/0/1/0/all/0/1\">Kenza Bouzid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thieme_A/0/1/0/all/0/1\">Anja Thieme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1\">Maria Wetscherek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving And Analyzing Neural Speaker Embeddings for ASR. (arXiv:2301.04571v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04571","description":"<p>Neural speaker embeddings encode the speaker's speech characteristics through\na DNN model and are prevalent for speaker verification tasks. However, few\nstudies have investigated the usage of neural speaker embeddings for an ASR\nsystem. In this work, we present our efforts w.r.t integrating neural speaker\nembeddings into a conformer based hybrid HMM ASR system. For ASR, our improved\nembedding extraction pipeline in combination with the Weighted-Simple-Add\nintegration method results in x-vector and c-vector reaching on par performance\nwith i-vectors. We further compare and analyze different speaker embeddings. We\npresent our acoustic model improvements obtained by switching from newbob\nlearning rate schedule to one cycle learning schedule resulting in a ~3%\nrelative WER reduction on Switchboard, additionally reducing the overall\ntraining time by 17%. By further adding neural speaker embeddings, we gain\nadditional ~3% relative WER improvement on Hub5'00. Our best Conformer-based\nhybrid ASR system with speaker embeddings achieves 9.0% WER on Hub5'00 and\nHub5'01 with training on SWB 300h.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luscher_C/0/1/0/all/0/1\">Christoph L&#xfc;scher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Personalized Utterance Style (PUS) based Dialogue Strategy for Efficient Service Requirement Elicitation. (arXiv:2301.04582v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04582","description":"<p>With the flourish of services on the Internet, a prerequisite for service\nproviders to precisely deliver services to their customers is to capture user\nrequirements comprehensively, accurately, and efficiently. This is called the\n``Service Requirement Elicitation (SRE)'' task. Considering the amount of\ncustomers is huge, it is an inefficient way for service providers to interact\nwith each user by face-to-face dialog. Therefore, to elicit user requirements\nwith the assistance of virtual intelligent assistants has become a mainstream\nway. Since user requirements generally consist of different levels of details\nand need to be satisfied by services from multiple domains, there is a huge\npotential requirement space for SRE to explore to elicit complete requirements.\nConsidering that traditional dialogue system with static slots cannot be\ndirectly applied to the SRE task, it is a challenge to design an efficient\ndialogue strategy to guide users to express their complete and accurate\nrequirements in such a huge potential requirement space. Based on the\nphenomenon that users tend to express requirements subjectively in a sequential\nmanner, we propose a Personalized Utterance Style (PUS) module to perceive the\npersonalized requirement expression habits, and then apply PUS to an dialogue\nstrategy to efficiently complete the SRE task. Specifically, the dialogue\nstrategy chooses suitable response actions for dynamically updating the\ndialogue state. With the assistance of PUS extracted from dialogue history, the\nsystem can shrink the search scope of potential requirement space. Experiment\nresults show that the dialogue strategy with PUS can elicit more accurate user\nrequirements with fewer dialogue rounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Demin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Min Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongjie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory Augmented Large Language Models are Computationally Universal. (arXiv:2301.04589v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04589","description":"<p>We show that transformer-based large language models are computationally\nuniversal when augmented with an external memory. Any deterministic language\nmodel that conditions on strings of bounded length is equivalent to a finite\nautomaton, hence computationally limited. However, augmenting such models with\na read-write memory creates the possibility of processing arbitrarily large\ninputs and, potentially, simulating any algorithm. We establish that an\nexisting large language model, Flan-U-PaLM 540B, can be combined with an\nassociative read-write memory to exactly simulate the execution of a universal\nTuring machine, $U_{15,2}$. A key aspect of the finding is that it does not\nrequire any modification of the language model weights. Instead, the\nconstruction relies solely on designing a form of stored instruction computer\nthat can subsequently be programmed with a specific set of prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modelling low-resource accents without accent-specific TTS frontend. (arXiv:2301.04606v1 [eess.AS])","link":"http://arxiv.org/abs/2301.04606","description":"<p>This work focuses on modelling a speaker's accent that does not have a\ndedicated text-to-speech (TTS) frontend, including a grapheme-to-phoneme (G2P)\nmodule. Prior work on modelling accents assumes a phonetic transcription is\navailable for the target accent, which might not be the case for low-resource,\nregional accents. In our work, we propose an approach whereby we first augment\nthe target accent data to sound like the donor voice via voice conversion, then\ntrain a multi-speaker multi-accent TTS model on the combination of recordings\nand synthetic data, to generate the donor's voice speaking in the target\naccent. Throughout the procedure, we use a TTS frontend developed for the same\nlanguage but a different accent. We show qualitative and quantitative analysis\nwhere the proposed strategy achieves state-of-the-art results compared to other\ngenerative models. Our work demonstrates that low resource accents can be\nmodelled with relatively little data and without developing an accent-specific\nTTS frontend. Audio samples of our model converting to multiple accents are\navailable on our web page.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tinchev_G/0/1/0/all/0/1\">Georgi Tinchev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Czarnowska_M/0/1/0/all/0/1\">Marta Czarnowska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deja_K/0/1/0/all/0/1\">Kamil Deja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yanagisawa_K/0/1/0/all/0/1\">Kayoko Yanagisawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cotescu_M/0/1/0/all/0/1\">Marius Cotescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"tieval: An Evaluation Framework for Temporal Information Extraction Systems. (arXiv:2301.04643v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04643","description":"<p>Temporal information extraction (TIE) has attracted a great deal of interest\nover the last two decades, leading to the development of a significant number\nof datasets. Despite its benefits, having access to a large volume of corpora\nmakes it difficult when it comes to benchmark TIE systems. On the one hand,\ndifferent datasets have different annotation schemes, thus hindering the\ncomparison between competitors across different corpora. On the other hand, the\nfact that each corpus is commonly disseminated in a different format requires a\nconsiderable engineering effort for a researcher/practitioner to develop\nparsers for all of them. This constraint forces researchers to select a limited\namount of datasets to evaluate their systems which consequently limits the\ncomparability of the systems. Yet another obstacle that hinders the\ncomparability of the TIE systems is the evaluation metric employed. While most\nresearch works adopt traditional metrics such as precision, recall, and $F_1$,\na few others prefer temporal awareness -- a metric tailored to be more\ncomprehensive on the evaluation of temporal systems. Although the reason for\nthe absence of temporal awareness in the evaluation of most systems is not\nclear, one of the factors that certainly weights this decision is the necessity\nto implement the temporal closure algorithm in order to compute temporal\nawareness, which is not straightforward to implement neither is currently\neasily available. All in all, these problems have limited the fair comparison\nbetween approaches and consequently, the development of temporal extraction\nsystems. To mitigate these problems, we have developed tieval, a Python library\nthat provides a concise interface for importing different corpora and\nfacilitates system evaluation. In this paper, we present the first public\nrelease of tieval and highlight its most relevant features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sousa_H/0/1/0/all/0/1\">Hugo Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorge_A/0/1/0/all/0/1\">Al&#xed;pio Jorge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_R/0/1/0/all/0/1\">Ricardo Campos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata. (arXiv:2301.04647v1 [cs.CV])","link":"http://arxiv.org/abs/2301.04647","description":"<p>We learn a visual representation that captures information about the camera\nthat recorded a given photo. To do this, we train a multimodal embedding\nbetween image patches and the EXIF metadata that cameras automatically insert\ninto image files. Our model represents this metadata by simply converting it to\ntext and then processing it with a transformer. The features that we learn\nsignificantly outperform other self-supervised and supervised features on\ndownstream image forensics and calibration tasks. In particular, we\nsuccessfully localize spliced image regions \"zero shot\" by clustering the\nvisual embeddings for all of the patches within an image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chenhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owens_A/0/1/0/all/0/1\">Andrew Owens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAS: Self-Augmentation Strategy for Language Model Pre-training. (arXiv:2106.07176v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07176","description":"<p>The core of self-supervised learning for pre-training language models\nincludes pre-training task design as well as appropriate data augmentation.\nMost data augmentations in language model pre-training are context-independent.\nA seminal contextualized augmentation was recently proposed in ELECTRA and\nachieved state-of-the-art performance by introducing an auxiliary generation\nnetwork (generator) to produce contextualized data augmentation for the\ntraining of a main discrimination network (discriminator). This design,\nhowever, introduces extra computation cost of the generator and a need to\nadjust the relative capability between the generator and the discriminator. In\nthis paper, we propose a self-augmentation strategy (SAS) where a single\nnetwork is utilized for both regular pre-training and contextualized data\naugmentation for the training in later epochs. Essentially, this strategy\neliminates a separate generator and uses the single network to jointly conduct\ntwo pre-training tasks with MLM (Masked Language Modeling) and RTD (Replaced\nToken Detection) heads. It avoids the challenge to search for an appropriate\nsize of the generator, which is critical to the performance as evidenced in\nELECTRA and its subsequent variant models. In addition, SAS is a general\nstrategy that can be seamlessly combined with many new techniques emerging\nrecently or in the future, such as the disentangled attention mechanism from\nDeBERTa. Our experiments show that SAS is able to outperform ELECTRA and other\nstate-of-the-art models in the GLUE tasks with similar or less computation\ncost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingqiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ru He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1\">Liangzhu Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeaSuL: Weakly Supervised Dialogue Policy Learning: Reward Estimation for Multi-turn Dialogue. (arXiv:2108.01487v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.01487","description":"<p>An intelligent dialogue system in a multi-turn setting should not only\ngenerate the responses which are of good quality, but it should also generate\nthe responses which can lead to long-term success of the dialogue. Although,\nthe current approaches improved the response quality, but they over-look the\ntraining signals present in the dialogue data. We can leverage these signals to\ngenerate the weakly supervised training data for learning dialog policy and\nreward estimator, and make the policy take actions (generates responses) which\ncan foresee the future direction for a successful (rewarding) conversation. We\nsimulate the dialogue between an agent and a user (modelled similar to an agent\nwith supervised learning objective) to interact with each other. The agent uses\ndynamic blocking to generate ranked diverse responses and\nexploration-exploitation to select among the Top-K responses. Each simulated\nstate-action pair is evaluated (works as a weak annotation) with three quality\nmodules: Semantic Relevant, Semantic Coherence and Consistent Flow. Empirical\nstudies with two benchmarks indicate that our model can significantly\nout-perform the response quality and lead to a successful conversation on both\nautomatic evaluation and human judgement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Anant Khandelwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Enhancing Multi-filter Sequence-to-Sequence Model. (arXiv:2109.12399v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12399","description":"<p>Representation learning is important for solving sequence-to-sequence\nproblems in natural language processing. Representation learning transforms raw\ndata into vector-form representations while preserving their features. However,\ndata with significantly different features leads to heterogeneity in their\nrepresentations, which may increase the difficulty of convergence. We design a\nmulti-filter encoder-decoder model to resolve the heterogeneity problem in\nsequence-to-sequence tasks. The multi-filter model divides the latent space\ninto subspaces using a clustering algorithm and trains a set of decoders\n(filters) in which each decoder only concentrates on the features from its\ncorresponding subspace. As for the main contribution, we design a\nself-enhancing mechanism that uses a reinforcement learning algorithm to\noptimize the clustering algorithm without additional training data. We run\nsemantic parsing and machine translation experiments to indicate that the\nproposed model can outperform most benchmarks by at least 5\\%. We also\nempirically show the self-enhancing mechanism can improve performance by over\n10\\% and provide evidence to demonstrate the positive correlation between the\nmodel's performance and the latent space clustering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhaokun Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whinston_A/0/1/0/all/0/1\">Andrew Whinston</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Data Augmentation for Arabic-English Code-Switching Speech Recognition. (arXiv:2201.02550v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.02550","description":"<p>The pervasiveness of intra-utterance code-switching (CS) in spoken content\nrequires that speech recognition (ASR) systems handle mixed language. Designing\na CS-ASR system has many challenges, mainly due to data scarcity, grammatical\nstructure complexity, and domain mismatch. The most common method for\naddressing CS is to train an ASR system with the available transcribed CS\nspeech, along with monolingual data. In this work, we propose a zero-shot\nlearning methodology for CS-ASR by augmenting the monolingual data with\nartificially generating CS text. We based our approach on random lexical\nreplacements and Equivalence Constraint (EC) while exploiting aligned\ntranslation pairs to generate random and grammatically valid CS content. Our\nempirical results show a 65.5% relative reduction in language model perplexity,\nand 7.7% in ASR WER on two ecologically valid CS test sets. The human\nevaluation of the generated text using EC suggests that more than 80% is of\nadequate quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehak_N/0/1/0/all/0/1\">Najim Dehak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khudanpur_S/0/1/0/all/0/1\">Sanjeev Khudanpur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. (arXiv:2201.11903v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11903","description":"<p>We explore how generating a chain of thought -- a series of intermediate\nreasoning steps -- significantly improves the ability of large language models\nto perform complex reasoning. In particular, we show how such reasoning\nabilities emerge naturally in sufficiently large language models via a simple\nmethod called chain of thought prompting, where a few chain of thought\ndemonstrations are provided as exemplars in prompting. Experiments on three\nlarge language models show that chain of thought prompting improves performance\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\nempirical gains can be striking. For instance, prompting a 540B-parameter\nlanguage model with just eight chain of thought exemplars achieves state of the\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\nfinetuned GPT-3 with a verifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemSup: Semantic Supervision for Simple and Scalable Zero-shot Generalization. (arXiv:2202.13100v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.13100","description":"<p>Zero-shot learning is the problem of predicting instances over classes not\nseen during training. One approach to zero-shot learning is providing auxiliary\nclass information to the model. Prior works along this vein have largely used\nexpensive per-instance annotation or singular class-level descriptions, but\nper-instance descriptions are hard to scale and single class descriptions may\nnot be rich enough. Furthermore, these works have used natural-language\ndescriptions exclusively, simple biencoders models, and modality or task\nspecific methods. These approaches have several limitations: text supervision\nmay not always be available or optimal and biencoders may only learn coarse\nrelations between inputs and class descriptions. In this work, we present\nSemSup, a novel approach that uses (1) a scalable multiple description sampling\nmethod which improves performance over single descriptions, (2) alternative\ndescription formats such as JSON that are easy to generate and outperform text\non certain settings, and (3) hybrid lexical-semantic similarity to leverage\nfine-grained information in class descriptions. We demonstrate the\neffectiveness of SemSup across four datasets, two modalities, and three\ngeneralization settings. For example, across text and image datasets, SemSup\nincreases unseen class generalization accuracy by 15 points on average compared\nto the closest baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanjie_A/0/1/0/all/0/1\">Austin W. Hanjie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Troll Tweet Detection Using Contextualized Word Representations. (arXiv:2207.08230v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.08230","description":"<p>In this study, we aimed to address the growing concern of trolling behavior\non social media by developing and evaluating a set of model architectures for\nthe automatic detection of troll tweets. Utilizing deep learning techniques and\npre-trained word embedding methods such as BERT, ELMo, and GloVe, we evaluated\nthe performance of each architecture using metrics such as classification\naccuracy, F1 score, AUC, and precision. Our results indicate that BERT and ELMo\nembedding methods performed better than the GloVe method, likely due to their\nability to provide contextualized word embeddings that better capture the\nnuances and subtleties of language use in online social media. Additionally, we\nfound that CNN and GRU encoders performed similarly in terms of F1 score and\nAUC, suggesting their effectiveness in extracting relevant information from\ninput text. The best-performing method was found to be an ELMo-based\narchitecture that employed a GRU classifier, with an AUC score of 0.929. This\nresearch highlights the importance of utilizing contextualized word embeddings\nand appropriate encoder methods in the task of troll tweet detection, which can\nassist social-based systems in improving their performance in identifying and\naddressing trolling behavior on their platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_S/0/1/0/all/0/1\">Seyhmus Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zavrak_S/0/1/0/all/0/1\">Sultan Zavrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Text Classification Data and Models Using Aggregated Input Salience. (arXiv:2211.05485v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05485","description":"<p>Realizing when a model is right for a wrong reason is not trivial and\nrequires a significant effort by model developers. In some cases an input\nsalience method, which highlights the most important parts of the input, may\nreveal problematic reasoning. But scrutinizing highlights over many data\ninstances is tedious and often infeasible. Furthermore, analyzing examples in\nisolation does not reveal general patterns in the data or in the model's\nbehavior. In this paper we aim to address these issues and go from\nunderstanding single examples to understanding entire datasets and models. The\nmethodology we propose is based on aggregated salience maps, to which we apply\nclustering, nearest neighbor search and visualizations. Using this methodology\nwe address multiple distinct but common model developer needs by showing how\nproblematic data and model behavior can be identified and explained -- a\nnecessary first step for improving the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ebert_S/0/1/0/all/0/1\">Sebastian Ebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakobovits_A/0/1/0/all/0/1\">Alice Shoshana Jakobovits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filippova_K/0/1/0/all/0/1\">Katja Filippova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Seed-Guided Topic Discovery by Integrating Multiple Types of Contexts. (arXiv:2212.06002v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.06002","description":"<p>Instead of mining coherent topics from a given text corpus in a completely\nunsupervised manner, seed-guided topic discovery methods leverage user-provided\nseed words to extract distinctive and coherent topics so that the mined topics\ncan better cater to the user's interest. To model the semantic correlation\nbetween words and seeds for discovering topic-indicative terms, existing\nseed-guided approaches utilize different types of context signals, such as\ndocument-level word co-occurrences, sliding window-based local contexts, and\ngeneric linguistic knowledge brought by pre-trained language models. In this\nwork, we analyze and show empirically that each type of context information has\nits value and limitation in modeling word semantics under seed guidance, but\ncombining three types of contexts (i.e., word embeddings learned from local\ncontexts, pre-trained language model representations obtained from\ngeneral-domain training, and topic-indicative sentences retrieved based on seed\ninformation) allows them to complement each other for discovering quality\ntopics. We propose an iterative framework, SeedTopicMine, which jointly learns\nfrom the three types of contexts and gradually fuses their context signals via\nan ensemble ranking process. Under various sets of seeds and on multiple\ndatasets, SeedTopicMine consistently yields more coherent and accurate topics\nthan existing seed-guided topic discovery approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalski_M/0/1/0/all/0/1\">Martin Michalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yucheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NEEDED: Introducing Hierarchical Transformer to Eye Diseases Diagnosis. (arXiv:2212.13408v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.13408","description":"<p>With the development of natural language processing techniques(NLP),\nautomatic diagnosis of eye diseases using ophthalmology electronic medical\nrecords (OEMR) has become possible. It aims to evaluate the condition of both\neyes of a patient respectively, and we formulate it as a particular multi-label\nclassification task in this paper. Although there are a few related studies in\nother diseases, automatic diagnosis of eye diseases exhibits unique\ncharacteristics. First, descriptions of both eyes are mixed up in OEMR\ndocuments, with both free text and templated asymptomatic descriptions,\nresulting in sparsity and clutter of information. Second, OEMR documents\ncontain multiple parts of descriptions and have long document lengths. Third,\nit is critical to provide explainability to the disease diagnosis model. To\novercome those challenges, we present an effective automatic eye disease\ndiagnosis framework, NEEDED. In this framework, a preprocessing module is\nintegrated to improve the density and quality of information. Then, we design a\nhierarchical transformer structure for learning the contextualized\nrepresentations of each sentence in the OEMR document. For the diagnosis part,\nwe propose an attention-based predictor that enables traceable diagnosis by\nobtaining disease-specific information. Experiments on the real dataset and\ncomparison with several baseline models show the advantage and explainability\nof our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Z/0/1/0/all/0/1\">Zhiyuan Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Weiwei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wenjuan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanchun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comprehensive review of automatic text summarization techniques: method, data, evaluation and coding. (arXiv:2301.03403v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.03403","description":"<p>We provide a literature review about Automatic Text Summarization (ATS)\nsystems. We consider a citation-based approach. We start with some popular and\nwell-known papers that we have in hand about each topic we want to cover and we\nhave tracked the \"backward citations\" (papers that are cited by the set of\npapers we knew beforehand) and the \"forward citations\" (newer papers that cite\nthe set of papers we knew beforehand). In order to organize the different\nmethods, we present the diverse approaches to ATS guided by the mechanisms they\nuse to generate a summary. Besides presenting the methods, we also present an\nextensive review of the datasets available for summarization tasks and the\nmethods used to evaluate the quality of the summaries. Finally, we present an\nempirical exploration of these methods using the CNN Corpus dataset that\nprovides golden summaries for extractive and abstractive methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cajueiro_D/0/1/0/all/0/1\">Daniel O. Cajueiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nery_A/0/1/0/all/0/1\">Arthur G. Nery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavares_I/0/1/0/all/0/1\">Igor Tavares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_M/0/1/0/all/0/1\">Ma&#xed;sa K. De Melo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reis_S/0/1/0/all/0/1\">Silvia A. dos Reis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weigang_L/0/1/0/all/0/1\">Li Weigang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celestino_V/0/1/0/all/0/1\">Victor R. R. Celestino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Channel-aware Decoupling Network for Multi-turn Dialogue Comprehension. (arXiv:2301.03953v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.03953","description":"<p>Training machines to understand natural language and interact with humans is\none of the major goals of artificial intelligence. Recent years have witnessed\nan evolution from matching networks to pre-trained language models (PrLMs). In\ncontrast to the plain-text modeling as the focus of the PrLMs, dialogue texts\ninvolve multiple speakers and reflect special characteristics such as topic\ntransitions and structure dependencies between distant utterances. However, the\nrelated PrLM models commonly represent dialogues sequentially by processing the\npairwise dialogue history as a whole. Thus the hierarchical information on\neither utterance interrelation or speaker roles coupled in such representations\nis not well addressed. In this work, we propose compositional learning for\nholistic interaction across the utterances beyond the sequential\ncontextualization from PrLMs, in order to capture the utterance-aware and\nspeaker-aware representations entailed in a dialogue history. We decouple the\ncontextualized word representations by masking mechanisms in Transformer-based\nPrLM, making each word only focus on the words in current utterance, other\nutterances, and two speaker roles (i.e., utterances of sender and utterances of\nthe receiver), respectively. In addition, we employ domain-adaptive training\nstrategies to help the model adapt to the dialogue domains. Experimental\nresults show that our method substantially boosts the strong PrLM baselines in\nfour public benchmark datasets, achieving new state-of-the-art performance over\nprevious methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Longxiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}