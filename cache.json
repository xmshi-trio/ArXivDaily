{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Lay Text Summarisation Using Natural Language Processing: A Narrative Literature Review. (arXiv:2303.14222v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14222","description":"<p>Summarisation of research results in plain language is crucial for promoting\npublic understanding of research findings. The use of Natural Language\nProcessing to generate lay summaries has the potential to relieve researchers'\nworkload and bridge the gap between science and society. The aim of this\nnarrative literature review is to describe and compare the different text\nsummarisation approaches used to generate lay summaries. We searched the\ndatabases Web of Science, Google Scholar, IEEE Xplore, Association for\nComputing Machinery Digital Library and arXiv for articles published until 6\nMay 2022. We included original studies on automatic text summarisation methods\nto generate lay summaries. We screened 82 articles and included eight relevant\npapers published between 2020 and 2021, all using the same dataset. The results\nshow that transformer-based methods such as Bidirectional Encoder\nRepresentations from Transformers (BERT) and Pre-training with Extracted\nGap-sentences for Abstractive Summarization (PEGASUS) dominate the landscape of\nlay text summarisation, with all but one study using these methods. A\ncombination of extractive and abstractive summarisation methods in a hybrid\napproach was found to be most effective. Furthermore, pre-processing approaches\nto input text (e.g. applying extractive summarisation) or determining which\nsections of a text to include, appear critical. Evaluation metrics such as\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) were used, which do\nnot consider readability. To conclude, automatic lay text summarisation is\nunder-explored. Future research should consider long document lay text\nsummarisation, including clinical trial reports, and the development of\nevaluation metrics that consider readability of the lay summary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vinzelberg_O/0/1/0/all/0/1\">Oliver Vinzelberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_M/0/1/0/all/0/1\">Mark David Jenkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morison_G/0/1/0/all/0/1\">Gordon Morison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMinn_D/0/1/0/all/0/1\">David McMinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tieges_Z/0/1/0/all/0/1\">Zoe Tieges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIGMORPHON 2023 Shared Task of Interlinear Glossing: Baseline Model. (arXiv:2303.14234v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14234","description":"<p>Language documentation is a critical aspect of language preservation, often\nincluding the creation of Interlinear Glossed Text (IGT). Creating IGT is\ntime-consuming and tedious, and automating the process can save valuable\nannotator effort.\n</p>\n<p>This paper describes the baseline system for the SIGMORPHON 2023 Shared Task\nof Interlinear Glossing. In our system, we utilize a transformer architecture\nand treat gloss generation as a sequence labelling task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ginn_M/0/1/0/all/0/1\">Michael Ginn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depression detection in social media posts using affective and social norm features. (arXiv:2303.14279v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14279","description":"<p>We propose a deep architecture for depression detection from social media\nposts. The proposed architecture builds upon BERT to extract language\nrepresentations from social media posts and combines these representations\nusing an attentive bidirectional GRU network. We incorporate affective\ninformation, by augmenting the text representations with features extracted\nfrom a pretrained emotion classifier. Motivated by psychological literature we\npropose to incorporate profanity and morality features of posts and words in\nour architecture using a late fusion scheme. Our analysis indicates that\nmorality and profanity can be important features for depression detection. We\napply our model for depression detection on Reddit posts on the Pirina dataset,\nand further consider the setting of detecting depressed users, given multiple\nposts per user, proposed in the Reddit RSDD dataset. The inclusion of the\nproposed features yields state-of-the-art results in both settings, namely\n2.65% and 6.73% absolute improvement in F1 score respectively. Index Terms:\nDepression detection, BERT, Feature fusion, Emotion recognition, profanity,\nmorality\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Triantafyllopoulos_I/0/1/0/all/0/1\">Ilias Triantafyllopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paraskevopoulos_G/0/1/0/all/0/1\">Georgios Paraskevopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potamianos_A/0/1/0/all/0/1\">Alexandros Potamianos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voice-Based Conversational Agents and Knowledge Graphs for Improving News Search in Assisted Living. (arXiv:2303.14286v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14286","description":"<p>As the healthcare sector is facing major challenges, such as aging\npopulations, staff shortages, and common chronic diseases, delivering\nhigh-quality care to individuals has become very difficult. Conversational\nagents have shown to be a promising technology to alleviate some of these\nissues. In the form of digital health assistants, they have the potential to\nimprove the everyday life of the elderly and chronically ill people. This\nincludes, for example, medication reminders, routine checks, or social\nchit-chat. In addition, conversational agents can satisfy the fundamental need\nof having access to information about daily news or local events, which enables\nindividuals to stay informed and connected with the world around them. However,\nfinding relevant news sources and navigating the plethora of news articles\navailable online can be overwhelming, particularly for those who may have\nlimited technological literacy or health-related impairments. To address this\nchallenge, we propose an innovative solution that combines knowledge graphs and\nconversational agents for news search in assisted living. By leveraging graph\ndatabases to semantically structure news data and implementing an intuitive\nvoice-based interface, our system can help care-dependent people to easily\ndiscover relevant news articles and give personalized recommendations. We\nexplain our design choices, provide a system architecture, share insights of an\ninitial user test, and give an outlook on planned future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Phillip Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehtanz_N/0/1/0/all/0/1\">Nils Rehtanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jokinen_K/0/1/0/all/0/1\">Kristiina Jokinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT is becoming a Turing machine: Here are some ways to program it. (arXiv:2303.14310v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14310","description":"<p>We demonstrate that, through appropriate prompting, GPT-3 family of models\ncan be triggered to perform iterative behaviours necessary to execute (rather\nthan just write or recall) programs that involve loops, including several\npopular algorithms found in computer science curricula or software developer\ninterviews. We trigger execution and description of Iterations by Regimenting\nSelf-Attention (IRSA) in one (or a combination) of three ways: 1) Using strong\nrepetitive structure in an example of an execution path of a target program for\none particular input, 2) Prompting with fragments of execution paths, and 3)\nExplicitly forbidding (skipping) self-attention to parts of the generated text.\nOn a dynamic program execution, IRSA leads to larger accuracy gains than\nreplacing the model with the much more powerful GPT-4. IRSA has promising\napplications in education, as the prompts and responses resemble student\nassignments in data structures and algorithms classes. Our findings hold\nimplications for evaluating LLMs, which typically target the in-context\nlearning: We show that prompts that may not even cover one full task example\ncan trigger algorithmic behaviour, allowing solving problems previously thought\nof as hard for LLMs, such as logical puzzles. Consequently, prompt design plays\nan even more critical role in LLM performance than previously recognized.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jojic_A/0/1/0/all/0/1\">Ana Jojic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1\">Nebojsa Jojic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Attacks with Input-unique Triggers in NLP. (arXiv:2303.14325v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14325","description":"<p>Backdoor attack aims at inducing neural models to make incorrect predictions\nfor poison data while keeping predictions on the clean dataset unchanged, which\ncreates a considerable threat to current natural language processing (NLP)\nsystems. Existing backdoor attacking systems face two severe issues:firstly,\nmost backdoor triggers follow a uniform and usually input-independent pattern,\ne.g., insertion of specific trigger words, synonym replacement. This\nsignificantly hinders the stealthiness of the attacking model, leading the\ntrained backdoor model being easily identified as malicious by model probes.\nSecondly, trigger-inserted poisoned sentences are usually disfluent,\nungrammatical, or even change the semantic meaning from the original sentence,\nmaking them being easily filtered in the pre-processing stage. To resolve these\ntwo issues, in this paper, we propose an input-unique backdoor attack(NURA),\nwhere we generate backdoor triggers unique to inputs. IDBA generates\ncontext-related triggers by continuing writing the input with a language model\nlike GPT2. The generated sentence is used as the backdoor trigger. This\nstrategy not only creates input-unique backdoor triggers, but also preserves\nthe semantics of the original input, simultaneously resolving the two issues\nabove. Experimental results show that the IDBA attack is effective for attack\nand difficult to defend: it achieves high attack success rate across all the\nwidely applied benchmarks, while is immune to existing defending methods. In\naddition, it is able to generate fluent, grammatical, and diverse backdoor\ninputs, which can hardly be recognized through human inspection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xukun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Semantic Reader Project: Augmenting Scholarly Documents through AI-Powered Interactive Reading Interfaces. (arXiv:2303.14334v1 [cs.DL])","link":"http://arxiv.org/abs/2303.14334","description":"<p>Scholarly publications are key to the transfer of knowledge from scholars to\nothers. However, research papers are information-dense, and as the volume of\nthe scientific literature grows, the need for new technology to support the\nreading process grows. In contrast to the process of finding papers, which has\nbeen transformed by Internet technology, the experience of reading research\npapers has changed little in decades. The PDF format for sharing research\npapers is widely used due to its portability, but it has significant downsides\nincluding: static content, poor accessibility for low-vision readers, and\ndifficulty reading on mobile devices. This paper explores the question \"Can\nrecent advances in AI and HCI power intelligent, interactive, and accessible\nreading interfaces -- even for legacy PDFs?\" We describe the Semantic Reader\nProject, a collaborative effort across multiple institutions to explore\nautomatic creation of dynamic reading interfaces for research papers. Through\nthis project, we've developed ten research prototype interfaces and conducted\nusability studies with more than 300 participants and real-world users showing\nimproved reading experiences for scholars. We've also released a production\nreading interface for research papers that will incorporate the best features\nas they mature. We structure this paper around challenges scholars and the\npublic face when reading research papers -- Discovery, Efficiency,\nComprehension, Synthesis, and Accessibility -- and present an overview of our\nprogress and remaining open challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Joseph Chee Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Head_A/0/1/0/all/0/1\">Andrew Head</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1\">Jonathan Bragg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Amy X. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trier_C/0/1/0/all/0/1\">Cassidy Trier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasiades_C/0/1/0/all/0/1\">Chloe Anastasiades</a>, <a href=\"http://arxiv.org/find/cs/1/au:+August_T/0/1/0/all/0/1\">Tal August</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Authur_R/0/1/0/all/0/1\">Russell Authur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bragg_D/0/1/0/all/0/1\">Danielle Bragg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bransom_E/0/1/0/all/0/1\">Erin Bransom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cachola_I/0/1/0/all/0/1\">Isabel Cachola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Candra_S/0/1/0/all/0/1\">Stefan Candra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekhar_Y/0/1/0/all/0/1\">Yoganand Chandrasekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Sung Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_E/0/1/0/all/0/1\">Evie Yu-Yen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1\">Yvonne Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_R/0/1/0/all/0/1\">Rob Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fok_R/0/1/0/all/0/1\">Raymond Fok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fangzhou Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huff_R/0/1/0/all/0/1\">Regan Huff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tae Soo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinney_R/0/1/0/all/0/1\">Rodney Kinney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittur_A/0/1/0/all/0/1\">Aniket Kittur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hyeonsu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klevak_E/0/1/0/all/0/1\">Egor Klevak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langan_M/0/1/0/all/0/1\">Michael Langan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latzke_M/0/1/0/all/0/1\">Matt Latzke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lochner_J/0/1/0/all/0/1\">Jaron Lochner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacMillan_K/0/1/0/all/0/1\">Kelsey MacMillan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marsh_E/0/1/0/all/0/1\">Eric Marsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_T/0/1/0/all/0/1\">Tyler Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Aakanksha Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc-Uyen Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palani_S/0/1/0/all/0/1\">Srishti Palani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Soya Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulic_C/0/1/0/all/0/1\">Caroline Paulic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rachatasumrit_N/0/1/0/all/0/1\">Napol Rachatasumrit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1\">Smita Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayre_P/0/1/0/all/0/1\">Paul Sayre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zejiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siangliulue_P/0/1/0/all/0/1\">Pao Siangliulue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Huy Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuylen_M/0/1/0/all/0/1\">Madeleine van Zuylen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lucy Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilhelm_C/0/1/0/all/0/1\">Christopher Wilhelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Caroline Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiangjiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamarron_A/0/1/0/all/0/1\">Angele Zamarron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearst_M/0/1/0/all/0/1\">Marti A. Hearst</a>, et al. (1 additional author not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SmartBook: AI-Assisted Situation Report Generation. (arXiv:2303.14337v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14337","description":"<p>Emerging events, such as the COVID pandemic and the Ukraine Crisis, require a\ntime-sensitive comprehensive understanding of the situation to allow for\nappropriate decision-making and effective action response. Automated generation\nof situation reports can significantly reduce the time, effort, and cost for\ndomain experts when preparing their official human-curated reports. However, AI\nresearch toward this goal has been very limited, and no successful trials have\nyet been conducted to automate such report generation. We propose SmartBook, a\nnovel task formulation targeting situation report generation, which consumes\nlarge volumes of news data to produce a structured situation report with\nmultiple hypotheses (claims) summarized and grounded with rich links to factual\nevidence. We realize SmartBook for the Ukraine-Russia crisis by automatically\ngenerating intelligence analysis reports to assist expert analysts. The\nmachine-generated reports are structured in the form of timelines, with each\ntimeline organized by major events (or chapters), corresponding strategic\nquestions (or sections) and their grounded summaries (or section content). Our\nproposed framework automatically detects real-time event-related strategic\nquestions, which are more directed than manually-crafted analyst questions,\nwhich tend to be too complex, hard to parse, vague and high-level. Results from\nthorough qualitative evaluations show that roughly 82% of the questions in\nSmartbook have strategic importance, with at least 93% of the sections in the\nreport being tactically useful. Further, experiments show that expert analysts\ntend to add more information into the SmartBook reports, with only 2.3% of the\nexisting tokens being deleted, meaning SmartBook can serve as a useful\nfoundation for analysts to build upon when creating intelligence reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Revanth Gangi Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_Y/0/1/0/all/0/1\">Yi R. Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_P/0/1/0/all/0/1\">Paul Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+J_H/0/1/0/all/0/1\">Heng J</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis of GPT-3's Performance in Grammatical Error Correction. (arXiv:2303.14342v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14342","description":"<p>GPT-3 models are very powerful, achieving high performance on a variety of\nnatural language processing tasks. However, there is a relative lack of\ndetailed published analysis on how well they perform on the task of grammatical\nerror correction (GEC). To address this, we perform experiments testing the\ncapabilities of a GPT-3 model (text-davinci-003) against major GEC benchmarks,\ncomparing the performance of several different prompts, including a comparison\nof zero-shot and few-shot settings. We analyze intriguing or problematic\noutputs encountered with different prompt formats. We report the performance of\nour best prompt on the BEA-2019 and JFLEG datasets using a combination of\nautomatic metrics and human evaluations, revealing interesting differences\nbetween the preferences of human raters and the reference-based automatic\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coyne_S/0/1/0/all/0/1\">Steven Coyne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-augmented Frame Semantic Parsing with Hybrid Prompt-tuning. (arXiv:2303.14375v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14375","description":"<p>Frame semantics-based approaches have been widely used in semantic parsing\ntasks and have become mainstream. It remains challenging to disambiguate frame\nrepresentations evoked by target lexical units under different contexts.\nPre-trained Language Models (PLMs) have been used in semantic parsing and\nsignificantly improve the accuracy of neural parsers. However, the PLMs-based\napproaches tend to favor collocated patterns presented in the training data,\nleading to inaccurate outcomes. The intuition here is to design a mechanism to\noptimally use knowledge captured in semantic frames in conjunction with PLMs to\ndisambiguate frames. We propose a novel Knowledge-Augmented Frame Semantic\nParsing Architecture (KAF-SPA) to enhance semantic representation by\nincorporating accurate frame knowledge into PLMs during frame semantic parsing.\nSpecifically, a Memory-based Knowledge Extraction Module (MKEM) is devised to\nselect accurate frame knowledge and construct the continuous templates in the\nhigh dimensional vector space. Moreover, we design a Task-oriented Knowledge\nProbing Module (TKPM) using hybrid prompts (in terms of continuous and discrete\nprompts) to incorporate the selected knowledge into the PLMs and adapt PLMs to\nthe tasks of frame and argument identification. Experimental results on two\npublic FrameNet datasets demonstrate that our method significantly outperforms\nstrong baselines (by more than +3$\\%$ in F1), achieving state-of-art results on\nthe current benchmark. Ablation studies verify the effectiveness of KAF-SPA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yajing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing in Ethiopian Languages: Current State, Challenges, and Opportunities. (arXiv:2303.14406v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14406","description":"<p>This survey delves into the current state of natural language processing\n(NLP) for four Ethiopian languages: Amharic, Afaan Oromo, Tigrinya, and\nWolaytta. Through this paper, we identify key challenges and opportunities for\nNLP research in Ethiopia. Furthermore, we provide a centralized repository on\nGitHub that contains publicly available resources for various NLP tasks in\nthese languages. This repository can be updated periodically with contributions\nfrom other researchers. Our objective is to identify research gaps and\ndisseminate the information to NLP researchers interested in Ethiopian\nlanguages and encourage future research in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tonja_A/0/1/0/all/0/1\">Atnafu Lambebo Tonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belay_T/0/1/0/all/0/1\">Tadesse Destaw Belay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azime_I/0/1/0/all/0/1\">Israel Abebe Azime</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayele_A/0/1/0/all/0/1\">Abinew Ali Ayele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehamed_M/0/1/0/all/0/1\">Moges Ahmed Mehamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikova_O/0/1/0/all/0/1\">Olga Kolesnikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yimam_S/0/1/0/all/0/1\">Seid Muhie Yimam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sem4SAP: Synonymous Expression Mining From Open Knowledge Graph For Language Model Synonym-Aware Pretraining. (arXiv:2303.14425v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14425","description":"<p>The model's ability to understand synonymous expression is crucial in many\nkinds of downstream tasks. It will make the model to better understand the\nsimilarity between context, and more robust to the synonym substitution attack.\nHowever, many Pretrained Language Model (PLM) lack synonym knowledge due to\nlimitation of small-scale synsets and PLM's pretraining objectives. In this\npaper, we propose a framework called Sem4SAP to mine synsets from Open\nKnowledge Graph (Open-KG) and using the mined synsets to do synonym-aware\npretraining for language models. We propose to coarsly filter the content in\nOpen-KG and use the frequency information to better help the clustering process\nunder low-resource unsupervised conditions. We expand the mined synsets by\nmigrating core semantics between synonymous expressions.We also propose two\nnovel and effective synonym-aware pre-training methods for injecting synonym\nknowledge into PLMs.Extensive experiments demonstrate that Sem4SAP can\ndramatically outperform the original PLMs and other baselines on ten different\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhouhong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Sihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Hongwei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COFFEE: A Contrastive Oracle-Free Framework for Event Extraction. (arXiv:2303.14452v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14452","description":"<p>Event extraction is a complex information extraction task that involves\nextracting events from unstructured text. Prior classification-based methods\nrequire comprehensive entity annotations for joint training, while newer\ngeneration-based methods rely on heuristic templates containing oracle\ninformation such as event type, which is often unavailable in real-world\nscenarios. In this study, we consider a more realistic setting of this task,\nnamely the Oracle-Free Event Extraction (OFEE) task, where only the input\ncontext is given without any oracle information, including event type, event\nontology and trigger word. To solve this task, we propose a new framework,\ncalled COFFEE, which extracts the events solely based on the document context\nwithout referring to any oracle information. In particular, a contrastive\nselection model is introduced in COFFEE to rectify the generated triggers and\nhandle multi-event instances. The proposed COFFEE outperforms state-of-the-art\napproaches under the oracle-free setting of the event extraction task, as\nevaluated on a public event extraction benchmark ACE05.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meiru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zihao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indian Language Summarization using Pretrained Sequence-to-Sequence Models. (arXiv:2303.14461v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14461","description":"<p>The ILSUM shared task focuses on text summarization for two major Indian\nlanguages- Hindi and Gujarati, along with English. In this task, we experiment\nwith various pretrained sequence-to-sequence models to find out the best model\nfor each of the languages. We present a detailed overview of the models and our\napproaches in this paper. We secure the first rank across all three sub-tasks\n(English, Hindi and Gujarati). This paper also extensively analyzes the impact\nof k-fold cross-validation while experimenting with limited data size, and we\nalso perform various experiments with a combination of the original and a\nfiltered version of the data to determine the efficacy of the pretrained\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Urlana_A/0/1/0/all/0/1\">Ashok Urlana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1\">Sahil Manoj Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surange_N/0/1/0/all/0/1\">Nirmal Surange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Informed Machine Learning, Centrality, CNN, Relevant Document Detection, Repatriation of Indigenous Human Remains. (arXiv:2303.14475v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14475","description":"<p>Among the pressing issues facing Australian and other First Nations peoples\nis the repatriation of the bodily remains of their ancestors, which are\ncurrently held in Western scientific institutions. The success of securing the\nreturn of these remains to their communities for reburial depends largely on\nlocating information within scientific and other literature published between\n1790 and 1970 documenting their theft, donation, sale, or exchange between\ninstitutions. This article reports on collaborative research by data scientists\nand social science researchers in the Research, Reconcile, Renew Network (RRR)\nto develop and apply text mining techniques to identify this vital information.\nWe describe our work to date on developing a machine learning-based solution to\nautomate the process of finding and semantically analysing relevant texts.\nClassification models, particularly deep learning-based models, are known to\nhave low accuracy when trained with small amounts of labelled (i.e.\nrelevant/non-relevant) documents. To improve the accuracy of our detection\nmodel, we explore the use of an Informed Neural Network (INN) model that\ndescribes documentary content using expert-informed contextual knowledge. Only\na few labelled documents are used to provide specificity to the model, using\nconceptually related keywords identified by RRR experts in provenance research.\nThe results confirm the value of using an INN network model for identifying\nrelevant documents related to the investigation of the global commercial trade\nin Indigenous human remains. Empirical analysis suggests that this INN model\ncan be generalized for use by other researchers in the social sciences and\nhumanities who want to extract relevant information from large textual corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bashar_M/0/1/0/all/0/1\">Md Abul Bashar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_R/0/1/0/all/0/1\">Richi Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knapman_G/0/1/0/all/0/1\">Gareth Knapman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turnbull_P/0/1/0/all/0/1\">Paul Turnbull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fforde_C/0/1/0/all/0/1\">Cressida Fforde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GANTEE: Generative Adversatial Network for Taxonomy Entering Evaluation. (arXiv:2303.14480v1 [cs.AI])","link":"http://arxiv.org/abs/2303.14480","description":"<p>Taxonomy is formulated as directed acyclic concepts graphs or trees that\nsupport many downstream tasks. Many new coming concepts need to be added to an\nexisting taxonomy. The traditional taxonomy expansion task aims only at finding\nthe best position for new coming concepts in the existing taxonomy. However,\nthey have two drawbacks when being applied to the real-scenarios. The previous\nmethods suffer from low-efficiency since they waste much time when most of the\nnew coming concepts are indeed noisy concepts. They also suffer from\nlow-effectiveness since they collect training samples only from the existing\ntaxonomy, which limits the ability of the model to mine more hypernym-hyponym\nrelationships among real concepts. This paper proposes a pluggable framework\ncalled Generative Adversarial Network for Taxonomy Entering Evaluation (GANTEE)\nto alleviate these drawbacks. A generative adversarial network is designed in\nthis framework by discriminative models to alleviate the first drawback and the\ngenerative model to alleviate the second drawback. Two discriminators are used\nin GANTEE to provide long-term and short-term rewards, respectively. Moreover,\nto further improve the efficiency, pre-trained language models are used to\nretrieve the representation of the concepts quickly. The experiments on three\nreal-world large-scale datasets with two different languages show that GANTEE\nimproves the performance of the existing taxonomy expansion methods in both\neffectiveness and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhouhong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Sihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Hongwei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jian Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Transliteration Help Multilingual Language Modeling?. (arXiv:2201.12501v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12501","description":"<p>As there is a scarcity of large representative corpora for most languages, it\nis important for Multilingual Language Models (MLLM) to extract the most out of\nexisting corpora. In this regard, script diversity presents a challenge to\nMLLMs by reducing lexical overlap among closely related languages. Therefore,\ntransliterating closely related languages that use different writing scripts to\na common script may improve the downstream task performance of MLLMs. In this\npaper, we pretrain two ALBERT models to empirically measure the effect of\ntransliteration on MLLMs. We specifically focus on the Indo-Aryan language\nfamily, which has the highest script diversity in the world. Afterward, we\nevaluate our models on the IndicGLUE benchmark. We perform Mann-Whitney U test\nto rigorously verify whether the effect of transliteration is significant or\nnot. We find that transliteration benefits the low-resource languages without\nnegatively affecting the comparatively high-resource languages. We also measure\nthe cross-lingual representation similarity (CLRS) of the models using centered\nkernel alignment (CKA) on parallel sentences of eight languages from the\nFLORES-101 dataset. We find that the hidden representations of the\ntransliteration-based model have higher and more stable CLRS scores. Our code\nis available at Github (github.com/ibraheem-moosa/XLM-Indic) and Hugging Face\nHub (huggingface.co/ibraheemmoosa/xlmindic-base-multiscript and\nhuggingface.co/ibraheemmoosa/xlmindic-base-uniscript).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moosa_I/0/1/0/all/0/1\">Ibraheem Muhammad Moosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhter_M/0/1/0/all/0/1\">Mahmud Elahi Akhter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habib_A/0/1/0/all/0/1\">Ashfia Binte Habib</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Stream Transformer for Generic Event Boundary Captioning. (arXiv:2207.03038v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.03038","description":"<p>This paper describes our champion solution for the CVPR2022 Generic Event\nBoundary Captioning (GEBC) competition. GEBC requires the captioning model to\nhave a comprehension of instantaneous status changes around the given video\nboundary, which makes it much more challenging than conventional video\ncaptioning task. In this paper, a Dual-Stream Transformer with improvements on\nboth video content encoding and captions generation is proposed: (1) We utilize\nthree pre-trained models to extract the video features from different\ngranularities. Moreover, we exploit the types of boundary as hints to help the\nmodel generate captions. (2) We particularly design an model, termed as\nDual-Stream Transformer, to learn discriminative representations for boundary\ncaptioning. (3) Towards generating content-relevant and human-like captions, we\nimprove the description quality by designing a word-level ensemble strategy.\nThe promising results on the GEBC test split demonstrate the efficacy of our\nproposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hanhua Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Libo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Longyin Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Methods for Natural Language Processing: A Survey. (arXiv:2209.00099v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.00099","description":"<p>Recent work in natural language processing (NLP) has yielded appealing\nresults from scaling model parameters and training data; however, using only\nscale to improve performance means that resource consumption also grows. Such\nresources include data, time, storage, or energy, all of which are naturally\nlimited and unevenly distributed. This motivates research into efficient\nmethods that require fewer resources to achieve similar results. This survey\nsynthesizes and relates current methods and findings in efficient NLP. We aim\nto provide both guidance for conducting NLP under limited resources, and point\ntowards promising research directions for developing more efficient methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Treviso_M/0/1/0/all/0/1\">Marcos Treviso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Ji-Ung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1\">Tianchu Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aken_B/0/1/0/all/0/1\">Betty van Aken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qingqing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciosici_M/0/1/0/all/0/1\">Manuel R. Ciosici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassid_M/0/1/0/all/0/1\">Michael Hassid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heafield_K/0/1/0/all/0/1\">Kenneth Heafield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1\">Pedro H. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forde_J/0/1/0/all/0/1\">Jessica Zosa Forde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milder_P/0/1/0/all/0/1\">Peter Milder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_E/0/1/0/all/0/1\">Edwin Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Bidirectional Unsupervised Translation Through Multilingual Finetuning and Back-Translation. (arXiv:2209.02821v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.02821","description":"<p>We propose a two-stage approach for training a single NMT model to translate\nunseen languages both to and from English. For the first stage, we initialize\nan encoder-decoder model to pretrained XLM-R and RoBERTa weights, then perform\nmultilingual fine-tuning on parallel data in 40 languages to English. We find\nthis model can generalize to zero-shot translations on unseen languages. For\nthe second stage, we leverage this generalization ability to generate synthetic\nparallel data from monolingual datasets, then train with successive rounds of\nbidirectional back-translation.\n</p>\n<p>We term our approach EcXTra ({E}nglish-{c}entric Crosslingual ({X})\n{Tra}nsfer). Our approach is conceptually simple, only using a standard\ncross-entropy objective throughout, and also is data-driven, sequentially\nleveraging auxiliary parallel data and monolingual data. We evaluate our\nunsupervised NMT results on 7 low-resource languages, and find that each round\nof back-translation training further refines bidirectional performance. Our\nfinal single EcXTra-trained model achieves competitive translation performance\nin all translation directions, notably establishing a new state-of-the-art for\nEnglish-to-Kazakh (22.9 &gt; 10.4 BLEU).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bryan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasooli_M/0/1/0/all/0/1\">Mohammad Sadegh Rasooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Ajay Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation. (arXiv:2209.14627v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.14627","description":"<p>Open-domain dialogue systems aim to interact with humans through natural\nlanguage texts in an open-ended fashion. Despite the recent success of super\nlarge dialogue systems such as ChatGPT, using medium-to-small-sized dialogue\nsystems remains the common practice as they are more lightweight and\naccessible; however, generating diverse dialogue responses is challenging,\nespecially with smaller models. In this work, we propose an Equal-size Hard\nExpectation--Maximization (EqHard-EM) algorithm to train a multi-decoder model\nfor diverse dialogue generation. Our algorithm assigns a sample to a decoder in\na hard manner and additionally imposes an equal-assignment constraint to ensure\nthat all decoders are well-trained. We provide detailed theoretical analysis to\njustify our approach. Further, experiments on two large-scale open-domain\ndialogue datasets verify that our EqHard-EM algorithm generates high-quality\ndiverse responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuqiao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yongchang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanshuai Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model. (arXiv:2210.05335v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.05335","description":"<p>Multimodal semantic understanding often has to deal with uncertainty, which\nmeans the obtained messages tend to refer to multiple targets. Such uncertainty\nis problematic for our interpretation, including inter- and intra-modal\nuncertainty. Little effort has studied the modeling of this uncertainty,\nparticularly in pre-training on unlabeled datasets and fine-tuning in\ntask-specific downstream datasets. In this paper, we project the\nrepresentations of all modalities as probabilistic distributions via a\nProbability Distribution Encoder (PDE) by utilizing sequence-level\ninteractions. Compared to the existing deterministic methods, such uncertainty\nmodeling can convey richer multimodal semantic information and more complex\nrelationships. Furthermore, we integrate uncertainty modeling with popular\npre-training frameworks and propose suitable pre-training tasks:\nDistribution-based Vision-Language Contrastive learning (D-VLC),\nDistribution-based Masked Language Modeling (D-MLM), and Distribution-based\nImage-Text Matching (D-ITM). The fine-tuned models are applied to challenging\ndownstream tasks, including image-text retrieval, visual question answering,\nvisual reasoning, and visual entailment, and achieve state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yatai Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yuan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanru Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakai_T/0/1/0/all/0/1\">Tetsuya Sakai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot On-the-Fly Event Schema Induction. (arXiv:2210.06254v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06254","description":"<p>What are the events involved in a pandemic outbreak? What steps should be\ntaken when planning a wedding? The answers to these questions can be found by\ncollecting many documents on the complex event of interest, extracting relevant\ninformation, and analyzing it. We present a new approach in which large\nlanguage models are utilized to generate source documents that allow\npredicting, given a high-level event definition, the specific events,\narguments, and relations between them to construct a schema that describes the\ncomplex event in its entirety. Using our model, complete schemas on any topic\ncan be generated on-the-fly without any manual data collection, i.e., in a\nzero-shot manner. Moreover, we develop efficient methods to extract pertinent\ninformation from texts and demonstrate in a series of experiments that these\nschemas are considered to be more complete than human-curated ones in the\nmajority of examined scenarios. Finally, we show that this framework is\ncomparable in performance with previous supervised schema induction methods\nthat rely on collecting real texts while being more general and flexible\nwithout the need for a predefined ontology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dror_R/0/1/0/all/0/1\">Rotem Dror</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Entity Detection with Proposer and Regressor. (arXiv:2210.10260v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10260","description":"<p>Named entity recognition is a traditional task in natural language\nprocessing. In particular, nested entity recognition receives extensive\nattention for the widespread existence of the nesting scenario. The latest\nresearch migrates the well-established paradigm of set prediction in object\ndetection to cope with entity nesting. However, the manual creation of query\nvectors, which fail to adapt to the rich semantic information in the context,\nlimits these approaches. An end-to-end entity detection approach with proposer\nand regressor is presented in this paper to tackle the issues. First, the\nproposer utilizes the feature pyramid network to generate high-quality entity\nproposals. Then, the regressor refines the proposals for generating the final\nprediction. The model adopts encoder-only architecture and thus obtains the\nadvantages of the richness of query semantics, high precision of entity\nlocalization, and easiness of model training. Moreover, we introduce the novel\nspatially modulated attention and progressive refinement for further\nimprovement. Extensive experiments demonstrate that our model achieves advanced\nperformance in flat and nested NER, achieving a new state-of-the-art F1 score\nof 80.74 on the GENIA dataset and 72.38 on the WeiboNER dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xueru Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Changjiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haotian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Luguang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hong Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models. (arXiv:2210.16433v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16433","description":"<p>Fully-parametric language models generally require a huge number of model\nparameters to store the necessary knowledge for solving multiple natural\nlanguage tasks in zero/few-shot settings. In addition, it is hard to adapt to\nthe evolving world knowledge without the costly model re-training. In this\npaper, we develop a novel semi-parametric language model architecture,\nKnowledge-in-Context (KiC), which empowers a parametric text-to-text language\nmodel with a knowledge-rich external memory. Specifically, the external memory\ncontains six different types of knowledge: entity, dictionary, commonsense,\nevent, script, and causality knowledge. For each input instance, the KiC model\nadaptively selects a knowledge type and retrieves the most helpful pieces of\nknowledge. The input instance along with its knowledge augmentation is fed into\na text-to-text model (e.g., T5) to generate the output answer, where both the\ninput and the output are in natural language forms after prompting.\nInterestingly, we find that KiC can be identified as a special\nmixture-of-experts (MoE) model, where the knowledge selector plays the role of\na router that is used to determine the sequence-to-expert assignment in MoE.\nThis key observation inspires us to develop a novel algorithm for training KiC\nwith an instance-adaptive knowledge selector. As a knowledge-rich\nsemi-parametric language model, KiC only needs a much smaller parametric part\nto achieve superior zero-shot performance on unseen tasks. By evaluating on 40+\ndifferent tasks, we show that KiC_Large with 770M parameters easily outperforms\nlarge language models (LMs) that are 4-39x larger by a large margin. We also\ndemonstrate that KiC exhibits emergent abilities at a much smaller model scale\ncompared to the fully-parametric models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiaoman Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammatical Error Correction: A Survey of the State of the Art. (arXiv:2211.05166v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05166","description":"<p>Grammatical Error Correction (GEC) is the task of automatically detecting and\ncorrecting errors in text. The task not only includes the correction of\ngrammatical errors, such as missing prepositions and mismatched subject-verb\nagreement, but also orthographic and semantic errors, such as misspellings and\nword choice errors respectively. The field has seen significant progress in the\nlast decade, motivated in part by a series of five shared tasks, which drove\nthe development of rule-based methods, statistical classifiers, statistical\nmachine translation, and finally neural machine translation systems which\nrepresent the current dominant state of the art. In this survey paper, we\ncondense the field into a single article and first outline some of the\nlinguistic challenges of the task, introduce the most popular datasets that are\navailable to researchers (for both English and other languages), and summarise\nthe various methods and techniques that have been developed with a particular\nfocus on artificial error generation. We next describe the many different\napproaches to evaluation as well as concerns surrounding metric reliability,\nespecially in relation to subjective human judgements, before concluding with\nan overview of recent progress and suggestions for future work and remaining\nchallenges. We hope that this survey will serve as comprehensive resource for\nresearchers who are new to the field or who want to be kept apprised of recent\ndevelopments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bryant_C/0/1/0/all/0/1\">Christopher Bryant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qorib_M/0/1/0/all/0/1\">Muhammad Reza Qorib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hannan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briscoe_T/0/1/0/all/0/1\">Ted Briscoe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Temporal Modelling of Clinical Depression through Social Media Text. (arXiv:2211.07717v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07717","description":"<p>We describe the development of a model to detect user-level clinical\ndepression based on a user's temporal social media posts. Our model uses a\nDepression Symptoms Detection (DSD) classifier, which is trained on the largest\nexisting samples of clinician annotated tweets for clinical depression\nsymptoms. We subsequently use our DSD model to extract clinically relevant\nfeatures, e.g., depression scores and their consequent temporal patterns, as\nwell as user posting activity patterns, e.g., quantifying their ``no activity''\nor ``silence.'' Furthermore, to evaluate the efficacy of these extracted\nfeatures, we create three kinds of datasets including a test dataset, from two\nexisting well-known benchmark datasets for user-level depression detection. We\nthen provide accuracy measures based on single features, baseline features and\nfeature ablation tests, at several different levels of temporal granularity.\nThe relevant data distributions and clinical depression detection related\nsettings can be exploited to draw a complete picture of the impact of different\nfeatures across our created datasets. Finally, we show that, in general, only\nsemantic oriented representation models perform well. However, clinical\nfeatures may enhance overall performance provided that the training and testing\ndistribution is similar, and there is more data in a user's timeline. The\nconsequence is that the predictive capability of depression scores increase\nsignificantly while used in a more sensitive clinical depression detection\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1\">Nawshad Farruque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivapalan_S/0/1/0/all/0/1\">Sudhakar Sivapalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar R. Za&#xef;ane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually Grounded Commonsense Knowledge Acquisition. (arXiv:2211.12054v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.12054","description":"<p>Large-scale commonsense knowledge bases empower a broad range of AI\napplications, where the automatic extraction of commonsense knowledge (CKE) is\na fundamental and challenging problem. CKE from text is known for suffering\nfrom the inherent sparsity and reporting bias of commonsense in text. Visual\nperception, on the other hand, contains rich commonsense knowledge about\nreal-world entities, e.g., (person, can_hold, bottle), which can serve as\npromising sources for acquiring grounded commonsense knowledge. In this work,\nwe present CLEVER, which formulates CKE as a distantly supervised\nmulti-instance learning problem, where models learn to summarize commonsense\nrelations from a bag of images about an entity pair without any human\nannotation on image instances. To address the problem, CLEVER leverages\nvision-language pre-training models for deep understanding of each image in the\nbag, and selects informative instances from the bag to summarize commonsense\nentity relations via a novel contrastive attention mechanism. Comprehensive\nexperimental results in held-out and human evaluation show that CLEVER can\nextract commonsense knowledge in promising quality, outperforming pre-trained\nlanguage model-based methods by 3.9 AUC and 6.4 mAUC points. The predicted\ncommonsense scores show strong correlation with human judgment with a 0.78\nSpearman coefficient. Moreover, the extracted commonsense can also be grounded\ninto images with reasonable interpretability. The data and codes can be\nobtained at https://github.com/thunlp/CLEVER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianyu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengdi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Ruobing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning. (arXiv:2211.13437v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.13437","description":"<p>Cross-modal alignment is essential for vision-language pre-training (VLP)\nmodels to learn the correct corresponding information across different\nmodalities. For this purpose, inspired by the success of masked language\nmodeling (MLM) tasks in the NLP pre-training area, numerous masked modeling\ntasks have been proposed for VLP to further promote cross-modal interactions.\nThe core idea of previous masked modeling tasks is to focus on reconstructing\nthe masked tokens based on visible context for learning local-to-local\nalignment. However, most of them pay little attention to the global semantic\nfeatures generated for the masked data, resulting in a limited cross-modal\nalignment ability of global representations. Therefore, in this paper, we\npropose a novel Semantic Completion Learning (SCL) task, complementary to\nexisting masked modeling tasks, to facilitate global-to-local alignment.\nSpecifically, the SCL task complements the missing semantics of masked data by\ncapturing the corresponding information from the other modality, promoting\nlearning more representative global features which have a great impact on the\nperformance of downstream tasks. Moreover, we present a flexible vision\nencoder, which enables our model to perform image-text and video-text\nmultimodal tasks simultaneously. Experimental results show that our proposed\nmethod obtains state-of-the-art performance on various vision-language\nbenchmarks, such as visual question answering, image-text retrieval, and\nvideo-text retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yatai Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1\">Rongcheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1\">Weijie Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Few-Shot Temporal Action Detection. (arXiv:2211.14905v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.14905","description":"<p>Few-shot (FS) and zero-shot (ZS) learning are two different approaches for\nscaling temporal action detection (TAD) to new classes. The former adapts a\npretrained vision model to a new task represented by as few as a single video\nper class, whilst the latter requires no training examples by exploiting a\nsemantic description of the new class. In this work, we introduce a new\nmulti-modality few-shot (MMFS) TAD problem, which can be considered as a\nmarriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and new\nclass names jointly. To tackle this problem, we further introduce a novel\nMUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled by\nefficiently bridging pretrained vision and language models whilst maximally\nreusing already learned capacity. Concretely, we construct multi-modal prompts\nby mapping support videos into the textual token space of a vision-language\nmodel using a meta-learned adapter-equipped visual semantics tokenizer. To\ntackle large intra-class variation, we further design a query feature\nregulation scheme. Extensive experiments on ActivityNetv1.3 and THUMOS14\ndemonstrate that our MUPPET outperforms state-of-the-art alternative methods,\noften by a large margin. We also show that our MUPPET can be easily extended to\ntackle the few-shot object detection problem and again achieves the\nstate-of-the-art performance on MS-COCO dataset. The code will be available in\nhttps://github.com/sauradip/MUPPET\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sauradip Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Rua_J/0/1/0/all/0/1\">Juan-Manuel Perez-Rua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning. (arXiv:2212.01117v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01117","description":"<p>The spread of rumors along with breaking events seriously hinders the truth\nin the era of social media. Previous studies reveal that due to the lack of\nannotated resources, rumors presented in minority languages are hard to be\ndetected. Furthermore, the unforeseen breaking events not involved in\nyesterday's news exacerbate the scarcity of data resources. In this work, we\npropose a novel zero-shot framework based on prompt learning to detect rumors\nfalling in different domains or presented in different languages. More\nspecifically, we firstly represent rumor circulated on social media as diverse\npropagation threads, then design a hierarchical prompt encoding mechanism to\nlearn language-agnostic contextual representations for both prompts and rumor\ndata. To further enhance domain adaptation, we model the domain-invariant\nstructural features from the propagation threads, to incorporate structural\nposition representations of influential community response. In addition, a new\nvirtual response augmentation method is used to improve model training.\nExtensive experiments conducted on three real-world datasets demonstrate that\nour proposed model achieves much better performance than state-of-the-art\nmethods and exhibits a superior capacity for detecting rumors at early stages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_P/0/1/0/all/0/1\">Pengyao Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruifang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10405","description":"<p>Recently decades have witnessed the empirical success of framing Knowledge\nGraph (KG) embeddings via language models. However, language model-based KG\nembeddings are usually deployed as static artifacts, which are challenging to\nmodify without re-training after deployment. To address this issue, we propose\na new task of editing language model-based KG embeddings in this paper. The\nproposed task aims to enable data-efficient and fast updates to KG embeddings\nwithout damaging the performance of the rest. We build four new datasets:\nE-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge\nediting baselines demonstrating the limited ability of previous models to\nhandle the proposed challenging task. We further propose a simple yet strong\nbaseline dubbed KGEditor, which utilizes additional parametric layers of the\nhyper network to edit/add facts. Comprehensive experimental results demonstrate\nthat KGEditor can perform better when updating specific facts while not\naffecting the rest with low training resources. Code and datasets will be\navailable in https://github.com/zjunlp/PromptKG/tree/main/deltaKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1\">Bozhong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zelin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.02676","description":"<p>Learning from human preferences is important for language models to be\nhelpful and useful for humans, and to align with human and social values. Prior\nwork have achieved remarkable successes by learning from human feedback to\nunderstand and follow instructions. Nonetheless, these methods are either\nfounded on hand-picked model generations that are favored by human annotators,\nrendering them ineffective in terms of data utilization and challenging to\napply in general, or they depend on reward functions and reinforcement\nlearning, which are prone to imperfect reward function and extremely\nchallenging to optimize. In this work, we propose a novel technique, Chain of\nHindsight, that is easy to optimize and can learn from any form of feedback,\nregardless of its polarity. Our idea is inspired by how humans learn from\nextensive feedback presented in the form of languages. We convert all types of\nfeedback into sentences, which are then used to fine-tune the model, allowing\nus to take advantage of the language comprehension capabilities of language\nmodels. We condition the model on a sequence of model generations paired with\nfeedback. By doing so, models are trained to generate outputs based on\nfeedback, and models can learn to identify and correct negative attributes or\nerrors. Applying our method to large language models, we observed that Chain of\nHindsight significantly surpasses previous methods in aligning language models\nwith human preferences. We observed significant improvements on summarization\nand dialogue tasks and our approach is markedly preferred in human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sferrazza_C/0/1/0/all/0/1\">Carmelo Sferrazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. (arXiv:2302.14383v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.14383","description":"<p>We investigate compositional structures in data embeddings from pre-trained\nvision-language models (VLMs). Traditionally, compositionality has been\nassociated with algebraic operations on embeddings of words from a pre-existing\nvocabulary. In contrast, we seek to approximate representations from an encoder\nas combinations of a smaller set of vectors in the embedding space. These\nvectors can be seen as \"ideal words\" for generating concepts directly within\nthe embedding space of the model. We first present a framework for\nunderstanding compositional structures from a geometric perspective. We then\nexplain what these compositional structures entail probabilistically in the\ncase of VLM embeddings, providing intuitions for why they arise in practice.\nFinally, we empirically explore these structures in CLIP's embeddings and we\nevaluate their usefulness for solving different vision-language tasks such as\nclassification, debiasing, and retrieval. Our results show that simple linear\nalgebraic operations on embedding vectors can be used as compositional and\ninterpretable methods for regulating the behavior of VLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1\">Matthew Trager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1\">Pramuditha Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1\">Luca Zancato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1\">Parminder Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-Agnostic Meta-Learning for Natural Language Understanding Tasks in Finance. (arXiv:2303.02841v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.02841","description":"<p>Natural language understanding(NLU) is challenging for finance due to the\nlack of annotated data and the specialized language in that domain. As a\nresult, researchers have proposed to use pre-trained language model and\nmulti-task learning to learn robust representations. However, aggressive\nfine-tuning often causes over-fitting and multi-task learning may favor tasks\nwith significantly larger amounts data, etc. To address these problems, in this\npaper, we investigate model-agnostic meta-learning algorithm(MAML) in\nlow-resource financial NLU tasks. Our contribution includes: 1. we explore the\nperformance of MAML method with multiple types of tasks: GLUE datasets, SNLI,\nSci-Tail and Financial PhraseBank; 2. we study the performance of MAML method\nwith multiple single-type tasks: a real scenario stock price prediction problem\nwith twitter text data. Our models achieve the state-of-the-art performance\naccording to the experimental results, which demonstrate that our method can\nadapt fast and well to low-resource situations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bixing Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction. (arXiv:2303.05063v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.05063","description":"<p>Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated\nremarkable results in various natural language processing (NLP) tasks with\nin-context learning, which involves inference based on a few demonstration\nexamples. Despite their successes in NLP tasks, no investigation has been\nconducted to assess the ability of LLMs to perform document information\nextraction (DIE) using in-context learning. Applying LLMs to DIE poses two\nchallenges: the modality and task gap. To this end, we propose a simple but\neffective in-context learning framework called ICL-D3IE, which enables LLMs to\nperform DIE with different types of demonstration examples. Specifically, we\nextract the most difficult and distinct segments from hard training documents\nas hard demonstrations for benefiting all test instances. We design\ndemonstrations describing relationships that enable LLMs to understand\npositional relationships. We introduce formatting demonstrations for easy\nanswer extraction. Additionally, the framework improves diverse demonstrations\nby updating them iteratively. Our experiments on three widely used benchmark\ndatasets demonstrate that the ICL-D3IE framework enables GPT-3/ChatGPT to\nachieve superior performance when compared to previous pre-trained methods\nfine-tuned with full training in both the in-distribution (ID) setting and in\nthe out-of-distribution (OOD) setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiabang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parachute: Evaluating Interactive Human-LM Co-writing Systems. (arXiv:2303.06333v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2303.06333","description":"<p>A surge of advances in language models (LMs) has led to significant interest\nin using LMs to build co-writing systems, in which humans and LMs interactively\ncontribute to a shared writing artifact. However, there is a lack of studies\nassessing co-writing systems in interactive settings. We propose a\nhuman-centered evaluation framework, Parachute, for interactive co-writing\nsystems. Parachute showcases an integrative view of interaction evaluation,\nwhere each evaluation aspect consists of categorized practical metrics.\nFurthermore, we present Parachute with a use case to demonstrate how to\nevaluate and compare co-writing systems using Parachute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio Visual Language Maps for Robot Navigation. (arXiv:2303.07522v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2303.07522","description":"<p>While interacting in the world is a multi-sensory experience, many robots\ncontinue to predominantly rely on visual perception to map and navigate in\ntheir environments. In this work, we propose Audio-Visual-Language Maps\n(AVLMaps), a unified 3D spatial map representation for storing cross-modal\ninformation from audio, visual, and language cues. AVLMaps integrate the\nopen-vocabulary capabilities of multimodal foundation models pre-trained on\nInternet-scale data by fusing their features into a centralized 3D voxel grid.\nIn the context of navigation, we show that AVLMaps enable robot systems to\nindex goals in the map based on multimodal queries, e.g., textual descriptions,\nimages, or audio snippets of landmarks. In particular, the addition of audio\ninformation enables robots to more reliably disambiguate goal locations.\nExtensive experiments in simulation show that AVLMaps enable zero-shot\nmultimodal goal navigation from multimodal prompts and provide 50% better\nrecall in ambiguous scenarios. These capabilities extend to mobile robots in\nthe real world - navigating to landmarks referring to visual, audio, and\nspatial concepts. Videos and code are available at: https://avlmaps.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenguang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mees_O/0/1/0/all/0/1\">Oier Mees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain. (arXiv:2303.08179v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08179","description":"<p>This paper presents medBERTde, a pre-trained German BERT model specifically\ndesigned for the German medical domain. The model has been trained on a large\ncorpus of 4.7 Million German medical documents and has been shown to achieve\nnew state-of-the-art performance on eight different medical benchmarks covering\na wide range of disciplines and medical document types. In addition to\nevaluating the overall performance of the model, this paper also conducts a\nmore in-depth analysis of its capabilities. We investigate the impact of data\ndeduplication on the model's performance, as well as the potential benefits of\nusing more efficient tokenization methods. Our results indicate that\ndomain-specific models such as medBERTde are particularly useful for longer\ntexts, and that deduplication of training data does not necessarily lead to\nimproved performance. Furthermore, we found that efficient tokenization plays\nonly a minor role in improving model performance, and attribute most of the\nimproved performance to the large amount of training data. To encourage further\nresearch, the pre-trained model weights and new benchmarks based on\nradiological data are made publicly available for use by the scientific\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bressem_K/0/1/0/all/0/1\">Keno K. Bressem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papaioannou_J/0/1/0/all/0/1\">Jens-Michalis Papaioannou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grundmann_P/0/1/0/all/0/1\">Paul Grundmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borchert_F/0/1/0/all/0/1\">Florian Borchert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_L/0/1/0/all/0/1\">Lisa C. Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Leonhard Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_F/0/1/0/all/0/1\">Felix Busch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lina Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loyen_J/0/1/0/all/0/1\">Jan P. Loyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_S/0/1/0/all/0/1\">Stefan M. Niehues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augustin_M/0/1/0/all/0/1\">Moritz Augustin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosser_L/0/1/0/all/0/1\">Lennart Grosser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus R. Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aerts_H/0/1/0/all/0/1\">Hugo JWL. Aerts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loser_A/0/1/0/all/0/1\">Alexander L&#xf6;ser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NL4Opt Competition: Formulating Optimization Problems Based on Their Natural Language Descriptions. (arXiv:2303.08233v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08233","description":"<p>The Natural Language for Optimization (NL4Opt) Competition was created to\ninvestigate methods of extracting the meaning and formulation of an\noptimization problem based on its text description. Specifically, the goal of\nthe competition is to increase the accessibility and usability of optimization\nsolvers by allowing non-experts to interface with them using natural language.\nWe separate this challenging goal into two sub-tasks: (1) recognize and label\nthe semantic entities that correspond to the components of the optimization\nproblem; (2) generate a meaning representation (i.e., a logical form) of the\nproblem from its detected problem entities. The first task aims to reduce\nambiguity by detecting and tagging the entities of the optimization problems.\nThe second task creates an intermediate representation of the linear\nprogramming (LP) problem that is converted into a format that can be used by\ncommercial solvers. In this report, we present the LP word problem dataset and\nshared tasks for the NeurIPS 2022 competition. Furthermore, we investigate and\ncompare the performance of the ChatGPT large language model against the winning\nsolutions. Through this competition, we hope to bring interest towards the\ndevelopment of novel machine learning applications and datasets for\noptimization modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramamonjison_R/0/1/0/all/0/1\">Rindranirina Ramamonjison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Timothy T. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haley Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_B/0/1/0/all/0/1\">Bissan Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shiqi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mostajabdaveh_M/0/1/0/all/0/1\">Mahdi Mostajabdaveh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zirui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-4 Technical Report. (arXiv:2303.08774v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08774","description":"<p>We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+OpenAI/0/1/0/all/0/1\">OpenAI</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Consistent Learning: Cooperation between Generators and Discriminators. (arXiv:2303.09075v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.09075","description":"<p>Using generated data to improve the performance of downstream discriminative\nmodels has recently gained popularity due to the great development of\npre-trained language models. In most previous studies, generative models and\ndiscriminative models are trained separately and thus could not adapt to any\nchanges in each other. As a result, the generated samples can easily deviate\nfrom the real data distribution, while the improvement of the discriminative\nmodel quickly reaches saturation. Generative adversarial networks (GANs) train\ngenerative models via an adversarial process with discriminative models to\nachieve joint training. However, the training of standard GANs is notoriously\nunstable and often falls short of convergence. In this paper, to address these\nissues, we propose a $\\textit{self-consistent learning}$ framework, in which a\ndiscriminator and a generator are cooperatively trained in a closed-loop form.\nThe discriminator and the generator enhance each other during multiple rounds\nof alternating training until a scoring consensus is reached. This framework\nproves to be easy to train and free from instabilities such as mode collapse\nand non-convergence. Extensive experiments on sentence semantic matching\ndemonstrate the effectiveness of the proposed framework: the discriminator\nachieves 10+ AP of improvement on the zero-shot setting and new\nstate-of-the-art performance on the full-data setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhongshen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$P+$: Extended Textual Conditioning in Text-to-Image Generation. (arXiv:2303.09522v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.09522","description":"<p>We introduce an Extended Textual Conditioning space in text-to-image models,\nreferred to as $P+$. This space consists of multiple textual conditions,\nderived from per-layer prompts, each corresponding to a layer of the denoising\nU-net of the diffusion model.\n</p>\n<p>We show that the extended space provides greater disentangling and control\nover image synthesis. We further introduce Extended Textual Inversion (XTI),\nwhere the images are inverted into $P+$, and represented by per-layer tokens.\n</p>\n<p>We show that XTI is more expressive and precise, and converges faster than\nthe original Textual Inversion (TI) space. The extended inversion method does\nnot involve any noticeable trade-off between reconstruction and editability and\ninduces more regular inversions.\n</p>\n<p>We conduct a series of extensive experiments to analyze and understand the\nproperties of the new space, and to showcase the effectiveness of our method\nfor personalizing text-to-image models. Furthermore, we utilize the unique\nproperties of this space to achieve previously unattainable results in\nobject-style mixing using text-to-image models. Project page:\nhttps://prompt-plus.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1\">Andrey Voynov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qinghao Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1\">Kfir Aberman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Iso-FLOP Transformations for Maximizing Training Efficiency. (arXiv:2303.11525v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.11525","description":"<p>Recent works have explored the use of weight sparsity to improve the training\nefficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs).\nThese works aim to reduce training FLOPs but training with sparse weights often\nleads to accuracy loss or requires longer training schedules, making the\nresulting training efficiency less clear. In contrast, we focus on using\nsparsity to increase accuracy while using the same FLOPs as the dense model and\nshow training efficiency gains through higher accuracy. In this work, we\nintroduce Sparse-IFT, a family of Sparse Iso-FLOP Transformations which are\nused as drop-in replacements for dense layers to improve their representational\ncapacity and FLOP efficiency. Each transformation is parameterized by a single\nhyperparameter (sparsity level) and provides a larger search space to find\noptimal sparse masks. Without changing any training hyperparameters, replacing\ndense layers with Sparse-IFT leads to significant improvements across computer\nvision (CV) and natural language processing (NLP) tasks, including ResNet-18 on\nImageNet (+3.5%) and GPT-3 Small on WikiText-103 (-0.4 PPL), both matching\nlarger dense model variants that use 2x or more FLOPs. To our knowledge, this\nis the first work to demonstrate the use of sparsity for improving the accuracy\nof dense models via a simple-to-use set of sparse transformations. Code is\navailable at: https://github.com/CerebrasResearch/Sparse-IFT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Shreyas Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thangarasa_V/0/1/0/all/0/1\">Vithursan Thangarasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lie_S/0/1/0/all/0/1\">Sean Lie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13035","description":"<p>Electronic health records (EHRs) store an extensive array of patient\ninformation, encompassing medical histories, diagnoses, treatments, and test\noutcomes. These records are crucial for enabling healthcare providers to make\nwell-informed decisions regarding patient care. Summarizing clinical notes\nfurther assists healthcare professionals in pinpointing potential health risks\nand making better-informed decisions. This process contributes to reducing\nerrors and enhancing patient outcomes by ensuring providers have access to the\nmost pertinent and current patient data. Recent research has shown that\nincorporating prompts with large language models (LLMs) substantially boosts\nthe efficacy of summarization tasks. However, we show that this approach also\nleads to increased output variance, resulting in notably divergent outputs even\nwhen prompts share similar meanings. To tackle this challenge, we introduce a\nmodel-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft\nprompts to diminish variance while preserving the advantages of prompt-based\nsummarization. Experimental findings on multiple clinical note tasks and LLMs\nindicate that our method not only bolsters performance but also effectively\ncurbs variance for various LLMs, providing a more uniform and dependable\nsolution for summarizing vital medical information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yu-Neng Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness-guided Few-shot Prompting for Large Language Models. (arXiv:2303.13217v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13217","description":"<p>Large language models have demonstrated surprising ability to perform\nin-context learning, i.e., these models can be directly applied to solve\nnumerous downstream tasks by conditioning on a prompt constructed by a few\ninput-output examples. However, prior research has shown that in-context\nlearning can suffer from high instability due to variations in training\nexamples, example order, and prompt formats. Therefore, the construction of an\nappropriate prompt is essential for improving the performance of in-context\nlearning. In this paper, we revisit this problem from the view of predictive\nbias. Specifically, we introduce a metric to evaluate the predictive bias of a\nfixed prompt against labels or a given attributes. Then we empirically show\nthat prompts with higher bias always lead to unsatisfactory predictive quality.\nBased on this observation, we propose a novel search strategy based on the\ngreedy search to identify the near-optimal prompt for improving the performance\nof in-context learning. We perform comprehensive experiments with\nstate-of-the-art mainstream models such as GPT-3 on various downstream tasks.\nOur results indicate that our method can enhance the model's in-context\nlearning performance in an effective and interpretable manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Huan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1\">Yatao Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bingzhe Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUG: A General Meeting Understanding and Generation Benchmark. (arXiv:2303.13939v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13939","description":"<p>Listening to long video/audio recordings from video conferencing and online\ncourses for acquiring information is extremely inefficient. Even after ASR\nsystems transcribe recordings into long-form spoken language documents, reading\nASR transcripts only partly speeds up seeking information. It has been observed\nthat a range of NLP applications, such as keyphrase extraction, topic\nsegmentation, and summarization, significantly improve users' efficiency in\ngrasping important information. The meeting scenario is among the most valuable\nscenarios for deploying these spoken language processing (SLP) capabilities.\nHowever, the lack of large-scale public meeting datasets annotated for these\nSLP tasks severely hinders their advancement. To prompt SLP advancement, we\nestablish a large-scale general Meeting Understanding and Generation Benchmark\n(MUG) to benchmark the performance of a wide range of SLP tasks, including\ntopic segmentation, topic-level and session-level extractive summarization and\ntopic title generation, keyphrase extraction, and action item detection. To\nfacilitate the MUG benchmark, we construct and release a large-scale meeting\ndataset for comprehensive long-form SLP development, the AliMeeting4MUG Corpus,\nwhich consists of 654 recorded Mandarin meeting sessions with diverse topic\ncoverage, with manual annotations for SLP tasks on manual transcripts of\nmeeting recordings. To the best of our knowledge, the AliMeeting4MUG Corpus is\nso far the largest meeting corpus in scale and facilitates most SLP tasks. In\nthis paper, we provide a detailed introduction of this corpus, SLP tasks and\nevaluation methods, baseline systems and their performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhijie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning Skills of LLMs. (arXiv:2303.12810v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2303.12810","description":"<p>The potential of large language models (LLMs) to reason like humans has been\na highly contested topic in Machine Learning communities. However, the\nreasoning abilities of humans are multifaceted and can be seen in various\nforms, including analogical, spatial and moral reasoning, among others. This\nfact raises the question whether LLMs can perform equally well across all these\ndifferent domains. This research work aims to investigate the performance of\nLLMs on different reasoning tasks by conducting experiments that directly use\nor draw inspirations from existing datasets on analogical and spatial\nreasoning. Additionally, to evaluate the ability of LLMs to reason like human,\ntheir performance is evaluted on more open-ended, natural language questions.\nMy findings indicate that LLMs excel at analogical and moral reasoning, yet\nstruggle to perform as proficiently on spatial reasoning tasks. I believe these\nexperiments are crucial for informing the future development of LLMs,\nparticularly in contexts that require diverse reasoning proficiencies. By\nshedding light on the reasoning abilities of LLMs, this study aims to push\nforward our understanding of how they can better emulate the cognitive\nabilities of humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Shrivats Agrawal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}