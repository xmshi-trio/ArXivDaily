{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-10-24T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models. (arXiv:2210.11498v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11498","description":"<p>Traditional (fickle) adversarial examples involve finding a small\nperturbation that does not change an input's true label but confuses the\nclassifier into outputting a different prediction. Conversely, obstinate\nadversarial examples occur when an adversary finds a small perturbation that\npreserves the classifier's prediction but changes the true label of an input.\nAdversarial training and certified robust training have shown some\neffectiveness in improving the robustness of machine learnt models to fickle\nadversarial examples. We show that standard adversarial training methods\nfocused on reducing vulnerability to fickle adversarial examples may make a\nmodel more vulnerable to obstinate adversarial examples, with experiments for\nboth natural language inference and paraphrase identification tasks. To counter\nthis phenomenon, we introduce Balanced Adversarial Training, which incorporates\ncontrastive learning to increase robustness against both fickle and obstinate\nadversarial examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hannah Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_D/0/1/0/all/0/1\">David Evans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Communication breakdown: On the low mutual intelligibility between human and neural captioning. (arXiv:2210.11512v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11512","description":"<p>We compare the 0-shot performance of a neural caption-based image retriever\nwhen given as input either human-produced captions or captions generated by a\nneural captioner. We conduct this comparison on the recently introduced\nImageCoDe data-set \\citep{Krojer:etal:2022}, which contains hard distractors\nnearly identical to the images to be retrieved. We find that the neural\nretriever has much higher performance when fed neural rather than human\ncaptions, despite the fact that the former, unlike the latter, were generated\nwithout awareness of the distractors that make the task hard. Even more\nremarkably, when the same neural captions are given to human subjects, their\nretrieval performance is almost at chance level. Our results thus add to the\ngrowing body of evidence that, even when the ``language'' of neural models\nresembles English, this superficial resemblance might be deeply misleading.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1\">Roberto Dess&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gualdoni_E/0/1/0/all/0/1\">Eleonora Gualdoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franzon_F/0/1/0/all/0/1\">Francesca Franzon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boleda_G/0/1/0/all/0/1\">Gemma Boleda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1\">Marco Baroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Text Deidentification. (arXiv:2210.11528v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11528","description":"<p>Deidentification seeks to anonymize textual data prior to distribution.\nAutomatic deidentification primarily uses supervised named entity recognition\nfrom human-labeled data points. We propose an unsupervised deidentification\nmethod that masks words that leak personally-identifying information. The\napproach utilizes a specially trained reidentification model to identify\nindividuals from redacted personal documents. Motivated by K-anonymity based\nprivacy, we generate redactions that ensure a minimum reidentification rank for\nthe correct profile of the document. To evaluate this approach, we consider the\ntask of deidentifying Wikipedia Biographies, and evaluate using an adversarial\nreidentification metric. Compared to a set of unsupervised baselines, our\napproach deidentifies documents more completely while removing fewer words.\nQualitatively, we see that the approach eliminates many identifying aspects\nthat would fall outside of the common named entity based approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morris_J/0/1/0/all/0/1\">John X. Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1\">Justin T. Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zabih_R/0/1/0/all/0/1\">Ramin Zabih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CONSISTENT: Open-Ended Question Generation From News Articles. (arXiv:2210.11536v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11536","description":"<p>Recent work on question generation has largely focused on factoid questions\nsuch as who, what, where, when about basic facts. Generating open-ended why,\nhow, what, etc. questions that require long-form answers have proven more\ndifficult. To facilitate the generation of open-ended questions, we propose\nCONSISTENT, a new end-to-end system for generating open-ended questions that\nare answerable from and faithful to the input text. Using news articles as a\ntrustworthy foundation for experimentation, we demonstrate our model's strength\nover several baselines using both automatic and human=based evaluations. We\ncontribute an evaluation dataset of expert-generated open-ended questions.We\ndiscuss potential downstream applications for news media organizations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1\">Justin Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Dataset Shortcuts with Grammar Induction. (arXiv:2210.11560v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11560","description":"<p>Many NLP datasets have been found to contain shortcuts: simple decision rules\nthat achieve surprisingly high accuracy. However, it is difficult to discover\nshortcuts automatically. Prior work on automatic shortcut detection has focused\non enumerating features like unigrams or bigrams, which can find only low-level\nshortcuts, or relied on post-hoc model interpretability methods like saliency\nmaps, which reveal qualitative patterns without a clear statistical\ninterpretation. In this work, we propose to use probabilistic grammars to\ncharacterize and discover shortcuts in NLP datasets. Specifically, we use a\ncontext-free grammar to model patterns in sentence classification datasets and\nuse a synchronous context-free grammar to model datasets involving sentence\npairs. The resulting grammars reveal interesting shortcut features in a number\nof datasets, including both simple and high-level features, and automatically\nidentify groups of test examples on which conventional classifiers fail.\nFinally, we show that the features we discover can be used to generate\ndiagnostic contrast examples and incorporated into standard robust optimization\nmethods to improve worst-group accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friedman_D/0/1/0/all/0/1\">Dan Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wettig_A/0/1/0/all/0/1\">Alexander Wettig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Paraphrasing for Textual Enrichment. (arXiv:2210.11563v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11563","description":"<p>Understanding inferences and answering questions from text requires more than\nmerely recovering surface arguments, adjuncts, or strings associated with the\nquery terms. As humans, we interpret sentences as contextualized components of\na narrative or discourse, by both filling in missing information, and reasoning\nabout event consequences. In this paper, we define the process of rewriting a\ntextual expression (lexeme or phrase) such that it reduces ambiguity while also\nmaking explicit the underlying semantics that is not (necessarily) expressed in\nthe economy of sentence structure as Dense Paraphrasing (DP). We build the\nfirst complete DP dataset, provide the scope and design of the annotation task,\nand present results demonstrating how this DP process can enrich a source text\nto improve inferencing and QA task performance. The data and the source code\nwill be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1\">Jingxuan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rim_K/0/1/0/all/0/1\">Kyeongmin Rim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holderness_E/0/1/0/all/0/1\">Eben Holderness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pustejovsky_J/0/1/0/all/0/1\">James Pustejovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Human Strategies for Generating Word-Level Adversarial Examples. (arXiv:2210.11598v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11598","description":"<p>Adversarial examples in NLP are receiving increasing research attention. One\nline of investigation is the generation of word-level adversarial examples\nagainst fine-tuned Transformer models that preserve naturalness and\ngrammaticality. Previous work found that human- and machine-generated\nadversarial examples are comparable in their naturalness and grammatical\ncorrectness. Most notably, humans were able to generate adversarial examples\nmuch more effortlessly than automated attacks. In this paper, we provide a\ndetailed analysis of exactly how humans create these adversarial examples. By\nexploring the behavioural patterns of human workers during the generation\nprocess, we identify statistically significant tendencies based on which words\nhumans prefer to select for adversarial replacement (e.g., word frequencies,\nword saliencies, sentiment) as well as where and when words are replaced in an\ninput sequence. With our findings, we seek to inspire efforts that harness\nhuman strategies for more robust NLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mozes_M/0/1/0/all/0/1\">Maximilian Mozes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_B/0/1/0/all/0/1\">Bennett Kleinberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffin_L/0/1/0/all/0/1\">Lewis D. Griffin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The VolcTrans System for WMT22 Multilingual Machine Translation Task. (arXiv:2210.11599v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11599","description":"<p>This report describes our VolcTrans system for the WMT22 shared task on\nlarge-scale multilingual machine translation. We participated in the\nunconstrained track which allows the use of external resources. Our system is a\ntransformerbased multilingual model trained on data from multiple sources\nincluding the public training set from the data track, NLLB data provided by\nMeta AI, self-collected parallel corpora, and pseudo bitext from\nback-translation. A series of heuristic rules clean both bilingual and\nmonolingual texts. On the official test set, our system achieves 17.3 BLEU,\n21.9 spBLEU, and 41.9 chrF2++ on average over all language pairs. The average\ninference speed is 11.5 sentences per second using a single Nvidia Tesla V100\nGPU. Our code and trained models are available at\nhttps://github.com/xian8/wmt22\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xian Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingyuan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tag-Set-Sequence Learning for Generating Question-Answer Pairs. (arXiv:2210.11608v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11608","description":"<p>Transformer-based QG models can generate question-answer pairs (QAPs) with\nhigh qualities, but may also generate silly questions for certain texts. We\npresent a new method called tag-set sequence learning to tackle this problem,\nwhere a tag-set sequence is a sequence of tag sets to capture the syntactic and\nsemantic information of the underlying sentence, and a tag set consists of one\nor more language feature tags, including, for example, semantic-role-labeling,\npart-of-speech, named-entity-recognition, and sentiment-indication tags. We\nconstruct a system called TSS-Learner to learn tag-set sequences from given\ndeclarative sentences and the corresponding interrogative sentences, and derive\nanswers to the latter. We train a TSS-Learner model for the English language\nusing a small training dataset and show that it can indeed generate adequate\nQAPs for certain texts that transformer-based models do poorly. Human\nevaluation on the QAPs generated by TSS-Learner over SAT practice reading tests\nis encouraging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Self-Improve. (arXiv:2210.11610v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11610","description":"<p>Large Language Models (LLMs) have achieved excellent performances in various\ntasks. However, fine-tuning an LLM requires extensive supervision. Human, on\nthe other hand, may improve their reasoning abilities by self-thinking without\nexternal inputs. In this work, we demonstrate that an LLM is also capable of\nself-improving with only unlabeled datasets. We use a pre-trained LLM to\ngenerate \"high-confidence\" rationale-augmented answers for unlabeled questions\nusing Chain-of-Thought prompting and self-consistency, and fine-tune the LLM\nusing those self-generated solutions as target outputs. We show that our\napproach improves the general reasoning ability of a 540B-parameter LLM\n(74.4%-&gt;82.1% on GSM8K, 78.2%-&gt;83.0% on DROP, 90.0%-&gt;94.4% on OpenBookQA, and\n63.4%-&gt;67.9% on ANLI-A3) and achieves state-of-the-art-level performance,\nwithout any ground truth label. We conduct ablation studies and show that\nfine-tuning on reasoning is critical for self-improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuexin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for a higher power in the human evaluation of MT. (arXiv:2210.11612v1 [stat.AP])","link":"http://arxiv.org/abs/2210.11612","description":"<p>In MT evaluation, pairwise comparisons are conducted to identify the better\nsystem. In conducting the comparison, the experimenter must allocate a budget\nto collect Direct Assessment (DA) judgments. We provide a cost effective way to\nspend the budget, but show that typical budget sizes often do not allow for\nsolid comparison. Taking the perspective that the basis of solid comparison is\nin achieving statistical significance, we study the power (rate of achieving\nsignificance) on a large collection of pairwise DA comparisons. Due to the\nnature of statistical estimation, power is low for differentiating less than\n1-2 DA points, and to achieve a notable increase in power requires at least\n2-3x more samples. Applying variance reduction alone will not yield these\ngains, so we must face the reality of undetectable differences and spending\nincreases. In this context, we propose interim testing, an \"early stopping\"\ncollection procedure that yields more power per judgment collected, which\nadaptively focuses the budget on pairs that are borderline significant. Interim\ntesting can achieve up to a 27% efficiency gain when spending 3x the current\nbudget, or 18% savings at the current evaluation power.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Wei_J/0/1/0/all/0/1\">Johnny Tian-Zheng Wei</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kocmi_T/0/1/0/all/0/1\">Tom Kocmi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Federmann_C/0/1/0/all/0/1\">Christian Federmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Natural Language Generation from Instructions with Meta-Learning. (arXiv:2210.11617v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11617","description":"<p>Recent work has shown that language models (LMs) trained with multi-task\n\\textit{instructional learning} (MTIL) can solve diverse NLP tasks in zero- and\nfew-shot settings with improved performance compared to prompt tuning. MTIL\nillustrates that LMs can extract and use information about the task from\ninstructions beyond the surface patterns of the inputs and outputs. This\nsuggests that meta-learning may further enhance the utilization of instructions\nfor effective task transfer. In this paper we investigate whether meta-learning\napplied to MTIL can further improve generalization to unseen tasks in a\nzero-shot setting. Specifically, we propose to adapt meta-learning to MTIL in\nthree directions: 1) Model Agnostic Meta Learning (MAML), 2) Hyper-Network\n(HNet) based adaptation to generate task specific parameters conditioned on\ninstructions, and 3) an approach combining HNet and MAML. Through extensive\nexperiments on the large scale Natural Instructions V2 dataset, we show that\nour proposed approaches significantly improve over strong baselines in\nzero-shot settings. In particular, meta-learning improves the effectiveness of\ninstructions and is most impactful when the test tasks are strictly zero-shot\n(i.e. no similar tasks in the training set) and are \"hard\" for LMs,\nillustrating the potential of meta-learning for MTIL for out-of-distribution\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1\">Budhaditya Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitasking Models are Robust to Structural Failure: A Neural Model for Bilingual Cognitive Reserve. (arXiv:2210.11618v1 [cs.LG])","link":"http://arxiv.org/abs/2210.11618","description":"<p>We find a surprising connection between multitask learning and robustness to\nneuron failures. Our experiments show that bilingual language models retain\nhigher performance under various neuron perturbations, such as random\ndeletions, magnitude pruning and weight noise compared to equivalent\nmonolingual ones. We provide a theoretical justification for this robustness by\nmathematically analyzing linear representation learning and showing that\nmultitasking creates more robust representations. Our analysis connects\nrobustness to spectral properties of the learned representation and proves that\nmultitasking leads to higher robustness for diverse task vectors. We\nopen-source our code and models:\nhttps://github.com/giannisdaras/multilingual_robustness\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daras_G/0/1/0/all/0/1\">Giannis Daras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raoof_N/0/1/0/all/0/1\">Negin Raoof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkalitsiou_Z/0/1/0/all/0/1\">Zoi Gkalitsiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMaLL-100: Introducing Shallow Multilingual Machine Translation Model for Low-Resource Languages. (arXiv:2210.11621v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11621","description":"<p>In recent years, multilingual machine translation models have achieved\npromising performance on low-resource language pairs by sharing information\nbetween similar languages, thus enabling zero-shot translation. To overcome the\n\"curse of multilinguality\", these models often opt for scaling up the number of\nparameters, which makes their use in resource-constrained environments\nchallenging. We introduce SMaLL-100, a distilled version of the M2M-100 (12B)\nmodel, a massively multilingual machine translation model covering 100\nlanguages. We train SMaLL-100 with uniform sampling across all language pairs\nand therefore focus on preserving the performance of low-resource languages. We\nevaluate SMaLL-100 on different low-resource benchmarks: FLORES-101, Tatoeba,\nand TICO-19 and demonstrate that it outperforms previous massively multilingual\nmodels of comparable sizes (200-600M) while improving inference latency and\nmemory usage. Additionally, our model achieves comparable results to M2M-100\n(1.2B), while being 3.6x smaller and 4.3x faster at inference. Code and\npre-trained models: https://github.com/alirezamshi/small100\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1\">Alireza Mohammadshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1\">Alexandre Berard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brun_C/0/1/0/all/0/1\">Caroline Brun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Domains Be Transferred Across Languages in Multi-Domain Multilingual Neural Machine Translation?. (arXiv:2210.11628v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11628","description":"<p>Previous works mostly focus on either multilingual or multi-domain aspects of\nneural machine translation (NMT). This paper investigates whether the domain\ninformation can be transferred across languages on the composition of\nmulti-domain and multilingual NMT, particularly for the incomplete data\ncondition where in-domain bitext is missing for some language pairs. Our\nresults in the curated leave-one-domain-out experiments show that multi-domain\nmultilingual (MDML) NMT can boost zero-shot translation performance up to +10\ngains on BLEU, as well as aid the generalisation of multi-domain NMT to the\nmissing domain. We also explore strategies for effective integration of\nmultilingual and multi-domain NMT, including language and domain tag\ncombination and auxiliary task training. We find that learning domain-aware\nrepresentations and adding target-language tags to the encoder leads to\neffective MDML-NMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Thuy-Trang Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khadivi_S/0/1/0/all/0/1\">Shahram Khadivi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanli He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Large Language Models to Enhance Programming Error Messages. (arXiv:2210.11630v1 [cs.HC])","link":"http://arxiv.org/abs/2210.11630","description":"<p>A key part of learning to program is learning to understand programming error\nmessages. They can be hard to interpret and identifying the cause of errors can\nbe time-consuming. One factor in this challenge is that the messages are\ntypically intended for an audience that already knows how to program, or even\nfor programming environments that then use the information to highlight areas\nin code. Researchers have been working on making these errors more novice\nfriendly since the 1960s, however progress has been slow. The present work\ncontributes to this stream of research by using large language models to\nenhance programming error messages with explanations of the errors and\nsuggestions on how to fix the error. Large language models can be used to\ncreate useful and novice-friendly enhancements to programming error messages\nthat sometimes surpass the original programming error messages in\ninterpretability and actionability. These results provide further evidence of\nthe benefits of large language models for computing educators, highlighting\ntheir use in areas known to be challenging for students. We further discuss the\nbenefits and downsides of large language models and highlight future streams of\nresearch for enhancing programming error messages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leinonen_J/0/1/0/all/0/1\">Juho Leinonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellas_A/0/1/0/all/0/1\">Arto Hellas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarsa_S/0/1/0/all/0/1\">Sami Sarsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reeves_B/0/1/0/all/0/1\">Brent Reeves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1\">Paul Denny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prather_J/0/1/0/all/0/1\">James Prather</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_B/0/1/0/all/0/1\">Brett A. Becker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Semi-supervised End-to-end Automatic Speech Recognition using CycleGAN and Inter-domain Losses. (arXiv:2210.11642v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11642","description":"<p>We propose a novel method that combines CycleGAN and inter-domain losses for\nsemi-supervised end-to-end automatic speech recognition. Inter-domain loss\ntargets the extraction of an intermediate shared representation of speech and\ntext inputs using a shared network. CycleGAN uses cycle-consistent loss and the\nidentity mapping loss to preserve relevant characteristics of the input feature\nafter converting from one domain to another. As such, both approaches are\nsuitable to train end-to-end models on unpaired speech-text inputs. In this\npaper, we exploit the advantages from both inter-domain loss and CycleGAN to\nachieve better shared representation of unpaired speech and text inputs and\nthus improve the speech-to-text mapping. Our experimental results on the WSJ\neval92 and Voxforge (non English) show 8~8.5% character error rate reduction\nover the baseline, and the results on LibriSpeech test_clean also show\nnoticeable improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chia-Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIT at MixMT 2022: Fluent Translation Built on Giant Pre-trained Models. (arXiv:2210.11670v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11670","description":"<p>This paper describes the Stevens Institute of Technology's submission for the\nWMT 2022 Shared Task: Code-mixed Machine Translation (MixMT). The task\nconsisted of two subtasks, subtask $1$ Hindi/English to Hinglish and subtask\n$2$ Hinglish to English translation. Our findings lie in the improvements made\nthrough the use of large pre-trained multilingual NMT models and in-domain\ndatasets, as well as back-translation and ensemble techniques. The translation\noutput is automatically evaluated against the reference translations using\nROUGE-L and WER. Our system achieves the $1^{st}$ position on subtask $2$\naccording to ROUGE-L, WER, and human evaluation, $1^{st}$ position on subtask\n$1$ according to WER and human evaluation, and $3^{rd}$ position on subtask $1$\nwith respect to ROUGE-L metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Abdul Rafae Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanade_H/0/1/0/all/0/1\">Hrishikesh Kanade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budhrani_G/0/1/0/all/0/1\">Girish Amar Budhrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jhanglani_P/0/1/0/all/0/1\">Preet Jhanglani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jia Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLING: Sino Linguistic Evaluation of Large Language Models. (arXiv:2210.11689v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11689","description":"<p>To understand what kinds of linguistic knowledge are encoded by pretrained\nChinese language models (LMs), we introduce the benchmark of Sino LINGuistics\n(SLING), which consists of 38K minimal sentence pairs in Mandarin Chinese\ngrouped into 9 high-level linguistic phenomena. Each pair demonstrates the\nacceptability contrast of a specific syntactic or semantic phenomenon (e.g.,\nThe keys are lost vs. The keys is lost), and an LM should assign lower\nperplexity to the acceptable sentence. In contrast to the CLiMP dataset (Xiang\net al., 2021), which also contains Chinese minimal pairs and was created by\ntranslating the vocabulary of the English BLiMP dataset, the minimal pairs in\nSLING are derived primarily by applying syntactic and lexical transformations\nto naturally-occurring, linguist-annotated sentences from the Chinese Treebank\n9.0, thus addressing severe issues in CLiMP's data generation process. We test\n18 publicly available pretrained monolingual (e.g., BERT-base-zh, CPM) and\nmulti-lingual (e.g., mT5, XLM) language models on SLING. Our experiments show\nthat the average accuracy for LMs is far below human performance (69.7% vs.\n97.1%), while BERT-base-zh achieves the highest accuracy (84.8%) of all tested\nLMs, even much larger ones. Additionally, we find that most LMs have a strong\ngender and number (singular/plural) bias, and they perform better on local\nphenomena than hierarchical ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yixiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_R/0/1/0/all/0/1\">Rajesh Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem. (arXiv:2210.11694v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11694","description":"<p>Math word problem solver requires both precise relation reasoning about\nquantities in the text and reliable generation for the diverse equation.\nCurrent sequence-to-tree or relation extraction methods regard this only from a\nfixed view, struggling to simultaneously handle complex semantics and diverse\nequations. However, human solving naturally involves two consistent reasoning\nviews: top-down and bottom-up, just as math equations also can be expressed in\nmultiple equivalent forms: pre-order and post-order. We propose a multi-view\nconsistent contrastive learning for a more complete semantics-to-equation\nmapping. The entire process is decoupled into two independent but consistent\nviews: top-down decomposition and bottom-up construction, and the two reasoning\nviews are aligned in multi-granularity for consistency, enhancing global\ngeneration and precise reasoning. Experiments on multiple datasets across two\nlanguages show our approach significantly outperforms the existing baselines,\nespecially on complex problems. We also show after consistent alignment,\nmulti-view can absorb the merits of both views and generate more diverse\nresults consistent with the mathematical laws.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanna Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaoxia Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zeqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nong_Q/0/1/0/all/0/1\">Qingpeng Nong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficiently Tuned Parameters are Task Embeddings. (arXiv:2210.11705v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11705","description":"<p>Intermediate-task transfer can benefit a wide range of NLP tasks with\nproperly selected source datasets. However, it is computationally infeasible to\nexperiment with all intermediate transfer combinations, making choosing a\nuseful source task a challenging problem. In this paper, we anticipate that\ntask-specific parameters updated in parameter-efficient tuning methods are\nlikely to encode task-specific information. Therefore, such parameters can be\npredictive for inter-task transferability. Thus, we propose to exploit these\nefficiently tuned parameters as off-the-shelf task embeddings for the efficient\nselection of source datasets for intermediate-task transfer. We experiment with\n11 text classification tasks and 11 question answering tasks. Experimental\nresults show that our approach can consistently outperform existing inter-task\ntransferability prediction methods while being conceptually simple and\ncomputationally efficient. Our analysis also reveals that the ability of\nefficiently tuned parameters on transferability prediction is disentangled with\ntheir in-task performance. This allows us to use parameters from early\ncheckpoints as task embeddings to further improve efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning. (arXiv:2210.11708v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11708","description":"<p>Commonsense generation aims to generate a realistic sentence describing a\ndaily scene under the given concepts, which is very challenging, since it\nrequires models to have relational reasoning and compositional generalization\ncapabilities. Previous work focuses on retrieving prototype sentences for the\nprovided concepts to assist generation. They first use a sparse retriever to\nretrieve candidate sentences, then re-rank the candidates with a ranker.\nHowever, the candidates returned by their ranker may not be the most relevant\nsentences, since the ranker treats all candidates equally without considering\ntheir relevance to the reference sentences of the given concepts. Another\nproblem is that re-ranking is very expensive, but only using retrievers will\nseriously degrade the performance of their generation models. To solve these\nproblems, we propose the metric distillation rule to distill knowledge from the\nmetric (e.g., BLEU) to the ranker. We further transfer the critical knowledge\nsummarized by the distilled ranker to the retriever. In this way, the relevance\nscores of candidate sentences predicted by the ranker and retriever will be\nmore consistent with their quality measured by the metric. Experimental results\non the CommonGen benchmark verify the effectiveness of our proposed method: (1)\nOur generation model with the distilled ranker achieves a new state-of-the-art\nresult. (2) Our generation model with the distilled retriever even surpasses\nthe previous SOTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_A/0/1/0/all/0/1\">A-Long Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Weizhen Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bartuer Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Biao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiu_S/0/1/0/all/0/1\">SM Yiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modelling Multi-relations for Convolutional-based Knowledge Graph Embedding. (arXiv:2210.11711v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11711","description":"<p>Representation learning of knowledge graphs aims to embed entities and\nrelations into low-dimensional vectors. Most existing works only consider the\ndirect relations or paths between an entity pair. It is considered that such\napproaches disconnect the semantic connection of multi-relations between an\nentity pair, and we propose a convolutional and multi-relational representation\nlearning model, ConvMR. The proposed ConvMR model addresses the multi-relation\nissue in two aspects: (1) Encoding the multi-relations between an entity pair\ninto a unified vector that maintains the semantic connection. (2) Since not all\nrelations are necessary while joining multi-relations, we propose an\nattention-based relation encoder to automatically assign weights to different\nrelations based on semantic hierarchy. Experimental results on two popular\ndatasets, FB15k-237 and WN18RR, achieved consistent improvements on the mean\nrank. We also found that ConvMR is efficient to deal with less frequent\nentities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sirui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kok Wai Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dengya Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_C/0/1/0/all/0/1\">Chun Che Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design a Sustainable Micro-mobility Future: Trends and Challenges in the United States and European Union Using Natural Language Processing Techniques. (arXiv:2210.11714v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11714","description":"<p>Micro-mobility devices are rapidly gaining popularity since people could\nbenefit from their efficiency, low cost and sustainability. However, people\nstill face challenges that detain the development and full integration of these\ndevices. In the present study, we examined people's opinions and experiences\nabout micro-mobility in the US and the EU using social media data on Twitter.\nWe made use of topic modeling based on advanced natural language processing\ntechniques and categorized the data into seven topics: promotion and service,\nmobility, technical features, acceptance, recreation, infrastructure and\nregulations. Furthermore, using sentiment analysis, we investigated people's\npositive and negative attitudes towards specific aspects of these topics and\ncompared the patterns of the trends and challenges in the US and the EU. We\nfound that 1) promotion and service included the majority of Twitter\ndiscussions in the both regions, 2) the EU had more positive opinions than the\nUS, 3) micro-mobility devices were more widely used for utilitarian mobility\nand recreational purposes in the EU than in the US, and 4) compared to the EU,\npeople in the US had many more concerns related to infrastructure and\nregulation issues. These findings help us understand the trends and challenges\nand prioritize different aspects in micro-mobility to improve their safety and\nexperience across the two areas for designing a more sustainable micro-mobility\nfuture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avetisyan_L/0/1/0/all/0/1\">Lilit Avetisyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Sue Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pari_E/0/1/0/all/0/1\">Ehsan Moradi Pari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fred Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Shan Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empathetic Dialogue Generation via Sensitive Emotion Recognition and Sensible Knowledge Selection. (arXiv:2210.11715v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11715","description":"<p>Empathy, which is widely used in psychological counselling, is a key trait of\neveryday human conversations. Equipped with commonsense knowledge, current\napproaches to empathetic response generation focus on capturing implicit\nemotion within dialogue context, where the emotions are treated as a static\nvariable throughout the conversations. However, emotions change dynamically\nbetween utterances, which makes previous works difficult to perceive the\nemotion flow and predict the correct emotion of the target response, leading to\ninappropriate response. Furthermore, simply importing commonsense knowledge\nwithout harmonization may trigger the conflicts between knowledge and emotion,\nwhich confuse the model to choose incorrect information to guide the generation\nprocess. To address the above problems, we propose a Serial Encoding and\nEmotion-Knowledge interaction (SEEK) method for empathetic dialogue generation.\nWe use a fine-grained encoding strategy which is more sensitive to the emotion\ndynamics (emotion flow) in the conversations to predict the emotion-intent\ncharacteristic of response. Besides, we design a novel framework to model the\ninteraction between knowledge and emotion to generate more sensible response.\nExtensive experiments on EmpatheticDialogues demonstrate that SEEK outperforms\nthe strong baselines in both automatic and manual evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenxu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCSCSet: A Specialist-annotated Dataset for Medical-domain Chinese Spelling Correction. (arXiv:2210.11720v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11720","description":"<p>Chinese Spelling Correction (CSC) is gaining increasing attention due to its\npromise of automatically detecting and correcting spelling errors in Chinese\ntexts. Despite its extensive use in many applications, like search engines and\noptical character recognition systems, little has been explored in medical\nscenarios in which complex and uncommon medical entities are easily misspelled.\nCorrecting the misspellings of medical entities is arguably more difficult than\nthose in the open domain due to its requirements of specificdomain knowledge.\nIn this work, we define the task of Medical-domain Chinese Spelling Correction\nand propose MCSCSet, a large scale specialist-annotated dataset that contains\nabout 200k samples. In contrast to the existing open-domain CSC datasets,\nMCSCSet involves: i) extensive real-world medical queries collected from\nTencent Yidian, ii) corresponding misspelled sentences manually annotated by\nmedical specialists. To ensure automated dataset curation, MCSCSet further\noffers a medical confusion set consisting of the commonly misspelled characters\nof given Chinese medical terms. This enables one to create the medical\nmisspelling dataset automatically. Extensive empirical studies have shown\nsignificant performance gaps between the open-domain and medical-domain\nspelling correction, highlighting the need to develop high-quality datasets\nthat allow for Chinese spelling correction in specific domains. Moreover, our\nwork benchmarks several representative Chinese spelling correction models,\nestablishing baselines for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wangjie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zhihao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zijing Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianguang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploration of Data Efficiency in Intra-Dataset Task Transfer for Dialog Understanding. (arXiv:2210.11729v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11729","description":"<p>Transfer learning is an exciting area of Natural Language Processing that has\nthe potential to both improve model performance and increase data efficiency.\nThis study explores the effects of varying quantities of target task training\ndata on sequential transfer learning in the dialog domain. We hypothesize that\na model can utilize the information learned from a source task to better learn\na target task, thereby reducing the number of target task training samples\nrequired. Unintuitively, our data shows that often target task training data\nsize has minimal effect on how sequential transfer learning performs compared\nto the same model without transfer learning. Our results lead us to believe\nthat this unexpected result could be due to the effects of catastrophic\nforgetting, motivating further work into methods that prevent such forgetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ross_J/0/1/0/all/0/1\">Josiah Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoffe_L/0/1/0/all/0/1\">Luke Yoffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AfroLID: A Neural Language Identification Tool for African Languages. (arXiv:2210.11744v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11744","description":"<p>Language identification (LID) is a crucial precursor for NLP, especially for\nmining web data. Problematically, most of the world's $7000$+ languages today\nare not covered by LID technologies. We address this pressing issue for Africa\nby introducing~\\ourLID, a neural LID toolkit for $517$ African languages and\nvarieties.~\\ourLID~exploits a multi-domain web dataset manually curated from\nacross $14$ language families utilizing five orthographic systems. When\nevaluated on our blind Test set,~\\ourLID~achieves $95.89$ $F_1$-score. We also\ncompare~\\ourLID~to five existing LID tools that each cover a small number of\nAfrican languages, finding it to outperform them on most languages. We further\nshow the utility of~\\ourLID~in the wild by testing it on the acutely\nunder-served Twitter domain. Finally, we offer a number of controlled case\nstudies and perform a linguistically-motivated error analysis that allow us to\nboth showcase~\\ourLID's powerful capabilities and limitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adebara_I/0/1/0/all/0/1\">Ife Adebara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inciarte_A/0/1/0/all/0/1\">Alcides Alcoba Inciarte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransLIST: A Transformer-Based Linguistically Informed Sanskrit Tokenizer. (arXiv:2210.11753v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11753","description":"<p>Sanskrit Word Segmentation (SWS) is essential in making digitized texts\navailable and in deploying downstream tasks. It is, however, non-trivial\nbecause of the sandhi phenomenon that modifies the characters at the word\nboundaries, and needs special treatment. Existing lexicon driven approaches for\nSWS make use of Sanskrit Heritage Reader, a lexicon-driven shallow parser, to\ngenerate the complete candidate solution space, over which various methods are\napplied to produce the most valid solution. However, these approaches fail\nwhile encountering out-of-vocabulary tokens. On the other hand, purely\nengineering methods for SWS have made use of recent advances in deep learning,\nbut cannot make use of the latent word information on availability.\n</p>\n<p>To mitigate the shortcomings of both families of approaches, we propose\nTransformer based Linguistically Informed Sanskrit Tokenizer (TransLIST)\nconsisting of (1) a module that encodes the character input along with\nlatent-word information, which takes into account the sandhi phenomenon\nspecific to SWS and is apt to work with partial or no candidate solutions, (2)\na novel soft-masked attention to prioritize potential candidate words and (3) a\nnovel path ranking algorithm to rectify the corrupted predictions. Experiments\non the benchmark datasets for SWS show that TransLIST outperforms the current\nstate-of-the-art system by an average 7.2 points absolute gain in terms of\nperfect match (PM) metric. The codebase and datasets are publicly available at\nhttps://github.com/rsingha108/TransLIST\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sandhan_J/0/1/0/all/0/1\">Jivnesh Sandhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singha_R/0/1/0/all/0/1\">Rathin Singha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_N/0/1/0/all/0/1\">Narein Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samanta_S/0/1/0/all/0/1\">Suvendu Samanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behera_L/0/1/0/all/0/1\">Laxmidhar Behera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"University of Cape Town's WMT22 System: Multilingual Machine Translation for Southern African Languages. (arXiv:2210.11757v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11757","description":"<p>The paper describes the University of Cape Town's submission to the\nconstrained track of the WMT22 Shared Task: Large-Scale Machine Translation\nEvaluation for African Languages. Our system is a single multilingual\ntranslation model that translates between English and 8 South / South East\nAfrican Languages, as well as between specific pairs of the African languages.\nWe used several techniques suited for low-resource machine translation (MT),\nincluding overlap BPE, back-translation, synthetic training data generation,\nand adding more translation directions during training. Our results show the\nvalue of these techniques, especially for directions where very little or no\nbilingual training data is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elmadani_K/0/1/0/all/0/1\">Khalid N. Elmadani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_F/0/1/0/all/0/1\">Francois Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buys_J/0/1/0/all/0/1\">Jan Buys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntax-guided Localized Self-attention by Constituency Syntactic Distance. (arXiv:2210.11759v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11759","description":"<p>Recent works have revealed that Transformers are implicitly learning the\nsyntactic information in its lower layers from data, albeit is highly dependent\non the quality and scale of the training data. However, learning syntactic\ninformation from data is not necessary if we can leverage an external syntactic\nparser, which provides better parsing quality with well-defined syntactic\nstructures. This could potentially improve Transformer's performance and sample\nefficiency. In this work, we propose a syntax-guided localized self-attention\nfor Transformer that allows directly incorporating grammar structures from an\nexternal constituency parser. It prohibits the attention mechanism to\noverweight the grammatically distant tokens over close ones. Experimental\nresults show that our model could consistently improve translation performance\non a variety of machine translation datasets, ranging from small to large\ndataset sizes, and with different source languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Shengyuan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kai_J/0/1/0/all/0/1\">Jushi Kai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Haotian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bingyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Longtao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Unintended Social Bias in Toxic Language Datasets. (arXiv:2210.11762v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11762","description":"<p>With the rise of online hate speech, automatic detection of Hate Speech,\nOffensive texts as a natural language processing task is getting popular.\nHowever, very little research has been done to detect unintended social bias\nfrom these toxic language datasets. This paper introduces a new dataset\nToxicBias curated from the existing dataset of Kaggle competition named \"Jigsaw\nUnintended Bias in Toxicity Classification\". We aim to detect social biases,\ntheir categories, and targeted groups. The dataset contains instances annotated\nfor five different bias categories, viz., gender, race/ethnicity, religion,\npolitical, and LGBTQ. We train transformer-based models using our curated\ndatasets and report baseline performance for bias identification, target\ngeneration, and bias implications. Model biases and their mitigation are also\ndiscussed in detail. Our study motivates a systematic extraction of social bias\ndata from toxic language datasets. All the codes and dataset used for\nexperiments in this work are publicly available\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_N/0/1/0/all/0/1\">Nihar Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CEFR-Based Sentence Difficulty Annotation and Assessment. (arXiv:2210.11766v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11766","description":"<p>Controllable text simplification is a crucial assistive technique for\nlanguage learning and teaching. One of the primary factors hindering its\nadvancement is the lack of a corpus annotated with sentence difficulty levels\nbased on language ability descriptions. To address this problem, we created the\nCEFR-based Sentence Profile (CEFR-SP) corpus, containing 17k English sentences\nannotated with the levels based on the Common European Framework of Reference\nfor Languages assigned by English-education professionals. In addition, we\npropose a sentence-level assessment model to handle unbalanced level\ndistribution because the most basic and highly proficient sentences are\nnaturally scarce. In the experiments in this study, our method achieved a\nmacro-F1 score of 84.5% in the level assessment, thus outperforming strong\nbaselines employed in readability assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arase_Y/0/1/0/all/0/1\">Yuki Arase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Satoru Uchida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajiwara_T/0/1/0/all/0/1\">Tomoyuki Kajiwara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmentation with Projection: Towards an Effective and Efficient Data Augmentation Paradigm for Distillation. (arXiv:2210.11768v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11768","description":"<p>Knowledge distillation is one of the primary methods of transferring\nknowledge from large to small models. However, it requires massive\ntask-specific data, which may not be plausible in many real-world applications.\nData augmentation methods such as representation interpolation, token\nreplacement, or augmentation with models are applied to tackle this problem.\nHowever, these data augmentation methods either potentially cause shifts in\ndecision boundaries (representation interpolation), are not expressive enough\n(token replacement), or introduce too much computational overhead (augmentation\nwith models). To this end, we propose AugPro (Augmentation with Projection), an\neffective and efficient data augmentation method for distillation. Our method\nbuilds on top of representation interpolation augmentation methods to maintain\nthe diversity of expressions and converts the augmented data to tokens to avoid\nshifting decision boundaries. It uses simple operations that come with little\ncomputational overhead. The results on multiple GLUE tasks show that our\nmethods can improve distillation performance by a large margin at a low time\ncost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuexin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daogao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InforMask: Unsupervised Informative Masking for Language Model Pretraining. (arXiv:2210.11771v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11771","description":"<p>Masked language modeling is widely used for pretraining large language models\nfor natural language understanding (NLU). However, random masking is\nsuboptimal, allocating an equal masking rate for all tokens. In this paper, we\npropose InforMask, a new unsupervised masking strategy for training masked\nlanguage models. InforMask exploits Pointwise Mutual Information (PMI) to\nselect the most informative tokens to mask. We further propose two\noptimizations for InforMask to improve its efficiency. With a one-off\npreprocessing step, InforMask outperforms random masking and previously\nproposed masking strategies on the factual recall benchmark LAMA and the\nquestion answering benchmark SQuAD v1 and v2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadeq_N/0/1/0/all/0/1\">Nafis Sadeq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval. (arXiv:2210.11773v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11773","description":"<p>Sampling proper negatives from a large document pool is vital to effectively\ntrain a dense retrieval model. However, existing negative sampling strategies\nsuffer from the uninformative or false negative problem. In this work, we\nempirically show that according to the measured relevance scores, the negatives\nranked around the positives are generally more informative and less likely to\nbe false negatives. Intuitively, these negatives are not too hard (\\emph{may be\nfalse negatives}) or too easy (\\emph{uninformative}). They are the ambiguous\nnegatives and need more attention during training. Thus, we propose a simple\nambiguous negatives sampling method, SimANS, which incorporates a new sampling\nprobability distribution to focusing on sampling more ambiguous negatives.\nExtensive experiments on four public and one industry datasets show the\neffectiveness of our approach. Our code and data are publicly available at the\nlink: \\url{https://github.com/microsoft/SimXNS}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_A/0/1/0/all/0/1\">Anlei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_R/0/1/0/all/0/1\">Rangan Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing and Evaluating Faithfulness in Dialogue Summarization. (arXiv:2210.11777v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11777","description":"<p>Dialogue summarization is abstractive in nature, making it suffer from\nfactual errors. The factual correctness of summaries has the highest priority\nbefore practical applications. Many efforts have been made to improve\nfaithfulness in text summarization. However, there is a lack of systematic\nstudy on dialogue summarization systems. In this work, we first perform the\nfine-grained human analysis on the faithfulness of dialogue summaries and\nobserve that over 35% of generated summaries are faithfully inconsistent\nrespective the source dialogues. Furthermore, we present a new model-level\nfaithfulness evaluation method. It examines generation models with multi-choice\nquestions created by rule-based transformations. Experimental results show that\nour evaluation schema is a strong proxy for the factual correctness of\nsummarization models. The human-annotated faithfulness samples and the\nevaluation toolkit are released to facilitate future research toward faithful\ndialogue summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Document-level Temporal Structures for Building Temporal Dependency Graphs. (arXiv:2210.11787v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11787","description":"<p>We propose to leverage news discourse profiling to model document-level\ntemporal structures for building temporal dependency graphs. Our key\nobservation is that the functional roles of sentences used for profiling news\ndiscourse signify different time frames relevant to a news story and can,\ntherefore, help to recover the global temporal structure of a document. Our\nanalyses and experiments with the widely used knowledge distillation technique\nshow that discourse profiling effectively identifies distant inter-sentence\nevent and (or) time expression pairs that are temporally related and otherwise\ndifficult to locate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choubey_P/0/1/0/all/0/1\">Prafulla Kumar Choubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ruihong Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences. (arXiv:2210.11794v1 [cs.LG])","link":"http://arxiv.org/abs/2210.11794","description":"<p>Efficient Transformers have been developed for long sequence modeling, due to\ntheir subquadratic memory and time complexity. Sparse Transformer is a popular\napproach to improving the efficiency of Transformers by restricting\nself-attention to locations specified by the predefined sparse patterns.\nHowever, leveraging sparsity may sacrifice expressiveness compared to\nfull-attention, when important token correlations are multiple hops away. To\ncombine advantages of both the efficiency of sparse transformer and the\nexpressiveness of full-attention Transformer, we propose \\textit{Diffuser}, a\nnew state-of-the-art efficient Transformer. Diffuser incorporates all token\ninteractions within one attention layer while maintaining low computation and\nmemory costs. The key idea is to expand the receptive field of sparse attention\nusing Attention Diffusion, which computes multi-hop token correlations based on\nall paths between corresponding disconnected tokens, besides attention among\nneighboring tokens. Theoretically, we show the expressiveness of Diffuser as a\nuniversal sequence approximator for sequence-to-sequence modeling, and\ninvestigate its ability to approximate full-attention by analyzing the graph\nexpander property from the spectral perspective. Experimentally, we investigate\nthe effectiveness of Diffuser with extensive evaluations, including language\nmodeling, image modeling, and Long Range Arena (LRA). Evaluation results show\nthat Diffuser achieves improvements by an average of 0.94% on text\nclassification tasks and 2.30% on LRA, with 1.67$\\times$ memory savings\ncompared to state-of-the-art benchmarks, which demonstrates superior\nperformance of Diffuser in both expressiveness and efficiency aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1\">Aosong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rescue Implicit and Long-tail Cases: Nearest Neighbor Relation Extraction. (arXiv:2210.11800v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11800","description":"<p>Relation extraction (RE) has achieved remarkable progress with the help of\npre-trained language models. However, existing RE models are usually incapable\nof handling two situations: implicit expressions and long-tail relation types,\ncaused by language complexity and data sparsity. In this paper, we introduce a\nsimple enhancement of RE using $k$ nearest neighbors ($k$NN-RE). $k$NN-RE\nallows the model to consult training relations at test time through a\nnearest-neighbor search and provides a simple yet effective means to tackle the\ntwo issues above. Additionally, we observe that $k$NN-RE serves as an effective\nway to leverage distant supervision (DS) data for RE. Experimental results show\nthat the proposed $k$NN-RE achieves state-of-the-art performances on a variety\nof supervised RE datasets, i.e., ACE05, SciERC, and Wiki80, along with\noutperforming the best model to date on the i2b2 and Wiki80 datasets in the\nsetting of allowing using DS. Our code and models are available at:\nhttps://github.com/YukinoWan/kNN-RE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhuoyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Checkpoint Averaging for Neural Machine Translation. (arXiv:2210.11803v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11803","description":"<p>Checkpoint averaging is a simple and effective method to boost the\nperformance of converged neural machine translation models. The calculation is\ncheap to perform and the fact that the translation improvement almost comes for\nfree, makes it widely adopted in neural machine translation research. Despite\nthe popularity, the method itself simply takes the mean of the model parameters\nfrom several checkpoints, the selection of which is mostly based on empirical\nrecipes without many justifications. In this work, we revisit the concept of\ncheckpoint averaging and consider several extensions. Specifically, we\nexperiment with ideas such as using different checkpoint selection strategies,\ncalculating weighted average instead of simple mean, making use of gradient\ninformation and fine-tuning the interpolation weights on development data. Our\nresults confirm the necessity of applying checkpoint averaging for optimal\nperformance, but also suggest that the landscape between the converged\ncheckpoints is rather flat and not much further improvement compared to simple\naveraging is to be obtained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yingbo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herold_C/0/1/0/all/0/1\">Christian Herold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zijian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering New Intents Using Latent Variables. (arXiv:2210.11804v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11804","description":"<p>Discovering new intents is of great significance to establishing Bootstrapped\nTask-Oriented Dialogue System. Most existing methods either lack the ability to\ntransfer prior knowledge in the known intent data or fall into the dilemma of\nforgetting prior knowledge in the follow-up. More importantly, these methods do\nnot deeply explore the intrinsic structure of unlabeled data, so they can not\nseek out the characteristics that make an intent in general. In this paper,\nstarting from the intuition that discovering intents could be beneficial to the\nidentification of the known intents, we propose a probabilistic framework for\ndiscovering intents where intent assignments are treated as latent variables.\nWe adopt Expectation Maximization framework for optimization. Specifically, In\nE-step, we conduct discovering intents and explore the intrinsic structure of\nunlabeled data by the posterior of intent assignments. In M-step, we alleviate\nthe forgetting of prior knowledge transferred from known intents by optimizing\nthe discrimination of labeled data. Extensive experiments conducted in three\nchallenging real-world datasets demonstrate our method can achieve substantial\nimprovements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yunhua Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiju Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+QIu_X/0/1/0/all/0/1\">Xipeng QIu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustifying Sentiment Classification by Maximally Exploiting Few Counterfactuals. (arXiv:2210.11805v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11805","description":"<p>For text classification tasks, finetuned language models perform remarkably\nwell. Yet, they tend to rely on spurious patterns in training data, thus\nlimiting their performance on out-of-distribution (OOD) test data. Among recent\nmodels aiming to avoid this spurious pattern problem, adding extra\ncounterfactual samples to the training data has proven to be very effective.\nYet, counterfactual data generation is costly since it relies on human\nannotation. Thus, we propose a novel solution that only requires annotation of\na small fraction (e.g., 1%) of the original training data, and uses automatic\ngeneration of extra counterfactuals in an encoding vector space. We demonstrate\nthe effectiveness of our approach in sentiment classification, using IMDb data\nfor training and other sets for OOD tests (i.e., Amazon, SemEval and Yelp). We\nachieve noticeable accuracy improvements by adding only 1% manual\ncounterfactuals: +3% compared to adding +100% in-distribution training samples,\n+1.3% compared to alternate counterfactual approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raedt_M/0/1/0/all/0/1\">Maarten De Raedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godin_F/0/1/0/all/0/1\">Fr&#xe9;deric Godin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Encoder-Decoder Redundant for Neural Machine Translation?. (arXiv:2210.11807v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11807","description":"<p>Encoder-decoder architecture is widely adopted for sequence-to-sequence\nmodeling tasks. For machine translation, despite the evolution from long\nshort-term memory networks to Transformer networks, plus the introduction and\ndevelopment of attention mechanism, encoder-decoder is still the de facto\nneural network architecture for state-of-the-art models. While the motivation\nfor decoding information from some hidden space is straightforward, the strict\nseparation of the encoding and decoding steps into an encoder and a decoder in\nthe model architecture is not necessarily a must. Compared to the task of\nautoregressive language modeling in the target language, machine translation\nsimply has an additional source sentence as context. Given the fact that neural\nlanguage models nowadays can already handle rather long contexts in the target\nlanguage, it is natural to ask whether simply concatenating the source and\ntarget sentences and training a language model to do translation would work. In\nthis work, we investigate the aforementioned concept for machine translation.\nSpecifically, we experiment with bilingual translation, translation with\nadditional target monolingual data, and multilingual translation. In all cases,\nthis alternative approach performs on par with the baseline encoder-decoder\nTransformer, suggesting that an encoder-decoder architecture might be redundant\nfor neural machine translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yingbo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herold_C/0/1/0/all/0/1\">Christian Herold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zijian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Textless Metric for Speech-to-Speech Comparison. (arXiv:2210.11835v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11835","description":"<p>This paper proposes a textless speech-to-speech comparison metric that allows\ncomparing a speech hypothesis with a speech reference without falling-back to\ntheir text transcripts. We leverage recently proposed speech2unit encoders\n(such as HuBERT) to pseudo-transcribe the speech utterances into discrete\nacoustic units and propose a simple neural architecture that learns a\nspeech-based metric which correlates well with its text-based counterpart. Such\na textless metric could ultimately be interesting for speech-to-speech\ntranslation evaluation (for oral languages or languages with no reliable ASR\nsystem available).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_S/0/1/0/all/0/1\">Swen Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galibert_O/0/1/0/all/0/1\">Olivier Galibert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calapodescu_I/0/1/0/all/0/1\">Ioan Calapodescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The use of the word \"\\{gamma}\\u{psion}{\\nu}{\\alpha}{\\iota}\\k{appa}{\\omicron}\\k{appa}{\\tau}{\\omicron}{\\nu}{\\iota}{\\alpha}\" (femicide) in Greek-speaking Twitter. (arXiv:2210.11837v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11837","description":"<p>Between 2019 and 2022, Greek media attention has been attracted by a rather\nunusually high number of femicide cases which have been trending for several\nweeks up to months in the public debate and one of the contributing factors is\nthe feedback loop between traditional media and social media. In this paper we\nare investigating the use of the term\n\"\\{gamma}\\u{psion}{\\nu}{\\alpha}{\\iota}\\k{appa}{\\omicron}\\k{appa}{\\tau}{\\omicron}{\\nu}{\\iota}{\\alpha}\"\n(femicide) in Greek speaking twitter. More specifically, we approach the\nproblem from a stance detection perspective, aiming to automatically identify\nuser position with regards to the feministic semantics of the word. We also\ndiscuss findings from an identity analysis perspective and intercorrelations\nwith hate speech that have been identified in the collected corpus of tweets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aggistrioti_A/0/1/0/all/0/1\">Aglaia Aggistrioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bambili_E/0/1/0/all/0/1\">Efstathia Bambili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkatzoli_N/0/1/0/all/0/1\">Nikoleta Gkatzoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontostavlaki_A/0/1/0/all/0/1\">Athina Kontostavlaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsounidi_I/0/1/0/all/0/1\">Ioanna Tsounidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perifanos_K/0/1/0/all/0/1\">Konstantinos Perifanos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral Probing. (arXiv:2210.11860v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11860","description":"<p>Linguistic information is encoded at varying timescales (subwords, phrases,\netc.) and communicative levels, such as syntax and semantics. Contextualized\nembeddings have analogously been found to capture these phenomena at\ndistinctive layers and frequencies. Leveraging these findings, we develop a\nfully learnable frequency filter to identify spectral profiles for any given\ntask. It enables vastly more granular analyses than prior handcrafted filters,\nand improves on efficiency. After demonstrating the informativeness of spectral\nprobing over manual filters in a monolingual setting, we investigate its\nmultilingual characteristics across seven diverse NLP tasks in six languages.\nOur analyses identify distinctive spectral profiles which quantify cross-task\nsimilarity in a linguistically intuitive manner, while remaining consistent\nacross languages-highlighting their potential as robust, lightweight task\ndescriptors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_Eberstein_M/0/1/0/all/0/1\">Max M&#xfc;ller-Eberstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goot_R/0/1/0/all/0/1\">Rob van der Goot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LittleBird: Efficient Faster & Longer Transformer for Question Answering. (arXiv:2210.11870v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11870","description":"<p>BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a\nlimitation dealing with long inputs due to its attention mechanism. Longformer,\nETC and BigBird addressed this issue and effectively solved the quadratic\ndependency problem. However we find that these models are not sufficient, and\npropose LittleBird, a novel model based on BigBird with improved speed and\nmemory footprint while maintaining accuracy. In particular, we devise a more\nflexible and efficient position representation method based on Attention with\nLinear Biases (ALiBi). We also show that replacing the method of global\ninformation represented in the BigBird with pack and unpack attention is more\neffective. The proposed model can work on long inputs even after being\npre-trained on short inputs, and can be trained efficiently reusing existing\npre-trained language model for short inputs. This is a significant benefit for\nlow-resource languages where large amounts of long text data are difficult to\nobtain. As a result, our experiments show that LittleBird works very well in a\nvariety of languages, achieving high performance in question answering tasks,\nparticularly in KorQuAD2.0, Korean Question Answering Dataset for long\nparagraphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minchul Lee</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kijong Han</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Myeong Cheol Shin</a> (1) ((1) Kakao Enterprise Corp.)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep LSTM Spoken Term Detection using Wav2Vec 2.0 Recognizer. (arXiv:2210.11885v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11885","description":"<p>In recent years, the standard hybrid DNN-HMM speech recognizers are\noutperformed by the end-to-end speech recognition systems. One of the very\npromising approaches is the grapheme Wav2Vec 2.0 model, which uses the\nself-supervised pretraining approach combined with transfer learning of the\nfine-tuned speech recognizer. Since it lacks the pronunciation vocabulary and\nlanguage model, the approach is suitable for tasks where obtaining such models\nis not easy or almost impossible.\n</p>\n<p>In this paper, we use the Wav2Vec speech recognizer in the task of spoken\nterm detection over a large set of spoken documents. The method employs a deep\nLSTM network which maps the recognized hypothesis and the searched term into a\nshared pronunciation embedding space in which the term occurrences and the\nassigned scores are easily computed.\n</p>\n<p>The paper describes a bootstrapping approach that allows the transfer of the\nknowledge contained in traditional pronunciation vocabulary of DNN-HMM hybrid\nASR into the context of grapheme-based Wav2Vec. The proposed method outperforms\nthe previously published system based on the combination of the DNN-HMM hybrid\nASR and phoneme recognizer by a large margin on the MALACH data in both English\nand Czech languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Svec_J/0/1/0/all/0/1\">Jan &#x160;vec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehecka_J/0/1/0/all/0/1\">Jan Lehe&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smidl_L/0/1/0/all/0/1\">Lubo&#x161; &#x160;m&#xed;dl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STAR: SQL Guided Pre-Training for Context-dependent Text-to-SQL Parsing. (arXiv:2210.11888v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11888","description":"<p>In this paper, we propose a novel SQL guided pre-training framework STAR for\ncontext-dependent text-to-SQL parsing, which leverages contextual information\nto enrich natural language (NL) utterance and table schema representations for\ntext-to-SQL conversations. Concretely, we propose two novel pre-training\nobjectives which respectively explore the context-dependent interactions of NL\nutterances and SQL queries within each text-to-SQL conversation: (i) schema\nstate tracking (SST) objective that tracks and explores the schema states of\ncontext-dependent SQL queries in the form of schema-states by predicting and\nupdating the value of each schema slot during interaction; (ii) utterance\ndependency tracking (UDT) objective that employs weighted contrastive learning\nto pull together two semantically similar NL utterances and push away the\nrepresentations of semantically dissimilar NL utterances within each\nconversation. In addition, we construct a high-quality large-scale\ncontext-dependent text-to-SQL conversation corpus to pre-train STAR. Extensive\nexperiments show that STAR achieves new state-of-the-art performance on two\ndownstream benchmarks (SParC and CoSQL), significantly outperforming previous\npre-training methods and ranking first on the leaderboard. We believe the\nrelease of the constructed corpus, codebase and pre-trained STAR checkpoints\nwould push forward the research in this area. For reproducibility, we release\nour code and data at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/star.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zefeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioLORD: Learning Ontological Representations from Definitions (for Biomedical Concepts and their Textual Descriptions). (arXiv:2210.11892v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11892","description":"<p>This work introduces BioLORD, a new pre-training strategy for producing\nmeaningful representations for clinical sentences and biomedical concepts.\nState-of-the-art methodologies operate by maximizing the similarity in\nrepresentation of names referring to the same concept, and preventing collapse\nthrough contrastive learning. However, because biomedical names are not always\nself-explanatory, it sometimes results in non-semantic representations. BioLORD\novercomes this issue by grounding its concept representations using\ndefinitions, as well as short descriptions derived from a multi-relational\nknowledge graph consisting of biomedical ontologies. Thanks to this grounding,\nour model produces more semantic concept representations that match more\nclosely the hierarchical structure of ontologies. BioLORD establishes a new\nstate of the art for text similarity on both clinical sentences (MedSTS) and\nbiomedical concepts (MayoSRS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Remy_F/0/1/0/all/0/1\">Fran&#xe7;ois Remy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demuynck_K/0/1/0/all/0/1\">Kris Demuynck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spoken Term Detection and Relevance Score Estimation using Dot-Product of Pronunciation Embeddings. (arXiv:2210.11895v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11895","description":"<p>The paper describes a novel approach to Spoken Term Detection (STD) in large\nspoken archives using deep LSTM networks. The work is based on the previous\napproach of using Siamese neural networks for STD and naturally extends it to\ndirectly localize a spoken term and estimate its relevance score. The phoneme\nconfusion network generated by a phoneme recognizer is processed by the deep\nLSTM network which projects each segment of the confusion network into an\nembedding space. The searched term is projected into the same embedding space\nusing another deep LSTM network. The relevance score is then computed using a\nsimple dot-product in the embedding space and calibrated using a sigmoid\nfunction to predict the probability of occurrence. The location of the searched\nterm is then estimated from the sequence of output probabilities. The deep LSTM\nnetworks are trained in a self-supervised manner from paired recognition\nhypotheses on word and phoneme levels. The method is experimentally evaluated\non MALACH data in English and Czech languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Svec_J/0/1/0/all/0/1\">Jan &#x160;vec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smidl_L/0/1/0/all/0/1\">Lubo&#x161; &#x160;m&#xed;dl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psutka_J/0/1/0/all/0/1\">Josef V. Psutka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prazak_A/0/1/0/all/0/1\">Ale&#x161; Pra&#x17e;&#xe1;k</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Semi-supervised Approach for a Better Translation of Sentiment in Dialectical Arabic UGT. (arXiv:2210.11899v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11899","description":"<p>In the online world, Machine Translation (MT) systems are extensively used to\ntranslate User-Generated Text (UGT) such as reviews, tweets, and social media\nposts, where the main message is often the author's positive or negative\nattitude towards the topic of the text. However, MT systems still lack accuracy\nin some low-resource languages and sometimes make critical translation errors\nthat completely flip the sentiment polarity of the target word or phrase and\nhence delivers a wrong affect message. This is particularly noticeable in texts\nthat do not follow common lexico-grammatical standards such as the dialectical\nArabic (DA) used on online platforms. In this research, we aim to improve the\ntranslation of sentiment in UGT written in the dialectical versions of the\nArabic language to English. Given the scarcity of gold-standard parallel data\nfor DA-EN in the UGT domain, we introduce a semi-supervised approach that\nexploits both monolingual and parallel data for training an NMT system\ninitialised by a cross-lingual language model trained with supervised and\nunsupervised modeling objectives. We assess the accuracy of sentiment\ntranslation by our proposed system through a numerical 'sentiment-closeness'\nmeasure as well as human evaluation. We will show that our semi-supervised MT\nsystem can significantly help with correcting sentiment errors detected in the\nonline translation of dialectical Arabic UGT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1\">Constantin Orasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Emad Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tantawy_A/0/1/0/all/0/1\">Ashraf Tantawy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turning Fixed to Adaptive: Integrating Post-Evaluation into Simultaneous Machine Translation. (arXiv:2210.11900v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11900","description":"<p>Simultaneous machine translation (SiMT) starts its translation before reading\nthe whole source sentence and employs either fixed or adaptive policy to\ngenerate the target sentence. Compared to the fixed policy, the adaptive policy\nachieves better latency-quality tradeoffs by adopting a flexible translation\npolicy. If the policy can evaluate rationality before taking action, the\nprobability of incorrect actions will also decrease. However, previous methods\nlack evaluation of actions before taking them. In this paper, we propose a\nmethod of performing the adaptive policy via integrating post-evaluation into\nthe fixed policy. Specifically, whenever a candidate token is generated, our\nmodel will evaluate the rationality of the next action by measuring the change\nin the source content. Our model will then take different actions based on the\nevaluation results. Experiments on three translation tasks show that our method\ncan exceed strong baselines under all latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shoutao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploration of the Usage of Color Terms by Color-blind Participants in Online Discussion Platforms. (arXiv:2210.11905v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11905","description":"<p>Prominent questions about the role of sensory vs. linguistic input in the way\nwe acquire and use language have been extensively studied in the\npsycholinguistic literature. However, the relative effect of various factors in\na person's overall experience on their linguistic system remains unclear. We\nstudy this question by making a step forward towards a better understanding of\nthe conceptual perception of colors by color-blind individuals, as reflected in\ntheir spontaneous linguistic productions. Using a novel and carefully curated\ndataset, we show that red-green color-blind speakers use the \"red\" and \"green\"\ncolor terms in less predictable contexts, and in linguistic environments\nevoking mental image to a lower extent, when compared to their normal-sighted\ncounterparts. These findings shed some new and interesting light on the role of\nsensory experience on our linguistic system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabinovich_E/0/1/0/all/0/1\">Ella Rabinovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmeli_B/0/1/0/all/0/1\">Boaz Carmeli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$m^4Adapter$: Multilingual Multi-Domain Adaptation for Machine Translation with a Meta-Adapter. (arXiv:2210.11912v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11912","description":"<p>Multilingual neural machine translation models (MNMT) yield state-of-the-art\nperformance when evaluated on data from a domain and language pair seen at\ntraining time. However, when a MNMT model is used to translate under domain\nshift or to a new language pair, performance drops dramatically. We consider a\nvery challenging scenario: adapting the MNMT model both to a new domain and to\na new language pair at the same time. In this paper, we propose $m^4Adapter$\n(Multilingual Multi-Domain Adaptation for Machine Translation with a\nMeta-Adapter), which combines domain and language knowledge using meta-learning\nwith adapters. We present results showing that our approach is a\nparameter-efficient solution which effectively adapts a model to both a new\nlanguage pair and a new domain, while outperforming other adapter methods. An\nablation study also shows that our approach more effectively transfers domain\nknowledge across different languages and language information across different\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1\">Wen Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chronopoulou_A/0/1/0/all/0/1\">Alexandra Chronopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NEREL-BIO: A Dataset of Biomedical Abstracts Annotated with Nested Named Entities. (arXiv:2210.11913v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11913","description":"<p>This paper describes NEREL-BIO -- an annotation scheme and corpus of PubMed\nabstracts in Russian and smaller number of abstracts in English. NEREL-BIO\nextends the general domain dataset NEREL by introducing domain-specific entity\ntypes. NEREL-BIO annotation scheme covers both general and biomedical domains\nmaking it suitable for domain transfer experiments. NEREL-BIO provides\nannotation for nested named entities as an extension of the scheme employed for\nNEREL. Nested named entities may cross entity boundaries to connect to shorter\nentities nested within longer entities, making them harder to detect.\n</p>\n<p>NEREL-BIO contains annotations for 700+ Russian and 100+ English abstracts.\nAll English PubMed annotations have corresponding Russian counterparts. Thus,\nNEREL-BIO comprises the following specific features: annotation of nested named\nentities, it can be used as a benchmark for cross-domain (NEREL -&gt; NEREL-BIO)\nand cross-language (English -&gt; Russian) transfer. We experiment with both\ntransformer-based sequence models and machine reading comprehension (MRC)\nmodels and report their results.\n</p>\n<p>The dataset is freely available at https://github.com/nerel-ds/NEREL-BIO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loukachevitch_N/0/1/0/all/0/1\">Natalia Loukachevitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manandhar_S/0/1/0/all/0/1\">Suresh Manandhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_E/0/1/0/all/0/1\">Elina Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozhkov_I/0/1/0/all/0/1\">Igor Rozhkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braslavski_P/0/1/0/all/0/1\">Pavel Braslavski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_V/0/1/0/all/0/1\">Vladimir Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batura_T/0/1/0/all/0/1\">Tatiana Batura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutubalina_E/0/1/0/all/0/1\">Elena Tutubalina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling. (arXiv:2210.11929v1 [cs.CV])","link":"http://arxiv.org/abs/2210.11929","description":"<p>Recent large-scale video-language pre-trained models have shown appealing\nperformance on various downstream tasks. However, the pre-training process is\ncomputationally expensive due to the requirement of millions of video-text\npairs and the redundant data structure of each video. To mitigate these\nproblems, we propose LiteVL, which adapts a pre-trained image-language model\nBLIP into a video-text model directly on downstream tasks, without heavy\npre-training. To enhance the temporal modeling lacking in the image-language\nmodel, we propose to add temporal attention modules in the image encoder of\nBLIP with dynamic temporal scaling. Besides the model-wise adaptation, we also\npropose a non-parametric pooling mechanism to adaptively reweight the\nfine-grained video embedding conditioned on the text. Experimental results on\ntext-video retrieval and video question answering show that the proposed LiteVL\neven outperforms previous video-language pre-trained models by a clear margin,\nthough without any video-language pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongsheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chaofan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing over Long Tail Concepts for Medical Term Normalization. (arXiv:2210.11947v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11947","description":"<p>Medical term normalization consists in mapping a piece of text to a large\nnumber of output classes. Given the small size of the annotated datasets and\nthe extremely long tail distribution of the concepts, it is of utmost\nimportance to develop models that are capable to generalize to scarce or unseen\nconcepts. An important attribute of most target ontologies is their\nhierarchical structure. In this paper we introduce a simple and effective\nlearning strategy that leverages such information to enhance the\ngeneralizability of both discriminative and generative models. The evaluation\nshows that the proposed strategy produces state-of-the-art performance on seen\nconcepts and consistent improvements on unseen ones, allowing also for\nefficient zero-shot knowledge transfer across text typologies and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Portelli_B/0/1/0/all/0/1\">Beatrice Portelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaboro_S/0/1/0/all/0/1\">Simone Scaboro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santus_E/0/1/0/all/0/1\">Enrico Santus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedghamiz_H/0/1/0/all/0/1\">Hooman Sedghamiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1\">Emmanuele Chersoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1\">Giuseppe Serra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Entity Detection and Injection for Direct Speech Translation. (arXiv:2210.11981v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11981","description":"<p>In a sentence, certain words are critical for its semantic. Among them, named\nentities (NEs) are notoriously challenging for neural models. Despite their\nimportance, their accurate handling has been neglected in speech-to-text (S2T)\ntranslation research, and recent work has shown that S2T models perform poorly\nfor locations and notably person names, whose spelling is challenging unless\nknown in advance. In this work, we explore how to leverage dictionaries of NEs\nknown to likely appear in a given context to improve S2T model outputs. Our\nexperiments show that we can reliably detect NEs likely present in an utterance\nstarting from S2T encoder outputs. Indeed, we demonstrate that the current\ndetection quality is sufficient to improve NE accuracy in the translation with\na 31% reduction in person name errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_I/0/1/0/all/0/1\">Ilia Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rongqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shift-Reduce Task-Oriented Semantic Parsing with Stack-Transformers. (arXiv:2210.11984v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11984","description":"<p>Intelligent voice assistants, such as Apple Siri and Amazon Alexa, are widely\nused nowadays. These task-oriented dialog systems require a semantic parsing\nmodule in order to process user utterances and understand the action to be\nperformed. This semantic parsing component was initially implemented by\nrule-based or statistical slot-filling approaches for processing simple\nqueries; however, the appearance of more complex utterances demanded the\napplication of shift-reduce parsers or sequence-to-sequence models. While\nshift-reduce approaches initially demonstrated to be the best option, recent\nefforts on sequence-to-sequence systems pushed them to become the\nhighest-performing method for that task. In this article, we advance the\nresearch on shift-reduce semantic parsing for task-oriented dialog. In\nparticular, we implement novel shift-reduce parsers that rely on\nStack-Transformers. These allow to adequately model transition systems on the\ncutting-edge Transformer architecture, notably boosting shift-reduce parsing\nperformance. Additionally, we adapt alternative transition systems from\nconstituency parsing to task-oriented parsing, and empirically prove that the\nin-order algorithm substantially outperforms the commonly-used top-down\nstrategy. Finally, we extensively test our approach on multiple domains from\nthe Facebook TOP benchmark, improving over existing shift-reduce parsers and\nstate-of-the-art sequence-to-sequence models in both high-resource and\nlow-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1\">Daniel Fern&#xe1;ndez-Gonz&#xe1;lez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Speech Translation and Named Entity Recognition. (arXiv:2210.11987v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11987","description":"<p>Modern automatic translation systems aim at place the human at the center by\nproviding contextual support and knowledge. In this context, a critical task is\nenriching the output with information regarding the mentioned entities, which\nis currently achieved processing the generated translation with named entity\nrecognition (NER) and entity linking systems. In light of the recent promising\nresults shown by direct speech translation (ST) models and the known weaknesses\nof cascades (error propagation and additional latency), in this paper we\npropose multitask models that jointly perform ST and NER, and compare them with\na cascade baseline. The experimental results show that our models significantly\noutperform the cascade on the NER task (by 0.4-1.0 F1), without degradation in\nterms of translation quality, and with the same computational efficiency of a\nplain direct ST model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing text representations to capture (dis)similarity between political parties. (arXiv:2210.11989v1 [cs.CL])","link":"http://arxiv.org/abs/2210.11989","description":"<p>Even though fine-tuned neural language models have been pivotal in enabling\n\"deep\" automatic text analysis, optimizing text representations for specific\napplications remains a crucial bottleneck. In this study, we look at this\nproblem in the context of a task from computational social science, namely\nmodeling pairwise similarities between political parties. Our research question\nis what level of structural information is necessary to create robust text\nrepresentation, contrasting a strongly informed approach (which uses both claim\nspan and claim category annotations) with approaches that forgo one or both\ntypes of annotation with document structure-based heuristics. Evaluating our\nmodels on the manifestos of German parties for the 2021 federal election. We\nfind that heuristics that maximize within-party over between-party similarity\nalong with a normalization step lead to reliable party similarity prediction,\nwithout the need for manual annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ceron_T/0/1/0/all/0/1\">Tanise Ceron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blokker_N/0/1/0/all/0/1\">Nico Blokker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Performance-Efficiency Trade-Offs in Adapting Language Models to Text Classification Tasks. (arXiv:2210.12022v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12022","description":"<p>Pre-trained language models (LMs) obtain state-of-the-art performance when\nadapted to text classification tasks. However, when using such models in\nreal-world applications, efficiency considerations are paramount. In this\npaper, we study how different training procedures that adapt LMs to text\nclassification perform, as we vary model and train set size. More specifically,\nwe compare standard fine-tuning, prompting, and knowledge distillation (KD)\nwhen the teacher was trained with either fine-tuning or prompting. Our findings\nsuggest that even though fine-tuning and prompting work well to train large LMs\non large train sets, there are more efficient alternatives that can reduce\ncompute or data cost. Interestingly, we find that prompting combined with KD\ncan reduce compute and data cost at the same time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aina_L/0/1/0/all/0/1\">Laura Aina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voskarides_N/0/1/0/all/0/1\">Nikos Voskarides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_R/0/1/0/all/0/1\">Roi Blanco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models. (arXiv:2210.12023v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12023","description":"<p>We have recently witnessed a number of impressive results on hard\nmathematical reasoning problems with language models. At the same time, the\nrobustness of these models has also been called into question; recent works\nhave shown that models can rely on shallow patterns in the problem description\nwhen predicting a solution. Building on the idea of behavioral testing, we\npropose a novel framework, which pins down the causal effect of various factors\nin the input, e.g., the surface form of the problem text, the operands and math\noperators on the output solution. By grounding the behavioral analysis in a\ncausal graph describing an intuitive reasoning process, we study the behavior\nof language models in terms of robustness and sensitivity to direct\ninterventions in the input space. We apply our framework on a test bed of\nbivariate math word problems. Our analysis shows that robustness does not\nappear to continuously improve as a function of scale, but that the recent LLM,\nGPT-3-Instruct (175B), achieves a dramatic improvement in both robustness and\nsensitivity, compared to all other GPT variants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stolfo_A/0/1/0/all/0/1\">Alessandro Stolfo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1\">Kumar Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bootstrapping NLP tools across low-resourced African languages: an overview and prospects. (arXiv:2210.12027v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12027","description":"<p>Computing and Internet access are substantially growing markets in Southern\nAfrica, which brings with it increasing demands for local content and tools in\nindigenous African languages. Since most of those languages are low-resourced,\nefforts have gone into the notion of bootstrapping tools for one African\nlanguage from another. This paper provides an overview of these efforts for\nNiger-Congo B (`Bantu') languages. Bootstrapping grammars for geographically\ndistant languages has been shown to still have positive outcomes for morphology\nand rules or grammar-based natural language generation. Bootstrapping with\ndata-driven approaches to NLP tasks is difficult to use meaningfully regardless\ngeographic proximity, which is largely due to lexical diversity due to both\northography and vocabulary. Cladistic approaches in comparative linguistics may\ninform bootstrapping strategies and similarity measures might serve as proxy\nfor bootstrapping potential as well, with both fertile ground for further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keet_C/0/1/0/all/0/1\">C. Maria Keet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuro-Symbolic Causal Reasoning Meets Signaling Game for Emergent Semantic Communications. (arXiv:2210.12040v1 [cs.LG])","link":"http://arxiv.org/abs/2210.12040","description":"<p>Semantic communication (SC) aims to communicate reliably with minimal data\ntransfer while simultaneously providing seamless connectivity to heterogeneous\nservices and users. In this paper, a novel emergent SC (ESC) system framework\nis proposed and is composed of a signaling game for emergent language design\nand a neuro-symbolic (NeSy) artificial intelligence (AI) approach for causal\nreasoning. In order to design the language, the signaling game is solved using\nan alternating maximization between the communicating node's utilities. The\nemergent language helps create a context-aware transmit vocabulary (minimal\nsemantic representation) and aids the reasoning process (enabling\ngeneralization to unseen scenarios) by splitting complex messages into simpler\nreasoning tasks for the receiver. The causal description at the transmitter is\nthen modeled (a neural component) as a posterior distribution of the relevant\nattributes present in the data. Using the reconstructed causal state, the\nreceiver evaluates a set of logical formulas (symbolic part) to execute its\ntask. The nodes NeSy reasoning components are implemented by the recently\nproposed AI tool called Generative Flow Networks, and they are optimized for\nhigher semantic reliability. The ESC system is designed to enhance the novel\nmetrics of semantic information, reliability, distortion and similarity that\nare designed using rigorous algebraic properties from category theory thereby\ngeneralizing the metrics beyond Shannon's notion of uncertainty. Simulation\nresults validate the ability of ESC to communicate efficiently (with reduced\nbits) and achieve better semantic reliability than conventional wireless and\nstate-of-the-art systems that do not exploit causal reasoning capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1\">Christo Kurisummoottil Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1\">Walid Saad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clip-Tuning: Towards Derivative-free Prompt Learning with a Mixture of Rewards. (arXiv:2210.12050v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12050","description":"<p>Derivative-free prompt learning has emerged as a lightweight alternative to\nprompt tuning, which only requires model inference to optimize the prompts.\nHowever, existing work did not take full advantage of the over-parameterized\ncharacteristics of large pre-trained language models (PLMs). In this paper, we\npropose Clip-Tuning, a simple yet effective method that adopts diverse frozen\n\"thinned\" networks of PLMs to obtain a mixture of rewards and thus advance the\nderivative-free prompt learning. The thinned networks consist of all the hidden\nunits that survive a stationary dropout strategy, whose inference predictions\nreflect an ensemble of partial views over prompted training samples. Our method\noutperforms previous gradient-free prompt learning methods and achieves parity\nwith gradient-based counterparts on seven language understanding benchmarks\nunder few-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yekun Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Experiencer-Specific Emotion and Appraisal Prediction. (arXiv:2210.12078v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12078","description":"<p>Emotion classification in NLP assigns emotions to texts, such as sentences or\nparagraphs. With texts like \"I felt guilty when he cried\", focusing on the\nsentence level disregards the standpoint of each participant in the situation:\nthe writer (\"I\") and the other entity (\"he\") could in fact have different\naffective states. The emotions of different entities have been considered only\npartially in emotion semantic role labeling, a task that relates semantic roles\nto emotion cue words. Proposing a related task, we narrow the focus on the\nexperiencers of events, and assign an emotion (if any holds) to each of them.\nTo this end, we represent each emotion both categorically and with appraisal\nvariables, as a psychological access to explaining why a person develops a\nparticular emotion. On an event description corpus, our experiencer-aware\nmodels of emotions and appraisals outperform the experiencer-agnostic\nbaselines, showing that disregarding event participants is an\noversimplification for the emotion detection task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wegge_M/0/1/0/all/0/1\">Maximilian Wegge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troiano_E/0/1/0/all/0/1\">Enrica Troiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberlander_L/0/1/0/all/0/1\">Laura Oberl&#xe4;nder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Vision-and-Language Transformers Learn Grounded Predicate-Noun Dependencies?. (arXiv:2210.12079v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12079","description":"<p>Recent advances in vision-and-language modeling have seen the development of\nTransformer architectures that achieve remarkable performance on multimodal\nreasoning tasks. Yet, the exact capabilities of these black-box models are\nstill poorly understood. While much of previous work has focused on studying\ntheir ability to learn meaning at the word-level, their ability to track\nsyntactic dependencies between words has received less attention. We take a\nfirst step in closing this gap by creating a new multimodal task targeted at\nevaluating understanding of predicate-noun dependencies in a controlled setup.\nWe evaluate a range of state-of-the-art models and find that their performance\non the task varies considerably, with some models performing relatively well\nand others at chance level. In an effort to explain this variability, our\nanalyses indicate that the quality (and not only sheer quantity) of pretraining\ndata is essential. Additionally, the best performing models leverage\nfine-grained multimodal pretraining objectives in addition to the standard\nimage-text matching objectives. This study highlights that targeted and\ncontrolled evaluations are a crucial step for a precise and rigorous test of\nthe multimodal knowledge of vision-and-language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikolaus_M/0/1/0/all/0/1\">Mitja Nikolaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salin_E/0/1/0/all/0/1\">Emmanuelle Salin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayache_S/0/1/0/all/0/1\">Stephane Ayache</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fourtassi_A/0/1/0/all/0/1\">Abdellah Fourtassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favre_B/0/1/0/all/0/1\">Benoit Favre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoding a Neural Retriever's Latent Space for Query Suggestion. (arXiv:2210.12084v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12084","description":"<p>Neural retrieval models have superseded classic bag-of-words methods such as\nBM25 as the retrieval framework of choice. However, neural systems lack the\ninterpretability of bag-of-words models; it is not trivial to connect a query\nchange to a change in the latent space that ultimately determines the retrieval\nresults. To shed light on this embedding space, we learn a \"query decoder\"\nthat, given a latent representation of a neural search engine, generates the\ncorresponding query. We show that it is possible to decode a meaningful query\nfrom its latent representation and, when moving in the right direction in\nlatent space, to decode a query that retrieves the relevant paragraph. In\nparticular, the query decoder can be useful to understand \"what should have\nbeen asked\" to retrieve a particular paragraph from the collection. We employ\nthe query decoder to generate a large synthetic dataset of query reformulations\nfor MSMarco, leading to improved retrieval performance. On this data, we train\na pseudo-relevance feedback (PRF) T5 model for the application of query\nsuggestion that outperforms both query reformulation and PRF information\nretrieval baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1\">Leonard Adolphs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huebscher_M/0/1/0/all/0/1\">Michelle Chen Huebscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buck_C/0/1/0/all/0/1\">Christian Buck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girgin_S/0/1/0/all/0/1\">Sertan Girgin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1\">Olivier Bachem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciaramita_M/0/1/0/all/0/1\">Massimiliano Ciaramita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Multi-Turn Text-to-SQL Datasets with Self-Play. (arXiv:2210.12096v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12096","description":"<p>The task of context-dependent text-to-SQL aims to convert multi-turn user\nutterances to formal SQL queries. This is a challenging task due to both the\nscarcity of training data from which to learn complex contextual dependencies\nand to generalize to unseen databases. In this paper we explore augmenting the\ntraining datasets using self-play, which leverages contextual information to\nsynthesize new interactions to adapt the model to new databases. We first\ndesign a SQL-to-text model conditioned on a sampled goal query, which\nrepresents a user's intent, that then converses with a text-to-SQL semantic\nparser to generate new interactions. We then filter the synthesized\ninteractions and retrain the models with the augmented data. We find that\nself-play improves the accuracy of a strong baseline on SParC and CoSQL, two\nwidely used cross-domain text-to-SQL datasets. Our analysis shows that\nself-play simulates various conversational thematic relations, enhances\ncross-domain generalization and improves beam-search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zihuiwen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1\">Phil Blunsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linfeng Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Describing Sets of Images with Textual-PCA. (arXiv:2210.12112v1 [cs.CV])","link":"http://arxiv.org/abs/2210.12112","description":"<p>We seek to semantically describe a set of images, capturing both the\nattributes of single images and the variations within the set. Our procedure is\nanalogous to Principle Component Analysis, in which the role of projection\nvectors is replaced with generated phrases. First, a centroid phrase that has\nthe largest average semantic similarity to the images in the set is generated,\nwhere both the computation of the similarity and the generation are based on\npretrained vision-language models. Then, the phrase that generates the highest\nvariation among the similarity scores is generated, using the same models. The\nnext phrase maximizes the variance subject to being orthogonal, in the latent\nspace, to the highest-variance phrase, and the process continues. Our\nexperiments show that our method is able to convincingly capture the essence of\nimage sets and describe the individual elements in a semantically meaningful\nway within the context of the entire set. Our code is available at:\nhttps://github.com/OdedH/textual-pca.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hupert_O/0/1/0/all/0/1\">Oded Hupert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-to-Intent Using Acoustic-Textual Subword Representations from End-to-End ASR. (arXiv:2210.12134v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12134","description":"<p>Accurate prediction of the user intent to interact with a voice assistant\n(VA) on a device (e.g. on the phone) is critical for achieving naturalistic,\nengaging, and privacy-centric interactions with the VA. To this end, we present\na novel approach to predict the user's intent (the user speaking to the device\nor not) directly from acoustic and textual information encoded at subword\ntokens which are obtained via an end-to-end ASR model. Modeling directly the\nsubword tokens, compared to modeling of the phonemes and/or full words, has at\nleast two advantages: (i) it provides a unique vocabulary representation, where\neach token has a semantic meaning, in contrast to the phoneme-level\nrepresentations, (ii) each subword token has a reusable \"sub\"-word acoustic\npattern (that can be used to construct multiple full words), resulting in a\nlargely reduced vocabulary space than of the full words. To learn the subword\nrepresentations for the audio-to-intent classification, we extract: (i)\nacoustic information from an E2E-ASR model, which provides frame-level CTC\nposterior probabilities for the subword tokens, and (ii) textual information\nfrom a pre-trained continuous bag-of-words model capturing the semantic meaning\nof the subword tokens. The key to our approach is the way it combines acoustic\nsubword-level posteriors with text information using the notion of\npositional-encoding in order to account for multiple ASR hypotheses\nsimultaneously. We show that our approach provides more robust and richer\nrepresentations for audio-to-intent classification, and is highly accurate with\ncorrectly mitigating 93.3% of unintended user audio from invoking the smart\nassistant at 99% true positive rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dighe_P/0/1/0/all/0/1\">Pranay Dighe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_P/0/1/0/all/0/1\">Prateeth Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudovic_O/0/1/0/all/0/1\">Oggi Rudovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchi_E/0/1/0/all/0/1\">Erik Marchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xiaochuan Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed Tewfik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WikiWhy: Answering and Explaining Cause-and-Effect Questions. (arXiv:2210.12152v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12152","description":"<p>As large language models (LLMs) grow larger and more sophisticated, assessing\ntheir \"reasoning\" capabilities in natural language grows more challenging.\nRecent question answering (QA) benchmarks that attempt to assess reasoning are\noften limited by a narrow scope of covered situations and subject matters. We\nintroduce WikiWhy, a QA dataset built around a novel auxiliary task: explaining\nwhy an answer is true in natural language. WikiWhy contains over 9,000 \"why\"\nquestion-answer-rationale triples, grounded on Wikipedia facts across a diverse\nset of topics. Each rationale is a set of supporting statements connecting the\nquestion to the answer. WikiWhy serves as a benchmark for the reasoning\ncapabilities of LLMs because it demands rigorous explicit rationales for each\nanswer to demonstrate the acquisition of implicit commonsense knowledge, which\nis unlikely to be easily memorized. GPT-3 baselines achieve only 38.7%\nhuman-evaluated correctness in the end-to-end answer &amp; explain condition,\nleaving significant room for future improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1\">Matthew Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Aditya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Justin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Dialogue Generation with Disentangled Multi-grained Style Specification and Attribute Consistency Reward. (arXiv:2109.06717v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06717","description":"<p>Controllable text generation is an appealing but challenging task, which\nallows users to specify particular attributes of the generated outputs. In this\npaper, we propose a controllable dialogue generation model to steer response\ngeneration under multi-attribute constraints. Specifically, we define and\ncategorize the commonly used control attributes into global and local ones,\nwhich possess different granularities of effects on response generation. Then,\nwe significantly extend the conventional seq2seq framework by introducing a\nnovel two-stage decoder, which first uses a multi-grained style specification\nlayer to impose the stylistic constraints and determine word-level control\nstates of responses based on the attributes, and then employs a response\ngeneration layer to generate final responses maintaining both semantic\nrelevancy to the contexts and fidelity to the attributes. Furthermore, we train\nour model with an attribute consistency reward to promote response control with\nexplicit supervision signals. Extensive experiments and in-depth analyses on\ntwo datasets indicate that our model can significantly outperform competitive\nbaselines in terms of response quality, content diversity and controllability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhe Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhiwei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HydraSum: Disentangling Stylistic Features in Text Summarization using Multi-Decoder Models. (arXiv:2110.04400v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04400","description":"<p>Summarization systems make numerous \"decisions\" about summary properties\nduring inference, e.g. degree of copying, specificity and length of outputs,\netc. However, these are implicitly encoded within model parameters and specific\nstyles cannot be enforced. To address this, we introduce HydraSum, a new\nsummarization architecture that extends the single decoder framework of current\nmodels to a mixture-of-experts version with multiple decoders. We show that\nHydraSum's multiple decoders automatically learn contrasting summary styles\nwhen trained under the standard training objective without any extra\nsupervision. Through experiments on three summarization datasets (CNN, Newsroom\nand XSum), we show that HydraSum provides a simple mechanism to obtain\nstylistically-diverse summaries by sampling from either individual decoders or\ntheir mixtures, outperforming baseline models. Finally, we demonstrate that a\nsmall modification to the gating strategy during training can enforce an even\nstricter style partitioning, e.g. high- vs low-abstractiveness or high- vs\nlow-specificity, allowing users to sample from a larger area in the generation\nspace and vary summary styles along multiple dimensions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Fatema Rajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keyphrase Generation Beyond the Boundaries of Title and Abstract. (arXiv:2112.06776v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06776","description":"<p>Keyphrase generation aims at generating important phrases (keyphrases) that\nbest describe a given document. In scholarly domains, current approaches have\nlargely used only the title and abstract of the articles to generate\nkeyphrases. In this paper, we comprehensively explore whether the integration\nof additional information from the full text of a given article or from\nsemantically similar articles can be helpful for a neural keyphrase generation\nmodel or not. We discover that adding sentences from the full text,\nparticularly in the form of the extractive summary of the article can\nsignificantly improve the generation of both types of keyphrases that are\neither present or absent from the text. Experimental results with three widely\nused models for keyphrase generation along with one of the latest transformer\nmodels suitable for longer documents, Longformer Encoder-Decoder (LED) validate\nthe observation. We also present a new large-scale scholarly dataset FullTextKP\nfor keyphrase generation. Unlike prior large-scale datasets, FullTextKP\nincludes the full text of the articles along with the title and abstract. We\nrelease the source code at https://github.com/kgarg8/FullTextKP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_K/0/1/0/all/0/1\">Krishna Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1\">Jishnu Ray Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Learners' Phonetic Transfer of /i/ from Mandarin Chinese to General American English: A Case Study of a Chinese Learner with Advanced English. (arXiv:2112.13571v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.13571","description":"<p>The current paper concerns language transfer at the phonetic level and\nconcentrates on the transfer phenomenon in an advanced English language\nlearner's acquisition of the English vowels /i/ and its lax counterpart. By\ndetermining whether the Chinese English-language learner (ELL), named Vanya,\ncan accurately distinguish between /i/ and its lax counterpart, and pronounce\nthem precisely in General American English (GAE), this paper serves as a\nreference for further studying language transfer among Chinese ELLs. There were\ntwo objectives: first, the learner's perceptual ability to distinguish between\nvowels /i/ and its lax counterpart was examined; second, the effect of the\nphonetic transfer was determined. Two perception tests and a production test\nwere used to attain these two objectives. The results of two perception tests\ndemonstrated Vanya's perceptual competence in distinguishing between /i/ and\nits lax counterpart and laid a solid foundation for the validity of the\nsubsequent production test. Given that Vanya's production of F1 and F2 values\nof /i/ were highly similar across his first language (Mandarin Chinese) and\nsecond language (GAE) and that both values were lower than the typical values\nfor common /i/ in GAE, with an especially prominent disparity between the F2\nvalues, it is reasonable to conclude that a phonetic transfer occurred. The\nparticipant's high perceptual competence as an advanced-level ELL did not\nnoticeably moderate the effect of phonetic transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lintao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Memory as a Differentiable Search Index. (arXiv:2202.06991v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06991","description":"<p>In this paper, we demonstrate that information retrieval can be accomplished\nwith a single Transformer, in which all information about the corpus is encoded\nin the parameters of the model. To this end, we introduce the Differentiable\nSearch Index (DSI), a new paradigm that learns a text-to-text model that maps\nstring queries directly to relevant docids; in other words, a DSI model answers\nqueries directly using only its parameters, dramatically simplifying the whole\nretrieval process. We study variations in how documents and their identifiers\nare represented, variations in training procedures, and the interplay between\nmodels and corpus sizes. Experiments demonstrate that given appropriate design\nchoices, DSI significantly outperforms strong baselines such as dual encoder\nmodels. Moreover, DSI demonstrates strong generalization capabilities,\noutperforming a BM25 baseline in a zero-shot setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Harsh Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Kai Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neural-Symbolic Approach to Natural Language Understanding. (arXiv:2203.10557v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10557","description":"<p>Deep neural networks, empowered by pre-trained language models, have achieved\nremarkable results in natural language understanding (NLU) tasks. However,\ntheir performances can drastically deteriorate when logical reasoning is\nneeded. This is because NLU in principle depends on not only analogical\nreasoning, which deep neural networks are good at, but also logical reasoning.\nAccording to the dual-process theory, analogical reasoning and logical\nreasoning are respectively carried out by System 1 and System 2 in the human\nbrain. Inspired by the theory, we present a novel framework for NLU called\nNeural-Symbolic Processor (NSP), which performs analogical reasoning based on\nneural processing and logical reasoning based on both neural and symbolic\nprocessing. As a case study, we conduct experiments on two NLU tasks, question\nanswering (QA) and natural language inference (NLI), when numerical reasoning\n(a type of logical reasoning) is necessary. The experimental results show that\nour method significantly outperforms state-of-the-art methods in both tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SecureBERT: A Domain-Specific Language Model for Cybersecurity. (arXiv:2204.02685v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02685","description":"<p>Natural Language Processing (NLP) has recently gained wide attention in\ncybersecurity, particularly in Cyber Threat Intelligence (CTI) and cyber\nautomation. Increased connection and automation have revolutionized the world's\neconomic and cultural infrastructures, while they have introduced risks in\nterms of cyber attacks. CTI is information that helps cybersecurity analysts\nmake intelligent security decisions, that is often delivered in the form of\nnatural language text, which must be transformed to machine readable format\nthrough an automated procedure before it can be used for automated security\nmeasures.\n</p>\n<p>This paper proposes SecureBERT, a cybersecurity language model capable of\ncapturing text connotations in cybersecurity text (e.g., CTI) and therefore\nsuccessful in automation for many critical cybersecurity tasks that would\notherwise rely on human expertise and time-consuming manual efforts. SecureBERT\nhas been trained using a large corpus of cybersecurity text.To make SecureBERT\neffective not just in retaining general English understanding, but also when\napplied to text with cybersecurity implications, we developed a customized\ntokenizer as well as a method to alter pre-trained weights. The SecureBERT is\nevaluated using the standard Masked Language Model (MLM) test as well as two\nadditional standard NLP tasks. Our evaluation studies show that\nSecureBERT\\footnote{\\url{https://github.com/ehsanaghaei/SecureBERT}}\noutperforms existing similar models, confirming its capability for solving\ncrucial NLP tasks in cybersecurity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aghaei_E/0/1/0/all/0/1\">Ehsan Aghaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shadid_W/0/1/0/all/0/1\">Waseem Shadid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Shaer_E/0/1/0/all/0/1\">Ehab Al-Shaer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative or Contrastive? Phrase Reconstruction for Better Sentence Representation Learning. (arXiv:2204.09358v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09358","description":"<p>Though offering amazing contextualized token-level representations, current\npre-trained language models actually take less attention on acquiring\nsentence-level representation during its self-supervised pre-training. If\nself-supervised learning can be distinguished into two subcategories,\ngenerative and contrastive, then most existing studies show that sentence\nrepresentation learning may more benefit from the contrastive methods but not\nthe generative methods. However, contrastive learning cannot be well compatible\nwith the common token-level generative self-supervised learning, and does not\nguarantee good performance on downstream semantic retrieval tasks. Thus, to\nalleviate such obvious inconveniences, we instead propose a novel generative\nself-supervised learning objective based on phrase reconstruction. Empirical\nstudies show that our generative learning may yield powerful enough sentence\nrepresentation and achieve performance in Sentence Textual Similarity (STS)\ntasks on par with contrastive learning. Further, in terms of unsupervised\nsetting, our generative method outperforms previous state-of-the-art SimCSE on\nthe benchmark of downstream semantic retrieval tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bohong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning. (arXiv:2205.08232v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2205.08232","description":"<p>Recently, deep learning models have made great progress in MWP solving on\nanswer accuracy. However, they are uninterpretable since they mainly rely on\nshallow heuristics to achieve high performance without understanding and\nreasoning the grounded math logic. To address this issue and make a step\ntowards interpretable MWP solving, we first construct a high-quality MWP\ndataset named InterMWP which consists of 11,495 MWPs and annotates\ninterpretable logical formulas based on algebraic knowledge as the grounded\nlinguistic logic of each solution equation. Different from existing MWP\ndatasets, our InterMWP benchmark asks for a solver to not only output the\nsolution expressions but also predict the corresponding logical formulas. We\nfurther propose a novel approach with logical prompt and interpretation\ngeneration, called LogicSolver. For each MWP, our LogicSolver first retrieves\nsome highly-correlated algebraic knowledge and then passes them to the backbone\nmodel as prompts to improve the semantic representations of MWPs. With these\nimproved semantic representations, our LogicSolver generates corresponding\nsolution expressions and interpretable knowledge formulas in accord with the\ngenerated solution expressions, simultaneously. Experimental results show that\nour LogicSolver has stronger logical formula-based interpretability than\nbaselines while achieving higher answer accuracy with the help of logical\nprompts, simultaneously.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhicheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jinghui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarization as Indirect Supervision for Relation Extraction. (arXiv:2205.09837v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09837","description":"<p>Relation extraction (RE) models have been challenged by their reliance on\ntraining data with expensive annotations. Considering that summarization tasks\naim at acquiring concise expressions of synoptical information from the longer\ncontext, these tasks naturally align with the objective of RE, i.e., extracting\na kind of synoptical information that describes the relation of entity\nmentions. We present SuRE, which converts RE into a summarization formulation.\nSuRE leads to more precise and resource-efficient RE based on indirect\nsupervision from summarization tasks. To achieve this goal, we develop sentence\nand relation conversion techniques that essentially bridge the formulation of\nsummarization and RE tasks. We also incorporate constraint decoding techniques\nwith Trie scoring to further enhance summarization-based RE with robust\ninference. Experiments on three RE datasets demonstrate the effectiveness of\nSuRE in both full-dataset and low-resource settings, showing that summarization\nis a promising source of indirect supervision to improve RE models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyu Derek Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Autoregressive Neural Machine Translation: A Call for Clarity. (arXiv:2205.10577v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10577","description":"<p>Non-autoregressive approaches aim to improve the inference speed of\ntranslation models by only requiring a single forward pass to generate the\noutput sequence instead of iteratively producing each predicted token.\nConsequently, their translation quality still tends to be inferior to their\nautoregressive counterparts due to several issues involving output token\ninterdependence. In this work, we take a step back and revisit several\ntechniques that have been proposed for improving non-autoregressive translation\nmodels and compare their combined translation quality and speed implications\nunder third-party testing environments. We provide novel insights for\nestablishing strong baselines using length prediction or CTC-based architecture\nvariants and contribute standardized BLEU, chrF++, and TER scores using\nsacreBLEU on four translation tasks, which crucially have been missing as\ninconsistencies in the use of tokenized BLEU lead to deviations of up to 1.7\nBLEU points. Our open-sourced code is integrated into fairseq for\nreproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_R/0/1/0/all/0/1\">Robin M. Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pires_T/0/1/0/all/0/1\">Telmo Pires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peitz_S/0/1/0/all/0/1\">Stephan Peitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loof_J/0/1/0/all/0/1\">Jonas L&#xf6;&#xf6;f</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Do Compressed Multilingual Machine Translation Models Forget?. (arXiv:2205.10828v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10828","description":"<p>Recently, very large pre-trained models achieve state-of-the-art results in\nvarious natural language processing (NLP) tasks, but their size makes it more\nchallenging to apply them in resource-constrained environments. Compression\ntechniques allow to drastically reduce the size of the models and therefore\ntheir inference time with negligible impact on top-tier metrics. However, the\ngeneral performance averaged across multiple tasks and/or languages may hide a\ndrastic performance drop on under-represented features, which could result in\nthe amplification of biases encoded by the models. In this work, we assess the\nimpact of compression methods on Multilingual Neural Machine Translation models\n(MNMT) for various language groups, gender, and semantic biases by extensive\nanalysis of compressed models on different machine translation benchmarks, i.e.\nFLORES-101, MT-Gender, and DiBiMT. We show that the performance of\nunder-represented languages drops significantly, while the average BLEU metric\nonly slightly decreases. Interestingly, the removal of noisy memorization with\ncompression leads to a significant improvement for some medium-resource\nlanguages. Finally, we demonstrate that compression amplifies intrinsic gender\nand semantic biases, even in high-resource languages. Code:\nhttps://github.com/alirezamshi/bias-compressedMT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1\">Alireza Mohammadshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1\">Alexandre Berard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brun_C/0/1/0/all/0/1\">Caroline Brun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Template-based Method for Constrained Neural Machine Translation. (arXiv:2205.11255v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11255","description":"<p>Machine translation systems are expected to cope with various types of\nconstraints in many practical scenarios. While neural machine translation (NMT)\nhas achieved strong performance in unconstrained cases, it is non-trivial to\nimpose pre-specified constraints into the translation process of NMT models.\nAlthough many approaches have been proposed to address this issue, most\nexisting methods can not satisfy the following three desiderata at the same\ntime: (1) high translation quality, (2) high match accuracy, and (3) low\nlatency. In this work, we propose a template-based method that can yield\nresults with high translation quality and match accuracy and the inference\nspeed of our method is comparable with unconstrained NMT models. Our basic idea\nis to rearrange the generation of constrained and unconstrained tokens through\na template. Our method does not require any changes in the model architecture\nand the decoding algorithm. Experimental results show that the proposed\ntemplate-based approach can outperform several representative baselines in both\nlexically and structurally constrained translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models. (arXiv:2205.11432v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11432","description":"<p>Current Natural Language Inference (NLI) models achieve impressive results,\nsometimes outperforming humans when evaluating on in-distribution test sets.\nHowever, as these models are known to learn from annotation artefacts and\ndataset biases, it is unclear to what extent the models are learning the task\nof NLI instead of learning from shallow heuristics in their training data. We\naddress this issue by introducing a logical reasoning framework for NLI,\ncreating highly transparent model decisions that are based on logical rules.\nUnlike prior work, we show that improved interpretability can be achieved\nwithout decreasing the predictive accuracy. We almost fully retain performance\non SNLI, while also identifying the exact hypothesis spans that are responsible\nfor each model prediction. Using the e-SNLI human explanations, we verify that\nour model makes sensible decisions at a span level, despite not using any span\nlabels during training. We can further improve model performance and span-level\ndecisions by using the e-SNLI explanations during training. Finally, our model\nis more robust in a reduced data setting. When training with only 1,000\nexamples, out-of-distribution performance improves on the MNLI matched and\nmismatched validation sets by 13% and 16% relative to the baseline. Training\nwith fewer observations yields further improvements, both in-distribution and\nout-of-distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stacey_J/0/1/0/all/0/1\">Joe Stacey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubossarsky_H/0/1/0/all/0/1\">Haim Dubossarsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Contrastive Learning for Relation Extraction. (arXiv:2205.12491v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12491","description":"<p>Recent relation extraction (RE) works have shown encouraging improvements by\nconducting contrastive learning on silver labels generated by distant\nsupervision before fine-tuning on gold labels. Existing methods typically\nassume all these silver labels are accurate and treat them equally; however,\ndistant supervision is inevitably noisy -- some silver labels are more reliable\nthan others. In this paper, we propose fine-grained contrastive learning\n(FineCL) for RE, which leverages fine-grained information about which silver\nlabels are and are not noisy to improve the quality of learned relationship\nrepresentations for RE. We first assess the quality of silver labels via a\nsimple and automatic approach we call \"learning order denoising,\" where we\ntrain a language model to learn these relations and record the order of learned\ntraining instances. We show that learning order largely corresponds to label\naccuracy -- early-learned silver labels have, on average, more accurate labels\nthan later-learned silver labels. Then, during pre-training, we increase the\nweights of accurate labels within a novel contrastive learning objective.\nExperiments on several RE benchmarks show that FineCL makes consistent and\nsignificant performance gains over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hogan_W/0/1/0/all/0/1\">William Hogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Text Generation with Neurally-Decomposed Oracle. (arXiv:2205.14219v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14219","description":"<p>We propose a general and efficient framework to control auto-regressive\ngeneration models with NeurAlly-Decomposed Oracle (NADO). Given a pre-trained\nbase language model and a sequence-level boolean oracle function, we propose to\ndecompose the oracle function into token-level guidance to steer the base model\nin text generation. Specifically, the token-level guidance is approximated by a\nneural model trained with examples sampled from the base model, demanding no\nadditional auxiliary labeled data. Based on posterior regularization, we\npresent the closed-form optimal solution to incorporate the token-level\nguidance into the base model for controllable generation. We further provide a\ntheoretical analysis of how the approximation quality of NADO affects the\ncontrollable generation results. Experiments conducted on two applications: (1)\ntext generation with lexical constraints and (2) machine translation with\nformality control demonstrate that our framework efficiently guides the base\nmodel towards the given oracle while maintaining high generation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1\">Tao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Sidi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Classifiers are Unreliable for Concept Removal and Detection. (arXiv:2207.04153v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.04153","description":"<p>Neural network models trained on text data have been found to encode\nundesirable linguistic or sensitive concepts in their representation. Removing\nsuch concepts is non-trivial because of a complex relationship between the\nconcept, text input, and the learnt representation. Recent work has proposed\npost-hoc and adversarial methods to remove such unwanted concepts from a\nmodel's representation. Through an extensive theoretical and empirical\nanalysis, we show that these methods can be counter-productive: they are unable\nto remove the concepts entirely, and in the worst case may end up destroying\nall task-relevant features. The reason is the methods' reliance on a probing\nclassifier as a proxy for the concept. Even under the most favorable conditions\nfor learning a probing classifier when a concept's relevant features in\nrepresentation space alone can provide 100% accuracy, we prove that a probing\nclassifier is likely to use non-concept features and thus post-hoc or\nadversarial methods will fail to remove the concept correctly. These\ntheoretical implications are confirmed by experiments on models trained on\nsynthetic, Multi-NLI, and Twitter datasets. For sensitive applications of\nconcept removal such as fairness, we recommend caution against using these\nmethods and propose a spuriousness metric to gauge the quality of the final\nclassifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhinav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Amit Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The MuSe 2022 Multimodal Sentiment Analysis Challenge: Humor, Emotional Reactions, and Stress. (arXiv:2207.05691v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.05691","description":"<p>The Multimodal Sentiment Analysis Challenge (MuSe) 2022 is dedicated to\nmultimodal sentiment and emotion recognition. For this year's challenge, we\nfeature three datasets: (i) the Passau Spontaneous Football Coach Humor\n(Passau-SFCH) dataset that contains audio-visual recordings of German football\ncoaches, labelled for the presence of humour; (ii) the Hume-Reaction dataset in\nwhich reactions of individuals to emotional stimuli have been annotated with\nrespect to seven emotional expression intensities, and (iii) the Ulm-Trier\nSocial Stress Test (Ulm-TSST) dataset comprising of audio-visual data labelled\nwith continuous emotion values (arousal and valence) of people in stressful\ndispositions. Using the introduced datasets, MuSe 2022 2022 addresses three\ncontemporary affective computing problems: in the Humor Detection Sub-Challenge\n(MuSe-Humor), spontaneous humour has to be recognised; in the Emotional\nReactions Sub-Challenge (MuSe-Reaction), seven fine-grained `in-the-wild'\nemotions have to be predicted; and in the Emotional Stress Sub-Challenge\n(MuSe-Stress), a continuous prediction of stressed emotion values is featured.\nThe challenge is designed to attract different research communities,\nencouraging a fusion of their disciplines. Mainly, MuSe 2022 targets the\ncommunities of audio-visual emotion recognition, health informatics, and\nsymbolic sentiment analysis. This baseline paper describes the datasets as well\nas the feature sets extracted from them. A recurrent neural network with LSTM\ncells is used to set competitive baseline results on the test partitions for\neach sub-challenge. We report an Area Under the Curve (AUC) of .8480 for\nMuSe-Humor; .2801 mean (from 7-classes) Pearson's Correlations Coefficient for\nMuSe-Reaction, as well as .4931 Concordance Correlation Coefficient (CCC) and\n.4761 for valence and arousal in MuSe-Stress, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christ_L/0/1/0/all/0/1\">Lukas Christ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiriparian_S/0/1/0/all/0/1\">Shahin Amiriparian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1\">Alice Baird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzirakis_P/0/1/0/all/0/1\">Panagiotis Tzirakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kathan_A/0/1/0/all/0/1\">Alexander Kathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_N/0/1/0/all/0/1\">Niklas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messner_E/0/1/0/all/0/1\">Eva-Maria Me&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konig_A/0/1/0/all/0/1\">Andreas K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowen_A/0/1/0/all/0/1\">Alan Cowen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fuse It More Deeply! A Variational Transformer with Layer-Wise Latent Variable Inference for Text Generation. (arXiv:2207.06130v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.06130","description":"<p>The past several years have witnessed Variational Auto-Encoder's superiority\nin various text generation tasks. However, due to the sequential nature of the\ntext, auto-regressive decoders tend to ignore latent variables and then reduce\nto simple language models, known as the KL vanishing problem, which would\nfurther deteriorate when VAE is combined with Transformer-based structures. To\nameliorate this problem, we propose DELLA, a novel variational Transformer\nframework. DELLA learns a series of layer-wise latent variables with each\ninferred from those of lower layers and tightly coupled with the hidden states\nby low-rank tensor product. In this way, DELLA forces these posterior latent\nvariables to be fused deeply with the whole computation path and hence\nincorporate more information. We theoretically demonstrate that our method can\nbe regarded as entangling latent variables to avoid posterior information\ndecrease through layers, enabling DELLA to get higher non-zero KL values even\nwithout any annealing or thresholding tricks. Experiments on four unconditional\nand three conditional generation tasks show that DELLA could better alleviate\nKL vanishing and improve both quality and diversity compared to several strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems. (arXiv:2209.03755v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2209.03755","description":"<p>Mis- and disinformation are now a substantial global threat to our security\nand safety. To cope with the scale of online misinformation, one viable\nsolution is to automate the fact-checking of claims by retrieving and verifying\nagainst relevant evidence. While major recent advances have been achieved in\npushing forward the automatic fact-verification, a comprehensive evaluation of\nthe possible attack vectors against such systems is still lacking.\nParticularly, the automated fact-verification process might be vulnerable to\nthe exact disinformation campaigns it is trying to combat. In this work, we\nassume an adversary that automatically tampers with the online evidence in\norder to disrupt the fact-checking model via camouflaging the relevant\nevidence, or planting a misleading one. We first propose an exploratory\ntaxonomy that spans these two targets and the different threat model\ndimensions. Guided by this, we design and propose several potential attack\nmethods. We show that it is possible to subtly modify claim-salient snippets in\nthe evidence, in addition to generating diverse and claim-aligned evidence. As\na result, we highly degrade the fact-checking performance under many different\npermutations of the taxonomy's dimensions. The attacks are also robust against\npost-hoc modifications of the claim. Our analysis further hints at potential\nlimitations in models' inference when faced with contradicting evidence. We\nemphasize that these attacks can have harmful implications on the inspectable\nand human-in-the-loop usage scenarios of such models, and we conclude by\ndiscussing challenges and directions for future defenses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelnabi_S/0/1/0/all/0/1\">Sahar Abdelnabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vega-MT: The JD Explore Academy Translation System for WMT22. (arXiv:2209.09444v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.09444","description":"<p>We describe the JD Explore Academy's submission of the WMT 2022 shared\ngeneral translation task. We participated in all high-resource tracks and one\nmedium-resource track, including Chinese-English, German-English,\nCzech-English, Russian-English, and Japanese-English. We push the limit of our\nprevious work -- bidirectional training for translation by scaling up two main\nfactors, i.e. language pairs and model sizes, namely the \\textbf{Vega-MT}\nsystem. As for language pairs, we scale the \"bidirectional\" up to the\n\"multidirectional\" settings, covering all participating languages, to exploit\nthe common knowledge across languages, and transfer them to the downstream\nbilingual tasks. As for model sizes, we scale the Transformer-Big up to the\nextremely large model that owns nearly 4.7 Billion parameters, to fully enhance\nthe model capacity for our Vega-MT. Also, we adopt the data augmentation\nstrategies, e.g. cycle translation for monolingual data, and bidirectional\nself-training for bilingual and monolingual data, to comprehensively exploit\nthe bilingual and monolingual data. To adapt our Vega-MT to the general domain\ntest set, generalization tuning is designed. Based on the official automatic\nscores of constrained systems, in terms of the sacreBLEU shown in Figure-1, we\ngot the 1st place on {Zh-En (33.5), En-Zh (49.7), De-En (33.7), En-De (37.8),\nCs-En (54.9), En-Cs (41.4) and En-Ru (32.7)}, 2nd place on {Ru-En (45.1) and\nJa-En (25.6)}, and 3rd place on {En-Ja(41.5)}, respectively; W.R.T the COMET,\nwe got the 1st place on {Zh-En (45.1), En-Zh (61.7), De-En (58.0), En-De\n(63.2), Cs-En (74.7), Ru-En (64.9), En-Ru (69.6) and En-Ja (65.1)}, 2nd place\non {En-Cs (95.3) and Ja-En (40.6)}, respectively. Models will be released to\nfacilitate the MT community through GitHub and OmniForce Platform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zan_C/0/1/0/all/0/1\">Changtong Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Keqin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_B/0/1/0/all/0/1\">Baopu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Boan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shwai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qingyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation. (arXiv:2210.02952v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02952","description":"<p>Prompt tuning, or the conditioning of a frozen pretrained language model\n(PLM) with soft prompts learned from data, has demonstrated impressive\nperformance on a wide range of NLP tasks. However, prompt tuning requires a\nlarge training dataset to be effective and is outperformed by finetuning the\nentire PLM in data-scarce regimes. Previous work (Gu et al., 2022, Vu et al.,\n2022) proposed to transfer soft prompts pretrained on the source domain to the\ntarget domain. In this paper, we explore domain adaptation for prompt tuning, a\nproblem setting where unlabeled data from the target domain are available\nduring pretraining. We propose bOosting Prompt TunIng with doMain Adaptation\n(OPTIMA), which regularizes the decision boundary to be smooth around regions\nwhere source and target data distributions are similar. Extensive experiments\ndemonstrate that OPTIMA significantly enhances the transferability and\nsample-efficiency of prompt tuning compared to strong baselines. Moreover, in\nfew-shot settings, OPTIMA exceeds full-model tuning by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Language Models for Paragraph-Level Question Generation. (arXiv:2210.03992v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03992","description":"<p>Powerful generative models have led to recent progress in question generation\n(QG). However, it is difficult to measure advances in QG research since there\nare no standardized resources that allow a uniform comparison among approaches.\nIn this paper, we introduce QG-Bench, a multilingual and multidomain benchmark\nfor QG that unifies existing question answering datasets by converting them to\na standard QG setting. It includes general-purpose datasets such as SQuAD for\nEnglish, datasets from ten domains and two styles, as well as datasets in eight\ndifferent languages. Using QG-Bench as a reference, we perform an extensive\nanalysis of the capabilities of language models for the task. First, we propose\nrobust QG baselines based on fine-tuning generative language models. Then, we\ncomplement automatic evaluation based on standard metrics with an extensive\nmanual evaluation, which in turn sheds light on the difficulty of evaluating QG\nmodels. Finally, we analyse both the domain adaptability of these models as\nwell as the effectiveness of multilingual models in languages other than\nEnglish. QG-Bench is released along with the fine-tuned models presented in the\npaper https://github.com/asahi417/lm-question-generation, which are also\navailable as a demo https://autoqg.net/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ushio_A/0/1/0/all/0/1\">Asahi Ushio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alva_Manchego_F/0/1/0/all/0/1\">Fernando Alva-Manchego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters. (arXiv:2210.04284v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04284","description":"<p>Adapter Tuning, which freezes the pretrained language models (PLMs) and only\nfine-tunes a few extra modules, becomes an appealing efficient alternative to\nthe full model fine-tuning. Although computationally efficient, the recent\nAdapters often increase parameters (e.g. bottleneck dimension) for matching the\nperformance of full model fine-tuning, which we argue goes against their\noriginal intention. In this work, we re-examine the parameter-efficiency of\nAdapters through the lens of network pruning (we name such plug-in concept as\n\\texttt{SparseAdapter}) and find that SparseAdapter can achieve comparable or\nbetter performance than standard Adapters when the sparse ratio reaches up to\n80\\%. Based on our findings, we introduce an easy but effective setting\n``\\textit{Large-Sparse}'' to improve the model capacity of Adapters under the\nsame parameter budget. Experiments on five competitive Adapters upon three\nadvanced PLMs show that with proper sparse method (e.g. SNIP) and ratio (e.g.\n40\\%) SparseAdapter can consistently outperform their corresponding\ncounterpart. Encouragingly, with the \\textit{Large-Sparse} setting, we can\nobtain further appealing gains, even outperforming the full fine-tuning by a\nlarge margin. Our code will be released at:\nhttps://github.com/Shwai-He/SparseAdapter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shwai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1\">Daize Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HumSet: Dataset of Multilingual Information Extraction and Classification for Humanitarian Crisis Response. (arXiv:2210.04573v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04573","description":"<p>Timely and effective response to humanitarian crises requires quick and\naccurate analysis of large amounts of text data - a process that can highly\nbenefit from expert-assisted NLP systems trained on validated and annotated\ndata in the humanitarian response domain. To enable creation of such NLP\nsystems, we introduce and release HumSet, a novel and rich multilingual dataset\nof humanitarian response documents annotated by experts in the humanitarian\nresponse community. The dataset provides documents in three languages (English,\nFrench, Spanish) and covers a variety of humanitarian crises from 2018 to 2021\nacross the globe. For each document, HUMSET provides selected snippets\n(entries) as well as assigned classes to each entry annotated using common\nhumanitarian information analysis frameworks. HUMSET also provides novel and\nchallenging entry extraction and multi-label entry classification tasks. In\nthis paper, we take a first step towards approaching these tasks and conduct a\nset of experiments on Pre-trained Language Models (PLM) to establish strong\nbaselines for future research in this domain. The dataset is available at\nhttps://blog.thedeep.io/humset/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fekih_S/0/1/0/all/0/1\">Selim Fekih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamagnone_N/0/1/0/all/0/1\">Nicolo&#x27; Tamagnone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minixhofer_B/0/1/0/all/0/1\">Benjamin Minixhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrestha_R/0/1/0/all/0/1\">Ranjan Shrestha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contla_X/0/1/0/all/0/1\">Ximena Contla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oglethorpe_E/0/1/0/all/0/1\">Ewan Oglethorpe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepPerform: An Efficient Approach for Performance Testing of Resource-Constrained Neural Networks. (arXiv:2210.05370v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.05370","description":"<p>Today, an increasing number of Adaptive Deep Neural Networks (AdNNs) are\nbeing used on resource-constrained embedded devices. We observe that, similar\nto traditional software, redundant computation exists in AdNNs, resulting in\nconsiderable performance degradation. The performance degradation is dependent\non the input and is referred to as input-dependent performance bottlenecks\n(IDPBs). To ensure an AdNN satisfies the performance requirements of\nresource-constrained applications, it is essential to conduct performance\ntesting to detect IDPBs in the AdNN. Existing neural network testing methods\nare primarily concerned with correctness testing, which does not involve\nperformance testing. To fill this gap, we propose DeepPerform, a scalable\napproach to generate test samples to detect the IDPBs in AdNNs. We first\ndemonstrate how the problem of generating performance test samples detecting\nIDPBs can be formulated as an optimization problem. Following that, we\ndemonstrate how DeepPerform efficiently handles the optimization problem by\nlearning and estimating the distribution of AdNNs' computational consumption.\nWe evaluate DeepPerform on three widely used datasets against five popular AdNN\nmodels. The results show that DeepPerform generates test samples that cause\nmore severe performance degradation (FLOPs: increase up to 552\\%). Furthermore,\nDeepPerform is substantially more efficient than the baseline methods in\ngenerating test inputs(runtime overhead: only 6-10 milliseconds).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Simin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_M/0/1/0/all/0/1\">Mirazul Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting the clinical citation count of biomedical papers using multilayer perceptron neural network. (arXiv:2210.06346v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06346","description":"<p>The number of clinical citations received from clinical guidelines or\nclinical trials has been considered as one of the most appropriate indicators\nfor quantifying the clinical impact of biomedical papers. Therefore, the early\nprediction of the clinical citation count of biomedical papers is critical to\nscientific activities in biomedicine, such as research evaluation, resource\nallocation, and clinical translation. In this study, we designed a four-layer\nmultilayer perceptron neural network (MPNN) model to predict the clinical\ncitation count of biomedical papers in the future by using 9,822,620 biomedical\npapers published from 1985 to 2005. We extracted ninety-one paper features from\nthree dimensions as the input of the model, including twenty-one features in\nthe paper dimension, thirty-five in the reference dimension, and thirty-five in\nthe citing paper dimension. In each dimension, the features can be classified\ninto three categories, i.e., the citation-related features, the clinical\ntranslation-related features, and the topic-related features. Besides, in the\npaper dimension, we also considered the features that have previously been\ndemonstrated to be related to the citation counts of research papers. The\nresults showed that the proposed MPNN model outperformed the other five\nbaseline models, and the features in the reference dimension were the most\nimportant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuli Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qikai Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StoryER: Automatic Story Evaluation via Ranking, Rating and Reasoning. (arXiv:2210.08459v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08459","description":"<p>Existing automatic story evaluation methods place a premium on story lexical\nlevel coherence, deviating from human preference. We go beyond this limitation\nby considering a novel \\textbf{Story} \\textbf{E}valuation method that mimics\nhuman preference when judging a story, namely \\textbf{StoryER}, which consists\nof three sub-tasks: \\textbf{R}anking, \\textbf{R}ating and \\textbf{R}easoning.\nGiven either a machine-generated or a human-written story, StoryER requires the\nmachine to output 1) a preference score that corresponds to human preference,\n2) specific ratings and their corresponding confidences and 3) comments for\nvarious aspects (e.g., opening, character-shaping). To support these tasks, we\nintroduce a well-annotated dataset comprising (i) 100k ranked story pairs; and\n(ii) a set of 46k ratings and comments on various aspects of the story. We\nfinetune Longformer-Encoder-Decoder (LED) on the collected dataset, with the\nencoder responsible for preference score and aspect prediction and the decoder\nfor comment generation. Our comprehensive experiments result in a competitive\nbenchmark for each task, showing the high correlation to human preference. In\naddition, we have witnessed the joint learning of the preference scores, the\naspect ratings, and the comments brings gain in each single task. Our dataset\nand benchmarks are publicly available to advance the research of story\nevaluation tasks.\\footnote{Dataset and pre-trained model demo are available at\nanonymous website \\url{<a href=\"http://storytelling-lab.com/eval\">this http URL</a>} and\n\\url{https://github.com/sairin1202/StoryER}}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Duc Minh Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamura_H/0/1/0/all/0/1\">Hiroya Takamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyao_Y/0/1/0/all/0/1\">Yusuke Miyao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakayama_H/0/1/0/all/0/1\">Hideki Nakayama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Representation Learning with Generative Objective rather than Contrastive Objective. (arXiv:2210.08474v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08474","description":"<p>Though offering amazing contextualized token-level representations, current\npre-trained language models take less attention on accurately acquiring\nsentence-level representation during their self-supervised pre-training.\nHowever, contrastive objectives which dominate the current sentence\nrepresentation learning bring little linguistic interpretability and no\nperformance guarantee on downstream semantic tasks. We instead propose a novel\ngenerative self-supervised learning objective based on phrase reconstruction.\nTo overcome the drawbacks of previous generative methods, we carefully model\nintra-sentence structure by breaking down one sentence into pieces of important\nphrases. Empirical studies show that our generative learning achieves powerful\nenough performance improvement and outperforms the current state-of-the-art\ncontrastive methods not only on the STS benchmarks, but also on downstream\nsemantic retrieval and reranking tasks. Our code is available at\nhttps://github.com/chengzhipanpan/PaSeR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bohong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HCL-TAT: A Hybrid Contrastive Learning Method for Few-shot Event Detection with Task-Adaptive Threshold. (arXiv:2210.08806v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08806","description":"<p>Conventional event detection models under supervised learning settings suffer\nfrom the inability of transfer to newly-emerged event types owing to lack of\nsufficient annotations. A commonly-adapted solution is to follow a\nidentify-then-classify manner, which first identifies the triggers and then\nconverts the classification task via a few-shot learning paradigm. However,\nthese methods still fall far short of expectations due to: (i) insufficient\nlearning of discriminative representations in low-resource scenarios, and (ii)\ntrigger misidentification caused by the overlap of the learned representations\nof triggers and non-triggers. To address the problems, in this paper, we\npropose a novel Hybrid Contrastive Learning method with a Task-Adaptive\nThreshold (abbreviated as HCLTAT), which enables discriminative representation\nlearning with a two-view contrastive loss (support-support and\nprototype-query), and devises a easily-adapted threshold to alleviate\nmisidentification of triggers. Extensive experiments on the benchmark dataset\nFewEvent demonstrate the superiority of our method to achieve better results\ncompared to the state-of-the-arts. All the code and data of this paper will be\navailable for online public access.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Rui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dangyang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NADI 2022: The Third Nuanced Arabic Dialect Identification Shared Task. (arXiv:2210.09582v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09582","description":"<p>We describe findings of the third Nuanced Arabic Dialect Identification\nShared Task (NADI 2022). NADI aims at advancing state of the art Arabic NLP,\nincluding on Arabic dialects. It does so by affording diverse datasets and\nmodeling opportunities in a standardized context where meaningful comparisons\nbetween models and approaches are possible. NADI 2022 targeted both dialect\nidentification (Subtask 1) and dialectal sentiment analysis (Subtask 2) at the\ncountry level. A total of 41 unique teams registered for the shared task, of\nwhom 21 teams have actually participated (with 105 valid submissions). Among\nthese, 19 teams participated in Subtask 1 and 10 participated in Subtask 2. The\nwinning team achieved 27.06 F1 on Subtask 1 and F1=75.16 on Subtask 2,\nreflecting that the two subtasks remain challenging and motivating future work\nin this area. We describe methods employed by participating teams and offer an\noutlook for NADI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouamor_H/0/1/0/all/0/1\">Houda Bouamor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maestro-U: Leveraging joint speech-text representation learning for zero supervised speech ASR. (arXiv:2210.10027v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10027","description":"<p>Training state-of-the-art Automated Speech Recognition (ASR) models typically\nrequires a substantial amount of transcribed speech. In this work, we\ndemonstrate that a modality-matched joint speech and text model can be\nleveraged to train a massively multilingual ASR model without any supervised\n(manually transcribed) speech for some languages. This paper explores the use\nof jointly learnt speech and text representations in a massively multilingual,\nzero supervised speech, real-world setting to expand the set of languages\ncovered by ASR with only unlabeled speech and text in the target languages.\nUsing the FLEURS dataset, we define the task to cover $102$ languages, where\ntranscribed speech is available in $52$ of these languages and can be used to\nimprove end-to-end ASR quality on the remaining $50$. First, we show that by\ncombining speech representations with byte-level text representations and use\nof language embeddings, we can dramatically reduce the Character Error Rate\n(CER) on languages with no supervised speech from 64.8\\% to 30.8\\%, a relative\nreduction of 53\\%. Second, using a subset of South Asian languages we show that\nMaestro-U can promote knowledge transfer from languages with supervised speech\neven when there is limited to no graphemic overlap. Overall, Maestro-U closes\nthe gap to oracle performance by 68.5\\% relative and reduces the CER of 19\nlanguages below 15\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_P/0/1/0/all/0/1\">Pedro Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nanxin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering. (arXiv:2210.10176v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10176","description":"<p>Most Outside-Knowledge Visual Question Answering (OK-VQA) systems employ a\ntwo-stage framework that first retrieves external knowledge given the visual\nquestion and then predicts the answer based on the retrieved content. However,\nthe retrieved knowledge is often inadequate. Retrievals are frequently too\ngeneral and fail to cover specific knowledge needed to answer the question.\nAlso, the naturally available supervision (whether the passage contains the\ncorrect answer) is weak and does not guarantee question relevancy. To address\nthese issues, we propose an Entity-Focused Retrieval (EnFoRe) model that\nprovides stronger supervision during training and recognizes question-relevant\nentities to help retrieve more specific knowledge. Experiments show that our\nEnFoRe model achieves superior retrieval performance on OK-VQA, the currently\nlargest outside-knowledge VQA dataset. We also combine the retrieved knowledge\nwith state-of-the-art VQA models, and achieve a new state-of-the-art\nperformance on OK-VQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond J. Mooney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Type-supervised sequence labeling based on the heterogeneous star graph for named entity recognition. (arXiv:2210.10240v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10240","description":"<p>Named entity recognition is a fundamental task in natural language\nprocessing, identifying the span and category of entities in unstructured\ntexts. The traditional sequence labeling methodology ignores the nested\nentities, i.e. entities included in other entity mentions. Many approaches\nattempt to address this scenario, most of which rely on complex structures or\nhave high computation complexity. The representation learning of the\nheterogeneous star graph containing text nodes and type nodes is investigated\nin this paper. In addition, we revise the graph attention mechanism into a\nhybrid form to address its unreasonableness in specific topologies. The model\nperforms the type-supervised sequence labeling after updating nodes in the\ngraph. The annotation scheme is an extension of the single-layer sequence\nlabeling and is able to cope with the vast majority of nested entities.\nExtensive experiments on public NER datasets reveal the effectiveness of our\nmodel in extracting both flat and nested entities. The method achieved\nstate-of-the-art performance on both flat and nested datasets. The significant\nimprovement in accuracy reflects the superiority of the multi-layer labeling\nstrategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xueru Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Changjiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haotian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Luguang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hong Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continued Pretraining for Better Zero- and Few-Shot Promptability. (arXiv:2210.10258v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10258","description":"<p>Recently introduced language model prompting methods can achieve high\naccuracy in zero- and few-shot settings while requiring few to no learned\ntask-specific parameters. Nevertheless, these methods still often trail behind\nfull model finetuning. In this work, we investigate if a dedicated continued\npretraining stage could improve \"promptability\", i.e., zero-shot performance\nwith natural language prompts or few-shot performance with prompt tuning. We\nreveal settings where existing continued pretraining methods lack\npromptability. We also identify current methodological gaps, which we fill with\nthorough large-scale experiments. We demonstrate that a simple recipe,\ncontinued pretraining that incorporates a trainable prompt during multi-task\nlearning, leads to improved promptability in both zero- and few-shot settings\ncompared to existing methods, up to 31% relative. On the other hand, we find\nthat continued pretraining using MAML-style meta-learning, a method that\ndirectly optimizes few-shot promptability, yields subpar performance. We\nvalidate our findings with two prompt tuning methods, and, based on our\nresults, we provide concrete recommendations to optimize promptability for\ndifferent use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaofeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logan_R/0/1/0/all/0/1\">Robert L. Logan IV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walsh_P/0/1/0/all/0/1\">Pete Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagia_A/0/1/0/all/0/1\">Akshita Bhagia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groeneveld_D/0/1/0/all/0/1\">Dirk Groeneveld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models. (arXiv:2210.10289v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10289","description":"<p>Pre-trained language models (LMs), such as BERT (Devlin et al., 2018) and its\nvariants, have led to significant improvements on various NLP tasks in past\nyears. However, a theoretical framework for studying their relationships is\nstill missing. In this paper, we fill this gap by investigating the linear\ndependency between pre-trained LMs. The linear dependency of LMs is defined\nanalogously to the linear dependency of vectors. We propose Language Model\nDecomposition (LMD) to represent a LM using a linear combination of other LMs\nas basis, and derive the closed-form solution. A goodness-of-fit metric for LMD\nsimilar to the coefficient of determination is defined and used to measure the\nlinear dependency of a set of LMs. In experiments, we find that BERT and eleven\n(11) BERT-like LMs are 91% linearly dependent. This observation suggests that\ncurrent state-of-the-art (SOTA) LMs are highly \"correlated\". To further advance\nSOTA we need more diverse and novel LMs that are less dependent on existing\nLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revision Transformers: Getting RiT of No-Nos. (arXiv:2210.10332v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10332","description":"<p>Current transformer language models (LM) are large-scale models with billions\nof parameters. They have been shown to provide high performances on a variety\nof tasks but are also prone to shortcut learning and bias. Addressing such\nincorrect model behavior via parameter adjustments is very costly. This is\nparticularly problematic for updating dynamic concepts, such as moral values,\nwhich vary culturally or interpersonally. In this work, we question the current\ncommon practice of storing all information in the model parameters and propose\nthe Revision Transformer (RiT) employing information retrieval to facilitate\neasy model updating. The specific combination of a large-scale pre-trained LM\nthat inherently but also diffusely encodes world knowledge with a\nclear-structured revision engine makes it possible to update the model's\nknowledge with little effort and the help of user interaction. We exemplify RiT\non a moral dataset and simulate user feedback demonstrating strong performance\nin model revision even with small data. This way, users can easily design a\nmodel regarding their preferences, paving the way for more transparent and\npersonalized AI models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1\">Felix Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stammer_W/0/1/0/all/0/1\">Wolfgang Stammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A baseline revisited: Pushing the limits of multi-segment models for context-aware translation. (arXiv:2210.10906v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10906","description":"<p>This paper addresses the task of contextual translation using multi-segment\nmodels. Specifically we show that increasing model capacity further pushes the\nlimits of this approach and that deeper models are more suited to capture\ncontext dependencies. Furthermore, improvements observed with larger models can\nbe transferred to smaller models using knowledge distillation. Our experiments\nshow that this approach achieves competitive performance across several\nlanguages and benchmarks, without additional language-specific tuning and task\nspecific architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1\">Suvodeep Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauly_S/0/1/0/all/0/1\">Stanislas Lauly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadejde_M/0/1/0/all/0/1\">Maria Nadejde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_G/0/1/0/all/0/1\">Georgiana Dinu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts. (arXiv:2210.11292v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11292","description":"<p>Prompt tuning is a parameter-efficient tuning (PETuning) method for utilizing\npre-trained models (PTMs) that simply prepends a soft prompt to the input and\nonly optimizes the prompt to adapt PTMs to downstream tasks. Although it is\nparameter- and deployment-efficient, its performance still lags behind other\nstate-of-the-art PETuning methods. Besides, the training cost of prompt tuning\nis not significantly reduced due to the back-propagation through the entire\nmodel. Through empirical analyses, we shed some light on the lagging\nperformance of prompt tuning and recognize a trade-off between the propagation\ndistance from label signals to the inserted prompt and the influence of the\nprompt on model outputs. Further, we present Late Prompt Tuning (LPT) that\ninserts a late prompt into an intermediate layer of the PTM instead of the\ninput layer or all layers. The late prompt is obtained by a neural prompt\ngenerator conditioned on the hidden states before the prompt insertion layer\nand therefore is instance-dependent. Through extensive experimental results\nacross various tasks and PTMs, we show that LPT can achieve competitive\nperformance to full model tuning and other PETuning methods under both\nfull-data and few-shot scenarios while possessing faster training speed and\nlower memory cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Instruction-Finetuned Language Models. (arXiv:2210.11416v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.11416","description":"<p>Finetuning language models on a collection of datasets phrased as\ninstructions has been shown to improve model performance and generalization to\nunseen tasks. In this paper we explore instruction finetuning with a particular\nfocus on (1) scaling the number of tasks, (2) scaling the model size, and (3)\nfinetuning on chain-of-thought data. We find that instruction finetuning with\nthe above aspects dramatically improves performance on a variety of model\nclasses (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and\nevaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For\ninstance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM\n540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves\nstate-of-the-art performance on several benchmarks, such as 75.2% on five-shot\nMMLU. We also publicly release Flan-T5 checkpoints, which achieve strong\nfew-shot performance even compared to much larger models, such as PaLM 62B.\nOverall, instruction finetuning is a general method for improving the\nperformance and usability of pretrained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Eric Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1\">Siddhartha Brahma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhuyun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzgun_M/0/1/0/all/0/1\">Mirac Suzgun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1\">Gaurav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_S/0/1/0/all/0/1\">Slav Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed H. Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1\">Jeff Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_J/0/1/0/all/0/1\">Jacob Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are E2E ASR models ready for an industrial usage?. (arXiv:2112.12572v2 [eess.AS] CROSS LISTED)","link":"http://arxiv.org/abs/2112.12572","description":"<p>The Automated Speech Recognition (ASR) community experiences a major turning\npoint with the rise of the fully-neural (End-to-End, E2E) approaches. At the\nsame time, the conventional hybrid model remains the standard choice for the\npractical usage of ASR. According to previous studies, the adoption of E2E ASR\nin real-world applications was hindered by two main limitations: their ability\nto generalize on unseen domains and their high operational cost. In this paper,\nwe investigate both above-mentioned drawbacks by performing a comprehensive\nmulti-domain benchmark of several contemporary E2E models and a hybrid\nbaseline. Our experiments demonstrate that E2E models are viable alternatives\nfor the hybrid approach, and even outperform the baseline both in accuracy and\nin operational efficiency. As a result, our study shows that the generalization\nand complexity issues are no longer the major obstacle for industrial\nintegration, and draws the community's attention to other potential limitations\nof the E2E approaches in some specific use-cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vielzeuf_V/0/1/0/all/0/1\">Valentin Vielzeuf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Antipov_G/0/1/0/all/0/1\">Grigory Antipov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Natural Language Processing: Recent Advances, Challenges, and Future Directions. (arXiv:2201.00768v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2201.00768","description":"<p>Recent natural language processing (NLP) techniques have accomplished high\nperformance on benchmark datasets, primarily due to the significant improvement\nin the performance of deep learning. The advances in the research community\nhave led to great enhancements in state-of-the-art production systems for NLP\ntasks, such as virtual assistants, speech recognition, and sentiment analysis.\nHowever, such NLP systems still often fail when tested with adversarial\nattacks. The initial lack of robustness exposed troubling gaps in current\nmodels' language understanding capabilities, creating problems when NLP systems\nare deployed in real life. In this paper, we present a structured overview of\nNLP robustness research by summarizing the literature in a systemic way across\nvarious dimensions. We then take a deep-dive into the various dimensions of\nrobustness, across techniques, metrics, embeddings, and benchmarks. Finally, we\nargue that robustness should be multi-dimensional, provide insights into\ncurrent research, identify gaps in the literature to suggest directions worth\npursuing to address these gaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Omar_M/0/1/0/all/0/1\">Marwan Omar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Soohyeon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyang_D/0/1/0/all/0/1\">DaeHun Nyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohaisen_D/0/1/0/all/0/1\">David Mohaisen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}