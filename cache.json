{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-22T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])","link":"http://arxiv.org/abs/2303.11366","description":"<p>Recent advancements in decision-making large language model (LLM) agents have\ndemonstrated impressive performance across various benchmarks. However, these\nstate-of-the-art approaches typically necessitate internal model fine-tuning,\nexternal model fine-tuning, or policy optimization over a defined state space.\nImplementing these methods can prove challenging due to the scarcity of\nhigh-quality training data or the lack of well-defined state space. Moreover,\nthese agents do not possess certain qualities inherent to human decision-making\nprocesses, specifically the ability to learn from mistakes. Self-reflection\nallows humans to efficiently solve novel problems through a process of trial\nand error. Building on recent research, we propose Reflexion, an approach that\nendows an agent with dynamic memory and self-reflection capabilities to enhance\nits existing reasoning trace and task-specific action choice abilities. To\nachieve full automation, we introduce a straightforward yet effective heuristic\nthat enables the agent to pinpoint hallucination instances, avoid repetition in\naction sequences, and, in some environments, construct an internal memory map\nof the given environment. To assess our approach, we evaluate the agent's\nability to complete decision-making tasks in AlfWorld environments and\nknowledge-intensive, search-based question-and-answer tasks in HotPotQA\nenvironments. We observe success rates of 97% and 51%, respectively, and\nprovide a discussion on the emergent property of self-reflection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shinn_N/0/1/0/all/0/1\">Noah Shinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labash_B/0/1/0/all/0/1\">Beck Labash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopinath_A/0/1/0/all/0/1\">Ashwin Gopinath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action. (arXiv:2303.11381v1 [cs.CV])","link":"http://arxiv.org/abs/2303.11381","description":"<p>We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of\nvision experts to achieve multimodal reasoning and action. In this paper, we\ndefine and explore a comprehensive list of advanced vision tasks that are\nintriguing to solve, but may exceed the capabilities of existing vision and\nvision-language models. To achieve such advanced visual intelligence, MM-REACT\nintroduces a textual prompt design that can represent text descriptions,\ntextualized spatial coordinates, and aligned file names for dense visual\nsignals such as images and videos. MM-REACT's prompt design allows language\nmodels to accept, associate, and process multimodal information, thereby\nfacilitating the synergetic combination of ChatGPT and various vision experts.\nZero-shot experiments demonstrate MM-REACT's effectiveness in addressing the\nspecified capabilities of interests and its wide application in different\nscenarios that require advanced visual understanding. Furthermore, we discuss\nand compare MM-REACT's system paradigm with an alternative approach that\nextends language models for multimodal scenarios through joint finetuning.\nCode, demo, video, and visualization are available at\nhttps://multimodal-react.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azarnasab_E/0/1/0/all/0/1\">Ehsan Azarnasab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Faisal Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v1 [cs.CV])","link":"http://arxiv.org/abs/2303.11403","description":"<p>Large Language Models (LLMs) have so far impressed the world, with\nunprecedented capabilities that emerge in models at large scales. On the vision\nside, transformer models (i.e., ViT) are following the same trend, achieving\nthe best performance on challenging benchmarks. With the abundance of such\nunimodal models, a natural question arises; do we need also to follow this\ntrend to tackle multimodal tasks? In this work, we propose to rather direct\neffort to efficient adaptations of existing models, and propose to augment\nLanguage Models with perception. Existing approaches for adapting pretrained\nmodels for vision-language tasks still rely on several key components that\nhinder their efficiency. In particular, they still train a large number of\nparameters, rely on large multimodal pretraining, use encoders (e.g., CLIP)\ntrained on huge image-text datasets, and add significant inference overhead. In\naddition, most of these approaches have focused on Zero-Shot and In Context\nLearning, with little to no effort on direct finetuning. We investigate the\nminimal computational effort needed to adapt unimodal models for multimodal\ntasks and propose a new challenging setup, alongside different approaches, that\nefficiently adapts unimodal pretrained models. We show that by freezing more\nthan 99\\% of total parameters, training only one linear projection layer, and\nprepending only one trainable token, our approach (dubbed eP-ALM) significantly\noutperforms other baselines on VQA and Captioning across Image, Video, and\nAudio modalities, following the proposed setup. The code will be available\nhere: https://github.com/mshukor/eP-ALM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1\">Mustafa Shukor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dancette_C/0/1/0/all/0/1\">Corentin Dancette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind meets machine: Unravelling GPT-4's cognitive psychology. (arXiv:2303.11436v1 [cs.CL])","link":"http://arxiv.org/abs/2303.11436","description":"<p>Commonsense reasoning is a basic ingredient of intelligence in humans,\nempowering the ability to deduce conclusions based on the observations of\nsurroundings. Large language models (LLMs) are emerging as potent tools\nincreasingly capable of performing human-level tasks. The recent development in\nthe form of GPT-4 and its demonstrated success in tasks complex to humans such\nas medical exam, bar exam and others has led to an increased confidence in the\nLLMs to become perfect instruments of intelligence. Though, the GPT-4 paper has\nshown performance on some common sense reasoning tasks, a comprehensive\nassessment of GPT-4 on common sense reasoning tasks, particularly on the\nexisting well-established datasets is missing. In this study, we focus on the\nevaluation of GPT-4's performance on a set of common sense reasoning questions\nfrom the widely used CommonsenseQA dataset along with tools from cognitive\npsychology. In doing so, we understand how GPT-4 processes and integrates\ncommon sense knowledge with contextual information, providing insight into the\nunderlying cognitive processes that enable its ability to generate common sense\nresponses. We show that GPT-4 exhibits a high level of accuracy in answering\ncommon sense questions, outperforming its predecessor, GPT-3 and GPT-3.5. We\nshow that the accuracy of GPT-4 on CommonSenseQA is 83 % and it has been shown\nin the original study that human accuracy over the same data was 89 %.\nAlthough, GPT-4 falls short of the human performance, it is a substantial\nimprovement from the original 56.5 % in the original language model used by the\nCommonSenseQA study. Our results strengthen the already available assessments\nand confidence on GPT-4's common sense reasoning abilities which have\nsignificant potential to revolutionize the field of AI, by enabling machines to\nbridge the gap between human and machine reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sifatkaur/0/1/0/all/0/1\">Sifatkaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Manmeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+SB_V/0/1/0/all/0/1\">Vaisakh SB</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malviya_N/0/1/0/all/0/1\">Neetiraj Malviya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimizing Fuzzy Interpretations in Fuzzy Description Logics by Using Crisp Bisimulations. (arXiv:2303.11438v1 [cs.AI])","link":"http://arxiv.org/abs/2303.11438","description":"<p>The problem of minimizing finite fuzzy interpretations in fuzzy description\nlogics (FDLs) is worth studying. For example, the structure of a fuzzy/weighted\nsocial network can be treated as a fuzzy interpretation in FDLs, where actors\nare individuals and actions are roles. Minimizing the structure of a\nfuzzy/weighted social network makes it more compact, thus making network\nanalysis tasks more efficient. In this work, we study the problem of minimizing\na finite fuzzy interpretation in a FDL by using the largest crisp\nauto-bisimulation. The considered FDLs use the Baaz projection operator and\ntheir semantics is specified using an abstract algebra of fuzzy truth values,\nwhich can be any linear and complete residuated lattice. We provide an\nefficient algorithm with a complexity of $O((m \\log{l} + n) \\log{n})$ for\nminimizing a given finite fuzzy interpretation $\\mathcal{I}$, where $n$ is the\nsize of the domain of $\\mathcal{I}$, $m$ is number of nonzero instances of\natomic roles of $\\mathcal{I}$ and $l$ is the number of different fuzzy values\nused for instances of atomic roles of $\\mathcal{I}$. We prove that the fuzzy\ninterpretation returned by the algorithm is minimal among the ones that\npreserve fuzzy TBoxes and ABoxes under certain conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Linh Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models and Simple, Stupid Bugs. (arXiv:2303.11455v1 [cs.SE])","link":"http://arxiv.org/abs/2303.11455","description":"<p>With the advent of powerful neural language models, AI-based systems to\nassist developers in coding tasks are becoming widely available; Copilot is one\nsuch system. Copilot uses Codex, a large language model (LLM), to complete code\nconditioned on a preceding \"prompt\". Codex, however, is trained on public\nGitHub repositories, viz., on code that may include bugs and vulnerabilities.\nPrevious studies [1], [2] show Codex reproduces vulnerabilities seen in\ntraining. In this study, we examine how prone Codex is to generate an\ninteresting bug category, single statement bugs, commonly referred to as\nsimple, stupid bugs or SStuBs in the MSR community. We find that Codex and\nsimilar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs\nas much as 2x as likely than known, verbatim correct code. We explore the\nconsequences of the Codex generated SStuBs and propose avoidance strategies\nthat suggest the possibility of reducing the production of known, verbatim\nSStubs, and increase the possibility of producing known, verbatim fixes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jesse_K/0/1/0/all/0/1\">Kevin Jesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_T/0/1/0/all/0/1\">Toufique Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devanbu_P/0/1/0/all/0/1\">Premkumar T. Devanbu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_E/0/1/0/all/0/1\">Emily Morgan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Behavior: A Comprehensive Survey. (arXiv:2303.11504v1 [cs.CL])","link":"http://arxiv.org/abs/2303.11504","description":"<p>Transformer language models have received widespread public attention, yet\ntheir generated text is often surprising even to NLP researchers. In this\nsurvey, we discuss over 250 recent studies of English language model behavior\nbefore task-specific fine-tuning. Language models possess basic capabilities in\nsyntax, semantics, pragmatics, world knowledge, and reasoning, but these\ncapabilities are sensitive to specific inputs and surface features. Despite\ndramatic increases in generated text quality as models scale to hundreds of\nbillions of parameters, the models are still prone to unfactual responses,\ncommonsense errors, memorized text, and social biases. Many of these weaknesses\ncan be framed as over-generalizations or under-generalizations of learned\npatterns in text. We synthesize recent results to highlight what is currently\nknown about what large language models can and cannot do.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tyler A. Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency. (arXiv:2303.11525v1 [cs.LG])","link":"http://arxiv.org/abs/2303.11525","description":"<p>Recent works have explored the use of weight sparsity to improve the training\nefficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs).\nThese works aim to reduce training FLOPs but training with sparse weights often\nleads to accuracy loss or requires longer train schedules, making the resulting\ntraining efficiency less clear. In contrast, we focus on using sparsity to\nincrease accuracy while using the same FLOPS as the dense model and show\ntraining efficiency gains through higher accuracy. In this work, we introduce\nSIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in\nreplacements for dense layers to improve their representational capacity and\nFLOP efficiency. Each transformation is parameterized by a single parameter\n(sparsity level) and provides a larger search space to find optimal sparse\nmasks. Without changing any training hyperparameters, replacing dense layers\nwith SIFT leads to significant improvements across computer vision (CV) and\nnatural language processing (NLP) tasks, including ResNet-18 on ImageNet\n(+3.5%) and GPT-3 Small on WikiText-103 (-0.4 PPL), both matching larger dense\nmodel variants with 2x or more FLOPs. To the best of our knowledge, this is the\nfirst work to demonstrate the use of sparsity for improving accuracy of dense\nmodels via a simple-to-use set of sparse transformations. Code is available at:\nhttps://github.com/CerebrasResearch/SIFT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Shreyas Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thangarasa_V/0/1/0/all/0/1\">Vithursan Thangarasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lie_S/0/1/0/all/0/1\">Sean Lie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Difficulty in learning chirality for Transformer fed with SMILES. (arXiv:2303.11593v1 [cs.LG])","link":"http://arxiv.org/abs/2303.11593","description":"<p>Recent years have seen development of descriptor generation based on\nrepresentation learning of extremely diverse molecules, especially those that\napply natural language processing (NLP) models to SMILES, a literal\nrepresentation of molecular structure. However, little research has been done\non how these models understand chemical structure. To address this, we\ninvestigated the relationship between the learning progress of SMILES and\nchemical structure using a representative NLP model, the Transformer. The\nresults suggest that while the Transformer learns partial structures of\nmolecules quickly, it requires extended training to understand overall\nstructures. Consistently, the accuracy of molecular property predictions using\ndescriptors generated from models at different learning steps was similar from\nthe beginning to the end of training. Furthermore, we found that the\nTransformer requires particularly long training to learn chirality and\nsometimes stagnates with low translation accuracy due to misunderstanding of\nenantiomers. These findings are expected to deepen understanding of NLP models\nin chemistry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoshikai_Y/0/1/0/all/0/1\">Yasuhiro Yoshikai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizuno_T/0/1/0/all/0/1\">Tadahaya Mizuno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemoto_S/0/1/0/all/0/1\">Shumpei Nemoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusuhara_H/0/1/0/all/0/1\">Hiroyuki Kusuhara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers in Speech Processing: A Survey. (arXiv:2303.11607v1 [cs.CL])","link":"http://arxiv.org/abs/2303.11607","description":"<p>The remarkable success of transformers in the field of natural language\nprocessing has sparked the interest of the speech-processing community, leading\nto an exploration of their potential for modeling long-range dependencies\nwithin speech sequences. Recently, transformers have gained prominence across\nvarious speech-related domains, including automatic speech recognition, speech\nsynthesis, speech translation, speech para-linguistics, speech enhancement,\nspoken dialogue systems, and numerous multimodal applications. In this paper,\nwe present a comprehensive survey that aims to bridge research studies from\ndiverse subfields within speech technology. By consolidating findings from\nacross the speech technology landscape, we provide a valuable resource for\nresearchers interested in harnessing the power of transformers to advance the\nfield. We identify the challenges encountered by transformers in speech\nprocessing while also offering insights into potential solutions to address\nthese issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Latif_S/0/1/0/all/0/1\">Siddique Latif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_A/0/1/0/all/0/1\">Aun Zaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuayahuitl_H/0/1/0/all/0/1\">Heriberto Cuayahuitl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamshad_F/0/1/0/all/0/1\">Fahad Shamshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoukat_M/0/1/0/all/0/1\">Moazzam Shoukat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qadir_J/0/1/0/all/0/1\">Junaid Qadir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heterogeneous-Branch Collaborative Learning for Dialogue Generation. (arXiv:2303.11621v1 [cs.CL])","link":"http://arxiv.org/abs/2303.11621","description":"<p>With the development of deep learning, advanced dialogue generation methods\nusually require a greater amount of computational resources. One promising\napproach to obtaining a high-performance and lightweight model is knowledge\ndistillation, which relies heavily on the pre-trained powerful teacher.\nCollaborative learning, also known as online knowledge distillation, is an\neffective way to conduct one-stage group distillation in the absence of a\nwell-trained large teacher model. However, previous work has a severe branch\nhomogeneity problem due to the same training objective and the independent\nidentical training sets. To alleviate this problem, we consider the dialogue\nattributes in the training of network branches. Each branch learns the\nattribute-related features based on the selected subset. Furthermore, we\npropose a dual group-based knowledge distillation method, consisting of\npositive distillation and negative distillation, to further diversify the\nfeatures of different branches in a steadily and interpretable way. The\nproposed approach significantly improves branch heterogeneity and outperforms\nstate-of-the-art collaborative learning methods on two widely used open-domain\ndialogue datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shaoxiong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Content Retrievability in Search with Controllable Query Generation. (arXiv:2303.11648v1 [cs.IR])","link":"http://arxiv.org/abs/2303.11648","description":"<p>An important goal of online platforms is to enable content discovery, i.e.\nallow users to find a catalog entity they were not familiar with. A\npre-requisite to discover an entity, e.g. a book, with a search engine is that\nthe entity is retrievable, i.e. there are queries for which the system will\nsurface such entity in the top results. However, machine-learned search engines\nhave a high retrievability bias, where the majority of the queries return the\nsame entities. This happens partly due to the predominance of narrow intent\nqueries, where users create queries using the title of an already known entity,\ne.g. in book search 'harry potter'. The amount of broad queries where users\nwant to discover new entities, e.g. in music search 'chill lyrical electronica\nwith an atmospheric feeling to it', and have a higher tolerance to what they\nmight find, is small in comparison. We focus here on two factors that have a\nnegative impact on the retrievability of the entities (I) the training data\nused for dense retrieval models and (II) the distribution of narrow and broad\nintent queries issued in the system. We propose CtrlQGen, a method that\ngenerates queries for a chosen underlying intent-narrow or broad. We can use\nCtrlQGen to improve factor (I) by generating training data for dense retrieval\nmodels comprised of diverse synthetic queries. CtrlQGen can also be used to\ndeal with factor (II) by suggesting queries with broader intents to users. Our\nresults on datasets from the domains of music, podcasts, and books reveal that\nwe can significantly decrease the retrievability bias of a dense retrieval\nmodel when using CtrlQGen. First, by using the generated queries as training\ndata for dense models we make 9% of the entities retrievable (go from zero to\nnon-zero retrievability). Second, by suggesting broader queries to users, we\ncan make 12% of the entities retrievable in the best case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Penha_G/0/1/0/all/0/1\">Gustavo Penha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palumbo_E/0/1/0/all/0/1\">Enrico Palumbo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_M/0/1/0/all/0/1\">Maryam Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alice Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouchard_H/0/1/0/all/0/1\">Hugues Bouchard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Yet Effective Synthetic Dataset Construction for Unsupervised Opinion Summarization. (arXiv:2303.11660v1 [cs.CL])","link":"http://arxiv.org/abs/2303.11660","description":"<p>Opinion summarization provides an important solution for summarizing opinions\nexpressed among a large number of reviews. However, generating aspect-specific\nand general summaries is challenging due to the lack of annotated data. In this\nwork, we propose two simple yet effective unsupervised approaches to generate\nboth aspect-specific and general opinion summaries by training on synthetic\ndatasets constructed with aspect-related review contents. Our first approach,\nSeed Words Based Leave-One-Out (SW-LOO), identifies aspect-related portions of\nreviews simply by exact-matching aspect seed words and outperforms existing\nmethods by 3.4 ROUGE-L points on SPACE and 0.5 ROUGE-1 point on OPOSUM+ for\naspect-specific opinion summarization. Our second approach, Natural Language\nInference Based Leave-One-Out (NLI-LOO) identifies aspect-related sentences\nutilizing an NLI model in a more general setting without using seed words and\noutperforms existing approaches by 1.2 ROUGE-L points on SPACE for\naspect-specific opinion summarization and remains competitive on other metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Ming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jie Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_Y/0/1/0/all/0/1\">Yogarshi Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_K/0/1/0/all/0/1\">Kalpit Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1\">Miguel Ballesteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benajiba_Y/0/1/0/all/0/1\">Yassine Benajiba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Open-domain Paradox for Chatbots: Common Ground as the Basis for Human-like Dialogue. (arXiv:2303.11708v1 [cs.CL])","link":"http://arxiv.org/abs/2303.11708","description":"<p>There is a surge in interest in the development of open-domain chatbots,\ndriven by the recent advancements of large language models. The \"openness\" of\nthe dialogue is expected to be maximized by providing minimal information to\nthe users about the common ground they can expect, including the presumed joint\nactivity. However, evidence suggests that the effect is the opposite. Asking\nusers to \"just chat about anything\" results in a very narrow form of dialogue,\nwhich we refer to as the \"open-domain paradox\". In this paper, we explain this\nparadox through the theory of common ground as the basis for human-like\ncommunication. Furthermore, we question the assumptions behind open-domain\nchatbots and identify paths forward for enabling common ground in\nhuman-computer dialogue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skantze_G/0/1/0/all/0/1\">Gabriel Skantze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogruoz_A/0/1/0/all/0/1\">A. Seza Do&#x11f;ru&#xf6;z</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEAPT: Learning Adaptive Prefix-to-prefix Translation For Simultaneous Machine Translation. (arXiv:2303.11750v1 [cs.CL])","link":"http://arxiv.org/abs/2303.11750","description":"<p>Simultaneous machine translation, which aims at a real-time translation, is\nuseful in many live scenarios but very challenging due to the trade-off between\naccuracy and latency. To achieve the balance for both, the model needs to wait\nfor appropriate streaming text (READ policy) and then generates its translation\n(WRITE policy). However, WRITE policies of previous work either are specific to\nthe method itself due to the end-to-end training or suffer from the input\nmismatch between training and decoding for the non-end-to-end training.\nTherefore, it is essential to learn a generic and better WRITE policy for\nsimultaneous machine translation. Inspired by strategies utilized by human\ninterpreters and \"wait\" policies, we propose a novel adaptive prefix-to-prefix\ntraining policy called LEAPT, which allows our machine translation model to\nlearn how to translate source sentence prefixes and make use of the future\ncontext. Experiments show that our proposed methods greatly outperform\ncompetitive baselines and achieve promising results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuangtao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaodong Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Intermediate English Learners outdid ChatGPT in deep cohesion: Evidence from English narrative writing. (arXiv:2303.11812v1 [cs.CL])","link":"http://arxiv.org/abs/2303.11812","description":"<p>ChatGPT is a publicly available chatbot that can quickly generate texts on\ngiven topics, but it is unknown whether the chatbot is really superior to human\nwriters in all aspects of writing and whether its writing quality can be\nprominently improved on the basis of updating commands. Consequently, this\nstudy compared the writing performance on a narrative topic by ChatGPT and\nChinese intermediate English (CIE) learners so as to reveal the chatbot's\nadvantage and disadvantage in writing. The data were analyzed in terms of five\ndiscourse components using Coh-Metrix (a special instrument for analyzing\nlanguage discourses), and the results revealed that ChatGPT performed better\nthan human writers in narrativity, word concreteness, and referential cohesion,\nbut worse in syntactic simplicity and deep cohesion in its initial version.\nAfter more revision commands were updated, while the resulting version was\nfacilitated in syntactic simplicity, yet it is still lagged far behind CIE\nlearners' writing in deep cohesion. In addition, the correlation analysis of\nthe discourse components suggests that narrativity was correlated with\nreferential cohesion in both ChatGPT and human writers, but the correlations\nvaried within each group.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tongquan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Siyi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Siruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_A/0/1/0/all/0/1\">Aijing He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Cross-Domain Rumor Detection with Contrastive Learning and Cross-Attention. (arXiv:2303.11945v1 [cs.SI])","link":"http://arxiv.org/abs/2303.11945","description":"<p>Massive rumors usually appear along with breaking news or trending topics,\nseriously hindering the truth. Existing rumor detection methods are mostly\nfocused on the same domain, and thus have poor performance in cross-domain\nscenarios due to domain shift. In this work, we propose an end-to-end\ninstance-wise and prototype-wise contrastive learning model with a\ncross-attention mechanism for cross-domain rumor detection. The model not only\nperforms cross-domain feature alignment but also enforces target samples to\nalign with the corresponding prototypes of a given source domain. Since target\nlabels in a target domain are unavailable, we use a clustering-based approach\nwith carefully initialized centers by a batch of source domain samples to\nproduce pseudo labels. Moreover, we use a cross-attention mechanism on a pair\nof source data and target data with the same labels to learn domain-invariant\nrepresentations. Because the samples in a domain pair tend to express similar\nsemantic patterns, especially on the people's attitudes (e.g., supporting or\ndenying) towards the same category of rumors, the discrepancy between a pair of\nthe source domain and target domain will be decreased. We conduct experiments\non four groups of cross-domain datasets and show that our proposed model\nachieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ran_H/0/1/0/all/0/1\">Hongyan Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Caiyan Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logical Reasoning over Natural Language as Knowledge Representation: A Survey. (arXiv:2303.12023v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12023","description":"<p>Logical reasoning is central to human cognition and intelligence. Past\nresearch of logical reasoning within AI uses formal language as knowledge\nrepresentation~(and symbolic reasoners). However, reasoning with formal\nlanguage has proved challenging~(e.g., brittleness and knowledge-acquisition\nbottleneck). This paper provides a comprehensive overview on a new paradigm of\nlogical reasoning, which uses natural language as knowledge representation~(and\npretrained language models as reasoners), including philosophical definition\nand categorization of logical reasoning, advantages of the new paradigm,\nbenchmarks and methods, challenges of the new paradigm, desirable tasks &amp;\nmethods in the future, and relation to related NLP fields. This new paradigm is\npromising since it not only alleviates many challenges of formal representation\nbut also has advantages over end-to-end neural methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zonglin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xinya Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1\">Rui Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jinjie Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"cTBL: Augmenting Large Language Models for Conversational Tables. (arXiv:2303.12024v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12024","description":"<p>An open challenge in multimodal conversational AI requires augmenting large\nlanguage models with information from textual and non-textual sources for\nmulti-turn dialogue. To address this problem, this paper introduces\nConversational Tables (cTBL), a three-step encoder-decoder approach to retrieve\ntabular information and generate dialogue responses grounded on the retrieved\ninformation. cTBL uses Transformer encoder embeddings for Dense Table Retrieval\nand obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over\nsparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs\ntabular knowledge retrieval using both encoder and decoder models, resulting in\nup to 46% relative improvement in ROUGE scores and better human evaluation for\nresponse generation on HyrbiDialogue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundar_A/0/1/0/all/0/1\">Anirudh S Sundar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_L/0/1/0/all/0/1\">Larry Heck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wearing Masks Implies Refuting Trump?: Towards Target-specific User Stance Prediction across Events in COVID-19 and US Election 2020. (arXiv:2303.12029v1 [cs.SI])","link":"http://arxiv.org/abs/2303.12029","description":"<p>People who share similar opinions towards controversial topics could form an\necho chamber and may share similar political views toward other topics as well.\nThe existence of such connections, which we call connected behavior, gives\nresearchers a unique opportunity to predict how one would behave for a future\nevent given their past behaviors. In this work, we propose a framework to\nconduct connected behavior analysis. Neural stance detection models are trained\non Twitter data collected on three seemingly independent topics, i.e., wearing\na mask, racial equality, and Trump, to detect people's stance, which we\nconsider as their online behavior in each topic-related event. Our results\nreveal a strong connection between the stances toward the three topical events\nand demonstrate the power of past behaviors in predicting one's future\nbehavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Haewoon Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1\">Jisun An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grading Conversational Responses Of Chatbots. (arXiv:2303.12038v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12038","description":"<p>Chatbots have long been capable of answering basic questions and even\nresponding to obscure prompts, but recently their improvements have been far\nmore significant. Modern chatbots like Open AIs ChatGPT3 not only have the\nability to answer basic questions but can write code and movie scripts and\nimitate well-known people. In this paper, we analyze ChatGPTs' responses to\nvarious questions from a dataset of queries from the popular Quora forum. We\nsubmitted sixty questions to ChatGPT and scored the answers based on three\nindustry-standard metrics for grading machine translation: BLEU, METEOR, and\nROUGE. These metrics allow us to compare the machine responses with the most\nupvoted human answer to the same question to assess ChatGPT's ability to submit\na humanistic reply. The results showed that while the responses and translation\nabilities of ChatGPT are remarkable, they still fall short of what a typical\nhuman reaction would be.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosario_G/0/1/0/all/0/1\">Grant Rosario</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting. (arXiv:2303.12057v1 [cs.CY])","link":"http://arxiv.org/abs/2303.12057","description":"<p>The mass aggregation of knowledge embedded in large language models (LLMs)\nholds the promise of new solutions to problems of observability and measurement\nin the social sciences. We examine the utility of one such model for a\nparticularly difficult measurement task: measuring the latent ideology of\nlawmakers, which allows us to better understand functions that are core to\ndemocracy, such as how politics shape policy and how political actors represent\ntheir constituents. We scale the senators of the 116th United States Congress\nalong the liberal-conservative spectrum by prompting ChatGPT to select the more\nliberal (or conservative) senator in pairwise comparisons. We show that the LLM\nproduced stable answers across repeated iterations, did not hallucinate, and\nwas not simply regurgitating information from a single source. This new scale\nstrongly correlates with pre-existing liberal-conservative scales such as\nNOMINATE, but also differs in several important ways, such as correctly placing\nsenators who vote against their party for far-left or far-right ideological\nreasons on the extreme ends. The scale also highly correlates with ideological\nmeasures based on campaign giving and political activists' perceptions of these\nsenators. In addition to the potential for better-automated data collection and\ninformation retrieval, our results suggest LLMs are likely to open new avenues\nfor measuring latent constructs like ideology that rely on aggregating large\nquantities of data from public sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Patrick Y. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_J/0/1/0/all/0/1\">Joshua A. Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagler_J/0/1/0/all/0/1\">Jonathan Nagler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messing_S/0/1/0/all/0/1\">Solomon Messing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoXum: Cross-modal Visual and Textural Summarization of Videos. (arXiv:2303.12060v1 [cs.CV])","link":"http://arxiv.org/abs/2303.12060","description":"<p>Video summarization aims to distill the most important information from a\nsource video to produce either an abridged clip or a textual narrative.\nTraditionally, different methods have been proposed depending on whether the\noutput is a video or text, thus ignoring the correlation between the two\nsemantically related tasks of visual summarization and textual summarization.\nWe propose a new joint video and text summarization task. The goal is to\ngenerate both a shortened video clip along with the corresponding textual\nsummary from a long video, collectively referred to as a cross-modal summary.\nThe generated shortened video clip and text narratives should be semantically\nwell aligned. To this end, we first build a large-scale human-annotated dataset\n-- VideoXum (X refers to different modalities). The dataset is reannotated\nbased on ActivityNet. After we filter out the videos that do not meet the\nlength requirements, 14,001 long videos remain in our new dataset. Each video\nin our reannotated dataset has human-annotated video summaries and the\ncorresponding narrative summaries. We then design a novel end-to-end model --\nVTSUM-BILP to address the challenges of our proposed task. Moreover, we propose\na new metric called VT-CLIPScore to help evaluate the semantic consistency of\ncross-modality summary. The proposed model achieves promising performance on\nthis new task and establishes a benchmark for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jingyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_H/0/1/0/all/0/1\">Hang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Ming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_J/0/1/0/all/0/1\">Jenhao Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1\">Chiuman Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracking, exploring and analyzing recent developments in German-language online press in the face of the coronavirus crisis: cOWIDplus Analysis and cOWIDplus Viewer. (arXiv:2005.13316v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.13316","description":"<p>The coronavirus pandemic may be the largest crisis the world has had to face\nsince World War II. It does not come as a surprise that it is also having an\nimpact on language as our primary communication tool. We present three\ninter-connected resources that are designed to capture and illustrate these\neffects on a subset of the German language: An RSS corpus of German-language\nnewsfeeds (with freely available untruncated unigram frequency lists), a static\nbut continuously updated HTML page tracking the diversity of the used\nvocabulary and a web application that enables other researchers and the broader\npublic to explore these effects without any or with little knowledge of corpus\nrepresentation/exploration or statistical analyses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolfer_S/0/1/0/all/0/1\">Sascha Wolfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koplenig_A/0/1/0/all/0/1\">Alexander Koplenig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michaelis_F/0/1/0/all/0/1\">Frank Michaelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Spitzer_C/0/1/0/all/0/1\">Carolin M&#xfc;ller-Spitzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing. (arXiv:2111.09543v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.09543","description":"<p>This paper presents a new pre-trained language model, DeBERTaV3, which\nimproves the original DeBERTa model by replacing mask language modeling (MLM)\nwith replaced token detection (RTD), a more sample-efficient pre-training task.\nOur analysis shows that vanilla embedding sharing in ELECTRA hurts training\nefficiency and model performance. This is because the training losses of the\ndiscriminator and the generator pull token embeddings in different directions,\ncreating the \"tug-of-war\" dynamics. We thus propose a new gradient-disentangled\nembedding sharing method that avoids the tug-of-war dynamics, improving both\ntraining efficiency and the quality of the pre-trained model. We have\npre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its\nexceptional performance on a wide range of downstream natural language\nunderstanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an\nexample, the DeBERTaV3 Large model achieves a 91.37% average score, which is\n1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art\n(SOTA) among the models with a similar structure. Furthermore, we have\npre-trained a multi-lingual model mDeBERTa and observed a larger improvement\nover strong baselines compared to English models. For example, the mDeBERTa\nBase achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6%\nimprovement over XLM-R Base, creating a new SOTA on this benchmark. We have\nmade our pre-trained models and inference code publicly available at\nhttps://github.com/microsoft/DeBERTa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-trained Token-replaced Detection Model as Few-shot Learner. (arXiv:2203.03235v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03235","description":"<p>Pre-trained masked language models have demonstrated remarkable ability as\nfew-shot learners. In this paper, as an alternative, we propose a novel\napproach to few-shot learning with pre-trained token-replaced detection models\nlike ELECTRA. In this approach, we reformulate a classification or a regression\ntask as a token-replaced detection problem. Specifically, we first define a\ntemplate and label description words for each task and put them into the input\nto form a natural language prompt. Then, we employ the pre-trained\ntoken-replaced detection model to predict which label description word is the\nmost original (i.e., least replaced) among all label description words in the\nprompt. A systematic evaluation on 16 datasets demonstrates that our approach\noutperforms few-shot learners with pre-trained masked language models in both\none-sentence and two-sentence learning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zicheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shoushan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guodong Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Token-level Contrastive Framework for Sign Language Translation. (arXiv:2204.04916v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04916","description":"<p>Sign Language Translation (SLT) is a promising technology to bridge the\ncommunication gap between the deaf and the hearing people. Recently,\nresearchers have adopted Neural Machine Translation (NMT) methods, which\nusually require large-scale corpus for training, to achieve SLT. However, the\npublicly available SLT corpus is very limited, which causes the collapse of the\ntoken representations and the inaccuracy of the generated tokens. To alleviate\nthis issue, we propose ConSLT, a novel token-level \\textbf{Con}trastive\nlearning framework for \\textbf{S}ign \\textbf{L}anguage \\textbf{T}ranslation ,\nwhich learns effective token representations by incorporating token-level\ncontrastive learning into the SLT decoding process. Concretely, ConSLT treats\neach token and its counterpart generated by different dropout masks as positive\npairs during decoding, and then randomly samples $K$ tokens in the vocabulary\nthat are not in the current sentence to construct negative examples. We conduct\ncomprehensive experiments on two benchmarks (PHOENIX14T and CSL-Daily) for both\nend-to-end and cascaded settings. The experimental results demonstrate that\nConSLT can achieve better translation quality than the strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Biao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Peigen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Cong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yidong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaodong Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Inclusivity, Equity, and Accessibility of NLP Technology: A Case Study for Indian Languages. (arXiv:2205.12676v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12676","description":"<p>In order for NLP technology to be widely applicable, fair, and useful, it\nneeds to serve a diverse set of speakers across the world's languages, be\nequitable, i.e., not unduly biased towards any particular language, and be\ninclusive of all users, particularly in low-resource settings where compute\nconstraints are common. In this paper, we propose an evaluation paradigm that\nassesses NLP technologies across all three dimensions. While diversity and\ninclusion have received attention in recent literature, equity is currently\nunexplored. We propose to address this gap using the Gini coefficient, a\nwell-established metric used for estimating societal wealth inequality. Using\nour paradigm, we highlight the distressed state of current technologies for\nIndian (IN) languages (a linguistically large and diverse set, with a varied\nspeaker population), across all three dimensions. To improve upon these\nmetrics, we demonstrate the importance of region-specific choices in model\nbuilding and dataset creation, and more importantly, propose a novel,\ngeneralisable approach to optimal resource allocation during fine-tuning.\nFinally, we discuss steps to mitigate these biases and encourage the community\nto employ multi-faceted evaluation when building linguistically diverse and\nequitable technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanuja_S/0/1/0/all/0/1\">Simran Khanuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Maximum Linear Arrangement Problem for trees under projectivity and planarity. (arXiv:2206.06924v5 [cs.DS] UPDATED)","link":"http://arxiv.org/abs/2206.06924","description":"<p>A linear arrangement is a mapping $\\pi$ from the $n$ vertices of a graph $G$\nto $n$ distinct consecutive integers. Linear arrangements can be represented by\ndrawing the vertices along a horizontal line and drawing the edges as\nsemicircles above said line. In this setting, the length of an edge is defined\nas the absolute value of the difference between the positions of its two\nvertices in the arrangement, and the cost of an arrangement as the sum of all\nedge lengths. Here we study two variants of the Maximum Linear Arrangement\nproblem (MaxLA), which consists of finding an arrangement that maximizes the\ncost. In the planar variant for free trees, vertices have to be arranged in\nsuch a way that there are no edge crossings. In the projective variant for\nrooted trees, arrangements have to be planar and the root of the tree cannot be\ncovered by any edge. In this paper we present algorithms that are linear in\ntime and space to solve planar and projective MaxLA for trees. We also prove\nseveral properties of maximum projective and planar arrangements, and show that\ncaterpillar trees maximize planar MaxLA over all trees of a fixed size thereby\ngeneralizing a previous extremal result on trees.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1\">Juan Luis Esteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Covertly Unsafe Text within Natural Language Systems. (arXiv:2210.09306v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2210.09306","description":"<p>An increasingly prevalent problem for intelligent technologies is text\nsafety, as uncontrolled systems may generate recommendations to their users\nthat lead to injury or life-threatening consequences. However, the degree of\nexplicitness of a generated statement that can cause physical harm varies. In\nthis paper, we distinguish types of text that can lead to physical harm and\nestablish one particularly underexplored category: covertly unsafe text. Then,\nwe further break down this category with respect to the system's information\nand discuss solutions to mitigate the generation of text in each of these\nsubcategories. Ultimately, our work defines the problem of covertly unsafe\nlanguage that causes physical harm and argues that this subtle yet dangerous\nissue needs to be prioritized by stakeholders and regulators. We highlight\nmitigation strategies to inspire future researchers to tackle this challenging\nproblem and help improve safety within smart systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_A/0/1/0/all/0/1\">Alex Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_A/0/1/0/all/0/1\">Anisha Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subbiah_M/0/1/0/all/0/1\">Melanie Subbiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allaway_E/0/1/0/all/0/1\">Emily Allaway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Judge_J/0/1/0/all/0/1\">John Judge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patton_D/0/1/0/all/0/1\">Desmond Patton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimber_B/0/1/0/all/0/1\">Bruce Bimber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collecting Interactive Multi-modal Datasets for Grounded Language Understanding. (arXiv:2211.06552v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.06552","description":"<p>Human intelligence can remarkably adapt quickly to new tasks and\nenvironments. Starting from a very young age, humans acquire new skills and\nlearn how to solve new tasks either by imitating the behavior of others or by\nfollowing provided natural language instructions. To facilitate research which\ncan enable similar capabilities in machines, we made the following\ncontributions (1) formalized the collaborative embodied agent using natural\nlanguage task; (2) developed a tool for extensive and scalable data collection;\nand (3) collected the first dataset for interactive grounded language\nunderstanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1\">Shrestha Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabzadeh_N/0/1/0/all/0/1\">Negar Arabzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teruel_M/0/1/0/all/0/1\">Milagro Teruel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zholus_A/0/1/0/all/0/1\">Artem Zholus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrynnik_A/0/1/0/all/0/1\">Alexey Skrynnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail Burtsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinet_K/0/1/0/all/0/1\">Kavya Srinet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1\">Aleksandr Panov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3. (arXiv:2211.09699v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.09699","description":"<p>Knowledge-based visual question answering (VQA) involves questions that\nrequire world knowledge beyond the image to yield the correct answer. Large\nlanguage models (LMs) like GPT-3 are particularly helpful for this task because\nof their strong knowledge retrieval and reasoning capabilities. To enable LM to\nunderstand images, prior work uses a captioning model to convert images into\ntext. However, when summarizing an image in a single caption sentence, which\nvisual entities to describe are often underspecified. Generic image captions\noften miss visual details essential for the LM to answer visual questions\ncorrectly. To address this challenge, we propose PromptCap (Prompt-guided image\nCaptioning), a captioning model designed to serve as a better connector between\nimages and black-box LMs. Different from generic captions, PromptCap takes a\nnatural-language prompt to control the visual entities to describe in the\ngenerated caption. The prompt contains a question that the caption should aid\nin answering. To avoid extra annotation, PromptCap is trained by examples\nsynthesized with GPT-3 and existing datasets. We demonstrate PromptCap's\neffectiveness on an existing pipeline in which GPT-3 is prompted with image\ncaptions to carry out VQA. PromptCap outperforms generic captions by a large\nmargin and achieves state-of-the-art accuracy on knowledge-based VQA tasks\n(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that\nPromptCap generalizes well to unseen domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yushi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_H/0/1/0/all/0/1\">Hang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I Can't Believe There's No Images! Learning Visual Tasks Using only Language Data. (arXiv:2211.09778v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.09778","description":"<p>Many high-level skills that are required for computer vision tasks, such as\nparsing questions, comparing and contrasting semantics, and writing\ndescriptions, are also required in other domains such as natural language\nprocessing. In this paper, we ask whether it is possible to learn those skills\nfrom textual data and then transfer them to vision tasks without ever training\non visual training data. Key to our approach is exploiting the joint embedding\nspace of contrastively trained vision and language encoders. In practice, there\ncan be systematic differences between embedding spaces for different modalities\nin contrastive models, and we analyze how these differences affect our approach\nand study strategies to mitigate this concern. We produce models using only\ntext training data on four representative tasks: image captioning, visual\nentailment, visual question answering and visual news, and evaluate them on\nstandard benchmarks using images. We find these models generally perform close\nto models trained on images, while surpassing prior work for captioning and\nvisual entailment in this text only setting by over 9 points, and outperforming\nall prior work on visual news by over 30 points. We also showcase a variety of\nstylistic image captioning models that are trained using no image data and no\nhuman-curated language data, but instead using readily-available text data from\nbooks, the web, or language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Sophia Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_C/0/1/0/all/0/1\">Christopher Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counteracts: Testing Stereotypical Representation in Pre-trained Language Models. (arXiv:2301.04347v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.04347","description":"<p>Language models have demonstrated strong performance on various natural\nlanguage understanding tasks. Similar to humans, language models could also\nhave their own bias that is learned from the training data. As more and more\ndownstream tasks integrate language models as part of the pipeline, it is\nnecessary to understand the internal stereotypical representation and the\nmethods to mitigate the negative effects. In this paper, we proposed a simple\nmethod to test the internal stereotypical representation in pre-trained\nlanguage models using counterexamples. We mainly focused on gender bias, but\nthe method can be extended to other types of bias. We evaluated models on 9\ndifferent cloze-style prompts consisting of knowledge and base prompts. Our\nresults indicate that pre-trained language models show a certain amount of\nrobustness when using unrelated knowledge, and prefer shallow linguistic cues,\nsuch as word position and syntactic structure, to alter the internal\nstereotypical representation. Such findings shed light on how to manipulate\nlanguage models in a neutral approach for both finetuning and evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Damin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning. (arXiv:2302.14115v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2302.14115","description":"<p>In this work, we introduce Vid2Seq, a multi-modal single-stage dense event\ncaptioning model pretrained on narrated videos which are readily-available at\nscale. The Vid2Seq architecture augments a language model with special time\ntokens, allowing it to seamlessly predict event boundaries and textual\ndescriptions in the same output sequence. Such a unified model requires\nlarge-scale training data, which is not available in current annotated\ndatasets. We show that it is possible to leverage unlabeled narrated videos for\ndense video captioning, by reformulating sentence boundaries of transcribed\nspeech as pseudo event boundaries, and using the transcribed speech sentences\nas pseudo event captions. The resulting Vid2Seq model pretrained on the\nYT-Temporal-1B dataset improves the state of the art on a variety of dense\nvideo captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.\nVid2Seq also generalizes well to the tasks of video paragraph captioning and\nvideo clip captioning, and to few-shot settings. Our code is publicly available\nat https://antoyang.github.io/vid2seq.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_P/0/1/0/all/0/1\">Paul Hongsuck Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pont_Tuset_J/0/1/0/all/0/1\">Jordi Pont-Tuset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QAID: Question Answering Inspired Few-shot Intent Detection. (arXiv:2303.01593v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.01593","description":"<p>Intent detection with semantically similar fine-grained intents is a\nchallenging task. To address it, we reformulate intent detection as a\nquestion-answering retrieval task by treating utterances and intent names as\nquestions and answers. To that end, we utilize a question-answering retrieval\narchitecture and adopt a two stages training schema with batch contrastive\nloss. In the pre-training stage, we improve query representations through\nself-supervised training. Then, in the fine-tuning stage, we increase\ncontextualized token-level similarity scores between queries and answers from\nthe same intent. Our results on three few-shot intent detection benchmarks\nachieve state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_A/0/1/0/all/0/1\">Asaf Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetzler_M/0/1/0/all/0/1\">Matan Vetzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mass_Y/0/1/0/all/0/1\">Yosi Mass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazar_K/0/1/0/all/0/1\">Koren Lazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1\">Doron Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmeli_B/0/1/0/all/0/1\">Boaz Carmeli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?. (arXiv:2303.05382v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.05382","description":"<p>ChatGPT, developed by OpenAI, is one of the milestone large language models\n(LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive\nlanguage understanding capability of LLM, particularly in generating\nconversational response. As LLMs start to gain more attention in various\nresearch or engineering domains, it is time to envision how LLM may\nrevolutionize the way we approach intelligent transportation systems. This\npaper explores the future applications of LLM in addressing key transportation\nproblems. By leveraging LLM with cross-modal encoder, an intelligent system can\nalso process traffic data from different modalities and execute transportation\noperations through an LLM. We present and validate these potential\ntransportation applications equipped by LLM. To further demonstrate this\npotential, we also provide a concrete smartphone-based crash report\nauto-generation and analysis framework as a use case. Despite the potential\nbenefits, challenges related to data privacy, data quality, and model bias must\nbe considered. Overall, the use of LLM in intelligent transport systems holds\npromise for more efficient, intelligent, and sustainable transportation systems\nthat further improve daily life around the world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_O/0/1/0/all/0/1\">Ou Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdel_Aty_M/0/1/0/all/0/1\">Mohamed Abdel-Aty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongdong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shengxuan Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Efficient Learning of Natural Language to Linear Temporal Logic Translators for Robot Task Specification. (arXiv:2303.08006v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08006","description":"<p>To make robots accessible to a broad audience, it is critical to endow them\nwith the ability to take universal modes of communication, like commands given\nin natural language, and extract a concrete desired task specification, defined\nusing a formal language like linear temporal logic (LTL). In this paper, we\npresent a learning-based approach for translating from natural language\ncommands to LTL specifications with very limited human-labeled training data.\nThis is in stark contrast to existing natural-language to LTL translators,\nwhich require large human-labeled datasets, often in the form of labeled pairs\nof LTL formulas and natural language commands, to train the translator. To\nreduce reliance on human data, our approach generates a large synthetic\ntraining dataset through algorithmic generation of LTL formulas, conversion to\nstructured English, and then exploiting the paraphrasing capabilities of modern\nlarge language models (LLMs) to synthesize a diverse corpus of natural language\ncommands corresponding to the LTL formulas. We use this generated data to\nfinetune an LLM and apply a constrained decoding procedure at inference time to\nensure the returned LTL formula is syntactically correct. We evaluate our\napproach on three existing LTL/natural language datasets and show that we can\ntranslate natural language commands at 75\\% accuracy with far less human data\n($\\le$12 annotations). Moreover, when training on large human-annotated\ndatasets, our method achieves higher test accuracy (95\\% on average) than prior\nwork. Finally, we show the translated formulas can be used to plan\nlong-horizon, multi-stage tasks on a 12D quadrotor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jiayi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_G/0/1/0/all/0/1\">Glen Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berenson_D/0/1/0/all/0/1\">Dmitry Berenson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLEN: General-Purpose Event Detection for Thousands of Types. (arXiv:2303.09093v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.09093","description":"<p>The development of event extraction systems has been hindered by the absence\nof wide-coverage, large-scale datasets. To make event extraction systems more\naccessible, we build a general-purpose event detection dataset GLEN, which\ncovers 3,465 different event types, making it over 20x larger in ontology than\nany current dataset. GLEN is created by utilizing the DWD Overlay, which\nprovides a mapping between Wikidata Qnodes and PropBank rolesets. This enables\nus to use the abundant existing annotation for PropBank as distant supervision.\nIn addition, we also propose a new multi-stage event detection model\nspecifically designed to handle the large ontology size and partial labels in\nGLEN. We show that our model exhibits superior performance (~10% F1 gain)\ncompared to both conventional classification baselines and newer\ndefinition-based models. Finally, we perform error analysis and show that label\nnoise is still the largest challenge for improving performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Q/0/1/0/all/0/1\">Qiusi Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conger_K/0/1/0/all/0/1\">Kathryn Conger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1\">Martha Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning. (arXiv:2303.10475v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.10475","description":"<p>Task semantics can be expressed by a set of input-to-output examples or a\npiece of textual instruction. Conventional machine learning approaches for\nnatural language processing (NLP) mainly rely on the availability of\nlarge-scale sets of task-specific examples. Two issues arise: first, collecting\ntask-specific labeled examples does not apply to scenarios where tasks may be\ntoo complicated or costly to annotate, or the system is required to handle a\nnew task immediately; second, this is not user-friendly since end-users are\nprobably more willing to provide task description rather than a set of examples\nbefore using the system. Therefore, the community is paying increasing interest\nin a new supervision-seeking paradigm for NLP: learning from task instructions.\nDespite its impressive progress, there are some common issues that the\ncommunity struggles with. This survey paper tries to summarize the current\nresearch on instruction learning, particularly, by answering the following\nquestions: (i) what is task instruction, and what instruction types exist? (ii)\nhow to model instructions? (iii) what factors influence and explain the\ninstructions' performance? (iv) what challenges remain in instruction learning?\nTo our knowledge, this is the first comprehensive survey about textual\ninstructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1\">Renze Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocRED-FE: A Document-Level Fine-Grained Entity And Relation Extraction Dataset. (arXiv:2303.11141v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.11141","description":"<p>Joint entity and relation extraction (JERE) is one of the most important\ntasks in information extraction. However, most existing works focus on\nsentence-level coarse-grained JERE, which have limitations in real-world\nscenarios. In this paper, we construct a large-scale document-level\nfine-grained JERE dataset DocRED-FE, which improves DocRED with Fine-Grained\nEntity Type. Specifically, we redesign a hierarchical entity type schema\nincluding 11 coarse-grained types and 119 fine-grained types, and then\nre-annotate DocRED manually according to this schema. Through comprehensive\nexperiments we find that: (1) DocRED-FE is challenging to existing JERE models;\n(2) Our fine-grained entity types promote relation classification. We make\nDocRED-FE with instruction and the code for our baselines publicly available at\nhttps://github.com/PKU-TANGENT/DOCRED-FE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Weimin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dawei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yu Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}