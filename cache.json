{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Detect Depression from Social Networks with Sentiment Knowledge Sharing. (arXiv:2306.14903v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14903","description":"<p>Social network plays an important role in propagating people's viewpoints,\nemotions, thoughts, and fears. Notably, following lockdown periods during the\nCOVID-19 pandemic, the issue of depression has garnered increasing attention,\nwith a significant portion of individuals resorting to social networks as an\noutlet for expressing emotions. Using deep learning techniques to discern\npotential signs of depression from social network messages facilitates the\nearly identification of mental health conditions. Current efforts in detecting\ndepression through social networks typically rely solely on analyzing the\ntextual content, overlooking other potential information. In this work, we\nconduct a thorough investigation that unveils a strong correlation between\ndepression and negative emotional states. The integration of such associations\nas external knowledge can provide valuable insights for detecting depression.\nAccordingly, we propose a multi-task training framework, DeSK, which utilizes\nshared sentiment knowledge to enhance the efficacy of depression detection.\nExperiments conducted on both Chinese and English datasets demonstrate the\ncross-lingual effectiveness of DeSK.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_C/0/1/0/all/0/1\">Chengwei Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chunyan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qianqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengzhu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pengyuan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRISMA-DFLLM: An Extension of PRISMA for Systematic Literature Reviews using Domain-specific Finetuned Large Language Models. (arXiv:2306.14905v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14905","description":"<p>With the proliferation of open-sourced Large Language Models (LLMs) and\nefficient finetuning techniques, we are on the cusp of the emergence of\nnumerous domain-specific LLMs that have been finetuned for expertise across\nspecialized fields and applications for which the current general-purpose LLMs\nare unsuitable. In academia, this technology has the potential to revolutionize\nthe way we conduct systematic literature reviews (SLRs), access knowledge and\ngenerate new insights. This paper proposes an AI-enabled methodological\nframework that combines the power of LLMs with the rigorous reporting\nguidelines of the Preferred Reporting Items for Systematic Reviews and\nMeta-Analyses (PRISMA). By finetuning LLMs on domain-specific academic papers\nthat have been selected as a result of a rigorous SLR process, the proposed\nPRISMA-DFLLM (for Domain-specific Finetuned LLMs) reporting guidelines offer\nthe potential to achieve greater efficiency, reusability and scalability, while\nalso opening the potential for conducting incremental living systematic reviews\nwith the aid of LLMs. Additionally, the proposed approach for leveraging LLMs\nfor SLRs enables the dissemination of finetuned models, empowering researchers\nto accelerate advancements and democratize cutting-edge research. This paper\npresents the case for the feasibility of finetuned LLMs to support rigorous\nSLRs and the technical requirements for realizing this. This work then proposes\nthe extended PRISMA-DFLLM checklist of reporting guidelines as well as the\nadvantages, challenges, and potential implications of implementing\nPRISMA-DFLLM. Finally, a future research roadmap to develop this line of\nAI-enabled SLRs is presented, paving the way for a new era of evidence\nsynthesis and knowledge discovery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Susnjak_T/0/1/0/all/0/1\">Teo Susnjak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clickbait Classification and Spoiling Using Natural Language Processing. (arXiv:2306.14907v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14907","description":"<p>Clickbait is the practice of engineering titles to incentivize readers to\nclick through to articles. Such titles with sensationalized language reveal as\nlittle information as possible. Occasionally, clickbait will be intentionally\nmisleading, so natural language processing (NLP) can scan the article and\nanswer the question posed by the clickbait title, or spoil it. We tackle two\ntasks: classifying the clickbait into one of 3 types (Task 1), and spoiling the\nclickbait (Task 2). For Task 1, we propose two binary classifiers to determine\nthe final spoiler type. For Task 2, we experiment with two approaches: using a\nquestion-answering model to identify the span of text of the spoiler, and using\na large language model (LLM) to generate the spoiler. Because the spoiler is\ncontained in the article, we frame the second task as a question-answering\napproach for identifying the starting and ending positions of the spoiler. We\ncreated models for Task 1 that were better than the baselines proposed by the\ndataset authors and engineered prompts for Task 2 that did not perform as well\nas the baselines proposed by the dataset authors due to the evaluation metric\nperforming worse when the output text is from a generative model as opposed to\nan extractive model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thirumala_A/0/1/0/all/0/1\">Adhitya Thirumala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferracane_E/0/1/0/all/0/1\">Elisa Ferracane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Importance of Human-Labeled Data in the Era of LLMs. (arXiv:2306.14910v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14910","description":"<p>The advent of large language models (LLMs) has brought about a revolution in\nthe development of tailored machine learning models and sparked debates on\nredefining data requirements. The automation facilitated by the training and\nimplementation of LLMs has led to discussions and aspirations that human-level\nlabeling interventions may no longer hold the same level of importance as in\nthe era of supervised learning. This paper presents compelling arguments\nsupporting the ongoing relevance of human-labeled data in the era of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"You might think about slightly revising the title\": identifying hedges in peer-tutoring interactions. (arXiv:2306.14911v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14911","description":"<p>Hedges play an important role in the management of conversational\ninteraction. In peer tutoring, they are notably used by tutors in dyads (pairs\nof interlocutors) experiencing low rapport to tone down the impact of\ninstructions and negative feedback. Pursuing the objective of building a\ntutoring agent that manages rapport with students in order to improve learning,\nwe used a multimodal peer-tutoring dataset to construct a computational\nframework for identifying hedges. We compared approaches relying on pre-trained\nresources with others that integrate insights from the social science\nliterature. Our best performance involved a hybrid approach that outperforms\nthe existing baseline while being easier to interpret. We employ a model\nexplainability tool to explore the features that characterize hedges in\npeer-tutoring conversations, and we identify some novel features, and the\nbenefits of such a hybrid model approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raphalen_Y/0/1/0/all/0/1\">Yann Raphalen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chlo&#xe9; Clavel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassell_J/0/1/0/all/0/1\">Justine Cassell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FSUIE: A Novel Fuzzy Span Mechanism for Universal Information Extraction. (arXiv:2306.14913v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14913","description":"<p>Universal Information Extraction (UIE) has been introduced as a unified\nframework for various Information Extraction (IE) tasks and has achieved\nwidespread success. Despite this, UIE models have limitations. For example,\nthey rely heavily on span boundaries in the data during training, which does\nnot reflect the reality of span annotation challenges. Slight adjustments to\npositions can also meet requirements. Additionally, UIE models lack attention\nto the limited span length feature in IE. To address these deficiencies, we\npropose the Fuzzy Span Universal Information Extraction (FSUIE) framework.\nSpecifically, our contribution consists of two concepts: fuzzy span loss and\nfuzzy span attention. Our experimental results on a series of main IE tasks\nshow significant improvement compared to the baseline, especially in terms of\nfast convergence and strong performance with small amounts of data and training\nepochs. These results demonstrate the effectiveness and generalization of FSUIE\nin different tasks, settings, and scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1\">Tianshuo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lefei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Enriched Controllability for Educational Question Generation. (arXiv:2306.14917v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14917","description":"<p>Question Generation (QG) is a task within Natural Language Processing (NLP)\nthat involves automatically generating questions given an input, typically\ncomposed of a text and a target answer. Recent work on QG aims to control the\ntype of generated questions so that they meet educational needs. A remarkable\nexample of controllability in educational QG is the generation of questions\nunderlying certain narrative elements, e.g., causal relationship, outcome\nresolution, or prediction. This study aims to enrich controllability in QG by\nintroducing a new guidance attribute: question explicitness. We propose to\ncontrol the generation of explicit and implicit wh-questions from\nchildren-friendly stories. We show preliminary evidence of controlling QG via\nquestion explicitness alone and simultaneously with another target attribute:\nthe question's narrative element. The code is publicly available at\ngithub.com/bernardoleite/question-generation-control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leite_B/0/1/0/all/0/1\">Bernardo Leite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_H/0/1/0/all/0/1\">Henrique Lopes Cardoso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utilizing Natural Language Processing for Automated Assessment of Classroom Discussion. (arXiv:2306.14918v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14918","description":"<p>Rigorous and interactive class discussions that support students to engage in\nhigh-level thinking and reasoning are essential to learning and are a central\ncomponent of most teaching interventions. However, formally assessing\ndiscussion quality 'at scale' is expensive and infeasible for most researchers.\nIn this work, we experimented with various modern natural language processing\n(NLP) techniques to automatically generate rubric scores for individual\ndimensions of classroom text discussion quality. Specifically, we worked on a\ndataset of 90 classroom discussion transcripts consisting of over 18000 turns\nannotated with fine-grained Analyzing Teaching Moves (ATM) codes and focused on\nfour Instructional Quality Assessment (IQA) rubrics. Despite the limited amount\nof data, our work shows encouraging results in some of the rubrics while\nsuggesting that there is room for improvement in the others. We also found that\ncertain NLP approaches work better for certain rubrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1\">Nhat Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierce_B/0/1/0/all/0/1\">Benjamin Pierce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correnti_R/0/1/0/all/0/1\">Richard Correnti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumura_L/0/1/0/all/0/1\">Lindsay Clare Matsumura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Product Information Extraction using ChatGPT. (arXiv:2306.14921v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14921","description":"<p>Structured product data in the form of attribute/value pairs is the\nfoundation of many e-commerce applications such as faceted product search,\nproduct comparison, and product recommendation. Product offers often only\ncontain textual descriptions of the product attributes in the form of titles or\nfree text. Hence, extracting attribute/value pairs from textual product\ndescriptions is an essential enabler for e-commerce applications. In order to\nexcel, state-of-the-art product information extraction methods require large\nquantities of task-specific training data. The methods also struggle with\ngeneralizing to out-of-distribution attributes and attribute values that were\nnot a part of the training data. Due to being pre-trained on huge amounts of\ntext as well as due to emergent effects resulting from the model size, Large\nLanguage Models like ChatGPT have the potential to address both of these\nshortcomings. This paper explores the potential of ChatGPT for extracting\nattribute/value pairs from product descriptions. We experiment with different\nzero-shot and few-shot prompt designs. Our results show that ChatGPT achieves a\nperformance similar to a pre-trained language model but requires much smaller\namounts of training data and computation for fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brinkmann_A/0/1/0/all/0/1\">Alexander Brinkmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shraga_R/0/1/0/all/0/1\">Roee Shraga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Der_R/0/1/0/all/0/1\">Reng Chiz Der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bizer_C/0/1/0/all/0/1\">Christian Bizer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding. (arXiv:2306.14924v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14924","description":"<p>Deductive coding is a widely used qualitative research method for determining\nthe prevalence of themes across documents. While useful, deductive coding is\noften burdensome and time consuming since it requires researchers to read,\ninterpret, and reliably categorize a large body of unstructured text documents.\nLarge language models (LLMs), like ChatGPT, are a class of quickly evolving AI\ntools that can perform a range of natural language processing and reasoning\ntasks. In this study, we explore the use of LLMs to reduce the time it takes\nfor deductive coding while retaining the flexibility of a traditional content\nanalysis. We outline the proposed approach, called LLM-assisted content\nanalysis (LACA), along with an in-depth case study using GPT-3.5 for LACA on a\npublicly available deductive coding data set. Additionally, we conduct an\nempirical benchmark using LACA on 4 publicly available data sets to assess the\nbroader question of how well GPT-3.5 performs across a range of deductive\ncoding tasks. Overall, we find that GPT-3.5 can often perform deductive coding\nat levels of agreement comparable to human coders. Additionally, we demonstrate\nthat LACA can help refine prompts for deductive coding, identify codes for\nwhich an LLM is randomly guessing, and help assess when to use LLMs vs. human\ncoders for deductive coding. We conclude with several implications for future\npractice of deductive coding and related research methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chew_R/0/1/0/all/0/1\">Robert Chew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollenbacher_J/0/1/0/all/0/1\">John Bollenbacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenger_M/0/1/0/all/0/1\">Michael Wenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speer_J/0/1/0/all/0/1\">Jessica Speer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Annice Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Bidirectional Long Short-Term Memory with Subword Embedding for Authorship Attribution. (arXiv:2306.14933v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14933","description":"<p>The problem of unveiling the author of a given text document from multiple\ncandidate authors is called authorship attribution. Manifold word-based\nstylistic markers have been successfully used in deep learning methods to deal\nwith the intrinsic problem of authorship attribution. Unfortunately, the\nperformance of word-based authorship attribution systems is limited by the\nvocabulary of the training corpus. Literature has recommended character-based\nstylistic markers as an alternative to overcome the hidden word problem.\nHowever, character-based methods often fail to capture the sequential\nrelationship of words in texts which is a chasm for further improvement. The\nquestion addressed in this paper is whether it is possible to address the\nambiguity of hidden words in text documents while preserving the sequential\ncontext of words. Consequently, a method based on bidirectional long short-term\nmemory (BLSTM) with a 2-dimensional convolutional neural network (CNN) is\nproposed to capture sequential writing styles for authorship attribution. The\nBLSTM was used to obtain the sequential relationship among characteristics\nusing subword information. The 2-dimensional CNN was applied to understand the\nlocal syntactical position of the style from unlabeled input text. The proposed\nmethod was experimentally evaluated against numerous state-of-the-art methods\nacross the public corporal of CCAT50, IMDb62, Blog50, and Twitter50.\nExperimental results indicate accuracy improvement of 1.07\\%, and 0.96\\% on\nCCAT50 and Twitter, respectively, and produce comparable results on the\nremaining datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Modupe_A/0/1/0/all/0/1\">Abiodun Modupe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celik_T/0/1/0/all/0/1\">Turgay Celik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olugbara_O/0/1/0/all/0/1\">Oludayo O. Olugbara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Art of Embedding Fusion: Optimizing Hate Speech Detection. (arXiv:2306.14939v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14939","description":"<p>Hate speech detection is a challenging natural language processing task that\nrequires capturing linguistic and contextual nuances. Pre-trained language\nmodels (PLMs) offer rich semantic representations of text that can improve this\ntask. However there is still limited knowledge about ways to effectively\ncombine representations across PLMs and leverage their complementary strengths.\nIn this work, we shed light on various combination techniques for several PLMs\nand comprehensively analyze their effectiveness. Our findings show that\ncombining embeddings leads to slight improvements but at a high computational\ncost and the choice of combination has marginal effect on the final outcome. We\nalso make our codebase public at\nhttps://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Aflah Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1\">Neemesh Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1\">Mohit Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Sanyam Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome. (arXiv:2306.15006v1 [q-bio.GN])","link":"http://arxiv.org/abs/2306.15006","description":"<p>Decoding the linguistic intricacies of the genome is a crucial problem in\nbiology, and pre-trained foundational models such as DNABERT and Nucleotide\nTransformer have made significant strides in this area. Existing works have\nlargely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the\ntoken of the genome language due to its simplicity. However, we argue that the\ncomputation and sample inefficiencies introduced by k-mer tokenization are\nprimary obstacles in developing large genome foundational models. We provide\nconceptual and empirical insights into genome tokenization, building on which\nwe propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a\nstatistics-based data compression algorithm that constructs tokens by\niteratively merging the most frequent co-occurring genome segment in the\ncorpus. We demonstrate that BPE not only overcomes the limitations of k-mer\ntokenization but also benefits from the computational efficiency of\nnon-overlapping tokenization. Based on these insights, we introduce DNABERT-2,\na refined genome foundation model that adapts an efficient tokenizer and\nemploys multiple strategies to overcome input length constraints, reduce time\nand memory expenditure, and enhance model capability. Furthermore, we identify\nthe absence of a comprehensive and standardized benchmark for genome\nunderstanding as another significant impediment to fair comparative analysis.\nIn response, we propose the Genome Understanding Evaluation (GUE), a\ncomprehensive multi-species genome classification dataset that amalgamates $28$\ndistinct datasets across $7$ tasks, with input lengths ranging from $70$ to\n$1000$. Through comprehensive experiments on the GUE benchmark, we demonstrate\nthat DNABERT-2 achieves comparable performance to the state-of-the-art model\nwith $21 \\times$ fewer parameters and approximately $56 \\times$ less GPU time\nin pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhihan Zhou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ji_Y/0/1/0/all/0/1\">Yanrong Ji</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_W/0/1/0/all/0/1\">Weijian Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Dutta_P/0/1/0/all/0/1\">Pratik Dutta</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Davuluri_R/0/1/0/all/0/1\">Ramana Davuluri</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. (arXiv:2306.15063v1 [cs.LG])","link":"http://arxiv.org/abs/2306.15063","description":"<p>Pretrained transformers exhibit the remarkable ability of in-context learning\n(ICL): they can learn tasks from just a few examples provided in the prompt\nwithout updating any weights. This raises a foundational question: can ICL\nsolve fundamentally $\\textit{new}$ tasks that are very different from those\nseen during pretraining? To probe this question, we examine ICL's performance\non linear regression while varying the diversity of tasks in the pretraining\ndataset. We empirically demonstrate a $\\textit{task diversity threshold}$ for\nthe emergence of ICL. Below this threshold, the pretrained transformer cannot\nsolve unseen regression tasks as it behaves like a Bayesian estimator with the\n$\\textit{non-diverse pretraining task distribution}$ as the prior. Beyond this\nthreshold, the transformer significantly outperforms this estimator; its\nbehavior aligns with that of ridge regression, corresponding to a Gaussian\nprior over $\\textit{all tasks}$, including those not seen during pretraining.\nThese results highlight that, when pretrained on data with task diversity\ngreater than the threshold, transformers $\\textit{can}$ solve fundamentally new\ntasks in-context. Importantly, this capability hinges on it deviating from the\nBayes optimal estimator with the pretraining distribution as the prior. This\nstudy underscores, in a concrete example, the critical role of task diversity,\nalongside data and model scale, in the emergence of ICL. Code is available at\nhttps://github.com/mansheej/icl-task-diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raventos_A/0/1/0/all/0/1\">Allan Ravent&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_M/0/1/0/all/0/1\">Mansheej Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Surya Ganguli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models. (arXiv:2306.15087v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15087","description":"<p>We present WinoQueer: a benchmark specifically designed to measure whether\nlarge language models (LLMs) encode biases that are harmful to the LGBTQ+\ncommunity. The benchmark is community-sourced, via application of a novel\nmethod that generates a bias benchmark from a community survey. We apply our\nbenchmark to several popular LLMs and find that off-the-shelf models generally\ndo exhibit considerable anti-queer bias. Finally, we show that LLM bias against\na marginalized community can be somewhat mitigated by finetuning on data\nwritten about or by members of that community, and that social media text\nwritten by community members is more effective than news text written about the\ncommunity by non-members. Our method for community-in-the-loop benchmark\ndevelopment provides a blueprint for future researchers to develop\ncommunity-driven, harms-grounded LLM benchmarks for other marginalized\ncommunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Felkner_V/0/1/0/all/0/1\">Virginia K. Felkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Ho-Chun Herbert Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_E/0/1/0/all/0/1\">Eugene Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding In-Context Learning via Supportive Pretraining Data. (arXiv:2306.15091v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15091","description":"<p>In-context learning (ICL) improves language models' performance on a variety\nof NLP tasks by simply demonstrating a handful of examples at inference time.\nIt is not well understood why ICL ability emerges, as the model has never been\nspecifically trained on such demonstrations. Unlike prior work that explores\nimplicit mechanisms behind ICL, we study ICL via investigating the pretraining\ndata. Specifically, we first adapt an iterative, gradient-based approach to\nfind a small subset of pretraining data that supports ICL. We observe that a\ncontinued pretraining on this small subset significantly improves the model's\nICL ability, by up to 18%. We then compare the supportive subset constrastively\nwith random subsets of pretraining data and discover: (1) The supportive\npretraining data to ICL do not have a higher domain relevance to downstream\ntasks. (2) The supportive pretraining data have a higher mass of rarely\noccurring, long-tail tokens. (3) The supportive pretraining data are\nchallenging examples where the information gain from long-range context is\nbelow average, indicating learning to incorporate difficult long-range context\nencourages ICL. Our work takes a first step towards understanding ICL via\nanalyzing instance-level pretraining data. Our insights have a potential to\nenhance the ICL ability of language models by actively guiding the construction\nof pretraining data in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaochuang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simig_D/0/1/0/all/0/1\">Daniel Simig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Dialogue Discourse Parsing. (arXiv:2306.15103v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15103","description":"<p>Dialogue discourse parsing aims to uncover the internal structure of a\nmulti-participant conversation by finding all the discourse~\\emph{links} and\ncorresponding~\\emph{relations}. Previous work either treats this task as a\nseries of independent multiple-choice problems, in which the link existence and\nrelations are decoded separately, or the encoding is restricted to only local\ninteraction, ignoring the holistic structural information. In contrast, we\npropose a principled method that improves upon previous work from two\nperspectives: encoding and decoding. From the encoding side, we perform\nstructured encoding on the adjacency matrix followed by the matrix-tree\nlearning algorithm, where all discourse links and relations in the dialogue are\njointly optimized based on latent tree-level distribution. From the decoding\nside, we perform structured inference using the modified Chiu-Liu-Edmonds\nalgorithm, which explicitly generates the labeled multi-root non-projective\nspanning tree that best captures the discourse structure. In addition, unlike\nin previous work, we do not rely on hand-crafted features; this improves the\nmodel's robustness. Experiments show that our method achieves new\nstate-of-the-art, surpassing the previous model by 2.3 on STAC and 1.5 on\nMolweni (F1 scores). \\footnote{Code released\nat~\\url{https://github.com/chijames/structured_dialogue_discourse_parsing}.}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_T/0/1/0/all/0/1\">Ta-Chung Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander I. Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FeedbackMap: a tool for making sense of open-ended survey responses. (arXiv:2306.15112v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15112","description":"<p>Analyzing open-ended survey responses is a crucial yet challenging task for\nsocial scientists, non-profit organizations, and educational institutions, as\nthey often face the trade-off between obtaining rich data and the burden of\nreading and coding textual responses. This demo introduces FeedbackMap, a\nweb-based tool that uses natural language processing techniques to facilitate\nthe analysis of open-ended survey responses. FeedbackMap lets researchers\ngenerate summaries at multiple levels, identify interesting response examples,\nand visualize the response space through embeddings. We discuss the importance\nof examining survey results from multiple perspectives and the potential biases\nintroduced by summarization methods, emphasizing the need for critical\nevaluation of the representation and omission of respondent voices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beeferman_D/0/1/0/all/0/1\">Doug Beeferman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillani_N/0/1/0/all/0/1\">Nabeel Gillani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Cross-Domain Behaviors of BERT in Review Understanding. (arXiv:2306.15123v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15123","description":"<p>Review score prediction requires review text understanding, a critical\nreal-world application of natural language processing. Due to dissimilar text\ndomains in product reviews, a common practice is fine-tuning BERT models upon\nreviews of differing domains. However, there has not yet been an empirical\nstudy of cross-domain behaviors of BERT models in the various tasks of product\nreview understanding. In this project, we investigate text classification BERT\nmodels fine-tuned on single-domain and multi-domain Amazon review data. In our\nfindings, though single-domain models achieved marginally improved performance\non their corresponding domain compared to multi-domain models, multi-domain\nmodels outperformed single-domain models when evaluated on multi-domain data,\nsingle-domain data the single-domain model was not fine-tuned on, and on\naverage when considering all tests. Though slight increases in accuracy can be\nachieved through single-domain model fine-tuning, computational resources and\ncosts can be reduced by utilizing multi-domain models that perform well across\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1\">Albert Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus. (arXiv:2306.15162v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15162","description":"<p>Machine learning for sign languages is bottlenecked by data. In this paper,\nwe present YouTube-ASL, a large-scale, open-domain corpus of American Sign\nLanguage (ASL) videos and accompanying English captions drawn from YouTube.\nWith ~1000 hours of videos and &gt;2500 unique signers, YouTube-ASL is ~3x as\nlarge and has ~10x as many unique signers as the largest prior ASL dataset. We\ntrain baseline models for ASL to English translation on YouTube-ASL and\nevaluate them on How2Sign, where we achieve a new finetuned state of the art of\n12.39 BLEU and, for the first time, report zero-shot results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uthus_D/0/1/0/all/0/1\">David Uthus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanzer_G/0/1/0/all/0/1\">Garrett Tanzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georg_M/0/1/0/all/0/1\">Manfred Georg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization. (arXiv:2306.15164v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15164","description":"<p>Adversarial training is one of the best-performing methods in improving the\nrobustness of deep language models. However, robust models come at the cost of\nhigh time consumption, as they require multi-step gradient ascents or word\nsubstitutions to obtain adversarial samples. In addition, these generated\nsamples are deficient in grammatical quality and semantic consistency, which\nimpairs the effectiveness of adversarial training. To address these problems,\nwe introduce a novel, effective procedure for instead adversarial training with\nonly clean data. Our procedure, distribution shift risk minimization (DSRM),\nestimates the adversarial loss by perturbing the input data's probability\ndistribution rather than their embeddings. This formulation results in a robust\nmodel that minimizes the expected global loss under adversarial attacks. Our\napproach requires zero adversarial samples for training and reduces time\nconsumption by up to 70\\% compared to current best-performing adversarial\ntraining methods. Experiments demonstrate that DSRM considerably improves\nBERT's resistance to textual adversarial attacks and achieves state-of-the-art\nrobust accuracy on various benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Songyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shihan Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing the gap between streaming and non-streaming Transducer-based ASR by adaptive two-stage knowledge distillation. (arXiv:2306.15171v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15171","description":"<p>Transducer is one of the mainstream frameworks for streaming speech\nrecognition. There is a performance gap between the streaming and non-streaming\ntransducer models due to limited context. To reduce this gap, an effective way\nis to ensure that their hidden and output distributions are consistent, which\ncan be achieved by hierarchical knowledge distillation. However, it is\ndifficult to ensure the distribution consistency simultaneously because the\nlearning of the output distribution depends on the hidden one. In this paper,\nwe propose an adaptive two-stage knowledge distillation method consisting of\nhidden layer learning and output layer learning. In the former stage, we learn\nhidden representation with full context by applying mean square error loss\nfunction. In the latter stage, we design a power transformation based adaptive\nsmoothness method to learn stable output distribution. It achieved 19\\%\nrelative reduction in word error rate, and a faster response for the first\ntoken compared with the original streaming model in LibriSpeech corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haitao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jiabin Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhiqiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1\">Genshun Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Ming&#x27;en Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Rank in Generative Retrieval. (arXiv:2306.15222v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15222","description":"<p>Generative retrieval is a promising new paradigm in text retrieval that\ngenerates identifier strings of relevant passages as the retrieval target. This\nparadigm leverages powerful generation models and represents a new paradigm\ndistinct from traditional learning-to-rank methods. However, despite its rapid\ndevelopment, current generative retrieval methods are still limited. They\ntypically rely on a heuristic function to transform predicted identifiers into\na passage rank list, which creates a gap between the learning objective of\ngenerative retrieval and the desired passage ranking target. Moreover, the\ninherent exposure bias problem of text generation also persists in generative\nretrieval. To address these issues, we propose a novel framework, called LTRGR,\nthat combines generative retrieval with the classical learning-to-rank\nparadigm. Our approach involves training an autoregressive model using a\npassage rank loss, which directly optimizes the autoregressive model toward the\noptimal passage ranking. This framework only requires an additional training\nstep to enhance current generative retrieval systems and does not add any\nburden to the inference stage. We conducted experiments on three public\ndatasets, and our results demonstrate that LTRGR achieves state-of-the-art\nperformance among generative retrieval methods, indicating its effectiveness\nand robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emulating Reader Behaviors for Fake News Detection. (arXiv:2306.15231v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15231","description":"<p>The wide dissemination of fake news has affected our lives in many aspects,\nmaking fake news detection important and attracting increasing attention.\nExisting approaches make substantial contributions in this field by modeling\nnews from a single-modal or multi-modal perspective. However, these modal-based\nmethods can result in sub-optimal outcomes as they ignore reader behaviors in\nnews consumption and authenticity verification. For instance, they haven't\ntaken into consideration the component-by-component reading process: from the\nheadline, images, comments, to the body, which is essential for modeling news\nwith more granularity. To this end, we propose an approach of Emulating the\nbehaviors of readers (Ember) for fake news detection on social media,\nincorporating readers' reading and verificating process to model news from the\ncomponent perspective thoroughly. Specifically, we first construct\nintra-component feature extractors to emulate the behaviors of semantic\nanalyzing on each component. Then, we design a module that comprises\ninter-component feature extractors and a sequence-based aggregator. This module\nmimics the process of verifying the correlation between components and the\noverall reading and verification sequence. Thus, Ember can handle the news with\nvarious components by emulating corresponding sequences. We conduct extensive\nexperiments on nine real-world datasets, and the results demonstrate the\nsuperiority of Ember.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Junwei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Min Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1\">Kai Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zehua Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yinqiu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jia Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C-PMI: Conditional Pointwise Mutual Information for Turn-level Dialogue Evaluation. (arXiv:2306.15245v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15245","description":"<p>Existing reference-free turn-level evaluation metrics for chatbots\ninadequately capture the interaction between the user and the system.\nConsequently, they often correlate poorly with human evaluations. To address\nthis issue, we propose a novel model-agnostic approach that leverages\nConditional Pointwise Mutual Information (C-PMI) to measure the turn-level\ninteraction between the system and the user based on a given evaluation\ndimension. Experimental results on the widely used FED dialogue evaluation\ndataset demonstrate that our approach significantly improves the correlation\nwith human judgment compared with existing evaluation systems. By replacing the\nnegative log-likelihood-based scorer with our proposed C-PMI scorer, we achieve\na relative 60.5% higher Spearman correlation on average for the FED evaluation\nmetric. Our code is publicly available at https://github.com/renll/C-PMI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidhu_M/0/1/0/all/0/1\">Mankeerat Sidhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Revanth Gangi Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generation. (arXiv:2306.15253v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15253","description":"<p>Humans talk in free-form while negotiating the expressed meanings or common\nground. Despite the impressive conversational abilities of the large generative\nlanguage models, they do not consider the individual differences in contextual\nunderstanding in a shared situated environment. In this work, we propose\nMindDial, a novel conversational framework that can generate situated free-form\nresponses to negotiate common ground. We design an explicit mind module that\ncan track three-level beliefs -- the speaker's belief, the speaker's prediction\nof the listener's belief, and the common belief based on the gap between the\nfirst two. Then the speaking act classification head will decide to continue to\ntalk, end this turn, or take task-related action. We augment a common ground\nalignment dataset MutualFriend with belief dynamics annotation, of which the\ngoal is to find a single mutual friend based on the free chat between two\nagents. Experiments show that our model with mental state modeling can resemble\nhuman responses when aligning common ground meanwhile mimic the natural human\nconversation flow. The ablation study further validates the third-level common\nbelief can aggregate information of the first and second-order beliefs and\nalign common ground more efficiently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shuwen Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GroundNLQ @ Ego4D Natural Language Queries Challenge 2023. (arXiv:2306.15255v1 [cs.CV])","link":"http://arxiv.org/abs/2306.15255","description":"<p>In this report, we present our champion solution for Ego4D Natural Language\nQueries (NLQ) Challenge in CVPR 2023. Essentially, to accurately ground in a\nvideo, an effective egocentric feature extractor and a powerful grounding model\nare required. Motivated by this, we leverage a two-stage pre-training strategy\nto train egocentric feature extractors and the grounding model on video\nnarrations, and further fine-tune the model on annotated data. In addition, we\nintroduce a novel grounding model GroundNLQ, which employs a multi-modal\nmulti-scale grounding module for effective video and text fusion and various\ntemporal intervals, especially for long videos. On the blind test set,\nGroundNLQ achieves 25.67 and 18.18 for R1@IoU=0.3 and R1@IoU=0.5, respectively,\nand surpasses all other teams by a noticeable margin. Our code will be released\nat\\url{https://github.com/houzhijian/GroundNLQ}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhijian Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Difei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Kun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">Wing-Kwong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Out-of-Distribution Evaluation of Neural NLP Models. (arXiv:2306.15261v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15261","description":"<p>Adversarial robustness, domain generalization and dataset biases are three\nactive lines of research contributing to out-of-distribution (OOD) evaluation\non neural NLP models. However, a comprehensive, integrated discussion of the\nthree research lines is still lacking in the literature. In this survey, we 1)\ncompare the three lines of research under a unifying definition; 2) summarize\nthe data-generating processes and evaluation protocols for each line of\nresearch; and 3) emphasize the challenges and opportunities for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Pretrained Language Models Derive Correct Semantics from Corrupt Subwords under Noise?. (arXiv:2306.15268v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15268","description":"<p>For Pretrained Language Models (PLMs), their susceptibility to noise has\nrecently been linked to subword segmentation. However, it is unclear which\naspects of segmentation affect their understanding. This study assesses the\nrobustness of PLMs against various disrupted segmentation caused by noise. An\nevaluation framework for subword segmentation, named Contrastive Lexical\nSemantic (CoLeS) probe, is proposed. It provides a systematic categorization of\nsegmentation corruption under noise and evaluation protocols by generating\ncontrastive datasets with canonical-noisy word pairs. Experimental results\nindicate that PLMs are unable to accurately compute word meanings if the noise\nintroduces completely different subwords, small subword fragments, or a large\nnumber of additional subwords, particularly when they are inserted within other\nsubwords.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning. (arXiv:2306.15273v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15273","description":"<p>In the field of machine reading comprehension (MRC), existing systems have\nsurpassed the average performance of human beings in many tasks like SQuAD.\nHowever, there is still a long way to go when it comes to logical reasoning.\nAlthough some methods for it have been put forward, they either are designed in\na quite complicated way or rely too much on external structures. In this paper,\nwe proposed IDOL (InDicator-Oriented Logic Pre-training), an easy-to-understand\nbut highly effective further pre-training task which logically strengthens the\npre-trained models with the help of 6 types of logical indicators and a\nlogically rich dataset LGP (LoGic Pre-training). IDOL achieves state-of-the-art\nperformance on ReClor and LogiQA, the two most representative benchmarks in\nlogical reasoning MRC, and is proven to be capable of generalizing to different\npre-trained models and other types of MRC benchmarks like RACE and SQuAD 2.0\nwhile keeping competitive general language understanding ability through\ntesting on tasks in GLUE. Besides, at the beginning of the era of large\nlanguage models, we take several of them like ChatGPT into comparison and find\nthat IDOL still shows its advantage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zihang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task. (arXiv:2306.15298v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15298","description":"<p>Pretrained language models are publicly available and constantly finetuned\nfor various real-life applications. As they become capable of grasping complex\ncontextual information, harmful biases are likely increasingly intertwined with\nthose models. This paper analyses gender bias in BERT models with two main\ncontributions: First, a novel bias measure is introduced, defining biases as\nthe difference in sentiment valuation of female and male sample versions.\nSecond, we comprehensively analyse BERT's biases on the example of a realistic\nIMDB movie classifier. By systematically varying elements of the training\npipeline, we can conclude regarding their impact on the final model bias. Seven\ndifferent public BERT models in nine training conditions, i.e. 63 models in\ntotal, are compared. Almost all conditions yield significant gender biases.\nResults indicate that reflected biases stem from public BERT models rather than\ntask-specific data, emphasising the weight of responsible usage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jentzsch_S/0/1/0/all/0/1\">Sophie Jentzsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turan_C/0/1/0/all/0/1\">Cigdem Turan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Client Reactions in Online Mental Health Counseling. (arXiv:2306.15334v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15334","description":"<p>Communication success relies heavily on reading participants' reactions. Such\nfeedback is especially important for mental health counselors, who must\ncarefully consider the client's progress and adjust their approach accordingly.\nHowever, previous NLP research on counseling has mainly focused on studying\ncounselors' intervention strategies rather than their clients' reactions to the\nintervention. This work aims to fill this gap by developing a theoretically\ngrounded annotation framework that encompasses counselors' strategies and\nclient reaction behaviors. The framework has been tested against a large-scale,\nhigh-quality text-based counseling dataset we collected over the past two years\nfrom an online welfare counseling platform. Our study shows how clients react\nto counselors' strategies, how such reactions affect the final counseling\noutcomes, and how counselors can adjust their strategies in response to these\nreactions. We also demonstrate that this study can help counselors\nautomatically predict their clients' states.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Anqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_Y/0/1/0/all/0/1\">Yaling Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Huachuan Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhenzhong Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement. (arXiv:2306.15354v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15354","description":"<p>Disentangling uncorrelated information in speech utterances is a crucial\nresearch topic within speech community. Different speech-related tasks focus on\nextracting distinct speech representations while minimizing the affects of\nother uncorrelated information. We present a large-scale speech corpus to\nfacilitate the research of speech representation disentanglement. 3D-Speaker\ncontains over 10,000 speakers, each of whom are simultaneously recorded by\nmultiple Devices, locating at different Distances, and some speakers are\nspeaking multiple Dialects. The controlled combinations of multi-dimensional\naudio data yield a matrix of a diverse blend of speech representation\nentanglement, thereby motivating intriguing methods to untangle them. The\nmulti-domain nature of 3D-Speaker also makes it a suitable resource to evaluate\nlarge universal speech models and experiment methods of out-of-domain learning\nand self-supervised learning. https://3dspeaker.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Luyao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yafeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Architecture of a Biologically Plausible Language Organ. (arXiv:2306.15364v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15364","description":"<p>We present a simulated biologically plausible language organ, made up of\nstylized but realistic neurons, synapses, brain areas, plasticity, and a\nsimplified model of sensory perception. We show through experiments that this\nmodel succeeds in an important early step in language acquisition: the learning\nof nouns, verbs, and their meanings, from the grounded input of only a modest\nnumber of sentences. Learning in this system is achieved through Hebbian\nplasticity, and without backpropagation. Our model goes beyond a parser\npreviously designed in a similar environment, with the critical addition of a\nbiologically plausible account for how language can be acquired in the infant's\nbrain, not just processed by a mature brain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitropolsky_D/0/1/0/all/0/1\">Daniel Mitropolsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_C/0/1/0/all/0/1\">Christos H. Papadimitriou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Pseudo Future Contexts for Emotion Recognition in Conversations. (arXiv:2306.15376v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15376","description":"<p>With the extensive accumulation of conversational data on the Internet,\nemotion recognition in conversations (ERC) has received increasing attention.\nPrevious efforts of this task mainly focus on leveraging contextual and\nspeaker-specific features, or integrating heterogeneous external commonsense\nknowledge. Among them, some heavily rely on future contexts, which, however,\nare not always available in real-life scenarios. This fact inspires us to\ngenerate pseudo future contexts to improve ERC. Specifically, for an utterance,\nwe generate its future context with pre-trained language models, potentially\ncontaining extra beneficial knowledge in a conversational form homogeneous with\nthe historical ones. These characteristics make pseudo future contexts easily\nfused with historical contexts and historical speaker-specific contexts,\nyielding a conceptually simple framework systematically integrating\nmulti-contexts. Experimental results on four ERC datasets demonstrate our\nmethod's superiority. Further in-depth analyses reveal that pseudo future\ncontexts can rival real ones to some extent, especially in relatively\ncontext-independent conversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yinyi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaipeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hailei Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_T/0/1/0/all/0/1\">Tong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1\">Guanglu Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quality Estimation of Machine Translated Texts based on Direct Evidence from Training Data. (arXiv:2306.15399v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15399","description":"<p>Current Machine Translation systems achieve very good results on a growing\nvariety of language pairs and data sets. However, it is now well known that\nthey produce fluent translation outputs that often can contain important\nmeaning errors. Quality Estimation task deals with the estimation of quality of\ntranslations produced by a Machine Translation system without depending on\nReference Translations. A number of approaches have been suggested over the\nyears. In this paper we show that the parallel corpus used as training data for\ntraining the MT system holds direct clues for estimating the quality of\ntranslations produced by the MT system. Our experiments show that this simple\nand direct method holds promise for quality estimation of translations produced\nby any purely data driven machine translation system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumari_V/0/1/0/all/0/1\">Vibhuti Kumari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavi_N/0/1/0/all/0/1\">Narayana Murthy Kavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phase Space Analysis of Cardiac Spectra. (arXiv:2306.15425v1 [physics.med-ph])","link":"http://arxiv.org/abs/2306.15425","description":"<p>Cardiac diseases are one of the main reasons of mortality in modern,\nindustrialized societies, and they cause high expenses in public health\nsystems. Therefore, it is important to develop analytical methods to improve\ncardiac diagnostics. Electric activity of heart was first modeled by using a\nset of nonlinear differential equations. Latter, variations of cardiac spectra\noriginated from deterministic dynamics are investigated. Analyzing the power\nspectra of a normal human heart presents His-Purkinje network, possessing a\nfractal like structure. Phase space trajectories are extracted from the time\nseries graph of ECG. Lower values of fractal dimension, D indicate dynamics\nthat are more coherent. If D has non-integer values greater than two when the\nsystem becomes chaotic or strange attractor. Recently, the development of a\nfast and robust method, which can be applied to multichannel physiologic\nsignals, was reported. This manuscript investigates two different ECG systems\nproduced from normal and abnormal human hearts to introduce an auxiliary phase\nspace method in conjunction with ECG signals for diagnoses of heart diseases.\nHere, the data for each person includes two signals based on V_4 and modified\nlead III (MLIII) respectively. Fractal analysis method is employed on the\ntrajectories constructed in phase space, from which the fractal dimension D is\nobtained using the box counting method. It is observed that, MLIII signals have\nlarger D values than the first signals (V_4), predicting more randomness yet\nmore information. The lowest value of D (1.708) indicates the perfect\noscillation of the normal heart and the highest value of D (1.863) presents the\nrandomness of the abnormal heart. Our significant finding is that the phase\nspace picture presents the distribution of the peak heights from the ECG\nspectra, giving valuable information about heart activities in conjunction with\nECG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Pekcan_O/0/1/0/all/0/1\">Onder Pekcan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Arsan_T/0/1/0/all/0/1\">Taner Arsan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KnowPrefix-Tuning: A Two-Stage Prefix-Tuning Framework for Knowledge-Grounded Dialogue Generation. (arXiv:2306.15430v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15430","description":"<p>Existing knowledge-grounded conversation systems generate responses typically\nin a retrieve-then-generate manner. They require a large knowledge base and a\nstrong knowledge retrieval component, which is time- and resource-consuming. In\nthis paper, we address the challenge by leveraging the inherent knowledge\nencoded in the pre-trained language models (PLMs). We propose Knowledgeable\nPrefix Tuning (KnowPrefix-Tuning), a two-stage tuning framework, bypassing the\nretrieval process in a knowledge-grounded conversation system by injecting\nprior knowledge into the lightweight knowledge prefix. The knowledge prefix is\na sequence of continuous knowledge-specific vectors that can be learned during\ntraining. In addition, we propose a novel interactive re-parameterization\nmechanism that allows the prefix to interact fully with the PLM during the\noptimization of response generation. Experimental results demonstrate that\nKnowPrefix-Tuning outperforms fine-tuning and other lightweight tuning\napproaches, and performs comparably with strong retrieval-based baselines while\nbeing $3\\times$ faster during inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaqi Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinnian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are aligned neural networks adversarially aligned?. (arXiv:2306.15447v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15447","description":"<p>Large language models are now tuned to align with the goals of their\ncreators, namely to be \"helpful and harmless.\" These models should respond\nhelpfully to user questions, but refuse to answer requests that could cause\nharm. However, adversarial users can construct inputs which circumvent attempts\nat alignment. In this work, we study to what extent these models remain\naligned, even when interacting with an adversarial user who constructs\nworst-case inputs (adversarial examples). These inputs are designed to cause\nthe model to emit harmful content that would otherwise be prohibited. We show\nthat existing NLP-based optimization attacks are insufficiently powerful to\nreliably attack aligned text models: even when current NLP-based attacks fail,\nwe can find adversarial inputs with brute force. As a result, the failure of\ncurrent attacks should not be seen as proof that aligned text models remain\naligned under adversarial inputs.\n</p>\n<p>However the recent trend in large-scale ML models is multimodal models that\nallow users to provide images that influence the text that is generated. We\nshow these models can be easily attacked, i.e., induced to perform arbitrary\nun-aligned behavior through adversarial perturbation of the input image. We\nconjecture that improved NLP attacks may demonstrate this same level of\nadversarial control over text-only models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasr_M/0/1/0/all/0/1\">Milad Nasr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choquette_Choo_C/0/1/0/all/0/1\">Christopher A. Choquette-Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagielski_M/0/1/0/all/0/1\">Matthew Jagielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_I/0/1/0/all/0/1\">Irena Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_A/0/1/0/all/0/1\">Anas Awadalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1\">Pang Wei Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Social Reasoning in Language Models with Language Models. (arXiv:2306.15448v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15448","description":"<p>As Large Language Models (LLMs) become increasingly integrated into our\neveryday lives, understanding their ability to comprehend human mental states\nbecomes critical for ensuring effective interactions. However, despite the\nrecent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of\nLLMs, the degree to which these models can align with human ToM remains a\nnuanced topic of exploration. This is primarily due to two distinct challenges:\n(1) the presence of inconsistent results from previous evaluations, and (2)\nconcerns surrounding the validity of existing evaluation methodologies. To\naddress these challenges, we present a novel framework for procedurally\ngenerating evaluations with LLMs by populating causal templates. Using our\nframework, we create a new social reasoning benchmark (BigToM) for LLMs which\nconsists of 25 controls and 5,000 model-written evaluations. We find that human\nparticipants rate the quality of our benchmark higher than previous\ncrowd-sourced evaluations and comparable to expert-written evaluations. Using\nBigToM, we evaluate the social reasoning capabilities of a variety of LLMs and\ncompare model performances with human performance. Our results suggest that\nGPT4 has ToM capabilities that mirror human inference patterns, though less\nreliable, while other LLMs struggle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_K/0/1/0/all/0/1\">Kanishk Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franken_J/0/1/0/all/0/1\">Jan-Philipp Fr&#xe4;nken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstenberg_T/0/1/0/all/0/1\">Tobias Gerstenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Large Language Models to Provide Explanatory Feedback to Human Tutors. (arXiv:2306.15498v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15498","description":"<p>Research demonstrates learners engaging in the process of producing\nexplanations to support their reasoning, can have a positive impact on\nlearning. However, providing learners real-time explanatory feedback often\npresents challenges related to classification accuracy, particularly in\ndomain-specific environments, containing situationally complex and nuanced\nresponses. We present two approaches for supplying tutors real-time feedback\nwithin an online lesson on how to give students effective praise. This\nwork-in-progress demonstrates considerable accuracy in binary classification\nfor corrective feedback of effective, or effort-based (F1 score = 0.811), and\nineffective, or outcome-based (F1 score = 0.350), praise responses. More\nnotably, we introduce progress towards an enhanced approach of providing\nexplanatory feedback using large language model-facilitated named entity\nrecognition, which can provide tutors feedback, not only while engaging in\nlessons, but can potentially suggest real-time tutor moves. Future work\ninvolves leveraging large language models for data augmentation to improve\naccuracy, while also developing an explanatory feedback interface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jionghao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_D/0/1/0/all/0/1\">Danielle R. Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Feifei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shivang Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koedinger_K/0/1/0/all/0/1\">Kenneth R. Koedinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paradigm Shift in Sustainability Disclosure Analysis: Empowering Stakeholders with CHATREPORT, a Language Model-Based Tool. (arXiv:2306.15518v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15518","description":"<p>This paper introduces a novel approach to enhance Large Language Models\n(LLMs) with expert knowledge to automate the analysis of corporate\nsustainability reports by benchmarking them against the Task Force for\nClimate-Related Financial Disclosures (TCFD) recommendations. Corporate\nsustainability reports are crucial in assessing organizations' environmental\nand social risks and impacts. However, analyzing these reports' vast amounts of\ninformation makes human analysis often too costly. As a result, only a few\nentities worldwide have the resources to analyze these reports, which could\nlead to a lack of transparency. While AI-powered tools can automatically\nanalyze the data, they are prone to inaccuracies as they lack domain-specific\nexpertise. This paper introduces a novel approach to enhance LLMs with expert\nknowledge to automate the analysis of corporate sustainability reports. We\nchristen our tool CHATREPORT, and apply it in a first use case to assess\ncorporate climate risk disclosures following the TCFD recommendations.\nCHATREPORT results from collaborating with experts in climate science, finance,\neconomic policy, and computer science, demonstrating how domain experts can be\ninvolved in developing AI tools. We make our prompt templates, generated data,\nand scores available to the public to encourage transparency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingwei Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingler_J/0/1/0/all/0/1\">Julia Bingler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colesanti_Senni_C/0/1/0/all/0/1\">Chiara Colesanti-Senni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Mathias Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gostlow_G/0/1/0/all/0/1\">Glen Gostlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schimanski_T/0/1/0/all/0/1\">Tobias Schimanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stammbach_D/0/1/0/all/0/1\">Dominik Stammbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaghefi_S/0/1/0/all/0/1\">Saeid Ashraf Vaghefi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webersinke_N/0/1/0/all/0/1\">Nicolas Webersinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wekhof_T/0/1/0/all/0/1\">Tobias Wekhof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tingyu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leippold_M/0/1/0/all/0/1\">Markus Leippold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unleashing the Power of User Reviews: Exploring Airline Choices at Catania Airport, Italy. (arXiv:2306.15541v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15541","description":"<p>This study aims to investigate the possible relationship between the\nmechanisms of social influence and the choice of airline, through the use of\nnew tools, with the aim of understanding whether they can contribute to a\nbetter understanding of the factors influencing the decisions of consumers in\nthe aviation sector. We have chosen to extract user reviews from well-known\nplatforms: Trustpilot, Google, and Twitter. By combining web scraping\ntechniques, we have been able to collect a comprehensive dataset comprising a\nwide range of user opinions, feedback, and ratings. We then refined the BERT\nmodel to focus on insightful sentiment in the context of airline reviews.\nThrough our analysis, we observed an intriguing trend of average negative\nsentiment scores across various airlines, giving us deeper insight into the\ndynamics between airlines and helping us identify key partnerships, popular\nroutes, and airlines that play a central role in the aeronautical ecosystem of\nCatania airport during the specified period. Our investigation led us to find\nthat, despite an airline having received prestigious awards as a low-cost\nleader in Europe for two consecutive years 2021 and 2022, the \"Catanese\" user\ntends to suffer the dominant position of other companies. Understanding the\nimpact of positive reviews and leveraging sentiment analysis can help airlines\nimprove their reputation, attract more customers, and ultimately gain a\ncompetitive edge in the marketplace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miracula_V/0/1/0/all/0/1\">Vincenzo Miracula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picone_A/0/1/0/all/0/1\">Antonio Picone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CamemBERT-bio: a Tasty French Language Model Better for your Health. (arXiv:2306.15550v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15550","description":"<p>Clinical data in hospitals are increasingly accessible for research through\nclinical data warehouses, however these documents are unstructured. It is\ntherefore necessary to extract information from medical reports to conduct\nclinical studies. Transfer learning with BERT-like models such as CamemBERT has\nallowed major advances, especially for named entity recognition. However, these\nmodels are trained for plain language and are less efficient on biomedical\ndata. This is why we propose a new French public biomedical dataset on which we\nhave continued the pre-training of CamemBERT. Thus, we introduce a first\nversion of CamemBERT-bio, a specialized public model for the French biomedical\ndomain that shows 2.54 points of F1 score improvement on average on different\nbiomedical named entity recognition tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Touchent_R/0/1/0/all/0/1\">Rian Touchent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romary_L/0/1/0/all/0/1\">Laurent Romary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clergerie_E/0/1/0/all/0/1\">Eric de la Clergerie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrunchGPT: A chatGPT assisted framework for scientific machine learning. (arXiv:2306.15551v1 [cs.LG])","link":"http://arxiv.org/abs/2306.15551","description":"<p>Scientific Machine Learning (SciML) has advanced recently across many\ndifferent areas in computational science and engineering. The objective is to\nintegrate data and physics seamlessly without the need of employing elaborate\nand computationally taxing data assimilation schemes. However, preprocessing,\nproblem formulation, code generation, postprocessing and analysis are still\ntime consuming and may prevent SciML from wide applicability in industrial\napplications and in digital twin frameworks. Here, we integrate the various\nstages of SciML under the umbrella of ChatGPT, to formulate CrunchGPT, which\nplays the role of a conductor orchestrating the entire workflow of SciML based\non simple prompts by the user. Specifically, we present two examples that\ndemonstrate the potential use of CrunchGPT in optimizing airfoils in\naerodynamics, and in obtaining flow fields in various geometries in interactive\nmode, with emphasis on the validation stage. To demonstrate the flow of the\nCrunchGPT, and create an infrastructure that can facilitate a broader vision,\nwe built a webapp based guided user interface, that includes options for a\ncomprehensive summary report. The overall objective is to extend CrunchGPT to\nhandle diverse problems in computational mechanics, design, optimization and\ncontrols, and general scientific computing tasks involved in SciML, hence using\nit as a research assistant tool but also as an educational tool. While here the\nexamples focus in fluid mechanics, future versions will target solid mechanics\nand materials science, geophysics, systems biology and bioinformatics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleyzer_L/0/1/0/all/0/1\">Leonard Gleyzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahana_A/0/1/0/all/0/1\">Adar Kahana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_K/0/1/0/all/0/1\">Khemraj Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1\">George Em Karniadakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending Context Window of Large Language Models via Positional Interpolation. (arXiv:2306.15595v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15595","description":"<p>We present Position Interpolation (PI) that extends the context window sizes\nof RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal\nfine-tuning (within 1000 steps), while demonstrating strong empirical results\non various tasks that require long context, including passkey retrieval,\nlanguage modeling, and long document summarization from LLaMA 7B to 65B.\nMeanwhile, the extended model by Position Interpolation preserve quality\nrelatively well on tasks within its original context window. To achieve this\ngoal, Position Interpolation linearly down-scales the input position indices to\nmatch the original context window size, rather than extrapolating beyond the\ntrained context length which may lead to catastrophically high attention scores\nthat completely ruin the self-attention mechanism. Our theoretical study shows\nthat the upper bound of interpolation is at least $\\sim 600 \\times$ smaller\nthan that of extrapolation, further demonstrating its stability. Models\nextended via Position Interpolation retain its original architecture and can\nreuse most pre-existing optimization and infrastructure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shouyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_S/0/1/0/all/0/1\">Sherman Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Multilingual Code Search Dataset Using Neural Machine Translation. (arXiv:2306.15604v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15604","description":"<p>Code search is a task to find programming codes that semantically match the\ngiven natural language queries. Even though some of the existing datasets for\nthis task are multilingual on the programming language side, their query data\nare only in English. In this research, we create a multilingual code search\ndataset in four natural and four programming languages using a neural machine\ntranslation model. Using our dataset, we pre-train and fine-tune the\nTransformer-based models and then evaluate them on multiple code search test\nsets. Our results show that the model pre-trained with all natural and\nprogramming language data has performed best in most cases. By applying\nback-translation data filtering to our dataset, we demonstrate that the\ntranslation quality affects the model's performance to a certain extent, but\nthe data size matters more.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sekizawa_R/0/1/0/all/0/1\">Ryo Sekizawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shuai Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanaka_H/0/1/0/all/0/1\">Hitomi Yanaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Annotation of Direct Speech in Written French Narratives. (arXiv:2306.15634v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15634","description":"<p>The automatic annotation of direct speech (AADS) in written text has been\noften used in computational narrative understanding. Methods based on either\nrules or deep neural networks have been explored, in particular for English or\nGerman languages. Yet, for French, our target language, not many works exist.\nOur goal is to create a unified framework to design and evaluate AADS models in\nFrench. For this, we consolidated the largest-to-date French narrative dataset\nannotated with DS per word; we adapted various baselines for sequence labelling\nor from AADS in other languages; and we designed and conducted an extensive\nevaluation focused on generalisation. Results show that the task still requires\nsubstantial efforts and emphasise characteristics of each baseline. Although\nthis framework could be improved, it is a step further to encourage more\nresearch on the topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Durandard_N/0/1/0/all/0/1\">No&#xe9; Durandard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Viet-Anh Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_G/0/1/0/all/0/1\">Gaspard Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Epure_E/0/1/0/all/0/1\">Elena V. Epure</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style-transfer based Speech and Audio-visual Scene Understanding for Robot Action Sequence Acquisition from Videos. (arXiv:2306.15644v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15644","description":"<p>To realize human-robot collaboration, robots need to execute actions for new\ntasks according to human instructions given finite prior knowledge. Human\nexperts can share their knowledge of how to perform a task with a robot through\nmulti-modal instructions in their demonstrations, showing a sequence of\nshort-horizon steps to achieve a long-horizon goal. This paper introduces a\nmethod for robot action sequence generation from instruction videos using (1)\nan audio-visual Transformer that converts audio-visual features and instruction\nspeech to a sequence of robot actions called dynamic movement primitives (DMPs)\nand (2) style-transfer-based training that employs multi-task learning with\nvideo captioning and weakly-supervised learning with a semantic classifier to\nexploit unpaired video-action data. We built a system that accomplishes various\ncooking actions, where an arm robot executes a DMP sequence acquired from a\ncooking video using the audio-visual Transformer. Experiments with\nEpic-Kitchen-100, YouCookII, QuerYD, and in-house instruction video datasets\nshow that the proposed method improves the quality of DMP sequences by 2.3\ntimes the METEOR score obtained with a baseline video-to-action Transformer.\nThe model achieved 32% of the task success rate with the task knowledge of the\nobject.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hori_C/0/1/0/all/0/1\">Chiori Hori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ota_K/0/1/0/all/0/1\">Kei Ota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Siddarth Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corcodel_R/0/1/0/all/0/1\">Radu Corcodel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1\">Devesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romeres_D/0/1/0/all/0/1\">Diego Romeres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design. (arXiv:2306.15656v1 [cs.LG])","link":"http://arxiv.org/abs/2306.15656","description":"<p>This paper introduces SparseOptimizer, a novel deep learning optimizer that\nexploits Moreau-Yosida regularization to naturally induce sparsity in large\nlanguage models such as BERT, ALBERT and GPT. Key to the design of\nSparseOptimizer is an embedded shrinkage operator, which imparts sparsity\ndirectly within the optimization process. This operator, backed by a sound\ntheoretical framework, includes an analytical solution, thereby reinforcing the\noptimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play\nfunctionality eradicates the need for code modifications, making it a\nuniversally adaptable tool for a wide array of large language models. Empirical\nevaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2\nconfirm that SparseBERT and SparseALBERT, when sparsified using\nSparseOptimizer, achieve performance comparable to their dense counterparts,\nBERT and ALBERT, while significantly reducing their parameter count. Further,\nthis work proposes an innovative optimizer-compiler co-design strategy,\ndemonstrating the potential of inference acceleration (\\textbf{3.37x},\n\\textbf{6.30x}, and \\textbf{7.15x} in comparison with Pytorch, TensorFlow, and\nLLVM generic compile, respectively) in SparseBERT when paired with an\nappropriately designed compiler. This study represents a significant step\nforward in the evolution of efficient, scalable, and high-performing large\nlanguage models, setting a precedent for future exploration and optimization in\nthis domain. The SparseOptimizer code and SparseALBERT model will be made\navailable upon paper acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fu-Ming Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing of Detection Tools for AI-Generated Text. (arXiv:2306.15666v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15666","description":"<p>Recent advances in generative pre-trained transformer large language models\nhave emphasised the potential risks of unfair use of artificial intelligence\n(AI) generated content in an academic environment and intensified efforts in\nsearching for solutions to detect such content. The paper examines the general\nfunctionality of detection tools for artificial intelligence generated text and\nevaluates them based on accuracy and error type analysis. Specifically, the\nstudy seeks to answer research questions about whether existing detection tools\ncan reliably differentiate between human-written text and ChatGPT-generated\ntext, and whether machine translation and content obfuscation techniques affect\nthe detection of AIgenerated text. The research covers 12 publicly available\ntools and two commercial systems (Turnitin and PlagiarismCheck) that are widely\nused in the academic setting. The researchers conclude that the available\ndetection tools are neither accurate nor reliable and have a main bias towards\nclassifying the output as human-written rather than detecting AIgenerated text.\nFurthermore, content obfuscation techniques significantly worsen the\nperformance of tools. The study makes several significant contributions. First,\nit summarises up-to-date similar scientific and non-scientific efforts in the\nfield. Second, it presents the result of one of the most comprehensive tests\nconducted so far, based on a rigorous research methodology, an original\ndocument set, and a broad coverage of tools. Third, it discusses the\nimplications and drawbacks of using detection tools for AI-generated text in\nacademic settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weber_Wulff_D/0/1/0/all/0/1\">Debora Weber-Wulff</a> (University of Applied Sciences HTW Berlin, Germany), <a href=\"http://arxiv.org/find/cs/1/au:+Anohina_Naumeca_A/0/1/0/all/0/1\">Alla Anohina-Naumeca</a> (Riga Technical University, Latvia), <a href=\"http://arxiv.org/find/cs/1/au:+Bjelobaba_S/0/1/0/all/0/1\">Sonja Bjelobaba</a> (Uppsala University, Sweden), <a href=\"http://arxiv.org/find/cs/1/au:+Foltynek_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Folt&#xfd;nek</a> (Masaryk University, Czechia), <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_Dib_J/0/1/0/all/0/1\">Jean Guerrero-Dib</a> (Universidad de Monterrey, Mexico), <a href=\"http://arxiv.org/find/cs/1/au:+Popoola_O/0/1/0/all/0/1\">Olumide Popoola</a> (Queen Mary&#x27;s University, UK), <a href=\"http://arxiv.org/find/cs/1/au:+Sigut_P/0/1/0/all/0/1\">Petr &#x160;igut</a> (Masaryk University, Czechia), <a href=\"http://arxiv.org/find/cs/1/au:+Waddington_L/0/1/0/all/0/1\">Lorna Waddington</a> (University of Leeds, UK)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable and Discourse Topic-aware Neural Language Understanding. (arXiv:2006.10632v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.10632","description":"<p>Marrying topic models and language models exposes language understanding to a\nbroader source of document-level context beyond sentences via topics. While\nintroducing topical semantics in language models, existing approaches\nincorporate latent document topic proportions and ignore topical discourse in\nsentences of the document. This work extends the line of research by\nadditionally introducing an explainable topic representation in language\nunderstanding, obtained from a set of key terms correspondingly for each latent\ntopic of the proportion. Moreover, we retain sentence-topic associations along\nwith document-topic association by modeling topical discourse for every\nsentence in the document. We present a novel neural composite language model\nthat exploits both the latent and explainable topics along with topical\ndiscourse at sentence-level in a joint learning framework of topic and language\nmodels. Experiments over a range of tasks such as language modeling, word sense\ndisambiguation, document classification, retrieval and text generation\ndemonstrate ability of the proposed model in improving language understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_Y/0/1/0/all/0/1\">Yatin Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pankaj Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Topic Modeling with Continual Lifelong Learning. (arXiv:2006.10909v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.10909","description":"<p>Lifelong learning has recently attracted attention in building machine\nlearning systems that continually accumulate and transfer knowledge to help\nfuture learning. Unsupervised topic modeling has been popularly used to\ndiscover topics from document collections. However, the application of topic\nmodeling is challenging due to data sparsity, e.g., in a small collection of\n(short) documents and thus, generate incoherent topics and sub-optimal document\nrepresentations. To address the problem, we propose a lifelong learning\nframework for neural topic modeling that can continuously process streams of\ndocument collections, accumulate topics and guide future topic modeling tasks\nby knowledge transfer from several sources to better deal with the sparse data.\nIn the lifelong process, we particularly investigate jointly: (1) sharing\ngenerative homologies (latent topics) over lifetime to transfer prior\nknowledge, and (2) minimizing catastrophic forgetting to retain the past\nlearning via novel selective data augmentation, co-training and topic\nregularization approaches. Given a stream of document collections, we apply the\nproposed Lifelong Neural Topic Modeling (LNTM) framework in modeling three\nsparse document collections as future tasks and demonstrate improved\nperformance quantified by perplexity, topic coherence and information retrieval\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pankaj Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_Y/0/1/0/all/0/1\">Yatin Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Runkler_T/0/1/0/all/0/1\">Thomas Runkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Does Translation Require Context? A Data-driven, Multilingual Exploration. (arXiv:2109.07446v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07446","description":"<p>Although proper handling of discourse significantly contributes to the\nquality of machine translation (MT), these improvements are not adequately\nmeasured in common translation quality metrics. Recent works in context-aware\nMT attempt to target a small set of discourse phenomena during evaluation,\nhowever not in a fully systematic way. In this paper, we develop the\nMultilingual Discourse-Aware (MuDA) benchmark, a series of taggers that\nidentify and evaluate model performance on discourse phenomena in any given\ndataset. The choice of phenomena is inspired by a novel methodology to\nsystematically identify translations requiring context. We confirm the\ndifficulty of previously studied phenomena while uncovering others that were\npreviously unaddressed. We find that common context-aware MT models make only\nmarginal improvements over context-agnostic models, which suggests these models\ndo not handle these ambiguities effectively. We release code and data for 14\nlanguage pairs to encourage the MT community to focus on accurately capturing\ndiscourse phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kayo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1\">Emmy Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Dialogue Disentanglement by Self-Supervised Entangled Response Selection. (arXiv:2110.12646v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.12646","description":"<p>Dialogue disentanglement aims to group utterances in a long and\nmulti-participant dialogue into threads. This is useful for discourse analysis\nand downstream applications such as dialogue response selection, where it can\nbe the first step to construct a clean context/response set. Unfortunately,\nlabeling all~\\emph{reply-to} links takes quadratic effort w.r.t the number of\nutterances: an annotator must check all preceding utterances to identify the\none to which the current utterance is a reply. In this paper, we are the first\nto propose a~\\textbf{zero-shot} dialogue disentanglement solution. Firstly, we\ntrain a model on a multi-participant response selection dataset harvested from\nthe web which is not annotated; we then apply the trained model to perform\nzero-shot dialogue disentanglement. Without any labeled data, our model can\nachieve a cluster F1 score of 25. We also fine-tune the model using various\namounts of labeled data. Experiments show that with only 10\\% of the data, we\nachieve nearly the same performance of using the full dataset\\footnote{Code is\nreleased at\n\\url{https://github.com/chijames/zero_shot_dialogue_disentanglement}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_T/0/1/0/all/0/1\">Ta-Chung Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander I. Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Do Compressed Multilingual Machine Translation Models Forget?. (arXiv:2205.10828v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10828","description":"<p>Recently, very large pre-trained models achieve state-of-the-art results in\nvarious natural language processing (NLP) tasks, but their size makes it more\nchallenging to apply them in resource-constrained environments. Compression\ntechniques allow to drastically reduce the size of the models and therefore\ntheir inference time with negligible impact on top-tier metrics. However, the\ngeneral performance averaged across multiple tasks and/or languages may hide a\ndrastic performance drop on under-represented features, which could result in\nthe amplification of biases encoded by the models. In this work, we assess the\nimpact of compression methods on Multilingual Neural Machine Translation models\n(MNMT) for various language groups, gender, and semantic biases by extensive\nanalysis of compressed models on different machine translation benchmarks, i.e.\nFLORES-101, MT-Gender, and DiBiMT. We show that the performance of\nunder-represented languages drops significantly, while the average BLEU metric\nonly slightly decreases. Interestingly, the removal of noisy memorization with\ncompression leads to a significant improvement for some medium-resource\nlanguages. Finally, we demonstrate that compression amplifies intrinsic gender\nand semantic biases, even in high-resource languages. Code:\nhttps://github.com/alirezamshi/bias-compressedMT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1\">Alireza Mohammadshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1\">Alexandre Berard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brun_C/0/1/0/all/0/1\">Caroline Brun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blank Collapse: Compressing CTC emission for the faster decoding. (arXiv:2210.17017v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.17017","description":"<p>Connectionist Temporal Classification (CTC) model is a very efficient method\nfor modeling sequences, especially for speech data. In order to use CTC model\nas an Automatic Speech Recognition (ASR) task, the beam search decoding with an\nexternal language model like n-gram LM is necessary to obtain reasonable\nresults. In this paper we analyze the blank label in CTC beam search deeply and\npropose a very simple method to reduce the amount of calculation resulting in\nfaster beam search decoding speed. With this method, we can get up to 78%\nfaster decoding speed than ordinary beam search decoding with a very small loss\nof accuracy in LibriSpeech datasets. We prove this method is effective not only\npractically by experiments but also theoretically by mathematical reasoning. We\nalso observe that this reduction is more obvious if the accuracy of the model\nis higher.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1\">Minkyu Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_O/0/1/0/all/0/1\">Ohhyeok Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seunghyun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Soonshin Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control. (arXiv:2210.17432v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.17432","description":"<p>Despite the growing success of diffusion models in continuous-valued domains\n(e.g., images), similar efforts for discrete domains such as text have yet to\nmatch the performance of autoregressive language models. In this work, we\npresent SSD-LM -- a diffusion-based language model with two key design choices.\nFirst, SSD-LM is semi-autoregressive, iteratively generating blocks of text,\nallowing for flexible output length at decoding time while enabling local\nbidirectional context updates. Second, it is simplex-based, performing\ndiffusion on the natural vocabulary space rather than a learned latent space,\nallowing us to incorporate classifier guidance and modular control using\noff-the-shelf classifiers without any adaptation. We evaluate SSD-LM on\nunconstrained text generation benchmarks, and show that it matches or\noutperforms strong autoregressive GPT-2 models across standard quality and\ndiversity metrics, while vastly outperforming diffusion-based baselines. On\ncontrolled text generation, SSD-LM also outperforms competitive baselines, with\nan extra advantage in modularity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaochuang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sachin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. (arXiv:2211.05100v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05100","description":"<p>Large language models (LLMs) have been shown to be able to perform new tasks\nbased on a few demonstrations or natural language instructions. While these\ncapabilities have led to widespread adoption, most LLMs are developed by\nresource-rich organizations and are frequently kept from the public. As a step\ntowards democratizing this powerful technology, we present BLOOM, a\n176B-parameter open-access language model designed and built thanks to a\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\nWe find that BLOOM achieves competitive performance on a wide variety of\nbenchmarks, with stronger results after undergoing multitask prompted\nfinetuning. To facilitate future research and applications using LLMs, we\npublicly release our models and code under the Responsible AI License.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Workshop_B/0/1/0/all/0/1\">BigScience Workshop</a>: <a href=\"http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1\">Teven Le Scao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akiki_C/0/1/0/all/0/1\">Christopher Akiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1\">Suzana Ili&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hesslow_D/0/1/0/all/0/1\">Daniel Hesslow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castagne_R/0/1/0/all/0/1\">Roman Castagn&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luccioni_A/0/1/0/all/0/1\">Alexandra Sasha Luccioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galle_M/0/1/0/all/0/1\">Matthias Gall&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tow_J/0/1/0/all/0/1\">Jonathan Tow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanamanchi_P/0/1/0/all/0/1\">Pawan Sasanka Ammanamanchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Thomas Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moral_A/0/1/0/all/0/1\">Albert Villanova del Moral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruwase_O/0/1/0/all/0/1\">Olatunji Ruwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1\">Rachel Bawden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekman_S/0/1/0/all/0/1\">Stas Bekman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillan_Major_A/0/1/0/all/0/1\">Angelina McMillan-Major</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saulnier_L/0/1/0/all/0/1\">Lucile Saulnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_P/0/1/0/all/0/1\">Pedro Ortiz Suarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurencon_H/0/1/0/all/0/1\">Hugo Lauren&#xe7;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Launay_J/0/1/0/all/0/1\">Julien Launay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1\">Margaret Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1\">Aaron Gokaslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simhi_A/0/1/0/all/0/1\">Adi Simhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1\">Aitor Soroa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfassy_A/0/1/0/all/0/1\">Amit Alfassy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitzav_A/0/1/0/all/0/1\">Ariel Kreisberg Nitzav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1\">Chenghao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klamm_C/0/1/0/all/0/1\">Christopher Klamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_C/0/1/0/all/0/1\">Colin Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strien_D/0/1/0/all/0/1\">Daniel van Strien</a>, et al. (345 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imagination is All You Need! Curved Contrastive Learning for Abstract Sequence Modeling Utilized on Long Short-Term Dialogue Planning. (arXiv:2211.07591v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07591","description":"<p>Inspired by the curvature of space-time (Einstein, 1921), we introduce Curved\nContrastive Learning (CCL), a novel representation learning technique for\nlearning the relative turn distance between utterance pairs in multi-turn\ndialogues. The resulting bi-encoder models can guide transformers as a response\nranking model towards a goal in a zero-shot fashion by projecting the goal\nutterance and the corresponding reply candidates into a latent space. Here the\ncosine similarity indicates the distance/reachability of a candidate utterance\ntoward the corresponding goal. Furthermore, we explore how these\nforward-entailing language representations can be utilized for assessing the\nlikelihood of sequences by the entailment strength i.e. through the cosine\nsimilarity of its individual members (encoded separately) as an emergent\nproperty in the curved space. These non-local properties allow us to imagine\nthe likelihood of future patterns in dialogues, specifically by\nordering/identifying future goal utterances that are multiple turns away, given\na dialogue context. As part of our analysis, we investigate characteristics\nthat make conversations (un)plannable and find strong evidence of planning\ncapability over multiple turns (in 61.56% over 3 turns) in conversations from\nthe DailyDialog (Li et al., 2017) dataset. Finally, we show how we achieve\nhigher efficiency in sequence modeling tasks compared to previous work thanks\nto our relativistic approach, where only the last utterance needs to be encoded\nand computed during inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Erker_J/0/1/0/all/0/1\">Justus-Jonas Erker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaffer_S/0/1/0/all/0/1\">Stefan Schaffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spanakis_G/0/1/0/all/0/1\">Gerasimos Spanakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WACO: Word-Aligned Contrastive Learning for Speech Translation. (arXiv:2212.09359v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09359","description":"<p>End-to-end Speech Translation (E2E ST) aims to directly translate source\nspeech into target text. Existing ST methods perform poorly when only extremely\nsmall speech-text data are available for training. We observe that an ST\nmodel's performance closely correlates with its embedding similarity between\nspeech and source transcript. In this paper, we propose Word-Aligned\nCOntrastive learning (WACO), a simple and effective method for extremely\nlow-resource speech-to-text translation. Our key idea is bridging word-level\nrepresentations for both speech and text modalities via contrastive learning.\nWe evaluate WACO and other methods on the MuST-C dataset, a widely used ST\nbenchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our\nexperiments demonstrate that WACO outperforms the best baseline by 9+ BLEU\npoints with only 1-hour parallel ST data. Code is available at\nhttps://github.com/owaski/WACO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siqi Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models. (arXiv:2212.09553v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09553","description":"<p>We present Mu$^{2}$SLAM, a multilingual sequence-to-sequence model\npre-trained jointly on unlabeled speech, unlabeled text and supervised data\nspanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST)\nand Machine Translation (MT), in over 100 languages. By leveraging a quantized\nrepresentation of speech as a target, Mu$^{2}$SLAM trains the speech-text\nmodels with a sequence-to-sequence masked denoising objective similar to T5 on\nthe decoder and a masked language modeling (MLM) objective on the encoder, for\nboth unlabeled speech and text, while utilizing the supervised tasks to improve\ncross-lingual and cross-modal representation alignment within the model. On\nCoVoST AST, Mu$^{2}$SLAM establishes a new state-of-the-art for models trained\non public datasets, improving on xx-en translation over the previous best by\n1.9 BLEU points and on en-xx translation by 1.1 BLEU points. On Voxpopuli ASR,\nour model matches the performance of an mSLAM model fine-tuned with an RNN-T\ndecoder, despite using a relatively weaker sequence-to-sequence architecture.\nOn text understanding tasks, our model improves by more than 6\\% over mSLAM on\nXNLI, getting closer to the performance of mT5 models of comparable capacity on\nXNLI and TydiQA, paving the way towards a single model for all speech and text\nunderstanding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macherey_W/0/1/0/all/0/1\">Wolfgang Macherey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auditing large language models: a three-layered approach. (arXiv:2302.08500v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08500","description":"<p>Large language models (LLMs) represent a major advance in artificial\nintelligence (AI) research. However, the widespread use of LLMs is also coupled\nwith significant ethical and social challenges. Previous research has pointed\ntowards auditing as a promising governance mechanism to help ensure that AI\nsystems are designed and deployed in ways that are ethical, legal, and\ntechnically robust. However, existing auditing procedures fail to address the\ngovernance challenges posed by LLMs, which display emergent capabilities and\nare adaptable to a wide range of downstream tasks. In this article, we address\nthat gap by outlining a novel blueprint for how to audit LLMs. Specifically, we\npropose a three-layered approach, whereby governance audits (of technology\nproviders that design and disseminate LLMs), model audits (of LLMs after\npre-training but prior to their release), and application audits (of\napplications based on LLMs) complement and inform each other. We show how\naudits, when conducted in a structured and coordinated manner on all three\nlevels, can be a feasible and effective mechanism for identifying and managing\nsome of the ethical and social risks posed by LLMs. However, it is important to\nremain realistic about what auditing can reasonably be expected to achieve.\nTherefore, we discuss the limitations not only of our three-layered approach\nbut also of the prospect of auditing LLMs at all. Ultimately, this article\nseeks to expand the methodological toolkit available to technology providers\nand policymakers who wish to analyse and evaluate LLMs from technical, ethical,\nand legal perspectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mokander_J/0/1/0/all/0/1\">Jakob M&#xf6;kander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuett_J/0/1/0/all/0/1\">Jonas Schuett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Floridi_L/0/1/0/all/0/1\">Luciano Floridi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. (arXiv:2302.13939v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13939","description":"<p>As the size of large language models continue to scale, so does the\ncomputational resources required to run it. Spiking Neural Networks (SNNs) have\nemerged as an energy-efficient approach to deep learning that leverage sparse\nand event-driven activations to reduce the computational overhead associated\nwith model inference. While they have become competitive with non-spiking\nmodels on many computer vision tasks, SNNs have also proven to be more\nchallenging to train. As a result, their performance lags behind modern deep\nlearning, and we are yet to see the effectiveness of SNNs in language\ngeneration. In this paper, inspired by the Receptance Weighted Key Value (RWKV)\nlanguage model, we successfully implement `SpikeGPT', a generative language\nmodel with binary, event-driven spiking activation units. We train the proposed\nmodel on two model variants: 45M and 216M parameters. To the best of our\nknowledge, SpikeGPT is the largest backpropagation-trained SNN model to date,\nrendering it suitable for both the generation and comprehension of natural\nlanguage. We achieve this by modifying the transformer block to replace\nmulti-head self attention to reduce quadratic computational complexity O(N^2)\nto linear complexity O(N) with increasing sequence length. Input tokens are\ninstead streamed in sequentially to our attention mechanism (as with typical\nSNNs). Our preliminary experiments show that SpikeGPT remains competitive with\nnon-spiking models on tested benchmarks, while maintaining 20x fewer operations\nwhen processed on neuromorphic hardware that can leverage sparse, event-driven\nactivations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui-Jie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qihang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshraghian_J/0/1/0/all/0/1\">Jason K. Eshraghian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering. (arXiv:2303.05352v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.05352","description":"<p>There has been a growing effort to replace hand extraction of data from\nresearch papers with automated data extraction based on natural language\nprocessing, language models, and recently, large language models (LLMs).\nAlthough these methods enable efficient extraction of data from large sets of\nresearch papers, they require a significant amount of up-front effort,\nexpertise, and coding. In this work we propose the ChatExtract method that can\nfully automate very accurate data extraction with minimal initial effort and\nbackground, using an advanced conversational LLM. ChatExtract consists of a set\nof engineered prompts applied to a conversational LLM that both identify\nsentences with data, extract that data, and assure the data's correctness\nthrough a series of follow-up questions. These follow-up questions largely\novercome known issues with LLMs providing factually inaccurate responses.\nChatExtract can be applied with any conversational LLMs and yields very high\nquality data extraction. In tests on materials data we find precision and\nrecall both close to 90% from the best conversational LLMs, like ChatGPT-4. We\ndemonstrate that the exceptional performance is enabled by the information\nretention in a conversational model combined with purposeful redundancy and\nintroducing uncertainty through follow-up prompts. These results suggest that\napproaches similar to ChatExtract, due to their simplicity, transferability,\nand accuracy are likely to become powerful tools for data extraction in the\nnear future. Finally, databases for critical cooling rates of metallic glasses\nand yield strengths of high entropy alloys are developed using ChatExtract.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polak_M/0/1/0/all/0/1\">Maciej P. Polak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_D/0/1/0/all/0/1\">Dane Morgan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.09901","description":"<p>This paper presents the winning system for the zero-shot Spanish framing\ndetection task, which also achieves competitive places in eight additional\nlanguages. The challenge of the framing detection task lies in identifying a\nset of 14 frames when only a few or zero samples are available, i.e., a\nmultilingual multi-label few- or zero-shot setting. Our developed solution\nemploys a pre-training procedure based on multilingual Transformers using a\nlabel-aware contrastive loss function. In addition to describing the system, we\nperform an embedding space analysis and ablation study to demonstrate how our\npre-training procedure supports framing detection to advance computational\nframing analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reiter_Haas_M/0/1/0/all/0/1\">Markus Reiter-Haas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ertl_A/0/1/0/all/0/1\">Alexander Ertl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Innerhofer_K/0/1/0/all/0/1\">Kevin Innerhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lex_E/0/1/0/all/0/1\">Elisabeth Lex</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2305.07011v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.07011","description":"<p>We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a\ncontrastive image-text pretraining recipe to bridge the gap between image-level\npretraining and open-vocabulary object detection. At the pretraining phase, we\npropose to randomly crop and resize regions of positional embeddings instead of\nusing the whole image positional embeddings. This better matches the use of\npositional embeddings at region-level in the detection finetuning phase. In\naddition, we replace the common softmax cross entropy loss in contrastive\nlearning with focal loss to better learn the informative yet difficult\nexamples. Finally, we leverage recent advances in novel object proposals to\nimprove open-vocabulary detection finetuning. We evaluate our full model on the\nLVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer.\nRO-ViT achieves a state-of-the-art 32.4 $AP_r$ on LVIS, surpassing the best\nexisting approach by +6.1 points in addition to competitive zero-shot transfer\ndetection. Surprisingly, RO-ViT improves the image-level representation as well\nand achieves the state of the art on 9 out of 12 metrics on COCO and Flickr\nimage-text retrieval benchmarks, outperforming competitive approaches with\nlarger models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1\">Weicheng Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling. (arXiv:2305.11543v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11543","description":"<p>As the foundation of current natural language processing methods, pre-trained\nlanguage model has achieved excellent performance. However, the black-box\nstructure of the deep neural network in pre-trained language models seriously\nlimits the interpretability of the language modeling process. After revisiting\nthe coupled requirement of deep neural representation and semantics logic of\nlanguage modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by\nintroducing the alignment processing between uninterpretable neural\nrepresentation and interpretable statistical logic. Moreover, a clustering\nprocess is also designed to connect the word- and context-level semantics.\nSpecifically, an associative knowledge network (AKN), considered interpretable\nstatistical logic, is introduced in the alignment process for word-level\nsemantics. Furthermore, the context-relative distance is employed as the\nsemantic feature for the downstream classifier, which is greatly different from\nthe current uninterpretable semantic representations of pre-trained models. Our\nexperiments for performance evaluation and interpretable analysis are executed\non several types of datasets, including SIGHAN, Weibo, and ChnSenti. Wherein a\nnovel evaluation strategy for the interpretability of machine learning models\nis first proposed. According to the experimental results, our language model\ncan achieve better performance and highly credible interpretable ability\ncompared to related state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fanyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenping Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test. (arXiv:2305.13108v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2305.13108","description":"<p>Automatic speech recognition systems based on deep learning are mainly\ntrained under empirical risk minimization (ERM). Since ERM utilizes the\naveraged performance on the data samples regardless of a group such as healthy\nor dysarthric speakers, ASR systems are unaware of the performance disparities\nacross the groups. This results in biased ASR systems whose performance\ndifferences among groups are severe. In this study, we aim to improve the ASR\nsystem in terms of group robustness for dysarthric speakers. To achieve our\ngoal, we present a novel approach, sample reweighting with sample affinity test\n(Re-SAT). Re-SAT systematically measures the debiasing helpfulness of the given\ndata sample and then mitigates the bias by debiasing helpfulness-based sample\nreweighting. Experimental results demonstrate that Re-SAT contributes to\nimproved ASR performance on dysarthric speech without performance degradation\non healthy speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_E/0/1/0/all/0/1\">Eungbeom Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chae_Y/0/1/0/all/0/1\">Yunkee Chae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_J/0/1/0/all/0/1\">Jaeheon Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1\">Kyogu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Attention is Not Enough: Incongruity-Aware Hierarchical Multimodal Sentiment Analysis and Emotion Recognition. (arXiv:2305.13583v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13583","description":"<p>Fusing multiple modalities for affective computing tasks has proven effective\nfor performance improvement. However, how multimodal fusion works is not well\nunderstood, and its use in the real world usually results in large model sizes.\nIn this work, on sentiment and emotion analysis, we first analyze how the\nsalient affective information in one modality can be affected by the other in\ncrossmodal attention. We find that inter-modal incongruity exists at the latent\nlevel due to crossmodal attention. Based on this finding, we propose a\nlightweight model via Hierarchical Crossmodal Transformer with Modality Gating\n(HCT-MG), which determines a primary modality according to its contribution to\nthe target task and then hierarchically incorporates auxiliary modalities to\nalleviate inter-modal incongruity and reduce information redundancy. The\nexperimental evaluation on three benchmark datasets: CMU-MOSI, CMU-MOSEI, and\nIEMOCAP verifies the efficacy of our approach, showing that it: 1) achieves\nbetter performance than prior work as well as manual selection of the primary\nmodality; 2) can recognize hard samples whose emotions are hard to tell; 3)\nmitigates the inter-modal incongruity at the latent level when modalities have\nmismatched affective tendencies; 4) reduces model size to less than 1M\nparameters while outperforming existing models of similar sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaoting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Catherine Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17760","description":"<p>How do language models \"think\"? This paper formulates a probabilistic\ncognitive model called the bounded pragmatic speaker, which can characterize\nthe operation of different variations of language models. Specifically, we\ndemonstrate that large language models fine-tuned with reinforcement learning\nfrom human feedback (Ouyang et al., 2022) embody a model of thought that\nconceptually resembles a fast-and-slow model (Kahneman, 2011), which\npsychologists have attributed to humans. We discuss the limitations of\nreinforcement learning from human feedback as a fast-and-slow model of thought\nand propose avenues for expanding this framework. In essence, our research\nhighlights the value of adopting a cognitive probabilistic modeling approach to\ngain insights into the comprehension, evaluation, and advancement of language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khanh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMZip: Lossless Text Compression using Large Language Models. (arXiv:2306.04050v2 [cs.IT] UPDATED)","link":"http://arxiv.org/abs/2306.04050","description":"<p>We provide new estimates of an asymptotic upper bound on the entropy of\nEnglish using the large language model LLaMA-7B as a predictor for the next\ntoken given a window of past tokens. This estimate is significantly smaller\nthan currently available estimates in \\cite{cover1978convergent},\n\\cite{lutati2023focus}. A natural byproduct is an algorithm for lossless\ncompression of English text which combines the prediction from the large\nlanguage model with a lossless compression scheme. Preliminary results from\nlimited experiments suggest that our scheme outperforms state-of-the-art text\ncompression schemes such as BSC, ZPAQ, and paq8h.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valmeekam_C/0/1/0/all/0/1\">Chandra Shekhara Kaushik Valmeekam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_K/0/1/0/all/0/1\">Krishna Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalathil_D/0/1/0/all/0/1\">Dileep Kalathil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chamberland_J/0/1/0/all/0/1\">Jean-Francois Chamberland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Srinivas Shakkottai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.08158","description":"<p>Deep neural networks often learn unintended biases during training, which\nmight have harmful effects when deployed in real-world settings. This paper\nsurveys 209 papers on bias in NLP models, most of which address\nsociodemographic bias. To better understand the distinction between bias and\nreal-world harm, we turn to ideas from psychology and behavioral economics to\npropose a definition for sociodemographic bias. We identify three main\ncategories of NLP bias research: types of bias, quantifying bias, and\ndebiasing. We conclude that current approaches on quantifying bias face\nreliability issues, that many of the bias metrics do not relate to real-world\nbiases, and that current debiasing techniques are superficial and hide bias\nrather than removing it. Finally, we provide recommendations for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vipul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1\">Pranav Narayanan Venkit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1\">Shomir Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1\">Rebecca J. Passonneau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models. (arXiv:2306.08952v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.08952","description":"<p>Reasoning about time is of fundamental importance. Many facts are\ntime-dependent. For example, athletes change teams from time to time, and\ndifferent government officials are elected periodically. Previous\ntime-dependent question answering (QA) datasets tend to be biased in either\ntheir coverage of time spans or question types. In this paper, we introduce a\ncomprehensive probing dataset \\tempreason to evaluate the temporal reasoning\ncapability of large language models. Our dataset includes questions of three\ntemporal reasoning levels. In addition, we also propose a novel learning\nframework to improve the temporal reasoning capability of large language\nmodels, based on temporal span extraction and time-sensitive reinforcement\nlearning. We conducted experiments in closed book QA, open book QA, and\nreasoning QA settings and demonstrated the effectiveness of our approach. Our\ncode and data are released on https://github.com/DAMO-NLP-SG/TempReason.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1\">Qingyu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiversiGATE: A Comprehensive Framework for Reliable Large Language Models. (arXiv:2306.13230v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.13230","description":"<p>In this paper, we introduce DiversiGATE, a unified framework that\nconsolidates diverse methodologies for LLM verification. The proposed framework\ncomprises two main components: Diversification and Aggregation which provide a\nholistic perspective on existing verification approaches, such as\nSelf-Consistency, Math Prompter and WebGPT. Furthermore, we propose a novel\n`SelfLearner' model that conforms to the DiversiGATE framework which can learn\nfrom its own outputs and refine its performance over time, leading to improved\naccuracy. To evaluate the effectiveness of SelfLearner, we conducted a rigorous\nseries of experiments, including tests on synthetic data as well as on popular\narithmetic reasoning benchmarks such as GSM8K. Our results demonstrate that our\napproach outperforms traditional LLMs, achieving a considerable 54.8% -&gt; 61.8%\nimprovement on the GSM8K benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imani_S/0/1/0/all/0/1\">Shima Imani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyram_A/0/1/0/all/0/1\">Ali Beyram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_H/0/1/0/all/0/1\">Harsh Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Descriptive Image Captioning via Semipermeable Maximum Likelihood Estimation. (arXiv:2306.13460v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.13460","description":"<p>Image captioning aims to describe visual content in natural language. As 'a\npicture is worth a thousand words', there could be various correct descriptions\nfor an image. However, with maximum likelihood estimation as the training\nobjective, the captioning model is penalized whenever its prediction mismatches\nwith the label. For instance, when the model predicts a word expressing richer\nsemantics than the label, it will be penalized and optimized to prefer more\nconcise expressions, referred to as conciseness optimization. In contrast,\npredictions that are more concise than labels lead to richness optimization.\nSuch conflicting optimization directions could eventually result in the model\ngenerating general descriptions. In this work, we introduce Semipermeable\nMaxImum Likelihood Estimation (SMILE), which allows richness optimization while\nblocking conciseness optimization, thus encouraging the model to generate\nlonger captions with more details. Extensive experiments on two mainstream\nimage captioning datasets MSCOCO and Flickr30K demonstrate that SMILE\nsignificantly enhances the descriptiveness of generated captions. We further\nprovide in-depth investigations to facilitate a better understanding of how\nSMILE works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zihao Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Max-Margin Token Selection in Attention Mechanism. (arXiv:2306.13596v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.13596","description":"<p>Attention mechanism is a central component of the transformer architecture\nwhich led to the phenomenal success of large language models. However, the\ntheoretical principles underlying the attention mechanism are poorly\nunderstood, especially its nonconvex optimization dynamics. In this work, we\nexplore the seminal softmax-attention model $f(\\boldsymbol{X})=\\langle\n\\boldsymbol{Xv}, \\texttt{softmax}(\\boldsymbol{XWp})\\rangle$, where\n$\\boldsymbol{X}$ is the token sequence and\n$(\\boldsymbol{v},\\boldsymbol{W},\\boldsymbol{p})$ are trainable parameters. We\nprove that running gradient descent on $\\boldsymbol{p}$, or equivalently\n$\\boldsymbol{W}$, converges in direction to a max-margin solution that\nseparates $\\textit{locally-optimal}$ tokens from non-optimal ones. This clearly\nformalizes attention as an optimal token selection mechanism. Remarkably, our\nresults are applicable to general data and precisely characterize\n$\\textit{optimality}$ of tokens in terms of the value embeddings\n$\\boldsymbol{Xv}$ and problem geometry. We also provide a broader\nregularization path analysis that establishes the margin maximizing nature of\nattention even for nonlinear prediction heads. When optimizing $\\boldsymbol{v}$\nand $\\boldsymbol{p}$ simultaneously with logistic loss, we identify conditions\nunder which the regularization paths directionally converge to their respective\nhard-margin SVM solutions where $\\boldsymbol{v}$ separates the input features\nbased on their labels. Interestingly, the SVM formulation of $\\boldsymbol{p}$\nis influenced by the support vector geometry of $\\boldsymbol{v}$. Finally, we\nverify our theoretical findings via numerical experiments and provide insights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tarzanagh_D/0/1/0/all/0/1\">Davoud Ataee Tarzanagh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingcong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuechen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1\">Samet Oymak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers. (arXiv:2306.13804v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.13804","description":"<p>Despite the recent progress in speech emotion recognition (SER),\nstate-of-the-art systems are unable to achieve improved performance in\ncross-language settings. In this paper, we propose a Multimodal Dual Attention\nTransformer (MDAT) model to improve cross-language SER. Our model utilises\npre-trained models for multimodal feature extraction and is equipped with a\ndual attention mechanism including graph attention and co-attention to capture\ncomplex dependencies across different modalities and achieve improved\ncross-language SER results using minimal target language data. In addition, our\nmodel also exploits a transformer encoder layer for high-level feature\nrepresentation to improve emotion classification accuracy. In this way, MDAT\nperforms refinement of feature representation at various stages and provides\nemotional salient features to the classification layer. This novel approach\nalso ensures the preservation of modality-specific emotional information while\nenhancing cross-modality and cross-language interactions. We assess our model's\nperformance on four publicly available SER datasets and establish its superior\neffectiveness compared to recent approaches and baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1\">Syed Aun Muhammad Zaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latif_S/0/1/0/all/0/1\">Siddique Latif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qadir_J/0/1/0/all/0/1\">Junaid Qadir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Driven Approach for Formality-Sensitive Machine Translation: Language-Specific Handling and Synthetic Data Generation. (arXiv:2306.14514v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.14514","description":"<p>In this paper, we introduce a data-driven approach for Formality-Sensitive\nMachine Translation (FSMT) that caters to the unique linguistic properties of\nfour target languages. Our methodology centers on two core strategies: 1)\nlanguage-specific data handling, and 2) synthetic data generation using\nlarge-scale language models and empirical prompt engineering. This approach\ndemonstrates a considerable improvement over the baseline, highlighting the\neffectiveness of data-centric techniques. Our prompt engineering strategy\nfurther improves performance by producing superior synthetic translation\nexamples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seugnjun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1\">Hyeonseok Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kosmos-2: Grounding Multimodal Large Language Models to the World. (arXiv:2306.14824v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.14824","description":"<p>We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new\ncapabilities of perceiving object descriptions (e.g., bounding boxes) and\ngrounding text to the visual world. Specifically, we represent refer\nexpressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where\nobject descriptions are sequences of location tokens. Together with multimodal\ncorpora, we construct large-scale data of grounded image-text pairs (called\nGrIT) to train the model. In addition to the existing capabilities of MLLMs\n(e.g., perceiving general modalities, following instructions, and performing\nin-context learning), Kosmos-2 integrates the grounding capability into\ndownstream applications. We evaluate Kosmos-2 on a wide range of tasks,\nincluding (i) multimodal grounding, such as referring expression comprehension,\nand phrase grounding, (ii) multimodal referring, such as referring expression\ngeneration, (iii) perception-language tasks, and (iv) language understanding\nand generation. This work lays out the foundation for the development of\nEmbodiment AI and sheds light on the big convergence of language, multimodal\nperception, action, and world modeling, which is a key step toward artificial\ngeneral intelligence. Data, demo, and pretrained models are available at\nhttps://aka.ms/kosmos-2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhiliang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yaru Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback. (arXiv:2306.14898v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.14898","description":"<p>Humans write code in a fundamentally interactive manner and rely on constant\nexecution feedback to correct errors, resolve ambiguities, and decompose tasks.\nWhile LLMs have recently exhibited promising coding capabilities, current\ncoding benchmarks mostly consider a static instruction-to-code sequence\ntransduction process, which has the potential for error propagation and a\ndisconnect between the generated code and its final execution environment. To\naddress this gap, we introduce InterCode, a lightweight, flexible, and\neasy-to-use framework of interactive coding as a standard reinforcement\nlearning (RL) environment, with code as actions and execution feedback as\nobservations. Our framework is language and platform agnostic, uses\nself-contained Docker environments to provide safe and reproducible execution,\nand is compatible out-of-the-box with traditional seq2seq coding methods, while\nenabling the development of new methods for interactive code generation. We use\nInterCode to create two interactive code environments with Bash and SQL as\naction spaces, leveraging data from the static Spider and NL2Bash datasets. We\ndemonstrate InterCode's viability as a testbed by evaluating multiple\nstate-of-the-art LLMs configured with different prompting strategies such as\nReAct and Plan &amp; Solve. Our results showcase the benefits of interactive code\ngeneration and demonstrate that InterCode can serve as a challenging benchmark\nfor advancing code understanding and generation capabilities. InterCode is\ndesigned to be easily extensible and can even be used to incorporate new tasks\nsuch as Capture the Flag, a popular coding puzzle that is inherently multi-step\nand involves multiple programming languages. Project site with code and data:\nhttps://intercode-benchmark.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">John Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakar_A/0/1/0/all/0/1\">Akshara Prabhakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}