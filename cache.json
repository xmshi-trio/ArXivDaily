{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Learning with Rejection for Abstractive Text Summarization. (arXiv:2302.08531v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08531","description":"<p>State-of-the-art abstractive summarization systems frequently hallucinate\ncontent that is not supported by the source document, mainly due to noise in\nthe training dataset. Existing methods opt to drop the noisy samples or tokens\nfrom the training set entirely, reducing the effective training set size and\ncreating an artificial propensity to copy words from the source. In this work,\nwe propose a training objective for abstractive summarization based on\nrejection learning, in which the model learns whether or not to reject\npotentially noisy tokens. We further propose a regularized decoding objective\nthat penalizes non-factual candidate summaries during inference by using the\nrejection probability learned during training. We show that our method\nconsiderably improves the factuality of generated summaries in automatic and\nhuman evaluations when compared to five baseline models and that it does so\nwhile increasing the abstractiveness of the generated summaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingyi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Foundation Models for Natural Language Processing -- Pre-trained Language Models Integrating Media. (arXiv:2302.08575v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08575","description":"<p>This open access book provides a comprehensive overview of the state of the\nart in research and applications of Foundation Models and is intended for\nreaders familiar with basic Natural Language Processing (NLP) concepts. Over\nthe recent years, a revolutionary new paradigm has been developed for training\nmodels for NLP. These models are first pre-trained on large collections of text\ndocuments to acquire general syntactic knowledge and semantic information.\nThen, they are fine-tuned for specific tasks, which they can often solve with\nsuperhuman accuracy. When the models are large enough, they can be instructed\nby prompts to solve new tasks without any fine-tuning. Moreover, they can be\napplied to a wide range of different media and problem domains, ranging from\nimage and video processing to robot control learning. Because they provide a\nblueprint for solving many tasks in artificial intelligence, they have been\ncalled Foundation Models. After a brief introduction to basic NLP models the\nmain pre-trained language models BERT, GPT and sequence-to-sequence transformer\nare described, as well as the concepts of self-attention and context-sensitive\nembedding. Then, different approaches to improving these models are discussed,\nsuch as expanding the pre-training criteria, increasing the length of input\ntexts, or including extra knowledge. An overview of the best-performing models\nfor about twenty application areas is then presented, e.g., question answering,\ntranslation, story generation, dialog systems, generating images from text,\netc. For each application area, the strengths and weaknesses of current models\nare discussed, and an outlook on further developments is given. In addition,\nlinks are provided to freely available program code. A concluding chapter\nsummarizes the economic opportunities, mitigation of risks, and potential\ndevelopments of AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paass_G/0/1/0/all/0/1\">Gerhard Paa&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giesselbach_S/0/1/0/all/0/1\">Sven Giesselbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keep it Neutral: Using Natural Language Inference to Improve Generation. (arXiv:2302.08577v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08577","description":"<p>We explore incorporating natural language inference (NLI) into the text\ngenerative pipeline by using a pre-trained NLI model to assess whether a\ngenerated sentence entails, contradicts, or is neutral to the prompt and\npreceding text. First, we show that the NLI task is predictive of generation\nerrors made by GPT-3. We use these results to develop an NLI-informed\ngeneration procedure for GPT-J. Then, we evaluate these generations by\nobtaining human annotations on error types and overall quality. We find that an\nNLI strategy of maximizing entailment improves text generation when the nucleus\nsampling randomness parameter value is high, while one which maximizes\ncontradiction is in fact productive when the parameter value is low. Overall,\nthough, we demonstrate that an NLI strategy of maximizing the neutral class\nprovides the highest quality of generated text (significantly better than the\nvanilla generations), regardless of parameter value.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mersinias_M/0/1/0/all/0/1\">Michail Mersinias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretraining Language Models with Human Preferences. (arXiv:2302.08582v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08582","description":"<p>Language models (LMs) are pretrained to imitate internet text, including\ncontent that would violate human preferences if generated by an LM: falsehoods,\noffensive comments, personally identifiable information, low-quality or buggy\ncode, and more. Here, we explore alternative objectives for pretraining LMs in\na way that also guides them to generate text aligned with human preferences. We\nbenchmark five objectives for pretraining with human feedback across three\ntasks and study how they affect the trade-off between alignment and\ncapabilities of pretrained LMs. We find a Pareto-optimal and simple approach\namong those we explored: conditional training, or learning distribution over\ntokens conditional on their human preference scores given by a reward model.\nConditional training reduces the rate of undesirable content by up to an order\nof magnitude, both when generating without a prompt and with an\nadversarially-chosen prompt. Moreover, conditional training maintains the\ndownstream task performance of standard LM pretraining, both before and after\ntask-specific finetuning. Pretraining with human feedback results in much\nbetter preference satisfaction than standard LM pretraining followed by\nfinetuning with feedback, i.e., learning and then unlearning undesirable\nbehavior. Our results suggest that we should move beyond imitation learning\nwhen pretraining LMs and incorporate human preferences from the start of\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1\">Tomasz Korbak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kejian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhalerao_R/0/1/0/all/0/1\">Rasika Bhalerao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buckley_C/0/1/0/all/0/1\">Christopher L. Buckley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JEIT: Joint End-to-End Model and Internal Language Model Training for Speech Recognition. (arXiv:2302.08583v1 [eess.AS])","link":"http://arxiv.org/abs/2302.08583","description":"<p>We propose JEIT, a joint end-to-end (E2E) model and internal language model\n(ILM) training method to inject large-scale unpaired text into ILM during E2E\ntraining which improves rare-word speech recognition. With JEIT, the E2E model\ncomputes an E2E loss on audio-transcript pairs while its ILM estimates a\ncross-entropy loss on unpaired text. The E2E model is trained to minimize a\nweighted sum of E2E and ILM losses. During JEIT, ILM absorbs knowledge from\nunpaired text while the E2E training serves as regularization. Unlike ILM\nadaptation methods, JEIT does not require a separate adaptation step and avoids\nthe need for Kullback-Leibler divergence regularization of ILM. We also show\nthat modular hybrid autoregressive transducer (MHAT) performs better than HAT\nin the JEIT framework, and is much more robust than HAT during ILM adaptation.\nTo push the limit of unpaired text injection, we further propose a combined\nJEIT and JOIST training (CJJT) that benefits from modality matching, encoder\ntext injection and ILM training. Both JEIT and CJJT can foster a more effective\nLM fusion. With 100B unpaired sentences, JEIT/CJJT improves rare-word\nrecognition accuracy by up to 16.4% over a model trained without unpaired text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Weiran Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Tongzhou Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Variani_E/0/1/0/all/0/1\">Ehsan Variani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntactic Structure Processing in the Brain while Listening. (arXiv:2302.08589v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08589","description":"<p>Syntactic parsing is the task of assigning a syntactic structure to a\nsentence. There are two popular syntactic parsing methods: constituency and\ndependency parsing. Recent works have used syntactic embeddings based on\nconstituency trees, incremental top-down parsing, and other word syntactic\nfeatures for brain activity prediction given the text stimuli to study how the\nsyntax structure is represented in the brain's language network. However, the\neffectiveness of dependency parse trees or the relative predictive power of the\nvarious syntax parsers across brain areas, especially for the listening task,\nis yet unexplored. In this study, we investigate the predictive power of the\nbrain encoding models in three settings: (i) individual performance of the\nconstituency and dependency syntactic parsing based embedding methods, (ii)\nefficacy of these syntactic parsing based embedding methods when controlling\nfor basic syntactic signals, (iii) relative effectiveness of each of the\nsyntactic embedding methods when controlling for the other. Further, we explore\nthe relative importance of syntactic information (from these syntactic\nembedding methods) versus semantic information using BERT embeddings. We find\nthat constituency parsers help explain activations in the temporal lobe and\nmiddle-frontal gyrus, while dependency parsers better encode syntactic\nstructure in the angular gyrus and posterior cingulate cortex. Although\nsemantic signals from BERT are more effective compared to any of the syntactic\nfeatures or embedding methods, syntactic embedding methods explain additional\nvariance for a few brain regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oota_S/0/1/0/all/0/1\">Subba Reddy Oota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marreddy_M/0/1/0/all/0/1\">Mounika Marreddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surampud_B/0/1/0/all/0/1\">Bapi Raju Surampud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What A Situated Language-Using Agent Must be Able to Do: A Top-Down Analysis. (arXiv:2302.08590v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08590","description":"<p>Even in our increasingly text-intensive times, the primary site of language\nuse is situated, co-present interaction. It is primary ontogenetically and\nphylogenetically, and it is arguably also still primary in negotiating everyday\nsocial situations. Situated interaction is also the final frontier of Natural\nLanguage Processing, where, compared to the area of text processing, very\nlittle progress has been made in the past decade, and where a myriad of\npractical applications is waiting to be unlocked. While the usual approach in\nthe field is to reach, bottom-up, for the ever next \"adjacent possible\", in\nthis paper I attempt a top-down analysis of what the demands are that\nunrestricted situated interaction makes on the participating agent, and suggest\nways in which this analysis can structure computational models and research on\nthem. Specifically, I discuss representational demands (the building up and\napplication of world model, language model, situation model, discourse model,\nand agent model) and what I call anchoring processes (incremental processing,\nincremental learning, conversational grounding, multimodal grounding) that bind\nthe agent to the here, now, and us.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlangen_D/0/1/0/all/0/1\">David Schlangen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08624","description":"<p>In this paper, we present InstructABSA, Aspect-Based Sentiment Analysis\n(ABSA) using instruction learning paradigm for all ABSA subtasks: Aspect Term\nExtraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint Task\nmodeling. Our method introduces positive, negative, and neutral examples to\neach training sample, and instruction tunes the model (Tk-Instruct Base) for\neach ABSA subtask, yielding significant performance improvements. Experimental\nresults on the Sem Eval 2014 dataset demonstrate that InstructABSA outperforms\nthe previous state-of-the-art (SOTA) approaches on all three ABSA subtasks\n(ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger\nmodels. In particular, InstructABSA surpasses the SOTA on the restaurant ATE\nsubtask by 7.31% points and on the Laptop Joint Task by 8.63% points. Our\nresults also suggest a strong generalization ability to unseen tasks across all\nthree subtasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scaria_K/0/1/0/all/0/1\">Kevin Scaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawant_S/0/1/0/all/0/1\">Saurabh Arjun Sawant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-aware Self-training for Low-resource Neural Sequence Labeling. (arXiv:2302.08659v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08659","description":"<p>Neural sequence labeling (NSL) aims at assigning labels for input language\ntokens, which covers a broad range of applications, such as named entity\nrecognition (NER) and slot filling, etc. However, the satisfying results\nachieved by traditional supervised-based approaches heavily depend on the large\namounts of human annotation data, which may not be feasible in real-world\nscenarios due to data privacy and computation efficiency issues. This paper\npresents SeqUST, a novel uncertain-aware self-training framework for NSL to\naddress the labeled data scarcity issue and to effectively utilize unlabeled\ndata. Specifically, we incorporate Monte Carlo (MC) dropout in Bayesian neural\nnetwork (BNN) to perform uncertainty estimation at the token level and then\nselect reliable language tokens from unlabeled data based on the model\nconfidence and certainty. A well-designed masked sequence labeling task with a\nnoise-robust loss supports robust training, which aims to suppress the problem\nof noisy pseudo labels. In addition, we develop a Gaussian-based consistency\nregularization technique to further improve the model robustness on\nGaussian-distributed perturbed representations. This effectively alleviates the\nover-fitting dilemma originating from pseudo-labeled augmented data. Extensive\nexperiments over six benchmarks demonstrate that our SeqUST framework\neffectively improves the performance of self-training, and consistently\noutperforms strong baselines by a large margin in low-resource scenarios\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Ming Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aoying Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Subtask Graph Generation from Instructional Videos. (arXiv:2302.08672v1 [cs.LG])","link":"http://arxiv.org/abs/2302.08672","description":"<p>Real-world tasks consist of multiple inter-dependent subtasks (e.g., a dirty\npan needs to be washed before it can be used for cooking). In this work, we aim\nto model the causal dependencies between such subtasks from instructional\nvideos describing the task. This is a challenging problem since complete\ninformation about the world is often inaccessible from videos, which demands\nrobust learning mechanisms to understand the causal structure of events. We\npresent Multimodal Subtask Graph Generation (MSG2), an approach that constructs\na Subtask Graph defining the dependency between a task's subtasks relevant to a\ntask from noisy web videos. Graphs generated by our multimodal approach are\ncloser to human-annotated graphs compared to prior approaches. MSG2 further\nperforms the downstream task of next subtask prediction 85% and 30% more\naccurately than recent video transformer models in the ProceL and CrossTask\ndatasets, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Yunseok Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1\">Sungryull Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logeswaran_L/0/1/0/all/0/1\">Lajanugen Logeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tiange Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Moontae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DREEAM: Guiding Attention with Evidence for Improving Document-Level Relation Extraction. (arXiv:2302.08675v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08675","description":"<p>Document-level relation extraction (DocRE) is the task of identifying all\nrelations between each entity pair in a document. Evidence, defined as\nsentences containing clues for the relationship between an entity pair, has\nbeen shown to help DocRE systems focus on relevant texts, thus improving\nrelation extraction. However, evidence retrieval (ER) in DocRE faces two major\nissues: high memory consumption and limited availability of annotations. This\nwork aims at addressing these issues to improve the usage of ER in DocRE.\nFirst, we propose DREEAM, a memory-efficient approach that adopts evidence\ninformation as the supervisory signal, thereby guiding the attention modules of\nthe DocRE system to assign high weights to evidence. Second, we propose a\nself-training strategy for DREEAM to learn ER from automatically-generated\nevidence on massive data without evidence annotations. Experimental results\nreveal that our approach exhibits state-of-the-art performance on the DocRED\nbenchmark for both DocRE and ER. To the best of our knowledge, DREEAM is the\nfirst approach to employ ER self-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Youmi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">An Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Propaganda Processing. (arXiv:2302.08709v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08709","description":"<p>Propaganda campaigns have long been used to influence public opinion via\ndisseminating biased and/or misleading information. Despite the increasing\nprevalence of propaganda content on the Internet, few attempts have been made\nby AI researchers to analyze such content. We introduce the task of multimodal\npropaganda processing, where the goal is to automatically analyze propaganda\ncontent. We believe that this task presents a long-term challenge to AI\nresearchers and that successful processing of propaganda could bring machine\nunderstanding one important step closer to human understanding. We discuss the\ntechnical challenges associated with this task and outline the steps that need\nto be taken to address it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ng_V/0/1/0/all/0/1\">Vincent Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate Speech and Offensive Language Detection using an Emotion-aware Shared Encoder. (arXiv:2302.08777v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08777","description":"<p>The rise of emergence of social media platforms has fundamentally altered how\npeople communicate, and among the results of these developments is an increase\nin online use of abusive content. Therefore, automatically detecting this\ncontent is essential for banning inappropriate information, and reducing\ntoxicity and violence on social media platforms. The existing works on hate\nspeech and offensive language detection produce promising results based on\npre-trained transformer models, however, they considered only the analysis of\nabusive content features generated through annotated datasets. This paper\naddresses a multi-task joint learning approach which combines external\nemotional features extracted from another corpora in dealing with the\nimbalanced and scarcity of labeled datasets. Our analysis are using two\nwell-known Transformer-based models, BERT and mBERT, where the later is used to\naddress abusive content detection in multi-lingual scenarios. Our model jointly\nlearns abusive content detection with emotional features by sharing\nrepresentations through transformers' shared encoder. This approach increases\ndata efficiency, reduce overfitting via shared representations, and ensure fast\nlearning by leveraging auxiliary information. Our findings demonstrate that\nemotional knowledge helps to more reliably identify hate speech and offensive\nlanguage across datasets. Our hate speech detection Multi-task model exhibited\n3% performance improvement over baseline models, but the performance of\nmulti-task models were not significant for offensive language detection task.\nMore interestingly, in both tasks, multi-task models exhibits less false\npositive errors compared to single task scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mnassri_K/0/1/0/all/0/1\">Khouloud Mnassri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajapaksha_P/0/1/0/all/0/1\">Praboda Rajapaksha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farahbakhsh_R/0/1/0/all/0/1\">Reza Farahbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crespi_N/0/1/0/all/0/1\">Noel Crespi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Response Generation for Chinese Reading Comprehension. (arXiv:2302.08817v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08817","description":"<p>Machine reading comprehension (MRC) is an important area of conversation\nagents and draws a lot of attention. However, there is a notable limitation to\ncurrent MRC benchmarks: The labeled answers are mostly either spans extracted\nfrom the target corpus or the choices of the given candidates, ignoring the\nnatural aspect of high-quality responses. As a result, MRC models trained on\nthese datasets can not generate human-like responses in real QA scenarios. To\nthis end, we construct a new dataset called Penguin to promote the research of\nMRC, providing a training and test bed for natural response generation to real\nscenarios. Concretely, Penguin consists of 200k training data with high-quality\nfluent, and well-informed responses. Penguin is the first benchmark towards\nnatural response generation in Chinese MRC on a relatively large scale. To\naddress the challenges in Penguin, we develop two strong baselines: end-to-end\nand two-stage frameworks. Following that, we further design Prompt-BART:\nfine-tuning the pre-trained generative language models with a mixture of prefix\nprompts in Penguin. Extensive experiments validated the effectiveness of this\ndesign.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yinan Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baoyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"False perspectives on human language: why statistics needs linguistics. (arXiv:2302.08822v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08822","description":"<p>A sharp tension exists about the nature of human language between two\nopposite parties: those who believe that statistical surface distributions, in\nparticular using measures like surprisal, provide a better understanding of\nlanguage processing, vs. those who believe that discrete hierarchical\nstructures implementing linguistic information such as syntactic ones are a\nbetter tool. In this paper, we show that this dichotomy is a false one. Relying\non the fact that statistical measures can be defined on the basis of either\nstructural or non-structural models, we provide empirical evidence that only\nmodels of surprisal that reflect syntactic structure are able to account for\nlanguage regularities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Greco_M/0/1/0/all/0/1\">Matteo Greco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cometa_A/0/1/0/all/0/1\">Andrea Cometa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artoni_F/0/1/0/all/0/1\">Fiorenzo Artoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_R/0/1/0/all/0/1\">Robert Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moro_A/0/1/0/all/0/1\">Andrea Moro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring External Knowledge for Accurate modeling of Visual and Language Problems. (arXiv:2302.08901v1 [cs.CV])","link":"http://arxiv.org/abs/2302.08901","description":"<p>The interest in Artificial Intelligence (AI) and its applications has seen\nunprecedented growth in the last few years. The success can be partly\nattributed to the advancements of deep neural networks made in the sub-fields\nof AI such as Computer Vision (CV) and Natural Language Processing (NLP). The\npromising research area that this dissertation focuses on is visual and\nlanguage understanding which involves many challenging tasks, i.e.,\nclassification, detection, segmentation, machine translation and captioning,\netc. The state-of-the-art methods for solving these problems usually involves\nonly two parts: source data and target labels, which is rather insufficient\nespecially when the dataset is small. Meanwhile, many external tools or sources\ncan provide extra useful information (external knowledge) that can help improve\nthe performance of these methods. For example, a detection model has been\napplied to provide better object features than state-of-the-art ResNet for\nimage captioning models. Inspired by this observation, we developed a\nmethodology that we can first extract external knowledge and then integrate it\nwith the original models. The external knowledge has to be extracted from the\ndataset, or can directly come from external, e.g., grammar rules or scene\ngraphs. We apply this methodology to different AI tasks, including machine\ntranslation and image captioning and improve the original state-of-the-art\nmodels by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuewen Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Massively Multilingual Shallow Fusion with Large Language Models. (arXiv:2302.08917v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08917","description":"<p>While large language models (LLM) have made impressive progress in natural\nlanguage processing, it remains unclear how to utilize them in improving\nautomatic speech recognition (ASR). In this work, we propose to train a single\nmultilingual language model (LM) for shallow fusion in multiple languages. We\npush the limits of the multilingual LM to cover up to 84 languages by scaling\nup using a mixture-of-experts LLM, i.e., generalist language model (GLaM). When\nthe number of experts increases, GLaM dynamically selects only two at each\ndecoding step to keep the inference computation roughly constant. We then apply\nGLaM to a multilingual shallow fusion task based on a state-of-the-art\nend-to-end model. Compared to a dense LM of similar computation during\ninference, GLaM reduces the WER of an English long-tail test set by 4.4%\nrelative. In a multilingual shallow fusion task, GLaM improves 41 out of 50\nlanguages with an average relative WER reduction of 3.85%, and a maximum\nreduction of 10%. Compared to the baseline model, GLaM achieves an average WER\nreduction of 5.53% over 43 languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Ke Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabrera_R/0/1/0/all/0/1\">Rodrigo Cabrera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Implicit Distribution Alignment Networks for Cross-Corpus Speech Emotion Recognition. (arXiv:2302.08921v1 [cs.SD])","link":"http://arxiv.org/abs/2302.08921","description":"<p>In this paper, we propose a novel deep transfer learning method called deep\nimplicit distribution alignment networks (DIDAN) to deal with cross-corpus\nspeech emotion recognition (SER) problem, in which the labeled training\n(source) and unlabeled testing (target) speech signals come from different\ncorpora. Specifically, DIDAN first adopts a simple deep regression network\nconsisting of a set of convolutional and fully connected layers to directly\nregress the source speech spectrums into the emotional labels such that the\nproposed DIDAN can own the emotion discriminative ability. Then, such ability\nis transferred to be also applicable to the target speech samples regardless of\ncorpus variance by resorting to a well-designed regularization term called\nimplicit distribution alignment (IDA). Unlike widely-used maximum mean\ndiscrepancy (MMD) and its variants, the proposed IDA absorbs the idea of sample\nreconstruction to implicitly align the distribution gap, which enables DIDAN to\nlearn both emotion discriminative and corpus invariant features from speech\nspectrums. To evaluate the proposed DIDAN, extensive cross-corpus SER\nexperiments on widely-used speech emotion corpora are carried out. Experimental\nresults show that the proposed DIDAN can outperform lots of recent\nstate-of-the-art methods in coping with the cross-corpus SER tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jincen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_Y/0/1/0/all/0/1\">Yuan Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenming Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_H/0/1/0/all/0/1\">Hailun Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Li Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Data Types More Problems: A Temporal Analysis of Complexity, Stability, and Sensitivity in Privacy Policies. (arXiv:2302.08936v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08936","description":"<p>Collecting personally identifiable information (PII) on data subjects has\nbecome big business. Data brokers and data processors are part of a\nmulti-billion-dollar industry that profits from collecting, buying, and selling\nconsumer data. Yet there is little transparency in the data collection industry\nwhich makes it difficult to understand what types of data are being collected,\nused, and sold, and thus the risk to individual data subjects. In this study,\nwe examine a large textual dataset of privacy policies from 1997-2019 in order\nto investigate the data collection activities of data brokers and data\nprocessors. We also develop an original lexicon of PII-related terms\nrepresenting PII data types curated from legislative texts. This mesoscale\nanalysis looks at privacy policies overtime on the word, topic, and network\nlevels to understand the stability, complexity, and sensitivity of privacy\npolicies over time. We find that (1) privacy legislation correlates with\nchanges in stability and turbulence of PII data types in privacy policies; (2)\nthe complexity of privacy policies decreases over time and becomes more\nregularized; (3) sensitivity rises over time and shows spikes that are\ncorrelated with events when new privacy legislation is introduced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovato_J/0/1/0/all/0/1\">Juniper Lovato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_P/0/1/0/all/0/1\">Philip Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suchdev_P/0/1/0/all/0/1\">Parisa Suchdev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">Peter S. Dodds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entry Separation using a Mixed Visual and Textual Language Model: Application to 19th century French Trade Directories. (arXiv:2302.08948v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08948","description":"<p>When extracting structured data from repetitively organized documents, such\nas dictionaries, directories, or even newspapers, a key challenge is to\ncorrectly segment what constitutes the basic text regions for the target\ndatabase. Traditionally, such a problem was tackled as part of the layout\nanalysis and was mostly based on visual clues for dividing (top-down)\napproaches. Some agglomerating (bottom-up) approaches started to consider\ntextual information to link similar contents, but they required a proper\nover-segmentation of fine-grained units. In this work, we propose a new\npragmatic approach whose efficiency is demonstrated on 19th century French\nTrade Directories. We propose to consider two sub-problems: coarse layout\ndetection (text columns and reading order), which is assumed to be effective\nand not detailed here, and a fine-grained entry separation stage for which we\npropose to adapt a state-of-the-art Named Entity Recognition (NER) approach. By\ninjecting special visual tokens, coding, for instance, indentation or breaks,\ninto the token stream of the language model used for NER purpose, we can\nleverage both textual and visual knowledge simultaneously. Code, data, results\nand models are available at\nhttps://github.com/soduco/paper-entryseg-icdar23-code,\nhttps://huggingface.co/HueyNemud/ (icdar23-entrydetector* variants)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dumenieu_B/0/1/0/all/0/1\">Bertrand Dum&#xe9;nieu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Carlinet_E/0/1/0/all/0/1\">Edwin Carlinet</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Abadie_N/0/1/0/all/0/1\">Nathalie Abadie</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Chazalon_J/0/1/0/all/0/1\">Joseph Chazalon</a> (2) ((1) LaD&#xe9;HiS, CRH, EHESS, France, (2) EPITA Research Laboratory (LRE), France, (3) Univ. Gustave Eiffel, IGN-ENSG, LaSTIG, France)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handling the Alignment for Wake Word Detection: A Comparison Between Alignment-Based, Alignment-Free and Hybrid Approaches. (arXiv:2302.08950v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08950","description":"<p>Wake word detection exists in most intelligent homes and portable devices. It\noffers these devices the ability to \"wake up\" when summoned at a low cost of\npower and computing. This paper focuses on understanding alignment's role in\ndeveloping a wake-word system that answers a generic phrase. We discuss three\napproaches. The first is alignment-based, where the model is trained with\nframe-wise cross-entropy. The second is alignment-free, where the model is\ntrained with CTC. The third, proposed by us, is a hybrid solution in which the\nmodel is trained with a small set of aligned data and then tuned with a\nsizeable unaligned dataset. We compare the three approaches and evaluate the\nimpact of the different aligned-to-unaligned ratios for hybrid training. Our\nresults show that the alignment-free system performs better alignment-based for\nthe target operating point, and with a small fraction of the data (20%), we can\ntrain a model that complies with our initial constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_V/0/1/0/all/0/1\">Vinicius Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yiteng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1\">Yuan Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaojun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1\">Li Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Ming Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages. (arXiv:2302.08956v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08956","description":"<p>Africa is home to over 2000 languages from over six language families and has\nthe highest linguistic diversity among all continents. This includes 75\nlanguages with at least one million speakers each. Yet, there is little NLP\nresearch conducted on African languages. Crucial in enabling such research is\nthe availability of high-quality annotated datasets. In this paper, we\nintroduce AfriSenti, which consists of 14 sentiment datasets of 110,000+ tweets\nin 14 African languages (Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda,\nMoroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili,\nTigrinya, Twi, Xitsonga, and Yor\\`ub\\'a) from four language families annotated\nby native speakers. The data is used in SemEval 2023 Task 12, the first\nAfro-centric SemEval shared task. We describe the data collection methodology,\nannotation process, and related challenges when curating each of the datasets.\nWe conduct experiments with different sentiment classification baselines and\ndiscuss their usefulness. We hope AfriSenti enables new work on\nunder-represented languages. The dataset is available at\nhttps://github.com/afrisenti-semeval/afrisent-semeval-2023 and can also be\nloaded as a huggingface datasets\n(https://huggingface.co/datasets/shmuhammad/AfriSenti).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayele_A/0/1/0/all/0/1\">Abinew Ali Ayele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ousidhoum_N/0/1/0/all/0/1\">Nedjma Ousidhoum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yimam_S/0/1/0/all/0/1\">Seid Muhie Yimam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Ibrahim Sa&#x27;id Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beloucif_M/0/1/0/all/0/1\">Meriem Beloucif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hourrane_O/0/1/0/all/0/1\">Oumaima Hourrane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brazdil_P/0/1/0/all/0/1\">Pavel Brazdil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_F/0/1/0/all/0/1\">Felermino D&#xe1;rio M&#xe1;rio Ant&#xf3;nio Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_D/0/1/0/all/0/1\">Davis Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1\">Salomey Osei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_B/0/1/0/all/0/1\">Bello Shehu Bello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_F/0/1/0/all/0/1\">Falalu Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwadabe_T/0/1/0/all/0/1\">Tajuddeen Gwadabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rutunda_S/0/1/0/all/0/1\">Samuel Rutunda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belay_T/0/1/0/all/0/1\">Tadesse Belay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messelle_W/0/1/0/all/0/1\">Wendimu Baye Messelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balcha_H/0/1/0/all/0/1\">Hailu Beshada Balcha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chala_S/0/1/0/all/0/1\">Sisay Adugna Chala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gebremichael_H/0/1/0/all/0/1\">Hagos Tesfahun Gebremichael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opoku_B/0/1/0/all/0/1\">Bernard Opoku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arthur_S/0/1/0/all/0/1\">Steven Arthur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Like a Good Nearest Neighbor: Practical Content Moderation with Sentence Transformers. (arXiv:2302.08957v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08957","description":"<p>Modern text classification systems have impressive capabilities but are\ninfeasible to deploy and use reliably due to their dependence on prompting and\nbillion-parameter language models. SetFit (Tunstall et al., 2022) is a recent,\npractical approach that fine-tunes a Sentence Transformer under a contrastive\nlearning paradigm and achieves similar results to more unwieldy systems. Text\nclassification is important for addressing the problem of domain drift in\ndetecting harmful content, which plagues all social media platforms. Here, we\npropose Like a Good Nearest Neighbor (LaGoNN), an inexpensive modification to\nSetFit that requires no additional parameters or hyperparameters but modifies\ninput with information about its nearest neighbor, for example, the label and\ntext, in the training data, making novel data appear similar to an instance on\nwhich the model was optimized. LaGoNN is effective at the task of detecting\nharmful content and generally improves performance compared to SetFit. To\ndemonstrate the value of our system, we conduct a thorough study of text\nclassification systems in the context of content moderation under four label\ndistributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bates_L/0/1/0/all/0/1\">Luke Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales. (arXiv:2302.08961v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08961","description":"<p>The quality of text-to-image generation is continuously improving, yet the\nboundaries of its applicability are still unclear. In particular, refinement of\nthe text input with the objective of achieving better results - commonly called\nprompt engineering - so far seems to have not been geared towards work with\npre-existing texts. We investigate whether text-to-image generation and prompt\nengineering could be used to generate basic illustrations of popular\nfairytales. Using Midjourney v4, we engage in action research with a dual aim:\nto attempt to generate 5 believable illustrations for each of 5 popular\nfairytales, and to define a prompt engineering process that starts from a\npre-existing text and arrives at an illustration of it. We arrive at a\ntentative 4-stage process: i) initial prompt, ii) composition adjustment, iii)\nstyle refinement, and iv) variation selection. We also discuss three reasons\nwhy the generation model struggles with certain illustrations: difficulties\nwith counts, bias from stereotypical configurations and inability to depict\noverly fantastic situations. Our findings are not limited to the specific\ngeneration model and are intended to be generalisable to future ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruskov_M/0/1/0/all/0/1\">Martin Ruskov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Fine-Grained Information: Identifying the Type and Location of Translation Errors. (arXiv:2302.08975v1 [cs.CL])","link":"http://arxiv.org/abs/2302.08975","description":"<p>Fine-grained information on translation errors is helpful for the translation\nevaluation community. Existing approaches can not synchronously consider error\nposition and type, failing to integrate the error information of both. In this\npaper, we propose Fine-Grained Translation Error Detection (FG-TED) task,\naiming at identifying both the position and the type of translation errors on\ngiven source-hypothesis sentence pairs. Besides, we build an FG-TED model to\npredict the \\textbf{addition} and \\textbf{omission} errors -- two typical\ntranslation accuracy errors. First, we use a word-level classification paradigm\nto form our model and use the shortcut learning reduction to relieve the\ninfluence of monolingual features. Besides, we construct synthetic datasets for\nmodel training, and relieve the disagreement of data labeling in authoritative\ndatasets, making the experimental benchmark concordant. Experiments show that\nour model can identify both error type and position concurrently, and gives\nstate-of-the-art results on the restored dataset. Our model also delivers more\nreliable predictions on low-resource and transfer scenarios than existing\nbaselines. The related datasets and the source code will be released in the\nfuture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_K/0/1/0/all/0/1\">Keqin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F.Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing and Evaluating Interfaces that Highlight News Coverage Diversity Using Discord Questions. (arXiv:2302.08997v1 [cs.HC])","link":"http://arxiv.org/abs/2302.08997","description":"<p>Modern news aggregators do the hard work of organizing a large news stream,\ncreating collections for a given news story with tens of source options. This\npaper shows that navigating large source collections for a news story can be\nchallenging without further guidance. In this work, we design three interfaces\n-- the Annotated Article, the Recomposed Article, and the Question Grid --\naimed at accompanying news readers in discovering coverage diversity while they\nread. A first usability study with 10 journalism experts confirms the designed\ninterfaces all reveal coverage diversity and determine each interface's\npotential use cases and audiences. In a second usability study, we developed\nand implemented a reading exercise with 95 novice news readers to measure\nexposure to coverage diversity. Results show that Annotated Article users are\nable to answer questions 34% more completely than with two existing interfaces\nwhile finding the interface equally easy to use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murakhovska_L/0/1/0/all/0/1\">Lidiya Murakhovs&#x27;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang &#x27;Anthony&#x27; Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CK-Transformer: Commonsense Knowledge Enhanced Transformers for Referring Expression Comprehension. (arXiv:2302.09027v1 [cs.CV])","link":"http://arxiv.org/abs/2302.09027","description":"<p>The task of multimodal referring expression comprehension (REC), aiming at\nlocalizing an image region described by a natural language expression, has\nrecently received increasing attention within the research comminity. In this\npaper, we specifically focus on referring expression comprehension with\ncommonsense knowledge (KB-Ref), a task which typically requires reasoning\nbeyond spatial, visual or semantic information. We propose a novel framework\nfor Commonsense Knowledge Enhanced Transformers (CK-Transformer) which\neffectively integrates commonsense knowledge into the representations of\nobjects in an image, facilitating identification of the target objects referred\nto by the expressions. We conduct extensive experiments on several benchmarks\nfor the task of KB-Ref. Our results show that the proposed CK-Transformer\nachieves a new state of the art, with an absolute improvement of 3.14% accuracy\nover the existing state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1\">Helen Yannakoudakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1\">Ekaterina Shutova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex QA and language models hybrid architectures, Survey. (arXiv:2302.09051v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09051","description":"<p>This paper provides a survey of the state of the art of hybrid language\nmodels architectures and strategies for \"complex\" question-answering (QA, CQA,\nCPS). Very large language models are good at leveraging public data on standard\nproblems but once you want to tackle more specific complex questions or\nproblems you may need specific architecture, knowledge, skills, tasks, methods,\nsensitive data, performance, human approval and versatile feedback... This\nsurvey extends findings from the robust community edited research papers BIG,\nBLOOM and HELM which open source, benchmark and analyze limits and challenges\nof large language models in terms of tasks complexity and strict evaluation on\naccuracy (e.g. fairness, robustness, toxicity, ...). It identifies the key\nelements used with Large Language Models (LLM) to solve complex questions or\nproblems. Recent projects like ChatGPT and GALACTICA have allowed\nnon-specialists to grasp the great potential as well as the equally strong\nlimitations of language models in complex QA. Hybridizing these models with\ndifferent components could allow to overcome these different limits and go much\nfurther. We discuss some challenges associated with complex QA, including\ndomain adaptation, decomposition and efficient multi-step QA, long form QA,\nnon-factoid QA, safety and multi-sensitivity data protection, multimodal\nsearch, hallucinations, QA explainability and truthfulness, time dimension.\nTherefore we review current solutions and promising strategies, using elements\nsuch as hybrid LLM architectures, human-in-the-loop reinforcement learning,\nprompting adaptation, neuro-symbolic and structured knowledge grounding,\nprogram synthesis, and others. We analyze existing solutions and provide an\noverview of the current research and trends in the area of complex QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daull_X/0/1/0/all/0/1\">Xavier Daull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellot_P/0/1/0/all/0/1\">Patrice Bellot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruno_E/0/1/0/all/0/1\">Emmanuel Bruno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_V/0/1/0/all/0/1\">Vincent Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murisasco_E/0/1/0/all/0/1\">Elisabeth Murisasco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aligning AI With Shared Human Values. (arXiv:2008.02275v6 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2008.02275","description":"<p>We show how to assess a language model's knowledge of basic concepts of\nmorality. We introduce the ETHICS dataset, a new benchmark that spans concepts\nin justice, well-being, duties, virtues, and commonsense morality. Models\npredict widespread moral judgments about diverse text scenarios. This requires\nconnecting physical and social world knowledge to value judgements, a\ncapability that may enable us to steer chatbot outputs or eventually regularize\nopen-ended reinforcement learning agents. With the ETHICS dataset, we find that\ncurrent language models have a promising but incomplete ability to predict\nbasic human ethical judgements. Our work shows that progress can be made on\nmachine ethics today, and it provides a steppingstone toward AI that is aligned\nwith human values.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1\">Collin Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Critch_A/0/1/0/all/0/1\">Andrew Critch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jerry Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Keyphrase Extraction via Interpretable Neural Networks. (arXiv:2203.07640v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07640","description":"<p>Keyphrase extraction aims at automatically extracting a list of \"important\"\nphrases representing the key concepts in a document. Prior approaches for\nunsupervised keyphrase extraction resorted to heuristic notions of phrase\nimportance via embedding clustering or graph centrality, requiring extensive\ndomain expertise. Our work presents a simple alternative approach which defines\nkeyphrases as document phrases that are salient for predicting the topic of the\ndocument. To this end, we propose INSPECT -- an approach that uses\nself-explaining models for identifying influential keyphrases in a document by\nmeasuring the predictive impact of input phrases on the downstream task of the\ndocument topic classification. We show that this novel method not only\nalleviates the need for ad-hoc heuristics but also achieves state-of-the-art\nresults in unsupervised keyphrase extraction in four datasets across two\ndomains: scientific publications and news articles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Rishabh Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1\">Vidhisha Balachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saldanha_E/0/1/0/all/0/1\">Emily Saldanha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glenski_M/0/1/0/all/0/1\">Maria Glenski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volkova_S/0/1/0/all/0/1\">Svitlana Volkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Descartes: Generating Short Descriptions of Wikipedia Articles. (arXiv:2205.10012v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10012","description":"<p>Wikipedia is one of the richest knowledge sources on the Web today. In order\nto facilitate navigating, searching, and maintaining its content, Wikipedia's\nguidelines state that all articles should be annotated with a so-called short\ndescription indicating the article's topic (e.g., the short description of beer\nis \"Alcoholic drink made from fermented cereal grains\"). Nonetheless, a large\nfraction of articles (ranging from 10.2% in Dutch to 99.7% in Kazakh) have no\nshort description yet, with detrimental effects for millions of Wikipedia\nusers. Motivated by this problem, we introduce the novel task of automatically\ngenerating short descriptions for Wikipedia articles and propose Descartes, a\nmultilingual model for tackling it. Descartes integrates three sources of\ninformation to generate an article description in a target language: the text\nof the article in all its language versions, the already-existing descriptions\n(if any) of the article in other languages, and semantic type information\nobtained from a knowledge graph. We evaluate a Descartes model trained for\nhandling 25 languages simultaneously, showing that it beats baselines\n(including a strong translation-based baseline) and performs on par with\nmonolingual models tailored for specific languages. A human evaluation on three\nlanguages further shows that the quality of Descartes's descriptions is largely\nindistinguishable from that of human-written descriptions; e.g., 91.3% of our\nEnglish descriptions (vs. 92.1% of human-written descriptions) pass the bar for\ninclusion in Wikipedia, suggesting that Descartes is ready for production, with\nthe potential to support human editors in filling a major gap in today's\nWikipedia across languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakota_M/0/1/0/all/0/1\">Marija Sakota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Write and Paint: Generative Vision-Language Models are Unified Modal Learners. (arXiv:2206.07699v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07699","description":"<p>Recent advances in vision-language pre-training have pushed the\nstate-of-the-art on various vision-language tasks, making machines more capable\nof multi-modal writing (image-to-text generation) and painting (text-to-image\ngeneration). However, few studies investigate if these two essential\ncapabilities can be learned together and boost each other, making a versatile\nand powerful multi-modal foundation model. In this work, we disclose the\npotential of symmetric generative vision-language pre-training in learning to\nwrite and paint concurrently, and propose a new unified modal model, named\nDaVinci, trained with prefix language modeling and prefix image modeling, a\nsimple generative self-supervised objective on image-text pairs. Thanks to the\nproposed prefix multi-modal modeling framework, DaVinci is simple to train,\nscalable to huge data, adaptable to both writing and painting tasks, and also\nstrong on other vision, text, and multi-modal understanding tasks. DaVinci\nachieves competitive performance on a wide range of 27 generation/understanding\ntasks and demonstrates the superiority of combining vision/language generative\npre-training. Furthermore, we carefully benchmark the performance of different\nvision-language pre-training objectives on different scales of pre-training\ndatasets on a heterogeneous and broad distribution coverage. Our results\ndemonstrate the potential of exploiting self-supervision in both language and\nvision inputs, and establish new, stronger baselines for future comparisons at\ndifferent data scales. The code and pre-trained models are available at\nhttps://github.com/shizhediao/DaVinci.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1\">Shizhe Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinsong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiawei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation. (arXiv:2210.00193v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.00193","description":"<p>We present FRMT, a new dataset and evaluation benchmark for Few-shot\nRegion-aware Machine Translation, a type of style-targeted translation. The\ndataset consists of professional translations from English into two regional\nvariants each of Portuguese and Mandarin Chinese. Source documents are selected\nto enable detailed analysis of phenomena of interest, including lexically\ndistinct terms and distractor terms. We explore automatic evaluation metrics\nfor FRMT and validate their correlation with expert human evaluation across\nboth region-matched and mismatched rating scenarios. Finally, we present a\nnumber of baseline models for this task, and offer guidelines for how\nresearchers can train, evaluate, and compare their own models. Our dataset and\nevaluation code are publicly available: https://bit.ly/frmt-task\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riley_P/0/1/0/all/0/1\">Parker Riley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozat_T/0/1/0/all/0/1\">Timothy Dozat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botha_J/0/1/0/all/0/1\">Jan A. Botha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrette_D/0/1/0/all/0/1\">Dan Garrette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesa_J/0/1/0/all/0/1\">Jason Riesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Kernel-Based View of Language Model Fine-Tuning. (arXiv:2210.05643v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.05643","description":"<p>It has become standard to solve NLP tasks by fine-tuning pre-trained language\nmodels (LMs), especially in low-data settings. There is minimal theoretical\nunderstanding of empirical success, e.g., why fine-tuning a model with $10^8$\nor more parameters on a couple dozen training points does not result in\noverfitting. We investigate whether the Neural Tangent Kernel (NTK) - which\noriginated as a model to study the gradient descent dynamics of infinitely wide\nnetworks with suitable random initialization - describes fine-tuning of\npre-trained LMs. This study was inspired by the decent performance of NTK for\ncomputer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam\nand use Tensor Programs (Yang, 2020) to characterize conditions under which the\nNTK lens may describe fine-tuning updates to pre-trained language models.\nExtensive experiments on 14 NLP tasks validate our theory and show that\nformulating the downstream task as a masked word prediction problem through\nprompting often induces kernel-based dynamics during fine-tuning. Finally, we\nuse this kernel view to propose an explanation for the success of\nparameter-efficient subspace-based fine-tuning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malladi_S/0/1/0/all/0/1\">Sadhika Malladi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wettig_A/0/1/0/all/0/1\">Alexander Wettig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dingli Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Sanjeev Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alibaba-Translate China's Submission for WMT 2022 Metrics Shared Task. (arXiv:2210.09683v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09683","description":"<p>In this report, we present our submission to the WMT 2022 Metrics Shared\nTask. We build our system based on the core idea of UNITE (Unified Translation\nEvaluation), which unifies source-only, reference-only, and\nsource-reference-combined evaluation scenarios into one single model.\nSpecifically, during the model pre-training phase, we first apply the\npseudo-labeled data examples to continuously pre-train UNITE. Notably, to\nreduce the gap between pre-training and fine-tuning, we use data cropping and a\nranking-based score normalization strategy. During the fine-tuning phase, we\nuse both Direct Assessment (DA) and Multidimensional Quality Metrics (MQM) data\nfrom past years' WMT competitions. Specially, we collect the results from\nmodels with different pre-trained language model backbones, and use different\nensembling strategies for involved translation directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_K/0/1/0/all/0/1\">Keqin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alibaba-Translate China's Submission for WMT 2022 Quality Estimation Shared Task. (arXiv:2210.10049v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10049","description":"<p>In this paper, we present our submission to the sentence-level MQM benchmark\nat Quality Estimation Shared Task, named UniTE (Unified Translation\nEvaluation). Specifically, our systems employ the framework of UniTE, which\ncombined three types of input formats during training with a pre-trained\nlanguage model. First, we apply the pseudo-labeled data examples for the\ncontinuously pre-training phase. Notably, to reduce the gap between\npre-training and fine-tuning, we use data pruning and a ranking-based score\nnormalization strategy. For the fine-tuning phase, we use both Direct\nAssessment (DA) and Multidimensional Quality Metrics (MQM) data from past\nyears' WMT competitions. Finally, we collect the source-only evaluation\nresults, and ensemble the predictions generated by two UniTE models, whose\nbackbones are XLM-R and InfoXLM, respectively. Results show that our models\nreach 1st overall ranking in the Multilingual and English-Russian settings, and\n2nd overall ranking in English-German and Chinese-English settings, showing\nrelatively strong performances in this year's quality estimation competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_K/0/1/0/all/0/1\">Keqin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F.Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modular Hybrid Autoregressive Transducer. (arXiv:2210.17049v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.17049","description":"<p>Text-only adaptation of a transducer model remains challenging for end-to-end\nspeech recognition since the transducer has no clearly separated acoustic model\n(AM), language model (LM) or blank model. In this work, we propose a modular\nhybrid autoregressive transducer (MHAT) that has structurally separated label\nand blank decoders to predict label and blank distributions, respectively,\nalong with a shared acoustic encoder. The encoder and label decoder outputs are\ndirectly projected to AM and internal LM scores and then added to compute label\nposteriors. We train MHAT with an internal LM loss and a HAT loss to ensure\nthat its internal LM becomes a standalone neural LM that can be effectively\nadapted to text. Moreover, text adaptation of MHAT fosters a much better LM\nfusion than internal LM subtraction-based methods. On Google's large-scale\nproduction data, a multi-domain MHAT adapted with 100B sentences achieves\nrelative WER reductions of up to 12.4% without LM fusion and 21.5% with LM\nfusion from 400K-hour trained HAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tongzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gary Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audhkhasi_K/0/1/0/all/0/1\">Kartik Audhkhasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emond_J/0/1/0/all/0/1\">Jesse Emond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">W. Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Variani_E/0/1/0/all/0/1\">Ehsan Variani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yinghui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_P/0/1/0/all/0/1\">Pedro J. Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Building Text-To-Speech Systems for the Next Billion Users. (arXiv:2211.09536v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.09536","description":"<p>Deep learning based text-to-speech (TTS) systems have been evolving rapidly\nwith advances in model architectures, training methodologies, and\ngeneralization across speakers and languages. However, these advances have not\nbeen thoroughly investigated for Indian language speech synthesis. Such\ninvestigation is computationally expensive given the number and diversity of\nIndian languages, relatively lower resource availability, and the diverse set\nof advances in neural TTS that remain untested. In this paper, we evaluate the\nchoice of acoustic models, vocoders, supplementary loss functions, training\nschedules, and speaker and language diversity for Dravidian and Indo-Aryan\nlanguages. Based on this, we identify monolingual models with FastPitch and\nHiFi-GAN V1, trained jointly on male and female speakers to perform the best.\nWith this setup, we train and evaluate TTS models for 13 languages and find our\nmodels to significantly improve upon existing models in all languages as\nmeasured by mean opinion scores. We open-source all models on the Bhashini\nplatform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gokul Karthik Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_P/0/1/0/all/0/1\">Praveen S V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandakumar_K/0/1/0/all/0/1\">Karthik Nandakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints. (arXiv:2212.05055v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.05055","description":"<p>Training large, deep neural networks to convergence can be prohibitively\nexpensive. As a result, often only a small selection of popular, dense models\nare reused across different contexts and tasks. Increasingly, sparsely\nactivated models, which seek to decouple model size from computation costs, are\nbecoming an attractive alternative to dense models. Although more efficient in\nterms of quality and computation cost, sparse models remain data-hungry and\ncostly to train from scratch in the large scale regime. In this work, we\npropose sparse upcycling -- a simple way to reuse sunk training costs by\ninitializing a sparsely activated Mixture-of-Experts model from a dense\ncheckpoint. We show that sparsely upcycled T5 Base, Large, and XL language\nmodels and Vision Transformer Base and Large models, respectively,\nsignificantly outperform their dense counterparts on SuperGLUE and ImageNet,\nusing only ~50% of the initial dense pretraining sunk cost. The upcycled models\nalso outperform sparse models trained from scratch on 100% of the initial dense\npretraining computation budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Komatsuzaki_A/0/1/0/all/0/1\">Aran Komatsuzaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1\">Joan Puigcerver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_C/0/1/0/all/0/1\">Carlos Riquelme Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1\">Basil Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise. (arXiv:2212.11685v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.11685","description":"<p>In this paper, we introduce a novel dIffusion language modEl pre-training\nframework for text generation, which we call GENIE. GENIE is a large-scale\npretrained diffusion language model that consists of an encoder and a\ndiffusion-based decoder, which can generate text by gradually transforming a\nrandom noise sequence into a coherent text sequence. To pre-train GENIE on a\nlarge-scale language corpus, we design a new continuous paragraph denoise\nobjective, which encourages the diffusion-decoder to reconstruct a clean text\nparagraph from a corrupted version, while preserving the semantic and syntactic\ncoherence. We evaluate GENIE on four downstream text generation benchmarks,\nnamely XSum, CNN/DailyMail, Gigaword, and CommonGen. Our experimental results\nshow that GENIE achieves comparable performance with the state-of-the-art\nautoregressive models on these benchmarks, and generates more diverse text\nsamples. The code and models of GENIE are available at\nhttps://github.com/microsoft/ProphetNet/tree/master/GENIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenghao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextDescriptives: A Python package for calculating a large variety of metrics from text. (arXiv:2301.02057v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.02057","description":"<p>TextDescriptives is a Python package for calculating a large variety of\nmetrics from text. It is built on top of spaCy and can be easily integrated\ninto existing workflows. The package has already been used for analysing the\nlinguistic stability of clinical texts, creating features for predicting\nneuropsychiatric conditions, and analysing linguistic goals of primary school\nstudents. This paper describes the package and its features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1\">Lasse Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Enevoldsen_K/0/1/0/all/0/1\">Kenneth Enevoldsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case Study using Latent Dirichlet Allocation Method. (arXiv:2301.03029v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.03029","description":"<p>Topic Modelling (TM) is from the research branches of natural language\nunderstanding (NLU) and natural language processing (NLP) that is to facilitate\ninsightful analysis from large documents and datasets, such as a summarisation\nof main topics and the topic changes. This kind of discovery is getting more\npopular in real-life applications due to its impact on big data analytics. In\nthis study, from the social-media and healthcare domain, we apply popular\nLatent Dirichlet Allocation (LDA) methods to model the topic changes in Swedish\nnewspaper articles about Coronavirus. We describe the corpus we created\nincluding 6515 articles, methods applied, and statistics on topic changes over\napproximately 1 year and two months period of time from 17th January 2020 to\n13th March 2021. We hope this work can be an asset for grounding applications\nof topic modelling and can be inspiring for similar case studies in an era with\npandemics, to support socio-economic impact research as well as clinical and\nhealthcare analytics. Our data and source code are openly available at\nhttps://github. com/poethan/Swed_Covid_TM Keywords: Latent Dirichlet Allocation\n(LDA); Topic Modelling; Coronavirus; Pandemics; Natural Language Understanding;\nBERT-topic\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Griciute_B/0/1/0/all/0/1\">Bernadeta Grici&#x16b;t&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Chain-of-Thought Reasoning in Language Models. (arXiv:2302.00923v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00923","description":"<p>Large language models (LLMs) have shown impressive performance on complex\nreasoning by leveraging chain-of-thought (CoT) prompting to generate\nintermediate reasoning chains as the rationale to infer the answer. However,\nexisting CoT studies have focused on the language modality. We propose\nMultimodal-CoT that incorporates language (text) and vision (images) modalities\ninto a two-stage framework that separates rationale generation and answer\ninference. In this way, answer inference can leverage better generated\nrationales that are based on multimodal information. With Multimodal-CoT, our\nmodel under 1 billion parameters outperforms the previous state-of-the-art LLM\n(GPT-3.5) by 16 percentage points (75.17%-&gt;91.68% accuracy) on the ScienceQA\nbenchmark and even surpasses human performance. Code is publicly available\navailable at https://github.com/amazon-science/mm-cot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alex Smola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$IC^3$: Image Captioning by Committee Consensus. (arXiv:2302.01328v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2302.01328","description":"<p>If you ask a human to describe an image, they might do so in a thousand\ndifferent ways. Traditionally, image captioning models are trained to\napproximate the reference distribution of image captions, however, doing so\nencourages captions that are viewpoint-impoverished. Such captions often focus\non only a subset of the possible details, while ignoring potentially useful\ninformation in the scene. In this work, we introduce a simple, yet novel,\nmethod: \"Image Captioning by Committee Consensus\" ($IC^3$), designed to\ngenerate a single caption that captures high-level details from several\nviewpoints. Notably, humans rate captions produced by $IC^3$ at least as\nhelpful as baseline SOTA models more than two thirds of the time, and $IC^3$\ncaptions can improve the performance of SOTA automated recall systems by up to\n84%, indicating significant material improvements over existing SOTA approaches\nfor visual description. Our code is publicly available at\nhttps://github.com/DavidMChan/caption-by-committee\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1\">David M. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myers_A/0/1/0/all/0/1\">Austin Myers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayanarasimhan_S/0/1/0/all/0/1\">Sudheendra Vijayanarasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1\">David A. Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canny_J/0/1/0/all/0/1\">John Canny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLACES: Prompting Language Models for Social Conversation Synthesis. (arXiv:2302.03269v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03269","description":"<p>Collecting high quality conversational data can be very expensive for most\napplications and infeasible for others due to privacy, ethical, or similar\nconcerns. A promising direction to tackle this problem is to generate synthetic\ndialogues by prompting large language models. In this work, we use a small set\nof expert-written conversations as in-context examples to synthesize a social\nconversation dataset using prompting. We perform several thorough evaluations\nof our synthetic conversations compared to human-collected conversations. This\nincludes various dimensions of conversation quality with human evaluation\ndirectly on the synthesized conversations, and interactive human evaluation of\nchatbots fine-tuned on the synthetically generated dataset. We additionally\ndemonstrate that this prompting approach is generalizable to multi-party\nconversations, providing potential to create new synthetic data for multi-party\ntasks. Our synthetic multi-party conversations were rated more favorably across\nall measured dimensions compared to conversation excerpts sampled from a\nhuman-collected multi-party dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maximillian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenbaum_A/0/1/0/all/0/1\">Andy Rosenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03494","description":"<p>Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,\nwhich is the focus of this study. Eleven categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.06675","description":"<p>We present a method to formulate algorithm discovery as program search, and\napply it to discover optimization algorithms for deep neural network training.\nWe leverage efficient search techniques to explore an infinite and sparse\nprogram space. To bridge the large generalization gap between proxy and target\ntasks, we also introduce program selection and simplification strategies. Our\nmethod discovers a simple and effective optimization algorithm, $\\textbf{Lion}$\n($\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}$).\nIt is more memory-efficient than Adam as it only keeps track of the momentum.\nDifferent from adaptive optimizers, its update has the same magnitude for each\nparameter calculated through the sign operation. We compare Lion with widely\nused optimizers, such as Adam and Adafactor, for training a variety of models\non different tasks. On image classification, Lion boosts the accuracy of ViT by\nup to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On\nvision-language contrastive learning, we achieve 88.3% $\\textit{zero-shot}$ and\n91.1% $\\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best\nresults by 2% and 0.1%, respectively. On diffusion models, Lion outperforms\nAdam by achieving a better FID score and reducing the training compute by up to\n2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion\nexhibits a similar or better performance compared to Adam. Our analysis of Lion\nreveals that its performance gain grows with the training batch size. It also\nrequires a smaller learning rate than Adam due to the larger norm of the update\nproduced by the sign function. Additionally, we examine the limitations of Lion\nand identify scenarios where its improvements are small or not statistically\nsignificant. The implementation of Lion is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Da Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Real_E/0/1/0/all/0/1\">Esteban Real</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xuanyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_T/0/1/0/all/0/1\">Thang Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yifeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning Model Attribution Challenge. (arXiv:2302.06716v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.06716","description":"<p>We present the findings of the Machine Learning Model Attribution Challenge.\nFine-tuned machine learning models may derive from other trained models without\nobvious attribution characteristics. In this challenge, participants identify\nthe publicly-available base models that underlie a set of anonymous, fine-tuned\nlarge language models (LLMs) using only textual output of the models.\nContestants aim to correctly attribute the most fine-tuned models, with ties\nbroken in the favor of contestants whose solutions use fewer calls to the\nfine-tuned models' API. The most successful approaches were manual, as\nparticipants observed similarities between model outputs and developed\nattribution heuristics based on public documentation of the base models, though\nseveral teams also submitted automated, statistical solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merkhofer_E/0/1/0/all/0/1\">Elizabeth Merkhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_D/0/1/0/all/0/1\">Deepesh Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_H/0/1/0/all/0/1\">Hyrum S. Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manville_K/0/1/0/all/0/1\">Keith Manville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1\">Lily Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gante_J/0/1/0/all/0/1\">Jo&#xe3;o Gante</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Chat Assistants can Improve Conversations about Divisive Topics. (arXiv:2302.07268v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2302.07268","description":"<p>A rapidly increasing amount of human conversation occurs online. But\ndivisiveness and conflict can fester in text-based interactions on social media\nplatforms, in messaging apps, and on other digital forums. Such toxicity\nincreases polarization and, importantly, corrodes the capacity of diverse\nsocieties to develop efficient solutions to complex social problems that impact\neveryone. Scholars and civil society groups promote interventions that can make\ninterpersonal conversations less divisive or more productive in offline\nsettings, but scaling these efforts to the amount of discourse that occurs\nonline is extremely challenging. We present results of a large-scale experiment\nthat demonstrates how online conversations about divisive topics can be\nimproved with artificial intelligence tools. Specifically, we employ a large\nlanguage model to make real-time, evidence-based recommendations intended to\nimprove participants' perception of feeling understood in conversations. We\nfind that these interventions improve the reported quality of the conversation,\nreduce political divisiveness, and improve the tone, without systematically\nchanging the content of the conversation or moving people's policy attitudes.\nThese findings have important implications for future research on social media,\npolitical deliberation, and the growing community of scholars interested in the\nplace of artificial intelligence within computational social science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Argyle_L/0/1/0/all/0/1\">Lisa P. Argyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busby_E/0/1/0/all/0/1\">Ethan Busby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gubler_J/0/1/0/all/0/1\">Joshua Gubler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bail_C/0/1/0/all/0/1\">Chris Bail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howe_T/0/1/0/all/0/1\">Thomas Howe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rytting_C/0/1/0/all/0/1\">Christopher Rytting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wingate_D/0/1/0/all/0/1\">David Wingate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficiency 360: Efficient Vision Transformers. (arXiv:2302.08374v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2302.08374","description":"<p>Transformers are widely used for solving tasks in natural language\nprocessing, computer vision, speech, and music domains. In this paper, we talk\nabout the efficiency of transformers in terms of memory (the number of\nparameters), computation cost (number of floating points operations), and\nperformance of models, including accuracy, the robustness of the model, and\nfair \\&amp; bias-free features. We mainly discuss the vision transformer for the\nimage classification task. Our contribution is to introduce an efficient 360\nframework, which includes various aspects of the vision transformer, to make it\nmore efficient for industrial applications. By considering those applications,\nwe categorize them into multiple dimensions such as privacy, robustness,\ntransparency, fairness, inclusiveness, continual learning, probabilistic\nmodels, approximation, computational complexity, and spectral complexity. We\ncompare various vision transformer models based on their performance, the\nnumber of parameters, and the number of floating point operations (FLOPs) on\nmultiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patro_B/0/1/0/all/0/1\">Badri N. Patro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agneeswaran_V/0/1/0/all/0/1\">Vijay Srinivas Agneeswaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks. (arXiv:2302.08399v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.08399","description":"<p>Intuitive psychology is a pillar of common-sense reasoning. The replication\nof this reasoning in machine intelligence is an important stepping-stone on the\nway to human-like artificial intelligence. Several recent tasks and benchmarks\nfor examining this reasoning in Large-Large Models have focused in particular\non belief attribution in Theory-of-Mind tasks. These tasks have shown both\nsuccesses and failures. We consider in particular a recent purported success\ncase, and show that small variations that maintain the principles of ToM turn\nthe results on their head. We argue that in general, the zero-hypothesis for\nmodel evaluation in intuitive psychology should be skeptical, and that outlying\nfailure cases should outweigh average success rates. We also consider what\npossible future successes on Theory-of-Mind tasks by more powerful LLMs would\nmean for ToM tasks with people.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1\">Tomer Ullman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}