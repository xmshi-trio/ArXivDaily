{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-10-19T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in Bangla with Multi-Feature and Multi-Modal Analysis. (arXiv:2310.11465v1 [cs.LG])","link":"http://arxiv.org/abs/2310.11465","description":"<p>This study presents a large multi-modal Bangla YouTube clickbait dataset\nconsisting of 253,070 data points collected through an automated process using\nthe YouTube API and Python web automation frameworks. The dataset contains 18\ndiverse features categorized into metadata, primary content, engagement\nstatistics, and labels for individual videos from 58 Bangla YouTube channels. A\nrigorous preprocessing step has been applied to denoise, deduplicate, and\nremove bias from the features, ensuring unbiased and reliable analysis. As the\nlargest and most robust clickbait corpus in Bangla to date, this dataset\nprovides significant value for natural language processing and data science\nresearchers seeking to advance modeling of clickbait phenomena in low-resource\nlanguages. Its multi-modal nature allows for comprehensive analyses of\nclickbait across content, user interactions, and linguistic dimensions to\ndevelop more sophisticated detection methods with cross-linguistic\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imran_A/0/1/0/all/0/1\">Abdullah Al Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shovon_M/0/1/0/all/0/1\">Md Sakib Hossain Shovon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mridha_M/0/1/0/all/0/1\">M. F. Mridha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations. (arXiv:2310.11501v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11501","description":"<p>Recent work has aimed to capture nuances of human behavior by using LLMs to\nsimulate responses from particular demographics in settings like social science\nexperiments and public opinion surveys. However, there are currently no\nestablished ways to discuss or evaluate the quality of such LLM simulations.\nMoreover, there is growing concern that these LLM simulations are flattened\ncaricatures of the personas that they aim to simulate, failing to capture the\nmultidimensionality of people and perpetuating stereotypes. To bridge these\ngaps, we present CoMPosT, a framework to characterize LLM simulations using\nfour dimensions: Context, Model, Persona, and Topic. We use this framework to\nmeasure open-ended LLM simulations' susceptibility to caricature, defined via\ntwo criteria: individuation and exaggeration. We evaluate the level of\ncaricature in scenarios from existing work on LLM simulations. We find that for\nGPT-4, simulations of certain demographics (political and marginalized groups)\nand topics (general, uncontroversial) are highly susceptible to caricature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Myra Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccardi_T/0/1/0/all/0/1\">Tiziano Piccardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. (arXiv:2310.11511v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11511","description":"<p>Despite their remarkable capabilities, large language models (LLMs) often\nproduce responses containing factual inaccuracies due to their sole reliance on\nthe parametric knowledge they encapsulate. Retrieval-Augmented Generation\n(RAG), an ad hoc approach that augments LMs with retrieval of relevant\nknowledge, decreases such issues. However, indiscriminately retrieving and\nincorporating a fixed number of retrieved passages, regardless of whether\nretrieval is necessary, or passages are relevant, diminishes LM versatility or\ncan lead to unhelpful response generation. We introduce a new framework called\nSelf-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's\nquality and factuality through retrieval and self-reflection. Our framework\ntrains a single arbitrary LM that adaptively retrieves passages on-demand, and\ngenerates and reflects on retrieved passages and its own generations using\nspecial tokens, called reflection tokens. Generating reflection tokens makes\nthe LM controllable during the inference phase, enabling it to tailor its\nbehavior to diverse task requirements. Experiments show that Self-RAG (7B and\n13B parameters) significantly outperforms state-of-the-art LLMs and\nretrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\nreasoning and fact verification tasks, and it shows significant gains in\nimproving factuality and citation accuracy for long-form generations relative\nto these models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zeqiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic News Summerization. (arXiv:2310.11520v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11520","description":"<p>Natural Language Processing is booming with its applications in the real\nworld, one of which is Text Summarization for large texts including news\narticles. This research paper provides an extensive comparative evaluation of\nextractive and abstractive approaches for news text summarization, with an\nemphasis on the ROUGE score analysis. The study employs the CNN-Daily Mail\ndataset, which consists of news articles and human-generated reference\nsummaries. The evaluation employs ROUGE scores to assess the efficacy and\nquality of generated summaries. After Evaluation, we integrate the\nbest-performing models on a web application to assess their real-world\ncapabilities and user experience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dheer_K/0/1/0/all/0/1\">Kavach Dheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhankhar_A/0/1/0/all/0/1\">Arpit Dhankhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Group Preference Optimization: Few-Shot Alignment of Large Language Models. (arXiv:2310.11523v1 [cs.LG])","link":"http://arxiv.org/abs/2310.11523","description":"<p>Many applications of large language models (LLMs), ranging from chatbots to\ncreative writing, require nuanced subjective judgments that can differ\nsignificantly across different groups. Existing alignment algorithms can be\nexpensive to align for each group, requiring prohibitive amounts of\ngroup-specific preference data and computation for real-world use cases. We\nintroduce Group Preference Optimization (GPO), an alignment framework that\nsteers language models to preferences of individual groups in a few-shot\nmanner. In GPO, we augment the base LLM with an independent transformer module\ntrained to predict the preferences of a group for the LLM generations. For\nfew-shot learning, we parameterize this module as an in-context autoregressive\ntransformer and train it via meta-learning on several groups. We empirically\nvalidate the efficacy of GPO through rigorous evaluations using LLMs with\nvaried sizes on three human opinion adaptation tasks. These tasks involve\nadapting to the preferences of US demographic groups, global countries, and\nindividual users. Our results demonstrate that GPO not only aligns models more\naccurately but also requires fewer group-specific preferences, and less\ntraining and inference computing resources, outperforming existing strategies\nsuch as in-context steering and fine-tuning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Siyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_J/0/1/0/all/0/1\">John Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1\">Aditya Grover</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-stage Large Language Model Correction for Speech Recognition. (arXiv:2310.11532v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11532","description":"<p>In this paper, we investigate the usage of large language models (LLMs) to\nimprove the performance of competitive speech recognition systems. Different\nfrom traditional language models that focus on one single data domain, the rise\nof LLMs brings us the opportunity to push the limit of state-of-the-art ASR\nperformance, and at the same time to achieve higher robustness and generalize\neffectively across multiple domains. Motivated by this, we propose a novel\nmulti-stage approach to combine traditional language model re-scoring and LLM\nprompting. Specifically, the proposed method has two stages: the first stage\nuses a language model to re-score an N-best list of ASR hypotheses and run a\nconfidence check; The second stage uses prompts to a LLM to perform ASR error\ncorrection on less confident results from the first stage. Our experimental\nresults demonstrate the effectiveness of the proposed method by showing a 10% ~\n20% relative improvement in WER over a competitive ASR system -- across\nmultiple test domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1\">Jie Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thai-Son Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuker_S/0/1/0/all/0/1\">Sebastian St&#xfc;ker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning. (arXiv:2310.11541v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11541","description":"<p>In this paper, we present a methodology for linguistic feature extraction,\nfocusing particularly on automatically syllabifying words in multiple\nlanguages, with a design to be compatible with a forced-alignment tool, the\nMontreal Forced Aligner (MFA). In both the textual and phonetic domains, our\nmethod focuses on the extraction of phonetic transcriptions from text, stress\nmarks, and a unified automatic syllabification (in text and phonetic domains).\nThe system was built with open-source components and resources. Through an\nablation study, we demonstrate the efficacy of our approach in automatically\nsyllabifying words from several languages (English, French and Spanish).\nAdditionally, we apply the technique to the transcriptions of the CMU ARCTIC\ndataset, generating valuable annotations available\nonline\\footnote{\\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for\nspeech representation learning, speech unit discovery, and disentanglement of\nspeech factors in several speech-related fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tits_N/0/1/0/all/0/1\">No&#xe9; Tits</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging. (arXiv:2310.11564v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11564","description":"<p>While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language\nModels (LLMs) with general, aggregate human preferences, it is suboptimal for\nlearning diverse, individual perspectives. In this work, we study Reinforcement\nLearning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are\naligned to multiple (sometimes conflicting) preferences by modeling alignment\nas a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong\nsingle-objective baselines, we show that we can achieve personalized alignment\nby decomposing preferences into multiple dimensions. These dimensions are\ndefined based on personalizations that are declared as desirable by the user.\nIn this work, we show that they can be efficiently trained independently in a\ndistributed manner and combined effectively post-hoc through parameter merging.\nThe code is available at https://github.com/joeljang/RLPHF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungone Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What is a good question? Task-oriented asking with fact-level masking. (arXiv:2310.11571v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11571","description":"<p>Asking questions is an important element of real-life collaboration on\nreasoning tasks like question answering. For example, a legal assistant chatbot\nmay be unable to make accurate recommendations without specific information on\nthe user's circumstances. However, large language models are usually deployed\nto solve reasoning tasks directly without asking follow-up questions to the\nuser or third parties. We term this problem task-oriented asking (TOA).\nZero-shot chat models can perform TOA, but their training is primarily based on\nnext-token prediction rather than whether questions contribute to successful\ncollaboration. To enable the training and evaluation of TOA models, we present\na definition and framework for natural language task-oriented asking, the\nproblem of generating questions that result in answers useful for a reasoning\ntask. We also present fact-level masking (FLM), a procedure for converting\nnatural language datasets into self-supervised TOA datasets by omitting\nparticular critical facts. Finally, we generate a TOA dataset from the HotpotQA\ndataset using FLM and evaluate several zero-shot language models on it. Our\nexperiments show that current zero-shot models struggle to ask questions that\nretrieve useful information, as compared to human annotators. These results\ndemonstrate an opportunity to use FLM datasets and the TOA framework to train\nand evaluate better TOA models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toles_M/0/1/0/all/0/1\">Matthew Toles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yukun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gravano_L/0/1/0/all/0/1\">Luis Gravano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BasahaCorpus: An Expanded Linguistic Resource for Readability Assessment in Central Philippine Languages. (arXiv:2310.11584v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11584","description":"<p>Current research on automatic readability assessment (ARA) has focused on\nimproving the performance of models in high-resource languages such as English.\nIn this work, we introduce and release BasahaCorpus as part of an initiative\naimed at expanding available corpora and baseline models for readability\nassessment in lower resource languages in the Philippines. We compiled a corpus\nof short fictional narratives written in Hiligaynon, Minasbate, Karay-a, and\nRinconada -- languages belonging to the Central Philippine family tree subgroup\n-- to train ARA models using surface-level, syllable-pattern, and n-gram\noverlap features. We also propose a new hierarchical cross-lingual modeling\napproach that takes advantage of a language's placement in the family tree to\nincrease the amount of available training data. Our study yields encouraging\nresults that support previous work showcasing the efficacy of cross-lingual\nmodels in low-resource settings, as well as similarities in highly informative\nlinguistic features for mutually intelligible languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochmar_E/0/1/0/all/0/1\">Ekaterina Kochmar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliciting Human Preferences with Language Models. (arXiv:2310.11589v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11589","description":"<p>Language models (LMs) can be directed to perform target tasks by using\nlabeled examples or natural language prompts. But selecting examples or writing\nprompts for can be challenging--especially in tasks that involve unusual edge\ncases, demand precise articulation of nebulous preferences, or require an\naccurate mental model of LM behavior. We propose to use *LMs themselves* to\nguide the task specification process. In this paper, we introduce **Generative\nActive Task Elicitation (GATE)**: a learning framework in which models elicit\nand infer intended behavior through free-form, language-based interaction with\nusers. We study GATE in three domains: email validation, content\nrecommendation, and moral reasoning. In preregistered experiments, we show that\nLMs prompted to perform GATE (e.g., by generating open-ended questions or\nsynthesizing informative edge cases) elicit responses that are often more\ninformative than user-written prompts or labels. Users report that interactive\ntask elicitation requires less effort than prompting or example labeling and\nsurfaces novel considerations not initially anticipated by users. Our findings\nsuggest that LM-driven elicitation can be a powerful tool for aligning models\nto complex human preferences and values.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Belinda Z. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamkin_A/0/1/0/all/0/1\">Alex Tamkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Evaluation of Personalized Text Generation using Large Language Models. (arXiv:2310.11593v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11593","description":"<p>Personalized text generation presents a specialized mechanism for delivering\ncontent that is specific to a user's personal context. While the research\nprogress in this area has been rapid, evaluation still presents a challenge.\nTraditional automated metrics such as BLEU and ROUGE primarily measure lexical\nsimilarity to human-written references, and are not able to distinguish\npersonalization from other subtle semantic aspects, thus falling short of\ncapturing the nuances of personalized generated content quality. On the other\nhand, human judgments are costly to obtain, especially in the realm of\npersonalized evaluation. Inspired by these challenges, we explore the use of\nlarge language models (LLMs) for evaluating personalized text generation, and\nexamine their ability to understand nuanced user context. We present AuPEL, a\nnovel evaluation method that distills three major semantic aspects of the\ngenerated text: personalization, quality and relevance, and automatically\nmeasures these aspects. To validate the effectiveness of AuPEL, we design\ncarefully controlled experiments and compare the accuracy of the evaluation\njudgments made by LLMs versus that of judgements made by human annotators, and\nconduct rigorous analyses of the consistency and sensitivity of the proposed\nmetric. We find that, compared to existing evaluation metrics, AuPEL not only\ndistinguishes and ranks models based on their personalization abilities more\naccurately, but also presents commendable consistency and efficiency for this\ntask. Our work suggests that using LLMs as the evaluators of personalized text\ngeneration is superior to traditional text similarity metrics, even though\ninteresting new challenges still remain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiepu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yi Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1\">Qiaozhu Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Michael Bendersky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as Zero-Shot Trajectory Generators. (arXiv:2310.11604v1 [cs.RO])","link":"http://arxiv.org/abs/2310.11604","description":"<p>Large Language Models (LLMs) have recently shown promise as high-level\nplanners for robots when given access to a selection of low-level skills.\nHowever, it is often assumed that LLMs do not possess sufficient knowledge to\nbe used for the low-level trajectories themselves. In this work, we address\nthis assumption thoroughly, and investigate if an LLM (GPT-4) can directly\npredict a dense sequence of end-effector poses for manipulation skills, when\ngiven access to only object detection and segmentation vision models. We study\nhow well a single task-agnostic prompt, without any in-context examples, motion\nprimitives, or external trajectory optimisers, can perform across 26 real-world\nlanguage-based tasks, such as \"open the bottle cap\" and \"wipe the plate with\nthe sponge\", and we investigate which design choices in this prompt are the\nmost effective. Our conclusions raise the assumed limit of LLMs for robotics,\nand we reveal for the first time that LLMs do indeed possess an understanding\nof low-level robot control sufficient for a range of common tasks, and that\nthey can additionally detect failures and then re-plan trajectories\naccordingly. Videos, code, and prompts are available at:\nhttps://www.robot-learning.uk/language-models-trajectory-generators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_T/0/1/0/all/0/1\">Teyun Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1\">Norman Di Palo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach. (arXiv:2310.11616v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11616","description":"<p>This study uncovers the factor of general intelligence, or g, in language\nmodels, extending the psychometric theory traditionally applied to humans and\ncertain animal species. Utilizing factor analysis on two extensive datasets -\nOpen LLM Leaderboard with 1,232 models and General Language Understanding\nEvaluation (GLUE) Leaderboard with 88 models - we find compelling evidence for\na unidimensional, highly stable g factor that accounts for 85% of the variance\nin model performance. The study also finds a moderate correlation of .48\nbetween model size and g. The discovery of g in language models offers a\nunified metric for model evaluation and opens new avenues for more robust,\ng-based model ability assessment. These findings lay the foundation for\nunderstanding and future research on artificial general intelligence from a\npsychometric perspective and have practical implications for model evaluation\nand development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilic_D/0/1/0/all/0/1\">David Ili&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn Your Tokens: Word-Pooled Tokenization for Language Modeling. (arXiv:2310.11628v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11628","description":"<p>Language models typically tokenize text into subwords, using a deterministic,\nhand-engineered heuristic of combining characters into longer surface-level\nstrings such as 'ing' or whole words. Recent literature has repeatedly shown\nthe limitations of such a tokenization strategy, particularly for documents not\nwritten in English and for representing numbers. On the other extreme,\nbyte/character-level language models are much less restricted but suffer from\nincreased sequence description lengths and a subsequent quadratic expansion in\nself-attention computation. Recent attempts to compress and limit these context\nlengths with fixed size convolutions is helpful but completely ignores the word\nboundary. This paper considers an alternative 'learn your tokens' scheme which\nutilizes the word boundary to pool bytes/characters into word representations,\nwhich are fed to the primary language model, before again decoding individual\ncharacters/bytes per word in parallel. We find that our moderately expressive\nand moderately fast end-to-end tokenizer outperform by over 300% both subwords\nand byte/character models over the intrinsic language modeling metric of\nnext-word prediction across datasets. It particularly outshines on rare words,\noutperforming by a factor of 30! We extensively study the language modeling\nsetup for all three categories of tokenizers and theoretically analyze how our\nend-to-end models can also be a strong trade-off in efficiency and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thawani_A/0/1/0/all/0/1\">Avijit Thawani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanekar_S/0/1/0/all/0/1\">Saurabh Ghanekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations. (arXiv:2310.11634v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11634","description":"<p>Humans possess a remarkable ability to assign novel interpretations to\nlinguistic expressions, enabling them to learn new words and understand\ncommunity-specific connotations. However, Large Language Models (LLMs) have a\nknowledge cutoff and are costly to finetune repeatedly. Therefore, it is\ncrucial for LLMs to learn novel interpretations in-context. In this paper, we\nsystematically analyse the ability of LLMs to acquire novel interpretations\nusing in-context learning. To facilitate our study, we introduce MAGNIFICo, an\nevaluation suite implemented within a text-to-SQL semantic parsing framework\nthat incorporates diverse tokens and prompt settings to simulate real-world\ncomplexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a\nsurprisingly robust capacity for comprehending novel interpretations from\nnatural language descriptions as well as from discussions within long\nconversations. Nevertheless, our findings also highlight the need for further\nimprovements, particularly when interpreting unfamiliar words or when composing\nmultiple novel interpretations simultaneously in the same example.\nAdditionally, our analysis uncovers the semantic predispositions in LLMs and\nreveals the impact of recency bias for information presented in long contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Arkil Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattamishra_S/0/1/0/all/0/1\">Satwik Bhattamishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Systematic Assessment of Factual Knowledge in Large Language Models. (arXiv:2310.11638v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11638","description":"<p>Previous studies have relied on existing question-answering benchmarks to\nevaluate the knowledge stored in large language models (LLMs). However, this\napproach has limitations regarding factual knowledge coverage, as it mostly\nfocuses on generic domains which may overlap with the pretraining data. This\npaper proposes a framework to systematically assess the factual knowledge of\nLLMs by leveraging knowledge graphs (KGs). Our framework automatically\ngenerates a set of questions and expected answers from the facts stored in a\ngiven KG, and then evaluates the accuracy of LLMs in answering these questions.\nWe systematically evaluate the state-of-the-art LLMs with KGs in generic and\nspecific domains. The experiment shows that ChatGPT is consistently the top\nperformer across all domains. We also find that LLMs performance depends on the\ninstruction finetuning, domain and question complexity and is prone to\nadversarial context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Linhao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Thuy-Trang Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model. (arXiv:2310.11648v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11648","description":"<p>Despite tremendous improvements in natural language generation, summarization\nmodels still suffer from the unfaithfulness issue. Previous work evaluates\nfaithfulness either using models trained on the other tasks or in-domain\nsynthetic data, or prompting a large model such as ChatGPT. This paper proposes\nto do zero-shot faithfulness evaluation simply with a moderately-sized\nfoundation language model. We introduce a new metric FFLM, which is a\ncombination of probability changes based on the intuition that prefixing a\npiece of text that is consistent with the output will increase the probability\nof predicting the output. Experiments show that FFLM performs competitively\nwith or even outperforms ChatGPT on both inconsistency detection and\nfaithfulness rating with 24x fewer parameters. FFLM also achieves improvements\nover other strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Qi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Siyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yizhu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Field-testing items using artificial intelligence: Natural language processing with transformers. (arXiv:2310.11655v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11655","description":"<p>Five thousand variations of the RoBERTa model, an artificially intelligent\n\"transformer\" that can understand text language, completed an English literacy\nexam with 29 multiple-choice questions. Data were used to calculate the\npsychometric properties of the items, which showed some degree of agreement to\nthose obtained from human examinee data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maeda_H/0/1/0/all/0/1\">Hotaka Maeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents. (arXiv:2310.11667v1 [cs.AI])","link":"http://arxiv.org/abs/2310.11667","description":"<p>Humans are social beings; we pursue social goals in our daily interactions,\nwhich is a crucial aspect of social intelligence. Yet, AI systems' abilities in\nthis realm remain elusive. We present SOTOPIA, an open-ended environment to\nsimulate complex social interactions between artificial agents and evaluate\ntheir social intelligence. In our environment, agents role-play and interact\nunder a wide variety of scenarios; they coordinate, collaborate, exchange, and\ncompete with each other to achieve complex social goals. We simulate the\nrole-play interaction between LLM-based agents and humans within this task\nspace and evaluate their performance with a holistic evaluation framework\ncalled SOTOPIA-Eval. With SOTOPIA, we find significant differences between\nthese models in terms of their social intelligence, and we identify a subset of\nSOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models.\nWe find that on this subset, GPT-4 achieves a significantly lower goal\ncompletion rate than humans and struggles to exhibit social commonsense\nreasoning and strategic communication skills. These findings demonstrate\nSOTOPIA's promise as a general platform for research on evaluating and\nimproving social intelligence in artificial agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuhui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_L/0/1/0/all/0/1\">Leena Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruohong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhengyang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning. (arXiv:2310.11670v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11670","description":"<p>Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in\nadapting the pre-trained language models to downstream tasks while only\nupdating a small number of parameters. Despite the success, most existing\nmethods independently adapt to each task without considering knowledge transfer\nbetween tasks and are limited to low-data regimes. To overcome this issue, we\npropose Prototype-based HyperAdapter (PHA), a novel framework built on the\nadapter-tuning and hypernetwork. It introduces an instance-dense retriever and\na prototypical hypernetwork to generate the conditional modules in a\nsample-efficient manner. This leads to comparable performance improvements\nagainst existing PEFT methods on multi-task learning and few-shot transfer\nlearning. More importantly, when the available data size gets smaller, our\nmethod outperforms other strong baselines by a large margin. Based on our\nextensive empirical experiments across various datasets, we demonstrate that\nPHA strikes a better trade-off between trainable parameters, accuracy on stream\ntasks, and sample efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhaofeng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixEdit: Revisiting Data Augmentation and Beyond for Grammatical Error Correction. (arXiv:2310.11671v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11671","description":"<p>Data Augmentation through generating pseudo data has been proven effective in\nmitigating the challenge of data scarcity in the field of Grammatical Error\nCorrection (GEC). Various augmentation strategies have been widely explored,\nmost of which are motivated by two heuristics, i.e., increasing the\ndistribution similarity and diversity of pseudo data. However, the underlying\nmechanism responsible for the effectiveness of these strategies remains poorly\nunderstood. In this paper, we aim to clarify how data augmentation improves GEC\nmodels. To this end, we introduce two interpretable and computationally\nefficient measures: Affinity and Diversity. Our findings indicate that an\nexcellent GEC data augmentation strategy characterized by high Affinity and\nappropriate Diversity can better improve the performance of GEC models. Based\non this observation, we propose MixEdit, a data augmentation approach that\nstrategically and dynamically augments realistic data, without requiring extra\nmonolingual corpora. To verify the correctness of our findings and the\neffectiveness of the proposed MixEdit, we conduct experiments on mainstream\nEnglish and Chinese GEC datasets. The results show that MixEdit substantially\nimproves GEC models and is complementary to traditional data augmentation\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jingheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-ended Commonsense Reasoning with Unrestricted Answer Scope. (arXiv:2310.11672v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11672","description":"<p>Open-ended Commonsense Reasoning is defined as solving a commonsense question\nwithout providing 1) a short list of answer candidates and 2) a pre-defined\nanswer scope. Conventional ways of formulating the commonsense question into a\nquestion-answering form or utilizing external knowledge to learn\nretrieval-based methods are less applicable in the open-ended setting due to an\ninherent challenge. Without pre-defining an answer scope or a few candidates,\nopen-ended commonsense reasoning entails predicting answers by searching over\nan extremely large searching space. Moreover, most questions require implicit\nmulti-hop reasoning, which presents even more challenges to our problem. In\nthis work, we leverage pre-trained language models to iteratively retrieve\nreasoning paths on the external knowledge base, which does not require\ntask-specific supervision. The reasoning paths can help to identify the most\nprecise answer to the commonsense question. We conduct experiments on two\ncommonsense benchmark datasets. Compared to other approaches, our proposed\nmethod achieves better performance both quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1\">Chen Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xujiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osaki_T/0/1/0/all/0/1\">Takao Osaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuda_K/0/1/0/all/0/1\">Katsushi Matsuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Descriptive Knowledge Graph in Biomedical Domain. (arXiv:2310.11681v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11681","description":"<p>We present a novel system that automatically extracts and generates\ninformative and descriptive sentences from the biomedical corpus and\nfacilitates the efficient search for relational knowledge. Unlike previous\nsearch engines or exploration systems that retrieve unconnected passages, our\nsystem organizes descriptive sentences as a relational graph, enabling\nresearchers to explore closely related biomedical entities (e.g., diseases\ntreated by a chemical) or indirectly connected entities (e.g., potential drugs\nfor treating a disease). Our system also uses ChatGPT and a fine-tuned relation\nsynthesis model to generate concise and reliable descriptive sentences from\nretrieved information, reducing the need for extensive human reading effort.\nWith our system, researchers can easily obtain both high-level knowledge and\ndetailed references and interactively steer to the information of interest. We\nspotlight the application of our system in COVID-19 research, illustrating its\nutility in areas such as drug repurposing and literature curation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kerui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention. (arXiv:2310.11685v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11685","description":"<p>Large transformer models have achieved state-of-the-art results in numerous\nnatural language processing tasks. Among the pivotal components of the\ntransformer architecture, the attention mechanism plays a crucial role in\ncapturing token interactions within sequences through the utilization of\nsoftmax function.\n</p>\n<p>Conversely, linear attention presents a more computationally efficient\nalternative by approximating the softmax operation with linear complexity.\nHowever, it exhibits substantial performance degradation when compared to the\ntraditional softmax attention mechanism.\n</p>\n<p>In this paper, we bridge the gap in our theoretical understanding of the\nreasons behind the practical performance gap between softmax and linear\nattention. By conducting a comprehensive comparative analysis of these two\nattention mechanisms, we shed light on the underlying reasons for why softmax\nattention outperforms linear attention in most scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yichuan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs. (arXiv:2310.11689v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11689","description":"<p>Large language models (LLMs) have recently shown great advances in a variety\nof tasks, including natural language understanding and generation. However,\ntheir use in high-stakes decision-making scenarios is still limited due to the\npotential for errors. Selective prediction is a technique that can be used to\nimprove the reliability of the LLMs by allowing them to abstain from making\npredictions when they are unsure of the answer. In this work, we propose a\nnovel framework for adaptation with self-evaluation to improve the selective\nprediction performance of LLMs. Our framework is based on the idea of using\nparameter-efficient tuning to adapt the LLM to the specific task at hand while\nimproving its ability to perform self-evaluation. We evaluate our method on a\nvariety of question-answering (QA) datasets and show that it outperforms\nstate-of-the-art selective prediction methods. For example, on the CoQA\nbenchmark, our method improves the AUACC from 91.23% to 92.63% and improves the\nAUROC from 74.61% to 80.25%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jinsung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1\">Sayna Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1\">Sercan O Arik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MISAR: A Multimodal Instructional System with Augmented Reality. (arXiv:2310.11699v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11699","description":"<p>Augmented reality (AR) requires the seamless integration of visual, auditory,\nand linguistic channels for optimized human-computer interaction. While\nauditory and visual inputs facilitate real-time and contextual user guidance,\nthe potential of large language models (LLMs) in this landscape remains largely\nuntapped. Our study introduces an innovative method harnessing LLMs to\nassimilate information from visual, auditory, and contextual modalities.\nFocusing on the unique challenge of task performance quantification in AR, we\nutilize egocentric video, speech, and context analysis. The integration of LLMs\nfacilitates enhanced state estimation, marking a step towards more adaptive AR\nsystems. Code, dataset, and demo will be available at\nhttps://github.com/nguyennm1024/misar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jing Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nguyen Manh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_A/0/1/0/all/0/1\">Ali Vosoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Co-Speech Gesture for Multimodal Aphasia Type Detection. (arXiv:2310.11710v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11710","description":"<p>Aphasia, a language disorder resulting from brain damage, requires accurate\nidentification of specific aphasia types, such as Broca's and Wernicke's\naphasia, for effective treatment. However, little attention has been paid to\ndeveloping methods to detect different types of aphasia. Recognizing the\nimportance of analyzing co-speech gestures for distinguish aphasia types, we\npropose a multimodal graph neural network for aphasia type detection using\nspeech and corresponding gesture patterns. By learning the correlation between\nthe speech and gesture modalities for each aphasia type, our model can generate\ntextual representations sensitive to gesture information, leading to accurate\naphasia type detection. Extensive experiments demonstrate the superiority of\nour approach over existing methods, achieving state-of-the-art results (F1\n84.2\\%). We also show that gesture features outperform acoustic features,\nhighlighting the significance of gesture expression in detecting aphasia types.\nWe provide the codes for reproducibility purposes\\footnote{Code:\n\\url{https://github.com/DSAIL-SKKU/Multimodal-Aphasia-Type-Detection_EMNLP_2023}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Daeun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Sejung Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hyolim Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungbae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jinyoung Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets. (arXiv:2310.11715v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11715","description":"<p>Named Entity Recognition (NER) frequently suffers from the problem of\ninsufficient labeled data, particularly in fine-grained NER scenarios. Although\n$K$-shot learning techniques can be applied, their performance tends to\nsaturate when the number of annotations exceeds several tens of labels. To\novercome this problem, we utilize existing coarse-grained datasets that offer a\nlarge number of annotations. A straightforward approach to address this problem\nis pre-finetuning, which employs coarse-grained data for representation\nlearning. However, it cannot directly utilize the relationships between\nfine-grained and coarse-grained entities, although a fine-grained entity type\nis likely to be a subcategory of a coarse-grained entity type. We propose a\nfine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage\nthe hierarchical structure explicitly. In addition, we present an inconsistency\nfiltering method to eliminate coarse-grained entities that are inconsistent\nwith fine-grained entity types to avoid performance degradation. Our\nexperimental results show that our method outperforms both $K$-shot learning\nand supervised learning methods when dealing with a small number of\nfine-grained annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Su Ah Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seokjin Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1\">Woohwan Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning. (arXiv:2310.11716v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11716","description":"<p>Recent advancements in Large Language Models (LLMs) have expanded the\nhorizons of natural language understanding and generation. Notably, the output\ncontrol and alignment with the input of LLMs can be refined through instruction\ntuning. However, as highlighted in several studies, low-quality data in the\ntraining set are usually detrimental to instruction tuning, resulting in\ninconsistent or even misleading LLM outputs. We propose a novel method, termed\n\"reflection-tuning,\" which addresses the problem by self-improvement and\njudging capabilities of LLMs. This approach utilizes an oracle LLM to recycle\nthe original training data by introspecting and enhancing the quality of\ninstructions and responses in the data. Extensive experiments on widely used\nevaluation benchmarks show that LLMs trained with our recycled data outperform\nthose trained with existing datasets in various benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Ming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lichang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiuhai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shwai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding. (arXiv:2310.11721v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11721","description":"<p>Chain-of-Thought (CoT) is a technique that guides Large Language Models\n(LLMs) to decompose complex tasks into multi-step reasoning through\nintermediate steps in natural language form. Briefly, CoT enables LLMs to think\nstep by step. However, although many Natural Language Understanding (NLU) tasks\nalso require thinking step by step, LLMs perform less well than small-scale\nMasked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose\nChain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt\ntuning, to implement step-by-step thinking for MLMs on NLU tasks. From the\nperspective of CoT, CoTT's two-step framework enables MLMs to implement task\ndecomposition; CoTT's prompt tuning allows intermediate steps to be used in\nnatural language form. Thereby, the success of CoT can be extended to NLU tasks\nthrough MLMs. To verify the effectiveness of CoTT, we conduct experiments on\ntwo NLU tasks: hierarchical classification and relation extraction, and the\nresults show that CoTT outperforms baselines and achieves state-of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Caoyun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jidong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaohui Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantify Health-Related Atomic Knowledge in Chinese Medical Large Language Models: A Computational Analysis. (arXiv:2310.11722v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11722","description":"<p>Large Language Models (LLMs) have the potential to revolutionize the way\nusers self-diagnose through search engines by offering direct and efficient\nsuggestions. Recent studies primarily focused on the quality of LLMs evaluated\nby GPT-4 or their ability to pass medical exams, no studies have quantified the\nextent of health-related atomic knowledge stored in LLMs' memory, which is the\nbasis of LLMs to provide more factual suggestions. In this paper, we first\nconstructed a benchmark, including the most common types of atomic knowledge in\nuser self-diagnosis queries, with 17 atomic types and a total of 14, 048 pieces\nof atomic knowledge. Then, we evaluated both generic and specialized LLMs on\nthe benchmark. The experimental results showcased that generic LLMs perform\nbetter than specialized LLMs in terms of atomic knowledge and\ninstruction-following ability. Error analysis revealed that both generic and\nspecialized LLMs are sycophantic, e.g., always catering to users' claims when\nit comes to unknown knowledge. Besides, generic LLMs showed stronger safety,\nwhich can be learned by specialized LLMs through distilled data. We further\nexplored different types of data commonly adopted for fine-tuning specialized\nLLMs, i.e., real-world, semi-distilled, and distilled data, and found that\ndistilled data can benefit LLMs most.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yaxin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1\">Feng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting. (arXiv:2310.11732v1 [cs.LG])","link":"http://arxiv.org/abs/2310.11732","description":"<p>Despite the significant progress made in practical applications of aligned\nlanguage models (LMs), they tend to be overconfident in output answers compared\nto the corresponding pre-trained LMs. In this work, we systematically evaluate\nthe impact of the alignment process on logit-based uncertainty calibration of\nLMs under the multiple-choice setting. We first conduct a thoughtful empirical\nstudy on how aligned LMs differ in calibration from their pre-trained\ncounterparts. Experimental results reveal that there are two distinct\nuncertainties in LMs under the multiple-choice setting, which are responsible\nfor the answer decision and the format preference of the LMs, respectively.\nThen, we investigate the role of these two uncertainties on aligned LM's\ncalibration through fine-tuning in simple synthetic alignment schemes and\nconclude that one reason for aligned LMs' overconfidence is the conflation of\nthese two types of uncertainty. Furthermore, we examine the utility of common\npost-hoc calibration methods for aligned LMs and propose an easy-to-implement\nand sample-efficient method to calibrate aligned LMs. We hope our findings\ncould provide insights into the design of more reliable alignment processes for\nLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1\">Guande He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias in Emotion Recognition with ChatGPT. (arXiv:2310.11753v1 [cs.RO])","link":"http://arxiv.org/abs/2310.11753","description":"<p>This technical report explores the ability of ChatGPT in recognizing emotions\nfrom text, which can be the basis of various applications like interactive\nchatbots, data annotation, and mental health analysis. While prior research has\nshown ChatGPT's basic ability in sentiment analysis, its performance in more\nnuanced emotion recognition is not yet explored. Here, we conducted experiments\nto evaluate its performance of emotion recognition across different datasets\nand emotion labels. Our findings indicate a reasonable level of reproducibility\nin its performance, with noticeable improvement through fine-tuning. However,\nthe performance varies with different emotion labels and datasets, highlighting\nan inherent instability and possible bias. The choice of dataset and emotion\nlabels significantly impacts ChatGPT's emotion recognition performance. This\npaper sheds light on the importance of dataset and label selection, and the\npotential of fine-tuning in enhancing ChatGPT's emotion recognition\ncapabilities, providing a groundwork for better integration of emotion analysis\nin applications using ChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wake_N/0/1/0/all/0/1\">Naoki Wake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanehira_A/0/1/0/all/0/1\">Atsushi Kanehira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasabuchi_K/0/1/0/all/0/1\">Kazuhiro Sasabuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamatsu_J/0/1/0/all/0/1\">Jun Takamatsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikeuchi_K/0/1/0/all/0/1\">Katsushi Ikeuchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction. (arXiv:2310.11761v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11761","description":"<p>Large language models (LLMs) have demonstrated great potential for\ndomain-specific applications, such as the law domain. However, recent disputes\nover GPT-4's law evaluation raise questions concerning their performance in\nreal-world legal tasks. To systematically investigate their competency in the\nlaw, we design practical baseline solutions based on LLMs and test on the task\nof legal judgment prediction. In our solutions, LLMs can work alone to answer\nopen questions or coordinate with an information retrieval (IR) system to learn\nfrom similar cases or solve simplified multi-choice questions. We show that\nsimilar cases and multi-choice options, namely label candidates, included in\nprompts can help LLMs recall domain knowledge that is critical for expertise\nlegal reasoning. We additionally present an intriguing paradox wherein an IR\nsystem surpasses the performance of LLM+IR due to limited gains acquired by\nweaker LLMs from powerful IR systems. In such cases, the role of LLMs becomes\nredundant. Our evaluation pipeline can be easily extended into other tasks to\nfacilitate evaluations in other domains. Code is available at\nhttps://github.com/srhthu/LM-CompEval-Legal\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shui_R/0/1/0/all/0/1\">Ruihao Shui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotated Job Ads with Named Entity Recognition. (arXiv:2310.11769v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11769","description":"<p>We have trained a named entity recognition (NER) model that screens Swedish\njob ads for different kinds of useful information (e.g. skills required from a\njob seeker). It was obtained by fine-tuning KB-BERT. The biggest challenge we\nfaced was the creation of a labelled dataset, which required manual annotation.\nThis paper gives an overview of the methods we employed to make the annotation\nprocess more efficient and to ensure high quality data. We also report on the\nperformance of the resulting model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stollenwerk_F/0/1/0/all/0/1\">Felix Stollenwerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fastlund_N/0/1/0/all/0/1\">Niklas Fastlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyqvist_A/0/1/0/all/0/1\">Anna Nyqvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohman_J/0/1/0/all/0/1\">Joey &#xd6;hman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling. (arXiv:2310.11772v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11772","description":"<p>Topic segmentation is critical for obtaining structured long documents and\nimproving downstream tasks like information retrieval. Due to its ability of\nautomatically exploring clues of topic shift from a large amount of labeled\ndata, recent supervised neural models have greatly promoted the development of\nlong document topic segmentation, but leaving the deeper relationship of\nsemantic coherence and topic segmentation underexplored. Therefore, this paper\nenhances the supervised model's ability to capture coherence from both\nstructure and similarity perspectives to further improve the topic segmentation\nperformance, including the Topic-aware Sentence Structure Prediction (TSSP) and\nContrastive Semantic Similarity Learning (CSSL). Specifically, the TSSP task is\nproposed to force the model to comprehend structural information by learning\nthe original relations of adjacent sentences in a disarrayed document, which is\nconstructed by jointly disrupting the original document at the topic and\nsentence levels. In addition, we utilize inter- and intra-topic information to\nconstruct contrastive samples and design the CSSL objective to ensure that the\nsentences representations in the same topic have higher semantic similarity,\nwhile those in different topics are less similar. Extensive experiments show\nthat the Longformer with our approach significantly outperforms old\nstate-of-the-art (SOTA) methods. Our approach improves $F_{1}$ of old SOTA by\n3.42 (73.74 -&gt; 77.16) and reduces $P_{k}$ by 1.11 points (15.0 -&gt; 13.89) on\nWIKI-727K and achieves an average reduction of 0.83 points on $P_{k}$ on\nWikiSection. The average $P_{k}$ drop of 2.82 points on the two out-of-domain\ndatasets also illustrates the robustness of our approach\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale. (arXiv:2310.11778v1 [cs.CY])","link":"http://arxiv.org/abs/2310.11778","description":"<p>The recent surge in the research of diffusion models has accelerated the\nadoption of text-to-image models in various Artificial Intelligence Generated\nContent (AIGC) commercial products. While these exceptional AIGC products are\ngaining increasing recognition and sparking enthusiasm among consumers, the\nquestions regarding whether, when, and how these models might unintentionally\nreinforce existing societal stereotypes remain largely unaddressed. Motivated\nby recent advancements in language agents, here we introduce a novel agent\narchitecture tailored for stereotype detection in text-to-image models. This\nversatile agent architecture is capable of accommodating free-form detection\ntasks and can autonomously invoke various tools to facilitate the entire\nprocess, from generating corresponding instructions and images, to detecting\nstereotypes. We build the stereotype-relevant benchmark based on multiple\nopen-text datasets, and apply this architecture to commercial products and\npopular open source text-to-image models. We find that these models often\ndisplay serious stereotypes when it comes to certain prompts about personal\ncharacteristics, social cultural context and crime-related aspects. In summary,\nthese empirical findings underscore the pervasive existence of stereotypes\nacross social dimensions, including gender, race, and religion, which not only\nvalidate the effectiveness of our proposed approach, but also emphasize the\ncritical necessity of addressing potential ethical risks in the burgeoning\nrealm of AIGC. As AIGC continues its rapid expansion trajectory, with new\nmodels and plugins emerging daily in staggering numbers, the challenge lies in\nthe timely detection and mitigation of potential biases within these models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_T/0/1/0/all/0/1\">Tian Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tingyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen M. Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zibin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bingzhe Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Annotation Handbook: A Practical Guide for Machine Learning Projects. (arXiv:2310.11780v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11780","description":"<p>This handbook is a hands-on guide on how to approach text annotation tasks.\nIt provides a gentle introduction to the topic, an overview of theoretical\nconcepts as well as practical advice. The topics covered are mostly technical,\nbut business, ethical and regulatory issues are also touched upon. The focus\nlies on readability and conciseness rather than completeness and scientific\nrigor. Experience with annotation and knowledge of machine learning are useful\nbut not required. The document may serve as a primer or reference book for a\nwide range of professions such as team leaders, project managers, IT\narchitects, software developers and machine learning engineers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stollenwerk_F/0/1/0/all/0/1\">Felix Stollenwerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohman_J/0/1/0/all/0/1\">Joey &#xd6;hman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrelli_D/0/1/0/all/0/1\">Danila Petrelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallero_E/0/1/0/all/0/1\">Emma Waller&#xf6;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsson_F/0/1/0/all/0/1\">Fredrik Olsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengtsson_C/0/1/0/all/0/1\">Camilla Bengtsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horndahl_A/0/1/0/all/0/1\">Andreas Horndahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandler_G/0/1/0/all/0/1\">Gabriela Zarzar Gandler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Nushu: An Exploration of Language Emergence in Sisterhood -Through the Lens of Computational Linguistics. (arXiv:2310.11870v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11870","description":"<p>This paper presents \"AI Nushu,\" an emerging language system inspired by Nushu\n(women's scripts), the unique language created and used exclusively by ancient\nChinese women who were thought to be illiterate under a patriarchal society. In\nthis interactive installation, two artificial intelligence (AI) agents are\ntrained in the Chinese dictionary and the Nushu corpus. By continually\nobserving their environment and communicating, these agents collaborate towards\ncreating a standard writing system to encode Chinese. It offers an artistic\ninterpretation of the creation of a non-western script from a computational\nlinguistics perspective, integrating AI technology with Chinese cultural\nheritage and a feminist viewpoint.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuqian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yuying Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Ze Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhijun Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chuyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kejiang Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braud_T/0/1/0/all/0/1\">Tristan Braud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chang Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadipour_A/0/1/0/all/0/1\">Ali Asadipour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models. (arXiv:2310.11877v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11877","description":"<p>Large language models (LLMs) have been shown to possess impressive\ncapabilities, while also raising crucial concerns about the faithfulness of\ntheir responses. A primary issue arising in this context is the management of\nunanswerable queries by LLMs, which often results in hallucinatory behavior,\ndue to overconfidence. In this paper, we explore the behavior of LLMs when\npresented with unanswerable queries. We ask: do models \\textbf{represent} the\nfact that the question is unanswerable when generating a hallucinatory answer?\nOur results show strong indications that such models encode the answerability\nof an input query, with the representation of the first decoded token often\nbeing a strong indicator. These findings shed new light on the spatial\norganization within the latent representations of LLMs, unveiling previously\nunexplored facets of these models. Moreover, they pave the way for the\ndevelopment of improved decoding techniques with better adherence to factual\ngeneration, particularly in scenarios where query unanswerability is a concern.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Slobodkin_A/0/1/0/all/0/1\">Aviv Slobodkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Dissonance to Insights: Dissecting Disagreements in Rationale Dataset Construction for Case Outcome Classification. (arXiv:2310.11878v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11878","description":"<p>In legal NLP, Case Outcome Classification (COC) must not only be accurate but\nalso trustworthy and explainable. Existing work in explainable COC has been\nlimited to annotations by a single expert. However, it is well-known that\nlawyers may disagree in their assessment of case facts. We hence collect a\nnovel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two\nexperts in the domain of international human rights law, for whom we observe\nweak agreement. We study their disagreements and build a two-level\ntask-independent taxonomy, supplemented with COC-specific subcategories. To our\nknowledge, this is the first work in the legal NLP that focuses on human label\nvariation. We quantitatively assess different taxonomy categories and find that\ndisagreements mainly stem from underspecification of the legal context, which\nposes challenges given the typically limited granularity and noise in COC\nmetadata. We further assess the explainablility of SOTA COC models on RAVE and\nobserve limited agreement between models and experts. Overall, our case study\nreveals hitherto underappreciated complexities in creating benchmark datasets\nin legal NLP that revolve around identifying aspects of a case's facts\nsupposedly relevant to its outcome.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shanshan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1\">Santosh T.Y.S.S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichim_O/0/1/0/all/0/1\">Oana Ichim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risini_I/0/1/0/all/0/1\">Isabella Risini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabmair_M/0/1/0/all/0/1\">Matthias Grabmair</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks. (arXiv:2310.11884v1 [cs.AI])","link":"http://arxiv.org/abs/2310.11884","description":"<p>In this paper, we review recent approaches for explaining concepts in neural\nnetworks. Concepts can act as a natural link between learning and reasoning:\nonce the concepts are identified that a neural learning system uses, one can\nintegrate those concepts with a reasoning system for inference or use a\nreasoning system to act upon them to improve or enhance the learning system. On\nthe other hand, knowledge can not only be extracted from neural networks but\nconcept knowledge can also be inserted into neural network architectures. Since\nintegrating learning and reasoning is at the core of neuro-symbolic AI, the\ninsights gained from this survey can serve as an important step towards\nrealizing neuro-symbolic AI based on explainable concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanza_S/0/1/0/all/0/1\">Sergio Lanza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rather a Nurse than a Physician -- Contrastive Explanations under Investigation. (arXiv:2310.11906v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11906","description":"<p>Contrastive explanations, where one decision is explained in contrast to\nanother, are supposed to be closer to how humans explain a decision than\nnon-contrastive explanations, where the decision is not necessarily referenced\nto an alternative. This claim has never been empirically validated. We analyze\nfour English text-classification datasets (SST2, DynaSent, BIOS and\nDBpedia-Animals). We fine-tune and extract explanations from three different\nmodels (RoBERTa, GTP-2, and T5), each in three different sizes and apply three\npost-hoc explainability methods (LRP, GradientxInput, GradNorm). We furthermore\ncollect and release human rationale annotations for a subset of 100 samples\nfrom the BIOS dataset for contrastive and non-contrastive settings. A\ncross-comparison between model-based rationales and human annotations, both in\ncontrastive and non-contrastive settings, yields a high agreement between the\ntwo settings for models as well as for humans. Moreover, model-based\nexplanations computed in both settings align equally well with human\nrationales. Thus, we empirically find that humans do not necessarily explain in\na contrastive manner.9 pages, long paper at ACL 2022 proceedings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eberle_O/0/1/0/all/0/1\">Oliver Eberle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabello_L/0/1/0/all/0/1\">Laura Cabello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1\">Stephanie Brandl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs. (arXiv:2310.11917v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11917","description":"<p>Semi-inductive link prediction (LP) in knowledge graphs (KG) is the task of\npredicting facts for new, previously unseen entities based on context\ninformation. Although new entities can be integrated by retraining the model\nfrom scratch in principle, such an approach is infeasible for large-scale KGs,\nwhere retraining is expensive and new entities may arise frequently. In this\npaper, we propose and describe a large-scale benchmark to evaluate\nsemi-inductive LP models. The benchmark is based on and extends Wikidata5M: It\nprovides transductive, k-shot, and 0-shot LP tasks, each varying the available\ninformation from (i) only KG structure, to (ii) including textual mentions, and\n(iii) detailed descriptions of the entities. We report on a small study of\nrecent approaches and found that semi-inductive LP performance is far from\ntransductive performance on long-tail entities throughout all experiments. The\nbenchmark provides a test bed for further research into integrating context and\ntextual information in semi-inductive LP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kochsiek_A/0/1/0/all/0/1\">Adrian Kochsiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemulla_R/0/1/0/all/0/1\">Rainer Gemulla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating semantic subspaces of Transformer sentence embeddings through linear structural probing. (arXiv:2310.11923v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11923","description":"<p>The question of what kinds of linguistic information are encoded in different\nlayers of Transformer-based language models is of considerable interest for the\nNLP community. Existing work, however, has overwhelmingly focused on word-level\nrepresentations and encoder-only language models with the masked-token training\nobjective. In this paper, we present experiments with semantic structural\nprobing, a method for studying sentence-level representations via finding a\nsubspace of the embedding space that provides suitable task-specific pairwise\ndistances between data-points. We apply our method to language models from\ndifferent families (encoder-only, decoder-only, encoder-decoder) and of\ndifferent sizes in the context of two tasks, semantic textual similarity and\nnatural-language inference. We find that model families differ substantially in\ntheir performance and layer dynamics, but that the results are largely\nmodel-size invariant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1\">Dmitry Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounded and Well-rounded: A Methodological Approach to the Study of Cross-modal and Cross-lingual Grounding. (arXiv:2310.11938v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11938","description":"<p>Grounding has been argued to be a crucial component towards the development\nof more complete and truly semantically competent artificial intelligence\nsystems. Literature has divided into two camps: While some argue that grounding\nallows for qualitatively different generalizations, others believe it can be\ncompensated by mono-modal data quantity. Limited empirical evidence has emerged\nfor or against either position, which we argue is due to the methodological\nchallenges that come with studying grounding and its effects on NLP systems.\n</p>\n<p>In this paper, we establish a methodological framework for studying what the\neffects are - if any - of providing models with richer input sources than\ntext-only. The crux of it lies in the construction of comparable samples of\npopulations of models trained on different input modalities, so that we can\ntease apart the qualitative effects of different input sources from\nquantifiable model performances. Experiments using this framework reveal\nqualitative differences in model behavior between cross-modally grounded,\ncross-lingually grounded, and ungrounded models, which we measure both at a\nglobal dataset level as well as for specific word representations, depending on\nhow concrete their semantics is.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mickus_T/0/1/0/all/0/1\">Timothee Mickus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zosa_E/0/1/0/all/0/1\">Elaine Zosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paperno_D/0/1/0/all/0/1\">Denis Paperno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models. (arXiv:2310.11954v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11954","description":"<p>AI-empowered music processing is a diverse field that encompasses dozens of\ntasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension\ntasks (e.g., music classification). For developers and amateurs, it is very\ndifficult to grasp all of these task to satisfy their requirements in music\nprocessing, especially considering the huge differences in the representations\nof music data and the model applicability across platforms among various tasks.\nConsequently, it is necessary to build a system to organize and integrate these\ntasks, and thus help practitioners to automatically analyze their demand and\ncall suitable tools as solutions to fulfill their requirements. Inspired by the\nrecent success of large language models (LLMs) in task automation, we develop a\nsystem, named MusicAgent, which integrates numerous music-related tools and an\nautonomous workflow to address user requirements. More specifically, we build\n1) toolset that collects tools from diverse sources, including Hugging Face,\nGitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g.,\nChatGPT) to organize these tools and automatically decompose user requests into\nmultiple sub-tasks and invoke corresponding music tools. The primary goal of\nthis system is to free users from the intricacies of AI-music tools, enabling\nthem to concentrate on the creative aspect. By granting users the freedom to\neffortlessly combine tools, the system offers a seamless and enriching music\nexperience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dingyao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Peiling Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emptying the Ocean with a Spoon: Should We Edit Models?. (arXiv:2310.11958v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11958","description":"<p>We call into question the recently popularized method of direct model editing\nas a means of correcting factual errors in LLM generations. We contrast model\nediting with three similar but distinct approaches that pursue better defined\nobjectives: (1) retrieval-based architectures, which decouple factual memory\nfrom inference and linguistic capabilities embodied in LLMs; (2) concept\nerasure methods, which aim at preventing systemic bias in generated text; and\n(3) attribution methods, which aim at grounding generations into identified\ntextual sources. We argue that direct model editing cannot be trusted as a\nsystematic remedy for the disadvantages inherent to LLMs, and while it has\nproven potential in improving model explainability, it opens risks by\nreinforcing the notion that models can be trusted for factuality. We call for\ncautious promotion and application of model editing as part of the LLM\ndeployment process, and for responsibly limiting the use cases of LLMs to those\nnot relying on editing as a critical component.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhadad_M/0/1/0/all/0/1\">Michael Elhadad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences. (arXiv:2310.11960v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11960","description":"<p>Transformer-based models have achieved state-of-the-art performance in many\nareas. However, the quadratic complexity of self-attention with respect to the\ninput length hinders the applicability of Transformer-based models to long\nsequences. To address this, we present Fast Multipole Attention, a new\nattention mechanism that uses a divide-and-conquer strategy to reduce the time\nand memory complexity of attention for sequences of length $n$ from\n$\\mathcal{O}(n^2)$ to $\\mathcal{O}(n \\log n)$ or $O(n)$, while retaining a\nglobal receptive field. The hierarchical approach groups queries, keys, and\nvalues into $\\mathcal{O}( \\log n)$ levels of resolution, where groups at\ngreater distances are increasingly larger in size and the weights to compute\ngroup quantities are learned. As such, the interaction between tokens far from\neach other is considered in lower resolution in an efficient hierarchical\nmanner. The overall complexity of Fast Multipole Attention is $\\mathcal{O}(n)$\nor $\\mathcal{O}(n \\log n)$, depending on whether the queries are down-sampled\nor not. This multi-level divide-and-conquer strategy is inspired by fast\nsummation methods from $n$-body physics and the Fast Multipole Method. We\nperform evaluation on autoregressive and bidirectional language modeling tasks\nand compare our Fast Multipole Attention model with other efficient attention\nvariants on medium-size datasets. We find empirically that the Fast Multipole\nTransformer performs much better than other efficient transformers in terms of\nmemory size and accuracy. The Fast Multipole Attention mechanism has the\npotential to empower large language models with much greater sequence lengths,\ntaking the full context into account in an efficient, naturally hierarchical\nmanner during training and when generating long sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yanming Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_G/0/1/0/all/0/1\">Giang Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sterck_H/0/1/0/all/0/1\">Hans De Sterck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMR Parsing with Causal Hierarchical Attention and Pointers. (arXiv:2310.11964v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11964","description":"<p>Translation-based AMR parsers have recently gained popularity due to their\nsimplicity and effectiveness. They predict linearized graphs as free texts,\navoiding explicit structure modeling. However, this simplicity neglects\nstructural locality in AMR graphs and introduces unnecessary tokens to\nrepresent coreferences. In this paper, we introduce new target forms of AMR\nparsing and a novel model, CHAP, which is equipped with causal hierarchical\nattention and the pointer mechanism, enabling the integration of structures\ninto the Transformer decoder. We empirically explore various alternative\nmodeling options. Experiments show that our model outperforms baseline models\non four out of five benchmarks in the setting of no additional data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_C/0/1/0/all/0/1\">Chao Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Filling in the Gaps: Efficient Event Coreference Resolution using Graph Autoencoder Networks. (arXiv:2310.11965v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11965","description":"<p>We introduce a novel and efficient method for Event Coreference Resolution\n(ECR) applied to a lower-resourced language domain. By framing ECR as a graph\nreconstruction task, we are able to combine deep semantic embeddings with\nstructural coreference chain knowledge to create a parameter-efficient family\nof Graph Autoencoder models (GAE). Our method significantly outperforms\nclassical mention-pair methods on a large Dutch event coreference corpus in\nterms of overall score, efficiency and training speed. Additionally, we show\nthat our models are consistently able to classify more difficult coreference\nlinks and are far more robust in low-data settings when compared to\ntransformer-based mention-pair coreference algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Langhe_L/0/1/0/all/0/1\">Loic De Langhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clercq_O/0/1/0/all/0/1\">Orph&#xe9;e De Clercq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoste_V/0/1/0/all/0/1\">Veronique Hoste</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation. (arXiv:2310.11976v1 [cs.CL])","link":"http://arxiv.org/abs/2310.11976","description":"<p>Diffusion models have garnered considerable interest in the field of text\ngeneration. Several studies have explored text diffusion models with different\nstructures and applied them to various tasks, including named entity\nrecognition and summarization. However, there exists a notable disparity\nbetween the \"easy-first\" text generation process of current diffusion models\nand the \"keyword-first\" natural text generation process of humans, which has\nreceived limited attention. To bridge this gap, we propose InfoDiffusion, a\nnon-autoregressive text diffusion model. Our approach introduces a\n\"keyinfo-first\" generation strategy and incorporates a noise schedule based on\nthe amount of text information. In addition, InfoDiffusion combines\nself-conditioning with a newly proposed partially noising model structure.\nExperimental results show that InfoDiffusion outperforms the baseline model in\nterms of generation quality and diversity, as well as exhibiting higher\nsampling efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers. (arXiv:2310.11984v1 [cs.LG])","link":"http://arxiv.org/abs/2310.11984","description":"<p>Since its introduction, the transformer model has demonstrated outstanding\nperformance across various tasks. However, there are still unresolved issues\nregarding length generalization, particularly in algorithmic tasks. In this\npaper, we investigate the inherent capabilities of transformer models in\nlearning arithmetic algorithms, such as addition and multiplication. Through\nexperiments and attention analysis, we identify a number of crucial factors for\nachieving optimal length generalization. We show that transformer models are\nable to generalize to long lengths with the help of targeted attention biasing.\nWe then introduce Attention Bias Calibration (ABC), a calibration stage that\nenables the model to automatically learn the proper attention biases, which we\nlink to mechanisms in relative position encoding. We demonstrate that using\nABC, the transformer model can achieve unprecedented perfect length\ngeneralization on certain arithmetic tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1\">Shaoxiong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yining Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sociotechnical Safety Evaluation of Generative AI Systems. (arXiv:2310.11986v1 [cs.AI])","link":"http://arxiv.org/abs/2310.11986","description":"<p>Generative AI systems produce a range of risks. To ensure the safety of\ngenerative AI systems, these risks must be evaluated. In this paper, we make\ntwo main contributions toward establishing such evaluations. First, we propose\na three-layered framework that takes a structured, sociotechnical approach to\nevaluating these risks. This framework encompasses capability evaluations,\nwhich are the main current approach to safety evaluation. It then reaches\nfurther by building on system safety principles, particularly the insight that\ncontext determines whether a given capability may cause harm. To account for\nrelevant context, our framework adds human interaction and systemic impacts as\nadditional layers of evaluation. Second, we survey the current state of safety\nevaluation of generative AI systems and create a repository of existing\nevaluations. Three salient evaluation gaps emerge from this analysis. We\npropose ways forward to closing these gaps, outlining practical steps as well\nas roles and responsibilities for different actors. Sociotechnical safety\nevaluation is a tractable approach to the robust and comprehensive safety\nevaluation of generative AI systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weidinger_L/0/1/0/all/0/1\">Laura Weidinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauh_M/0/1/0/all/0/1\">Maribeth Rauh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchal_N/0/1/0/all/0/1\">Nahema Marchal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzini_A/0/1/0/all/0/1\">Arianna Manzini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1\">Lisa Anne Hendricks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mateos_Garcia_J/0/1/0/all/0/1\">Juan Mateos-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergman_S/0/1/0/all/0/1\">Stevie Bergman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kay_J/0/1/0/all/0/1\">Jackie Kay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffin_C/0/1/0/all/0/1\">Conor Griffin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bariach_B/0/1/0/all/0/1\">Ben Bariach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_I/0/1/0/all/0/1\">Iason Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isaac_W/0/1/0/all/0/1\">William Isaac</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs. (arXiv:2310.12008v1 [cs.CL])","link":"http://arxiv.org/abs/2310.12008","description":"<p>Knowledge graph entity typing (KGET) aims at inferring plausible types of\nentities in knowledge graphs. Existing approaches to KGET focus on how to\nbetter encode the knowledge provided by the neighbors and types of an entity\ninto its representation. However, they ignore the semantic knowledge provided\nby the way in which types can be clustered together. In this paper, we propose\na novel method called Multi-view Contrastive Learning for knowledge graph\nEntity Typing (MCLET), which effectively encodes the coarse-grained knowledge\nprovided by clusters into entity and type embeddings. MCLET is composed of\nthree modules: i) Multi-view Generation and Encoder module, which encodes\nstructured information from entity-type, entity-cluster and cluster-type views;\nii) Cross-view Contrastive Learning module, which encourages different views to\ncollaboratively improve view-specific representations of entities and types;\niii) Entity Typing Prediction module, which integrates multi-head attention and\na Mixture-of-Experts strategy to infer missing entity types. Extensive\nexperiments show the strong performance of MCLET compared to the\nstate-of-the-art\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiwei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Basulto_V/0/1/0/all/0/1\">V&#xed;ctor Guti&#xe9;rrez-Basulto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhiliang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection. (arXiv:2310.12011v1 [cs.CL])","link":"http://arxiv.org/abs/2310.12011","description":"<p>Commonsense Knowledge Graphs (CSKGs) are crucial for commonsense reasoning,\nyet constructing them through human annotations can be costly. As a result,\nvarious automatic methods have been proposed to construct CSKG with larger\nsemantic coverage. However, these unsupervised approaches introduce spurious\nnoise that can lower the quality of the resulting CSKG, which cannot be tackled\neasily by existing denoising algorithms due to the unique characteristics of\nnodes and structures in CSKGs. To address this issue, we propose Gold (Global\nand Local-aware Denoising), a denoising framework for CSKGs that incorporates\nentity semantic information, global rules, and local structural information\nfrom the CSKG. Experiment results demonstrate that Gold outperforms all\nbaseline methods in noise detection tasks on synthetic noisy CSKG benchmarks.\nFurthermore, we show that denoising a real-world CSKG is effective and even\nbenefits the downstream zero-shot commonsense question-answering task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zheye Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation. (arXiv:2310.12020v1 [cs.RO])","link":"http://arxiv.org/abs/2310.12020","description":"<p>The convergence of embodied agents and large language models (LLMs) has\nbrought significant advancements to embodied instruction following.\nParticularly, the strong reasoning capabilities of LLMs make it possible for\nrobots to perform long-horizon tasks without expensive annotated\ndemonstrations. However, public benchmarks for testing the long-horizon\nreasoning capabilities of language-conditioned robots in various scenarios are\nstill missing. To fill this gap, this work focuses on the tabletop manipulation\ntask and releases a simulation benchmark, \\textit{LoHoRavens}, which covers\nvarious long-horizon reasoning aspects spanning color, size, space, arithmetics\nand reference. Furthermore, there is a key modality bridging problem for\nlong-horizon manipulation tasks with LLMs: how to incorporate the observation\nfeedback during robot execution for the LLM's closed-loop planning, which is\nhowever less studied by prior work. We investigate two methods of bridging the\nmodality gap: caption generation and learnable interface for incorporating\nexplicit and implicit observation feedback to the LLM, respectively. These\nmethods serve as the two baselines for our proposed benchmark. Experiments show\nthat both methods struggle to solve some tasks, indicating long-horizon\nmanipulation tasks are still challenging for current popular models. We expect\nthe proposed public benchmark and baselines can help the community develop\nbetter models for long-horizon tabletop manipulation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicke_P/0/1/0/all/0/1\">Philipp Wicke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senel_L/0/1/0/all/0/1\">L&#xfc;tfi Kerem &#x15e;enel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueredo_L/0/1/0/all/0/1\">Luis Figueredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naceri_A/0/1/0/all/0/1\">Abdeldjallil Naceri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddadin_S/0/1/0/all/0/1\">Sami Haddadin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CORE: A Few-Shot Company Relation Classification Dataset for Robust Domain Adaptation. (arXiv:2310.12024v1 [cs.CL])","link":"http://arxiv.org/abs/2310.12024","description":"<p>We introduce CORE, a dataset for few-shot relation classification (RC)\nfocused on company relations and business entities. CORE includes 4,708\ninstances of 12 relation types with corresponding textual evidence extracted\nfrom company Wikipedia pages. Company names and business entities pose a\nchallenge for few-shot RC models due to the rich and diverse information\nassociated with them. For example, a company name may represent the legal\nentity, products, people, or business divisions depending on the context.\nTherefore, deriving the relation type between entities is highly dependent on\ntextual context. To evaluate the performance of state-of-the-art RC models on\nthe CORE dataset, we conduct experiments in the few-shot domain adaptation\nsetting. Our results reveal substantial performance gaps, confirming that\nmodels trained on different domains struggle to adapt to CORE. Interestingly,\nwe find that models trained on CORE showcase improved out-of-domain\nperformance, which highlights the importance of high-quality data for robust\ndomain adaptation. Specifically, the information richness embedded in business\nentities allows models to focus on contextual nuances, reducing their reliance\non superficial clues such as relation-specific verbs. In addition to the\ndataset, we provide relevant code snippets to facilitate reproducibility and\nencourage further research in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borchert_P/0/1/0/all/0/1\">Philipp Borchert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weerdt_J/0/1/0/all/0/1\">Jochen De Weerdt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coussement_K/0/1/0/all/0/1\">Kristof Coussement</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caigny_A/0/1/0/all/0/1\">Arno De Caigny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models. (arXiv:2310.12049v1 [cs.CL])","link":"http://arxiv.org/abs/2310.12049","description":"<p>Existing text scaling methods often require a large corpus, struggle with\nshort texts, or require labeled data. We develop a text scaling method that\nleverages the pattern recognition capabilities of generative large language\nmodels (LLMs). Specifically, we propose concept-guided chain-of-thought\n(CGCoT), which uses prompts designed to summarize ideas and identify target\nparties in texts to generate concept-specific breakdowns, in many ways similar\nto guidance for human coder content analysis. CGCoT effectively shifts pairwise\ntext comparisons from a reasoning problem to a pattern recognition problem. We\nthen pairwise compare concept-specific breakdowns using an LLM. We use the\nresults of these pairwise comparisons to estimate a scale using the\nBradley-Terry model. We use this approach to scale affective speech on Twitter.\nOur measures correlate more strongly with human judgments than alternative\napproaches like Wordfish. Besides a small set of pilot data to develop the\nCGCoT prompts, our measures require no additional labeled data and produce\nbinary predictions comparable to a RoBERTa-Large model fine-tuned on thousands\nof human-labeled tweets. We demonstrate how combining substantive knowledge\nwith LLMs can create state-of-the-art measures of abstract concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Patrick Y. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagler_J/0/1/0/all/0/1\">Jonathan Nagler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_J/0/1/0/all/0/1\">Joshua A. Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messing_S/0/1/0/all/0/1\">Solomon Messing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education. (arXiv:2310.12059v1 [cs.CL])","link":"http://arxiv.org/abs/2310.12059","description":"<p>In this paper, we evaluate the ability of large language models (LLMs) to\nperform multiple choice symbol binding (MCSB) for multiple choice question\nanswering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus\non Vietnamese, with fewer challenging MCQA datasets than in English. The two\nexisting datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent\nresearch in Vietnamese natural language processing (NLP) has focused on the\nVietnamese National High School Graduation Examination (VNHSGE) from 2019 to\n2023 to evaluate ChatGPT. However, these studies have mainly focused on how\nChatGPT solves the VNHSGE step by step. We aim to create a novel and\nhigh-quality dataset by providing structured guidelines for typing LaTeX\nformulas for mathematics, physics, chemistry, and biology. This dataset can be\nused to evaluate the MCSB ability of LLMs and smaller language models (LMs)\nbecause it is typed in a strict LaTeX style. We focus on predicting the\ncharacter (A, B, C, or D) that is the most likely answer to a question, given\nthe context of the question. Our evaluation of six well-known LLMs, namely\nBLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the\nViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising\nresults on the MCSB ability of LLMs for Vietnamese. The dataset is available\nfor research purposes only.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc-Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quoc-Nam Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Book for the Annotation of Diverse Cross-Document Coreference of Entities in News Articles. (arXiv:2310.12064v1 [cs.CL])","link":"http://arxiv.org/abs/2310.12064","description":"<p>This paper presents a scheme for annotating coreference across news articles,\nextending beyond traditional identity relations by also considering\nnear-identity and bridging relations. It includes a precise description of how\nto set up Inception, a respective annotation tool, how to annotate entities in\nnews articles, connect them with diverse coreferential relations, and link them\nacross documents to Wikidata's global knowledge graph. This multi-layered\nannotation approach is discussed in the context of the problem of media bias.\nOur main contribution lies in providing a methodology for creating a diverse\ncross-document coreference corpus which can be applied to the analysis of media\nbias by word-choice and labelling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vogel_J/0/1/0/all/0/1\">Jakob Vogel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPEED: Speculative Pipelined Execution for Efficient Decoding. (arXiv:2310.12072v1 [cs.CL])","link":"http://arxiv.org/abs/2310.12072","description":"<p>Generative Large Language Models (LLMs) based on the Transformer architecture\nhave recently emerged as a dominant foundation model for a wide range of\nNatural Language Processing tasks. Nevertheless, their application in real-time\nscenarios has been highly restricted due to the significant inference latency\nassociated with these models. This is particularly pronounced due to the\nautoregressive nature of generative LLM inference, where tokens are generated\nsequentially since each token depends on all previous output tokens. It is\ntherefore challenging to achieve any token-level parallelism, making inference\nextremely memory-bound. In this work, we propose SPEED, which improves\ninference efficiency by speculatively executing multiple future tokens in\nparallel with the current token using predicted values based on early-layer\nhidden states. For Transformer decoders that employ parameter sharing, the\nmemory operations for the tokens executing in parallel can be amortized, which\nallows us to accelerate generative LLM inference. We demonstrate the efficiency\nof our method in terms of latency reduction relative to model accuracy and\ndemonstrate how speculation allows for training deeper decoders with parameter\nsharing with minimal runtime overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hooper_C/0/1/0/all/0/1\">Coleman Hooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadzadeh_H/0/1/0/all/0/1\">Hiva Mohammadzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genc_H/0/1/0/all/0/1\">Hasan Genc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Sophia Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures. (arXiv:2310.12074v1 [cs.CL])","link":"http://arxiv.org/abs/2310.12074","description":"<p>This paper introduces a new IncidentAI dataset for safety prevention.\nDifferent from prior corpora that usually contain a single task, our dataset\ncomprises three tasks: named entity recognition, cause-effect extraction, and\ninformation retrieval. The dataset is annotated by domain experts who have at\nleast six years of practical experience as high-pressure gas conservation\nmanagers. We validate the contribution of the dataset in the scenario of safety\nprevention. Preliminary results on the three tasks show that NLP techniques are\nbeneficial for analyzing incident reports to prevent future failures. The\ndataset facilitates future research in NLP and incident management communities.\nThe access to the dataset is also provided (the IncidentAI dataset is available\nat: https://github.com/Cinnamon/incident-ai-dataset).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_S/0/1/0/all/0/1\">Shumpei Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Tien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizokuchi_H/0/1/0/all/0/1\">Hiroki Mizokuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Anh D. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huu-Hiep Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Dung Tien Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Benefit of Generative Foundation Models for Human Activity Recognition. (arXiv:2310.12085v1 [cs.CV])","link":"http://arxiv.org/abs/2310.12085","description":"<p>In human activity recognition (HAR), the limited availability of annotated\ndata presents a significant challenge. Drawing inspiration from the latest\nadvancements in generative AI, including Large Language Models (LLMs) and\nmotion synthesis models, we believe that generative AI can address this data\nscarcity by autonomously generating virtual IMU data from text descriptions.\nBeyond this, we spotlight several promising research pathways that could\nbenefit from generative AI for the community, including the generating\nbenchmark datasets, the development of foundational models specific to HAR, the\nexploration of hierarchical structures within HAR, breaking down complex\nactivities, and applications in health sensing and activity summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Z/0/1/0/all/0/1\">Zikang Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Hyeokhyen Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plotz_T/0/1/0/all/0/1\">Thomas Pl&#xf6;tz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])","link":"http://arxiv.org/abs/2310.12086","description":"<p>Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread\nattention owing to their myriad of practical applications, yet their adoption\nhas been constrained by issues of fact-conflicting hallucinations across web\nplatforms. The assessment of factuality in text, produced by LLMs, remains\ninadequately explored, extending not only to the judgment of vanilla facts but\nalso encompassing the evaluation of factual errors emerging in complex\ninferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a\nfact-conflicting hallucination detection benchmark meticulously designed for\nLLMs. Functioning as a pivotal tool in evaluating factuality within\n\"Query-Respons\" contexts, our benchmark assimilates a large-scale dataset,\nencapsulating a broad spectrum of factuality patterns, such as vanilla,\nmulti-hops, comparison, and set-operation patterns. A distinctive feature of\nour benchmark is its incorporation of fact-based chains of evidence, thereby\nfacilitating comprehensive and conducive factual reasoning throughout the\nassessment process. We evaluate multiple LLMs, demonstrating the effectiveness\nof the benchmark and current methods fall short of faithfully detecting factual\nerrors. Furthermore, we present TRUTH-TRIANGULATOR that synthesizes reflective\nconsiderations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming\nto yield more credible detection through the amalgamation of predictive results\nand evidence. The benchmark dataset and source code will be made available in\nhttps://github.com/zjunlp/FactCHD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Duanzheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1\">Honghao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengxi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chengfei Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling. (arXiv:2310.12100v1 [cs.CL])","link":"http://arxiv.org/abs/2310.12100","description":"<p>Large language models (LLMs) and vision language models (VLMs) demonstrate\nexcellent performance on a wide range of tasks by scaling up parameter counts\nfrom O(10^9) to O(10^{12}) levels and further beyond. These large scales make\nit impossible to adapt and deploy fully specialized models given a task of\ninterest. Parameter-efficient fine-tuning (PEFT) emerges as a promising\ndirection to tackle the adaptation and serving challenges for such large\nmodels. We categorize PEFT techniques into two types: intrusive and\nnon-intrusive. Intrusive PEFT techniques directly change a model's internal\narchitecture. Though more flexible, they introduce significant complexities for\ntraining and serving. Non-intrusive PEFT techniques leave the internal\narchitecture unchanged and only adapt model-external parameters, such as\nembeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT\ntechnique that achieves competitive performance compared to SoTA intrusive PEFT\n(LoRA) and full model fine-tuning (FT) on various tasks. We evaluate using both\ntext-only and multimodal tasks, with experiments that account for both\nparameter-count scaling and training regime (with and without instruction\ntuning).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabral_T/0/1/0/all/0/1\">Tanmaya Dabral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiageng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1\">Geoff Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chun-Ta Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yi Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Michael Bendersky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers. (arXiv:2310.12118v1 [cs.CL])","link":"http://arxiv.org/abs/2310.12118","description":"<p>Neural networks have revolutionized language modeling and excelled in various\ndownstream tasks. However, the extent to which these models achieve\ncompositional generalization comparable to human cognitive abilities remains a\ntopic of debate. While existing approaches in the field have mainly focused on\nnovel architectures and alternative learning paradigms, we introduce a\npioneering method harnessing the power of dataset cartography (Swayamdipta et\nal., 2020). By strategically identifying a subset of compositional\ngeneralization data using this approach, we achieve a remarkable improvement in\nmodel accuracy, yielding enhancements of up to 10% on CFQ and COGS datasets.\nNotably, our technique incorporates dataset cartography as a curriculum\nlearning criterion, eliminating the need for hyperparameter tuning while\nconsistently achieving superior performance. Our findings highlight the\nuntapped potential of dataset cartography in unleashing the full capabilities\nof compositional generalization within Transformer models. Our code is\navailable at https://github.com/cyberiada/cartography-for-compositionality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ince_O/0/1/0/all/0/1\">Osman Batur &#x130;nce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeraati_T/0/1/0/all/0/1\">Tanin Zeraati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yagcioglu_S/0/1/0/all/0/1\">Semih Yagcioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1\">Yadollah Yaghoobzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1\">Erkut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1\">Aykut Erdem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks. (arXiv:2310.12126v1 [cs.LG])","link":"http://arxiv.org/abs/2310.12126","description":"<p>We introduce SHARCS for adaptive inference that takes into account the\nhardness of input samples. SHARCS can train a router on any transformer\nnetwork, enabling the model to direct different samples to sub-networks with\nvarying widths. Our experiments demonstrate that: (1) SHARCS outperforms or\ncomplements existing per-sample adaptive inference methods across various\nclassification tasks in terms of accuracy vs. FLOPs; (2) SHARCS generalizes\nacross different architectures and can be even applied to compressed and\nefficient transformer encoders to further improve their efficiency; (3) SHARCS\ncan provide a 2 times inference speed up at an insignificant drop in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sachin Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation. (arXiv:2310.12127v1 [cs.CL])","link":"http://arxiv.org/abs/2310.12127","description":"<p>Recent instruction fine-tuned models can solve multiple NLP tasks when\nprompted to do so, with machine translation (MT) being a prominent use case.\nHowever, current research often focuses on standard performance benchmarks,\nleaving compelling fairness and ethical considerations behind. In MT, this\nmight lead to misgendered translations, resulting, among other harms, in the\nperpetuation of stereotypes and prejudices. In this work, we address this gap\nby investigating whether and to what extent such models exhibit gender bias in\nmachine translation and how we can mitigate it. Concretely, we compute\nestablished gender bias metrics on the WinoMT corpus from English to German and\nSpanish. We discover that IFT models default to male-inflected translations,\neven disregarding female occupational stereotypes. Next, using interpretability\nmethods, we unveil that models systematically overlook the pronoun indicating\nthe gender of a target occupation in misgendered translations. Finally, based\non this finding, we propose an easy-to-implement and effective bias mitigation\nsolution based on few-shot learning that leads to significantly fairer\ntranslations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1\">Giuseppe Attanasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plaza_del_Arco_F/0/1/0/all/0/1\">Flor Miriam Plaza-del-Arco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nozza_D/0/1/0/all/0/1\">Debora Nozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning. (arXiv:2310.12128v1 [cs.CV])","link":"http://arxiv.org/abs/2310.12128","description":"<p>Text-to-image (T2I) generation has seen significant growth over the past few\nyears. Despite this, there has been little work on generating diagrams with T2I\nmodels. A diagram is a symbolic/schematic representation that explains\ninformation using structurally rich and spatially complex visualizations (e.g.,\na dense combination of related objects, text labels, directional arrows,\nconnection lines, etc.). Existing state-of-the-art T2I models often fail at\ndiagram generation because they lack fine-grained object layout control when\nmany objects are densely connected via complex relations such as arrows/lines\nand also often fail to render comprehensible text labels. To address this gap,\nwe present DiagrammerGPT, a novel two-stage text-to-diagram generation\nframework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4)\nto generate more accurate open-domain, open-platform diagrams. In the first\nstage, we use LLMs to generate and iteratively refine 'diagram plans' (in a\nplanner-auditor feedback loop) which describe all the entities (objects and\ntext labels), their relationships (arrows or lines), and their bounding box\nlayouts. In the second stage, we use a diagram generator, DiagramGLIGEN, and a\ntext label rendering module to generate diagrams following the diagram plans.\nTo benchmark the text-to-diagram generation task, we introduce AI2D-Caption, a\ndensely annotated diagram dataset built on top of the AI2D dataset. We show\nquantitatively and qualitatively that our DiagrammerGPT framework produces more\naccurate diagrams, outperforming existing T2I models. We also provide\ncomprehensive analysis including open-domain diagram generation, vector graphic\ndiagram generation in different platforms, human-in-the-loop diagram plan\nediting, and multimodal planner/auditor LLMs (e.g., GPT-4Vision). We hope our\nwork can inspire further research on diagram generation via T2I models and\nLLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zala_A/0/1/0/all/0/1\">Abhay Zala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Han Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudointelligence: A Unifying Framework for Language Model Evaluation. (arXiv:2310.12135v1 [cs.CL])","link":"http://arxiv.org/abs/2310.12135","description":"<p>With large language models surpassing human performance on an increasing\nnumber of benchmarks, we must take a principled approach for targeted\nevaluation of model capabilities. Inspired by pseudorandomness, we propose\npseudointelligence, which captures the maxim that \"(perceived) intelligence\nlies in the eye of the beholder\". That is, that claims of intelligence are\nmeaningful only when their evaluator is taken into account. Concretely, we\npropose a complexity-theoretic framework of model evaluation cast as a dynamic\ninteraction between a model and a learned evaluator. We demonstrate that this\nframework can be used to reason about two case studies in language model\nevaluation, as well as analyze existing evaluation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murty_S/0/1/0/all/0/1\">Shikhar Murty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paradise_O/0/1/0/all/0/1\">Orr Paradise</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pratyusha Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Mechanisms for Representing, Indexing and Manipulating Concepts. (arXiv:2310.12143v1 [cs.LG])","link":"http://arxiv.org/abs/2310.12143","description":"<p>Deep networks typically learn concepts via classifiers, which involves\nsetting up a model and training it via gradient descent to fit the\nconcept-labeled data. We will argue instead that learning a concept could be\ndone by looking at its moment statistics matrix to generate a concrete\nrepresentation or signature of that concept. These signatures can be used to\ndiscover structure across the set of concepts and could recursively produce\nhigher-level concepts by learning this structure from those signatures. When\nthe concepts are `intersected', signatures of the concepts can be used to find\na common theme across a number of related `intersected' concepts. This process\ncould be used to keep a dictionary of concepts so that inputs could correctly\nidentify and be routed to the set of concepts involved in the (latent)\ngeneration of the input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meka_R/0/1/0/all/0/1\">Raghu Meka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panigrahy_R/0/1/0/all/0/1\">Rina Panigrahy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_K/0/1/0/all/0/1\">Kulin Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Retrieval Augmentation for Long-Form Question Answering. (arXiv:2310.12150v1 [cs.CL])","link":"http://arxiv.org/abs/2310.12150","description":"<p>We present a study of retrieval-augmented language models (LMs) on long-form\nquestion answering. We analyze how retrieval augmentation impacts different\nLMs, by comparing answers generated from models while using the same evidence\ndocuments, and how differing quality of retrieval document set impacts the\nanswers generated from the same LM. We study various attributes of generated\nanswers (e.g., fluency, length, variance) with an emphasis on the attribution\nof generated long-form answers to in-context evidence documents. We collect\nhuman annotations of answer attribution and evaluate methods for automatically\njudging attribution. Our study provides new insights on how retrieval\naugmentation impacts long, knowledge-rich text generation of LMs. We further\nidentify attribution patterns for long text generation and analyze the main\nculprits of attribution errors. Together, our analysis reveals how retrieval\naugmentation impacts long knowledge-rich text generation and provide directions\nfor future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hung-Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Shane A. Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YATO: Yet Another deep learning based Text analysis Open toolkit. (arXiv:2209.13877v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.13877","description":"<p>We introduce YATO, an open-source, easy-to-use toolkit for text analysis with\ndeep learning. Different from existing heavily engineered toolkits and\nplatforms, YATO is lightweight and user-friendly for researchers from\ncross-disciplinary areas. Designed in a hierarchical structure, YATO supports\nfree combinations of three types of widely used features including 1)\ntraditional neural networks (CNN, RNN, etc.); 2) pre-trained language models\n(BERT, RoBERTa, ELECTRA, etc.); and 3) user-customized neural features via a\nsimple configurable file. Benefiting from the advantages of flexibility and\nease of use, YATO can facilitate fast reproduction and refinement of\nstate-of-the-art NLP models, and promote the cross-disciplinary applications of\nNLP techniques. The code, examples, and documentation are publicly available at\nhttps://github.com/jiesutd/YATO. A demo video is also available at\nhttps://www.youtube.com/playlist?list=PLJ0mhzMcRuDUlTkzBfAftOqiJRxYTTjXH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yile Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiageng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhiyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring and Narrowing the Compositionality Gap in Language Models. (arXiv:2210.03350v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03350","description":"<p>We investigate the ability of language models to perform compositional\nreasoning tasks where the overall solution depends on correctly composing the\nanswers to sub-problems. We measure how often models can correctly answer all\nsub-problems but not generate the overall solution, a ratio we call the\ncompositionality gap. We evaluate this ratio by asking multi-hop questions with\nanswers that require composing multiple facts unlikely to have been observed\ntogether during pretraining. In the GPT-3 family of models, as model size\nincreases we show that the single-hop question answering performance improves\nfaster than the multi-hop performance does, therefore the compositionality gap\ndoes not decrease. This surprising result suggests that while more powerful\nmodels memorize and recall more factual knowledge, they show no corresponding\nimprovement in their ability to perform this kind of compositional reasoning.\n</p>\n<p>We then demonstrate how elicitive prompting (such as chain of thought)\nnarrows the compositionality gap by reasoning explicitly. We present a new\nmethod, self-ask, that further improves on chain of thought. In our method, the\nmodel explicitly asks itself (and answers) follow-up questions before answering\nthe initial question. We finally show that self-ask's structured prompting lets\nus easily plug in a search engine to answer the follow-up questions, which\nadditionally improves accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1\">Ofir Press</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Muru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seq2Seq-SC: End-to-End Semantic Communication Systems with Pre-trained Language Model. (arXiv:2210.15237v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2210.15237","description":"<p>In this work, we propose a realistic semantic network called seq2seq-SC,\ndesigned to be compatible with 5G NR and capable of working with generalized\ntext datasets using a pre-trained language model. The goal is to achieve\nunprecedented communication efficiency by focusing on the meaning of messages\nin semantic communication. We employ a performance metric called semantic\nsimilarity, measured by BLEU for lexical similarity and SBERT for semantic\nsimilarity. Our findings demonstrate that seq2seq-SC outperforms previous\nmodels in extracting semantically meaningful information while maintaining\nsuperior performance. This study paves the way for continued advancements in\nsemantic communication and its prospective incorporation with future wireless\nsystems in 6G networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Ju-Hyung Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheen_E/0/1/0/all/0/1\">Eunsoo Sheen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_T/0/1/0/all/0/1\">Thomas Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2212.10764","description":"<p>Domain adaptation aims to transfer the knowledge learned on (data-rich)\nsource domains to (low-resource) target domains, and a popular method is\ninvariant representation learning, which matches and aligns the data\ndistributions on the feature space. Although this method is studied extensively\nand applied on classification and regression problems, its adoption on ranking\nproblems is sporadic, and the few existing implementations lack theoretical\njustifications. This paper revisits invariant representation learning for\nranking. Upon reviewing prior work, we found that they implement what we call\nitem-level alignment, which aligns the distributions of the items being ranked\nfrom all lists in aggregate but ignores their list structure. However, the list\nstructure should be leveraged, because it is intrinsic to ranking problems\nwhere the data and the metrics are defined and computed on lists, not the items\nby themselves. To close this discrepancy, we propose list-level alignment --\nlearning domain-invariant representations at the higher level of lists. The\nbenefits are twofold: it leads to the first domain adaptation generalization\nbound for ranking, in turn providing theoretical support for the proposed\nmethod, and it achieves better empirical transfer performance for unsupervised\ndomain adaptation on ranking tasks, including passage reranking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xian_R/0/1/0/all/0/1\">Ruicheng Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1\">Honglei Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Ji Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Kai Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuanhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Michael Bendersky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v8 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.02676","description":"<p>Learning from human preferences is important for language models to match\nhuman needs and to align with human and social values. Prior works have\nachieved remarkable successes by learning from human feedback to understand and\nfollow instructions. Nonetheless, these methods are either founded on\nhand-picked model generations that are favored by human annotators, rendering\nthem inefficient in terms of data utilization and challenging to apply in\ngeneral, or they depend on reinforcement learning, which often suffers from\nimperfect reward functions and relies on extremely challenging optimizations.\nIn this work, we propose a novel technique, Chain of Hindsight, that is easy to\noptimize and can learn from any form of feedback, regardless of its polarity.\nOur idea is inspired by how humans learn from extensive feedback presented in\nthe form of languages. We convert all types of feedback into sequences of\nsentences, which are then used to fine-tune the model, allowing us to take\nadvantage of the language comprehension capabilities of language models. We\ncondition the model on a sequence of model generations paired with feedback. By\ndoing so, the model is trained to generate outputs based on feedback, while\nlearning to identify and correct negative attributes or errors. Applying our\nmethod to large language models, we observed that Chain of Hindsight\nsignificantly surpasses previous methods in aligning language models with human\npreferences. We report significant improvements on summarization and dialogue\nbenchmarks, with our approach markedly preferred in human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sferrazza_C/0/1/0/all/0/1\">Carmelo Sferrazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting. (arXiv:2302.04813v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.04813","description":"<p>Recent work has shown how to prompt large language models with explanations\nto obtain strong performance on textual reasoning tasks, i.e., the\nchain-of-thought paradigm. However, subtly different explanations can yield\nwidely varying downstream task accuracy. Explanations that have not been\n\"tuned\" for a task, such as off-the-shelf explanations written by nonexperts,\nmay lead to mediocre performance. This paper tackles the problem of how to\noptimize explanation-infused prompts in a blackbox fashion. We first generate\nsets of candidate explanations for each example in the prompt using a\nleave-one-out scheme, then find an effective combination of these explanations\nwith a two-stage framework. We first evaluate explanations for each in-context\nexample in isolation according to two proxy metrics, log likelihood and\naccuracy on new examples. Then, we search over combinations of explanations to\nfind one that yields high performance against a silver-labeled development set.\nAcross four textual reasoning tasks spanning question answering, mathematical\nreasoning, and natural language inference, results show that our proxy metrics\ncorrelate with ground truth accuracy and our overall method can effectively\nimprove prompts over crowdworker annotations and naive search strategies\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-Text Retrieval by Supervised Sparse Multi-Grained Learning. (arXiv:2302.09473v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2302.09473","description":"<p>While recent progress in video-text retrieval has been advanced by the\nexploration of better representation learning, in this paper, we present a\nnovel multi-grained sparse learning framework, S3MA, to learn an aligned sparse\nspace shared between the video and the text for video-text retrieval. The\nshared sparse space is initialized with a finite number of sparse concepts,\neach of which refers to a number of words. With the text data at hand, we learn\nand update the shared sparse space in a supervised manner using the proposed\nsimilarity and alignment losses. Moreover, to enable multi-grained alignment,\nwe incorporate frame representations for better modeling the video modality and\ncalculating fine-grained and coarse-grained similarities. Benefiting from the\nlearned shared sparse space and multi-grained similarities, extensive\nexperiments on several video-text retrieval benchmarks demonstrate the\nsuperiority of S3MA over existing methods. Our code is available at\nhttps://github.com/yimuwangcs/Better_Cross_Modal_Retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yimu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments. (arXiv:2302.11649v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2302.11649","description":"<p>Grounding navigational commands to linear temporal logic (LTL) leverages its\nunambiguous semantics for reasoning about long-horizon tasks and verifying the\nsatisfaction of temporal constraints. Existing approaches require training data\nfrom the specific environment and landmarks that will be used in natural\nlanguage to understand commands in those environments. We propose Lang2LTL, a\nmodular system and a software package that leverages large language models\n(LLMs) to ground temporal navigational commands to LTL specifications in\nenvironments without prior language data. We comprehensively evaluate Lang2LTL\nfor five well-defined generalization behaviors. Lang2LTL demonstrates the\nstate-of-the-art ability of a single model to ground navigational commands to\ndiverse temporal specifications in 21 city-scaled environments. Finally, we\ndemonstrate a physical robot using Lang2LTL can follow 52 semantically diverse\nnavigational commands in two indoor environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jason Xinyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idrees_I/0/1/0/all/0/1\">Ifrah Idrees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sam Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schornstein_B/0/1/0/all/0/1\">Benjamin Schornstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1\">Stefanie Tellex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ankit Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixture of Soft Prompts for Controllable Data Generation. (arXiv:2303.01580v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.01580","description":"<p>Large language models (LLMs) effectively generate fluent text when the target\noutput follows natural language patterns. However, structured prediction tasks\nconfine the output format to a limited ontology, causing even very large models\nto struggle since they were never trained with such restrictions in mind. The\ndifficulty of using LLMs for direct prediction is exacerbated in few-shot\nlearning scenarios, which commonly arise due to domain shift and resource\nlimitations. We flip the problem on its head by leveraging the LLM as a tool\nfor data augmentation rather than direct prediction. Our proposed Mixture of\nSoft Prompts (MSP) serves as a parameter-efficient procedure for generating\ndata in a controlled manner. Denoising mechanisms are further applied to\nimprove the quality of synthesized data. Automatic metrics show our method is\ncapable of producing diverse and natural text, while preserving label\nsemantics. Moreover, MSP achieves state-of-the-art results on three benchmarks\nwhen compared against strong baselines. Our method offers an alternate\ndata-centric approach for applying LLMs to complex prediction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Derek Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Celine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yunan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosati_D/0/1/0/all/0/1\">Domenic Rosati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. (arXiv:2303.13408v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13408","description":"<p>The rise in malicious usage of large language models, such as fake content\ncreation and academic plagiarism, has motivated the development of approaches\nthat identify AI-generated text, including those based on watermarking or\noutlier detection. However, the robustness of these detection algorithms to\nparaphrases of AI-generated text remains unclear. To stress test these\ndetectors, we build a 11B parameter paraphrase generation model (DIPPER) that\ncan paraphrase paragraphs, condition on surrounding context, and control\nlexical diversity and content reordering. Using DIPPER to paraphrase text\ngenerated by three large language models (including GPT3.5-davinci-003)\nsuccessfully evades several detectors, including watermarking, GPTZero,\nDetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection\naccuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of\n1%), without appreciably modifying the input semantics.\n</p>\n<p>To increase the robustness of AI-generated text detection to paraphrase\nattacks, we introduce a simple defense that relies on retrieving\nsemantically-similar generations and must be maintained by a language model API\nprovider. Given a candidate text, our algorithm searches a database of\nsequences previously generated by the API, looking for sequences that match the\ncandidate text within a certain threshold. We empirically verify our defense\nusing a database of 15M generations from a fine-tuned T5-XXL model and find\nthat it can detect 80% to 97% of paraphrased generations across different\nsettings while only classifying 1% of human-written sequences as AI-generated.\nWe open-source our models, code and data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yixiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpinska_M/0/1/0/all/0/1\">Marzena Karpinska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SkillGPT: a RESTful API service for skill extraction and standardization using a Large Language Model. (arXiv:2304.11060v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.11060","description":"<p>We present SkillGPT, a tool for skill extraction and standardization (SES)\nfrom free-style job descriptions and user profiles with an open-source Large\nLanguage Model (LLM) as backbone. Most previous methods for similar tasks\neither need supervision or rely on heavy data-preprocessing and feature\nengineering. Directly prompting the latest conversational LLM for standard\nskills, however, is slow, costly and inaccurate. In contrast, SkillGPT utilizes\na LLM to perform its tasks in steps via summarization and vector similarity\nsearch, to balance speed with precision. The backbone LLM of SkillGPT is based\non Llama, free for academic use and thus useful for exploratory research and\nprototype development. Hence, our cost-free SkillGPT gives users the\nconvenience of conversational SES, efficiently and reliably.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Bo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_T/0/1/0/all/0/1\">Tijl De Bie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answering Questions by Meta-Reasoning over Multiple Chains of Thought. (arXiv:2304.13007v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.13007","description":"<p>Modern systems for multi-hop question answering (QA) typically break\nquestions into a sequence of reasoning steps, termed chain-of-thought (CoT),\nbefore arriving at a final answer. Often, multiple chains are sampled and\naggregated through a voting mechanism over the final answers, but the\nintermediate steps themselves are discarded. While such approaches improve\nperformance, they do not consider the relations between intermediate steps\nacross chains and do not provide a unified explanation for the predicted\nanswer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts\nlarge language models to meta-reason over multiple chains of thought, rather\nthan aggregating their answers. MCR examines different reasoning chains, mixes\ninformation between them and selects the most relevant facts in generating an\nexplanation and predicting the answer. MCR outperforms strong baselines on 7\nmulti-hop QA datasets. Moreover, our analysis reveals that MCR explanations\nexhibit high quality, enabling humans to verify its answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1\">Ori Yoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfson_T/0/1/0/all/0/1\">Tomer Wolfson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogin_B/0/1/0/all/0/1\">Ben Bogin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_U/0/1/0/all/0/1\">Uri Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutch_D/0/1/0/all/0/1\">Daniel Deutch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction. (arXiv:2304.14770v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14770","description":"<p>Universal Information Extraction (UIE) is an area of interest due to the\nchallenges posed by varying targets, heterogeneous structures, and\ndemand-specific schemas. However, previous works have only achieved limited\nsuccess by unifying a few tasks, such as Named Entity Recognition (NER) and\nRelation Extraction (RE), which fall short of being authentic UIE models\nparticularly when extracting other general schemas such as quadruples and\nquintuples. Additionally, these models used an implicit structural schema\ninstructor, which could lead to incorrect links between types, hindering the\nmodel's generalization and performance in low-resource scenarios. In this\npaper, we redefine the authentic UIE with a formal formulation that encompasses\nalmost all extraction schemas. To the best of our knowledge, we are the first\nto introduce UIE for any kind of schemas. In addition, we propose RexUIE, which\nis a Recursive Method with Explicit Schema Instructor for UIE. To avoid\ninterference between different types, we reset the position ids and attention\nmask matrices. RexUIE shows strong performance under both full-shot and\nfew-shot settings and achieves State-of-the-Art results on the tasks of\nextracting complex schemas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fubang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yangyang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization. (arXiv:2305.01951v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01951","description":"<p>Recent pre-trained language models (PLMs) achieve promising results in\nexisting abstractive summarization datasets. However, existing summarization\nbenchmarks overlap in time with the standard pre-training corpora and\nfinetuning datasets. Hence, the strong performance of PLMs may rely on the\nparametric knowledge that is memorized during pre-training and fine-tuning.\nMoreover, the knowledge memorized by PLMs may quickly become outdated, which\naffects the generalization performance of PLMs on future data. In this work, we\npropose TempoSum, a novel benchmark that contains data samples from 2010 to\n2022, to understand the temporal generalization ability of abstractive\nsummarization models. Through extensive human evaluation, we show that\nparametric knowledge stored in summarization models significantly affects the\nfaithfulness of the generated summaries on future data. Moreover, existing\nfaithfulness enhancement methods cannot reliably improve the faithfulness of\nsummarization models on future data. Finally, we discuss several\nrecommendations to the research community on how to evaluate and improve the\ntemporal generalization capability of text summarization models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheang_C/0/1/0/all/0/1\">Chi Seng Cheang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaocong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shudong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements. (arXiv:2305.03695v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03695","description":"<p>Despite the much discussed capabilities of today's language models, they are\nstill prone to silly and unexpected commonsense failures. We consider a\nretrospective verification approach that reflects on the correctness of LM\noutputs, and introduce Vera, a general-purpose model that estimates the\nplausibility of declarative statements based on commonsense knowledge. Trained\non ~7M commonsense statements created from 19 QA datasets and two large-scale\nknowledge bases, and with a combination of three training objectives, Vera is a\nversatile model that effectively separates correct from incorrect statements\nacross diverse commonsense domains. When applied to solving commonsense\nproblems in the verification format, Vera substantially outperforms existing\nmodels that can be repurposed for commonsense verification, and it further\nexhibits generalization capabilities to unseen tasks and provides\nwell-calibrated outputs. We find that Vera excels at filtering LM-generated\ncommonsense knowledge and is useful in detecting erroneous commonsense\nstatements generated by models like ChatGPT in real-world settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiacheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dianzhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks. (arXiv:2305.10160v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10160","description":"<p>Data contamination has become prevalent and challenging with the rise of\nmodels pretrained on large automatically-crawled corpora. For closed models,\nthe training data becomes a trade secret, and even for open models, it is not\ntrivial to detect contamination. Strategies such as leaderboards with hidden\nanswers, or using test data which is guaranteed to be unseen, are expensive and\nbecome fragile with time. Assuming that all relevant actors value clean test\ndata and will cooperate to mitigate data contamination, what can be done? We\npropose three strategies that can make a difference: (1) Test data made public\nshould be encrypted with a public key and licensed to disallow derivative\ndistribution; (2) demand training exclusion controls from closed API holders,\nand protect your test data by refusing to evaluate without them; (3) avoid data\nwhich appears with its solution on the internet, and release the web-page\ncontext of internet-derived data along with the data. These strategies are\npractical and can be effective in preventing data contamination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1\">Alon Jacovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modality Data Augmentation for End-to-End Sign Language Translation. (arXiv:2305.11096v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11096","description":"<p>End-to-end sign language translation (SLT) aims to convert sign language\nvideos into spoken language texts directly without intermediate\nrepresentations. It has been a challenging task due to the modality gap between\nsign videos and texts and the data scarcity of labeled data. To tackle these\nchallenges, we propose a novel Cross-modality Data Augmentation (XmDA)\nframework to transfer the powerful gloss-to-text translation capabilities to\nend-to-end sign language translation (i.e. video-to-text) by exploiting pseudo\ngloss-text pairs from the sign gloss translation model. Specifically, XmDA\nconsists of two key components, namely, cross-modality mix-up and\ncross-modality knowledge distillation. The former explicitly encourages the\nalignment between sign video features and gloss embeddings to bridge the\nmodality gap. The latter utilizes the generation knowledge from gloss-to-text\nteacher models to guide the spoken language text generation. Experimental\nresults on two widely used SLT datasets, i.e., PHOENIX-2014T and CSL-Daily,\ndemonstrate that the proposed XmDA framework significantly and consistently\noutperforms the baseline models. Extensive analyses confirm our claim that XmDA\nenhances spoken language text generation by reducing the representation\ndistance between videos and texts, as well as improving the processing of\nlow-frequency words and long sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jinhui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning. (arXiv:2305.11508v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11508","description":"<p>The patient-centered medical dialogue systems strive to offer diagnostic\ninterpretation services to users who are less knowledgeable about medical\nknowledge, through emphasizing the importance of providing responses specific\nto the patients. It is difficult for the large language models (LLMs) to\nguarantee the specificity of responses in spite of its promising performance\neven in some tasks in medical field. Inspired by in-context learning, we\npropose PlugMed, a Plug-and-Play Medical Dialogue System, for addressing this\nchallenge. PlugMed is equipped with two modules, the prompt generation (PG)\nmodule and the response ranking (RR) module, to enhances LLMs' dialogue\nstrategies for improving the specificity of the dialogue. The PG module is\ndesigned to stimulate the imitative ability of LLMs by providing them with real\ndialogues from similar patients as prompts. The RR module incorporates\nfine-tuned small model as response filter to enable the selection of\nappropriate responses generated by LLMs. Furthermore, we introduce a new\nevaluation method based on matching both user's intent and high-frequency\nmedical term to effectively assess the specificity of the responses. We conduct\nexperimental evaluations on three medical dialogue datasets, and the results,\nincluding both automatic and human evaluation, demonstrate the effectiveness of\nour approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_C/0/1/0/all/0/1\">Chengfeng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenping Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhenwei Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yongqiang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate. (arXiv:2305.11595v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11595","description":"<p>Large Language Models (LLMs) have shown impressive capabilities in various\napplications, but they still face various inconsistency issues. Existing works\nprimarily focus on the inconsistency issues within a single LLM, while we\ncomplementarily explore the inter-consistency among multiple LLMs for\ncollaboration. To examine whether LLMs can collaborate effectively to achieve a\nconsensus for a shared goal, we focus on commonsense reasoning, and introduce a\nformal debate framework (FORD) to conduct a three-stage debate among LLMs with\nreal-world scenarios alignment: fair debate, mismatched debate, and roundtable\ndebate. Through extensive experiments on various datasets, LLMs can effectively\ncollaborate to reach a consensus despite noticeable inter-inconsistencies, but\nimbalances in their abilities can lead to domination by superior LLMs.\nLeveraging a more advanced LLM like GPT-4 as an authoritative judge can boost\ncollaboration performance. Our work contributes to understanding the\ninter-consistency among LLMs and lays the foundation for developing future\ncollaboration methods. Codes and data are available at\nhttps://github.com/Waste-Wood/FORD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_K/0/1/0/all/0/1\">Kai Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews. (arXiv:2305.11828v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11828","description":"<p>Medical systematic reviews play a vital role in healthcare decision making\nand policy. However, their production is time-consuming, limiting the\navailability of high-quality and up-to-date evidence summaries. Recent\nadvancements in large language models (LLMs) offer the potential to\nautomatically generate literature reviews on demand, addressing this issue.\nHowever, LLMs sometimes generate inaccurate (and potentially misleading) texts\nby hallucination or omission. In healthcare, this can make LLMs unusable at\nbest and dangerous at worst. We conducted 16 interviews with international\nsystematic review experts to characterize the perceived utility and risks of\nLLMs in the specific context of medical evidence reviews. Experts indicated\nthat LLMs can assist in the writing process by drafting summaries, generating\ntemplates, distilling information, and crosschecking information. They also\nraised concerns regarding confidently composed but inaccurate LLM outputs and\nother potential downstream harms, including decreased accountability and\nproliferation of low-quality reviews. Informed by this qualitative analysis, we\nidentify criteria for rigorous evaluation of biomedical LLMs aligned with\ndomain expert views.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1\">Hye Sun Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_I/0/1/0/all/0/1\">Iain J. Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trikalinos_T/0/1/0/all/0/1\">Thomas A. Trikalinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Entropy Rate Constancy in Text. (arXiv:2305.12084v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12084","description":"<p>The uniform information density (UID) hypothesis states that humans tend to\ndistribute information roughly evenly across an utterance or discourse. Early\nevidence in support of the UID hypothesis came from Genzel &amp; Charniak (2002),\nwhich proposed an entropy rate constancy principle based on the probability of\nEnglish text under n-gram language models. We re-evaluate the claims of Genzel\n&amp; Charniak (2002) with neural language models, failing to find clear evidence\nin support of entropy rate constancy. We conduct a range of experiments across\ndatasets, model sizes, and languages and discuss implications for the uniform\ninformation density hypothesis and linguistic theories of efficient\ncommunication more broadly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1\">Vivek Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomlin_N/0/1/0/all/0/1\">Nicholas Tomlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Compose Representations of Different Encoder Layers towards Improving Compositional Generalization. (arXiv:2305.12169v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12169","description":"<p>Recent studies have shown that sequence-to-sequence (seq2seq) models struggle\nwith compositional generalization (CG), i.e., the ability to systematically\ngeneralize to unseen compositions of seen components. There is mounting\nevidence that one of the reasons hindering CG is the representation of the\nencoder uppermost layer is entangled, i.e., the syntactic and semantic\nrepresentations of sequences are entangled. However, we consider that the\npreviously identified representation entanglement problem is not comprehensive\nenough. Additionally, we hypothesize that the source keys and values\nrepresentations passing into different decoder layers are also entangled.\nStarting from this intuition, we propose \\textsc{CompoSition} (\\textbf{Compo}se\n\\textbf{S}yntactic and Semant\\textbf{i}c Representa\\textbf{tion}s), an\nextension to seq2seq models which learns to compose representations of\ndifferent encoder layers dynamically for different tasks, since recent studies\nreveal that the bottom layers of the Transformer encoder contain more syntactic\ninformation and the top ones contain more semantic information. Specifically,\nwe introduce a \\textit{composed layer} between the encoder and decoder to\ncompose different encoder layers' representations to generate specific keys and\nvalues passing into different decoder layers. \\textsc{CompoSition} achieves\ncompetitive results on two comprehensive and realistic benchmarks, which\nempirically demonstrates the effectiveness of our proposal. Codes are available\nat~\\url{https://github.com/thinkaboutzero/COMPOSITION}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuangtao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yafang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Biao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yidong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaodong Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with Auxiliary Refined Knowledge. (arXiv:2305.12212v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12212","description":"<p>Multimodal Named Entity Recognition (MNER) on social media aims to enhance\ntextual entity prediction by incorporating image-based clues. Existing studies\nmainly focus on maximizing the utilization of pertinent image information or\nincorporating external knowledge from explicit knowledge bases. However, these\nmethods either neglect the necessity of providing the model with external\nknowledge, or encounter issues of high redundancy in the retrieved knowledge.\nIn this paper, we present PGIM -- a two-stage framework that aims to leverage\nChatGPT as an implicit knowledge base and enable it to heuristically generate\nauxiliary knowledge for more efficient entity prediction. Specifically, PGIM\ncontains a Multimodal Similar Example Awareness module that selects suitable\nexamples from a small number of predefined artificial samples. These examples\nare then integrated into a formatted prompt template tailored to the MNER and\nguide ChatGPT to generate auxiliary refined knowledge. Finally, the acquired\nknowledge is integrated with the original text and fed into a downstream model\nfor further processing. Extensive experiments show that PGIM outperforms\nstate-of-the-art methods on two classic MNER datasets and exhibits a stronger\nrobustness and generalization capability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Han Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhuo Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Di Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenkun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1\">Gang Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Simplification of Medical Texts. (arXiv:2305.12532v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12532","description":"<p>Automated text simplification aims to produce simple versions of complex\ntexts. This task is especially useful in the medical domain, where the latest\nmedical findings are typically communicated via complex and technical articles.\nThis creates barriers for laypeople seeking access to up-to-date medical\nfindings, consequently impeding progress on health literacy. Most existing work\non medical text simplification has focused on monolingual settings, with the\nresult that such evidence would be available only in just one language (most\noften, English). This work addresses this limitation via multilingual\nsimplification, i.e., directly simplifying complex texts into simplified texts\nin multiple languages. We introduce MultiCochrane, the first sentence-aligned\nmultilingual text simplification dataset for the medical domain in four\nlanguages: English, Spanish, French, and Farsi. We evaluate fine-tuned and\nzero-shot models across these languages, with extensive human assessments and\nanalyses. Although models can now generate viable simplified texts, we identify\noutstanding challenges that this dataset might be used to address.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joseph_S/0/1/0/all/0/1\">Sebastian Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazanas_K/0/1/0/all/0/1\">Kathryn Kazanas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reina_K/0/1/0/all/0/1\">Keziah Reina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanathan_V/0/1/0/all/0/1\">Vishnesh J. Ramanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MADNet: Maximizing Addressee Deduction Expectation for Multi-Party Conversation Generation. (arXiv:2305.12733v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12733","description":"<p>Modeling multi-party conversations (MPCs) with graph neural networks has been\nproven effective at capturing complicated and graphical information flows.\nHowever, existing methods rely heavily on the necessary addressee labels and\ncan only be applied to an ideal setting where each utterance must be tagged\nwith an addressee label. To study the scarcity of addressee labels which is a\ncommon issue in MPCs, we propose MADNet that maximizes addressee deduction\nexpectation in heterogeneous graph neural networks for MPC generation. Given an\nMPC with a few addressee labels missing, existing methods fail to build a\nconsecutively connected conversation graph, but only a few separate\nconversation fragments instead. To ensure message passing between these\nconversation fragments, four additional types of latent edges are designed to\ncomplete a fully-connected graph. Besides, to optimize the edge-type-dependent\nmessage passing for those utterances without addressee labels, an\nExpectation-Maximization-based method that iteratively generates silver\naddressee labels (E step), and optimizes the quality of generated responses (M\nstep), is designed. Experimental results on two Ubuntu IRC channel benchmarks\nshow that MADNet outperforms various baseline models on the task of MPC\ngeneration, especially under the more common and challenging setting where part\nof addressee labels are missing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jia-Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chao-Hong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Caiyuan Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale. (arXiv:2305.14124v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14124","description":"<p>Multilingual machine translation (MMT), trained on a mixture of parallel and\nmonolingual data, is key for improving translation in low-resource language\npairs. However, the literature offers conflicting results on the performance of\ndifferent methods of including monolingual data. To resolve this, we examine\nhow denoising autoencoding (DAE) and backtranslation (BT) impact MMT under\ndifferent data conditions and model scales. Unlike prior studies, we use a\nrealistic dataset of 100 translation directions and consider many domain\ncombinations of monolingual and test data. We find that monolingual data\ngenerally helps MMT, but models are surprisingly brittle to domain mismatches,\nespecially at smaller model scales. BT is beneficial when the parallel,\nmonolingual, and test data sources are similar but can be detrimental\notherwise, while DAE is less effective than previously reported. Next, we\nanalyze the impact of scale (from 90M to 1.6B parameters) and find it is\nimportant for both methods, particularly DAE. As scale increases, DAE\ntransitions from underperforming the parallel-only baseline at 90M to\nconverging with BT performance at 1.6B, and even surpassing it in low-resource.\nThese results offer new insights into how to best use monolingual data in MMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baziotis_C/0/1/0/all/0/1\">Christos Baziotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1\">Barry Haddow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question Answering as Programming for Solving Time-Sensitive Questions. (arXiv:2305.14221v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14221","description":"<p>Question answering plays a pivotal role in human daily life because it\ninvolves our acquisition of knowledge about the world. However, due to the\ndynamic and ever-changing nature of real-world facts, the answer can be\ncompletely different when the time constraint in the question changes.\nRecently, Large Language Models (LLMs) have shown remarkable intelligence in\nquestion answering, while our experiments reveal that the aforementioned\nproblems still pose a significant challenge to existing LLMs. This can be\nattributed to the LLMs' inability to perform rigorous reasoning based on\nsurface-level text semantics. To overcome this limitation, rather than\nrequiring LLMs to directly answer the question, we propose a novel approach\nwhere we reframe the $\\textbf{Q}$uestion $\\textbf{A}$nswering task\n$\\textbf{a}$s $\\textbf{P}$rogramming ($\\textbf{QAaP}$). Concretely, by\nleveraging modern LLMs' superior capability in understanding both natural\nlanguage and programming language, we endeavor to harness LLMs to represent\ndiversely expressed text as well-structured code and select the best matching\nanswer from multiple candidates through programming. We evaluate our QAaP\nframework on several time-sensitive question answering datasets and achieve\ndecent improvement, up to $14.5$% over strong baselines. Our codes and data are\navailable at https://github.com/TianHongZXY/qaap\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Difference-Masking: Choosing What to Mask in Continued Pretraining. (arXiv:2305.14577v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.14577","description":"<p>The self-supervised objective of masking-and-predicting has led to promising\nperformance gains on a variety of downstream tasks. However, while most\napproaches randomly mask tokens, there is strong intuition that deciding what\nto mask can substantially improve learning outcomes. We investigate this in\ncontinued pretraining setting in which pretrained models continue to pretrain\non domain-specific data before performing some downstream task. We introduce\nDifference-Masking, a masking strategy that automatically chooses what to mask\nduring continued pretraining by considering what makes a task domain different\nfrom the pretraining domain. Empirically, we find that Difference-Masking\noutperforms baselines on continued pretraining settings across four diverse\nlanguage-only and multimodal video tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilf_A/0/1/0/all/0/1\">Alex Wilf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akter_S/0/1/0/all/0/1\">Syeda Nahida Akter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_L/0/1/0/all/0/1\">Leena Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_S/0/1/0/all/0/1\">Sheryl Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mengrou Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey. (arXiv:2305.18703v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18703","description":"<p>Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP), providing a highly useful, task-agnostic foundation\nfor a wide range of applications. However, directly applying LLMs to solve\nsophisticated problems in specific domains meets many hurdles, caused by the\nheterogeneity of domain data, the sophistication of domain knowledge, the\nuniqueness of domain objectives, and the diversity of the constraints (e.g.,\nvarious social norms, cultural conformity, religious beliefs, and ethical\nstandards in the domain applications). Domain specification techniques are key\nto make large language models disruptive in many applications. Specifically, to\nsolve these hurdles, there has been a notable increase in research and\npractices conducted in recent years on the domain specialization of LLMs. This\nemerging field of study, with its substantial potential for impact,\nnecessitates a comprehensive and systematic review to better summarize and\nguide ongoing work in this area. In this article, we present a comprehensive\nsurvey on domain specification techniques for large language models, an\nemerging direction critical for large language model applications. First, we\npropose a systematic taxonomy that categorizes the LLM domain-specialization\ntechniques based on the accessibility to LLMs and summarizes the framework for\nall the subcategories as well as their relations and differences to each other.\nSecond, we present an extensive taxonomy of critical application domains that\ncan benefit dramatically from specialized LLMs, discussing their practical\nsignificance and open challenges. Last, we offer our insights into the current\nresearch status and future trends in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1\">Chen Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xujiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiaying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chengyuan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Can Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_T/0/1/0/all/0/1\">Tanmoy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianjiao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panalkar_A/0/1/0/all/0/1\">Amit Panalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Chris White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts. (arXiv:2306.04528v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.04528","description":"<p>The increasing reliance on Large Language Models (LLMs) across academia and\nindustry necessitates a comprehensive understanding of their robustness to\nprompts. In response to this vital need, we introduce PromptBench, a robustness\nbenchmark designed to measure LLMs' resilience to adversarial prompts. This\nstudy uses a plethora of adversarial textual attacks targeting prompts across\nmultiple levels: character, word, sentence, and semantic. The adversarial\nprompts, crafted to mimic plausible user errors like typos or synonyms, aim to\nevaluate how slight deviations can affect LLM outcomes while maintaining\nsemantic integrity. These prompts are then employed in diverse tasks, such as\nsentiment analysis, natural language inference, reading comprehension, machine\ntranslation, and math problem-solving. Our study generates 4788 adversarial\nprompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings\ndemonstrate that contemporary LLMs are not robust to adversarial prompts.\nFurthermore, we present comprehensive analysis to understand the mystery behind\nprompt robustness and its transferability. We then offer insightful robustness\nanalysis and pragmatic recommendations for prompt composition, beneficial to\nboth researchers and everyday users. Code is available at:\nhttps://github.com/microsoft/promptbench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kaijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiaheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zichen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias. (arXiv:2306.15895v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.15895","description":"<p>Large language models (LLMs) have been recently leveraged as training data\ngenerators for various natural language processing (NLP) tasks. While previous\nresearch has explored different approaches to training models using generated\ndata, they generally rely on simple class-conditional prompts, which may limit\nthe diversity of the generated data and inherit systematic biases of LLM. Thus,\nwe investigate training data generation with diversely attributed prompts\n(e.g., specifying attributes like length and style), which have the potential\nto yield diverse and attributed generated data. Our investigation focuses on\ndatasets with high cardinality and diverse domains, wherein we demonstrate that\nattributed prompts outperform simple class-conditional prompts in terms of the\nresulting model's performance. Additionally, we present a comprehensive\nempirical study on data generation encompassing vital aspects like bias,\ndiversity, and efficiency, and highlight three key observations: firstly,\nsynthetic datasets generated by simple prompts exhibit significant biases, such\nas regional bias; secondly, attribute diversity plays a pivotal role in\nenhancing model performance; lastly, attributed prompts achieve the performance\nof simple class-conditional prompts while utilizing only 5\\% of the querying\ncost of ChatGPT associated with the latter. The data and code are available on\n\\url{https://github.com/yueyu1030/AttrPrompt}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yuchen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jieyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1\">Alexander Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition. (arXiv:2307.07280v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.07280","description":"<p>While Automatic Speech Recognition (ASR) models have shown significant\nadvances with the introduction of unsupervised or self-supervised training\ntechniques, these improvements are still only limited to a subsection of\nlanguages and speakers. Transfer learning enables the adaptation of large-scale\nmultilingual models to not only low-resource languages but also to more\nspecific speaker groups. However, fine-tuning on data from new domains is\nusually accompanied by a decrease in performance on the original domain.\nTherefore, in our experiments, we examine how well the performance of\nlarge-scale ASR models can be approximated for smaller domains, with our own\ndataset of German Senior Voice Commands (SVC-de), and how much of the general\nspeech recognition performance can be preserved by selectively freezing parts\nof the model during training. To further increase the robustness of the ASR\nmodel to vocabulary and speakers outside of the fine-tuned domain, we apply\nExperience Replay for continual learning. By adding only a fraction of data\nfrom the original domain, we are able to reach Word-Error-Rates (WERs) below\n5\\% on the new domain, while stabilizing performance for general speech\nrecognition at acceptable WERs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosin_T/0/1/0/all/0/1\">Theresa Pekarek Rosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge. (arXiv:2307.08813v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.08813","description":"<p>Understanding protein interactions and pathway knowledge is crucial for\nunraveling the complexities of living systems and investigating the underlying\nmechanisms of biological functions and complex diseases. While existing\ndatabases provide curated biological data from literature and other sources,\nthey are often incomplete and their maintenance is labor-intensive,\nnecessitating alternative approaches. In this study, we propose to harness the\ncapabilities of large language models to address these issues by automatically\nextracting such knowledge from the relevant scientific literature. Toward this\ngoal, in this work, we investigate the effectiveness of different large\nlanguage models in tasks that involve recognizing protein interactions,\nidentifying genes associated with pathways affected by low-dose radiation, and\ngene regulatory relations. We thoroughly evaluate the performance of various\nmodels, highlight the significant findings, and discuss both the future\nopportunities and the remaining challenges associated with this approach. The\ncode and data are available at: https://github.com/boxorange/BioIE-LLM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1\">Gilchan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_B/0/1/0/all/0/1\">Byung-Jun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xihaier Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Marrero_V/0/1/0/all/0/1\">Vanessa L&#xf3;pez-Marrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Shinjae Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Shantenu Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI. (arXiv:2309.12444v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.12444","description":"<p>Generative Artificial Intelligence is set to revolutionize healthcare\ndelivery by transforming traditional patient care into a more personalized,\nefficient, and proactive process. Chatbots, serving as interactive\nconversational models, will probably drive this patient-centered transformation\nin healthcare. Through the provision of various services, including diagnosis,\npersonalized lifestyle recommendations, and mental health support, the\nobjective is to substantially augment patient health outcomes, all the while\nmitigating the workload burden on healthcare providers. The life-critical\nnature of healthcare applications necessitates establishing a unified and\ncomprehensive set of evaluation metrics for conversational models. Existing\nevaluation metrics proposed for various generic large language models (LLMs)\ndemonstrate a lack of comprehension regarding medical and health concepts and\ntheir significance in promoting patients' well-being. Moreover, these metrics\nneglect pivotal user-centered aspects, including trust-building, ethics,\npersonalization, empathy, user comprehension, and emotional support. The\npurpose of this paper is to explore state-of-the-art LLM-based evaluation\nmetrics that are specifically applicable to the assessment of interactive\nconversational models in healthcare. Subsequently, we present an comprehensive\nset of evaluation metrics designed to thoroughly assess the performance of\nhealthcare chatbots from an end-user perspective. These metrics encompass an\nevaluation of language processing abilities, impact on real-world clinical\ntasks, and effectiveness in user-interactive conversations. Finally, we engage\nin a discussion concerning the challenges associated with defining and\nimplementing these metrics, with particular emphasis on confounding factors\nsuch as the target audience, evaluation methods, and prompt techniques involved\nin the evaluation process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abbasian_M/0/1/0/all/0/1\">Mahyar Abbasian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khatibi_E/0/1/0/all/0/1\">Elahe Khatibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azimi_I/0/1/0/all/0/1\">Iman Azimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1\">David Oniani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abad_Z/0/1/0/all/0/1\">Zahra Shakeri Hossein Abad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thieme_A/0/1/0/all/0/1\">Alexander Thieme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriram_R/0/1/0/all/0/1\">Ram Sriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhongqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanshan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bryant Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1\">Olivier Gevaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li-Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Ramesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1\">Amir M. Rahmani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't throw away your value model! Making PPO even better via Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.15028","description":"<p>Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may\nseem unnecessary when generating natural language text based on\nstate-of-the-art reinforcement learning such as Proximal Policy Optimization\n(PPO). In this paper, we demonstrate that it is possible to get extra mileage\nout of PPO by integrating MCTS on top. The key idea is not to throw out the\nvalue network, a byproduct of PPO training for evaluating partial output\nsequences, when decoding text out of the policy network. More concretely, we\npresent a novel value-guided decoding algorithm called PPO-MCTS, which can\nintegrate the value network from PPO to work closely with the policy network\nduring inference-time generation. Compared to prior approaches based on MCTS\nfor controlled text generation, the key strength of our approach is to reduce\nthe fundamental mismatch of the scoring mechanisms of the partial outputs\nbetween training and test. Evaluation on four text generation tasks demonstrate\nthat PPO-MCTS greatly improves the preferability of generated text compared to\nthe standard practice of using only the PPO policy. Our results demonstrate the\npromise of search algorithms even on top of the aligned language models from\nPPO, and the under-explored benefit of the value network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiacheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Andrew Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations. (arXiv:2310.00313v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.00313","description":"<p>Large language models (LLMs) exhibit remarkable performance improvement\nthrough in-context learning (ICL) by leveraging task-specific examples in the\ninput. However, the mechanisms behind this improvement remain elusive. In this\nwork, we investigate embeddings and attention representations in Llama-2 70B\nand Vicuna 13B. Specifically, we study how embeddings and attention change\nafter in-context-learning, and how these changes mediate improvement in\nbehavior. We employ neuroscience-inspired techniques, such as representational\nsimilarity analysis (RSA), and propose novel methods for parameterized probing\nand attention ratio analysis (ARA, measuring the ratio of attention to relevant\nvs. irrelevant information). We designed three tasks with a priori\nrelationships among their conditions: reading comprehension, linear regression,\nand adversarial prompt injection. We formed hypotheses about expected\nsimilarities in task representations to investigate latent changes in\nembeddings and attention. Our analyses revealed a meaningful correlation\nbetween changes in both embeddings and attention representations with\nimprovements in behavioral performance after ICL. This empirical framework\nempowers a nuanced understanding of how latent representations affect LLM\nbehavior with and without ICL, offering valuable tools and insights for future\nresearch and practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yousefi_S/0/1/0/all/0/1\">Safoora Yousefi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Betthauser_L/0/1/0/all/0/1\">Leo Betthauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanbeig_H/0/1/0/all/0/1\">Hosein Hasanbeig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saran_A/0/1/0/all/0/1\">Akanksha Saran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milliere_R/0/1/0/all/0/1\">Rapha&#xeb;l Milli&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1\">Ida Momennejad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crystal: Introspective Reasoners Reinforced with Self-Feedback. (arXiv:2310.04921v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2310.04921","description":"<p>Extensive work has shown that the performance and interpretability of\ncommonsense reasoning can be improved via knowledge-augmented reasoning\nmethods, where the knowledge that underpins the reasoning process is explicitly\nverbalized and utilized. However, existing implementations, including\n\"chain-of-thought\" and its variants, fall short in capturing the introspective\nnature of knowledge required in commonsense reasoning, and in accounting for\nthe mutual adaptation between the generation and utilization of knowledge. We\npropose a novel method to develop an introspective commonsense reasoner,\nCrystal. To tackle commonsense problems, it first introspects for knowledge\nstatements related to the given question, and subsequently makes an informed\nprediction that is grounded in the previously introspected knowledge. The\nknowledge introspection and knowledge-grounded reasoning modes of the model are\ntuned via reinforcement learning to mutually adapt, where the reward derives\nfrom the feedback given by the model itself. Experiments show that Crystal\nsignificantly outperforms both the standard supervised finetuning and\nchain-of-thought distilled methods, and enhances the transparency of the\ncommonsense reasoning process. Our work ultimately validates the feasibility\nand potential of reinforcing a neural model with self-feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiacheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Storytelling with Question-Answer Plans. (arXiv:2310.05295v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05295","description":"<p>Visual storytelling aims to generate compelling narratives from image\nsequences. Existing models often focus on enhancing the representation of the\nimage sequence, e.g., with external knowledge sources or advanced graph\nstructures. Despite recent progress, the stories are often repetitive,\nillogical, and lacking in detail. To mitigate these issues, we present a novel\nframework which integrates visual representations with pretrained language\nmodels and planning. Our model translates the image sequence into a visual\nprefix, a sequence of continuous embeddings which language models can\ninterpret. It also leverages a sequence of question-answer pairs as a blueprint\nplan for selecting salient visual concepts and determining how they should be\nassembled into a narrative. Automatic and human evaluation on the VIST\nbenchmark (Huang et al., 2016) demonstrates that blueprint-based models\ngenerate stories that are more coherent, interesting, and natural compared to\ncompetitive baselines and state-of-the-art systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Speculative Decoding. (arXiv:2310.07177v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2310.07177","description":"<p>Speculative decoding is a pivotal technique to accelerate the inference of\nlarge language models (LLMs) by employing a smaller draft model to predict the\ntarget model's outputs. However, its efficacy can be limited due to the low\npredictive accuracy of the draft model, particularly when faced with diverse\ntext inputs and a significant capability gap between the draft and target\nmodels. We introduce online speculative decoding (OSD) to address this\nchallenge. The main idea is to continually update (multiple) draft model(s) on\nobserved user query data using the abundant excess computational power in an\nLLM serving cluster. Given that LLM inference is memory-bounded, the surplus\ncomputational power in a typical LLM serving cluster can be repurposed for\nonline retraining of draft models, thereby making the training cost-neutral.\nSince the query distribution of an LLM service is relatively simple, retraining\non query distribution enables the draft model to more accurately predict the\ntarget model's outputs, particularly on data originating from query\ndistributions. As the draft model evolves online, it aligns with the query\ndistribution in real time, mitigating distribution shifts. We develop a\nprototype of online speculative decoding based on online knowledge distillation\nand evaluate it using both synthetic and real query data on several popular\nLLMs. The results show a substantial increase in the token acceptance rate by\n0.1 to 0.65, which translates into 1.22x to 3.06x latency reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoxuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lanxiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailis_P/0/1/0/all/0/1\">Peter Bailis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhijie Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_A/0/1/0/all/0/1\">Alvin Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction. (arXiv:2310.07487v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.07487","description":"<p>Phonological reconstruction is one of the central problems in historical\nlinguistics where a proto-word of an ancestral language is determined from the\nobserved cognate words of daughter languages. Computational approaches to\nhistorical linguistics attempt to automate the task by learning models on\navailable linguistic data. Several ideas and techniques drawn from\ncomputational biology have been successfully applied in the area of\ncomputational historical linguistics. Following these lines, we adapt MSA\nTransformer, a protein language model, to the problem of automated phonological\nreconstruction. MSA Transformer trains on multiple sequence alignments as input\nand is, thus, apt for application on aligned cognate words. We, hence, name our\nmodel as Cognate Transformer. We also apply the model on another associated\ntask, namely, cognate reflex prediction, where a reflex word in a daughter\nlanguage is predicted based on cognate words from other daughter languages. We\nshow that our model outperforms the existing models on both tasks, especially\nwhen it is pre-trained on masked word prediction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akavarapu_V/0/1/0/all/0/1\">V.S.D.S.Mahesh Akavarapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Arnab Bhattacharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. (arXiv:2310.07521v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.07521","description":"<p>This survey addresses the crucial issue of factuality in Large Language\nModels (LLMs). As LLMs find applications across diverse domains, the\nreliability and accuracy of their outputs become vital. We define the\nFactuality Issue as the probability of LLMs to produce content inconsistent\nwith established facts. We first delve into the implications of these\ninaccuracies, highlighting the potential consequences and challenges posed by\nfactual errors in LLM outputs. Subsequently, we analyze the mechanisms through\nwhich LLMs store and process facts, seeking the primary causes of factual\nerrors. Our discussion then transitions to methodologies for evaluating LLM\nfactuality, emphasizing key metrics, benchmarks, and studies. We further\nexplore strategies for enhancing LLM factuality, including approaches tailored\nfor specific domains. We focus two primary LLM configurations standalone LLMs\nand Retrieval-Augmented LLMs that utilizes external data, we detail their\nunique challenges and potential enhancements. Our survey offers a structured\nguide for researchers aiming to fortify the factual reliability of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cunxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yuanhao Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiayang_C/0/1/0/all/0/1\">Cheng Jiayang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wenyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zehan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Expressive Power of Transformers with Chain of Thought. (arXiv:2310.07923v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2310.07923","description":"<p>Recent theoretical work has identified surprisingly simple reasoning\nproblems, such as checking if two nodes in a graph are connected or simulating\nfinite-state machines, that are provably unsolvable by standard transformers\nthat answer immediately after reading their input. However, in practice,\ntransformers' reasoning can be improved by allowing them to use a \"chain of\nthought\" or \"scratchpad\", i.e., generate and condition on a sequence of\nintermediate tokens before answering. Motivated by this, we ask: Does such\nintermediate generation fundamentally extend the computational power of a\ndecoder-only transformer? We show that the answer is yes, but the amount of\nincrease depends crucially on the amount of intermediate generation. For\ninstance, we find that transformer decoders with a logarithmic number of\ndecoding steps (w.r.t. the input length) push the limits of standard\ntransformers only slightly, while a linear number of decoding steps adds a\nclear new ability (under standard complexity conjectures): recognizing all\nregular languages. Our results also imply that linear steps keep transformer\ndecoders within context-sensitive languages, and polynomial steps make them\nrecognize exactly the class of polynomial-time solvable problems -- the first\nexact characterization of a type of transformers in terms of standard\ncomplexity classes. Together, our results provide a nuanced framework for\nunderstanding how the length of a transformer's chain of thought or scratchpad\nimpacts its reasoning power.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach. (arXiv:2310.08172v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.08172","description":"<p>Large Language Models (LLMs) have not only exhibited exceptional performance\nacross various tasks, but also demonstrated sparks of intelligence. Recent\nstudies have focused on assessing their capabilities on human exams and\nrevealed their impressive competence in different domains. However, cognitive\nresearch on the overall knowledge structure of LLMs is still lacking. In this\npaper, based on educational diagnostic assessment method, we conduct an\nevaluation using MoocRadar, a meticulously annotated human test dataset based\non Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain\ninsights of their cognitive capabilities. This research emphasizes the\nsignificance of investigating LLMs' knowledge and understanding the disparate\ncognitive patterns of LLMs. By shedding light on models' knowledge, researchers\ncan advance development and utilization of LLMs in a more informed and\neffective manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Developing a Natural Language Understanding Model to Characterize Cable News Bias. (arXiv:2310.09166v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.09166","description":"<p>Media bias has been extensively studied by both social and computational\nsciences. However, current work still has a large reliance on human input and\nsubjective assessment to label biases. This is especially true for cable news\nresearch. To address these issues, we develop an unsupervised machine learning\nmethod to characterize the bias of cable news programs without any human input.\nThis method relies on the analysis of what topics are mentioned through Named\nEntity Recognition and how those topics are discussed through Stance Analysis\nin order to cluster programs with similar biases together. Applying our method\nto 2020 cable news transcripts, we find that program clusters are consistent\nover time and roughly correspond to the cable news network of the program. This\nmethod reveals the potential for future tools to objectively assess media bias\nand characterize unfamiliar media environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benson_S/0/1/0/all/0/1\">Seth P. Benson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruickshank_I/0/1/0/all/0/1\">Iain J. Cruickshank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of Bangla Social Media Posts. (arXiv:2310.09238v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.09238","description":"<p>Bangla is the 7th most widely spoken language globally, with a staggering 234\nmillion native speakers primarily hailing from India and Bangladesh. This\nmorphologically rich language boasts a rich literary tradition, encompassing\ndiverse dialects and language-specific challenges. Despite its linguistic\nrichness and history, Bangla remains categorized as a low-resource language\nwithin the natural language processing (NLP) and speech community. This paper\npresents our submission to Task 2 (Sentiment Analysis of Bangla Social Media\nPosts) of the BLP Workshop. We experiment with various Transformer-based\narchitectures to solve this task. Our quantitative results show that transfer\nlearning really helps in better learning of the models in this low-resource\nlanguage scenario. This becomes evident when we further finetune a model which\nhas already been finetuned on twitter data for sentiment analysis task and that\nfinetuned model performs the best among all other models. We also perform a\ndetailed error analysis where we find some instances where ground truth labels\nneed to be relooked at. We obtain a micro-F1 of 67.02\\% on the test set and our\nperformance in this shared task is ranked at 21 in the leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Saumajit Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanda_A/0/1/0/all/0/1\">Albert Nanda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Expression Tree Decoding Strategy for Mathematical Equation Generation. (arXiv:2310.09619v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.09619","description":"<p>Generating mathematical equations from natural language requires an accurate\nunderstanding of the relations among math expressions. Existing approaches can\nbe broadly categorized into token-level and expression-level generation. The\nformer treats equations as a mathematical language, sequentially generating\nmath tokens. Expression-level methods generate each expression one by one.\nHowever, each expression represents a solving step, and there naturally exist\nparallel or dependent relations between these steps, which are ignored by\ncurrent sequential methods. Therefore, we integrate tree structure into the\nexpression-level generation and advocate an expression tree decoding strategy.\nTo generate a tree with expression as its node, we employ a layer-wise parallel\ndecoding strategy: we decode multiple independent expressions (leaf nodes) in\nparallel at each layer and repeat parallel decoding layer by layer to\nsequentially generate these parent node expressions that depend on others.\nBesides, a bipartite matching algorithm is adopted to align multiple\npredictions with annotations for each layer. Experiments show our method\noutperforms other baselines, especially for these equations with complex\nstructures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nong_Q/0/1/0/all/0/1\">Qingpeng Nong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zeqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanna Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting. (arXiv:2310.09716v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2310.09716","description":"<p>Query rewriting plays a vital role in enhancing conversational search by\ntransforming context-dependent user queries into standalone forms. Existing\napproaches primarily leverage human-rewritten queries as labels to train query\nrewriting models. However, human rewrites may lack sufficient information for\noptimal retrieval performance. To overcome this limitation, we propose\nutilizing large language models (LLMs) as query rewriters, enabling the\ngeneration of informative query rewrites through well-designed instructions. We\ndefine four essential properties for well-formed rewrites and incorporate all\nof them into the instruction. In addition, we introduce the role of rewrite\neditors for LLMs when initial query rewrites are available, forming a\n\"rewrite-then-edit\" process. Furthermore, we propose distilling the rewriting\ncapabilities of LLMs into smaller models to reduce rewriting latency. Our\nexperimental evaluation on the QReCC dataset demonstrates that informative\nquery rewrites can yield substantially improved retrieval performance compared\nto human rewrites, especially with sparse retrievers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fanghua Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shenghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.10378","description":"<p>Multilingual large-scale Pretrained Language Models (PLMs) have been shown to\nstore considerable amounts of factual knowledge, but large variations are\nobserved across languages. With the ultimate goal of ensuring that users with\ndifferent language backgrounds obtain consistent feedback from the same model,\nwe study the cross-lingual consistency (CLC) of factual knowledge in various\nmultilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC)\nmetric to evaluate knowledge consistency across languages independently from\naccuracy. Using this metric, we conduct an in-depth analysis of the determining\nfactors for CLC, both at model level and at language-pair level. Among other\nresults, we find that increasing model size leads to higher factual probing\naccuracy in most languages, but does not improve cross-lingual consistency.\nFinally, we conduct a case study on CLC when new factual associations are\ninserted in the PLMs via model editing. Results on a small sample of facts\ninserted in English reveal a clear pattern whereby the new piece of knowledge\ntransfers only to languages with which English has a high RankC score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jirui Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1\">Raquel Fern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models. (arXiv:2310.10449v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.10449","description":"<p>Text summarization is a critical Natural Language Processing (NLP) task with\napplications ranging from information retrieval to content generation.\nLeveraging Large Language Models (LLMs) has shown remarkable promise in\nenhancing summarization techniques. This paper embarks on an exploration of\ntext summarization with a diverse set of LLMs, including MPT-7b-instruct,\nfalcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment\nwas performed with different hyperparameters and evaluated the generated\nsummaries using widely accepted metrics such as the Bilingual Evaluation\nUnderstudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation\n(ROUGE) Score, and Bidirectional Encoder Representations from Transformers\n(BERT) Score. According to the experiment, text-davinci-003 outperformed the\nothers. This investigation involved two distinct datasets: CNN Daily Mail and\nXSum. Its primary objective was to provide a comprehensive understanding of the\nperformance of Large Language Models (LLMs) when applied to different datasets.\nThe assessment of these models' effectiveness contributes valuable insights to\nresearchers and practitioners within the NLP domain. This work serves as a\nresource for those interested in harnessing the potential of LLMs for text\nsummarization and lays the foundation for the development of advanced\nGenerative AI applications aimed at addressing a wide spectrum of business\nchallenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basyal_L/0/1/0/all/0/1\">Lochan Basyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghvi_M/0/1/0/all/0/1\">Mihir Sanghvi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking. (arXiv:2310.10520v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.10520","description":"<p>Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring\nand annotating task-oriented dialogues, which can be time consuming and costly.\nHowever, DST extends beyond simple slot-filling and requires effective updating\nstrategies for tracking dialogue state as conversations progress. In this\npaper, we propose ParsingDST, a new In-Context Learning (ICL) method, to\nintroduce additional intricate updating strategies in zero-shot DST. Our\napproach reformulates the DST task by leveraging powerful Large Language Models\n(LLMs) and translating the original dialogue text to JSON through semantic\nparsing as an intermediate state. We also design a novel framework that\nincludes more modules to ensure the effectiveness of updating strategies in the\ntext-to-JSON process. Experimental results demonstrate that our approach\noutperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant\nimprovements in Joint Goal Accuracy (JGA) and slot accuracy compared to\nexisting ICL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guanting Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiran Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing the Power of LLMs: Evaluating Human-AI Text Co-Creation through the Lens of News Headline Generation. (arXiv:2310.10706v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.10706","description":"<p>To explore how humans can best leverage LLMs for writing and how interacting\nwith these models affects feelings of ownership and trust in the writing\nprocess, we compared common human-AI interaction types (e.g., guiding system,\nselecting from system outputs, post-editing outputs) in the context of\nLLM-assisted news headline generation. While LLMs alone can generate\nsatisfactory news headlines, on average, human control is needed to fix\nundesirable model outputs. Of the interaction methods, guiding and selecting\nmodel output added the most benefit with the lowest cost (in time and effort).\nFurther, AI assistance did not harm participants' perception of control\ncompared to freeform editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zijian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_Renner_A/0/1/0/all/0/1\">Alison Smith-Renner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetreault_J/0/1/0/all/0/1\">Joel R. Tetreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaimes_A/0/1/0/all/0/1\">Alejandro Jaimes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-10-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}