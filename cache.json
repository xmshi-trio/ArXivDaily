{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-12T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Accessible Instruction-Following Agent. (arXiv:2305.06358v1 [cs.AI])","link":"http://arxiv.org/abs/2305.06358","description":"<p>Humans can collaborate and complete tasks based on visual signals and\ninstruction from the environment. Training such a robot is difficult especially\ndue to the understanding of the instruction and the complicated environment.\nPrevious instruction-following agents are biased to English-centric corpus,\nmaking it unrealizable to be applied to users that use multiple languages or\neven low-resource languages. Nevertheless, the instruction-following agents are\npre-trained in a mode that assumes the user can observe the environment, which\nlimits its accessibility. In this work, we're trying to generalize the success\nof instruction-following agents to non-English languages with little corpus\nresources, and improve its intractability and accessibility. We introduce UVLN\n(Universal Vision-Language Navigation), a novel machine-translation\ninstructional augmented framework for cross-lingual vision-language navigation,\nwith a novel composition of state-of-the-art large language model (GPT3) with\nthe image caption model (BLIP). We first collect a multilanguage\nvision-language navigation dataset via machine translation. Then we extend the\nstandard VLN training objectives to a multilingual setting via a cross-lingual\nlanguage encoder. The alignment between different languages is captured through\na shared vision and action context via a cross-modal transformer, which encodes\nthe inputs of language instruction, visual observation, and action decision\nsequences. To improve the intractability, we connect our agent with the large\nlanguage model that informs the situation and current state to the user and\nalso explains the action decisions. Experiments over Room Across Room Dataset\nprove the effectiveness of our approach. And the qualitative results show the\npromising intractability and accessibility of our instruction-following agent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kairui Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits Siamese-BLOOM. (arXiv:2305.06404v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06404","description":"<p>Text embeddings are useful features for several NLP applications, such as\nsentence similarity, text clustering, and semantic search. In this paper, we\npresent a Low-rank Adaptation with a Contrastive objective on top of 8-bit\nSiamese-BLOOM, a multilingual large language model optimized to produce\nsemantically meaningful word embeddings. The innovation is threefold. First, we\ncast BLOOM weights to 8-bit values. Second, we fine-tune BLOOM with a scalable\nadapter (LoRA) and 8-bit Adam optimizer for sentence similarity classification.\nThird, we apply a Siamese architecture on BLOOM model with a contrastive\nobjective to ease the multi-lingual labeled data scarcity. The experiment\nresults show the quality of learned embeddings from LACoS-BLOOM is proportional\nto the number of model parameters and the amount of unlabeled training data.\nWith the parameter efficient fine-tuning design, we are able to run BLOOM 7.1\nbillion parameters end-to-end on a single GPU machine with 32GB memory.\nCompared to previous solution Sentence-BERT, we achieve significant improvement\non both English and multi-lingual STS tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen-Yu Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Brian Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsi_D/0/1/0/all/0/1\">Davood Shamsi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Method to Automate the Discharge Summary Hospital Course for Neurology Patients. (arXiv:2305.06416v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06416","description":"<p>Generation of automated clinical notes have been posited as a strategy to\nmitigate physician burnout. In particular, an automated narrative summary of a\npatient's hospital stay could supplement the hospital course section of the\ndischarge summary that inpatient physicians document in electronic health\nrecord (EHR) systems. In the current study, we developed and evaluated an\nautomated method for summarizing the hospital course section using\nencoder-decoder sequence-to-sequence transformer models. We fine tuned BERT and\nBART models and optimized for factuality through constraining beam search,\nwhich we trained and tested using EHR data from patients admitted to the\nneurology unit of an academic medical center. The approach demonstrated good\nROUGE scores with an R-2 of 13.76. In a blind evaluation, two board-certified\nphysicians rated 62% of the automated summaries as meeting the standard of\ncare, which suggests the method may be useful clinically. To our knowledge,\nthis study is among the first to demonstrate an automated method for generating\na discharge summary hospital course that approaches a quality level of what a\nphysician would write.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartman_V/0/1/0/all/0/1\">Vince C. Hartman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapat_S/0/1/0/all/0/1\">Sanika S. Bapat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiner_M/0/1/0/all/0/1\">Mark G. Weiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navi_B/0/1/0/all/0/1\">Babak B. Navi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sholle_E/0/1/0/all/0/1\">Evan T. Sholle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campion_T/0/1/0/all/0/1\">Thomas R. Campion, Jr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bot or Human? Detecting ChatGPT Imposters with A Single Question. (arXiv:2305.06424v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06424","description":"<p>Large language models like ChatGPT have recently demonstrated impressive\ncapabilities in natural language understanding and generation, enabling various\napplications including translation, essay writing, and chit-chatting. However,\nthere is a concern that they can be misused for malicious purposes, such as\nfraud or denial-of-service attacks. Therefore, it is crucial to develop methods\nfor detecting whether the party involved in a conversation is a bot or a human.\nIn this paper, we propose a framework named FLAIR, Finding Large language model\nAuthenticity via a single Inquiry and Response, to detect conversational bots\nin an online manner. Specifically, we target a single question scenario that\ncan effectively differentiate human users from bots. The questions are divided\ninto two categories: those that are easy for humans but difficult for bots\n(e.g., counting, substitution, positioning, noise filtering, and ASCII art),\nand those that are easy for bots but difficult for humans (e.g., memorization\nand computation). Our approach shows different strengths of these questions in\ntheir effectiveness, providing a new way for online service providers to\nprotect themselves against nefarious activities and ensure that they are\nserving real users. We open-sourced our dataset on\nhttps://github.com/hongwang600/FLAIR and welcome contributions from the\ncommunity to enrich such detection datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mispronunciation Detection of Basic Quranic Recitation Rules using Deep Learning. (arXiv:2305.06429v1 [cs.SD])","link":"http://arxiv.org/abs/2305.06429","description":"<p>In Islam, readers must apply a set of pronunciation rules called Tajweed\nrules to recite the Quran in the same way that the angel Jibrael taught the\nProphet, Muhammad. The traditional process of learning the correct application\nof these rules requires a human who must have a license and great experience to\ndetect mispronunciation. Due to the increasing number of Muslims around the\nworld, the number of Tajweed teachers is not enough nowadays for daily\nrecitation practice for every Muslim. Therefore, lots of work has been done for\nautomatic Tajweed rules' mispronunciation detection to help readers recite\nQuran correctly in an easier way and shorter time than traditional learning\nways. All previous works have three common problems. First, most of them\nfocused on machine learning algorithms only. Second, they used private datasets\nwith no benchmark to compare with. Third, they did not take into consideration\nthe sequence of input data optimally, although the speech signal is time\nseries. To overcome these problems, we proposed a solution that consists of\nMel-Frequency Cepstral Coefficient (MFCC) features with Long Short-Term Memory\n(LSTM) neural networks which use the time series, to detect mispronunciation in\nTajweed rules. In addition, our experiments were performed on a public dataset,\nthe QDAT dataset, which contains more than 1500 voices of the correct and\nincorrect recitation of three Tajweed rules (Separate stretching , Tight Noon ,\nand Hide ). To the best of our knowledge, the QDAT dataset has not been used by\nany research paper yet. We compared the performance of the proposed LSTM model\nwith traditional machine learning algorithms used in SoTA. The LSTM model with\ntime series showed clear superiority over traditional machine learning. The\naccuracy achieved by LSTM on the QDAT dataset was 96%, 95%, and 96% for the\nthree rules (Separate stretching, Tight Noon, and Hide), respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harere_A/0/1/0/all/0/1\">Ahmad Al Harere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jallad_K/0/1/0/all/0/1\">Khloud Al Jallad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Grounded Graph Convolutional Network. (arXiv:2305.06434v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06434","description":"<p>Graph Convolutional Networks (GCNs) have shown strong performance in learning\ntext representations for various tasks such as text classification, due to its\nexpressive power in modeling graph structure data (e.g., a literature citation\nnetwork). Most existing GCNs are limited to deal with documents included in a\npre-defined graph, i.e., it cannot be generalized to out-of-graph documents. To\naddress this issue, we propose to transform the document graph into a word\ngraph, to decouple data samples (i.e., documents in training and test sets) and\na GCN model by using a document-independent graph. Such word-level GCN could\ntherefore naturally inference out-of-graph documents in an inductive way. The\nproposed Word-level Graph (WGraph) can not only implicitly learning word\npresentation with commonly-used word co-occurrences in corpora, but also\nincorporate extra global semantic dependency derived from inter-document\nrelationships (e.g., literature citations). An inductive Word-grounded Graph\nConvolutional Network (WGCN) is proposed to learn word and document\nrepresentations based on WGraph in a supervised manner. Experiments on text\nclassification with and without citation networks evidence that the proposed\nWGCN model outperforms existing methods in terms of effectiveness and\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhibin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-yun Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps. (arXiv:2305.06472v1 [cs.LG])","link":"http://arxiv.org/abs/2305.06472","description":"<p>Prognostics and health management (PHM) technology plays a critical role in\nindustrial production and equipment maintenance by identifying and predicting\npossible equipment failures and damages, thereby allowing necessary maintenance\nmeasures to be taken to enhance equipment service life and reliability while\nreducing production costs and downtime. In recent years, PHM technology based\non artificial intelligence (AI) has made remarkable achievements in the context\nof the industrial IoT and big data, and it is widely used in various\nindustries, such as railway, energy, and aviation, for condition monitoring,\nfault prediction, and health management. The emergence of large-scale\nfoundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of\nAI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved\nfrom a research paradigm of single-modal, single-task, and limited-data to a\nmulti-modal, multi-task, massive data, and super-large model paradigm. ChatGPT\nrepresents a landmark achievement in this research paradigm, offering hope for\ngeneral artificial intelligence due to its highly intelligent natural language\nunderstanding ability. However, the PHM field lacks a consensus on how to\nrespond to this significant change in the AI field, and a systematic review and\nroadmap is required to elucidate future development directions. To fill this\ngap, this paper systematically expounds on the key components and latest\ndevelopments of LSF-Models. Then, we systematically answered how to build the\nLSF-Model applicable to PHM tasks and outlined the challenges and future\ndevelopment roadmaps for this research paradigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan-Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muxia Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Model for Translation of Text from Indian Languages to Bharti Braille Characters. (arXiv:2305.06475v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06475","description":"<p>People who are visually impaired face a lot of difficulties while studying.\nOne of the major causes to this is lack of available text in Bharti Braille\nscript. In this paper, we have suggested a scheme to convert text in major\nIndian languages into Bharti Braille. The system uses a hybrid approach where\nat first the text in Indian language is given to a rule based system and in\ncase if there is any ambiguity then it is resolved by applying a LSTM based\nmodel. The developed model has also been tested and found to have produced near\naccurate results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nisheeth Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katyayan_P/0/1/0/all/0/1\">Pragya Katyayan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Contextualized Plan Prediction for Embodied Task Completion. (arXiv:2305.06485v1 [cs.RO])","link":"http://arxiv.org/abs/2305.06485","description":"<p>Task planning is an important component of traditional robotics systems\nenabling robots to compose fine grained skills to perform more complex tasks.\nRecent work building systems for translating natural language to executable\nactions for task completion in simulated embodied agents is focused on directly\npredicting low level action sequences that would be expected to be directly\nexecutable by a physical robot. In this work, we instead focus on predicting a\nhigher level plan representation for one such embodied task completion dataset\n- TEACh, under the assumption that techniques for high-level plan prediction\nfrom natural language are expected to be more transferable to physical robot\nsystems. We demonstrate that better plans can be predicted using multimodal\ncontext, and that plan prediction and plan execution modules are likely\ndependent on each other and hence it may not be ideal to fully decouple them.\nFurther, we benchmark execution of oracle plans to quantify the scope for\nimprovement in plan prediction models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inan_M/0/1/0/all/0/1\">Mert &#x130;nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1\">Aishwarya Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gella_S/0/1/0/all/0/1\">Spandana Gella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_P/0/1/0/all/0/1\">Patrick Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications. (arXiv:2305.06522v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06522","description":"<p>Large-scale pre-trained language models have shown outstanding performance in\na variety of NLP tasks. However, they are also known to be significantly\nbrittle against specifically crafted adversarial examples, leading to\nincreasing interest in probing the adversarial robustness of NLP systems. We\nintroduce RSMI, a novel two-stage framework that combines randomized smoothing\n(RS) with masked inference (MI) to improve the adversarial robustness of NLP\nsystems. RS transforms a classifier into a smoothed classifier to obtain robust\nrepresentations, whereas MI forces a model to exploit the surrounding context\nof a masked token in an input sequence. RSMI improves adversarial robustness by\n2 to 3 times over existing state-of-the-art methods on benchmark datasets. We\nalso perform in-depth qualitative analysis to validate the effectiveness of the\ndifferent stages of RSMI and probe the impact of its components through\nextensive ablations. By empirically proving the stability of RSMI, we put it\nforward as a practical method to robustly train large-scale NLP models. Our\ncode and datasets are available at https://github.com/Han8931/rsmi_nlp\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1\">Han Cheol Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruochen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_M/0/1/0/all/0/1\">Megh Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_X/0/1/0/all/0/1\">Xu Chi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Good are Commercial Large Language Models on African Languages?. (arXiv:2305.06530v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06530","description":"<p>Recent advancements in Natural Language Processing (NLP) has led to the\nproliferation of large pretrained language models. These models have been shown\nto yield good performance, using in-context learning, even on unseen tasks and\nlanguages. They have also been exposed as commercial APIs as a form of\nlanguage-model-as-a-service, with great adoption. However, their performance on\nAfrican languages is largely unknown. We present a preliminary analysis of\ncommercial large language models on two tasks (machine translation and text\nclassification) across eight African languages, spanning different language\nfamilies and geographical areas. Our results suggest that commercial language\nmodels produce below-par performance on African languages. We also find that\nthey perform better on text classification than machine translation. In\ngeneral, our findings present a call-to-action to ensure African languages are\nwell represented in commercial large language models, given their growing\npopularity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ojo_J/0/1/0/all/0/1\">Jessica Ojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogueji_K/0/1/0/all/0/1\">Kelechi Ogueji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment. (arXiv:2305.06535v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06535","description":"<p>Recent legislation of the \"right to be forgotten\" has led to the interest in\nmachine unlearning, where the learned models are endowed with the function to\nforget information about specific training instances as if they have never\nexisted in the training set. Previous work mainly focuses on computer vision\nscenarios and largely ignores the essentials of unlearning in NLP field, where\ntext data contains more explicit and sensitive personal information than\nimages. In this paper, we propose a general unlearning framework called KGA to\ninduce forgetfulness. Different from previous work that tries to recover\ngradients or forces models to perform close to one specific distribution, KGA\nmaintains distribution differences (i.e., knowledge gap). This relaxes the\ndistribution assumption. Furthermore, we first apply the unlearning method to\nvarious NLP tasks (i.e., classification, translation, response generation) and\npropose several unlearning evaluation metrics with pertinence. Experiments on\nlarge-scale datasets show that KGA yields comprehensive improvements over\nbaselines, where extensive analyses further validate the effectiveness of KGA\nand provide insight into unlearning for NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Wei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic uncertainty guides the extension of conventions to new referents. (arXiv:2305.06539v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06539","description":"<p>A long tradition of studies in psycholinguistics has examined the formation\nand generalization of ad hoc conventions in reference games, showing how newly\nacquired conventions for a given target transfer to new referential contexts.\nHowever, another axis of generalization remains understudied: how do\nconventions formed for one target transfer to completely distinct targets, when\nspecific lexical choices are unlikely to repeat? This paper presents two dyadic\nstudies (N = 240) that address this axis of generalization, focusing on the\nrole of nameability -- the a priori likelihood that two individuals will share\nthe same label. We leverage the recently-released KiloGram dataset, a\ncollection of abstract tangram images that is orders of magnitude larger than\npreviously available, exhibiting high diversity of properties like nameability.\nOur first study asks how nameability shapes convention formation, while the\nsecond asks how new conventions generalize to entirely new targets of\nreference. Our results raise new questions about how ad hoc conventions extend\nbeyond target-specific re-use of specific lexical choices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eliav_R/0/1/0/all/0/1\">Ron Eliav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_A/0/1/0/all/0/1\">Anya Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert D. Hawkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeoGLUE: A GeoGraphic Language Understanding Evaluation Benchmark. (arXiv:2305.06545v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06545","description":"<p>With a fast developing pace of geographic applications, automatable and\nintelligent models are essential to be designed to handle the large volume of\ninformation. However, few researchers focus on geographic natural language\nprocessing, and there has never been a benchmark to build a unified standard.\nIn this work, we propose a GeoGraphic Language Understanding Evaluation\nbenchmark, named GeoGLUE. We collect data from open-released geographic\nresources and introduce six natural language understanding tasks, including\ngeographic textual similarity on recall, geographic textual similarity on\nrerank, geographic elements tagging, geographic composition analysis,\ngeographic where what cut, and geographic entity alignment. We also pro vide\nevaluation experiments and analysis of general baselines, indicating the\neffectiveness and significance of the GeoGLUE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Ruixue Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1\">Ning Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofeng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Incremental Lifelong Learning in an Open World. (arXiv:2305.06555v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06555","description":"<p>Lifelong learning (LL) is an important ability for NLP models to learn new\ntasks continuously. Architecture-based approaches are reported to be effective\nimplementations for LL models. However, it is non-trivial to extend previous\napproaches to domain incremental LL scenarios since they either require access\nto task identities in the testing phase or cannot handle samples from unseen\ntasks. In this paper, we propose \\textbf{Diana}: a\n\\underline{d}ynam\\underline{i}c \\underline{a}rchitecture-based\nlifelo\\underline{n}g le\\underline{a}rning model that tries to learn a sequence\nof tasks with a prompt-enhanced language model. Four types of hierarchically\norganized prompts are used in Diana to capture knowledge from different\ngranularities. Specifically, we dedicate task-level prompts to capture\ntask-specific knowledge to retain high LL performances and maintain\ninstance-level prompts to learn knowledge shared across input samples to\nimprove the model's generalization performance. Moreover, we dedicate separate\nprompts to explicitly model unseen tasks and introduce a set of prompt key\nvectors to facilitate knowledge sharing between tasks. Extensive experiments\ndemonstrate that Diana outperforms state-of-the-art LL models, especially in\nhandling unseen tasks. We release the code and data at\n\\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/diana}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Hao Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-Tailed Question Answering in an Open World. (arXiv:2305.06557v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06557","description":"<p>Real-world data often have an open long-tailed distribution, and building a\nunified QA model supporting various tasks is vital for practical QA\napplications. However, it is non-trivial to extend previous QA approaches since\nthey either require access to seen tasks of adequate samples or do not\nexplicitly model samples from unseen tasks. In this paper, we define Open\nLong-Tailed QA (OLTQA) as learning from long-tailed distributed data and\noptimizing performance over seen and unseen QA tasks. We propose an OLTQA model\nthat encourages knowledge sharing between head, tail and unseen tasks, and\nexplicitly mines knowledge from a large pre-trained language model (LM).\nSpecifically, we organize our model through a pool of fine-grained components\nand dynamically combine these components for an input to facilitate knowledge\nsharing. A retrieve-then-rerank frame is further introduced to select\nin-context examples, which guild the LM to generate text that express knowledge\nfor QA tasks. Moreover, a two-stage training approach is introduced to\npre-train the framework by knowledge distillation (KD) from the LM and then\njointly train the frame and a QA model through an adaptive mutual KD method. On\na large-scale OLTQA dataset we curate from 43 existing QA datasets, our model\nconsistently outperforms the state-of-the-art. We release the code and data at\n\\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/oltqa}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Hao Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A First Look at LLM-Powered Generative News Recommendation. (arXiv:2305.06566v1 [cs.IR])","link":"http://arxiv.org/abs/2305.06566","description":"<p>Personalized news recommendation systems have become essential tools for\nusers to navigate the vast amount of online news content, yet existing news\nrecommenders face significant challenges such as the cold-start problem, user\nprofile modeling, and news content understanding. Previous works have typically\nfollowed an inflexible routine to address a particular challenge through model\ndesign, but are limited in their ability to understand news content and capture\nuser interests. In this paper, we introduce GENRE, an LLM-powered generative\nnews recommendation framework, which leverages pretrained semantic knowledge\nfrom large language models to enrich news data. Our aim is to provide a\nflexible and unified solution for news recommendation by moving from model\ndesign to prompt design. We showcase the use of GENRE for personalized news\ngeneration, user profiling, and news summarization. Extensive experiments with\nvarious popular recommendation models demonstrate the effectiveness of GENRE.\nWe will publish our code and data for other researchers to reproduce our work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qijiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakai_T/0/1/0/all/0/1\">Tetsuya Sakai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])","link":"http://arxiv.org/abs/2305.06569","description":"<p>Recommendation foundation model utilizes large language models (LLM) for\nrecommendation by converting recommendation tasks into natural language tasks.\nIt enables generative recommendation which directly generates the item(s) to\nrecommend rather than calculating a ranking score for each and every candidate\nitem in traditional recommendation models, simplifying the recommendation\npipeline from multi-stage filtering to single-stage filtering. To avoid\ngenerating excessively long text when deciding which item(s) to recommend,\ncreating LLM-compatible item IDs is essential for recommendation foundation\nmodels. In this study, we systematically examine the item indexing problem for\nrecommendation foundation models, using P5 as the representative backbone model\nand replicating its results with various indexing methods. To emphasize the\nimportance of item indexing, we first discuss the issues of several trivial\nitem indexing methods, such as independent indexing, title indexing, and random\nindexing. We then propose four simple yet effective solutions, including\nsequential indexing, collaborative indexing, semantic (content-based) indexing,\nand hybrid indexing. Our reproducibility study of P5 highlights the significant\ninfluence of item indexing methods on the model performance, and our results on\nreal-world datasets validate the effectiveness of our proposed solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Fused Gromov-Wasserstein Framework for Unsupervised Knowledge Graph Entity Alignment. (arXiv:2305.06574v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06574","description":"<p>Entity alignment is the task of identifying corresponding entities across\ndifferent knowledge graphs (KGs). Although recent embedding-based entity\nalignment methods have shown significant advancements, they still struggle to\nfully utilize KG structural information. In this paper, we introduce FGWEA, an\nunsupervised entity alignment framework that leverages the Fused\nGromov-Wasserstein (FGW) distance, allowing for a comprehensive comparison of\nentity semantics and KG structures within a joint optimization framework. To\naddress the computational challenges associated with optimizing FGW, we devise\na three-stage progressive optimization algorithm. It starts with a basic\nsemantic embedding matching, proceeds to approximate cross-KG structural and\nrelational similarity matching based on iterative updates of high-confidence\nentity links, and ultimately culminates in a global structural comparison\nbetween KGs. We perform extensive experiments on four entity alignment datasets\ncovering 14 distinct KGs across five languages. Without any supervision or\nhyper-parameter tuning, FGWEA surpasses 21 competitive baselines, including\ncutting-edge supervised entity alignment methods. Our code is available at\nhttps://github.com/squareRoot3/FusedGW-Entity-Alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jianheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kangfei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Dictionary Prompting Elicits Translation in Large Language Models. (arXiv:2305.06575v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06575","description":"<p>Large language models (LLMs) have shown surprisingly good performance in\nmultilingual neural machine translation (MNMT) even when trained without\nparallel data. Yet, despite the fact that the amount of training data is\ngigantic, they still struggle with translating rare words, particularly for\nlow-resource languages. Even worse, it is usually unrealistic to retrieve\nrelevant demonstrations for in-context learning with low-resource languages on\nLLMs, which restricts the practical use of LLMs for translation -- how should\nwe mitigate this problem? To this end, we present a novel method, CoD, which\naugments LLMs with prior knowledge with the chains of multilingual dictionaries\nfor a subset of input words to elicit translation abilities for LLMs. Extensive\nexperiments indicate that augmenting ChatGPT with CoD elicits large gains by up\nto 13x ChrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in\nCyrillic script) on FLORES-200 full devtest set. We further demonstrate the\nimportance of chaining the multilingual dictionaries, as well as the\nsuperiority of CoD to few-shot demonstration for low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2). (arXiv:2305.06586v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06586","description":"<p>We present the findings of SemEval-2023 Task 2 on Fine-grained Multilingual\nNamed Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the task\nfocused on methods to identify complex fine-grained named entities (like\nWRITTENWORK, VEHICLE, MUSICALGRP) across 12 languages, in both monolingual and\nmultilingual scenarios, as well as noisy settings. The task used the MultiCoNER\nV2 dataset, composed of 2.2 million instances in Bangla, Chinese, English,\nFarsi, French, German, Hindi, Italian., Portuguese, Spanish, Swedish, and\nUkrainian. MultiCoNER 2 was one of the most popular tasks of SemEval-2023. It\nattracted 842 submissions from 47 teams, and 34 teams submitted system papers.\nResults showed that complex entity types such as media titles and product names\nwere the most challenging. Methods fusing external knowledge into transformer\nmodels achieved the best performance, and the largest gains were on the\nCreative Work and Group classes, which are still challenging even with external\nknowledge. Some fine-grained classes proved to be more challenging than others,\nsuch as SCIENTIST, ARTWORK, and PRIVATECORP. We also observed that noisy data\nhas a significant impact on model performance, with an average drop of 10% on\nthe noisy subset. The task highlights the need for future research on improving\nNER robustness on noisy data containing complex entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fetahu_B/0/1/0/all/0/1\">Besnik Fetahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_S/0/1/0/all/0/1\">Sudipta Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rokhlenko_O/0/1/0/all/0/1\">Oleg Rokhlenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malmasi_S/0/1/0/all/0/1\">Shervin Malmasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FactKG: Fact Verification via Reasoning on Knowledge Graphs. (arXiv:2305.06590v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06590","description":"<p>In real world applications, knowledge graphs (KG) are widely used in various\ndomains (e.g. medical applications and dialogue agents). However, for fact\nverification, KGs have not been adequately utilized as a knowledge source. KGs\ncan be a valuable knowledge source in fact verification due to their\nreliability and broad applicability. A KG consists of nodes and edges which\nmakes it clear how concepts are linked together, allowing machines to reason\nover chains of topics. However, there are many challenges in understanding how\nthese machine-readable concepts map to information in text. To enable the\ncommunity to better use KGs, we introduce a new dataset, FactKG: Fact\nVerification via Reasoning on Knowledge Graphs. It consists of 108k natural\nlanguage claims with five types of reasoning: One-hop, Conjunction, Existence,\nMulti-hop, and Negation. Furthermore, FactKG contains various linguistic\npatterns, including colloquial style claims as well as written style claims to\nincrease practicality. Lastly, we develop a baseline approach and analyze\nFactKG over these reasoning types. We believe FactKG can advance both\nreliability and practicality in KG-based fact verification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yeonsu Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1\">Yohan Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews. (arXiv:2305.06595v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06595","description":"<p>The analysis of consumer sentiment, as expressed through reviews, can provide\na wealth of insight regarding the quality of a product. While the study of\nsentiment analysis has been widely explored in many popular languages,\nrelatively less attention has been given to the Bangla language, mostly due to\na lack of relevant data and cross-domain adaptability. To address this\nlimitation, we present BanglaBook, a large-scale dataset of Bangla book reviews\nconsisting of 158,065 samples classified into three broad categories: positive,\nnegative, and neutral. We provide a detailed statistical analysis of the\ndataset and employ a range of machine learning models to establish baselines\nincluding SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial\nperformance advantage of pre-trained models over models that rely on manually\ncrafted features, emphasizing the necessity for additional training resources\nin this domain. Additionally, we conduct an in-depth error analysis by\nexamining sentiment unigrams, which may provide insight into common\nclassification errors in under-resourced languages like Bangla. Our codes and\ndata are publicly available at https://github.com/mohsinulkabir14/BanglaBook.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Mohsinul Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahfuz_O/0/1/0/all/0/1\">Obayed Bin Mahfuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raiyan_S/0/1/0/all/0/1\">Syed Rifat Raiyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_H/0/1/0/all/0/1\">Hasan Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md Kamrul Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling Programming Thinking in Large Language Models Toward Code Generation. (arXiv:2305.06599v1 [cs.SE])","link":"http://arxiv.org/abs/2305.06599","description":"<p>Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive\nperformance in code generation. A large-scale study released that writing\nprograms requires programming thinking, i.e., analyzing and implementing\nrequirements in programming logic (e.g., sequence, branch, loop). Existing\nstudies use LLMs to generate programs from requirements directly and do not\nexplicitly introduce the programming thinking.\n</p>\n<p>This paper explores how to unlock the programming thinking of LLMs in code\ngeneration and proposes an approach named TiP. Our idea is to decompose code\ngeneration into two steps and progressively lead LLMs to analyze&amp;implement\nrequirements in programming logic. Specifically, TiP first generates a code\nsketch, which provides a high-level solving process using programming logic but\nomits implementation details (e.g., APIs). Then, TiP implements the sketch into\na program using specific programming languages. We conduct extensive\nexperiments on three public benchmarks (i.e., HumanEval, MBPP, and MBCPP). (1)\nTiP outperforms the state-of-the-art baseline - ChatGPT by up to 17.5% in\nPass@1, 11.02% in Pass@3, and 9.84% in Pass@5. (2) Human evaluation shows that\nTiP outperforms ChatGPT in three aspects (i.e., correctness, code quality, and\nmaintainability). (3) TiP is effective for different LLMs. (4) We explore\nmultiple choices (e.g., chain-of-thought) for the code sketch and validate the\nsuperiority of our design. (5) We discuss the complementarity between TiP and\npost-processing approaches (e.g., CodeT).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongmin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhi Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autocorrelations Decay in Texts and Applicability Limits of Language Models. (arXiv:2305.06615v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06615","description":"<p>We show that the laws of autocorrelations decay in texts are closely related\nto applicability limits of language models. Using distributional semantics we\nempirically demonstrate that autocorrelations of words in texts decay according\nto a power law. We show that distributional semantics provides coherent\nautocorrelations decay exponents for texts translated to multiple languages.\nThe autocorrelations decay in generated texts is quantitatively and often\nqualitatively different from the literary texts. We conclude that language\nmodels exhibiting Markov behavior, including large autoregressive language\nmodels, may have limitations when applied to long texts, whether analysis or\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mikhaylovskiy_N/0/1/0/all/0/1\">Nikolay Mikhaylovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Churilov_I/0/1/0/all/0/1\">Ilya Churilov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction. (arXiv:2305.06616v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06616","description":"<p>Continual few-shot relation extraction (RE) aims to continuously train a\nmodel for new relations with few labeled training data, of which the major\nchallenges are the catastrophic forgetting of old relations and the overfitting\ncaused by data sparsity. In this paper, we propose a new model, namely SCKD, to\naccomplish the continual few-shot RE task. Specifically, we design serial\nknowledge distillation to preserve the prior knowledge from previous models and\nconduct contrastive learning with pseudo samples to keep the representations of\nsamples in different relations sufficiently distinguishable. Our experiments on\ntwo benchmark datasets validate the effectiveness of SCKD for continual\nfew-shot RE and its superiority in knowledge transfer and memory utilization\nover state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zitao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Continual Relation Extraction by Distinguishing Analogous Semantics. (arXiv:2305.06620v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06620","description":"<p>Continual relation extraction (RE) aims to learn constantly emerging\nrelations while avoiding forgetting the learned relations. Existing works store\na small number of typical samples to re-train the model for alleviating\nforgetting. However, repeatedly replaying these samples may cause the\noverfitting problem. We conduct an empirical study on existing works and\nobserve that their performance is severely affected by analogous relations. To\naddress this issue, we propose a novel continual extraction model for analogous\nrelations. Specifically, we design memory-insensitive relation prototypes and\nmemory augmentation to overcome the overfitting problem. We also introduce\nintegrated training and focal knowledge distillation to enhance the performance\non analogous relations. Experimental results show the superiority of our model\nand demonstrate its effectiveness in distinguishing analogous relations and\novercoming overfitting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenzheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yuanning Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06626","description":"<p>Though majority vote among annotators is typically used for ground truth\nlabels in natural language processing, annotator disagreement in tasks such as\nhate speech detection may reflect differences among group opinions, not noise.\nThus, a crucial problem in hate speech detection is whether a statement is\noffensive to the demographic group that it targets, which may constitute a\nsmall fraction of the annotator pool. We construct a model that predicts\nindividual annotator ratings on potentially offensive text and combines this\ninformation with the predicted target group of the text to model the opinions\nof target group members. We show gains across a range of metrics, including\nraising performance over the baseline by 22% at predicting individual\nannotators' ratings and 33% at predicting variance among annotators, which\nprovides a method of measuring model uncertainty downstream. We find that\nannotators' ratings can be predicted using their demographic information and\nopinions on online content, without the need to track identifying annotator IDs\nthat link each annotator to their ratings. We also find that use of\nnon-invasive survey questions on annotators' online experiences helps to\nmaximize privacy and minimize unnecessary collection of demographic information\nwhen predicting annotators' opinions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1\">Eve Fleisig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abebe_R/0/1/0/all/0/1\">Rediet Abebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PROM: A Phrase-level Copying Mechanism with Pre-training for Abstractive Summarization. (arXiv:2305.06647v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06647","description":"<p>Based on the remarkable achievements of pre-trained language models in\nabstractive summarization, the copying mechanism has proved helpful by\nimproving the factuality, stability, and overall performance. This work\nproposes PROM, a new PhRase-level cOpying Mechanism that enhances attention on\nn-grams, which can be applied to zero-shot summarization with pre-training.\nPROM adds an indicator layer to explicitly pick up tokens in n-gram that can be\ncopied from the source, and calculates an auxiliary loss for the copying\nprediction. Empirical studies show that PROM makes significant improvements in\nfine-tuning on benchmarks. In zero-shot setting, PROM is utilized in the\nself-supervised pre-training on raw corpora and provides new general baselines\non a wide range of summarization datasets. Further analysis shows that PROM\nperforms more reasonable copying and contributes to faithfulness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinbei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QURG: Question Rewriting Guided Context-Dependent Text-to-SQL Semantic Parsing. (arXiv:2305.06655v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06655","description":"<p>Context-dependent Text-to-SQL aims to translate multi-turn natural language\nquestions into SQL queries. Despite various methods have exploited\ncontext-dependence information implicitly for contextual SQL parsing, there are\nfew attempts to explicitly address the dependencies between current question\nand question context. This paper presents QURG, a novel Question Rewriting\nGuided approach to help the models achieve adequate contextual understanding.\nSpecifically, we first train a question rewriting model to complete the current\nquestion based on question context, and convert them into a rewriting edit\nmatrix. We further design a two-stream matrix encoder to jointly model the\nrewriting relations between question and context, and the schema linking\nrelations between natural language and structured schema. Experimental results\nshow that QURG significantly improves the performances on two large-scale\ncontext-dependent datasets SParC and CoSQL, especially for hard and long-turn\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1\">Linzheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_D/0/1/0/all/0/1\">Dongling Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liqun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian-Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhao Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models. (arXiv:2305.06677v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06677","description":"<p>A salient characteristic of large pre-trained language models (PTLMs) is a\nremarkable improvement in their generalization capability and emergence of new\ncapabilities with increasing model capacity and pre-training dataset size.\nConsequently, we are witnessing the development of enormous models pushing the\nstate-of-the-art. It is, however, imperative to realize that this inevitably\nleads to prohibitively long training times, extortionate computing costs, and a\ndetrimental environmental impact. Significant efforts are underway to make PTLM\ntraining more efficient through innovations in model architectures, training\npipelines, and loss function design, with scant attention being paid to\noptimizing the utility of training data. The key question that we ask is\nwhether it is possible to train PTLMs by employing only highly informative\nsubsets of the training data while maintaining downstream performance? Building\nupon the recent progress in informative data subset selection, we show how we\ncan employ submodular optimization to select highly representative subsets of\nthe training corpora. Our results demonstrate that the proposed framework can\nbe applied to efficiently train multiple PTLMs (BERT, BioBERT, GPT-2) using\nonly a fraction of data while retaining up to $\\sim99\\%$ of the performance of\nthe fully-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Renduchintala_H/0/1/0/all/0/1\">H S V N S Kowndinya Renduchintala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1\">Krishnateja Killamsetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Sumit Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1\">Milan Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker Selection and Data Augmentation. (arXiv:2305.06683v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06683","description":"<p>This paper introduces a novel worker selection algorithm, enhancing\nannotation quality and reducing costs in challenging span-based sequence\nlabeling tasks in Natural Language Processing (NLP). Unlike previous studies\ntargeting simpler tasks, this study contends with the complexities of label\ninterdependencies in sequence labeling tasks. The proposed algorithm utilizes a\nCombinatorial Multi-Armed Bandit (CMAB) approach for worker selection. The\nchallenge of dealing with imbalanced and small-scale datasets, which hinders\noffline simulation of worker selection, is tackled using an innovative data\naugmentation method termed shifting, expanding, and shrinking (SES). The SES\nmethod is designed specifically for sequence labeling tasks. Rigorous testing\non CoNLL 2003 NER and Chinese OEI datasets showcased the algorithm's\nefficiency, with an increase in F1 score up to 100.04% of the expert-only\nbaseline, alongside cost savings up to 65.97%. The paper also encompasses a\ndataset-independent test emulating annotation evaluation through a Bernoulli\ndistribution, which still led to an impressive 97.56% F1 score of the expert\nbaseline and 59.88% cost savings. This research addresses and overcomes\nnumerous obstacles in worker selection for complex NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liner Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhixuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yaping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Erhong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*. (arXiv:2305.06721v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06721","description":"<p>To advance the neural encoding of Portuguese (PT), and a fortiori the\ntechnological preparation of this language for the digital age, we developed a\nTransformer-based foundation model that sets a new state of the art in this\nrespect for two of its variants, namely European Portuguese from Portugal\n(PT-PT) and American Portuguese from Brazil (PT-BR).\n</p>\n<p>To develop this encoder, which we named Albertina PT-*, a strong model was\nused as a starting point, DeBERTa, and its pre-training was done over data sets\nof Portuguese, namely over a data set we gathered for PT-PT and over the brWaC\ncorpus for PT-BR. The performance of Albertina and competing models was\nassessed by evaluating them on prominent downstream language processing tasks\nadapted for Portuguese.\n</p>\n<p>Both Albertina PT-PT and PT-BR versions are distributed free of charge and\nunder the most permissive license possible and can be run on consumer-grade\nhardware, thus seeking to contribute to the advancement of research and\ninnovation in language technology for Portuguese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_J/0/1/0/all/0/1\">Jo&#xe3;o Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_L/0/1/0/all/0/1\">Lu&#xed;s Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jo&#xe3;o Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branco_A/0/1/0/all/0/1\">Ant&#xf3;nio Branco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_R/0/1/0/all/0/1\">Rodrigo Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_H/0/1/0/all/0/1\">Henrique Lopes Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osorio_T/0/1/0/all/0/1\">Tom&#xe1;s Os&#xf3;rio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The First Parallel Corpora for Kurdish Sign Language. (arXiv:2305.06747v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06747","description":"<p>Kurdish Sign Language (KuSL) is the natural language of the Kurdish Deaf\npeople. We work on automatic translation between spoken Kurdish and KuSL. Sign\nlanguages evolve rapidly and follow grammatical rules that differ from spoken\nlanguages. Consequently,those differences should be considered during any\ntranslation. We proposed an avatar-based automatic translation of Kurdish texts\nin the Sorani (Central Kurdish) dialect into the Kurdish Sign language. We\ndeveloped the first parallel corpora for that pair that we use to train a\nStatistical Machine Translation (SMT) engine. We tested the outcome\nunderstandability and evaluated it using the Bilingual Evaluation Understudy\n(BLEU). Results showed 53.8% accuracy. Compared to the previous experiments in\nthe field, the result is considerably high. We suspect the reason to be the\nsimilarity between the structure of the two pairs. We plan to make the\nresources publicly available under CC BY-NC-SA 4.0 license on the Kurdish-BLARK\n(https://kurdishblark.github.io/).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamal_Z/0/1/0/all/0/1\">Zina Kamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1\">Hossein Hassani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks. (arXiv:2305.06754v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06754","description":"<p>Transformer architectures are complex and their use in NLP, while it has\nengendered many successes, makes their interpretability or explainability\nchallenging. Recent debates have shown that attention maps and attribution\nmethods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this\npaper, we present some of their limitations and introduce COCKATIEL, which\nsuccessfully addresses some of them. COCKATIEL is a novel, post-hoc,\nconcept-based, model-agnostic XAI technique that generates meaningful\nexplanations from the last layer of a neural net model trained on an NLP\nclassification task by using Non-Negative Matrix Factorization (NMF) to\ndiscover the concepts the model leverages to make predictions and by exploiting\na Sensitivity Analysis to estimate accurately the importance of each of these\nconcepts for the model. It does so without compromising the accuracy of the\nunderlying model or requiring a new one to be trained. We conduct experiments\nin single and multi-aspect sentiment analysis tasks and we show COCKATIEL's\nsuperior ability to discover concepts that align with humans' on Transformer\nmodels without any supervision, we objectively verify the faithfulness of its\nexplanations through fidelity metrics, and we showcase its ability to provide\nmeaningful explanations in two different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jourdan_F/0/1/0/all/0/1\">Fanny Jourdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picard_A/0/1/0/all/0/1\">Agustin Picard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fel_T/0/1/0/all/0/1\">Thomas Fel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risser_L/0/1/0/all/0/1\">Laurent Risser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loubes_J/0/1/0/all/0/1\">Jean Michel Loubes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asher_N/0/1/0/all/0/1\">Nicholas Asher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Idiomatic Multiword Expressions in Clinical Terminology using Definition-Based Representation Learning. (arXiv:2305.06801v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06801","description":"<p>This paper shines a light on the potential of definition-based semantic\nmodels for detecting idiomatic and semi-idiomatic multiword expressions (MWEs)\nin clinical terminology. Our study focuses on biomedical entities defined in\nthe UMLS ontology and aims to help prioritize the translation efforts of these\nentities. In particular, we develop an effective tool for scoring the\nidiomaticity of biomedical MWEs based on the degree of similarity between the\nsemantic representations of those MWEs and a weighted average of the\nrepresentation of their constituents. We achieve this using a biomedical\nlanguage model trained to produce similar representations for entity names and\ntheir definitions, called BioLORD. The importance of this definition-based\napproach is highlighted by comparing the BioLORD model to two other\nstate-of-the-art biomedical language models based on Transformer: SapBERT and\nCODER. Our results show that the BioLORD model has a strong ability to identify\nidiomatic MWEs, not replicated in other models. Our corpus-free idiomaticity\nestimation helps ontology translators to focus on more challenging MWEs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Remy_F/0/1/0/all/0/1\">Fran&#xe7;ois Remy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabibullina_A/0/1/0/all/0/1\">Alfiya Khabibullina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"THUIR@COLIEE 2023: Incorporating Structural Knowledge into Pre-trained Language Models for Legal Case Retrieval. (arXiv:2305.06812v1 [cs.IR])","link":"http://arxiv.org/abs/2305.06812","description":"<p>Legal case retrieval techniques play an essential role in modern intelligent\nlegal systems. As an annually well-known international competition, COLIEE is\naiming to achieve the state-of-the-art retrieval model for legal texts. This\npaper summarizes the approach of the championship team THUIR in COLIEE 2023. To\nbe specific, we design structure-aware pre-trained language models to enhance\nthe understanding of legal cases. Furthermore, we propose heuristic\npre-processing and post-processing approaches to reduce the influence of\nirrelevant messages. In the end, learning-to-rank methods are employed to merge\nfeatures with different dimensions. Experimental results demonstrate the\nsuperiority of our proposal. Official results show that our run has the best\nperformance among all submissions. The implementation of our method can be\nfound at https://github.com/CSHaitao/THUIR-COLIEE2023.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haitao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weihang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yueyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiqun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"THUIR@COLIEE 2023: More Parameters and Legal Knowledge for Legal Case Entailment. (arXiv:2305.06817v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06817","description":"<p>This paper describes the approach of the THUIR team at the COLIEE 2023 Legal\nCase Entailment task. This task requires the participant to identify a specific\nparagraph from a given supporting case that entails the decision for the query\ncase. We try traditional lexical matching methods and pre-trained language\nmodels with different sizes. Furthermore, learning-to-rank methods are employed\nto further improve performance. However, learning-to-rank is not very robust on\nthis task. which suggests that answer passages cannot simply be determined with\ninformation retrieval techniques. Experimental results show that more\nparameters and legal knowledge contribute to the legal case entailment task.\nFinally, we get the third place in COLIEE 2023. The implementation of our\nmethod can be found at https://github.com/CSHaitao/THUIR-COLIEE2023.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haitao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weihang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yueyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiqun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Computational Analysis of Suspense: Detecting Dangerous Situations. (arXiv:2305.06818v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06818","description":"<p>Suspense is an important tool in storytelling to keep readers engaged and\nwanting to read more. However, it has so far not been studied extensively in\nComputational Literary Studies. In this paper, we focus on one of the elements\nauthors can use to build up suspense: dangerous situations. We introduce a\ncorpus of texts annotated with dangerous situations, distinguishing between 7\ntypes of danger. Additionally, we annotate parts of the text that describe fear\nexperienced by a character, regardless of the actual presence of danger. We\npresent experiments towards the automatic detection of these situations,\nfinding that unsupervised baseline methods can provide valuable signals for the\ndetection, but more complex methods are necessary for further analysis. Not\nunexpectedly, the description of danger and fear often relies heavily on the\ncontext, both local (e.g., situations where danger is only mentioned, but not\nactually present) and global (e.g., \"storm\" being used in a literal sense in an\nadventure novel, but metaphorically in a romance novel).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zehe_A/0/1/0/all/0/1\">Albin Zehe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroter_J/0/1/0/all/0/1\">Julian Schr&#xf6;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hotho_A/0/1/0/all/0/1\">Andreas Hotho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models. (arXiv:2305.06841v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06841","description":"<p>While the Large Language Models (LLMs) dominate a majority of language\nunderstanding tasks, previous work shows that some of these results are\nsupported by modelling spurious correlations of training datasets. Authors\ncommonly assess model robustness by evaluating their models on\nout-of-distribution (OOD) datasets of the same task, but these datasets might\nshare the bias of the training dataset.\n</p>\n<p>We propose a simple method for measuring a scale of models' reliance on any\nidentified spurious feature and assess the robustness towards a large set of\nknown and newly found prediction biases for various pre-trained models and\ndebiasing methods in Question Answering (QA). We find that the reported OOD\ngains of debiasing methods can not be explained by mitigated reliance on biased\nfeatures, suggesting that biases are shared among QA datasets. We further\nevidence this by measuring that performance of OOD models depends on bias\nfeatures comparably to the ID model, motivating future work to refine the\nreports of LLMs' robustness to a level of known spurious features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mikula_L/0/1/0/all/0/1\">Luk&#xe1;&#x161; Mikula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrovic_M/0/1/0/all/0/1\">Marek Petrovi&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebCPM: Interactive Web Search for Chinese Long-form Question Answering. (arXiv:2305.06849v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06849","description":"<p>Long-form question answering (LFQA) aims at answering complex, open-ended\nquestions with detailed, paragraph-length responses. The de facto paradigm of\nLFQA necessitates two procedures: information retrieval, which searches for\nrelevant supporting facts, and information synthesis, which integrates these\nfacts into a coherent answer. In this paper, we introduce WebCPM, the first\nChinese LFQA dataset. One unique feature of WebCPM is that its information\nretrieval is based on interactive web search, which engages with a search\nengine in real time. Following WebGPT, we develop a web search interface. We\nrecruit annotators to search for relevant information using our interface and\nthen answer questions. Meanwhile, the web search behaviors of our annotators\nwould be recorded. In total, we collect 5,500 high-quality question-answer\npairs, together with 14,315 supporting facts and 121,330 web search actions. We\nfine-tune pre-trained language models to imitate human behaviors for web search\nand to generate answers based on the collected facts. Our LFQA pipeline, built\non these fine-tuned models, generates answers that are no worse than\nhuman-written ones in 32.5% and 47.5% of the cases on our dataset and DuReader,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zihan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Dian Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Lan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shihao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kunlun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Ruobing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IUST_NLP at SemEval-2023 Task 10: Explainable Detecting Sexism with Transformers and Task-adaptive Pretraining. (arXiv:2305.06892v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06892","description":"<p>This paper describes our system on SemEval-2023 Task 10: Explainable\nDetection of Online Sexism (EDOS). This work aims to design an automatic system\nfor detecting and classifying sexist content in online spaces. We propose a set\nof transformer-based pre-trained models with task-adaptive pretraining and\nensemble learning. The main contributions of our system include analyzing the\nperformance of different transformer-based pre-trained models and combining\nthese models, as well as providing an efficient method using large amounts of\nunlabeled data for model adaptive pretraining. We have also explored several\nother strategies. On the test dataset, our system achieves F1-scores of 83%,\n64%, and 47% on subtasks A, B, and C, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahmoudi_H/0/1/0/all/0/1\">Hadiseh Mahmoudi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AfriQA: Cross-lingual Open-Retrieval Question Answering for African Languages. (arXiv:2305.06897v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06897","description":"<p>African languages have far less in-language content available digitally,\nmaking it challenging for question answering systems to satisfy the information\nneeds of users. Cross-lingual open-retrieval question answering (XOR QA)\nsystems -- those that retrieve answer content from other languages while\nserving people in their native language -- offer a means of filling this gap.\nTo this end, we create AfriQA, the first cross-lingual QA dataset with a focus\non African languages. AfriQA includes 12,000+ XOR QA examples across 10 African\nlanguages. While previous datasets have focused primarily on languages where\ncross-lingual QA augments coverage from the target language, AfriQA focuses on\nlanguages where cross-lingual answer content is the only high-coverage source\nof answer content. Because of this, we argue that African languages are one of\nthe most important and realistic use cases for XOR QA. Our experiments\ndemonstrate the poor performance of automatic translation and multilingual\nretrieval methods. Overall, AfriQA proves challenging for state-of-the-art QA\nmodels. We hope that the dataset enables the development of more equitable QA\ntechnology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ogundepo_O/0/1/0/all/0/1\">Odunayo Ogundepo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwadabe_T/0/1/0/all/0/1\">Tajuddeen R. Gwadabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivera_C/0/1/0/all/0/1\">Clara E. Rivera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jonathan H. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DIOP_A/0/1/0/all/0/1\">Abdou Aziz DIOP</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikasote_C/0/1/0/all/0/1\">Claytone Sikasote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hacheme_G/0/1/0/all/0/1\">Gilles Hacheme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buzaaba_H/0/1/0/all/0/1\">Happy Buzaaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezeani_I/0/1/0/all/0/1\">Ignatius Ezeani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mabuya_R/0/1/0/all/0/1\">Rooweither Mabuya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1\">Salomey Osei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahira_A/0/1/0/all/0/1\">Albert Njoroge Kahira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen H. Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oladipo_A/0/1/0/all/0/1\">Akintunde Oladipo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owodunni_A/0/1/0/all/0/1\">Abraham Toluwase Owodunni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonja_A/0/1/0/all/0/1\">Atnafu Lambebo Tonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shode_I/0/1/0/all/0/1\">Iyanuoluwa Shode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajayi_T/0/1/0/all/0/1\">Tunde Oluwaseyi Ajayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siro_C/0/1/0/all/0/1\">Clemencia Siro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arthur_S/0/1/0/all/0/1\">Steven Arthur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeyemi_M/0/1/0/all/0/1\">Mofetoluwa Adeyemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahia_O/0/1/0/all/0/1\">Orevaoghene Ahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anuoluwapo_A/0/1/0/all/0/1\">Aremu Anuoluwapo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awosan_O/0/1/0/all/0/1\">Oyinkansola Awosan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chukwuneke_C/0/1/0/all/0/1\">Chiamaka Chukwuneke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opoku_B/0/1/0/all/0/1\">Bernard Opoku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayodele_A/0/1/0/all/0/1\">Awokoya Ayodele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otiende_V/0/1/0/all/0/1\">Verrah Otiende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mwase_C/0/1/0/all/0/1\">Christine Mwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinkala_B/0/1/0/all/0/1\">Boyd Sinkala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubungo_A/0/1/0/all/0/1\">Andre Niyongabo Rubungo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajisafe_D/0/1/0/all/0/1\">Daniel A. Ajisafe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onwuegbuzia_E/0/1/0/all/0/1\">Emeka Felix Onwuegbuzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mbow_H/0/1/0/all/0/1\">Habib Mbow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niyomutabazi_E/0/1/0/all/0/1\">Emile Niyomutabazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukonde_E/0/1/0/all/0/1\">Eunice Mukonde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawan_F/0/1/0/all/0/1\">Falalu Ibrahim Lawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Ibrahim Said Ahmad</a>, et al. (9 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model. (arXiv:2305.06908v1 [cs.SD])","link":"http://arxiv.org/abs/2305.06908","description":"<p>Denoising diffusion probabilistic models (DDPMs) have shown promising\nperformance for speech synthesis. However, a large number of iterative steps\nare required to achieve high sample quality, which restricts the inference\nspeed. Maintaining sample quality while increasing sampling speed has become a\nchallenging task. In this paper, we propose a \"Co\"nsistency \"Mo\"del-based\n\"Speech\" synthesis method, CoMoSpeech, which achieve speech synthesis through a\nsingle diffusion sampling step while achieving high audio quality. The\nconsistency constraint is applied to distill a consistency model from a\nwell-designed diffusion-based teacher model, which ultimately yields superior\nperformances in the distilled CoMoSpeech. Our experiments show that by\ngenerating audio recordings by a single sampling step, the CoMoSpeech achieves\nan inference speed more than 150 times faster than real-time on a single NVIDIA\nA100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based\nspeech synthesis truly practical. Meanwhile, objective and subjective\nevaluations on text-to-speech and singing voice synthesis show that the\nproposed teacher models yield the best audio quality, and the one-step sampling\nbased CoMoSpeech achieves the best inference speed with better or comparable\naudio quality to other conventional multi-step diffusion model baselines. Audio\nsamples are available at https://comospeech.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zhen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition. (arXiv:2305.06934v1 [cs.SE])","link":"http://arxiv.org/abs/2305.06934","description":"<p>Since the release of ChatGPT, numerous studies have highlighted the\nremarkable performance of ChatGPT, which often rivals or even surpasses human\ncapabilities in various tasks and domains. However, this paper presents a\ncontrasting perspective by demonstrating an instance where human performance\nexcels in typical tasks suited for ChatGPT, specifically in the domain of\ncomputer programming. We utilize the IEEExtreme Challenge competition as a\nbenchmark, a prestigious, annual international programming contest encompassing\na wide range of problems with different complexities. To conduct a thorough\nevaluation, we selected and executed a diverse set of 102 challenges, drawn\nfrom five distinct IEEExtreme editions, using three major programming\nlanguages: Python, Java, and C++. Our empirical analysis provides evidence that\ncontrary to popular belief, human programmers maintain a competitive edge over\nChatGPT in certain aspects of problem-solving within the programming context.\nIn fact, we found that the average score obtained by ChatGPT on the set of\nIEEExtreme programming problems is 3.9 to 5.8 times lower than the average\nhuman score, depending on the programming language. This paper elaborates on\nthese findings, offering critical insights into the limitations and potential\nareas of improvement for AI-based language models like ChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koubaa_A/0/1/0/all/0/1\">Anis Koubaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qureshi_B/0/1/0/all/0/1\">Basit Qureshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammar_A/0/1/0/all/0/1\">Adel Ammar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zahid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulila_W/0/1/0/all/0/1\">Wadii Boulila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghouti_L/0/1/0/all/0/1\">Lahouari Ghouti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Retrieval Augmented Generation. (arXiv:2305.06983v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06983","description":"<p>Despite the remarkable ability of large language models (LMs) to comprehend\nand generate language, they have a tendency to hallucinate and create factually\ninaccurate output. Augmenting LMs by retrieving information from external\nknowledge resources is one promising solution. Most existing\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\nretrieves information once based on the input. This is limiting, however, in\nmore general scenarios involving generation of long texts, where continually\ngathering information throughout the generation process is essential. There\nhave been some past efforts to retrieve information multiple times while\ngenerating outputs, which mostly retrieve documents at fixed intervals using\nthe previous context as queries. In this work, we provide a generalized view of\nactive retrieval augmented generation, methods that actively decide when and\nwhat to retrieve across the course of the generation. We propose\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\nretrieval-augmented generation method which iteratively uses a prediction of\nthe upcoming sentence to anticipate future content, which is then utilized as a\nquery to retrieve relevant documents to regenerate the sentence if it contains\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\nsuperior or competitive performance on all tasks, demonstrating the\neffectiveness of our method. Code and datasets are available at\nhttps://github.com/jzbjyb/FLARE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengbao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhiqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_Yu_J/0/1/0/all/0/1\">Jane Dwivedi-Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1\">Jamie Callan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Open-Domain Question Answering in the Era of Large Language Models. (arXiv:2305.06984v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06984","description":"<p>Lexical matching remains the de facto evaluation method for open-domain\nquestion answering (QA). Unfortunately, lexical matching fails completely when\na plausible candidate answer does not appear in the list of gold answers, which\nis increasingly the case as we shift from extractive to generative models. The\nrecent success of large language models (LLMs) for QA aggravates lexical\nmatching failures since candidate answers become longer, thereby making\nmatching with the gold answers even more challenging. Without accurate\nevaluation, the true progress in open-domain QA remains unknown. In this paper,\nwe conduct a thorough analysis of various open-domain QA models, including\nLLMs, by manually evaluating their answers on a subset of NQ-open, a popular\nbenchmark. Our assessments reveal that while the true performance of all models\nis significantly underestimated, the performance of the InstructGPT (zero-shot)\nLLM increases by nearly +60%, making it on par with existing top models, and\nthe InstructGPT (few-shot) model actually achieves a new state-of-the-art on\nNQ-open. We also find that more than 50% of lexical matching failures are\nattributed to semantically equivalent answers. We further demonstrate that\nregex matching ranks QA models consistent with human judgments, although still\nsuffering from unnecessary strictness. Finally, we demonstrate that automated\nevaluation models are a reasonable surrogate for lexical matching in some\ncircumstances, but not for long-form answers generated by LLMs. The automated\nmodels struggle in detecting hallucinations in LLM answers and are thus unable\nto evaluate LLMs. At this time, there appears to be no substitute for human\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamalloo_E/0/1/0/all/0/1\">Ehsan Kamalloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarke_C/0/1/0/all/0/1\">Charles L. A. Clarke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafiei_D/0/1/0/all/0/1\">Davood Rafiei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Chained Image-Language Model for Video Localization and Question Answering. (arXiv:2305.06988v1 [cs.CV])","link":"http://arxiv.org/abs/2305.06988","description":"<p>Recent studies have shown promising results on utilizing pre-trained\nimage-language models for video question answering. While these image-language\nmodels can efficiently bootstrap the representation learning of video-language\nmodels, they typically concatenate uniformly sampled video frames as visual\ninputs without explicit language-aware, temporal modeling. When only a portion\nof a video input is relevant to the language query, such uniform frame sampling\ncan often lead to missing important visual cues. Although humans often find a\nvideo moment to focus on and rewind the moment to answer questions, training a\nquery-aware video moment localizer often requires expensive annotations and\nhigh computational costs. To address this issue, we propose Self-Chained Video\nLocalization-Answering (SeViLA), a novel framework that leverages a single\nimage-language model (BLIP-2) to tackle both temporal keyframe localization and\nQA on videos. SeViLA framework consists of two modules: Localizer and Answerer,\nwhere both are parameter-efficiently fine-tuned from BLIP-2. We chain these\nmodules for cascaded inference and self-refinement. First, in the forward\nchain, the Localizer finds multiple language-aware keyframes in a video, which\nthe Answerer uses to predict the answer. Second, in the reverse chain, the\nAnswerer generates keyframe pseudo-labels to refine the Localizer, alleviating\nthe need for expensive video moment localization annotations. SeViLA\noutperforms several strong baselines/previous works on five video QA and event\nprediction tasks, and achieves the state-of-the-art in both fine-tuning\n(NExT-QA, STAR) and zero-shot (NExT-QA, STAR, How2QA, VLEP) settings. We show a\ncomprehensive analysis, e.g., the impact of Localizer, comparisons of Localizer\nwith other temporal localization models, pre-training/self-refinement of\nLocalizer, and varying the number of keyframes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shoubin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1\">Prateek Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMATCH++: Standardized and Extended Evaluation of Semantic Graphs. (arXiv:2305.06993v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06993","description":"<p>The Smatch metric is a popular method for evaluating graph distances, as is\nnecessary, for instance, to assess the performance of semantic graph parsing\nsystems. However, we observe some issues in the metric that jeopardize\nmeaningful evaluation. E.g., opaque pre-processing choices can affect results,\nand current graph-alignment solvers do not provide us with upper-bounds.\nWithout upper-bounds, however, fair evaluation is not guaranteed. Furthermore,\nadaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity)\nare spread out, and lack a unifying framework.\n</p>\n<p>For better inspection, we divide the metric into three modules:\npre-processing, alignment, and scoring. Examining each module, we specify its\ngoals and diagnose potential issues, for which we discuss and test mitigation\nstrategies. For pre-processing, we show how to fully conform to annotation\nguidelines that allow structurally deviating but valid graphs. For safer and\nenhanced alignment, we show the feasibility of optimal alignment in a standard\nevaluation setup, and develop a lossless graph compression method that shrinks\nthe search space and significantly increases efficiency. For improved scoring,\nwe propose standardized and extended metric calculation of fine-grained\nsub-graph meaning aspects. Our code is available at\nhttps://github.com/flipz357/smatchpp\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1\">Juri Opitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach. (arXiv:2305.07001v1 [cs.IR])","link":"http://arxiv.org/abs/2305.07001","description":"<p>In the past decades, recommender systems have attracted much attention in\nboth research and industry communities, and a large number of studies have been\ndevoted to developing effective recommendation models. Basically speaking,\nthese models mainly learn the underlying user preference from historical\nbehavior data, and then estimate the user-item matching relationships for\nrecommendations. Inspired by the recent progress on large language models\n(LLMs), we take a different approach to developing the recommendation models,\nconsidering recommendation as instruction following by LLMs. The key idea is\nthat the preferences or needs of a user can be expressed in natural language\ndescriptions (called instructions), so that LLMs can understand and further\nexecute the instruction for fulfilling the recommendation task. Instead of\nusing public APIs of LLMs, we instruction tune an open-source LLM (3B\nFlan-T5-XL), in order to better adapt LLMs to recommender systems. For this\npurpose, we first design a general instruction format for describing the\npreference, intention, task form and context of a user in natural language.\nThen we manually design 39 instruction templates and automatically generate a\nlarge amount of user-personalized instruction data (252K instructions) with\nvarying types of preferences and intentions. To demonstrate the effectiveness\nof our approach, we instantiate the instruction templates into several\nwidely-studied recommendation (or search) tasks, and conduct extensive\nexperiments on these tasks with real-world datasets. Experiment results show\nthat the proposed approach can outperform several competitive baselines,\nincluding the powerful GPT-3.5, on these evaluation tasks. Our approach sheds\nlight on developing more user-friendly recommender systems, in which users can\nfreely communicate with the system and obtain more accurate recommendations via\nnatural language instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Ruobing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yupeng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Leyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting. (arXiv:2305.07004v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07004","description":"<p>Large language models (LLMs) demonstrate impressive multilingual capability,\nbut their performance varies substantially across different languages. In this\nwork, we introduce a simple yet effective method, called cross-lingual-thought\nprompting (XLT), to systematically improve the multilingual capability of LLMs.\nSpecifically, XLT is a generic template prompt that stimulates cross-lingual\nand logical reasoning skills to enhance task performance across languages. We\nconduct comprehensive evaluations on 7 typical benchmarks related to reasoning,\nunderstanding, and generation tasks, covering both high-resource and\nlow-resource languages. Experimental results show that XLT not only remarkably\nenhances the performance of various multilingual tasks but also significantly\nreduces the gap between the average performance and the best performance of\neach task in different languages. Notably, XLT brings over 10 points of average\nimprovement in arithmetic reasoning and open-domain question-answering tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1\">Ting Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation. (arXiv:2305.07005v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07005","description":"<p>Subword segmenters like BPE operate as a preprocessing step in neural machine\ntranslation and other (conditional) language models. They are applied to\ndatasets before training, so translation or text generation quality relies on\nthe quality of segmentations. We propose a departure from this paradigm, called\nsubword segmental machine translation (SSMT). SSMT unifies subword segmentation\nand MT in a single trainable model. It learns to segment target sentence words\nwhile jointly learning to generate target sentences. To use SSMT during\ninference we propose dynamic decoding, a text generation algorithm that adapts\nsegmentations as it generates translations. Experiments across 6 translation\ndirections show that SSMT improves chrF scores for morphologically rich\nagglutinative languages. Gains are strongest in the very low-resource scenario.\nSSMT also learns subwords that are closer to morphemes compared to baselines\nand proves more robust on a test set constructed for evaluating morphological\ncompositional generalisation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meyer_F/0/1/0/all/0/1\">Francois Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buys_J/0/1/0/all/0/1\">Jan Buys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2305.07011v1 [cs.CV])","link":"http://arxiv.org/abs/2305.07011","description":"<p>We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a\ncontrastive image-text pretraining recipe to bridge the gap between image-level\npretraining and open-vocabulary object detection. At the pretraining phase, we\npropose to randomly crop and resize regions of positional embeddings instead of\nusing the whole image positional embeddings. This better matches the use of\npositional embeddings at region-level in the detection finetuning phase. In\naddition, we replace the common softmax cross entropy loss in contrastive\nlearning with focal loss to better learn the informative yet difficult\nexamples. Finally, we leverage recent advances in novel object proposals to\nimprove open-vocabulary detection finetuning. We evaluate our full model on the\nLVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer.\nRO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the best\nexisting approach by +5.8 points in addition to competitive zero-shot transfer\ndetection. Surprisingly, RO-ViT improves the image-level representation as well\nand achieves the state of the art on 9 out of 12 metrics on COCO and Flickr\nimage-text retrieval benchmarks, outperforming competitive approaches with\nlarger models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1\">Weicheng Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A General-Purpose Multilingual Document Encoder. (arXiv:2305.07016v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07016","description":"<p>Massively multilingual pretrained transformers (MMTs) have tremendously\npushed the state of the art on multilingual NLP and cross-lingual transfer of\nNLP models in particular. While a large body of work leveraged MMTs to mine\nparallel data and induce bilingual document embeddings, much less effort has\nbeen devoted to training general-purpose (massively) multilingual document\nencoder that can be used for both supervised and unsupervised document-level\ntasks. In this work, we pretrain a massively multilingual document encoder as a\nhierarchical transformer model (HMDE) in which a shallow document transformer\ncontextualizes sentence representations produced by a state-of-the-art\npretrained multilingual sentence encoder. We leverage Wikipedia as a readily\navailable source of comparable documents for creating training data, and train\nHMDE by means of a cross-lingual contrastive objective, further exploiting the\ncategory hierarchy of Wikipedia for creation of difficult negatives. We\nevaluate the effectiveness of HMDE in two arguably most common and prominent\ncross-lingual document-level tasks: (1) cross-lingual transfer for topical\ndocument classification and (2) cross-lingual document retrieval. HMDE is\nsignificantly more effective than (i) aggregations of segment-based\nrepresentations and (ii) multilingual Longformer. Crucially, owing to its\nmassively multilingual lower transformer, HMDE successfully generalizes to\nlanguages unseen in document-level pretraining. We publicly release our code\nand models at\nhttps://github.com/ogaloglu/pre-training-multilingual-document-encoders .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galoglu_O/0/1/0/all/0/1\">Onur Galo&#x11f;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litschko_R/0/1/0/all/0/1\">Robert Litschko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Categorical Vector Space Semantics for Lambek Calculus with a Relevant Modality. (arXiv:2005.03074v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.03074","description":"<p>We develop a categorical compositional distributional semantics for Lambek\nCalculus with a Relevant Modality !L*, which has a limited edition of the\ncontraction and permutation rules. The categorical part of the semantics is a\nmonoidal biclosed category with a coalgebra modality, very similar to the\nstructure of a Differential Category. We instantiate this category to finite\ndimensional vector spaces and linear maps via \"quantisation\" functors and work\nwith three concrete interpretations of the coalgebra modality. We apply the\nmodel to construct categorical and concrete semantic interpretations for the\nmotivating example of !L*: the derivation of a phrase with a parasitic gap. The\neffectiveness of the concrete interpretations are evaluated via a\ndisambiguation task, on an extension of a sentence disambiguation dataset to\nparasitic gap phrases, using BERT, Word2Vec, and FastText vectors and\nRelational tensors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McPheat_L/0/1/0/all/0/1\">Lachlan McPheat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1\">Mehrnoosh Sadrzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wazni_H/0/1/0/all/0/1\">Hadi Wazni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijnholds_G/0/1/0/all/0/1\">Gijs Wijnholds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Transformer Decoder with Compressed Sub-layers. (arXiv:2101.00542v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00542","description":"<p>The large attention-based encoder-decoder network (Transformer) has become\nprevailing recently due to its effectiveness. But the high computation\ncomplexity of its decoder raises the inefficiency issue. By examining the\nmathematic formulation of the decoder, we show that under some mild conditions,\nthe architecture could be simplified by compressing its sub-layers, the basic\nbuilding block of Transformer, and achieves a higher parallelism. We thereby\npropose Compressed Attention Network, whose decoder layer consists of only one\nsub-layer instead of three. Extensive experiments on 14 WMT machine translation\ntasks show that our model is 1.42x faster with performance on par with a strong\nbaseline. This strong baseline is already 2x faster than the widely used\nstandard baseline without loss in performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars. (arXiv:2109.04939v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04939","description":"<p>In computational linguistics, it has been shown that hierarchical structures\nmake language models (LMs) more human-like. However, the previous literature\nhas been agnostic about a parsing strategy of the hierarchical models. In this\npaper, we investigated whether hierarchical structures make LMs more\nhuman-like, and if so, which parsing strategy is most cognitively plausible. In\norder to address this question, we evaluated three LMs against human reading\ntimes in Japanese with head-final left-branching structures: Long Short-Term\nMemory (LSTM) as a sequential model and Recurrent Neural Network Grammars\n(RNNGs) with top-down and left-corner parsing strategies as hierarchical\nmodels. Our computational modeling demonstrated that left-corner RNNGs\noutperformed top-down RNNGs and LSTM, suggesting that hierarchical and\nleft-corner architectures are more cognitively plausible than top-down or\nsequential architectures. In addition, the relationships between the cognitive\nplausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be\ndiscussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_R/0/1/0/all/0/1\">Ryo Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noji_H/0/1/0/all/0/1\">Hiroshi Noji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseki_Y/0/1/0/all/0/1\">Yohei Oseki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification. (arXiv:2203.11155v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.11155","description":"<p>Quantum density matrix represents all the information of the entire quantum\nsystem, and novel models of meaning employing density matrices naturally model\nlinguistic phenomena such as hyponymy and linguistic ambiguity, among others in\nquantum question answering tasks. Naturally, we argue that applying the quantum\ndensity matrix into classical Question Answering (QA) tasks can show more\neffective performance. Specifically, we (i) design a new mechanism based on\nLong Short-Term Memory (LSTM) to accommodate the case when the inputs are\nmatrixes; (ii) apply the new mechanism to QA problems with Convolutional Neural\nNetwork (CNN) and gain the LSTM-based QA model with the quantum density matrix.\nExperiments of our new model on TREC-QA and WIKI-QA data sets show encouraging\nresults. Similarly, we argue that the quantum density matrix can also enhance\nthe image feature information and the relationship between the features for the\nclassical image classification. Thus, we (i) combine density matrices and CNN\nto design a new mechanism; (ii) apply the new mechanism to some representative\nclassical image classification tasks. A series of experiments show that the\napplication of quantum density matrix in image classification has the\ngeneralization and high efficiency on different datasets. The application of\nquantum density matrix both in classical question answering tasks and classical\nimage classification tasks show more effective performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">X. Q. Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">H. Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language modeling via stochastic processes. (arXiv:2203.11370v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.11370","description":"<p>Modern language models can generate high-quality short texts. However, they\noften meander or are incoherent when generating longer texts. These issues\narise from the next-token-only language modeling objective. Recent work in\nself-supervised learning suggests that models can learn good latent\nrepresentations via contrastive learning, which can be effective for\ndiscriminative tasks. Our work analyzes the application of contrastive\nrepresentations for generative tasks, like long text generation. We propose one\napproach for leveraging constrastive representations, which we call Time\nControl (TC). TC first learns a contrastive representation of the target text\ndomain, then generates text by decoding from these representations. Compared to\ndomain-specific methods and fine-tuning GPT2 across a variety of text domains,\nTC performs competitively to methods specific for learning sentence\nrepresentations on discourse coherence. On long text generation settings, TC\npreserves the text structure both in terms of ordering (up to $+15\\%$ better)\nand text length consistency (up to $+90\\%$ better).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rose E Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysing similarities between legal court documents using natural language processing approaches based on Transformers. (arXiv:2204.07182v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2204.07182","description":"<p>Recent advances in Artificial Intelligence (AI) have leveraged promising\nresults in solving complex problems in the area of Natural Language Processing\n(NLP), being an important tool to help in the expeditious resolution of\njudicial proceedings in the legal area. In this context, this work targets the\nproblem of detecting the degree of similarity between judicial documents that\ncan be achieved in the inference group, by applying six NLP techniques based on\nthe transformers architecture to a case study of legal proceedings in the\nBrazilian judicial system. The NLP transformer-based models, namely BERT, GPT-2\nand RoBERTa, were pre-trained using a general purpose corpora of the Brazilian\nPortuguese language, and then were fine-tuned and specialised for the legal\nsector using 210,000 legal proceedings. Vector representations of each legal\ndocument were calculated based on their embeddings, which were used to cluster\nthe lawsuits, calculating the quality of each model based on the cosine of the\ndistance between the elements of the group to its centroid. We noticed that\nmodels based on transformers presented better performance when compared to\nprevious traditional NLP techniques, with the RoBERTa model specialised for the\nBrazilian Portuguese language presenting the best results. This methodology can\nbe also applied to other case studies for different languages, making it\npossible to advance in the current state of the art in the area of NLP applied\nto the legal sector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_R/0/1/0/all/0/1\">Raphael Souza de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascimento_E/0/1/0/all/0/1\">Erick Giovani Sperandio Nascimento</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating the Personality of White-Box Language Models. (arXiv:2204.12000v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.12000","description":"<p>Technology for open-ended language generation, a key application of\nartificial intelligence, has advanced to a great extent in recent years.\nLarge-scale language models, which are trained on large corpora of text, are\nbeing used in a wide range of applications everywhere, from virtual assistants\nto conversational bots. While these language models output fluent text,\nexisting research shows that these models can and do capture human biases. Many\nof these biases, especially those that could potentially cause harm, are being\nwell-investigated. On the other hand, studies that infer and change human\npersonality traits inherited by these models have been scarce or non-existent.\nOur work seeks to address this gap by exploring the personality traits of\nseveral large-scale language models designed for open-ended text generation and\nthe datasets used for training them. We build on the popular Big Five factors\nand develop robust methods that quantify the personality traits of these models\nand their underlying datasets. In particular, we trigger the models with a\nquestionnaire designed for personality assessment and subsequently classify the\ntext responses into quantifiable traits using a Zero-shot classifier. Our\nestimation scheme sheds light on an important anthropomorphic element found in\nsuch AI models and can help stakeholders decide how they should be applied as\nwell as how society could perceive them. Additionally, we examined approaches\nto alter these personalities, adding to our understanding of how AI models can\nbe adapted to specific contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karra_S/0/1/0/all/0/1\">Saketh Reddy Karra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1\">Son The Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulabandhula_T/0/1/0/all/0/1\">Theja Tulabandhula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minority Stress Experienced by LGBTQ Online Communities during the COVID-19 Pandemic. (arXiv:2205.09511v3 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2205.09511","description":"<p>The COVID-19 pandemic has disproportionately impacted the lives of\nminorities, such as members of the LGBTQ community (lesbian, gay, bisexual,\ntransgender, and queer) due to pre-existing social disadvantages and health\ndisparities. Although extensive research has been carried out on the impact of\nthe COVID-19 pandemic on different aspects of the general population's lives,\nfew studies are focused on the LGBTQ population. In this paper, we develop and\nevaluate two sets of machine learning classifiers using a pre-pandemic and a\nduring-pandemic dataset to identify Twitter posts exhibiting minority stress,\nwhich is a unique pressure faced by the members of the LGBTQ population due to\ntheir sexual and gender identities. We demonstrate that our best pre- and\nduring-pandemic models show strong and stable performance for detecting posts\nthat contain minority stress. We investigate the linguistic differences in\nminority stress posts across pre- and during-pandemic periods. We find that\nanger words are strongly associated with minority stress during the COVID-19\npandemic. We explore the impact of the pandemic on the emotional states of the\nLGBTQ population by adopting propensity score-based matching to perform a\ncausal analysis. The results show that the LGBTQ population have a greater\nincrease in the usage of cognitive words and worsened observable attribute in\nthe usage of positive emotion words than the group of the general population\nwith similar pre-pandemic behavioral attributes. Our findings have implications\nfor the public health domain and policy-makers to provide adequate support,\nespecially with respect to mental health, to the LGBTQ population during future\ncrises.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yunhao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_G/0/1/0/all/0/1\">Gaurav Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_B/0/1/0/all/0/1\">Barbara Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aledavood_T/0/1/0/all/0/1\">Talayeh Aledavood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law. (arXiv:2209.06049v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.06049","description":"<p>NLP in the legal domain has seen increasing success with the emergence of\nTransformer-based Pre-trained Language Models (PLMs) pre-trained on legal text.\nPLMs trained over European and US legal text are available publicly; however,\nlegal text from other domains (countries), such as India, have a lot of\ndistinguishing characteristics. With the rapidly increasing volume of Legal NLP\napplications in various countries, it has become necessary to pre-train such\nLMs over legal text of other countries as well. In this work, we attempt to\ninvestigate pre-training in the Indian legal domain. We re-train (continue\npre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian\nlegal data, as well as train a model from scratch with a vocabulary based on\nIndian legal text. We apply these PLMs over three benchmark legal NLP tasks --\nLegal Statute Identification from facts, Semantic Segmentation of Court\nJudgment Documents, and Court Appeal Judgment Prediction -- over both Indian\nand non-Indian (EU, UK) datasets. We observe that our approach not only\nenhances performance on the new domain (Indian texts) but also over the\noriginal domain (European and UK texts). We also conduct explainability\nexperiments for a qualitative comparison of all these different PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Shounak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_A/0/1/0/all/0/1\">Arpan Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Full-Text Content to Characterize and Identify Best Seller Books. (arXiv:2210.02334v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02334","description":"<p>Artistic pieces can be studied from several perspectives, one example being\ntheir reception among readers over time. In the present work, we approach this\ninteresting topic from the standpoint of literary works, particularly assessing\nthe task of predicting whether a book will become a best seller. Dissimilarly\nfrom previous approaches, we focused on the full content of books and\nconsidered visualization and classification tasks. We employed visualization\nfor the preliminary exploration of the data structure and properties, involving\nSemAxis and linear discriminant analyses. Then, to obtain quantitative and more\nobjective results, we employed various classifiers. Such approaches were used\nalong with a dataset containing (i) books published from 1895 to 1924 and\nconsecrated as best sellers by the Publishers Weekly Bestseller Lists and (ii)\nliterary works published in the same period but not being mentioned in that\nlist. Our comparison of methods revealed that the best-achieved result -\ncombining a bag-of-words representation with a logistic regression classifier -\nled to an average accuracy of 0.75 both for the leave-one-out and 10-fold\ncross-validations. Such an outcome suggests that it is unfeasible to predict\nthe success of books with high accuracy using only the full content of the\ntexts. Nevertheless, our findings provide insights into the factors leading to\nthe relative success of a literary work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_G/0/1/0/all/0/1\">Giovana D. da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_F/0/1/0/all/0/1\">Filipi N. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arruda_H/0/1/0/all/0/1\">Henrique F. de Arruda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souza_B/0/1/0/all/0/1\">B&#xe1;rbara C. e Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_L/0/1/0/all/0/1\">Luciano da F. Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amancio_D/0/1/0/all/0/1\">Diego R. Amancio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Framework based Query Generation for Temporal Question Answering over Knowledge Graphs. (arXiv:2210.04490v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04490","description":"<p>Answering factual questions with temporal intent over knowledge graphs\n(temporal KGQA) attracts rising attention in recent years. In the generation of\ntemporal queries, existing KGQA methods ignore the fact that some intrinsic\nconnections between events can make them temporally related, which may limit\ntheir capability. We systematically analyze the possible interpretation of\ntemporal constraints and conclude the interpretation structures as the Semantic\nFramework of Temporal Constraints, SF-TCons. Based on the semantic framework,\nwe propose a temporal question answering method, SF-TQA, which generates query\ngraphs by exploring the relevant facts of mentioned entities, where the\nexploring process is restricted by SF-TCons. Our evaluations show that SF-TQA\nsignificantly outperforms existing methods on two benchmarks over different\nknowledge graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wentao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating self-supervised, weakly supervised and fully supervised training approaches for multi-domain automatic speech recognition: a study on Bangladeshi Bangla. (arXiv:2210.12921v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12921","description":"<p>Despite huge improvements in automatic speech recognition (ASR) employing\nneural networks, ASR systems still suffer from a lack of robustness and\ngeneralizability issues due to domain shifting. This is mainly because\nprincipal corpus design criteria are often not identified and examined\nadequately while compiling ASR datasets. In this study, we investigate the\nrobustness of the state-of-the-art transfer learning approaches such as\nself-supervised wav2vec 2.0 and weakly supervised Whisper as well as fully\nsupervised convolutional neural networks (CNNs) for multi-domain ASR. We also\ndemonstrate the significance of domain selection while building a corpus by\nassessing these models on a novel multi-domain Bangladeshi Bangla ASR\nevaluation benchmark - BanSpeech, which contains approximately 6.52 hours of\nhuman-annotated speech and 8085 utterances from 13 distinct domains. SUBAK.KO,\na mostly read speech corpus for the morphologically rich language Bangla, has\nbeen used to train the ASR systems. Experimental evaluation reveals that\nself-supervised cross-lingual pre-training is the best strategy compared to\nweak supervision and full supervision to tackle the multi-domain ASR task.\nMoreover, the ASR models trained on SUBAK.KO face difficulty recognizing speech\nfrom domains with mostly spontaneous speech. The BanSpeech will be publicly\navailable to meet the need for a challenging evaluation benchmark for Bangla\nASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samin_A/0/1/0/all/0/1\">Ahnaf Mozib Samin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobir_M/0/1/0/all/0/1\">M. Humayon Kobir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafee_M/0/1/0/all/0/1\">Md. Mushtaq Shahriyar Rafee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1\">M. Firoz Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mehedi Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1\">Partha Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kibria_S/0/1/0/all/0/1\">Shafkat Kibria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">M. Shahidur Rahman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Composition, Attention, or Both?. (arXiv:2210.12958v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12958","description":"<p>In this paper, we propose a novel architecture called Composition Attention\nGrammars (CAGs) that recursively compose subtrees into a single vector\nrepresentation with a composition function, and selectively attend to previous\nstructural information with a self-attention mechanism. We investigate whether\nthese components -- the composition function and the self-attention mechanism\n-- can both induce human-like syntactic generalization. Specifically, we train\nlanguage models (LMs) with and without these two components with the model\nsizes carefully controlled, and evaluate their syntactic generalization\nperformance against six test circuits on the SyntaxGym benchmark. The results\ndemonstrated that the composition function and the self-attention mechanism\nboth play an important role to make LMs more human-like, and closer inspection\nof linguistic phenomenon implied that the composition function allowed\nsyntactic features, but not semantic features, to percolate into subtree\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_R/0/1/0/all/0/1\">Ryo Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseki_Y/0/1/0/all/0/1\">Yohei Oseki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Tuning Language Models via Epistemic Neural Networks. (arXiv:2211.01568v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01568","description":"<p>Language models often pre-train on large unsupervised text corpora, then\nfine-tune on additional task-specific data. However, typical fine-tuning\nschemes do not prioritize the examples that they tune on. We show that, if you\ncan prioritize informative training data, you can achieve better performance\nwhile using fewer labels. To do this we augment a language model with an\nepinet: a small additional network that helps to estimate model uncertainty and\nforms an \\textit{epistemic neural network} (ENN). ENNs are neural networks that\ncan know what they don't know. Using an epinet to prioritize uncertain data, we\ncan fine-tune BERT on GLUE tasks to the same performance while using 2x less\ndata than training without prioritization. We also investigate performance in\nsynthetic neural network generative models designed to build understanding. In\neach setting, using an epinet outperforms heuristic active learning schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1\">Ian Osband</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asghari_S/0/1/0/all/0/1\">Seyed Mohammad Asghari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1\">Benjamin Van Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAleese_N/0/1/0/all/0/1\">Nat McAleese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aslanides_J/0/1/0/all/0/1\">John Aslanides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1\">Geoffrey Irving</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding Omission in Dialogue Summarization. (arXiv:2211.07145v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07145","description":"<p>Dialogue summarization aims to condense the lengthy dialogue into a concise\nsummary, and has recently achieved significant progress. However, the result of\nexisting methods is still far from satisfactory. Previous works indicated that\nomission is a major factor in affecting the quality of summarization, but few\nof them have further explored the omission problem, such as how omission\naffects summarization results and how to detect omission, which is critical for\nreducing omission and improving summarization quality. Moreover, analyzing and\ndetecting omission relies on summarization datasets with omission labels (i.e.,\nwhich dialogue utterances are omitted in the summarization), which are not\navailable in the current literature. In this paper, we propose the OLDS\ndataset, which provides high-quality Omission Labels for Dialogue\nSummarization. By analyzing this dataset, we find that a large improvement in\nsummarization quality can be achieved by providing ground-truth omission labels\nfor the summarization model to recover omission information, which demonstrates\nthe importance of omission detection for omission mitigation in dialogue\nsummarization. Therefore, we formulate an omission detection task and\ndemonstrate our proposed dataset can support the training and evaluation of\nthis task well. We also call for research action on omission detection based on\nour proposed datasets. Our dataset and codes are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhongkai Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery. (arXiv:2211.08316v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08316","description":"<p>Understanding users' intentions in e-commerce platforms requires commonsense\nknowledge. In this paper, we present FolkScope, an intention knowledge graph\nconstruction framework to reveal the structure of humans' minds about\npurchasing items. As commonsense knowledge is usually ineffable and not\nexpressed explicitly, it is challenging to perform information extraction.\nThus, we propose a new approach that leverages the generation power of large\nlanguage models~(LLMs) and human-in-the-loop annotation to semi-automatically\nconstruct the knowledge graph. LLMs first generate intention assertions via\ne-commerce-specific prompts to explain shopping behaviors, where the intention\ncan be an open reason or a predicate falling into one of 18 categories aligning\nwith ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibility\nand typicality labels of sampled intentions as training data in order to\npopulate human judgments to all automatic generations. Last, to structurize the\nassertions, we propose pattern mining and conceptualization to form more\ncondensed and abstract knowledge. Extensive evaluations and studies demonstrate\nthat our constructed knowledge graph can well model e-commerce knowledge and\nhave many potential applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaxin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yifan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency Methods. (arXiv:2211.08369v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08369","description":"<p>A popular approach to unveiling the black box of neural NLP models is to\nleverage saliency methods, which assign scalar importance scores to each input\ncomponent. A common practice for evaluating whether an interpretability method\nis faithful has been to use evaluation-by-agreement -- if multiple methods\nagree on an explanation, its credibility increases. However, recent work has\nfound that saliency methods exhibit weak rank correlations even when applied to\nthe same model instance and advocated for the use of alternative diagnostic\nmethods. In our work, we demonstrate that rank correlation is not a good fit\nfor evaluating agreement and argue that Pearson-$r$ is a better-suited\nalternative. We further show that regularization techniques that increase\nfaithfulness of attention explanations also increase agreement between saliency\nmethods. By connecting our findings to instance categories based on training\ndynamics, we show that the agreement of saliency method explanations is very\nlow for easy-to-learn instances. Finally, we connect the improvement in\nagreement across instance categories to local representation space statistics\nof instances, paving the way for work on analyzing which intrinsic model\nproperties improve their predisposition to interpretability methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jukic_J/0/1/0/all/0/1\">Josip Juki&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutek_M/0/1/0/all/0/1\">Martin Tutek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1\">Jan &#x160;najder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streaming Joint Speech Recognition and Disfluency Detection. (arXiv:2211.08726v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08726","description":"<p>Disfluency detection has mainly been solved in a pipeline approach, as\npost-processing of speech recognition. In this study, we propose\nTransformer-based encoder-decoder models that jointly solve speech recognition\nand disfluency detection, which work in a streaming manner. Compared to\npipeline approaches, the joint models can leverage acoustic information that\nmakes disfluency detection robust to recognition errors and provide non-verbal\nclues. Moreover, joint modeling results in low-latency and lightweight\ninference. We investigate two joint model variants for streaming disfluency\ndetection: a transcript-enriched model and a multi-task model. The\ntranscript-enriched model is trained on text with special tags indicating the\nstarting and ending points of the disfluent part. However, it has problems with\nlatency and standard language model adaptation, which arise from the additional\ndisfluency tags. We propose a multi-task model to solve such problems, which\nhas two output layers at the Transformer decoder; one for speech recognition\nand the other for disfluency detection. It is modeled to be conditioned on the\ncurrently recognized token with an additional token-dependency mechanism. We\nshow that the proposed joint models outperformed a BERT-based pipeline approach\nin both accuracy and latency, on both the Switchboard and the corpus of\nspontaneous Japanese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Futami_H/0/1/0/all/0/1\">Hayato Futami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsunoo_E/0/1/0/all/0/1\">Emiru Tsunoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibata_K/0/1/0/all/0/1\">Kentaro Shibata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashiwagi_Y/0/1/0/all/0/1\">Yosuke Kashiwagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okuda_T/0/1/0/all/0/1\">Takao Okuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08794","description":"<p>Due to the huge amount of parameters, fine-tuning of pretrained language\nmodels (PLMs) is prone to overfitting in the low resource scenarios. In this\nwork, we present a novel method that operates on the hidden representations of\na PLM to reduce overfitting. During fine-tuning, our method inserts random\nautoencoders between the hidden layers of a PLM, which transform activations\nfrom the previous layers into multi-view compressed representations before\nfeeding them into the upper layers. The autoencoders are plugged out after\nfine-tuning, so our method does not add extra parameters or increase\ncomputation cost during inference. Our method demonstrates promising\nperformance improvement across a wide range of sequence- and token-level\nlow-resource NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_M/0/1/0/all/0/1\">Megh Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning of Natural Language Processing Tasks: A Survey. (arXiv:2211.12701v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.12701","description":"<p>Continual learning (CL) is a learning paradigm that emulates the human\ncapability of learning and accumulating knowledge continually without\nforgetting the previously learned knowledge and also transferring the learned\nknowledge to help learn new tasks better. This survey presents a comprehensive\nreview and analysis of the recent progress of CL in NLP, which has significant\ndifferences from CL in computer vision and machine learning. It covers (1) all\nCL settings with a taxonomy of existing techniques; (2) catastrophic forgetting\n(CF) prevention, (3) knowledge transfer (KT), which is particularly important\nfor NLP tasks; and (4) some theory and the hidden challenge of inter-task class\nseparation (ICS). (1), (3) and (4) have not been included in the existing\nsurvey. Finally, a list of future directions is discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zixuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automaton-Based Representations of Task Knowledge from Generative Language Models. (arXiv:2212.01944v4 [cs.FL] UPDATED)","link":"http://arxiv.org/abs/2212.01944","description":"<p>Automaton-based representations of task knowledge play an important role in\ncontrol and planning for sequential decision-making problems. However,\nobtaining the high-level task knowledge required to build such automata is\noften difficult. Meanwhile, large-scale generative language models (GLMs) can\nautomatically generate relevant task knowledge. However, the textual outputs\nfrom GLMs cannot be formally verified or used for sequential decision-making.\nWe propose a novel algorithm named GLM2FSA, which constructs a finite state\nautomaton (FSA) encoding high-level task knowledge from a brief\nnatural-language description of the task goal. GLM2FSA first sends queries to a\nGLM to extract task knowledge in textual form, and then it builds an FSA to\nrepresent this text-based knowledge. The proposed algorithm thus fills the gap\nbetween natural-language task descriptions and automaton-based representations,\nand the constructed FSA can be formally verified against user-defined\nspecifications. We accordingly propose a method to iteratively refine the\nqueries to the GLM based on the outcomes, e.g., counter-examples, from\nverification. We demonstrate GLM2FSA's ability to build and refine\nautomaton-based representations of everyday tasks (e.g., crossing a road), and\nalso of tasks that require highly-specialized knowledge (e.g., executing secure\nmulti-party computation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaglione_J/0/1/0/all/0/1\">Jean-Rapha&#xeb;l Gaglione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neary_C/0/1/0/all/0/1\">Cyrus Neary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Few-Shot Relation Extraction via Representation Learning and Domain Adaptation. (arXiv:2212.02560v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.02560","description":"<p>Few-shot relation extraction aims to recognize novel relations with few\nlabeled sentences in each relation. Previous metric-based few-shot relation\nextraction algorithms identify relationships by comparing the prototypes\ngenerated by the few labeled sentences embedding with the embeddings of the\nquery sentences using a trained metric function. However, as these domains\nalways have considerable differences from those in the training dataset, the\ngeneralization ability of these approaches on unseen relations in many domains\nis limited. Since the prototype is necessary for obtaining relationships\nbetween entities in the latent space, we suggest learning more interpretable\nand efficient prototypes from prior knowledge and the intrinsic semantics of\nrelations to extract new relations in various domains more effectively. By\nexploring the relationships between relations using prior information, we\neffectively improve the prototype representation of relations. By using\ncontrastive learning to make the classification margins between sentence\nembedding more distinct, the prototype's geometric interpretability is\nenhanced. Additionally, utilizing a transfer learning approach for the\ncross-domain problem allows the generation process of the prototype to account\nfor the gap between other domains, making the prototype more robust and\nenabling the better extraction of associations across multiple domains. The\nexperiment results on the benchmark FewRel dataset demonstrate the advantages\nof the suggested method over some state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhongju Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenkun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Genghui Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention as a Guide for Simultaneous Speech Translation. (arXiv:2212.07850v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07850","description":"<p>The study of the attention mechanism has sparked interest in many fields,\nsuch as language modeling and machine translation. Although its patterns have\nbeen exploited to perform different tasks, from neural network understanding to\ntextual alignment, no previous work has analysed the encoder-decoder attention\nbehavior in speech translation (ST) nor used it to improve ST on a specific\ntask. In this paper, we fill this gap by proposing an attention-based policy\n(EDAtt) for simultaneous ST (SimulST) that is motivated by an analysis of the\nexisting attention relations between audio input and textual output. Its goal\nis to leverage the encoder-decoder attention scores to guide inference in real\ntime. Results on en-&gt;{de, es} show that the EDAtt policy achieves overall\nbetter results compared to the SimulST state of the art, especially in terms of\ncomputational-aware latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation. (arXiv:2212.08853v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08853","description":"<p>Language models with the Transformers structure have shown great performance\nin natural language processing. However, there still poses problems when\nfine-tuning pre-trained language models on downstream tasks, such as\nover-fitting or representation collapse. In this work, we propose HyPe, a\nsimple yet effective fine-tuning technique to alleviate such problems by\nperturbing hidden representations of Transformers layers. Unlike previous works\nthat only add noise to inputs or parameters, we argue that the hidden\nrepresentations of Transformers layers convey more diverse and meaningful\nlanguage information. Therefore, making the Transformers layers more robust to\nhidden representation perturbations can further benefit the fine-tuning of PLMs\nen bloc. We conduct extensive experiments and analyses on GLUE and other\nnatural language inference datasets. Results demonstrate that HyPe outperforms\nvanilla fine-tuning and enhances generalization of hidden representations from\ndifferent layers. In addition, HyPe acquires negligible computational\noverheads, and is better than and compatible with previous state-of-the-art\nfine-tuning techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can GPT-3 Perform Statutory Reasoning?. (arXiv:2302.06100v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.06100","description":"<p>Statutory reasoning is the task of reasoning with facts and statutes, which\nare rules written in natural language by a legislature. It is a basic legal\nskill. In this paper we explore the capabilities of the most capable GPT-3\nmodel, text-davinci-003, on an established statutory-reasoning dataset called\nSARA. We consider a variety of approaches, including dynamic few-shot\nprompting, chain-of-thought prompting, and zero-shot prompting. While we\nachieve results with GPT-3 that are better than the previous best published\nresults, we also identify several types of clear errors it makes. We\ninvestigate why these errors happen. We discover that GPT-3 has imperfect prior\nknowledge of the actual U.S. statutes on which SARA is based. More importantly,\nwe create simple synthetic statutes, which GPT-3 is guaranteed not to have seen\nduring training. We find GPT-3 performs poorly at answering straightforward\nquestions about these simple synthetic statutes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blair_Stanek_A/0/1/0/all/0/1\">Andrew Blair-Stanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzenberger_N/0/1/0/all/0/1\">Nils Holzenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Character-level Translations Worth the Wait? Comparing Character- and Subword-level Models for Machine Translation. (arXiv:2302.14220v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.14220","description":"<p>Pretrained character-level language models were recently shown to be\ncompetitive with popular subword models across a range of NLP tasks. However,\nthere has been little research on their effectiveness for neural machine\ntranslation (NMT). This work performs an extensive comparison across multiple\nlanguages and experimental conditions of state-of-the-art character- and\nsubword-level pre-trained models (ByT5 and mT5, respectively) on NMT, showing\nthe effectiveness of character-level modeling in translation, particularly in\ncases where training data is limited. In our analysis, we show how character\nmodels' performance gains are reflected in better translations of\northographically similar words and rare words. While evaluating the importance\nof source texts in driving model predictions, we highlight ByT5 word-level\npatterns suggesting an ability to modulate word and character-level information\nduring the translation, providing insights into a potential weakness of\ncharacter-level modeling. We conclude by assessing the efficiency tradeoff of\ncharacter models, suggesting their usage in non-time-critical scenarios to\nboost translation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edman_L/0/1/0/all/0/1\">Lukas Edman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1\">Gabriele Sarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noord_G/0/1/0/all/0/1\">Gertjan van Noord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-contained Beta-with-Spikes Approximation for Inference Under a Wright-Fisher Model. (arXiv:2303.04691v2 [q-bio.PE] UPDATED)","link":"http://arxiv.org/abs/2303.04691","description":"<p>We construct a reliable estimation of evolutionary parameters within the\nWright-Fisher model, which describes changes in allele frequencies due to\nselection and genetic drift, from time-series data. Such data exists for\nbiological populations, for example via artificial evolution experiments, and\nfor the cultural evolution of behavior, such as linguistic corpora that\ndocument historical usage of different words with similar meanings. Our method\nof analysis builds on a Beta-with-Spikes approximation to the distribution of\nallele frequencies predicted by the Wright-Fisher model. We introduce a\nself-contained scheme for estimating the parameters in the approximation, and\ndemonstrate its robustness with synthetic data, especially in the\nstrong-selection and near-extinction regimes where previous approaches fail. We\nfurther apply to allele frequency data for baker's yeast (Saccharomyces\ncerevisiae), finding a significant signal of selection in cases where\nindependent evidence supports such a conclusion. We further demonstrate the\npossibility of detecting time-points at which evolutionary parameters change in\nthe context of a historical spelling reform in the Spanish language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Montero_J/0/1/0/all/0/1\">Juan Guerrero Montero</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Blythe_R/0/1/0/all/0/1\">Richard A. Blythe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models. (arXiv:2304.01852v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01852","description":"<p>This paper presents a comprehensive survey of ChatGPT and GPT-4,\nstate-of-the-art large language models (LLM) from the GPT series, and their\nprospective applications across diverse domains. Indeed, key innovations such\nas large-scale pre-training that captures knowledge across the entire world\nwide web, instruction fine-tuning and Reinforcement Learning from Human\nFeedback (RLHF) have played significant roles in enhancing LLMs' adaptability\nand performance. We performed an in-depth analysis of 194 relevant papers on\narXiv, encompassing trend analysis, word cloud representation, and distribution\nanalysis across various application domains. The findings reveal a significant\nand increasing interest in ChatGPT/GPT-4 research, predominantly centered on\ndirect natural language processing applications, while also demonstrating\nconsiderable potential in areas ranging from education and history to\nmathematics, medicine, and physics. This study endeavors to furnish insights\ninto ChatGPT's capabilities, potential implications, ethical concerns, and\noffer direction for future advancements in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tianle Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuanyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jiaming Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Antong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mengshen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_N/0/1/0/all/0/1\">Ning Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dingang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_B/0/1/0/all/0/1\">Bao Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models. (arXiv:2304.09797v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.09797","description":"<p>The performance of Large Language Models (LLMs) in reasoning tasks depends\nheavily on prompt design, with Chain-of-Thought (CoT) and self-consistency\nbeing critical methods that enhance this ability. However, these methods do not\nfully exploit the answers generated by the LLM to guide subsequent responses.\nThis paper proposes a new prompting method, named Progressive-Hint Prompting\n(PHP), that enables automatic multiple interactions between users and LLMs by\nusing previously generated answers as hints to progressively guide toward the\ncorrect answers. PHP is orthogonal to CoT and self-consistency, making it easy\nto combine with state-of-the-art techniques to further improve performance. We\nconducted an extensive and comprehensive evaluation to demonstrate the\neffectiveness of the proposed method. Our experimental results on seven\nbenchmarks show that combining CoT and self-consistency with PHP significantly\nimproves accuracy while remaining highly efficient. For instance, with\ntext-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding\ncompared to Complex CoT, and a 46.17% reduction in sample paths with\nself-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances\non SVAMP (89.1% -&gt; 91.9%), GSM8K (92% -&gt; 95.5%), AQuA (76.4% -&gt; 79.9%) and MATH\n(50.3% -&gt; 53.9%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanyang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus & Walkden (2023). (arXiv:2305.00217v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.00217","description":"<p>In a recent paper published in the Journal of Language Evolution, Kauhanen,\nEinhaus &amp; Walkden (https://doi.org/10.1093/jole/lzad005, KEW) challenge the\nresults presented in one of my papers (Koplenig, Royal Society Open Science, 6,\n181274 (2019), https://doi.org/10.1098/rsos.181274), in which I tried to show\nthrough a series of statistical analyses that large numbers of L2 (second\nlanguage) speakers do not seem to affect the (grammatical or statistical)\ncomplexity of a language. To this end, I focus on the way in which the\nEthnologue assesses language status: a language is characterised as vehicular\nif, in addition to being used by L1 (first language) speakers, it should also\nhave a significant number of L2 users. KEW criticise both the use of\nvehicularity as a (binary) indicator of whether a language has a significant\nnumber of L2 users and the idea of imputing a zero proportion of L2 speakers to\nnon-vehicular languages whenever a direct estimate of that proportion is\nunavailable. While I recognise the importance of post-publication commentary on\npublished research, I show in this rejoinder that both points of criticism are\nexplicitly mentioned and analysed in my paper. In addition, I also comment on\nother points raised by KEW and demonstrate that both alternative analyses\noffered by KEW do not stand up to closer scrutiny.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koplenig_A/0/1/0/all/0/1\">Alexander Koplenig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Pipeline System of ASR and NLU with MLM-based Data Augmentation toward STOP Low-resource Challenge. (arXiv:2305.01194v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01194","description":"<p>This paper describes our system for the low-resource domain adaptation track\n(Track 3) in Spoken Language Understanding Grand Challenge, which is a part of\nICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a\npipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain\nwith upsampling. For NLU, we fine-tune BART on all the Track3 data and then on\nlow-resource domain data. We apply masked LM (MLM) -based data augmentation,\nwhere some of input tokens and corresponding target labels are replaced using\nMLM. We also apply a retrieval-based approach, where model input is augmented\nwith similar training samples. As a result, we achieved exact match (EM)\naccuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the\n1st place at the challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Futami_H/0/1/0/all/0/1\">Hayato Futami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_J/0/1/0/all/0/1\">Jessica Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shih-Lun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashiwagi_Y/0/1/0/all/0/1\">Yosuke Kashiwagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsunoo_E/0/1/0/all/0/1\">Emiru Tsunoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffuSum: Generation Enhanced Extractive Summarization with Diffusion. (arXiv:2305.01735v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01735","description":"<p>Extractive summarization aims to form a summary by directly extracting\nsentences from the source document. Existing works mostly formulate it as a\nsequence labeling problem by making individual sentence label predictions. This\npaper proposes DiffuSum, a novel paradigm for extractive summarization, by\ndirectly generating the desired summary sentence representations with diffusion\nmodels and extracting sentences based on sentence representation matching. In\naddition, DiffuSum jointly optimizes a contrastive sentence encoder with a\nmatching loss for sentence representation alignment and a multi-class\ncontrastive loss for representation diversity. Experimental results show that\nDiffuSum achieves the new state-of-the-art extractive results on CNN/DailyMail\nwith ROUGE scores of $44.83/22.56/40.56$. Experiments on the other two datasets\nwith different summary lengths also demonstrate the effectiveness of DiffuSum.\nThe strong performance of our framework shows the great potential of adapting\ngenerative models for extractive summarization. To encourage more following\nwork in the future, we have released our codes at\n\\url{https://github.com/hpzhang94/DiffuSum}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data. (arXiv:2305.02993v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02993","description":"<p>This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence\nNatural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2\ntasks, a Natural Language Inference (NLI) task, and an evidence selection task\non clinical trial data. The proposed challenges require multi-hop biomedical\nand numerical reasoning, which are of significant importance to the development\nof systems capable of large-scale interpretation and retrieval of medical\nevidence, to provide personalized evidence-based care.\n</p>\n<p>Task 1, the entailment task, received 643 submissions from 40 participants,\nand Task 2, the evidence selection task, received 364 submissions from 23\nparticipants. The tasks are challenging, with the majority of submitted systems\nfailing to significantly outperform the majority class baseline on the\nentailment task, and we observe significantly better performance on the\nevidence selection task than on the entailment task. Increasing the number of\nmodel parameters leads to a direct increase in performance, far more\nsignificant than the effect of biomedical pre-training. Future works could\nexplore the limitations of large models for generalization and numerical\ninference, and investigate methods to augment clinical datasets to allow for\nmore rigorous testing and to facilitate fine-tuning.\n</p>\n<p>We envisage that the dataset, models, and results of this task will be useful\nto the biomedical NLI and evidence retrieval communities. The dataset,\ncompetition leaderboard, and website are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jullien_M/0/1/0/all/0/1\">Ma&#xeb;l Jullien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frost_H/0/1/0/all/0/1\">Hannah Frost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ORegan_P/0/1/0/all/0/1\">Paul O&#x27;Regan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1\">Donal Landers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors. (arXiv:2305.05711v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05711","description":"<p>Large language models (LLMs) pre-trained on massive corpora have demonstrated\nimpressive few-shot learning ability on many NLP tasks. A common practice is to\nrecast the task into a text-to-text format such that generative LLMs of natural\nlanguage (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is\nnontrivial to perform information extraction (IE) tasks with NL-LLMs since the\noutput of the IE task is usually structured and therefore is hard to be\nconverted into plain text. In this paper, we propose to recast the structured\noutput in the form of code instead of natural language and utilize generative\nLLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular,\nnamed entity recognition and relation extraction. In contrast to NL-LLMs, we\nshow that Code-LLMs can be well-aligned with these IE tasks by designing\ncode-style prompts and formulating these IE tasks as code generation tasks.\nExperiment results on seven benchmarks show that our method consistently\noutperforms fine-tuning moderate-size pre-trained models specially designed for\nIE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further\nconduct a series of in-depth analyses to demonstrate the merits of leveraging\nCode-LLMs for IE tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1\">Qiong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanbin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable multimodal sentiment analysis based on textual modality descriptions by using large-scale language models. (arXiv:2305.06162v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06162","description":"<p>Multimodal sentiment analysis is an important area for understanding the\nuser's internal states. Deep learning methods were effective, but the problem\nof poor interpretability has gradually gained attention. Previous works have\nattempted to use attention weights or vector distributions to provide\ninterpretability. However, their explanations were not intuitive and can be\ninfluenced by different trained models. This study proposed a novel approach to\nprovide interpretability by converting nonverbal modalities into text\ndescriptions and by using large-scale language models for sentiment\npredictions. This provides an intuitive approach to directly interpret what\nmodels depend on with respect to making decisions from input texts, thus\nsignificantly improving interpretability. Specifically, we convert descriptions\nbased on two feature patterns for the audio modality and discrete action units\nfor the facial modality. Experimental results on two sentiment analysis tasks\ndemonstrated that the proposed approach maintained, or even improved\neffectiveness for sentiment analysis compared to baselines using conventional\nfeatures, with the highest improvement of 2.49% on the F1 score. The results\nalso showed that multimodal descriptions have similar characteristics on fusing\nmodalities as those of conventional fusion methods. The results demonstrated\nthat the proposed approach is interpretable and effective for multimodal\nsentiment analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okada_S/0/1/0/all/0/1\">Shogo Okada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success). (arXiv:2305.06299v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06299","description":"<p>Large language models, particularly GPT-3, are able to produce high quality\nsummaries of general domain news articles in few- and zero-shot settings.\nHowever, it is unclear if such models are similarly capable in more\nspecialized, high-stakes domains such as biomedicine. In this paper, we enlist\ndomain experts (individuals with medical training) to evaluate summaries of\nbiomedical articles generated by GPT-3, given zero supervision. We consider\nboth single- and multi-document settings. In the former, GPT-3 is tasked with\ngenerating regular and plain-language summaries of articles describing\nrandomized controlled trials; in the latter, we assess the degree to which\nGPT-3 is able to \\emph{synthesize} evidence reported across a collection of\narticles. We design an annotation scheme for evaluating model outputs, with an\nemphasis on assessing the factual accuracy of generated summaries. We find that\nwhile GPT-3 is able to summarize and simplify single biomedical articles\nfaithfully, it struggles to provide accurate aggregations of findings over\nmultiple documents. We release all data and annotations used in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaib_C/0/1/0/all/0/1\">Chantal Shaib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Millicent L. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_S/0/1/0/all/0/1\">Sebastian Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_I/0/1/0/all/0/1\">Iain J. Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}