{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-10-12T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Exploiting Language Models as a Source of Knowledge for Cognitive Agents. (arXiv:2310.06846v1 [cs.AI])","link":"http://arxiv.org/abs/2310.06846","description":"<p>Large language models (LLMs) provide capabilities far beyond sentence\ncompletion, including question answering, summarization, and natural-language\ninference. While many of these capabilities have potential application to\ncognitive systems, our research is exploiting language models as a source of\ntask knowledge for cognitive agents, that is, agents realized via a cognitive\narchitecture. We identify challenges and opportunities for using language\nmodels as an external knowledge source for cognitive systems and possible ways\nto improve the effectiveness of knowledge extraction by integrating extraction\nwith cognitive architecture capabilities, highlighting with examples from our\nrecent work in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_J/0/1/0/all/0/1\">James R. Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wray_R/0/1/0/all/0/1\">Robert E. Wray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laird_J/0/1/0/all/0/1\">John E. Laird</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Transformer-based Neural Text Representation Techniques on Bug Triaging. (arXiv:2310.06913v1 [cs.SE])","link":"http://arxiv.org/abs/2310.06913","description":"<p>Often, the first step in managing bug reports is related to triaging a bug to\nthe appropriate developer who is best suited to understand, localize, and fix\nthe target bug. Additionally, assigning a given bug to a particular part of a\nsoftware project can help to expedite the fixing process. However, despite the\nimportance of these activities, they are quite challenging, where days can be\nspent on the manual triaging process. Past studies have attempted to leverage\nthe limited textual data of bug reports to train text classification models\nthat automate this process -- to varying degrees of success. However, the\ntextual representations and machine learning models used in prior work are\nlimited by their expressiveness, often failing to capture nuanced textual\npatterns that might otherwise aid in the triaging process. Recently, large,\ntransformer-based, pre-trained neural text representation techniques such as\nBERT have achieved greater performance in several natural language processing\ntasks. However, the potential for using these techniques to improve upon prior\napproaches for automated bug triaging is not well studied or understood.\n</p>\n<p>Therefore, in this paper we offer one of the first investigations that\nfine-tunes transformer-based language models for the task of bug triaging on\nfour open source datasets, spanning a collective 53 years of development\nhistory with over 400 developers and over 150 software project components. Our\nstudy includes both a quantitative and qualitative analysis of effectiveness.\nOur findings illustrate that DeBERTa is the most effective technique across the\ntriaging tasks of developer and component assignment, and the measured\nperformance delta is statistically significant compared to other techniques.\nHowever, through our qualitative analysis, we also observe that each technique\npossesses unique abilities best suited to certain types of bug reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dipongkor_A/0/1/0/all/0/1\">Atish Kumar Dipongkor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_K/0/1/0/all/0/1\">Kevin Moran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE. (arXiv:2310.06918v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06918","description":"<p>The recent success of SimCSE has greatly advanced state-of-the-art sentence\nrepresentations. However, the original formulation of SimCSE does not fully\nexploit the potential of hard negative samples in contrastive learning. This\nstudy introduces an unsupervised contrastive learning framework that combines\nSimCSE with hard negative mining, aiming to enhance the quality of sentence\nembeddings. The proposed focal-InfoNCE function introduces self-paced\nmodulation terms in the contrastive objective, downweighting the loss\nassociated with easy negatives and encouraging the model focusing on hard\nnegatives. Experimentation on various STS benchmarks shows that our method\nimproves sentence embeddings in terms of Spearman's correlation and\nrepresentation alignment and uniformity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_P/0/1/0/all/0/1\">Pengyue Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Finetuning for Inference Acceleration of Large Language Models. (arXiv:2310.06927v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06927","description":"<p>We consider the problem of accurate sparse finetuning of large language\nmodels (LLMs), that is, finetuning pretrained LLMs on specialized tasks, while\ninducing sparsity in their weights. On the accuracy side, we observe that\nstandard loss-based finetuning may fail to recover accuracy, especially at high\nsparsities. To address this, we perform a detailed study of distillation-type\nlosses, determining an L2-based distillation approach we term SquareHead which\nenables accurate recovery even at higher sparsities, across all model types. On\nthe practical efficiency side, we show that sparse LLMs can be executed with\nspeedups by taking advantage of sparsity, for both CPU and GPU runtimes. While\nthe standard approach is to leverage sparsity for computational reduction, we\nobserve that in the case of memory-bound LLMs sparsity can also be leveraged\nfor reducing memory bandwidth. We exhibit end-to-end results showing speedups\ndue to sparsity, while recovering accuracy, on T5 (language translation),\nWhisper (speech translation), and open GPT-type (MPT for text generation). For\nMPT text generation, we show for the first time that sparse finetuning can\nreach 75% sparsity without accuracy drops, provide notable end-to-end speedups\nfor both CPU and GPU inference, and highlight that sparsity is also compatible\nwith quantization approaches. Models and software for reproducing our results\nare provided in Section 6.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurtic_E/0/1/0/all/0/1\">Eldar Kurtic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznedelev_D/0/1/0/all/0/1\">Denis Kuznedelev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frantar_E/0/1/0/all/0/1\">Elias Frantar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goin_M/0/1/0/all/0/1\">Michael Goin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document-Level Supervision for Multi-Aspect Sentiment Analysis Without Fine-grained Labels. (arXiv:2310.06940v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06940","description":"<p>Aspect-based sentiment analysis (ABSA) is a widely studied topic, most often\ntrained through supervision from human annotations of opinionated texts. These\nfine-grained annotations include identifying aspects towards which a user\nexpresses their sentiment, and their associated polarities (aspect-based\nsentiments). Such fine-grained annotations can be expensive and often\ninfeasible to obtain in real-world settings. There is, however, an abundance of\nscenarios where user-generated text contains an overall sentiment, such as a\nrating of 1-5 in user reviews or user-generated feedback, which may be\nleveraged for this task. In this paper, we propose a VAE-based topic modeling\napproach that performs ABSA using document-level supervision and without\nrequiring fine-grained labels for either aspects or sentiments. Our approach\nallows for the detection of multiple aspects in a document, thereby allowing\nfor the possibility of reasoning about how sentiment expressed through multiple\naspects comes together to form an observable overall document-level sentiment.\nWe demonstrate results on two benchmark datasets from two different domains,\nsignificantly outperforming a state-of-the-art baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_K/0/1/0/all/0/1\">Kasturi Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangadharaiah_R/0/1/0/all/0/1\">Rashmi Gangadharaiah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why bother with geometry? On the relevance of linear decompositions of Transformer embeddings. (arXiv:2310.06977v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06977","description":"<p>A recent body of work has demonstrated that Transformer embeddings can be\nlinearly decomposed into well-defined sums of factors, that can in turn be\nrelated to specific network inputs or components. There is however still a\ndearth of work studying whether these mathematical reformulations are\nempirically meaningful. In the present work, we study representations from\nmachine-translation decoders using two of such embedding decomposition methods.\nOur results indicate that, while decomposition-derived indicators effectively\ncorrelate with model performance, variation across different runs suggests a\nmore nuanced take on this question. The high variability of our measurements\nindicate that geometry reflects model-specific characteristics more than it\ndoes sentence-specific computations, and that similar training conditions do\nnot guarantee similar vector spaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mickus_T/0/1/0/all/0/1\">Timothee Mickus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_R/0/1/0/all/0/1\">Ra&#xfa;l V&#xe1;zquez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models. (arXiv:2310.06983v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06983","description":"<p>Recent research shows that Large Language Models (LLMs) exhibit a compelling\nlevel of proficiency in Theory of Mind (ToM) tasks. This ability to impute\nunobservable mental states to others is vital to human social cognition and may\nprove equally important in principal-agent relations between individual humans\nand Artificial Intelligences (AIs). In this paper, we explore how a mechanism\nstudied in developmental psychology known as Violation of Expectation (VoE) can\nbe implemented to reduce errors in LLM prediction about users by leveraging\nemergent ToM affordances. And we introduce a \\textit{metacognitive prompting}\nframework to apply VoE in the context of an AI tutor. By storing and retrieving\nfacts derived in cases where LLM expectation about the user was violated, we\nfind that LLMs are able to learn about users in ways that echo theories of\nhuman learning. Finally, we discuss latent hazards and augmentative\nopportunities associated with modeling user psychology and propose ways to\nmitigate risk along with possible directions for future inquiry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leer_C/0/1/0/all/0/1\">Courtland Leer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trost_V/0/1/0/all/0/1\">Vincent Trost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voruganti_V/0/1/0/all/0/1\">Vineeth Voruganti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation. (arXiv:2310.06987v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06987","description":"<p>The rapid progress in open-source large language models (LLMs) is\nsignificantly advancing AI development. Extensive efforts have been made before\nmodel release to align their behavior with human values, with the primary goal\nof ensuring their helpfulness and harmlessness. However, even carefully aligned\nmodels can be manipulated maliciously, leading to unintended behaviors, known\nas \"jailbreaks\". These jailbreaks are typically triggered by specific text\ninputs, often referred to as adversarial prompts. In this work, we propose the\ngeneration exploitation attack, an extremely simple approach that disrupts\nmodel alignment by only manipulating variations of decoding methods. By\nexploiting different generation strategies, including varying decoding\nhyper-parameters and sampling methods, we increase the misalignment rate from\n0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon,\nand MPT families, outperforming state-of-the-art attacks with $30\\times$ lower\ncomputational cost. Finally, we propose an effective alignment method that\nexplores diverse generation strategies, which can reasonably reduce the\nmisalignment rate under our attack. Altogether, our study underscores a major\nfailure in current safety evaluation and alignment procedures for open-source\nLLMs, strongly advocating for more comprehensive red teaming and better\nalignment before releasing such models. Our code is available at\nhttps://github.com/Princeton-SysML/Jailbreak_LLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yangsibo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Samyak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1\">Mengzhou Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs. (arXiv:2310.07008v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07008","description":"<p>Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield\npromising results in the Knowledge Graph Question Answering (KGQA) task.\nHowever, the capacity of the models is limited and the quality decreases for\nquestions with less popular entities. In this paper, we present a novel\napproach which works on top of the pre-trained Text-to-Text QA system to\naddress this issue. Our simple yet effective method performs filtering and\nre-ranking of generated candidates based on their types derived from Wikidata\n\"instance_of\" property.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salnikov_M/0/1/0/all/0/1\">Mikhail Salnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lysyuk_M/0/1/0/all/0/1\">Maria Lysyuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braslavski_P/0/1/0/all/0/1\">Pavel Braslavski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razzhigaev_A/0/1/0/all/0/1\">Anton Razzhigaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malykh_V/0/1/0/all/0/1\">Valentin Malykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1\">Alexander Panchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NEWTON: Are Large Language Models Capable of Physical Reasoning?. (arXiv:2310.07018v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07018","description":"<p>Large Language Models (LLMs), through their contextualized representations,\nhave been empirically proven to encapsulate syntactic, semantic, word sense,\nand common-sense knowledge. However, there has been limited exploration of\ntheir physical reasoning abilities, specifically concerning the crucial\nattributes for comprehending everyday objects. To address this gap, we\nintroduce NEWTON, a repository and benchmark for evaluating the physics\nreasoning skills of LLMs. Further, to enable domain-specific adaptation of this\nbenchmark, we present a pipeline to enable researchers to generate a variant of\nthis benchmark that has been customized to the objects and attributes relevant\nfor their application. The NEWTON repository comprises a collection of 2800\nobject-attribute pairs, providing the foundation for generating infinite-scale\nassessment templates. The NEWTON benchmark consists of 160K QA questions,\ncurated using the NEWTON repository to investigate the physical reasoning\ncapabilities of several mainstream language models across foundational,\nexplicit, and implicit reasoning tasks. Through extensive empirical analysis,\nour results highlight the capabilities of LLMs for physical reasoning. We find\nthat LLMs like GPT-4 demonstrate strong reasoning capabilities in\nscenario-based tasks but exhibit less consistency in object-attribute reasoning\ncompared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates\nits potential for evaluating and enhancing language models, paving the way for\ntheir integration into physically grounded settings, such as robotic\nmanipulation. Project site: https://newtonreasoning.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Ru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiafei Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1\">Siddhartha Srinivasa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Macro Mining from Interaction Traces at Scale. (arXiv:2310.07023v1 [cs.HC])","link":"http://arxiv.org/abs/2310.07023","description":"<p>Macros are building block tasks of our everyday smartphone activity (e.g.,\n\"login\", or \"booking a flight\"). Effectively extracting macros is important for\nunderstanding mobile interaction and enabling task automation. These macros are\nhowever difficult to extract at scale as they can be comprised of multiple\nsteps yet hidden within programmatic components of the app. In this paper, we\nintroduce a novel approach based on Large Language Models (LLMs) to\nautomatically extract semantically meaningful macros from both random and\nuser-curated mobile interaction traces. The macros produced by our approach are\nautomatically tagged with natural language descriptions and are fully\nexecutable. To examine the quality of extraction, we conduct multiple studies,\nincluding user evaluation, comparative analysis against human-curated tasks,\nand automatic execution of these macros. These experiments and analyses show\nthe effectiveness of our approach and the usefulness of extracted macros in\nvarious downstream applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Forrest Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records. (arXiv:2310.07059v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07059","description":"<p>Multi-label text classification (MLTC) tasks in the medical domain often face\nlong-tail label distribution, where rare classes have fewer training samples\nthan frequent classes. Although previous works have explored different model\narchitectures and hierarchical label structures to find important features,\nmost of them neglect to incorporate the domain knowledge from medical\nguidelines. In this paper, we present DKEC, Domain Knowledge Enhanced\nClassifier for medical diagnosis prediction with two innovations: (1) a\nlabel-wise attention mechanism that incorporates a heterogeneous graph and\ndomain ontologies to capture the semantic relationships between medical\nentities, (2) a simple yet effective group-wise training method based on\nsimilarity of labels to increase samples of rare classes. We evaluate DKEC on\ntwo real-world medical datasets: the RAA dataset, a collection of 4,417 patient\ncare reports from emergency medical services (EMS) incidents, and a subset of\n53,898 reports from the MIMIC-III dataset. Experimental results show that our\nmethod outperforms the state-of-the-art, particularly for the few-shot (tail)\nclasses. More importantly, we study the applicability of DKEC to different\nlanguage models and show that DKEC can help the smaller language models achieve\ncomparable performance to large language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xueren Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_R/0/1/0/all/0/1\">Ronald Dean Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stankovic_J/0/1/0/all/0/1\">John A. Stankovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alemzadeh_H/0/1/0/all/0/1\">Homa Alemzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models can Learn Rules. (arXiv:2310.07064v1 [cs.AI])","link":"http://arxiv.org/abs/2310.07064","description":"<p>When prompted with a few examples and intermediate steps, large language\nmodels (LLMs) have demonstrated impressive performance in various reasoning\ntasks. However, prompting methods that rely on implicit knowledge in an LLM\noften hallucinate incorrect answers when the implicit knowledge is wrong or\ninconsistent with the task. To tackle this problem, we present\nHypotheses-to-Theories (HtT), a framework that learns a rule library for\nreasoning with LLMs. HtT contains two stages, an induction stage and a\ndeduction stage. In the induction stage, an LLM is first asked to generate and\nverify rules over a set of training examples. Rules that appear and lead to\ncorrect answers sufficiently often are collected to form a rule library. In the\ndeduction stage, the LLM is then prompted to employ the learned rule library to\nperform reasoning to answer test questions. Experiments on both numerical\nreasoning and relational reasoning problems show that HtT improves existing\nprompting methods, with an absolute gain of 11-27% in accuracy. The learned\nrules are also transferable to different models and to different forms of the\nsame problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhaocheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hanjun Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding. (arXiv:2310.07075v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07075","description":"<p>Large language models (LLMs) have shown promising capabilities in using\nexternal tools to solve complex problems. However, existing approaches either\ninvolve fine-tuning on tool demonstrations, which do not generalize to new\ntools without additional training, or providing tool documentation in context,\nlimiting the number of tools. Both approaches often generate syntactically\ninvalid tool calls. In this paper, we propose ToolDec, a finite-state\nmachine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates\ntool-related errors for any tool-augmented LLMs by ensuring valid tool names\nand type-conforming arguments. Furthermore, ToolDec enables LLM to effectively\nselect tools using only the information contained in their names, with no need\nfor fine-tuning or in-context documentation. We evaluated multiple prior\nmethods and their ToolDec-enhanced versions on a variety of tasks involving\ntools like math functions, knowledge graph relations, and complex real-world\nRESTful APIs. Our experiments show that ToolDec reduces syntactic errors to\nzero, consequently achieving significantly better performance and as much as a\n2x speedup. We also show that ToolDec achieves superior generalization\nperformance on unseen tools, performing up to 8x better than the baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kexun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongqiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auditing and Robustifying COVID-19 Misinformation Datasets via Anticontent Sampling. (arXiv:2310.07078v1 [cs.LG])","link":"http://arxiv.org/abs/2310.07078","description":"<p>This paper makes two key contributions. First, it argues that highly\nspecialized rare content classifiers trained on small data typically have\nlimited exposure to the richness and topical diversity of the negative class\n(dubbed anticontent) as observed in the wild. As a result, these classifiers'\nstrong performance observed on the test set may not translate into real-world\nsettings. In the context of COVID-19 misinformation detection, we conduct an\nin-the-wild audit of multiple datasets and demonstrate that models trained with\nseveral prominently cited recent datasets are vulnerable to anticontent when\nevaluated in the wild. Second, we present a novel active learning pipeline that\nrequires zero manual annotation and iteratively augments the training data with\nchallenging anticontent, robustifying these classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1\">Clay H. Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+KhudaBukhsh_A/0/1/0/all/0/1\">Ashiqur R. KhudaBukhsh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting. (arXiv:2310.07081v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07081","description":"<p>Idioms are common in everyday language, but often pose a challenge to\ntranslators because their meanings do not follow from the meanings of their\nparts. Despite significant advances, machine translation systems still struggle\nto translate idiomatic expressions. We provide a simple characterization of\nidiomatic translation and related issues. This allows us to conduct a synthetic\nexperiment revealing a tipping point at which transformer-based machine\ntranslation models correctly default to idiomatic translations. To expand\nmultilingual resources, we compile a dataset of ~4k natural sentences\ncontaining idiomatic expressions in French, Finnish, and Japanese. To improve\ntranslation of natural idioms, we introduce two straightforward yet effective\ntechniques: the strategic upweighting of training loss on potentially idiomatic\nsentences, and using retrieval-augmented models. This not only improves the\naccuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in\nabsolute accuracy, but also holds potential benefits for non-idiomatic\nsentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1\">Emmy Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aditi Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diversity of Thought Improves Reasoning Abilities of Large Language Models. (arXiv:2310.07088v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07088","description":"<p>Large language models (LLMs) are documented to struggle in settings that\nrequire complex reasoning. Nevertheless, instructing the model to break down\nthe problem into smaller reasoning steps (Wei et al., 2022), or ensembling\nvarious generations through modifying decoding steps (Wang et al., 2023) boosts\nperformance. Current methods assume that the input prompt is fixed and expect\nthe decoding strategies to introduce the diversity needed for ensembling. In\nthis work, we relax this assumption and discuss how one can create and leverage\nvariations of the input prompt as a means to diversity of thought to improve\nmodel performance. We propose a method that automatically improves prompt\ndiversity by soliciting feedback from the LLM to ideate approaches that fit for\nthe problem. We then ensemble the diverse prompts in our method DIV-SE (DIVerse\nreasoning path Self-Ensemble) across multiple inference calls. We also propose\na cost-effective alternative where diverse prompts are used within a single\ninference call; we call this IDIV-SE (In-call DIVerse reasoning path\nSelf-Ensemble). Under a fixed generation budget, DIV-SE and IDIV-SE outperform\nthe previously discussed baselines using both GPT-3.5 and GPT-4 on several\nreasoning benchmarks, without modifying the decoding process. Additionally,\nDIV-SE advances state-of-the-art performance on recent planning benchmarks\n(Valmeekam et al., 2023), exceeding the highest previously reported accuracy by\nat least 29.6 percentage points on the most challenging 4/5 Blocksworld task.\nOur results shed light on how to enforce prompt diversity toward LLM reasoning\nand thereby improve the pareto frontier of the accuracy-cost trade-off.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naik_R/0/1/0/all/0/1\">Ranjita Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1\">Varun Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuksekgonul_M/0/1/0/all/0/1\">Mert Yuksekgonul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nushi_B/0/1/0/all/0/1\">Besmira Nushi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jaeger: A Concatenation-Based Multi-Transformer VQA Model. (arXiv:2310.07091v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07091","description":"<p>Document-based Visual Question Answering poses a challenging task between\nlinguistic sense disambiguation and fine-grained multimodal retrieval. Although\nthere has been encouraging progress in document-based question answering due to\nthe utilization of large language and open-world prior models\\cite{1}, several\nchallenges persist, including prolonged response times, extended inference\ndurations, and imprecision in matching. In order to overcome these challenges,\nwe propose Jaegar, a concatenation-based multi-transformer VQA model. To derive\nquestion features, we leverage the exceptional capabilities of RoBERTa\nlarge\\cite{2} and GPT2-xl\\cite{3} as feature extractors. Subsequently, we\nsubject the outputs from both models to a concatenation process. This operation\nallows the model to consider information from diverse sources concurrently,\nstrengthening its representational capability. By leveraging pre-trained models\nfor feature extraction, our approach has the potential to amplify the\nperformance of these models through concatenation. After concatenation, we\napply dimensionality reduction to the output features, reducing the model's\ncomputational effectiveness and inference time. Empirical results demonstrate\nthat our proposed model achieves competitive performance on Task C of the\nPDF-VQA Dataset. If the user adds any new data, they should make sure to style\nit as per the instructions provided in previous sections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1\">Jieting Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zewei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Penghao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yidong Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning. (arXiv:2310.07093v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07093","description":"<p>To advance argumentative stance prediction as a multimodal problem, the First\nShared Task in Multimodal Argument Mining hosted stance prediction in crucial\nsocial topics of gun control and abortion. Our exploratory study attempts to\nevaluate the necessity of images for stance prediction in tweets and compare\nout-of-the-box text-based large-language models (LLM) in few-shot settings\nagainst fine-tuned unimodal and multimodal models. Our work suggests an\nensemble of fine-tuned text-based language models (0.817 F1-score) outperforms\nboth the multimodal (0.677 F1-score) and text-based few-shot prediction using a\nrecent state-of-the-art LLM (0.550 F1-score). In addition to the differences in\nperformance, our findings suggest that the multimodal models tend to perform\nbetter when image content is summarized as natural language over their native\npixel structure and, using in-context examples improves few-shot performance of\nLLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Arushi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhibha Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilalpur_M/0/1/0/all/0/1\">Maneesh Bilalpur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Universal Transformer. (arXiv:2310.07096v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07096","description":"<p>The Universal Transformer (UT) is a variant of the Transformer that shares\nparameters across its layers. Empirical evidence shows that UTs have better\ncompositional generalization than Vanilla Transformers (VTs) in formal language\ntasks. The parameter-sharing also affords it better parameter efficiency than\nVTs. Despite its many advantages, scaling UT parameters is much more compute\nand memory intensive than scaling up a VT. This paper proposes the Sparse\nUniversal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE)\nand a new stick-breaking-based dynamic halting mechanism to reduce UT's\ncomputation complexity while retaining its parameter efficiency and\ngeneralization ability. Experiments show that SUT achieves the same performance\nas strong baseline models while only using half computation and parameters on\nWMT'14 and strong generalization results on formal language tasks (Logical\ninference and CFQ). The new halting mechanism also enables around 50\\%\nreduction in computation during inference with very little performance decrease\non formal language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shawn Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yikang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenfang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models. (arXiv:2310.07106v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07106","description":"<p>Deep Language Models (DLMs) provide a novel computational paradigm for\nunderstanding the mechanisms of natural language processing in the human brain.\nUnlike traditional psycholinguistic models, DLMs use layered sequences of\ncontinuous numerical vectors to represent words and context, allowing a\nplethora of emerging applications such as human-like text generation. In this\npaper we show evidence that the layered hierarchy of DLMs may be used to model\nthe temporal dynamics of language comprehension in the brain by demonstrating a\nstrong correlation between DLM layer depth and the time at which layers are\nmost predictive of the human brain. Our ability to temporally resolve\nindividual layers benefits from our use of electrocorticography (ECoG) data,\nwhich has a much higher temporal resolution than noninvasive methods like fMRI.\nUsing ECoG, we record neural activity from participants listening to a\n30-minute narrative while also feeding the same narrative to a high-performing\nDLM (GPT2-XL). We then extract contextual embeddings from the different layers\nof the DLM and use linear encoding models to predict neural activity. We first\nfocus on the Inferior Frontal Gyrus (IFG, or Broca's area) and then extend our\nmodel to track the increasing temporal receptive window along the linguistic\nprocessing hierarchy from auditory to syntactic and semantic areas. Our results\nreveal a connection between human language processing and DLMs, with the DLM's\nlayer-by-layer accumulation of contextual information mirroring the timing of\nneural activity in high-order language areas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_A/0/1/0/all/0/1\">Ariel Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_E/0/1/0/all/0/1\">Eric Ham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schain_M/0/1/0/all/0/1\">Mariano Schain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nastase_S/0/1/0/all/0/1\">Samuel Nastase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zada_Z/0/1/0/all/0/1\">Zaid Zada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabush_A/0/1/0/all/0/1\">Avigail Dabush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubrey_B/0/1/0/all/0/1\">Bobbi Aubrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gazula_H/0/1/0/all/0/1\">Harshvardhan Gazula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doyle_W/0/1/0/all/0/1\">Werner K Doyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devore_S/0/1/0/all/0/1\">Sasha Devore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dugan_P/0/1/0/all/0/1\">Patricia Dugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_D/0/1/0/all/0/1\">Daniel Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_M/0/1/0/all/0/1\">Michael Brenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassidim_A/0/1/0/all/0/1\">Avinatan Hassidim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devinsky_O/0/1/0/all/0/1\">Orrin Devinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flinker_A/0/1/0/all/0/1\">Adeen Flinker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasson_U/0/1/0/all/0/1\">Uri Hasson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Styles across Languages. (arXiv:2310.07135v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07135","description":"<p>Understanding how styles differ across languages is advantageous for training\nboth humans and computers to generate culturally appropriate text. We introduce\nan explanation framework to extract stylistic differences from multilingual LMs\nand compare styles across languages. Our framework (1) generates comprehensive\nstyle lexica in any language and (2) consolidates feature importances from LMs\ninto comparable lexical categories. We apply this framework to compare\npoliteness, creating the first holistic multilingual politeness dataset and\nexploring how politeness varies across four languages. Our approach enables an\neffective evaluation of how distinct linguistic categories contribute to\nstylistic variations and provides interpretable insights into how people\ncommunicate differently around the world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1\">Shreya Havaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pressimone_M/0/1/0/all/0/1\">Matthew Pressimone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1\">Eric Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AE-smnsMLC: Multi-Label Classification with Semantic Matching and Negative Label Sampling for Product Attribute Value Extraction. (arXiv:2310.07137v1 [cs.IR])","link":"http://arxiv.org/abs/2310.07137","description":"<p>Product attribute value extraction plays an important role for many\nreal-world applications in e-Commerce such as product search and\nrecommendation. Previous methods treat it as a sequence labeling task that\nneeds more annotation for position of values in the product text. This limits\ntheir application to real-world scenario in which only attribute values are\nweakly-annotated for each product without their position. Moreover, these\nmethods only use product text (i.e., product title and description) and do not\nconsider the semantic connection between the multiple attribute values of a\ngiven product and its text, which can help attribute value extraction. In this\npaper, we reformulate this task as a multi-label classification task that can\nbe applied for real-world scenario in which only annotation of attribute values\nis available to train models (i.e., annotation of positional information of\nattribute values is not available). We propose a classification model with\nsemantic matching and negative label sampling for attribute value extraction.\nSemantic matching aims to capture semantic interactions between attribute\nvalues of a given product and its text. Negative label sampling aims to enhance\nthe model's ability of distinguishing similar values belonging to the same\nattribute. Experimental results on three subsets of a large real-world\ne-Commerce dataset demonstrate the effectiveness and superiority of our\nproposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhongfen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Te Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting. (arXiv:2310.07146v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07146","description":"<p>Mental illness remains one of the most critical public health issues of our\ntime, due to the severe scarcity and accessibility limit of professionals.\nPsychotherapy requires high-level expertise to conduct deep, complex reasoning\nand analysis on the cognition modeling of the patients. In the era of Large\nLanguage Models, we believe it is the right time to develop AI assistance for\ncomputational psychotherapy. We study the task of cognitive distortion\ndetection and propose the Diagnosis of Thought (DoT) prompting. DoT performs\ndiagnosis on the patient's speech via three stages: subjectivity assessment to\nseparate the facts and the thoughts; contrastive reasoning to elicit the\nreasoning processes supporting and contradicting the thoughts; and schema\nanalysis to summarize the cognition schemas. The generated diagnosis rationales\nthrough the three stages are essential for assisting the professionals.\nExperiments demonstrate that DoT obtains significant improvements over ChatGPT\nfor cognitive distortion detection, while generating high-quality rationales\napproved by human experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources. (arXiv:2310.07147v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07147","description":"<p>Large Language Models (LLMs) have showcased remarkable impacts across a wide\nspectrum of natural language processing tasks. Fine-tuning these pre-trained\nmodels on downstream datasets provides further significant performance gains,\nbut this process has been challenging due to its extraordinary resource\nrequirements. To this end, existing efforts focus on parameter-efficient\nfine-tuning, which, unfortunately, fail to capitalize on the powerful potential\nof full-parameter fine-tuning. In this work, we propose QFT, a novel Quantized\nFull-parameter Tuning framework for LLMs that enables memory-efficient\nfine-tuning without harming performance. Our framework incorporates two novel\nideas: (i) we adopt the efficient Lion optimizer, which only keeps track of the\nmomentum and has consistent update magnitudes for each parameter, an inherent\nadvantage for robust quantization; and (ii) we quantize all model states and\nstore them as integer values, and present a gradient flow and parameter update\nscheme for the quantized weights. As a result, QFT reduces the model state\nmemory to 21% of the standard solution while achieving comparable performance,\ne.g., tuning a LLaMA-7B model requires only &lt;30GB of memory, satisfied by a\nsingle A6000 GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhikai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoxuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Banghua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qingyi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"A Tale of Two Movements\": Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction. (arXiv:2310.07155v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07155","description":"<p>Social media has become a major driver of social change, by facilitating the\nformation of online social movements. Automatically understanding the\nperspectives driving the movement and the voices opposing it, is a challenging\ntask as annotated data is difficult to obtain. We propose a weakly supervised\ngraph-based approach that explicitly models perspectives in\n#BackLivesMatter-related tweets. Our proposed approach utilizes a\nsocial-linguistic representation of the data. We convert the text to a graph by\nbreaking it into structured elements and connect it with the social network of\nauthors, then structured prediction is done over the elements for identifying\nperspectives. Our approach uses a small seed set of labeled examples. We\nexperiment with large language models for generating artificial training\nexamples, compare them to manual annotation, and find that it achieves\ncomparable performance. We perform quantitative and qualitative analyses using\na human-annotated test set. Our model outperforms multitask baselines by a\nlarge margin, successfully characterizing the perspectives supporting and\nopposing #BLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shamik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms. (arXiv:2310.07161v1 [cs.SD])","link":"http://arxiv.org/abs/2310.07161","description":"<p>Within the ambit of VoIP (Voice over Internet Protocol) telecommunications,\nthe complexities introduced by acoustic transformations merit rigorous\nanalysis. This research, rooted in the exploration of proprietary sender-side\ndenoising effects, meticulously evaluates platforms such as Google Meets and\nZoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset,\nensuring a structured examination tailored to various denoising settings and\nreceiver interfaces. A methodological novelty is introduced via the Oaxaca\ndecomposition, traditionally an econometric tool, repurposed herein to analyze\nacoustic-phonetic perturbations within VoIP systems. To further ground the\nimplications of these transformations, psychoacoustic metrics, specifically\nPESQ and STOI, were harnessed to furnish a comprehensive understanding of\nspeech alterations. Cumulatively, the insights garnered underscore the\nintricate landscape of VoIP-influenced acoustic dynamics. In addition to the\nprimary findings, a multitude of metrics are reported, extending the research\npurview. Moreover, out-of-domain benchmarking for both time and time-frequency\ndomain speech enhancement models is included, thereby enhancing the depth and\napplicability of this inquiry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konan_J/0/1/0/all/0/1\">Joseph Konan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhargave_O/0/1/0/all/0/1\">Ojas Bhargave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agnihotri_S/0/1/0/all/0/1\">Shikhar Agnihotri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shuo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yunyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ankit Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model. (arXiv:2310.07170v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07170","description":"<p>Despite the remarkable progress in natural language understanding with\npretrained Transformers, neural language models often do not handle commonsense\nknowledge well. Toward commonsense-aware models, there have been attempts to\nobtain knowledge, ranging from automatic acquisition to crowdsourcing. However,\nit is difficult to obtain a high-quality knowledge base at a low cost,\nespecially from scratch. In this paper, we propose PHALM, a method of building\na knowledge graph from scratch, by prompting both crowdworkers and a large\nlanguage model (LLM). We used this method to build a Japanese event knowledge\ngraph and trained Japanese commonsense generation models. Experimental results\nrevealed the acceptability of the built graph and inferences generated by the\ntrained models. We also report the difference in prompting humans and an LLM.\nOur code, data, and models are available at\ngithub.com/nlp-waseda/comet-atomic-ja.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ide_T/0/1/0/all/0/1\">Tatsuya Ide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murata_E/0/1/0/all/0/1\">Eiki Murata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_D/0/1/0/all/0/1\">Daisuke Kawahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_T/0/1/0/all/0/1\">Takato Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinzato_K/0/1/0/all/0/1\">Kenta Shinzato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1\">Toshinori Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])","link":"http://arxiv.org/abs/2310.07177","description":"<p>Speculative decoding is a pivotal technique to accelerate the inference of\nlarge language models (LLMs) by employing a smaller draft model to predict the\ntarget model's outputs. However, its efficacy can be limited due to the low\npredictive accuracy of the draft model, particularly when faced with diverse\ntext inputs and a significant capability gap between the draft and target\nmodels. We introduce online speculative decoding (OSD) to address this\nchallenge. The main idea is to continually update (multiple) draft model(s) on\nobserved user query data using the abundant excess computational power in an\nLLM serving cluster. Given that LLM inference is memory-bounded, the surplus\ncomputational power in a typical LLM serving cluster can be repurposed for\nonline retraining of draft models, thereby making the training cost-neutral.\nSince the query distribution of an LLM service is relatively simple, retraining\non query distribution enables the draft model to more accurately predict the\ntarget model's outputs, particularly on data originating from query\ndistributions. As the draft model evolves online, it aligns with the query\ndistribution in real time, mitigating distribution shifts. We develop a\nprototype of online speculative decoding based on online knowledge distillation\nand evaluate it using both synthetic and real query data on several popular\nLLMs. The results show a substantial increase in the token acceptance rate by\n0.1 to 0.65, which translates into 1.22x to 3.06x latency reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoxuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lanxiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailis_P/0/1/0/all/0/1\">Peter Bailis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhijie Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_A/0/1/0/all/0/1\">Alvin Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Gating in Mixture-of-Experts based Language Models. (arXiv:2310.07188v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07188","description":"<p>Large language models, such as OpenAI's ChatGPT, have demonstrated\nexceptional language understanding capabilities in various NLP tasks. Sparsely\nactivated mixture-of-experts (MoE) has emerged as a promising solution for\nscaling models while maintaining a constant number of computational operations.\nExisting MoE model adopts a fixed gating network where each token is computed\nby the same number of experts. However, this approach contradicts our intuition\nthat the tokens in each sequence vary in terms of their linguistic complexity\nand, consequently, require different computational costs. Little is discussed\nin prior research on the trade-off between computation per token and model\nperformance. This paper introduces adaptive gating in MoE, a flexible training\nstrategy that allows tokens to be processed by a variable number of experts\nbased on expert probability distribution. The proposed framework preserves\nsparsity while improving training efficiency. Additionally, curriculum learning\nis leveraged to further reduce training time. Extensive experiments on diverse\nNLP tasks show that adaptive gating reduces at most 22.5% training time while\nmaintaining inference quality. Moreover, we conduct a comprehensive analysis of\nthe routing decisions and present our insights when adaptive gating is used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiamin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qiang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yitao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yimin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Landscape of Large Language Models In Medical Question Answering: Observations and Open Questions. (arXiv:2310.07225v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07225","description":"<p>Large Language Models (LLMs) have shown promise in medical question answering\nby achieving passing scores in standardised exams and have been suggested as\ntools for supporting healthcare workers. Deploying LLMs into such a high-risk\ncontext requires a clear understanding of the limitations of these models. With\nthe rapid development and release of new LLMs, it is especially valuable to\nidentify patterns which exist across models and may, therefore, continue to\nappear in newer versions. In this paper, we evaluate a wide range of popular\nLLMs on their knowledge of medical questions in order to better understand\ntheir properties as a group. From this comparison, we provide preliminary\nobservations and raise open questions for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korgul_K/0/1/0/all/0/1\">Karolina Korgul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bean_A/0/1/0/all/0/1\">Andrew M. Bean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krones_F/0/1/0/all/0/1\">Felix Krones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCraith_R/0/1/0/all/0/1\">Robert McCraith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdi_A/0/1/0/all/0/1\">Adam Mahdi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs. (arXiv:2310.07251v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07251","description":"<p>In this position paper, we argue that instead of morally aligning LLMs to\nspecific set of ethical principles, we should infuse generic ethical reasoning\ncapabilities into them so that they can handle value pluralism at a global\nscale. When provided with an ethical policy, an LLM should be capable of making\ndecisions that are ethically consistent to the policy. We develop a framework\nthat integrates moral dilemmas with moral principles pertaining to different\nforamlisms of normative ethics, and at different levels of abstractions.\nInitial experiments with GPT-x models shows that while GPT-4 is a nearly\nperfect ethical reasoner, the models still have bias towards the moral values\nof Western and English speaking societies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Abhinav Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Aditi Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanmay_K/0/1/0/all/0/1\">Kumar Tanmay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_U/0/1/0/all/0/1\">Utkarsh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07276","description":"<p>Recent advancements in biological research leverage the integration of\nmolecules, proteins, and natural language to enhance drug discovery. However,\ncurrent models exhibit several limitations, such as the generation of invalid\nmolecular SMILES, underutilization of contextual information, and equal\ntreatment of structured and unstructured knowledge. To address these issues, we\npropose $\\mathbf{BioT5}$, a comprehensive pre-training framework that enriches\ncross-modal integration in biology with chemical knowledge and natural language\nassociations. $\\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular\nrepresentations and extracts knowledge from the surrounding context of\nbio-entities in unstructured biological literature. Furthermore,\n$\\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge,\nleading to more effective utilization of information. After fine-tuning, BioT5\nshows superior performance across a wide range of tasks, demonstrating its\nstrong capability of capturing underlying relations and properties of\nbio-entities. Our code is available at\n$\\href{https://github.com/QizhiPei/BioT5}{Github}$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_Q/0/1/0/all/0/1\">Qizhi Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinhua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kehan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kaiyuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing expressivity transfer in textless speech-to-speech translation. (arXiv:2310.07279v1 [cs.SD])","link":"http://arxiv.org/abs/2310.07279","description":"<p>Textless speech-to-speech translation systems are rapidly advancing, thanks\nto the integration of self-supervised learning techniques. However, existing\nstate-of-the-art systems fall short when it comes to capturing and transferring\nexpressivity accurately across different languages. Expressivity plays a vital\nrole in conveying emotions, nuances, and cultural subtleties, thereby enhancing\ncommunication across diverse languages. To address this issue this study\npresents a novel method that operates at the discrete speech unit level and\nleverages multilingual emotion embeddings to capture language-agnostic\ninformation. Specifically, we demonstrate how these embeddings can be used to\neffectively predict the pitch and duration of speech units in the target\nlanguage. Through objective and subjective experiments conducted on a\nFrench-to-English translation task, our findings highlight the superior\nexpressivity transfer achieved by our approach compared to current\nstate-of-the-art systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duret_J/0/1/0/all/0/1\">Jarod Duret</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_B/0/1/0/all/0/1\">Benjamin O&#x27;Brien</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a> (CAM)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT. (arXiv:2310.07282v1 [cs.AI])","link":"http://arxiv.org/abs/2310.07282","description":"<p>This paper conducts a comprehensive investigation into applying large\nlanguage models, particularly on BioBERT, in healthcare. It begins with\nthoroughly examining previous natural language processing (NLP) approaches in\nhealthcare, shedding light on the limitations and challenges these methods\nface. Following that, this research explores the path that led to the\nincorporation of BioBERT into healthcare applications, highlighting its\nsuitability for addressing the specific requirements of tasks related to\nbiomedical text mining. The analysis outlines a systematic methodology for\nfine-tuning BioBERT to meet the unique needs of the healthcare domain. This\napproach includes various components, including the gathering of data from a\nwide range of healthcare sources, data annotation for tasks like identifying\nmedical entities and categorizing them, and the application of specialized\npreprocessing techniques tailored to handle the complexities found in\nbiomedical texts. Additionally, the paper covers aspects related to model\nevaluation, with a focus on healthcare benchmarks and functions like processing\nof natural language in biomedical, question-answering, clinical document\nclassification, and medical entity recognition. It explores techniques to\nimprove the model's interpretability and validates its performance compared to\nexisting healthcare-focused language models. The paper thoroughly examines\nethical considerations, particularly patient privacy and data security. It\nhighlights the benefits of incorporating BioBERT into healthcare contexts,\nincluding enhanced clinical decision support and more efficient information\nretrieval. Nevertheless, it acknowledges the impediments and complexities of\nthis integration, encompassing concerns regarding data privacy, transparency,\nresource-intensive requirements, and the necessity for model customization to\nalign with diverse healthcare domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharaf_S/0/1/0/all/0/1\">Shyni Sharaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anoop_V/0/1/0/all/0/1\">V. S. Anoop</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction. (arXiv:2310.07284v1 [eess.AS])","link":"http://arxiv.org/abs/2310.07284","description":"<p>Humans possess an extraordinary ability to selectively focus on the sound\nsource of interest amidst complex acoustic environments, commonly referred to\nas cocktail party scenarios. In an attempt to replicate this remarkable\nauditory attention capability in machines, target speaker extraction (TSE)\nmodels have been developed. These models leverage the pre-registered cues of\nthe target speaker to extract the sound source of interest. However, the\neffectiveness of these models is hindered in real-world scenarios due to the\npotential variation or even absence of pre-registered cues. To address this\nlimitation, this study investigates the integration of natural language to\nenhance the flexibility and controllability of existing TSE models.\nSpecifically, we propose a model named LLM-TSE, wherein a large language model\n(LLM) to extract useful semantic cues from the user's typed text input, which\ncan complement the pre-registered cues or work independently to control the TSE\nprocess. Our experimental results demonstrate competitive performance when only\ntext-based cues are presented, and a new state-of-the-art is set when combined\nwith pre-registered acoustic cues. To the best of our knowledge, this is the\nfirst work that has successfully incorporated text-based cues to guide target\nspeaker extraction, which can be a cornerstone for cocktail party problem\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hao_X/0/1/0/all/0/1\">Xiang Hao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jibin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jianwei Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1\">Chenglin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_K/0/1/0/all/0/1\">Kay Chen Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators. (arXiv:2310.07289v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07289","description":"<p>Large language models (LLMs) outperform information retrieval techniques for\ndownstream knowledge-intensive tasks when being prompted to generate world\nknowledge. However, community concerns abound regarding the factuality and\npotential implications of using this uncensored knowledge. In light of this, we\nintroduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to\nsystematically and automatically evaluate generated knowledge from six\nimportant perspectives -- Factuality, Relevance, Coherence, Informativeness,\nHelpfulness and Validity. We conduct an extensive empirical analysis of the\ngenerated knowledge from three different types of LLMs on two widely studied\nknowledge-intensive tasks, i.e., open-domain question answering and\nknowledge-grounded dialogue. Surprisingly, our study reveals that the\nfactuality of generated knowledge, even if lower, does not significantly hinder\ndownstream tasks. Instead, the relevance and coherence of the outputs are more\nimportant than small factual mistakes. Further, we show how to use CONNER to\nimprove knowledge-intensive tasks by designing two strategies: Prompt\nEngineering and Knowledge Selection. Our evaluation code and LLM-generated\nknowledge with human annotations will be released to facilitate future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1\">Yatao Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zeyu Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bingzhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation. (arXiv:2310.07299v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07299","description":"<p>Grammatical Error Correction (GEC) systems play a vital role in assisting\npeople with their daily writing tasks. However, users may sometimes come across\na GEC system that initially performs well but fails to correct errors when the\ninputs are slightly modified. To ensure an ideal user experience, a reliable\nGEC system should have the ability to provide consistent and accurate\nsuggestions when encountering irrelevant context perturbations, which we refer\nto as context robustness. In this paper, we introduce RobustGEC, a benchmark\ndesigned to evaluate the context robustness of GEC systems. RobustGEC comprises\n5,000 GEC cases, each with one original error-correct sentence pair and five\nvariants carefully devised by human annotators. Utilizing RobustGEC, we reveal\nthat state-of-the-art GEC systems still lack sufficient robustness against\ncontext perturbations. In addition, we propose a simple yet effective method\nfor remitting this issue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_E/0/1/0/all/0/1\">Enbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions. (arXiv:2310.07301v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07301","description":"<p>Impressive progress has been made on chat models based on Large Language\nModels (LLMs) recently; however, there is a noticeable lag in multi-turn\nconversations between open-source chat models (e.g., Alpaca and Vicuna) and the\nleading chat models (e.g., ChatGPT and GPT-4). Through a series of analyses, we\nattribute the lag to the lack of enough high-quality multi-turn\ninstruction-tuning data. The available instruction-tuning data for the\ncommunity are either single-turn conversations or multi-turn ones with certain\nissues, such as non-human-like instructions, less detailed responses, or rare\ntopic shifts. In this paper, we address these challenges by introducing Parrot,\na highly scalable solution designed to automatically generate high-quality\ninstruction-tuning data, which are then used to enhance the effectiveness of\nchat models in multi-turn conversations. Specifically, we start by training the\nParrot-Ask model, which is designed to emulate real users in generating\ninstructions. We then utilize Parrot-Ask to engage in multi-turn conversations\nwith ChatGPT across a diverse range of topics, resulting in a collection of 40K\nhigh-quality multi-turn dialogues (Parrot-40K). These data are subsequently\nemployed to train a chat model that we have named Parrot-Chat. We demonstrate\nthat the dialogues gathered from Parrot-Ask markedly outperform existing\nmulti-turn instruction-following datasets in critical metrics, including topic\ndiversity, number of turns, and resemblance to human conversation. With only\n40K training examples, Parrot-Chat achieves strong performance against other\n13B open-source models across a range of instruction-following benchmarks, and\nparticularly excels in evaluations of multi-turn capabilities. We make all\ncodes, datasets, and two versions of the Parrot-Ask model based on LLaMA2-13B\nand KuaiYii-13B available at https://github.com/kwai/KwaiYii/Parrot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuchong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Che Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jinwen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Di Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1\">Kun Gai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SNOiC: Soft Labeling and Noisy Mixup based Open Intent Classification Model. (arXiv:2310.07306v1 [cs.LG])","link":"http://arxiv.org/abs/2310.07306","description":"<p>This paper presents a Soft Labeling and Noisy Mixup-based open intent\nclassification model (SNOiC). Most of the previous works have used\nthreshold-based methods to identify open intents, which are prone to\noverfitting and may produce biased predictions. Additionally, the need for more\navailable data for an open intent class presents another limitation for these\nexisting models. SNOiC combines Soft Labeling and Noisy Mixup strategies to\nreduce the biasing and generate pseudo-data for open intent class. The\nexperimental results on four benchmark datasets show that the SNOiC model\nachieves a minimum and maximum performance of 68.72\\% and 94.71\\%,\nrespectively, in identifying open intents. Moreover, compared to\nstate-of-the-art models, the SNOiC model improves the performance of\nidentifying open intents by 0.93\\% (minimum) and 12.76\\% (maximum). The model's\nefficacy is further established by analyzing various parameters used in the\nproposed model. An ablation study is also conducted, which involves creating\nthree model variants to validate the effectiveness of the SNOiC model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanwar_A/0/1/0/all/0/1\">Aditi Kanwar</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Seetha_A/0/1/0/all/0/1\">Aditi Seetha</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chouhan_S/0/1/0/all/0/1\">Satyendra Singh Chouhan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Niyogi_R/0/1/0/all/0/1\">Rajdeep Niyogi</a> (2) ((1) MNIT Jaipur, 302017, INDIA, (2) IIT Roorkee, 247667, INDIA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Impact of Cross-Domain Data on German Language Models. (arXiv:2310.07321v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07321","description":"<p>Traditionally, large language models have been either trained on general web\ncrawls or domain-specific data. However, recent successes of generative large\nlanguage models, have shed light on the benefits of cross-domain datasets. To\nexamine the significance of prioritizing data diversity over quality, we\npresent a German dataset comprising texts from five domains, along with another\ndataset aimed at containing high-quality data. Through training a series of\nmodels ranging between 122M and 750M parameters on both datasets, we conduct a\ncomprehensive benchmark on multiple downstream tasks. Our findings demonstrate\nthat the models trained on the cross-domain dataset outperform those trained on\nquality data alone, leading to improvements up to $4.45\\%$ over the previous\nstate-of-the-art. The models are available at\nhttps://huggingface.co/ikim-uk-essen\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dada_A/0/1/0/all/0/1\">Amin Dada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Aokun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Cheng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kaleb E Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idrissi_Yaghir_A/0/1/0/all/0/1\">Ahmad Idrissi-Yaghir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seibold_C/0/1/0/all/0/1\">Constantin Marc Seibold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heiliger_L/0/1/0/all/0/1\">Lars Heiliger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_C/0/1/0/all/0/1\">Christoph M. Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truhn_D/0/1/0/all/0/1\">Daniel Truhn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1\">Jan Egger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Instruction-tuning Large Language Models in Chinese. (arXiv:2310.07328v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07328","description":"<p>The success of ChatGPT validates the potential of large language models\n(LLMs) in artificial general intelligence (AGI). Subsequently, the release of\nLLMs has sparked the open-source community's interest in instruction-tuning,\nwhich is deemed to accelerate ChatGPT's replication process. However, research\non instruction-tuning LLMs in Chinese, the world's most spoken language, is\nstill in its early stages. Therefore, this paper makes an in-depth empirical\nstudy of instruction-tuning LLMs in Chinese, which can serve as a cookbook that\nprovides valuable findings for effectively customizing LLMs that can better\nrespond to Chinese instructions. Specifically, we systematically explore the\nimpact of LLM bases, parameter-efficient methods, instruction data types, which\nare the three most important elements for instruction-tuning. Besides, we also\nconduct experiment to study the impact of other factors, e.g., chain-of-thought\ndata and human-value alignment. We hope that this empirical study can make a\nmodest contribution to the open Chinese version of ChatGPT. This paper will\nrelease a powerful Chinese LLMs that is comparable to ChatGLM. The code and\ndata are available at https://github.com/PhoebusSi/Alpaca-CoT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_Q/0/1/0/all/0/1\">Qingyi Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances. (arXiv:2310.07343v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07343","description":"<p>Although large language models (LLMs) are impressive in solving various\ntasks, they can quickly be outdated after deployment. Maintaining their\nup-to-date status is a pressing concern in the current era. This paper provides\na comprehensive review of recent advances in aligning LLMs with the\never-changing world knowledge without re-training from scratch. We categorize\nresearch works systemically and provide in-depth comparisons and discussion. We\nalso discuss existing challenges and highlight future directions to facilitate\nresearch in this field. We release the paper list at\nhttps://github.com/hyintell/awesome-refreshing-llms\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Ling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namazi_Rad_M/0/1/0/all/0/1\">Mohammad-Reza Namazi-Rad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Effect of Language Models in Sequence Discriminative Training for Neural Transducers. (arXiv:2310.07345v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07345","description":"<p>In this work, we investigate the effect of language models (LMs) with\ndifferent context lengths and label units (phoneme vs. word) used in sequence\ndiscriminative training for phoneme-based neural transducers. Both lattice-free\nand N-best-list approaches are examined. For lattice-free methods with\nphoneme-level LMs, we propose a method to approximate the context history to\nemploy LMs with full-context dependency. This approximation can be extended to\narbitrary context length and enables the usage of word-level LMs in\nlattice-free methods. Moreover, a systematic comparison is conducted across\nlattice-free and N-best-list-based methods. Experimental results on Librispeech\nshow that using the word-level LM in training outperforms the phoneme-level LM.\nBesides, we find that the context size of the LM used for probability\ncomputation has a limited effect on performance. Moreover, our results reveal\nthe pivotal importance of the hypothesis space quality in sequence\ndiscriminative training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zijian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast-ELECTRA for Efficient Pre-training. (arXiv:2310.07347v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07347","description":"<p>ELECTRA pre-trains language models by detecting tokens in a sequence that\nhave been replaced by an auxiliary model. Although ELECTRA offers a significant\nboost in efficiency, its potential is constrained by the training cost brought\nby the auxiliary model. Notably, this model, which is jointly trained with the\nmain model, only serves to assist the training of the main model and is\ndiscarded post-training. This results in a substantial amount of training cost\nbeing expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which\nleverages an existing language model as the auxiliary model. To construct a\nlearning curriculum for the main model, we smooth its output distribution via\ntemperature scaling following a descending schedule. Our approach rivals the\nperformance of state-of-the-art ELECTRA-style pre-training methods, while\nsignificantly eliminating the computation and memory cost brought by the joint\ntraining of the auxiliary model. Our method also reduces the sensitivity to\nhyper-parameters and enhances the pre-training stability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic laws in biology. (arXiv:2310.07387v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07387","description":"<p>Linguistic laws, the common statistical patterns of human language, have been\ninvestigated by quantitative linguists for nearly a century. Recently,\nbiologists from a range of disciplines have started to explore the prevalence\nof these laws beyond language, finding patterns consistent with linguistic laws\nacross multiple levels of biological organisation, from molecular (genomes,\ngenes, and proteins) to organismal (animal behaviour) to ecological\n(populations and ecosystems). We propose a new conceptual framework for the\nstudy of linguistic laws in biology, comprising and integrating distinct levels\nof analysis, from description to prediction to theory building. Adopting this\nframework will provide critical new insights into the fundamental rules of\norganisation underpinning natural systems, unifying linguistic laws and core\ntheory in biology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Semple_S/0/1/0/all/0/1\">Stuart Semple</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gustison_M/0/1/0/all/0/1\">Morgan L. Gustison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation. (arXiv:2310.07397v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07397","description":"<p>Target-oriented dialogue systems, designed to proactively steer conversations\ntoward predefined targets or accomplish specific system-side goals, are an\nexciting area in conversational AI. In this work, by formulating a &lt;dialogue\nact, topic&gt; pair as the conversation target, we explore a novel problem of\npersonalized target-oriented dialogue by considering personalization during the\ntarget accomplishment process. However, there remains an emergent need for\nhigh-quality datasets, and building one from scratch requires tremendous human\neffort. To address this, we propose an automatic dataset curation framework\nusing a role-playing approach. Based on this framework, we construct a\nlarge-scale personalized target-oriented dialogue dataset, TopDial, which\ncomprises about 18K multi-turn dialogues. The experimental results show that\nthis dataset is of high quality and could contribute to exploring personalized\ntarget-oriented dialogue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dongding Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_C/0/1/0/all/0/1\">Chak Tou Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation. (arXiv:2310.07403v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07403","description":"<p>Direct speech-to-speech translation (S2ST) translates speech from one\nlanguage into another using a single model. However, due to the presence of\nlinguistic and acoustic diversity, the target speech follows a complex\nmultimodal distribution, posing challenges to achieving both high-quality\ntranslations and fast decoding speeds for S2ST models. In this paper, we\npropose DASpeech, a non-autoregressive direct S2ST model which realizes both\nfast and high-quality S2ST. To better capture the complex distribution of the\ntarget speech, DASpeech adopts the two-pass architecture to decompose the\ngeneration process into two steps, where a linguistic decoder first generates\nthe target text, and an acoustic decoder then generates the target speech based\non the hidden states of the linguistic decoder. Specifically, we use the\ndecoder of DA-Transformer as the linguistic decoder, and use FastSpeech 2 as\nthe acoustic decoder. DA-Transformer models translations with a directed\nacyclic graph (DAG). To consider all potential paths in the DAG during\ntraining, we calculate the expected hidden states for each target token via\ndynamic programming, and feed them into the acoustic decoder to predict the\ntarget mel-spectrogram. During inference, we select the most probable path and\ntake hidden states on that path as input to the acoustic decoder. Experiments\non the CVSS Fr-En benchmark demonstrate that DASpeech can achieve comparable or\neven better performance than the state-of-the-art S2ST model Translatotron 2,\nwhile preserving up to 18.53x speedup compared to the autoregressive baseline.\nCompared with the previous non-autoregressive S2ST model, DASpeech does not\nrely on knowledge distillation and iterative decoding, achieving significant\nimprovements in both translation quality and decoding speed. Furthermore,\nDASpeech shows the ability to preserve the speaker's voice of the source speech\nduring translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qingkai Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting the adapters for code-switching in multilingual ASR. (arXiv:2310.07423v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07423","description":"<p>Recently, large pre-trained multilingual speech models have shown potential\nin scaling Automatic Speech Recognition (ASR) to many low-resource languages.\nSome of these models employ language adapters in their formulation, which helps\nto improve monolingual performance and avoids some of the drawbacks of\nmulti-lingual modeling on resource-rich languages. However, this formulation\nrestricts the usability of these models on code-switched speech, where two\nlanguages are mixed together in the same utterance. In this work, we propose\nways to effectively fine-tune such models on code-switched speech, by\nassimilating information from both language adapters at each language\nadaptation point in the network. We also model code-switching as a sequence of\nlatent binary sequences that can be used to guide the flow of information from\neach language adapter at the frame level. The proposed approaches are evaluated\non three code-switched datasets encompassing Arabic, Mandarin, and Hindi\nlanguages paired with English, showing consistent improvements in\ncode-switching performance with at least 10\\% absolute reduction in CER across\nall test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Atharva Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Ajinkya Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couceiro_M/0/1/0/all/0/1\">Miguel Couceiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldarmaki_H/0/1/0/all/0/1\">Hanan Aldarmaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction. (arXiv:2310.07487v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07487","description":"<p>Phonological reconstruction is one of the central problems in historical\nlinguistics where a proto-word of an ancestral language is determined from the\nobserved cognate words of daughter languages. Computational approaches to\nhistorical linguistics attempt to automate the task by learning models on\navailable linguistic data. Several ideas and techniques drawn from\ncomputational biology have been successfully applied in the area of\ncomputational historical linguistics. Following these lines, we adapt MSA\nTransformer, a protein language model, to the problem of automated phonological\nreconstruction. MSA Transformer trains on multiple sequence alignments as input\nand is, thus, apt for application on aligned cognate words. We, hence, name our\nmodel as Cognate Transformer. We also apply the model on another associated\ntask, namely, cognate reflex prediction, where a reflex word in a daughter\nlanguage is predicted based on cognate words from other daughter languages. We\nshow that our model outperforms the existing models on both tasks, especially\nwhen it is pre-trained on masked word prediction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akavarapu_V/0/1/0/all/0/1\">V.S.D.S. Mahesh Akavarapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Arnab Bhattacharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KwaiYiiMath: Technical Report. (arXiv:2310.07488v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07488","description":"<p>Recent advancements in large language models (LLMs) have demonstrated\nremarkable abilities in handling a variety of natural language processing (NLP)\ndownstream tasks, even on mathematical tasks requiring multi-step reasoning. In\nthis report, we introduce the KwaiYiiMath which enhances the mathematical\nreasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT)\nand Reinforced Learning from Human Feedback (RLHF), including on both English\nand Chinese mathematical tasks. Meanwhile, we also constructed a small-scale\nChinese primary school mathematics test set (named KMath), consisting of 188\nexamples to evaluate the correctness of the problem-solving process generated\nby the models. Empirical studies demonstrate that KwaiYiiMath can achieve\nstate-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with\nthe similar size models, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jiayi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiaoyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengzong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhirui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengnan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xue Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xucheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yiqiao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1\">Chao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chengru Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Junchen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zijia Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Di Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1\">Kun Gai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. (arXiv:2310.07521v1 [cs.CL])","link":"http://arxiv.org/abs/2310.07521","description":"<p>This survey addresses the crucial issue of factuality in Large Language\nModels (LLMs). As LLMs find applications across diverse domains, the\nreliability and accuracy of their outputs become vital. We define the\nFactuality Issue as the probability of LLMs to produce content inconsistent\nwith established facts. We first delve into the implications of these\ninaccuracies, highlighting the potential consequences and challenges posed by\nfactual errors in LLM outputs. Subsequently, we analyze the mechanisms through\nwhich LLMs store and process facts, seeking the primary causes of factual\nerrors. Our discussion then transitions to methodologies for evaluating LLM\nfactuality, emphasizing key metrics, benchmarks, and studies. We further\nexplore strategies for enhancing LLM factuality, including approaches tailored\nfor specific domains. We focus two primary LLM configurations standalone LLMs\nand Retrieval-Augmented LLMs that utilizes external data, we detail their\nunique challenges and potential enhancements. Our survey offers a structured\nguide for researchers aiming to fortify the factual reliability of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cunxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yuanhao Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiayang_C/0/1/0/all/0/1\">Cheng Jiayang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wenyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zehan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v8 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08614","description":"<p>Question answering over RDF data like knowledge graphs has been greatly\nadvanced, with a number of good systems providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, the IR and NLP\ncommunities have addressed QA over text, but such systems barely utilize\nsemantic data and knowledge. This paper presents a method for complex questions\nthat can seamlessly operate over a mixture of RDF datasets and text corpora, or\nindividual sources, in a unified framework. Our method, called UNIQORN, builds\na context graph on-the-fly, by retrieving question-relevant evidences from the\nRDF data and/or a text corpus, using fine-tuned BERT models. The resulting\ngraph typically contains all question-relevant evidences but also a lot of\nnoise. UNIQORN copes with this input by a graph algorithm for Group Steiner\nTrees, that identifies the best answer candidates in the context graph.\nExperimental results on several benchmarks of complex questions with multiple\nentities and relations, show that UNIQORN significantly outperforms\nstate-of-the-art methods for heterogeneous QA -- in a full training mode, as\nwell as in zero-shot settings. The graph-based methodology provides\nuser-interpretable evidence for the complete answering process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Soumajit Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledgeable Salient Span Mask for Enhancing Language Models as Knowledge Base. (arXiv:2204.07994v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07994","description":"<p>Pre-trained language models (PLMs) like BERT have made significant progress\nin various downstream NLP tasks. However, by asking models to do cloze-style\ntests, recent work finds that PLMs are short in acquiring knowledge from\nunstructured text. To understand the internal behaviour of PLMs in retrieving\nknowledge, we first define knowledge-baring (K-B) tokens and knowledge-free\n(K-F) tokens for unstructured text and ask professional annotators to label\nsome samples manually. Then, we find that PLMs are more likely to give wrong\npredictions on K-B tokens and attend less attention to those tokens inside the\nself-attention module. Based on these observations, we develop two solutions to\nhelp the model learn more knowledge from unstructured text in a fully\nself-supervised manner. Experiments on knowledge-intensive tasks show the\neffectiveness of the proposed methods. To our best knowledge, we are the first\nto explore fully self-supervised learning of knowledge in continual\npre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cunxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fuli Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Multilingual Semantic Parser. (arXiv:2301.12920v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12920","description":"<p>Current multilingual semantic parsing (MSP) datasets are almost all collected\nby translating the utterances in the existing datasets from the resource-rich\nlanguage to the target language. However, manual translation is costly. To\nreduce the translation effort, this paper proposes the first active learning\nprocedure for MSP (AL-MSP). AL-MSP selects only a subset from the existing\ndatasets to be translated. We also propose a novel selection method that\nprioritizes the examples diversifying the logical form structures with more\nlexical choices, and a novel hyperparameter tuning method that needs no extra\nannotation cost. Our experiments show that AL-MSP significantly reduces\ntranslation costs with ideal selection methods. Our selection method with\nproper hyperparameters yields better parsing performance than the other\nbaselines on two multilingual datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query2doc: Query Expansion with Large Language Models. (arXiv:2303.07678v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2303.07678","description":"<p>This paper introduces a simple yet effective query expansion approach,\ndenoted as query2doc, to improve both sparse and dense retrieval systems. The\nproposed method first generates pseudo-documents by few-shot prompting large\nlanguage models (LLMs), and then expands the query with generated\npseudo-documents. LLMs are trained on web-scale text corpora and are adept at\nknowledge memorization. The pseudo-documents from LLMs often contain highly\nrelevant information that can aid in query disambiguation and guide the\nretrievers. Experimental results demonstrate that query2doc boosts the\nperformance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and\nTREC DL, without any model fine-tuning. Furthermore, our method also benefits\nstate-of-the-art dense retrievers in terms of both in-domain and out-of-domain\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chat with the Environment: Interactive Multimodal Perception Using Large Language Models. (arXiv:2303.08268v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2303.08268","description":"<p>Programming robot behavior in a complex world faces challenges on multiple\nlevels, from dextrous low-level skills to high-level planning and reasoning.\nRecent pre-trained Large Language Models (LLMs) have shown remarkable reasoning\nability in few-shot robotic planning. However, it remains challenging to ground\nLLMs in multimodal sensory input and continuous action output, while enabling a\nrobot to interact with its environment and acquire novel information as its\npolicies unfold. We develop a robot interaction scenario with a partially\nobservable state, which necessitates a robot to decide on a range of epistemic\nactions in order to sample sensory information among multiple modalities,\nbefore being able to execute the task correctly. Matcha (Multimodal environment\nchatting) agent, an interactive perception framework, is therefore proposed\nwith an LLM as its backbone, whose ability is exploited to instruct epistemic\nactions and to reason over the resulting multimodal sensations (vision, sound,\nhaptics, proprioception), as well as to plan an entire task execution based on\nthe interactively acquired information. Our study demonstrates that LLMs can\nprovide high-level planning and reasoning skills and control interactive robot\nbehavior in a multimodal environment, while multimodal modules with the context\nof the environmental state help ground the LLMs and extend their processing\nability. The project website can be found at https://matcha-agent.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xufeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengdi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hafez_M/0/1/0/all/0/1\">Muhammad Burhan Hafez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation. (arXiv:2303.08518v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08518","description":"<p>Large Language Models (LLMs) are popular for their impressive abilities, but\nthe need for model-specific fine-tuning or task-specific prompt engineering can\nhinder their generalization. We propose UPRISE (Universal Prompt Retrieval for\nImproving zero-Shot Evaluation), which tunes a lightweight and versatile\nretriever that automatically retrieves prompts for a given zero-shot task\ninput. Specifically, we demonstrate universality in a cross-task and\ncross-model scenario: the retriever is tuned on a diverse set of tasks, but\ntested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for\ntuning the retriever, but test the retriever on different LLMs of much larger\nscales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that\nUPRISE mitigates the hallucination problem in our experiments with ChatGPT,\nsuggesting its potential to improve even the strongest LLMs. Our model and code\nare available at https://github.com/microsoft/LMOps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Daixuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Junyu Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yuefeng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianfeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Denvy Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. (arXiv:2303.08896v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08896","description":"<p>Generative Large Language Models (LLMs) such as GPT-3 are capable of\ngenerating highly fluent responses to a wide variety of user prompts. However,\nLLMs are known to hallucinate facts and make non-factual statements which can\nundermine trust in their output. Existing fact-checking approaches either\nrequire access to the output probability distribution (which may not be\navailable for systems such as ChatGPT) or external databases that are\ninterfaced via separate, often complex, modules. In this work, we propose\n\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check\nthe responses of black-box models in a zero-resource fashion, i.e. without an\nexternal database. SelfCheckGPT leverages the simple idea that if an LLM has\nknowledge of a given concept, sampled responses are likely to be similar and\ncontain consistent facts. However, for hallucinated facts, stochastically\nsampled responses are likely to diverge and contradict one another. We\ninvestigate this approach by using GPT-3 to generate passages about individuals\nfrom the WikiBio dataset, and manually annotate the factuality of the generated\npassages. We demonstrate that SelfCheckGPT can: i) detect non-factual and\nfactual sentences; and ii) rank passages in terms of factuality. We compare our\napproach to several baselines and show that our approach has considerably\nhigher AUC-PR scores in sentence-level hallucination detection and higher\ncorrelation scores in passage-level factuality assessment compared to grey-box\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manakul_P/0/1/0/all/0/1\">Potsawee Manakul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liusie_A/0/1/0/all/0/1\">Adian Liusie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Interpretable Mental Health Analysis with Large Language Models. (arXiv:2304.03347v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.03347","description":"<p>The latest large language models (LLMs) such as ChatGPT, exhibit strong\ncapabilities in automated mental health analysis. However, existing relevant\nstudies bear several limitations, including inadequate evaluations, lack of\nprompting strategies, and ignorance of exploring LLMs for explainability. To\nbridge these gaps, we comprehensively evaluate the mental health analysis and\nemotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore\nthe effects of different prompting strategies with unsupervised and distantly\nsupervised emotional information. Based on these prompts, we explore LLMs for\ninterpretable mental health analysis by instructing them to generate\nexplanations for each of their decisions. We convey strict human evaluations to\nassess the quality of the generated explanations, leading to a novel dataset\nwith 163 human-assessed explanations. We benchmark existing automatic\nevaluation metrics on this dataset to guide future related works. According to\nthe results, ChatGPT shows strong in-context learning ability but still has a\nsignificant gap with advanced task-specific methods. Careful prompt engineering\nwith emotional cues and expert-written few-shot examples can also effectively\nimprove performance on mental health analysis. In addition, ChatGPT generates\nexplanations that approach human performance, showing its great potential in\nexplainable mental health analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Ziyan Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.11082","description":"<p>An important aspect in developing language models that interact with humans\nis aligning their behavior to be useful and unharmful for their human users.\nThis is usually achieved by tuning the model in a way that enhances desired\nbehaviors and inhibits undesired ones, a process referred to as alignment. In\nthis paper, we propose a theoretical approach called Behavior Expectation\nBounds (BEB) which allows us to formally investigate several inherent\ncharacteristics and limitations of alignment in large language models.\nImportantly, we prove that within the limits of this framework, for any\nbehavior that has a finite probability of being exhibited by the model, there\nexist prompts that can trigger the model into outputting this behavior, with\nprobability that increases with the length of the prompt. This implies that any\nalignment process that attenuates an undesired behavior but does not remove it\naltogether, is not safe against adversarial prompting attacks. Furthermore, our\nframework hints at the mechanism by which leading alignment approaches such as\nreinforcement learning from human feedback make the LLM prone to being prompted\ninto the undesired behaviors. This theoretical result is being experimentally\ndemonstrated in large scale by the so called contemporary \"chatGPT jailbreaks\",\nwhere adversarial users trick the LLM into breaking its alignment guardrails by\ntriggering it into acting as a malicious persona. Our results expose\nfundamental limitations in alignment of LLMs and bring to the forefront the\nneed to devise reliable mechanisms for ensuring AI safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolf_Y/0/1/0/all/0/1\">Yotam Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1\">Noam Wies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avnery_O/0/1/0/all/0/1\">Oshri Avnery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dissecting Recall of Factual Associations in Auto-Regressive Language Models. (arXiv:2304.14767v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14767","description":"<p>Transformer-based language models (LMs) are known to capture factual\nknowledge in their parameters. While previous work looked into where factual\nassociations are stored, only little is known about how they are retrieved\ninternally during inference. We investigate this question through the lens of\ninformation flow. Given a subject-relation query, we study how the model\naggregates information about the subject and relation to predict the correct\nattribute. With interventions on attention edges, we first identify two\ncritical points where information propagates to the prediction: one from the\nrelation positions followed by another from the subject positions. Next, by\nanalyzing the information at these points, we unveil a three-step internal\nmechanism for attribute extraction. First, the representation at the\nlast-subject position goes through an enrichment process, driven by the early\nMLP sublayers, to encode many subject-related attributes. Second, information\nfrom the relation propagates to the prediction. Third, the prediction\nrepresentation \"queries\" the enriched subject to extract the attribute. Perhaps\nsurprisingly, this extraction is typically done via attention heads, which\noften encode subject-attribute mappings in their parameters. Overall, our\nfindings introduce a comprehensive view of how factual associations are stored\nand extracted internally in LMs, facilitating future research on knowledge\nlocalization and editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1\">Jasmijn Bastings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filippova_K/0/1/0/all/0/1\">Katja Filippova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Multimodal Model Merging. (arXiv:2304.14933v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.14933","description":"<p>Model merging (e.g., via interpolation or task arithmetic) fuses multiple\nmodels trained on different tasks to generate a multi-task solution. The\ntechnique has been proven successful in previous studies, where the models are\ntrained on similar tasks and with the same initialization. In this paper, we\nexpand on this concept to a multimodal setup by merging transformers trained on\ndifferent modalities. Furthermore, we conduct our study for a novel goal where\nwe can merge vision, language, and cross-modal transformers of a\nmodality-specific architecture to create a parameter-efficient\nmodality-agnostic architecture. Through comprehensive experiments, we\nsystematically investigate the key factors impacting model performance after\nmerging, including initialization, merging mechanisms, and model architectures.\nWe also propose two metrics that assess the distance between weights to be\nmerged and can serve as an indicator of the merging outcomes. Our analysis\nleads to an effective training recipe for matching the performance of the\nmodality-agnostic baseline (i.e., pre-trained from scratch) via model merging.\nOur method also outperforms naive merging significantly on various tasks, with\nimprovements of 3% on VQA, 7% on COCO retrieval, 25% on NLVR2, 14% on Flickr30k\nand 3% on ADE20k. Our code is available at https://github.com/ylsung/vl-merging\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yi-Lin Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TidyBot: Personalized Robot Assistance with Large Language Models. (arXiv:2305.05658v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2305.05658","description":"<p>For a robot to personalize physical assistance effectively, it must learn\nuser preferences that can be generally reapplied to future scenarios. In this\nwork, we investigate personalization of household cleanup with robots that can\ntidy up rooms by picking up objects and putting them away. A key challenge is\ndetermining the proper place to put each object, as people's preferences can\nvary greatly depending on personal taste or cultural background. For instance,\none person may prefer storing shirts in the drawer, while another may prefer\nthem on the shelf. We aim to build systems that can learn such preferences from\njust a handful of examples via prior interactions with a particular person. We\nshow that robots can combine language-based planning and perception with the\nfew-shot summarization capabilities of large language models (LLMs) to infer\ngeneralized user preferences that are broadly applicable to future\ninteractions. This approach enables fast adaptation and achieves 91.2% accuracy\non unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away\n85.0% of objects in real-world test scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jimmy Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonova_R/0/1/0/all/0/1\">Rika Antonova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_A/0/1/0/all/0/1\">Adam Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepert_M/0/1/0/all/0/1\">Marion Lepert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1\">Jeannette Bohg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusinkiewicz_S/0/1/0/all/0/1\">Szymon Rusinkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1\">Thomas Funkhouser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks. (arXiv:2305.05862v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05862","description":"<p>The most recent large language models(LLMs) such as ChatGPT and GPT-4 have\nshown exceptional capabilities of generalist models, achieving state-of-the-art\nperformance on a wide range of NLP tasks with little or no adaptation. How\neffective are such models in the financial domain? Understanding this basic\nquestion would have a significant impact on many downstream financial\nanalytical tasks. In this paper, we conduct an empirical study and provide\nexperimental evidences of their performance on a wide variety of financial text\nanalytical problems, using eight benchmark datasets from five categories of\ntasks. We report both the strengths and limitations of the current models by\ncomparing them to the state-of-the-art fine-tuned approaches and the recently\nreleased domain-specific pretrained models. We hope our study can help\nunderstand the capability of the existing models in the financial domain and\nfacilitate further improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Samuel Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1\">Yulong Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhiqiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaomo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sameena Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Schema-adaptable Knowledge Graph Construction. (arXiv:2305.08703v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08703","description":"<p>Conventional Knowledge Graph Construction (KGC) approaches typically follow\nthe static information extraction paradigm with a closed set of pre-defined\nschema. As a result, such approaches fall short when applied to dynamic\nscenarios or domains, whereas a new type of knowledge emerges. This\nnecessitates a system that can handle evolving schema automatically to extract\ninformation for KGC. To address this need, we propose a new task called\nschema-adaptable KGC, which aims to continually extract entity, relation, and\nevent based on a dynamically changing schema graph without re-training. We\nfirst split and convert existing datasets based on three principles to build a\nbenchmark, i.e., horizontal schema expansion, vertical schema expansion, and\nhybrid schema expansion; then investigate the schema-adaptable performance of\nseveral well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We\nfurther propose a simple yet effective baseline dubbed \\textsc{AdaKGC}, which\ncontains schema-enriched prefix instructor and schema-conditioned dynamic\ndecoding to better handle evolving schema. Comprehensive experimental results\nillustrate that AdaKGC can outperform baselines but still have room for\nimprovement. We hope the proposed work can deliver benefits to the community.\nCode and datasets available at https://github.com/zjunlp/AdaKGC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1\">Honghao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Rumination for Pre-trained Language Models. (arXiv:2305.08732v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08732","description":"<p>Previous studies have revealed that vanilla pre-trained language models\n(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,\nseveral works have attempted to integrate external knowledge into PLMs.\nHowever, despite the promising outcome, we empirically observe that PLMs may\nhave already encoded rich knowledge in their pre-trained parameters but fail to\nfully utilize them when applying them to knowledge-intensive tasks. In this\npaper, we propose a new paradigm dubbed Knowledge Rumination to help the\npre-trained language model utilize that related latent knowledge without\nretrieving it from the external corpus. By simply adding a prompt like \"As far\nas I know\" to the PLMs, we try to review related latent knowledge and inject\nthem back into the model for knowledge consolidation. We apply the proposed\nknowledge rumination to various language models, including RoBERTa, DeBERTa,\nand GPT-3. Experimental results on six commonsense reasoning tasks and GLUE\nbenchmarks demonstrate the effectiveness of our proposed approach, which proves\nthat the knowledge stored in PLMs can be better exploited to enhance\nperformance. Code is available in\nhttps://github.com/zjunlp/knowledge-rumination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shengyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Equivariant Transfer Learning from Pretrained Models. (arXiv:2305.09900v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.09900","description":"<p>Efficient transfer learning algorithms are key to the success of foundation\nmodels on diverse downstream tasks even with limited data. Recent works of Basu\net al. (2023) and Kaba et al. (2022) propose group averaging (equitune) and\noptimization-based methods, respectively, over features from group-transformed\ninputs to obtain equivariant outputs from non-equivariant neural networks.\nWhile Kaba et al. (2022) are only concerned with training from scratch, we find\nthat equitune performs poorly on equivariant zero-shot tasks despite good\nfinetuning results. We hypothesize that this is because pretrained models\nprovide better quality features for certain transformations than others and\nsimply averaging them is deleterious. Hence, we propose {\\lambda}-equitune that\naverages the features using importance weights, {\\lambda}s. These weights are\nlearned directly from the data using a small neural network, leading to\nexcellent zero-shot and finetuned results that outperform equitune. Further, we\nprove that {\\lambda}-equitune is equivariant and a universal approximator of\nequivariant functions. Additionally, we show that the method of Kaba et al.\n(2022) used with appropriate loss functions, which we call equizero, also gives\nexcellent zero-shot and finetuned performance. Both equitune and equizero are\nspecial cases of {\\lambda}-equitune. To show the simplicity and generality of\nour method, we validate on a wide range of diverse applications and models such\nas 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in\nnatural language generation (NLG), 4) compositional generalization in\nlanguages, and 5) image classification using pretrained CNNs such as Resnet and\nAlexnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Sourya Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katdare_P/0/1/0/all/0/1\">Pulkit Katdare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattigeri_P/0/1/0/all/0/1\">Prasanna Sattigeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenthamarakshan_V/0/1/0/all/0/1\">Vijil Chenthamarakshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1\">Katherine Driggs-Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_L/0/1/0/all/0/1\">Lav R. Varshney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13172","description":"<p>Despite the ability to train capable LLMs, the methodology for maintaining\ntheir relevancy and rectifying errors remains elusive. To this end, the past\nfew years have witnessed a surge in techniques for editing LLMs, the objective\nof which is to efficiently alter the behavior of LLMs within a specific domain\nwithout negatively impacting performance across other inputs. This paper\nembarks on a deep exploration of the problems, methods, and opportunities\nrelated to model editing for LLMs. In particular, we provide an exhaustive\noverview of the task definition and challenges associated with model editing,\nalong with an in-depth empirical analysis of the most progressive methods\ncurrently at our disposal. We also build a new benchmark dataset to facilitate\na more robust evaluation and pinpoint enduring issues intrinsic to existing\ntechniques. Our objective is to provide valuable insights into the\neffectiveness and feasibility of each editing technique, thereby assisting the\ncommunity in making informed decisions on the selection of the most appropriate\nmethod for a specific task or context. Code and datasets are available at\nhttps://github.com/zjunlp/EasyEdit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1\">Bozhong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. (arXiv:2305.14251v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14251","description":"<p>Evaluating the factuality of long-form text generated by large language\nmodels (LMs) is non-trivial because (1) generations often contain a mixture of\nsupported and unsupported pieces of information, making binary judgments of\nquality inadequate, and (2) human evaluation is time-consuming and costly. In\nthis paper, we introduce FACTSCORE, a new evaluation that breaks a generation\ninto a series of atomic facts and computes the percentage of atomic facts\nsupported by a reliable knowledge source. We conduct an extensive human\nevaluation to obtain FACTSCOREs of people biographies generated by several\nstate-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the\nretrieval-augmented PerplexityAI -- and report new analysis demonstrating the\nneed for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since\nhuman evaluation is costly, we also introduce an automated model that estimates\nFACTSCORE using retrieval and a strong language model, with less than a 2%\nerror rate. Finally, we use this automated metric to evaluate 6,500 generations\nfrom a new set of 13 recent LMs that would have cost $26K if evaluated by\nhumans, with various findings: GPT-4 and ChatGPT are more factual than public\nmodels, and Vicuna and Alpaca are some of the best public models. FACTSCORE is\navailable for public use via `pip install factscore`.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xinxi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1\">Pang Wei Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Language Models with Advantage-based Offline Policy Gradients. (arXiv:2305.14718v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14718","description":"<p>Language Models (LMs) achieve substantial language capabilities when\nfinetuned using Reinforcement Learning with Human Feedback (RLHF). However,\nRLHF is an unstable and data-hungry process that continually requires new\nhigh-quality LM-generated data for finetuning. We introduce Advantage-Leftover\nLunch RL (A-LoL), a new class of offline policy gradient algorithms that enable\nRL training on any pre-existing data. By assuming the entire LM output sequence\nas a single action, A-LoL allows incorporating sequence-level classifiers or\nhuman-designed scoring functions as rewards. Subsequently, by using LM's\ninternal sequence-level value estimate, A-LoL filters negative advantage\n(low-quality) data points during training, making it resilient to noise.\nOverall, A-LoL is an easy-to-implement LM training recipe that is\nsample-efficient and stable.\n</p>\n<p>We demonstrate the effectiveness of A-LoL and its variants with a set of four\ndifferent language generation tasks. We compare against both online RL (PPO)\nand recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL\nbaselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant\n(HHA), LMs trained with A-LoL methods achieve the highest diversity while also\nbeing rated more safe and helpful than baselines according to humans.\nAdditionally, in the remaining three tasks, A-LoL could optimize multiple\ndistinct reward functions even when using noisy or suboptimal training data. We\nalso release our experimental code. https://github.com/abaheti95/LoL-RL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baheti_A/0/1/0/all/0/1\">Ashutosh Baheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning. (arXiv:2305.14761v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14761","description":"<p>Charts are very popular for analyzing data, visualizing key insights and\nanswering complex reasoning questions about data. To facilitate chart-based\ndata analysis using natural language, several downstream tasks have been\nintroduced recently such as chart question answering and chart summarization.\nHowever, most of the methods that solve these tasks use pretraining on language\nor vision-language tasks that do not attempt to explicitly model the structure\nof the charts (e.g., how data is visually encoded and how chart elements are\nrelated to each other). To address this, we first build a large corpus of\ncharts covering a wide variety of topics and visual styles. We then present\nUniChart, a pretrained model for chart comprehension and reasoning. UniChart\nencodes the relevant text, data, and visual elements of charts and then uses a\nchart-grounded text decoder to generate the expected output in natural\nlanguage. We propose several chart-specific pretraining tasks that include: (i)\nlow-level tasks to extract the visual elements (e.g., bars, lines) and data\nfrom charts, and (ii) high-level tasks to acquire chart understanding and\nreasoning skills. We find that pretraining the model on a large corpus with\nchart-specific low- and high-level tasks followed by finetuning on three\ndown-streaming tasks results in state-of-the-art performance on three\ndownstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Masry_A/0/1/0/all/0/1\">Ahmed Masry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavehzadeh_P/0/1/0/all/0/1\">Parsa Kavehzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_X/0/1/0/all/0/1\">Xuan Long Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Enamul Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models. (arXiv:2305.15074v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15074","description":"<p>The performance of large language models (LLMs) on existing reasoning\nbenchmarks has significantly improved over the past years. In response, we\npresent JEEBench, a considerably more challenging benchmark dataset for\nevaluating the problem solving abilities of LLMs. We curate 515 challenging\npre-engineering mathematics, physics and chemistry problems from the highly\ncompetitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep\nin-domain knowledge is essential for solving problems in this benchmark. Our\nevaluation on various open-source and proprietary models reveals that the\nhighest performance, even after using techniques like self-consistency,\nself-refinement and chain-of-thought prompting, is less than 40\\%. The typical\nfailure modes of GPT-4, the best model, are errors in algebraic manipulation,\ndifficulty in grounding abstract concepts into mathematical equations\naccurately and failure in retrieving relevant domain-specific concepts. We also\nobserve that by mere prompting, GPT-4 is unable to assess risk introduced by\nnegative marking for incorrect answers. For this, we develop a post-hoc\nconfidence-thresholding method over self-consistency, which enables effective\nresponse selection. We hope that our challenging benchmark will guide future\nre-search in problem-solving using LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_D/0/1/0/all/0/1\">Daman Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1\">Himanshu Gaurav Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16340","description":"<p>Transformers have shown dominant performance across a range of domains\nincluding language and vision. However, their computational cost grows\nquadratically with the sequence length, making their usage prohibitive for\nresource-constrained applications. To counter this, our approach is to divide\nthe whole sequence into segments and use local attention mechanism on the\nindividual segments. We propose a segmented recurrent transformer (SRformer)\nthat combines segmented (local) attention with recurrent attention. The loss\ncaused by reducing the attention window length is compensated by aggregating\ninformation across segments with recurrent attention. SRformer leverages\nRecurrent Accumulate-and-Fire (RAF) neurons' inherent memory to update the\ncumulative product of keys and values. The segmented attention and lightweight\nRAF neurons ensure the efficiency of the proposed transformer. Such an approach\nleads to models with sequential processing capability at a lower\ncomputation/memory cost. We apply the proposed method to T5 and BART\ntransformers. The modified models are tested on summarization datasets\nincluding CNN-dailymail, XSUM, ArXiv, and MediaSUM. Notably, using segmented\ninputs of varied sizes, the proposed model achieves $6-22\\%$ higher ROUGE1\nscores than a segmented transformer and outperforms other recurrent transformer\napproaches. Furthermore, compared to full attention, the proposed model reduces\nthe computational complexity of cross attention by around $40\\%$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yinghan Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Sayeed Shafayet Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.09442","description":"<p>Deploying large language models (LMs) can pose hazards from harmful outputs\nsuch as toxic or false text. Prior work has introduced automated tools that\nelicit harmful outputs to identify these risks. While this is a valuable step\ntoward securing models, these approaches rely on a pre-existing way to\nefficiently classify undesirable outputs. Using a pre-existing classifier does\nnot allow for red-teaming to be tailored to the target model. Furthermore, when\nfailures can be easily classified in advance, red-teaming has limited marginal\nvalue because problems can be avoided by simply filtering training data and/or\nmodel outputs. Here, we consider red-teaming \"from scratch,\" in which the\nadversary does not begin with a way to classify failures. Our framework\nconsists of three steps: 1) Exploring the model's range of behaviors in the\ndesired context; 2) Establishing a definition and measurement for undesired\nbehavior (e.g., a classifier trained to reflect human evaluations); and 3)\nExploiting the model's flaws using this measure to develop diverse adversarial\nprompts. We use this approach to red-team GPT-3 to discover classes of inputs\nthat elicit false statements. In doing so, we construct the CommonClaim dataset\nof 20,000 statements labeled by humans as common-knowledge-true, common\nknowledge-false, or neither. We are making code and data available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1\">Stephen Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jason Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Joe Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culp_G/0/1/0/all/0/1\">Gatlen Culp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1\">Dylan Hadfield-Menell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution. (arXiv:2306.12424v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2306.12424","description":"<p>We introduce VisoGender, a novel dataset for benchmarking gender bias in\nvision-language models. We focus on occupation-related biases within a\nhegemonic system of binary gender, inspired by Winograd and Winogender schemas,\nwhere each image is associated with a caption containing a pronoun relationship\nof subjects and objects in the scene. VisoGender is balanced by gender\nrepresentation in professional roles, supporting bias evaluation in two ways:\ni) resolution bias, where we evaluate the difference between pronoun resolution\naccuracies for image subjects with gender presentations perceived as masculine\nversus feminine by human annotators and ii) retrieval bias, where we compare\nratios of professionals perceived to have masculine and feminine gender\npresentations retrieved for a gender-neutral search query. We benchmark several\nstate-of-the-art vision-language models and find that they demonstrate bias in\nresolving binary gender in complex scenes. While the direction and magnitude of\ngender bias depends on the task and the model being evaluated, captioning\nmodels are generally less biased than Vision-Language Encoders. Dataset and\ncode are available at https://github.com/oxai/visogender\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1\">Siobhan Mackenzie Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrantes_F/0/1/0/all/0/1\">Fernanda Gon&#xe7;alves Abrantes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hanwen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sodunke_G/0/1/0/all/0/1\">Grace Sodunke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1\">Aleksandar Shtedritski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of the Cambridge Multiple-Choice Questions Reading Dataset with a Focus on Candidate Response Distribution. (arXiv:2306.13047v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.13047","description":"<p>Multiple choice exams are widely used to assess candidates across a diverse\nrange of domains and tasks. To moderate question quality, newly proposed\nquestions often pass through pre-test evaluation stages before being deployed\ninto real-world exams. Currently, this evaluation process is manually\nintensive, which can lead to time lags in the question development cycle.\nStreamlining this process via automation can significantly enhance efficiency,\nhowever, there's a current lack of datasets with adequate pre-test analysis\ninformation. In this paper we analyse the Cambridge Multiple-Choice Questions\nReading Dataset; a multiple-choice comprehension dataset of questions at\ndifferent target levels, with corresponding candidate selection distributions.\nWe introduce the task of candidate distribution matching, propose several\nevaluation metrics for the task, and demonstrate that automatic systems trained\non RACE++ can be leveraged as baselines for our task. We further demonstrate\nthat these automatic systems can be used for practical pre-test evaluation\ntasks such as detecting underperforming distractors, where our detection\nsystems can automatically identify poor distractors that few candidates select.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liusie_A/0/1/0/all/0/1\">Adian Liusie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vatsal Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullooly_A/0/1/0/all/0/1\">Andrew Mullooly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knill_K/0/1/0/all/0/1\">Kate Knill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VerifAI: Verified Generative AI. (arXiv:2307.02796v2 [cs.DB] UPDATED)","link":"http://arxiv.org/abs/2307.02796","description":"<p>Generative AI has made significant strides, yet concerns about the accuracy\nand reliability of its outputs continue to grow. Such inaccuracies can have\nserious consequences such as inaccurate decision-making, the spread of false\ninformation, privacy violations, legal liabilities, and more. Although efforts\nto address these risks are underway, including explainable AI and responsible\nAI practices such as transparency, privacy protection, bias mitigation, and\nsocial and environmental responsibility, misinformation caused by generative AI\nwill remain a significant challenge. We propose that verifying the outputs of\ngenerative AI from a data management perspective is an emerging issue for\ngenerative AI. This involves analyzing the underlying data from multi-modal\ndata lakes, including text files, tables, and knowledge graphs, and assessing\nits quality and consistency. By doing so, we can establish a stronger\nfoundation for evaluating the outputs of generative AI models. Such an approach\ncan ensure the correctness of generative AI, promote transparency, and enable\ndecision-making with greater confidence. Our vision is to promote the\ndevelopment of verifiable generative AI and contribute to a more trustworthy\nand responsible use of AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_N/0/1/0/all/0/1\">Nan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Ju Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Lei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1\">Alon Halevy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models. (arXiv:2307.14539v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2307.14539","description":"<p>We introduce new jailbreak attacks on vision language models (VLMs), which\nuse aligned LLMs and are resilient to text-only jailbreak attacks.\nSpecifically, we develop cross-modality attacks on alignment where we pair\nadversarial images going through the vision encoder with textual prompts to\nbreak the alignment of the language model. Our attacks employ a novel\ncompositional strategy that combines an image, adversarially targeted towards\ntoxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the\nLLM draws the context to answer the generic prompt from the adversarial image.\nThe generation of benign-appearing adversarial images leverages a novel\nembedding-space-based methodology, operating with no access to the LLM model.\nInstead, the attacks require access only to the vision encoder and utilize one\nof our four embedding space targeting strategies. By not requiring access to\nthe LLM, the attacks lower the entry barrier for attackers, particularly when\nvision encoders such as CLIP are embedded in closed-source LLMs. The attacks\nachieve a high success rate across different VLMs, highlighting the risk of\ncross-modality alignment vulnerabilities, and the need for new alignment\napproaches for multi-modal models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shayegani_E/0/1/0/all/0/1\">Erfan Shayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1\">Nael Abu-Ghazaleh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools. (arXiv:2307.15770v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.15770","description":"<p>In the face of climate change, are companies really taking substantial steps\ntoward more sustainable operations? A comprehensive answer lies in the dense,\ninformation-rich landscape of corporate sustainability reports. However, the\nsheer volume and complexity of these reports make human analysis very costly.\nTherefore, only a few entities worldwide have the resources to analyze these\nreports at scale, which leads to a lack of transparency in sustainability\nreporting. Empowering stakeholders with LLM-based automatic analysis tools can\nbe a promising way to democratize sustainability report analysis. However,\ndeveloping such tools is challenging due to (1) the hallucination of LLMs and\n(2) the inefficiency of bringing domain experts into the AI development loop.\nIn this paper, we ChatReport, a novel LLM-based system to automate the analysis\nof corporate sustainability reports, addressing existing challenges by (1)\nmaking the answers traceable to reduce the harm of hallucination and (2)\nactively involving domain experts in the development loop. We make our\nmethodology, annotated datasets, and generated analyses of 1015 reports\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingwei Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingler_J/0/1/0/all/0/1\">Julia Bingler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colesanti_Senni_C/0/1/0/all/0/1\">Chiara Colesanti-Senni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Mathias Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gostlow_G/0/1/0/all/0/1\">Glen Gostlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schimanski_T/0/1/0/all/0/1\">Tobias Schimanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stammbach_D/0/1/0/all/0/1\">Dominik Stammbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaghefi_S/0/1/0/all/0/1\">Saeid Ashraf Vaghefi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webersinke_N/0/1/0/all/0/1\">Nicolas Webersinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wekhof_T/0/1/0/all/0/1\">Tobias Wekhof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tingyu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leippold_M/0/1/0/all/0/1\">Markus Leippold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4. (arXiv:2308.12067v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2308.12067","description":"<p>Multimodal large language models are typically trained in two stages: first\npre-training on image-text pairs, and then fine-tuning using supervised\nvision-language instruction data. Recent studies have shown that large language\nmodels can achieve satisfactory results even with a limited amount of\nhigh-quality instruction-following data. In this paper, we introduce\nInstructionGPT-4, which is fine-tuned on a small dataset comprising only 200\nexamples, amounting to approximately 6\\% of the instruction-following data used\nin the alignment dataset for MiniGPT-4. To achieve this, we first propose\nseveral metrics to access the quality of multimodal instruction data. Based on\nthese metrics, we present an effective and trainable data selector to\nautomatically identify and filter low-quality vision-language data. By\nemploying this method, InstructionGPT-4 outperforms the original MiniGPT-4 on\nvarious evaluations. Overall, our findings demonstrate that less but\nhigh-quality instruction tuning data is efficient in enabling multimodal large\nlanguage models to generate better output. Our code is available at\nhttps://github.com/waltonfuture/InstructionGPT-4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weiran Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision). (arXiv:2309.17421v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2309.17421","description":"<p>Large multimodal models (LMMs) extend large language models (LLMs) with\nmulti-sensory skills, such as visual understanding, to achieve stronger generic\nintelligence. In this paper, we analyze the latest model, GPT-4V(ision), to\ndeepen the understanding of LMMs. The analysis focuses on the intriguing tasks\nthat GPT-4V can perform, containing test samples to probe the quality and\ngenericity of GPT-4V's capabilities, its supported inputs and working modes,\nand the effective ways to prompt the model. In our approach to exploring\nGPT-4V, we curate and organize a collection of carefully designed qualitative\nsamples spanning a variety of domains and tasks. Observations from these\nsamples demonstrate that GPT-4V's unprecedented ability in processing\narbitrarily interleaved multimodal inputs and the genericity of its\ncapabilities together make GPT-4V a powerful multimodal generalist system.\nFurthermore, GPT-4V's unique capability of understanding visual markers drawn\non input images can give rise to new human-computer interaction methods such as\nvisual referring prompting. We conclude the report with in-depth discussions on\nthe emerging application scenarios and the future research directions for\nGPT-4V-based systems. We hope that this preliminary exploration will inspire\nfuture research on the next-generation multimodal task formulation, new ways to\nexploit and enhance LMMs to solve real-world problems, and gaining better\nunderstanding of multimodal foundation models. Finally, we acknowledge that the\nmodel under our study is solely the product of OpenAI's innovative work, and\nthey should be fully credited for its development. Please see the GPT-4V\ncontributions paper for the authorship and credit attribution:\nhttps://cdn.openai.com/contributions/gpt-4v.pdf\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chung-Ching Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications. (arXiv:2310.04381v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2310.04381","description":"<p>In this paper, we present Hermes, an end-to-end framework to automatically\ngenerate formal representations from natural language cellular specifications.\nWe first develop a neural constituency parser, NEUTREX, to process\ntransition-relevant texts and extract transition components (i.e., states,\nconditions, and actions). We also design a domain-specific language to\ntranslate these transition components to logical formulas by leveraging\ndependency parse trees. Finally, we compile these logical formulas to generate\ntransitions and create the formal model as finite state machines. To\ndemonstrate the effectiveness of Hermes, we evaluate it on 4G NAS, 5G NAS, and\n5G RRC specifications and obtain an overall accuracy of 81-87%, which is a\nsubstantial improvement over the state-of-the-art. Our security analysis of the\nextracted models uncovers 3 new vulnerabilities and identifies 19 previous\nattacks in 4G and 5G specifications, and 7 deviations in commercial 4G\nbasebands.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ishtiaq_A/0/1/0/all/0/1\">Abdullah Al Ishtiaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sarkar Snigdha Sarathi Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_S/0/1/0/all/0/1\">Syed Md Mukit Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjbar_A/0/1/0/all/0/1\">Ali Ranjbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kai Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhezheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akon_M/0/1/0/all/0/1\">Mujtahid Akon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_S/0/1/0/all/0/1\">Syed Rafiul Hussain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Large Language Models as Zero-shot Relation Extractors. (arXiv:2310.05028v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2310.05028","description":"<p>Relation extraction (RE) consistently involves a certain degree of labeled or\nunlabeled data even if under zero-shot setting. Recent studies have shown that\nlarge language models (LLMs) transfer well to new tasks out-of-the-box simply\ngiven a natural language prompt, which provides the possibility of extracting\nrelations from text without any data and parameter tuning. This work focuses on\nthe study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.\nOn the one hand, we analyze the drawbacks of existing RE prompts and attempt to\nincorporate recent prompt techniques such as chain-of-thought (CoT) to improve\nzero-shot RE. We propose the summarize-and-ask (\\textsc{SumAsk}) prompting, a\nsimple prompt recursively using LLMs to transform RE inputs to the effective\nquestion answering (QA) format. On the other hand, we conduct comprehensive\nexperiments on various benchmarks and settings to investigate the capabilities\nof LLMs on zero-shot RE. Specifically, we have the following findings: (i)\n\\textsc{SumAsk} consistently and significantly improves LLMs performance on\ndifferent model sizes, benchmarks and settings; (ii) Zero-shot prompting with\nChatGPT achieves competitive or superior results compared with zero-shot and\nfully supervised methods; (iii) LLMs deliver promising performance in\nextracting overlapping relations; (iv) The performance varies greatly regarding\ndifferent relations. Different from small language models, LLMs are effective\nin handling challenge none-of-the-above (NoTA) relation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guozheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_W/0/1/0/all/0/1\">Wenjun Ke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BRAINTEASER: Lateral Thinking Puzzles for Large Language Models. (arXiv:2310.05057v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05057","description":"<p>The success of language models has inspired the NLP community to attend to\ntasks that require implicit and complex reasoning, relying on human-like\ncommonsense mechanisms. While such vertical thinking tasks have been relatively\npopular, lateral thinking puzzles have received little attention. To bridge\nthis gap, we devise BRAINTEASER: a multiple-choice Question Answering task\ndesigned to test the model's ability to exhibit lateral thinking and defy\ndefault commonsense associations. We design a three-step procedure for creating\nthe first lateral thinking benchmark, consisting of data collection, distractor\ngeneration, and generation of adversarial examples, leading to 1,100 puzzles\nwith high-quality annotations. To assess the consistency of lateral reasoning\nby models, we enrich BRAINTEASER based on a semantic and contextual\nreconstruction of its questions. Our experiments with state-of-the-art\ninstruction- and commonsense language models reveal a significant gap between\nhuman and model performance, which is further widened when consistency across\nadversarial formats is considered. We make all of our code and data available\nto stimulate work on developing and evaluating lateral thinking models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sourati_Z/0/1/0/all/0/1\">Zhivar Sourati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05280","description":"<p>Recent advancements in Large Language Models empower them to follow freeform\ninstructions, including imitating generic or specific demographic personas in\nconversations. Generic personas refer to an individual from a demographic group\n(e.g. an Asian person), whereas specific personas can be actual names of\nhistorical figures. While the adoption of personas allows dialogue systems to\nbe more engaging and approachable to users, it also carries the potential risk\nof exacerbating social biases in model responses, further causing societal\nharms through interactions with users. In this paper, we systematically study\n\"persona biases\", which we define to be the sensitivity of harmful dialogue\nmodel behaviors to different persona adoptions. We categorize persona biases\ninto biases in harmful expression and harmful agreement, as well as establish a\ncomprehensive evaluation framework to measure persona biases in five aspects:\nOffensiveness, Toxic Continuation, Regard, Stereotype Agreement, and Toxic\nAgreement. Additionally, we propose to comprehensively investigate persona\nbiases through experimenting with UniversalPersona, a systematized persona\ndataset with a comprehensive list of both generic and specific model personas.\nThrough benchmarking on four different models, including Blender, ChatGPT,\nAlpaca, and Vicuna, our study uncovers significant persona biases in these\ndialogue systems.Findings of our study underscores the immediate need to\nrevisit the use of persona traits in dialogue agents, to ensure their safe\napplication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yixin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jieyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Large Language Models Post Hoc Explainers?. (arXiv:2310.05797v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05797","description":"<p>Large Language Models (LLMs) are increasingly used as powerful tools for a\nplethora of natural language processing (NLP) applications. A recent\ninnovation, in-context learning (ICL), enables LLMs to learn new tasks by\nsupplying a few examples in the prompt during inference time, thereby\neliminating the need for model fine-tuning. While LLMs have been utilized in\nseveral applications, their applicability in explaining the behavior of other\nmodels remains relatively unexplored. Despite the growing number of new\nexplanation techniques, many require white-box access to the model and/or are\ncomputationally expensive, highlighting a need for next-generation post hoc\nexplainers. In this work, we present the first framework to study the\neffectiveness of LLMs in explaining other predictive models. More specifically,\nwe propose a novel framework encompassing multiple prompting strategies: i)\nPerturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL,\nand iv) Explanation-based ICL, with varying levels of information about the\nunderlying ML model and the local neighborhood of the test sample. We conduct\nextensive experiments with real-world benchmark datasets to demonstrate that\nLLM-generated explanations perform on par with state-of-the-art post hoc\nexplainers using their ability to leverage ICL examples and their internal\nknowledge in generating model explanations. On average, across four datasets\nand two ML models, we observe that LLMs identify the most important feature\nwith 72.19% accuracy, opening up new frontiers in explainable artificial\nintelligence (XAI) to explore LLM-based explanation frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kroeger_N/0/1/0/all/0/1\">Nicholas Kroeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ley_D/0/1/0/all/0/1\">Dan Ley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_S/0/1/0/all/0/1\">Satyapriya Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Importance of Prompt Tuning for Automated Neuron Explanations. (arXiv:2310.06200v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.06200","description":"<p>Recent advances have greatly increased the capabilities of large language\nmodels (LLMs), but our understanding of the models and their safety has not\nprogressed as fast. In this paper we aim to understand LLMs deeper by studying\ntheir individual neurons. We build upon previous work showing large language\nmodels such as GPT-4 can be useful in explaining what each neuron in a language\nmodel does. Specifically, we analyze the effect of the prompt used to generate\nexplanations and show that reformatting the explanation prompt in a more\nnatural way can significantly improve neuron explanation quality and greatly\nreduce computational cost. We demonstrate the effects of our new prompts in\nthree different ways, incorporating both automated and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Justin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikarinen_T/0/1/0/all/0/1\">Tuomas Oikarinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatha_A/0/1/0/all/0/1\">Arjun Chatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Keng-Chi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yilan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_T/0/1/0/all/0/1\">Tsui-Wei Weng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructive Large Language Models Alignment with Diverse Feedback. (arXiv:2310.06450v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.06450","description":"<p>In recent research on large language models (LLMs), there has been a growing\nemphasis on aligning these models with human values to reduce the impact of\nharmful content. However, current alignment methods often rely solely on\nsingular forms of human feedback, such as preferences, annotated labels, or\nnatural language critiques, overlooking the potential advantages of combining\nthese feedback types. This limitation leads to suboptimal performance, even\nwhen ample training data is available. In this paper, we introduce Constructive\nand Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired\nby constructivist learning theory. Our approach involves collecting three\ndistinct types of feedback tailored to problems of varying difficulty levels\nwithin the training dataset. Specifically, we exploit critique feedback for\neasy problems, refinement feedback for medium problems, and preference feedback\nfor hard problems. By training our model with this diversified feedback, we\nachieve enhanced alignment performance while using less training data. To\nassess the effectiveness of CDF, we evaluate it against previous methods in\nthree downstream tasks: question answering, dialog generation, and text\nsummarization. Experimental results demonstrate that CDF achieves superior\nperformance even with a smaller training dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianshu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models. (arXiv:2310.06692v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.06692","description":"<p>Large language models (LLMs) have unveiled remarkable reasoning capabilities\nby exploiting chain-of-thought (CoT) prompting, which generates intermediate\nreasoning chains to serve as the rationale for deriving the answer. However,\ncurrent CoT methods either simply employ general prompts such as Let's think\nstep by step, or heavily rely on handcrafted task-specific demonstrations to\nattain preferable performances, thereby engendering an inescapable gap between\nperformance and generalization. To bridge this gap, we propose Meta-CoT, a\ngeneralizable CoT prompting method in mixed-task scenarios where the type of\ninput questions is unknown. Meta-CoT firstly categorizes the scenario based on\nthe input question and subsequently constructs diverse demonstrations from the\ncorresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys\nremarkable performances on ten public benchmark reasoning tasks and superior\ngeneralization capabilities. Notably, Meta-CoT achieves the state-of-the-art\nresult on SVAMP (93.7%) without any additional program-aided methods. Our\nfurther experiments on five out-of-distribution datasets verify the stability\nand generality of Meta-CoT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Anni Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-10-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}