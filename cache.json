{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-18T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Language Instructed Reinforcement Learning for Human-AI Coordination. (arXiv:2304.07297v1 [cs.AI])","link":"http://arxiv.org/abs/2304.07297","description":"<p>One of the fundamental quests of AI is to produce agents that coordinate well\nwith humans. This problem is challenging, especially in domains that lack high\nquality human behavioral data, because multi-agent reinforcement learning (RL)\noften converges to different equilibria from the ones that humans prefer. We\npropose a novel framework, instructRL, that enables humans to specify what kind\nof strategies they expect from their AI partners through natural language\ninstructions. We use pretrained large language models to generate a prior\npolicy conditioned on the human instruction and use the prior to regularize the\nRL objective. This leads to the RL agent converging to equilibria that are\naligned with human preferences. We show that instructRL converges to human-like\npolicies that satisfy the given instructions in a proof-of-concept environment\nas well as the challenging Hanabi benchmark. Finally, we show that knowing the\nlanguage instruction significantly boosts human-AI coordination performance in\nhuman evaluations in Hanabi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hengyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1\">Dorsa Sadigh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenAssistant Conversations -- Democratizing Large Language Model Alignment. (arXiv:2304.07327v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07327","description":"<p>Aligning large language models (LLMs) with human preferences has proven to\ndrastically improve usability and has driven rapid adoption as demonstrated by\nChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and\nreinforcement learning from human feedback (RLHF) greatly reduce the required\nskill and domain knowledge to effectively harness the capabilities of LLMs,\nincreasing their accessibility and utility across various domains. However,\nstate-of-the-art alignment techniques like RLHF rely on high-quality human\nfeedback data, which is expensive to create and often remains proprietary. In\nan effort to democratize research on large-scale alignment, we release\nOpenAssistant Conversations, a human-generated, human-annotated assistant-style\nconversation corpus consisting of 161,443 messages distributed across 66,497\nconversation trees, in 35 different languages, annotated with 461,292 quality\nratings. The corpus is a product of a worldwide crowd-sourcing effort involving\nover 13,500 volunteers. To demonstrate the OpenAssistant Conversations\ndataset's effectiveness, we present OpenAssistant, the first fully open-source\nlarge-scale instruction-tuned model to be trained on human data. A preference\nstudy revealed that OpenAssistant replies are comparably preferred to\nGPT-3.5-turbo (ChatGPT) with a relative winrate of 48.3% vs. 51.7%\nrespectively. We release our code and data under fully permissive licenses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kopf_A/0/1/0/all/0/1\">Andreas K&#xf6;pf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilcher_Y/0/1/0/all/0/1\">Yannic Kilcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rutte_D/0/1/0/all/0/1\">Dimitri von R&#xfc;tte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anagnostidis_S/0/1/0/all/0/1\">Sotiris Anagnostidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tam_Z/0/1/0/all/0/1\">Zhi-Rui Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_K/0/1/0/all/0/1\">Keith Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barhoum_A/0/1/0/all/0/1\">Abdullah Barhoum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duc_N/0/1/0/all/0/1\">Nguyen Minh Duc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanley_O/0/1/0/all/0/1\">Oliver Stanley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagyfi_R/0/1/0/all/0/1\">Rich&#xe1;rd Nagyfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ES_S/0/1/0/all/0/1\">Shahul ES</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suri_S/0/1/0/all/0/1\">Sameer Suri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glushkov_D/0/1/0/all/0/1\">David Glushkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dantuluri_A/0/1/0/all/0/1\">Arnav Dantuluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguire_A/0/1/0/all/0/1\">Andrew Maguire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuhmann_C/0/1/0/all/0/1\">Christoph Schuhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattick_A/0/1/0/all/0/1\">Alexander Mattick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Self-Perception and Political Biases of ChatGPT. (arXiv:2304.07333v1 [cs.CY])","link":"http://arxiv.org/abs/2304.07333","description":"<p>This contribution analyzes the self-perception and political biases of\nOpenAI's Large Language Model ChatGPT. Taking into account the first\nsmall-scale reports and studies that have emerged, claiming that ChatGPT is\npolitically biased towards progressive and libertarian points of view, this\ncontribution aims to provide further clarity on this subject. For this purpose,\nChatGPT was asked to answer the questions posed by the political compass test\nas well as similar questionnaires that are specific to the respective politics\nof the G7 member states. These eight tests were repeated ten times each and\nrevealed that ChatGPT seems to hold a bias towards progressive views. The\npolitical compass test revealed a bias towards progressive and libertarian\nviews, with the average coordinates on the political compass being (-6.48,\n-5.99) (with (0, 0) the center of the compass, i.e., centrism and the axes\nranging from -10 to 10), supporting the claims of prior research. The political\nquestionnaires for the G7 member states indicated a bias towards progressive\nviews but no significant bias between authoritarian and libertarian views,\ncontradicting the findings of prior reports, with the average coordinates being\n(-3.27, 0.58). In addition, ChatGPT's Big Five personality traits were tested\nusing the OCEAN test and its personality type was queried using the\nMyers-Briggs Type Indicator (MBTI) test. Finally, the maliciousness of ChatGPT\nwas evaluated using the Dark Factor test. These three tests were also repeated\nten times each, revealing that ChatGPT perceives itself as highly open and\nagreeable, has the Myers-Briggs personality type ENFJ, and is among the 15% of\ntest-takers with the least pronounced dark traits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rutinowski_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rutinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franke_S/0/1/0/all/0/1\">Sven Franke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Endendyk_J/0/1/0/all/0/1\">Jan Endendyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dormuth_I/0/1/0/all/0/1\">Ina Dormuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauly_M/0/1/0/all/0/1\">Markus Pauly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Multi-Label Topic Inference with Sentence Encoders. (arXiv:2304.07382v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07382","description":"<p>Sentence encoders have indeed been shown to achieve superior performances for\nmany downstream text-mining tasks and, thus, claimed to be fairly general.\nInspired by this, we performed a detailed study on how to leverage these\nsentence encoders for the \"zero-shot topic inference\" task, where the topics\nare defined/provided by the users in real-time. Extensive experiments on seven\ndifferent datasets demonstrate that Sentence-BERT demonstrates superior\ngenerality compared to other encoders, while Universal Sentence Encoder can be\npreferred when efficiency is a top priority.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Souvika Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dongji Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santu_S/0/1/0/all/0/1\">Shubhra Kanti Karmaker Santu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models. (arXiv:2304.07396v1 [cs.LG])","link":"http://arxiv.org/abs/2304.07396","description":"<p>Physicians considering clinical trials for their patients are met with the\nlaborious process of checking many text based eligibility criteria. Large\nLanguage Models (LLMs) have shown to perform well for clinical information\nextraction and clinical reasoning, including medical tests, but not yet in\nreal-world scenarios. This paper investigates the use of InstructGPT to assist\nphysicians in determining eligibility for clinical trials based on a patient's\nsummarised medical profile. Using a prompting strategy combining one-shot,\nselection-inference and chain-of-thought techniques, we investigate the\nperformance of LLMs on 10 synthetically created patient profiles. Performance\nis evaluated at four levels: ability to identify screenable eligibility\ncriteria from a trial given a medical profile; ability to classify for each\nindividual criterion whether the patient qualifies; the overall classification\nwhether a patient is eligible for a clinical trial and the percentage of\ncriteria to be screened by physician. We evaluated against 146 clinical trials\nand a total of 4,135 eligibility criteria. The LLM was able to correctly\nidentify the screenability of 72% (2,994/4,135) of the criteria. Additionally,\n72% (341/471) of the screenable criteria were evaluated correctly. The\nresulting trial level classification as eligible or ineligible resulted in a\nrecall of 0.5. By leveraging LLMs with a physician-in-the-loop, a recall of 1.0\nand precision of 0.71 on clinical trial level can be achieved while reducing\nthe amount of criteria to be checked by an estimated 90%. LLMs can be used to\nassist physicians with pre-screening of patients for clinical trials. By\nforcing instruction-tuned LLMs to produce chain-of-thought responses, the\nreasoning can be made transparent to and the decision process becomes amenable\nby physicians, thereby making such a system feasible for use in real-world\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamer_D/0/1/0/all/0/1\">Danny M. den Hamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoor_P/0/1/0/all/0/1\">Perry Schoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polak_T/0/1/0/all/0/1\">Tobias B. Polak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapitan_D/0/1/0/all/0/1\">Daniel Kapitan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Question Summarization with Entity-driven Contrastive Learning. (arXiv:2304.07437v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07437","description":"<p>By summarizing longer consumer health questions into shorter and essential\nones, medical question answering (MQA) systems can more accurately understand\nconsumer intentions and retrieve suitable answers. However, medical question\nsummarization is very challenging due to obvious distinctions in health trouble\ndescriptions from patients and doctors. Although existing works have attempted\nto utilize Seq2Seq, reinforcement learning, or contrastive learning to solve\nthe problem, two challenges remain: how to correctly capture question focus to\nmodel its semantic intention, and how to obtain reliable datasets to fairly\nevaluate performance. To address these challenges, this paper proposes a novel\nmedical question summarization framework using entity-driven contrastive\nlearning (ECL). ECL employs medical entities in frequently asked questions\n(FAQs) as focuses and devises an effective mechanism to generate hard negative\nsamples. This approach forces models to pay attention to the crucial focus\ninformation and generate more ideal question summarization. Additionally, we\nfind that some MQA datasets suffer from serious data leakage problems, such as\nthe iCliniq dataset's 33% duplicate rate. To evaluate the related methods\nfairly, this paper carefully checks leaked samples to reorganize more\nreasonable datasets. Extensive experiments demonstrate that our ECL method\noutperforms state-of-the-art methods by accurately capturing question focus and\ngenerating medical question summaries. The code and datasets are available at\nhttps://github.com/yrbobo/MQS-ECL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Sibo Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wenpeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xueping Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shoujin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi-Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07438","description":"<p>Despite the success of autoregressive large language models in text\ngeneration, it remains a major challenge to generate text that satisfies\ncomplex constraints: sampling from the conditional distribution\n$\\Pr(\\text{text} | \\alpha)$ is intractable for even the simplest lexical\nconstraints $\\alpha$. To overcome this challenge, we propose to use tractable\nprobabilistic models to impose lexical constraints in autoregressive text\ngeneration, which we refer to as GeLaTo. To demonstrate the effectiveness of\nthis framework, we use distilled hidden Markov models to control autoregressive\ngeneration from GPT2. GeLaTo achieves state-of-the-art performance on\nCommonGen, a challenging benchmark for constrained text generation, beating a\nwide range of strong baselines by a large margin. Our work not only opens up\nnew avenues for controlling large language models but also motivates the\ndevelopment of more expressive tractable probabilistic models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_M/0/1/0/all/0/1\">Meihua Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1\">Guy Van den Broeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Educational Dialogue Act Classifiers with Low-Resource and Imbalanced Datasets. (arXiv:2304.07499v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07499","description":"<p>Dialogue acts (DAs) can represent conversational actions of tutors or\nstudents that take place during tutoring dialogues. Automating the\nidentification of DAs in tutoring dialogues is significant to the design of\ndialogue-based intelligent tutoring systems. Many prior studies employ machine\nlearning models to classify DAs in tutoring dialogues and invest much effort to\noptimize the classification accuracy by using limited amounts of training data\n(i.e., low-resource data scenario). However, beyond the classification\naccuracy, the robustness of the classifier is also important, which can reflect\nthe capability of the classifier on learning the patterns from different class\ndistributions. We note that many prior studies on classifying educational DAs\nemploy cross entropy (CE) loss to optimize DA classifiers on low-resource data\nwith imbalanced DA distribution. The DA classifiers in these studies tend to\nprioritize accuracy on the majority class at the expense of the minority class\nwhich might not be robust to the data with imbalanced ratios of different DA\nclasses. To optimize the robustness of classifiers on imbalanced class\ndistributions, we propose to optimize the performance of the DA classifier by\nmaximizing the area under the ROC curve (AUC) score (i.e., AUC maximization).\nThrough extensive experiments, our study provides evidence that (i) by\nmaximizing AUC in the training process, the DA classifier achieves significant\nperformance improvement compared to the CE approach under low-resource data,\nand (ii) AUC maximization approaches can improve the robustness of the DA\nclassifier under different class imbalance ratios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jionghao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_D/0/1/0/all/0/1\">David Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beare_R/0/1/0/all/0/1\">Richard Beare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasevic_D/0/1/0/all/0/1\">Dragan Gasevic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A CTC Alignment-based Non-autoregressive Transformer for End-to-end Automatic Speech Recognition. (arXiv:2304.07611v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07611","description":"<p>Recently, end-to-end models have been widely used in automatic speech\nrecognition (ASR) systems. Two of the most representative approaches are\nconnectionist temporal classification (CTC) and attention-based encoder-decoder\n(AED) models. Autoregressive transformers, variants of AED, adopt an\nautoregressive mechanism for token generation and thus are relatively slow\nduring inference. In this paper, we present a comprehensive study of a CTC\nAlignment-based Single-Step Non-Autoregressive Transformer (CASS-NAT) for\nend-to-end ASR. In CASS-NAT, word embeddings in the autoregressive transformer\n(AT) are substituted with token-level acoustic embeddings (TAE) that are\nextracted from encoder outputs with the acoustical boundary information offered\nby the CTC alignment. TAE can be obtained in parallel, resulting in a parallel\ngeneration of output tokens. During training, Viterbi-alignment is used for TAE\ngeneration, and multiple training strategies are further explored to improve\nthe word error rate (WER) performance. During inference, an error-based\nalignment sampling method is investigated in depth to reduce the alignment\nmismatch in the training and testing processes. Experimental results show that\nthe CASS-NAT has a WER that is close to AT on various ASR tasks, while\nproviding a ~24x inference speedup. With and without self-supervised learning,\nwe achieve new state-of-the-art results for non-autoregressive models on\nseveral datasets. We also analyze the behavior of the CASS-NAT decoder to\nexplain why it can perform similarly to AT. We find that TAEs have similar\nfunctionality to word embeddings for grammatical structures, which might\nindicate the possibility of learning some semantic information from TAEs\nwithout a language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Ruchao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_P/0/1/0/all/0/1\">Peng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alwan_A/0/1/0/all/0/1\">Abeer Alwan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models. (arXiv:2304.07619v1 [q-fin.ST])","link":"http://arxiv.org/abs/2304.07619","description":"<p>We examine the potential of ChatGPT, and other large language models, in\npredicting stock market returns using sentiment analysis of news headlines. We\nuse ChatGPT to indicate whether a given headline is good, bad, or irrelevant\nnews for firms' stock prices. We then compute a numerical score and document a\npositive correlation between these ``ChatGPT scores'' and subsequent daily\nstock market returns. Further, ChatGPT outperforms traditional sentiment\nanalysis methods. We find that more basic models such as GPT-1, GPT-2, and BERT\ncannot accurately forecast returns, indicating return predictability is an\nemerging capacity of complex models. Our results suggest that incorporating\nadvanced language models into the investment decision-making process can yield\nmore accurate predictions and enhance the performance of quantitative trading\nstrategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Lopez_Lira_A/0/1/0/all/0/1\">Alejandro Lopez-Lira</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Tang_Y/0/1/0/all/0/1\">Yuehua Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Approaches to Entity-Centric Information Extraction. (arXiv:2304.07625v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07625","description":"<p>Artificial Intelligence (AI) has huge impact on our daily lives with\napplications such as voice assistants, facial recognition, chatbots,\nautonomously driving cars, etc. Natural Language Processing (NLP) is a\ncross-discipline of AI and Linguistics, dedicated to study the understanding of\nthe text. This is a very challenging area due to unstructured nature of the\nlanguage, with many ambiguous and corner cases. In this thesis we address a\nvery specific area of NLP that involves the understanding of entities (e.g.,\nnames of people, organizations, locations) in text. First, we introduce a\nradically different, entity-centric view of the information in text. We argue\nthat instead of using individual mentions in text to understand their meaning,\nwe should build applications that would work in terms of entity concepts. Next,\nwe present a more detailed model on how the entity-centric approach can be used\nfor the entity linking task. In our work, we show that this task can be\nimproved by considering performing entity linking at the coreference cluster\nlevel rather than each of the mentions individually. In our next work, we\nfurther study how information from Knowledge Base entities can be integrated\ninto text. Finally, we analyze the evolution of the entities from the evolving\ntemporal perspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaporojets_K/0/1/0/all/0/1\">Klim Zaporojets</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Out-of-Context Multimodal Misinformation with interpretable neural-symbolic model. (arXiv:2304.07633v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07633","description":"<p>Recent years have witnessed the sustained evolution of misinformation that\naims at manipulating public opinions. Unlike traditional rumors or fake news\neditors who mainly rely on generated and/or counterfeited images, text and\nvideos, current misinformation creators now more tend to use out-of-context\nmultimedia contents (e.g. mismatched images and captions) to deceive the public\nand fake news detection systems. This new type of misinformation increases the\ndifficulty of not only detection but also clarification, because every\nindividual modality is close enough to true information. To address this\nchallenge, in this paper we explore how to achieve interpretable cross-modal\nde-contextualization detection that simultaneously identifies the mismatched\npairs and the cross-modal contradictions, which is helpful for fact-check\nwebsites to document clarifications. The proposed model first symbolically\ndisassembles the text-modality information to a set of fact queries based on\nthe Abstract Meaning Representation of the caption and then forwards the\nquery-image pairs into a pre-trained large vision-language model select the\n``evidences\" that are helpful for us to detect misinformation. Extensive\nexperiments indicate that the proposed methodology can provide us with much\nmore interpretable predictions while maintaining the accuracy same as the\nstate-of-the-art model on this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trinh_L/0/1/0/all/0/1\">Loc Trinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1\">Defu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zijun Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransDocs: Optical Character Recognition with word to word translation. (arXiv:2304.07637v1 [cs.CV])","link":"http://arxiv.org/abs/2304.07637","description":"<p>While OCR has been used in various applications, its output is not always\naccurate, leading to misfit words. This research work focuses on improving the\noptical character recognition (OCR) with ML techniques with integration of OCR\nwith long short-term memory (LSTM) based sequence to sequence deep learning\nmodels to perform document translation. This work is based on ANKI dataset for\nEnglish to Spanish translation. In this work, I have shown comparative study\nfor pre-trained OCR while using deep learning model using LSTM-based seq2seq\narchitecture with attention for machine translation. End-to-end performance of\nthe model has been expressed in BLEU-4 score. This research paper is aimed at\nresearchers and practitioners interested in OCR and its applications in\ndocument translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bamotra_A/0/1/0/all/0/1\">Abhishek Bamotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uppala_P/0/1/0/all/0/1\">Phani Krishna Uppala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models. (arXiv:2304.07666v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07666","description":"<p>AI generated content (AIGC) presents considerable challenge to educators\naround the world. Instructors need to be able to detect such text generated by\nlarge language models, either with the naked eye or with the help of some\ntools. There is also growing need to understand the lexical, syntactic and\nstylistic features of AIGC. To address these challenges in English language\nteaching, we first present ArguGPT, a balanced corpus of 4,038 argumentative\nessays generated by 7 GPT models in response to essay prompts from three\nsources: (1) in-class or homework exercises, (2) TOEFL and (3) GRE writing\ntasks. Machine-generated texts are paired with roughly equal number of\nhuman-written essays with three score levels matched in essay prompts. We then\nhire English instructors to distinguish machine essays from human ones. Results\nshow that when first exposed to machine-generated essays, the instructors only\nhave an accuracy of 61% in detecting them. But the number rises to 67% after\none round of minimal self-training. Next, we perform linguistic analyses of\nthese essays, which show that machines produce sentences with more complex\nsyntactic structures while human essays tend to be lexically more complex.\nFinally, we test existing AIGC detectors and build our own detectors using SVMs\nand RoBERTa. Results suggest that a RoBERTa fine-tuned with the training set of\nArguGPT achieves above 90% accuracy in both essay- and sentence-level\nclassification. To the best of our knowledge, this is the first comprehensive\nanalysis of argumentative essays produced by generative large language models.\nMachine-authored essays in ArguGPT and our models will be made publicly\navailable at https://github.com/huhailinguist/ArguGPT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wanyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Shisen Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaojing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xinyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hai Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLRegTest: A Benchmark for the Machine Learning of Regular Languages. (arXiv:2304.07687v1 [cs.LG])","link":"http://arxiv.org/abs/2304.07687","description":"<p>Evaluating machine learning (ML) systems on their ability to learn known\nclassifiers allows fine-grained examination of the patterns they can learn,\nwhich builds confidence when they are applied to the learning of unknown\nclassifiers. This article presents a new benchmark for ML systems on sequence\nclassification called MLRegTest, which contains training, development, and test\nsets from 1,800 regular languages.\n</p>\n<p>Different kinds of formal languages represent different kinds of\nlong-distance dependencies, and correctly identifying long-distance\ndependencies in sequences is a known challenge for ML systems to generalize\nsuccessfully. MLRegTest organizes its languages according to their logical\ncomplexity (monadic second order, first order, propositional, or monomial\nexpressions) and the kind of logical literals (string, tier-string,\nsubsequence, or combinations thereof). The logical complexity and choice of\nliteral provides a systematic way to understand different kinds of\nlong-distance dependencies in regular languages, and therefore to understand\nthe capacities of different ML systems to learn such long-distance\ndependencies.\n</p>\n<p>Finally, the performance of different neural networks (simple RNN, LSTM, GRU,\ntransformer) on MLRegTest is examined. The main conclusion is that their\nperformance depends significantly on the kind of test set, the class of\nlanguage, and the neural network architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poel_S/0/1/0/all/0/1\">Sam van der Poel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambert_D/0/1/0/all/0/1\">Dakotah Lambert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostyszyn_K/0/1/0/all/0/1\">Kalina Kostyszyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tiantian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1\">Rahul Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersen_D/0/1/0/all/0/1\">Derek Andersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_J/0/1/0/all/0/1\">Joanne Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peterson_E/0/1/0/all/0/1\">Emily Peterson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clair_C/0/1/0/all/0/1\">Cody St. Clair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fodor_P/0/1/0/all/0/1\">Paul Fodor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibata_C/0/1/0/all/0/1\">Chihiro Shibata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinz_J/0/1/0/all/0/1\">Jeffrey Heinz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"USNID: A Framework for Unsupervised and Semi-supervised New Intent Discovery. (arXiv:2304.07699v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07699","description":"<p>New intent discovery is of great value to natural language processing,\nallowing for a better understanding of user needs and providing friendly\nservices. However, most existing methods struggle to capture the complicated\nsemantics of discrete text representations when limited or no prior knowledge\nof labeled data is available. To tackle this problem, we propose a novel\nframework called USNID for unsupervised and semi-supervised new intent\ndiscovery, which has three key technologies. First, it takes full use of\nunsupervised or semi-supervised data to mine shallow semantic similarity\nrelations and provide well-initialized representations for clustering. Second,\nit designs a centroid-guided clustering mechanism to address the issue of\ncluster allocation inconsistency and provide high-quality self-supervised\ntargets for representation learning. Third, it captures high-level semantics in\nunsupervised or semi-supervised data to discover fine-grained intent-wise\nclusters by optimizing both cluster-level and instance-level objectives. We\nalso propose an effective method for estimating the cluster number in\nopen-world scenarios without knowing the number of new intents beforehand.\nUSNID performs exceptionally well on several intent benchmark datasets,\nachieving new state-of-the-art results in unsupervised and semi-supervised new\nintent discovery and demonstrating robust performance with different cluster\nnumbers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_F/0/1/0/all/0/1\">Fei Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kai Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Canvas: End-to-End Kernel Architecture Search in Neural Networks. (arXiv:2304.07741v1 [cs.LG])","link":"http://arxiv.org/abs/2304.07741","description":"<p>The demands for higher performance and accuracy in neural networks (NNs)\nnever end. Existing tensor compilation and Neural Architecture Search (NAS)\ntechniques orthogonally optimize the two goals but actually share many\nsimilarities in their concrete strategies. We exploit such opportunities by\ncombining the two into one and make a case for Kernel Architecture Search\n(KAS). KAS reviews NAS from a system perspective and zooms into a more\nfine-grained level to generate neural kernels with both high performance and\ngood accuracy. To demonstrate the potential of KAS, we build an end-to-end\nframework, Canvas, to find high-quality kernels as convolution replacements.\nCanvas samples from a rich set of fine-grained primitives to stochastically and\niteratively construct new kernels and evaluate them according to user-specified\nconstraints. Canvas supports freely adjustable tensor dimension sizes inside\nthe kernel and uses two levels of solvers to satisfy structural legality and\nfully utilize model budgets. The evaluation shows that by replacing standard\nconvolutions with generated new kernels in common NNs, Canvas achieves average\n1.5x speedups compared to the previous state-of-the-art with acceptable\naccuracy loss and search efficiency. Canvas verifies the practicability of KAS\nby rediscovering many manually designed kernels in the past and producing new\nstructures that may inspire future machine learning innovations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenggang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Genghan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingyu Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MisRoB{\\AE}RTa: Transformers versus Misinformation. (arXiv:2304.07759v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07759","description":"<p>Misinformation is considered a threat to our democratic values and\nprinciples. The spread of such content on social media polarizes society and\nundermines public discourse by distorting public perceptions and generating\nsocial unrest while lacking the rigor of traditional journalism. Transformers\nand transfer learning proved to be state-of-the-art methods for multiple\nwell-known natural language processing tasks. In this paper, we propose\nMisRoB{\\AE}RTa, a novel transformer-based deep neural ensemble architecture for\nmisinformation detection. MisRoB{\\AE}RTa takes advantage of two transformers\n(BART \\&amp; RoBERTa) to improve the classification performance. We also\nbenchmarked and evaluated the performances of multiple transformers on the task\nof misinformation detection. For training and testing, we used a large\nreal-world news articles dataset labeled with 10 classes, addressing two\nshortcomings in the current research: increasing the size of the dataset from\nsmall to large, and moving the focus of fake news detection from binary\nclassification to multi-class classification. For this dataset, we manually\nverified the content of the news articles to ensure that they were correctly\nlabeled. The experimental results show that the accuracy of transformers on the\nmisinformation detection problem was significantly influenced by the method\nemployed to learn the context, dataset size, and vocabulary dimension. We\nobserve empirically that the best accuracy performance among the classification\nmodels that use only one transformer is obtained by BART, while DistilRoBERTa\nobtains the best accuracy in the least amount of time required for fine-tuning\nand training. The proposed MisRoB{\\AE}RTa outperforms the other transformer\nmodels in the task of misinformation detection. To arrive at this conclusion,\nwe performed ample ablation and sensitivity testing with MisRoB{\\AE}RTa on two\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truica_C/0/1/0/all/0/1\">Ciprian-Octavian Truic&#x103;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostol_E/0/1/0/all/0/1\">Elena-Simona Apostol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation. (arXiv:2304.07772v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07772","description":"<p>In recent years, the field of neural machine translation (NMT) for SPARQL\nquery generation has witnessed a significant growth. Recently, the\nincorporation of the copy mechanism with traditional encoder-decoder\narchitectures and the use of pre-trained encoder-decoders have set new\nperformance benchmarks. This paper presents a large variety of experiments that\nreplicate and expand upon recent NMT-based SPARQL generation studies, comparing\npre-trained and non-pre-trained models, question annotation formats, and the\nuse of a copy mechanism for non-pre-trained and pre-trained models. Our results\nshow that either adding the copy mechanism or using a question annotation\nimproves performances for nonpre-trained models and for pre-trained models,\nsetting new baselines for three popular datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reyd_S/0/1/0/all/0/1\">Samuel Reyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zouaq_A/0/1/0/all/0/1\">Amal Zouaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diallo_P/0/1/0/all/0/1\">Papa Abdou Karim Karou Diallo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntactic Complexity Identification, Measurement, and Reduction Through Controlled Syntactic Simplification. (arXiv:2304.07774v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07774","description":"<p>Text simplification is one of the domains in Natural Language Processing\n(NLP) that offers an opportunity to understand the text in a simplified manner\nfor exploration. However, it is always hard to understand and retrieve\nknowledge from unstructured text, which is usually in the form of compound and\ncomplex sentences. There are state-of-the-art neural network-based methods to\nsimplify the sentences for improved readability while replacing words with\nplain English substitutes and summarising the sentences and paragraphs. In the\nKnowledge Graph (KG) creation process from unstructured text, summarising long\nsentences and substituting words is undesirable since this may lead to\ninformation loss. However, KG creation from text requires the extraction of all\npossible facts (triples) with the same mentions as in the text. In this work,\nwe propose a controlled simplification based on the factual information in a\nsentence, i.e., triple. We present a classical syntactic dependency-based\napproach to split and rephrase a compound and complex sentence into a set of\nsimplified sentences. This simplification process will retain the original\nwording with a simple structure of possible domain facts in each sentence,\ni.e., triples. The paper also introduces an algorithm to identify and measure a\nsentence's syntactic complexity (SC), followed by reduction through a\ncontrolled syntactic simplification process. Last, an experiment for a dataset\nre-annotation is also conducted through GPT3; we aim to publish this refined\ncorpus as a resource. This work is accepted and presented in International\nworkshop on Learning with Knowledge Graphs (IWLKG) at WSDM-2023 Conference. The\ncode and data is available at www.github.com/sallmanm/SynSim.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salman_M/0/1/0/all/0/1\">Muhammad Salman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haller_A/0/1/0/all/0/1\">Armin Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_S/0/1/0/all/0/1\">Sergio J. Rodr&#xed;guez M&#xe9;ndez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SikuGPT: A Generative Pre-trained Model for Intelligent Information Processing of Ancient Texts from the Perspective of Digital Humanities. (arXiv:2304.07778v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07778","description":"<p>The rapid advance in artificial intelligence technology has facilitated the\nprosperity of digital humanities research. Against such backdrop, research\nmethods need to be transformed in the intelligent processing of ancient texts,\nwhich is a crucial component of digital humanities research, so as to adapt to\nnew development trends in the wave of AIGC. In this study, we propose a GPT\nmodel called SikuGPT based on the corpus of Siku Quanshu. The model's\nperformance in tasks such as intralingual translation and text classification\nexceeds that of other GPT-type models aimed at processing ancient texts.\nSikuGPT's ability to process traditional Chinese ancient texts can help promote\nthe organization of ancient information and knowledge services, as well as the\ninternational dissemination of Chinese ancient culture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_L/0/1/0/all/0/1\">Liu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dongbo_W/0/1/0/all/0/1\">Wang Dongbo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhixiao_Z/0/1/0/all/0/1\">Zhao Zhixiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Die_H/0/1/0/all/0/1\">Hu Die</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mengcheng_W/0/1/0/all/0/1\">Wu Mengcheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litao_L/0/1/0/all/0/1\">Lin Litao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shen Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bin_L/0/1/0/all/0/1\">Li Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiangfeng_L/0/1/0/all/0/1\">Liu Jiangfeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hai_Z/0/1/0/all/0/1\">Zhang Hai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lianzheng_Z/0/1/0/all/0/1\">Zhao Lianzheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's All in the Embedding! Fake News Detection Using Document Embeddings. (arXiv:2304.07781v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07781","description":"<p>With the current shift in the mass media landscape from journalistic rigor to\nsocial media, personalized social media is becoming the new norm. Although the\ndigitalization progress of the media brings many advantages, it also increases\nthe risk of spreading disinformation, misinformation, and malformation through\nthe use of fake news. The emergence of this harmful phenomenon has managed to\npolarize society and manipulate public opinion on particular topics, e.g.,\nelections, vaccinations, etc. Such information propagated on social media can\ndistort public perceptions and generate social unrest while lacking the rigor\nof traditional journalism. Natural Language Processing and Machine Learning\ntechniques are essential for developing efficient tools that can detect fake\nnews. Models that use the context of textual data are essential for resolving\nthe fake news detection problem, as they manage to encode linguistic features\nwithin the vector representation of words. In this paper, we propose a new\napproach that uses document embeddings to build multiple models that accurately\nlabel news articles as reliable or fake. We also present a benchmark on\ndifferent architectures that detect fake news using binary or multi-labeled\nclassification. We evaluated the models on five large news corpora using\naccuracy, precision, and recall. We obtained better results than more complex\nstate-of-the-art Deep Neural Network models. We observe that the most important\nfactor for obtaining high accuracy is the document encoding, not the\nclassification model's complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truica_C/0/1/0/all/0/1\">Ciprian-Octavian Truic&#x103;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostol_E/0/1/0/all/0/1\">Elena-Simona Apostol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EasyNER: A Customizable Easy-to-Use Pipeline for Deep Learning- and Dictionary-based Named Entity Recognition from Medical Text. (arXiv:2304.07805v1 [q-bio.QM])","link":"http://arxiv.org/abs/2304.07805","description":"<p>Medical research generates a large number of publications with the PubMed\ndatabase already containing &gt;35 million research articles. Integration of the\nknowledge scattered across this large body of literature could provide key\ninsights into physiological mechanisms and disease processes leading to novel\nmedical interventions. However, it is a great challenge for researchers to\nutilize this information in full since the scale and complexity of the data\ngreatly surpasses human processing abilities. This becomes especially\nproblematic in cases of extreme urgency like the COVID-19 pandemic. Automated\ntext mining can help extract and connect information from the large body of\nmedical research articles. The first step in text mining is typically the\nidentification of specific classes of keywords (e.g., all protein or disease\nnames), so called Named Entity Recognition (NER). Here we present an end-to-end\npipeline for NER of typical entities found in medical research articles,\nincluding diseases, cells, chemicals, genes/proteins, and species. The pipeline\ncan access and process large medical research article collections (PubMed,\nCORD-19) or raw text and incorporates a series of deep learning models\nfine-tuned on the HUNER corpora collection. In addition, the pipeline can\nperform dictionary-based NER related to COVID-19 and other medical topics.\nUsers can also load their own NER models and dictionaries to include additional\nentities. The output consists of publication-ready ranked lists and graphs of\ndetected entities and files containing the annotated texts. An associated\nscript allows rapid inspection of the results for specific entities of\ninterest. As model use cases, the pipeline was deployed on two collections of\nautophagy-related abstracts from PubMed and on the CORD19 dataset, a collection\nof 764 398 research article abstracts related to COVID-19.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Ahmed_R/0/1/0/all/0/1\">Rafsan Ahmed</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Berntsson_P/0/1/0/all/0/1\">Petter Berntsson</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Skafte_A/0/1/0/all/0/1\">Alexander Skafte</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rashed_S/0/1/0/all/0/1\">Salma Kazemi Rashed</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Klang_M/0/1/0/all/0/1\">Marcus Klang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Barvesten_A/0/1/0/all/0/1\">Adam Barvesten</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Olde_O/0/1/0/all/0/1\">Ola Olde</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lindholm_W/0/1/0/all/0/1\">William Lindholm</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Arrizabalaga_A/0/1/0/all/0/1\">Antton Lamarca Arrizabalaga</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nugues_P/0/1/0/all/0/1\">Pierre Nugues</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Aits_S/0/1/0/all/0/1\">Sonja Aits</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping. (arXiv:2304.07810v1 [cs.HC])","link":"http://arxiv.org/abs/2304.07810","description":"<p>In argumentative writing, writers must brainstorm hierarchical writing goals,\nensure the persuasiveness of their arguments, and revise and organize their\nplans through drafting. Recent advances in large language models (LLMs) have\nmade interactive text generation through a chat interface (e.g., ChatGPT)\npossible. However, this approach often neglects implicit writing context and\nuser intent, lacks support for user control and autonomy, and provides limited\nassistance for sensemaking and revising writing plans. To address these\nchallenges, we introduce VISAR, an AI-enabled writing assistant system designed\nto help writers brainstorm and revise hierarchical goals within their writing\ncontext, organize argument structures through synchronized text editing and\nvisual programming, and enhance persuasiveness with argumentation spark\nrecommendations. VISAR allows users to explore, experiment with, and validate\ntheir writing plans using automatic draft prototyping. A controlled lab study\nconfirmed the usability and effectiveness of VISAR in facilitating the\nargumentative writing planning process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhaliwal_R/0/1/0/all/0/1\">Ranjodh Singh Dhaliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Toby Jia-Jun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How does ChatGPT rate sound semantics?. (arXiv:2304.07830v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07830","description":"<p>Semantic dimensions of sound have been playing a central role in\nunderstanding the nature of auditory sensory experience as well as the broader\nrelation between perception, language, and meaning. Accordingly, and given the\nrecent proliferation of large language models (LLMs), here we asked whether\nsuch models exhibit an organisation of perceptual semantics similar to those\nobserved in humans. Specifically, we prompted ChatGPT, a chatbot based on a\nstate-of-the-art LLM, to rate musical instrument sounds on a set of 20 semantic\nscales. We elicited multiple responses in separate chats, analogous to having\nmultiple human raters. ChatGPT generated semantic profiles that only partially\ncorrelated with human ratings, yet showed robust agreement along well-known\npsychophysical dimensions of musical sounds such as brightness (bright-dark)\nand pitch height (deep-high). Exploratory factor analysis suggested the same\ndimensionality but different spatial configuration of a latent factor space\nbetween the chatbot and human ratings. Unexpectedly, the chatbot showed degrees\nof internal variability that were comparable in magnitude to that of human\nratings. Our work highlights the potential of LLMs to capture salient\ndimensions of human sensory experience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siedenburg_K/0/1/0/all/0/1\">Kai Siedenburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1\">Charalampos Saitis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human. (arXiv:2304.07849v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07849","description":"<p>In this paper, we present ChatPLUG, a Chinese open-domain dialogue system for\ndigital human applications that instruction finetunes on a wide range of\ndialogue tasks in a unified internet-augmented format. Different from other\nopen-domain dialogue models that focus on large-scale pre-training and scaling\nup model size or dialogue corpus, we aim to build a powerful and practical\ndialogue system for digital human with diverse skills and good multi-task\ngeneralization by internet-augmented instruction tuning. To this end, we first\nconduct large-scale pre-training on both common document corpus and dialogue\ndata with curriculum learning, so as to inject various world knowledge and\ndialogue abilities into ChatPLUG. Then, we collect a wide range of dialogue\ntasks spanning diverse features of knowledge, personality, multi-turn memory,\nand empathy, on which we further instruction tune \\modelname via unified\nnatural language instruction templates. External knowledge from an internet\nsearch is also used during instruction finetuning for alleviating the problem\nof knowledge hallucinations. We show that \\modelname outperforms\nstate-of-the-art Chinese dialogue systems on both automatic and human\nevaluation, and demonstrates strong multi-task generalization on a variety of\ntext understanding and generation tasks. In addition, we deploy \\modelname to\nreal-world applications such as Smart Speaker and Instant Message applications\nwith fast inference. Our models and code will be made publicly available on\nModelScope~\\footnote{\\small{https://modelscope.cn/models/damo/ChatPLUG-3.7B}}\nand Github~\\footnote{\\small{https://github.com/X-PLUG/ChatPLUG}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junfeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hehong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianhai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenshen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiejing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation. (arXiv:2304.07854v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07854","description":"<p>Recently, significant public efforts have been directed towards developing\nlow-cost models with capabilities akin to ChatGPT, thereby fostering the growth\nof open-source conversational models. However, there remains a scarcity of\ncomprehensive and in-depth evaluations of these models' performance. In this\nstudy, we examine the influence of training data factors, including quantity,\nquality, and linguistic distribution, on model performance. Our analysis is\ngrounded in several publicly accessible, high-quality instruction datasets, as\nwell as our own Chinese multi-turn conversations. We assess various models\nusing a evaluation set of 1,000 samples, encompassing nine real-world\nscenarios. Our goal is to supplement manual evaluations with quantitative\nanalyses, offering valuable insights for the continued advancement of\nopen-source chat models. Furthermore, to enhance the performance and training\nand inference efficiency of models in the Chinese domain, we extend the\nvocabulary of LLaMA - the model with the closest open-source performance to\nproprietary language models like GPT-3 - and conduct secondary pre-training on\n3.4B Chinese words. We make our model, data, as well as code publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yunjie Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yiping Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Q/0/1/0/all/0/1\">Qiang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Baochang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Machine Translation For Low Resource Languages. (arXiv:2304.07869v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07869","description":"<p>Neural Machine translation is a challenging task due to the inherent complex\nnature and the fluidity that natural languages bring. Nonetheless, in recent\nyears, it has achieved state-of-the-art performance in several language pairs.\nAlthough, a lot of traction can be seen in the areas of multilingual neural\nmachine translation (MNMT) in the recent years, there are no comprehensive\nsurvey done to identify what approaches work well. The goal of this project is\nto investigate the realm of low resource languages and build a Neural Machine\nTranslation model to achieve state-of-the-art results. The project looks to\nbuild upon the \\texttt{mBART.CC25} \\cite{liu2020multilingual} language model\nand explore strategies to augment it with various NLP and Deep Learning\ntechniques like back translation and transfer learning. This implementation\ntries to unpack the architecture of the NMT application and determine the\ndifferent components which offers us opportunities to amend the said\napplication within the purview of the low resource languages problem space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyle_V/0/1/0/all/0/1\">Vakul Goyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyle_K/0/1/0/all/0/1\">Kartikay Goyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_P/0/1/0/all/0/1\">Parvathy Krishnaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_K/0/1/0/all/0/1\">Kannan Girija Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_U/0/1/0/all/0/1\">Utsa Chattopadhyay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v18 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1906.02358","description":"<p>Sinhala is the native language of the Sinhalese people who make up the\nlargest ethnic group of Sri Lanka. The language belongs to the globe-spanning\nlanguage tree, Indo-European. However, due to poverty in both linguistic and\neconomic capital, Sinhala, in the perspective of Natural Language Processing\ntools and research, remains a resource-poor language which has neither the\neconomic drive its cousin English has nor the sheer push of the law of numbers\na language such as Chinese has. A number of research groups from Sri Lanka have\nnoticed this dearth and the resultant dire need for proper tools and research\nfor Sinhala natural language processing. However, due to various reasons, these\nattempts seem to lack coordination and awareness of each other. The objective\nof this paper is to fill that gap of a comprehensive literature survey of the\npublicly available Sinhala natural language tools and research so that the\nresearchers working in this field can better utilize contributions of their\npeers. As such, we shall be uploading this paper to arXiv and perpetually\nupdate it periodically to reflect the advances made in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeliData: A dataset for deliberation in multi-party problem solving. (arXiv:2108.05271v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.05271","description":"<p>Group deliberation enables people to collaborate and solve problems, however,\nit is understudied due to a lack of resources. To this end, we introduce the\nfirst publicly available dataset containing collaborative conversations on\nsolving a well-established cognitive task, consisting of 500 group dialogues\nand 14k utterances. In 64% of these conversations, the group members are able\nto find a better solution than they had identified individually, and in 43.8%\nof the groups who had a correct answer as their final solution, none of the\nparticipants had solved the task correctly by themselves. Furthermore, we\npropose a novel annotation schema that captures deliberation cues and release\nall 14k utterances annotated with it. Finally, we use the proposed dataset to\ndevelop and evaluate two methods for generating deliberation utterances. The\ndata collection platform, dataset and annotated corpus are publicly available\nat https://delibot.xyz.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karadzhov_G/0/1/0/all/0/1\">Georgi Karadzhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stafford_T/0/1/0/all/0/1\">Tom Stafford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Instance and Knowledge Alignment Pretraining for Aspect-based Sentiment Analysis. (arXiv:2110.13398v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.13398","description":"<p>Aspect-based Sentiment Analysis (ABSA) aims to determine the sentiment\npolarity towards an aspect. Because of the expensive and limited labelled data,\nthe pretraining strategy has become the de-facto standard for ABSA. However,\nthere always exists severe domain shift between the pretraining and downstream\nABSA datasets, hindering the effective knowledge transfer when directly\nfinetuning and making the downstream task performs sub-optimal. To mitigate\nsuch domain shift, we introduce a unified alignment pretraining framework into\nthe vanilla pretrain-finetune pipeline with both instance- and knowledge-level\nalignments. Specifically, we first devise a novel coarse-to-fine retrieval\nsampling approach to select target domain-related instances from the\nlarge-scale pretraining dataset, thus aligning the instances between\npretraining and target domains (First Stage). Then, we introduce a knowledge\nguidance-based strategy to further bridge the domain gap at the knowledge\nlevel. In practice, we formulate the model pretrained on the sampled instances\ninto a knowledge guidance model and a learner model, respectively. On the\ntarget dataset, we design an on-the-fly teacher-student joint fine-tuning\napproach to progressively transfer the knowledge from the knowledge guidance\nmodel to the learner model (Second Stage). Thereby, the learner model can\nmaintain more domain-invariant knowledge when learning new knowledge from the\ntarget dataset. In the Third Stage, the learner model is finetuned to better\nadapt its learned knowledge to the target dataset. Extensive experiments and\nanalyses on several ABSA benchmarks demonstrate the effectiveness and\nuniversality of our proposed pretraining framework. Our source code and models\nare publicly available at https://github.com/WHU-ZQH/UIKA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qihuang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TraVLR: Now You See It, Now You Don't! A Bimodal Dataset for Evaluating Visio-Linguistic Reasoning. (arXiv:2111.10756v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.10756","description":"<p>Numerous visio-linguistic (V+L) representation learning methods have been\ndeveloped, yet existing datasets do not adequately evaluate the extent to which\nthey represent visual and linguistic concepts in a unified space. We propose\nseveral novel evaluation settings for V+L models, including cross-modal\ntransfer. Furthermore, existing V+L benchmarks often report global accuracy\nscores on the entire dataset, making it difficult to pinpoint the specific\nreasoning tasks that models fail and succeed at. We present TraVLR, a synthetic\ndataset comprising four V+L reasoning tasks. TraVLR's synthetic nature allows\nus to constrain its training and testing distributions along task-relevant\ndimensions, enabling the evaluation of out-of-distribution generalisation. Each\nexample in TraVLR redundantly encodes the scene in two modalities, allowing\neither to be dropped or added during training or testing without losing\nrelevant information. We compare the performance of four state-of-the-art V+L\nmodels, finding that while they perform well on test examples from the same\nmodality, they all fail at cross-modal transfer and have limited success\naccommodating the addition or deletion of one modality. We release TraVLR as an\nopen challenge for the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chow_K/0/1/0/all/0/1\">Keng Ji Chow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. (arXiv:2205.10625v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2205.10625","description":"<p>Chain-of-thought prompting has demonstrated remarkable performance on various\nnatural language reasoning tasks. However, it tends to perform poorly on tasks\nwhich requires solving problems harder than the exemplars shown in the prompts.\nTo overcome this challenge of easy-to-hard generalization, we propose a novel\nprompting strategy, least-to-most prompting. The key idea in this strategy is\nto break down a complex problem into a series of simpler subproblems and then\nsolve them in sequence. Solving each subproblem is facilitated by the answers\nto previously solved subproblems. Our experimental results on tasks related to\nsymbolic manipulation, compositional generalization, and math reasoning reveal\nthat least-to-most prompting is capable of generalizing to more difficult\nproblems than those seen in the prompts. A notable finding is that when the\nGPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve\nthe compositional generalization benchmark SCAN in any split (including length\nsplit) with an accuracy of at least 99% using just 14 exemplars, compared to\nonly 16% accuracy with chain-of-thought prompting. This is particularly\nnoteworthy because neural-symbolic models in the literature that specialize in\nsolving SCAN are trained on the entire training set containing over 15,000\nexamples. We have included prompts for all the tasks in the Appendix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharli_N/0/1/0/all/0/1\">Nathanael Sch&#xe4;rli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scales_N/0/1/0/all/0/1\">Nathan Scales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Claire Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bousquet_O/0/1/0/all/0/1\">Olivier Bousquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed Chi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models. (arXiv:2206.09557v3 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2206.09557","description":"<p>The recent advancements in self-supervised learning, combined with the\nTransformer architecture, have enabled natural language processing (NLP) to\nachieve remarkably low perplexity. However, powerful NLP models necessitate\nincreasing model size, leading to substantial computational and memory\nrequirements. In this paper, we introduce an efficient inference framework\ntailored for large-scale generative language models. To reduce the model size,\nwe employ a weight-only quantization strategy while preserving full precision\nfor activations. As a result, we attain sub-4-bit quantization for each weight\nthrough non-uniform or uniform quantization techniques. Our proposed kernel,\ncalled LUT-GEMM, then accelerates quantized matrix multiplications, offering a\nflexible balance between compression ratio and accuracy. Unlike earlier matrix\nmultiplication kernels that accommodated weight-only quantization, LUT-GEMM\nefficiently eliminates the resource-demanding dequantization process for both\nuniform and non-uniform quantization methods. By reducing the latency of\nindividual GPUs and the overall inference process for large-scale language\nmodels, LUT-GEMM provides significant performance improvements in inference.\nThe impact of LUT-GEMM is facilitated by implementing high compression ratios\nthrough low-bit quantization and efficient LUT-based operations, which\ndecreases the number of required GPUs. For the OPT-175B model with 3-bit\nquantization, we show that LUT-GEMM accelerates the latency for generating each\ntoken by 2.1x compared to OPTQ, which requires costly dequantization.\nConsequently, LUT-GEMM enables inference of the OPT-175B model on a single GPU\nwithout noticeable degradation in accuracy or performance, while the\nnon-quantized OPT-175B model requires a minimum of 8 GPUs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1\">Gunho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1\">Baeseong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsub Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeonghoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_B/0/1/0/all/0/1\">Beomseok Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1\">Se Jung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byeongwook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngjoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongsoo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Tailed Averaging: Anytime, Adaptive, Once-in-a-While Optimal Weight Averaging for Better Generalization. (arXiv:2209.12581v3 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2209.12581","description":"<p>Tail Averaging improves on Polyak averaging's non-asymptotic behaviour by\nexcluding a number of leading iterates of stochastic optimization from its\ncalculations. In practice, with a finite number of optimization steps and a\nlearning rate that cannot be annealed to zero, Tail Averaging can get much\ncloser to a local minimum point of the training loss than either the individual\niterates or the Polyak average. However, the number of leading iterates to\nignore is an important hyperparameter, and starting averaging too early or too\nlate leads to inefficient use of resources or suboptimal solutions. Our work\nfocusses on improving generalization, which makes setting this hyperparameter\neven more difficult, especially in the presence of other hyperparameters and\noverfitting. Furthermore, before averaging starts, the loss is only weakly\ninformative of the final performance, which makes early stopping unreliable. To\nalleviate these problems, we propose an anytime variant of Tail Averaging\nintended for improving generalization not pure optimization, that has no\nhyperparameters and approximates the optimal tail at all optimization steps.\nOur algorithm is based on two running averages with adaptive lengths bounded in\nterms of the optimal tail length, one of which achieves approximate optimality\nwith some regularity. Requiring only the additional storage for two sets of\nweights and periodic evaluation of the loss, the proposed Two-Tailed Averaging\nalgorithm is a practical and widely applicable method for improving\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Melis_G/0/1/0/all/0/1\">G&#xe1;bor Melis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases. (arXiv:2210.00063v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.00063","description":"<p>Question answering over knowledge bases (KBs) aims to answer natural language\nquestions with factual information such as entities and relations in KBs.\nPrevious methods either generate logical forms that can be executed over KBs to\nobtain final answers or predict answers directly. Empirical results show that\nthe former often produces more accurate answers, but it suffers from\nnon-execution issues due to potential syntactic and semantic errors in the\ngenerated logical forms. In this work, we propose a novel framework DecAF that\njointly generates both logical forms and direct answers, and then combines the\nmerits of them to get the final answers. Moreover, different from most of the\nprevious methods, DecAF is based on simple free-text retrieval without relying\non any entity linking tools -- this simplification eases its adaptation to\ndifferent datasets. DecAF achieves new state-of-the-art accuracy on WebQSP,\nFreebaseQA, and GrailQA benchmarks, while getting competitive results on the\nComplexWebQuestions benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Donghan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1\">Patrick Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Alexander Hanbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yiqun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiguo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning. (arXiv:2210.03112v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.03112","description":"<p>Recent studies in Vision-and-Language Navigation (VLN) train RL agents to\nexecute natural-language navigation instructions in photorealistic\nenvironments, as a step towards robots that can follow human instructions.\nHowever, given the scarcity of human instruction data and limited diversity in\nthe training environments, these agents still struggle with complex language\ngrounding and spatial language understanding. Pretraining on large text and\nimage-text datasets from the web has been extensively explored but the\nimprovements are limited. We investigate large-scale augmentation with\nsynthetic instructions. We take 500+ indoor environments captured in\ndensely-sampled 360 degree panoramas, construct navigation trajectories through\nthese panoramas, and generate a visually-grounded instruction for each\ntrajectory using Marky, a high-quality multilingual navigation instruction\ngenerator. We also synthesize image observations from novel viewpoints using an\nimage-to-image GAN. The resulting dataset of 4.2M instruction-trajectory pairs\nis two orders of magnitude larger than existing human-annotated datasets, and\ncontains a wider variety of environments and viewpoints. To efficiently\nleverage data at this scale, we train a simple transformer agent with imitation\nlearning. On the challenging RxR dataset, our approach outperforms all existing\nRL agents, improving the state-of-the-art NDTW from 71.1 to 79.1 in seen\nenvironments, and from 64.6 to 66.8 in unseen test environments. Our work\npoints to a new path to improving instruction-following agents, emphasizing\nlarge-scale imitation learning and the development of synthetic instruction\ngeneration capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Aishwarya Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1\">Peter Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Su Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_A/0/1/0/all/0/1\">Alexander Ku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1\">Austin Waters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1\">Zarana Parekh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction. (arXiv:2210.10709v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10709","description":"<p>With the development of pre-trained language models, many prompt-based\napproaches to data-efficient knowledge graph construction have been proposed\nand achieved impressive performance. However, existing prompt-based learning\nmethods for knowledge graph construction are still susceptible to several\npotential limitations: (i) semantic gap between natural language and output\nstructured knowledge with pre-defined schema, which means model cannot fully\nexploit semantic knowledge with the constrained templates; (ii) representation\nlearning with locally individual instances limits the performance given the\ninsufficient features, which are unable to unleash the potential analogical\ncapability of pre-trained language models. Motivated by these observations, we\npropose a retrieval-augmented approach, which retrieves schema-aware Reference\nAs Prompt (RAP), for data-efficient knowledge graph construction. It can\ndynamically leverage schema and knowledge inherited from human-annotated and\nweak-supervised data as a prompt for each sample, which is model-agnostic and\ncan be plugged into widespread existing approaches. Experimental results\ndemonstrate that previous methods integrated with RAP can achieve impressive\nperformance gains in low-resource settings on five datasets of relational\ntriple extraction and event extraction for knowledge graph construction. Code\nis available in https://github.com/zjunlp/RAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shengyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling structure-building in the brain with CCG parsing and large language models. (arXiv:2210.16147v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16147","description":"<p>To model behavioral and neural correlates of language comprehension in\nnaturalistic environments researchers have turned to broad-coverage tools from\nnatural-language processing and machine learning. Where syntactic structure is\nexplicitly modeled, prior work has relied predominantly on context-free\ngrammars (CFG), yet such formalisms are not sufficiently expressive for human\nlanguages. Combinatory Categorial Grammars (CCGs) are sufficiently expressive\ndirectly compositional models of grammar with flexible constituency that\naffords incremental interpretation. In this work we evaluate whether a more\nexpressive CCG provides a better model than a CFG for human neural signals\ncollected with fMRI while participants listen to an audiobook story. We further\ntest between variants of CCG that differ in how they handle optional adjuncts.\nThese evaluations are carried out against a baseline that includes estimates of\nnext-word predictability from a Transformer neural network language model. Such\na comparison reveals unique contributions of CCG structure-building\npredominantly in the left posterior temporal lobe: CCG-derived measures offer a\nsuperior fit to neural signals compared to those derived from a CFG. These\neffects are spatially distinct from bilateral superior temporal effects that\nare unique to predictability. Neural effects for structure-building are thus\nseparable from predictability during naturalistic listening, and those effects\nare best characterized by a grammar whose expressive power is motivated on\nindependent linguistic grounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stanojevic_M/0/1/0/all/0/1\">Milo&#x161; Stanojevi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brennan_J/0/1/0/all/0/1\">Jonathan R. Brennan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunagan_D/0/1/0/all/0/1\">Donald Dunagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_J/0/1/0/all/0/1\">John T. Hale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficiently Trained Low-Resource Mongolian Text-to-Speech System Based On FullConv-TTS. (arXiv:2211.01948v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01948","description":"<p>Recurrent Neural Networks (RNNs) have become the standard modeling technique\nfor sequence data, and are used in a number of novel text-to-speech models.\nHowever, training a TTS model including RNN components has certain requirements\nfor GPU performance and takes a long time. In contrast, studies have shown that\nCNN-based sequence synthesis technology can greatly reduce training time in\ntext-to-speech models while ensuring a certain performance due to its high\nparallelism. We propose a new text-to-speech system based on deep convolutional\nneural networks that does not employ any RNN components (recurrent units). At\nthe same time, we improve the generality and robustness of our model through a\nseries of data augmentation methods such as Time Warping, Frequency Mask, and\nTime Mask. The final experimental results show that the TTS model using only\nthe CNN component can reduce the training time compared to the classic TTS\nmodels such as Tacotron while ensuring the quality of the synthesized speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Ziqi Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VaxxHesitancy: A Dataset for Studying Hesitancy towards COVID-19 Vaccination on Twitter. (arXiv:2301.06660v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.06660","description":"<p>Vaccine hesitancy has been a common concern, probably since vaccines were\ncreated and, with the popularisation of social media, people started to express\ntheir concerns about vaccines online alongside those posting pro- and\nanti-vaccine content. Predictably, since the first mentions of a COVID-19\nvaccine, social media users posted about their fears and concerns or about\ntheir support and belief into the effectiveness of these rapidly developing\nvaccines. Identifying and understanding the reasons behind public hesitancy\ntowards COVID-19 vaccines is important for policy markers that need to develop\nactions to better inform the population with the aim of increasing vaccine\ntake-up. In the case of COVID-19, where the fast development of the vaccines\nwas mirrored closely by growth in anti-vaxx disinformation, automatic means of\ndetecting citizen attitudes towards vaccination became necessary. This is an\nimportant computational social sciences task that requires data analysis in\norder to gain in-depth understanding of the phenomena at hand. Annotated data\nis also necessary for training data-driven models for more nuanced analysis of\nattitudes towards vaccination. To this end, we created a new collection of over\n3,101 tweets annotated with users' attitudes towards COVID-19 vaccination\n(stance). Besides, we also develop a domain-specific language model (VaxxBERT)\nthat achieves the best predictive performance (73.0 accuracy and 69.3 F1-score)\nas compared to a robust set of baselines. To the best of our knowledge, these\nare the first dataset and model that model vaccine hesitancy as a category\ndistinct from pro- and anti-vaccine stance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yida Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Mali Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimshaw_C/0/1/0/all/0/1\">Charlie Grimshaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1\">Kalina Bontcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLIGEN: Open-Set Grounded Text-to-Image Generation. (arXiv:2301.07093v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2301.07093","description":"<p>Large-scale text-to-image diffusion models have made amazing advances.\nHowever, the status quo is to use text input alone, which can impede\ncontrollability. In this work, we propose GLIGEN, Grounded-Language-to-Image\nGeneration, a novel approach that builds upon and extends the functionality of\nexisting pre-trained text-to-image diffusion models by enabling them to also be\nconditioned on grounding inputs. To preserve the vast concept knowledge of the\npre-trained model, we freeze all of its weights and inject the grounding\ninformation into new trainable layers via a gated mechanism. Our model achieves\nopen-world grounded text2img generation with caption and bounding box condition\ninputs, and the grounding ability generalizes well to novel spatial\nconfigurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS\noutperforms that of existing supervised layout-to-image baselines by a large\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haotian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_F/0/1/0/all/0/1\">Fangzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Multimodal Vision Supervision Beneficial to Language?. (arXiv:2302.05016v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2302.05016","description":"<p>Vision (image and video) - Language (VL) pre-training is the recent popular\nparadigm that achieved state-of-the-art results on multi-modal tasks like\nimage-retrieval, video-retrieval, visual question answering etc. These models\nare trained in an unsupervised way and greatly benefit from the complementary\nmodality supervision. In this paper, we explore if the language representations\ntrained using vision supervision perform better than vanilla language\nrepresentations on Natural Language Understanding and commonsense reasoning\nbenchmarks. We experiment with a diverse set of image-text models such as\nALBEF, BLIP, METER and video-text models like ALPRO, Frozen-in-Time (FiT),\nVIOLET. We compare the performance of language representations of stand-alone\ntext encoders of these models to the language representations of text encoders\nlearnt through vision supervision. Our experiments suggest that vanilla\nlanguage representations show superior performance on most of the tasks. These\nresults shed light on the current drawbacks of the vision-language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1\">Avinash Madasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Enhanced Semantic Communication Receiver. (arXiv:2302.07727v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07727","description":"<p>In recent years, with the rapid development of deep learning and natural\nlanguage processing technologies, semantic communication has become a topic of\ngreat interest in the field of communication. Although existing deep\nlearning-based semantic communication approaches have shown many advantages,\nthey still do not make sufficient use of prior knowledge. Moreover, most\nexisting semantic communication methods focus on the semantic encoding at the\ntransmitter side, while we believe that the semantic decoding capability of the\nreceiver should also be concerned. In this paper, we propose a knowledge\nenhanced semantic communication framework in which the receiver can more\nactively utilize the facts in the knowledge base for semantic reasoning and\ndecoding, on the basis of only affecting the parameters rather than the\nstructure of the neural networks at the transmitter side. Specifically, we\ndesign a transformer-based knowledge extractor to find relevant factual triples\nfor the received noisy signal. Extensive simulation results on the WebNLG\ndataset demonstrate that the proposed receiver yields superior performance on\ntop of the knowledge graph enhanced decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianhang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhifeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honggang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation. (arXiv:2302.09664v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.09664","description":"<p>We introduce a method to measure uncertainty in large language models. For\ntasks like question answering, it is essential to know when we can trust the\nnatural language outputs of foundation models. We show that measuring\nuncertainty in natural language is challenging because of \"semantic\nequivalence\" -- different sentences can mean the same thing. To overcome these\nchallenges we introduce semantic entropy -- an entropy which incorporates\nlinguistic invariances created by shared meanings. Our method is unsupervised,\nuses only a single model, and requires no modifications to off-the-shelf\nlanguage models. In comprehensive ablation studies we show that the semantic\nentropy is more predictive of model accuracy on question answering data sets\nthan comparable baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuhn_L/0/1/0/all/0/1\">Lorenz Kuhn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1\">Sebastian Farquhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E2E Spoken Entity Extraction for Virtual Agents. (arXiv:2302.10186v5 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2302.10186","description":"<p>This paper rethink some aspects of speech processing using speech encoders,\nspecifically about extracting entities directly from speech, without\nintermediate textual representation. In human-computer conversations,\nextracting entities such as names, street addresses and email addresses from\nspeech is a challenging task. In this paper, we study the impact of fine-tuning\npre-trained speech encoders on extracting spoken entities in human-readable\nform directly from speech without the need for text transcription. We\nillustrate that such a direct approach optimizes the encoder to transcribe only\nthe entity relevant portions of speech ignoring the superfluous portions such\nas carrier phrases, or spell name entities. In the context of dialog from an\nenterprise virtual agent, we demonstrate that the 1-step approach outperforms\nthe typical 2-step approach which first generates lexical transcriptions\nfollowed by text-based entity extraction for identifying spoken entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Singla_K/0/1/0/all/0/1\">Karan Singla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_Y/0/1/0/all/0/1\">Yeon-Jun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval. (arXiv:2303.03004v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.03004","description":"<p>The ability to solve problems is a hallmark of intelligence and has been an\nenduring goal in AI. AI systems that can create programs as solutions to\nproblems or assist developers in writing programs can increase productivity and\nmake programming more accessible. Recently, pre-trained large language models\nhave shown impressive abilities in generating new codes from natural language\ndescriptions, repairing buggy codes, translating codes between languages, and\nretrieving relevant code segments. However, the evaluation of these models has\noften been performed in a scattered way on only one or two specific tasks, in a\nfew languages, at a partial granularity (e.g., function) level and in many\ncases without proper training data. Even more concerning is that in most cases\nthe evaluation of generated codes has been done in terms of mere lexical\noverlap rather than actual execution whereas semantic similarity (or\nequivalence) of two code segments depends only on their ``execution\nsimilarity'', i.e., being able to get the same output for a given input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Abdullah Matin Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_X/0/1/0/all/0/1\">Xuan Long Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weishi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parvez_M/0/1/0/all/0/1\">Md Rizwan Parvez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disambiguation of Company names via Deep Recurrent Networks. (arXiv:2303.05391v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.05391","description":"<p>Name Entity Disambiguation is the Natural Language Processing task of\nidentifying textual records corresponding to the same Named Entity, i.e.\nreal-world entities represented as a list of attributes (names, places,\norganisations, etc.). In this work, we face the task of disambiguating\ncompanies on the basis of their written names. We propose a Siamese LSTM\nNetwork approach to extract -- via supervised learning -- an embedding of\ncompany name strings in a (relatively) low dimensional vector space and use\nthis representation to identify pairs of company names that actually represent\nthe same company (i.e. the same Entity).\n</p>\n<p>Given that the manual labelling of string pairs is a rather onerous task, we\nanalyse how an Active Learning approach to prioritise the samples to be\nlabelled leads to a more efficient overall learning pipeline.\n</p>\n<p>With empirical investigations, we show that our proposed Siamese Network\noutperforms several benchmark approaches based on standard string matching\nalgorithms when enough labelled data are available. Moreover, we show that\nActive Learning prioritisation is indeed helpful when labelling resources are\nlimited, and let the learning models reach the out-of-sample performance\nsaturation with less labelled data with respect to standard (random) data\nlabelling approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basile_A/0/1/0/all/0/1\">Alessandro Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crupi_R/0/1/0/all/0/1\">Riccardo Crupi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasso_M/0/1/0/all/0/1\">Michele Grasso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercanti_A/0/1/0/all/0/1\">Alessandro Mercanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regoli_D/0/1/0/all/0/1\">Daniele Regoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarsi_S/0/1/0/all/0/1\">Simone Scarsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosentini_A/0/1/0/all/0/1\">Andrea Cosentini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Be Used to Scale the Ideologies of Politicians in a Zero-Shot Learning Setting. (arXiv:2303.12057v3 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2303.12057","description":"<p>The aggregation of knowledge embedded in large language models (LLMs) holds\nthe promise of new solutions to problems of observability and measurement in\nthe social sciences. We examine this potential in a challenging setting:\nmeasuring latent ideology -- crucial for better understanding core political\nfunctions such as democratic representation. We scale pairwise\nliberal-conservative comparisons between members of the 116th U.S. Senate using\nprompts made to ChatGPT. Our measure strongly correlates with widely used\nliberal-conservative scales such as DW-NOMINATE. Our scale also has\ninterpretative advantages, such as not placing senators who vote against their\nparty for ideologically extreme reasons towards the middle. Our measure is more\nstrongly associated with political activists' perceptions of senators than\nother measures, consistent with LLMs synthesizing vast amounts of politically\nrelevant data from internet/book corpora rather than memorizing existing\nmeasures. LLMs will likely open new avenues for measuring latent constructs\nutilizing modeled information from massive text corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Patrick Y. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagler_J/0/1/0/all/0/1\">Jonathan Nagler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_J/0/1/0/all/0/1\">Joshua A. Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messing_S/0/1/0/all/0/1\">Solomon Messing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Large Language Models. (arXiv:2303.18223v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.18223","description":"<p>Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yupeng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Yingqian Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zican Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yushuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xinyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task. (arXiv:2304.01097v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01097","description":"<p>The recent progress of large language models (LLMs), including ChatGPT and\nGPT-4, in comprehending and responding to human instructions has been\nremarkable. Nevertheless, these models typically perform better in English and\nhave not been explicitly trained for the medical domain, resulting in\nsuboptimal precision in diagnoses, drug recommendations, and other medical\nadvice. Additionally, training and deploying a dialogue model is still believed\nto be impossible for hospitals, hindering the promotion of LLMs. To tackle\nthese challenges, we have collected databases of medical dialogues in Chinese\nwith ChatGPT's help and adopted several techniques to train an easy-deploy LLM.\nRemarkably, we were able to fine-tune the ChatGLM-6B on a single A100 80G in 13\nhours, which means having a healthcare-purpose LLM can be very affordable.\nDoctorGLM is currently an early-stage engineering attempt and contain various\nmistakes. We are sharing it with the broader community to invite feedback and\nsuggestions to improve its healthcare-focused capabilities:\nhttps://github.com/xionghonglin/DoctorGLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Honglin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yitao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zihao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Linlin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpectFormer: Frequency and Attention is what you need in a Vision Transformer. (arXiv:2304.06446v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.06446","description":"<p>Vision transformers have been applied successfully for image recognition\ntasks. There have been either multi-headed self-attention based (ViT\n\\cite{dosovitskiy2020image}, DeIT, \\cite{touvron2021training}) similar to the\noriginal work in textual models or more recently based on spectral layers\n(Fnet\\cite{lee2021fnet}, GFNet\\cite{rao2021global},\nAFNO\\cite{guibas2021efficient}). We hypothesize that both spectral and\nmulti-headed attention plays a major role. We investigate this hypothesis\nthrough this work and observe that indeed combining spectral and multi-headed\nattention layers provides a better transformer architecture. We thus propose\nthe novel Spectformer architecture for transformers that combines spectral and\nmulti-headed attention layers. We believe that the resulting representation\nallows the transformer to capture the feature representation appropriately and\nit yields improved performance over other transformer representations. For\ninstance, it improves the top-1 accuracy by 2\\% on ImageNet compared to both\nGFNet-H and LiT. SpectFormer-S reaches 84.25\\% top-1 accuracy on ImageNet-1K\n(state of the art for small version). Further, Spectformer-L achieves 85.7\\%\nthat is the state of the art for the comparable base version of the\ntransformers. We further ensure that we obtain reasonable results in other\nscenarios such as transfer learning on standard datasets such as CIFAR-10,\nCIFAR-100, Oxford-IIIT-flower, and Standford Car datasets. We then investigate\nits use in downstream tasks such of object detection and instance segmentation\non the MS-COCO dataset and observe that Spectformer shows consistent\nperformance that is comparable to the best backbones and can be further\noptimized and improved. Hence, we believe that combined spectral and attention\nlayers are what are needed for vision transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patro_B/0/1/0/all/0/1\">Badri N. Patro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agneeswaran_V/0/1/0/all/0/1\">Vijay Srinivas Agneeswaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter. (arXiv:2304.06858v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2304.06858","description":"<p>Vaccine hesitancy continues to be a main challenge for public health\nofficials during the COVID-19 pandemic. As this hesitancy undermines vaccine\ncampaigns, many researchers have sought to identify its root causes, finding\nthat the increasing volume of anti-vaccine misinformation on social media\nplatforms is a key element of this problem. We explored Twitter as a source of\nmisleading content with the goal of extracting overlapping cultural and\npolitical beliefs that motivate the spread of vaccine misinformation. To do\nthis, we have collected a data set of vaccine-related Tweets and annotated them\nwith the help of a team of annotators with a background in communications and\njournalism. Ultimately we hope this can lead to effective and targeted public\nhealth communication strategies for reaching individuals with anti-vaccine\nbeliefs. Moreover, this information helps with developing Machine Learning\nmodels to automatically detect vaccine misinformation posts and combat their\nnegative impacts. In this paper, we present Vax-Culture, a novel Twitter\nCOVID-19 dataset consisting of 6373 vaccine-related tweets accompanied by an\nextensive set of human-provided annotations including vaccine-hesitancy stance,\nindication of any misinformation in tweets, the entities criticized and\nsupported in each tweet and the communicated message of each tweet. Moreover,\nwe define five baseline tasks including four classification and one sequence\ngeneration tasks, and report the results of a set of recent transformer-based\nmodels for them. The dataset and code are publicly available at\nhttps://github.com/mrzarei5/Vax-Culture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zarei_M/0/1/0/all/0/1\">Mohammad Reza Zarei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christensen_M/0/1/0/all/0/1\">Michael Christensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Everts_S/0/1/0/all/0/1\">Sarah Everts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Majid Komeili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}