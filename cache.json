{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-27T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Data leakage in cross-modal retrieval training: A case study. (arXiv:2302.12258v1 [cs.SD])","link":"http://arxiv.org/abs/2302.12258","description":"<p>The recent progress in text-based audio retrieval was largely propelled by\nthe release of suitable datasets. Since the manual creation of such datasets is\na laborious task, obtaining data from online resources can be a cheap solution\nto create large-scale datasets. We study the recently proposed SoundDesc\nbenchmark dataset, which was automatically sourced from the BBC Sound Effects\nweb page. In our analysis, we find that SoundDesc contains several duplicates\nthat cause leakage of training data to the evaluation data. This data leakage\nultimately leads to overly optimistic retrieval performance estimates in\nprevious benchmarks. We propose new training, validation, and testing splits\nfor the dataset that we make available online. To avoid weak contamination of\nthe test data, we pool audio files that share similar recording setups. In our\nexperiments, we find that the new splits serve as a more challenging benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weck_B/0/1/0/all/0/1\">Benno Weck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_X/0/1/0/all/0/1\">Xavier Serra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Benchmarking of Masked Language Models on Temporal Concept Drift with Multiple Views. (arXiv:2302.12297v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12297","description":"<p>Temporal concept drift refers to the problem of data changing over time. In\nNLP, that would entail that language (e.g. new expressions, meaning shifts) and\nfactual knowledge (e.g. new concepts, updated facts) evolve over time. Focusing\non the latter, we benchmark $11$ pretrained masked language models (MLMs) on a\nseries of tests designed to evaluate the effect of temporal concept drift, as\nit is crucial that widely used language models remain up-to-date with the\never-evolving factual updates of the real world. Specifically, we provide a\nholistic framework that (1) dynamically creates temporal test sets of any time\ngranularity (e.g. month, quarter, year) of factual data from Wikidata, (2)\nconstructs fine-grained splits of tests (e.g. updated, new, unchanged facts) to\nensure comprehensive analysis, and (3) evaluates MLMs in three distinct ways\n(single-token probing, multi-token generation, MLM scoring). In contrast to\nprior work, our framework aims to unveil how robust an MLM is over time and\nthus to provide a signal in case it has become outdated, by leveraging multiple\nviews of evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Margatina_K/0/1/0/all/0/1\">Katerina Margatina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_Y/0/1/0/all/0/1\">Yogarshi Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+John_N/0/1/0/all/0/1\">Neha Anna John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benajiba_Y/0/1/0/all/0/1\">Yassine Benajiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1\">Miguel Ballesteros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In What Languages are Generative Language Models the Most Formal? Analyzing Formality Distribution across Languages. (arXiv:2302.12299v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12299","description":"<p>Multilingual generative language models (LMs) are increasingly fluent in a\nlarge variety of languages. Trained on the concatenation of corpora in multiple\nlanguages, they enable powerful transfer from high-resource languages to\nlow-resource ones. However, it is still unknown what cultural biases are\ninduced in the predictions of these models. In this work, we focus on one\nlanguage property highly influenced by culture: formality. We analyze the\nformality distributions of XGLM and BLOOM's predictions, two popular generative\nmultilingual language models, in 5 languages. We classify 1,200 generations per\nlanguage as formal, informal, or incohesive and measure the impact of the\nprompt formality on the predictions. Overall, we observe a diversity of\nbehaviors across the models and languages. For instance, XGLM generates\ninformal text in Arabic and Bengali when conditioned with informal prompts,\nmuch more than BLOOM. In addition, even though both models are highly biased\ntoward the formal style when prompted neutrally, we find that the models\ngenerate a significant amount of informal predictions even when prompted with\nformal text. We release with this work 6,000 annotated samples, paving the way\nfor future work on the formality of generative multilingual LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ersoy_A/0/1/0/all/0/1\">As&#x131;m Ersoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vizcarra_G/0/1/0/all/0/1\">Gerson Vizcarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayeesha_T/0/1/0/all/0/1\">Tasmiah Tahsin Mayeesha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1\">Benjamin Muller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing AI performance on less frequent aspects of language reveals insensitivity to underlying meaning. (arXiv:2302.12313v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12313","description":"<p>Advances in computational methods and big data availability have recently\ntranslated into breakthroughs in AI applications. With successes in bottom-up\nchallenges partially overshadowing shortcomings, the 'human-like' performance\nof Large Language Models has raised the question of how linguistic performance\nis achieved by algorithms. Given systematic shortcomings in generalization\nacross many AI systems, in this work we ask whether linguistic performance is\nindeed guided by language knowledge in Large Language Models. To this end, we\nprompt GPT-3 with a grammaticality judgement task and comprehension questions\non less frequent constructions that are thus unlikely to form part of Large\nLanguage Models' training data. These included grammatical 'illusions',\nsemantic anomalies, complex nested hierarchies and self-embeddings. GPT-3\nfailed for every prompt but one, often offering answers that show a critical\nlack of understanding even of high-frequency words used in these less frequent\ngrammatical constructions. The present work sheds light on the boundaries of\nthe alleged AI human-like linguistic competence and argues that, far from\nhuman-like, the next-word prediction abilities of LLMs may face issues of\nrobustness, when pushed beyond training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dentella_V/0/1/0/all/0/1\">Vittoria Dentella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_E/0/1/0/all/0/1\">Elliot Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_G/0/1/0/all/0/1\">Gary Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leivada_E/0/1/0/all/0/1\">Evelina Leivada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization. (arXiv:2302.12324v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12324","description":"<p>Effective figure captions are crucial for clear comprehension of scientific\nfigures, yet poor caption writing remains a common issue in scientific\narticles. Our study of arXiv cs.CL papers found that 53.88% of captions were\nrated as unhelpful or worse by domain experts, showing the need for better\ncaption generation. Previous efforts in figure caption generation treated it as\na vision task, aimed at creating a model to understand visual content and\ncomplex contextual information. Our findings, however, demonstrate that over\n75% of figure captions' tokens align with corresponding figure-mentioning\nparagraphs, indicating great potential for language technology to solve this\ntask. In this paper, we present a novel approach for generating figure captions\nin scientific documents using text summarization techniques. Our approach\nextracts sentences referencing the target figure, then summarizes them into a\nconcise caption. In the experiments on real-world arXiv papers (81.2% were\npublished at academic conferences), our method, using only text data,\noutperformed previous approaches in both automatic and human evaluations. We\nfurther conducted data-driven investigations into the two core challenges: (i)\nlow-quality author-written captions and (ii) the absence of a standard for good\ncaptions. We found that our models could generate improved captions for figures\nwith original captions rated as unhelpful, and the model trained on captions\nwith more than 30 tokens produced higher-quality captions. We also found that\ngood captions often include the high-level takeaway of the figure. Our work\nproves the effectiveness of text summarization in generating figure captions\nfor scholarly articles, outperforming prior vision-based approaches. Our\nfindings have practical implications for future figure captioning systems,\nimproving scientific communication clarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chieh-Yang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_T/0/1/0/all/0/1\">Ting-Yao Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1\">Ryan Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenkova_A/0/1/0/all/0/1\">Ani Nenkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungchul Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_G/0/1/0/all/0/1\">Gromit Yeuk-Yin Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_E/0/1/0/all/0/1\">Eunyee Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giles_C/0/1/0/all/0/1\">Clyde Lee Giles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models. (arXiv:2302.12343v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12343","description":"<p>Large Language Models (LLMs) have yielded fast and dramatic progress in NLP,\nand now offer strong few- and zero-shot capabilities on new tasks, reducing the\nneed for annotation. This is especially exciting for the medical domain, in\nwhich supervision is often scant and expensive. At the same time, model\npredictions are rarely so accurate that they can be trusted blindly. Clinicians\ntherefore tend to favor \"interpretable\" classifiers over opaque LLMs. For\nexample, risk prediction tools are often linear models defined over manually\ncrafted predictors that must be laboriously extracted from EHRs. We propose\nCHiLL (Crafting High-Level Latents), which uses LLMs to permit natural language\nspecification of high-level features for linear models via zero-shot feature\nextraction using expert-composed queries. This approach has the promise to\nempower physicians to use their domain expertise to craft features which are\nclinically meaningful for a downstream task of interest, without having to\nmanually extract these from raw EHR (as often done now). We are motivated by a\nreal-world risk prediction task, but as a reproducible proxy, we use MIMIC-III\nand MIMIC-CXR data and standard predictive tasks (e.g., 30-day readmission) to\nevaluate our approach. We find that linear models using automatically extracted\nfeatures are comparably performant to models using reference features, and\nprovide greater interpretability than linear models using \"Bag-of-Words\"\nfeatures. We verify that learned feature weights align well with clinical\nexpectations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McInerney_D/0/1/0/all/0/1\">Denis Jered McInerney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_G/0/1/0/all/0/1\">Geoffrey Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1\">Jan-Willem van de Meent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Victim Counts from Text. (arXiv:2302.12367v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12367","description":"<p>Decision-makers in the humanitarian sector rely on timely and exact\ninformation during crisis events. Knowing how many civilians were injured\nduring an earthquake is vital to allocate aids properly. Information about such\nvictim counts is often only available within full-text event descriptions from\nnewspapers and other reports. Extracting numbers from text is challenging:\nnumbers have different formats and may require numeric reasoning. This renders\npurely string matching-based approaches insufficient. As a consequence,\nfine-grained counts of injured, displaced, or abused victims beyond fatalities\nare often not extracted and remain unseen. We cast victim count extraction as a\nquestion answering (QA) task with a regression or classification objective. We\ncompare regex, dependency parsing, semantic role labeling-based approaches, and\nadvanced text-to-text models. Beyond model accuracy, we analyze extraction\nreliability and robustness which are key for this sensitive task. In\nparticular, we discuss model calibration and investigate few-shot and\nout-of-distribution performance. Ultimately, we make a comprehensive\nrecommendation on which model to select for different desiderata and data\ndomains. Our work is among the first to apply numeracy-focused large language\nmodels in a real-world use case with a positive impact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Mian Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoehr_N/0/1/0/all/0/1\">Niklas Stoehr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factual Consistency Oriented Speech Recognition. (arXiv:2302.12369v1 [eess.AS])","link":"http://arxiv.org/abs/2302.12369","description":"<p>This paper presents a novel optimization framework for automatic speech\nrecognition (ASR) with the aim of reducing hallucinations produced by an ASR\nmodel. The proposed framework optimizes the ASR model to maximize an expected\nfactual consistency score between ASR hypotheses and ground-truth\ntranscriptions, where the factual consistency score is computed by a separately\ntrained estimator. Experimental results using the AMI meeting corpus and the\nVoxPopuli corpus show that the ASR model trained with the proposed framework\ngenerates ASR hypotheses that have significantly higher consistency scores with\nground-truth transcriptions while maintaining the word error rates close to\nthose of cross entropy-trained ASR models. Furthermore, it is shown that\ntraining the ASR models with the proposed framework improves the speech\nsummarization quality as measured by the factual consistency of meeting\nconversation summaries generated by a large language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Prediction Oriented method with Multiple Supervisions for Emotion-Cause Pair Extraction. (arXiv:2302.12417v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12417","description":"<p>Emotion-cause pair extraction (ECPE) task aims to extract all the pairs of\nemotions and their causes from an unannotated emotion text. The previous works\nusually extract the emotion-cause pairs from two perspectives of emotion and\ncause. However, emotion extraction is more crucial to the ECPE task than cause\nextraction. Motivated by this analysis, we propose an end-to-end emotion-cause\nextraction approach oriented toward emotion prediction (EPO-ECPE), aiming to\nfully exploit the potential of emotion prediction to enhance emotion-cause pair\nextraction. Considering the strong dependence between emotion prediction and\nemotion-cause pair extraction, we propose a synchronization mechanism to share\ntheir improvement in the training process. That is, the improvement of emotion\nprediction can facilitate the emotion-cause pair extraction, and then the\nresults of emotion-cause pair extraction can also be used to improve the\naccuracy of emotion prediction simultaneously. For the emotion-cause pair\nextraction, we divide it into genuine pair supervision and fake pair\nsupervision, where the genuine pair supervision learns from the pairs with more\npossibility to be emotion-cause pairs. In contrast, fake pair supervision\nlearns from other pairs. In this way, the emotion-cause pairs can be extracted\ndirectly from the genuine pair, thereby reducing the difficulty of extraction.\nExperimental results show that our approach outperforms the 13 compared systems\nand achieves new state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guimin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics. (arXiv:2302.12433v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12433","description":"<p>We introduce ProofNet, a benchmark for autoformalization and formal proving\nof undergraduate-level mathematics. The ProofNet benchmarks consists of 371\nexamples, each consisting of a formal theorem statement in Lean 3, a natural\nlanguage theorem statement, and a natural language proof. The problems are\nprimarily drawn from popular undergraduate pure mathematics textbooks and cover\ntopics such as real and complex analysis, linear algebra, abstract algebra, and\ntopology. We intend for ProofNet to be a challenging benchmark that will drive\nprogress in autoformalization and automatic theorem proving. We report baseline\nresults on statement autoformalization via in-context learning. Moreover, we\nintroduce two novel statement autoformalization methods: prompt retrieval and\ndistilled backtranslation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azerbayev_Z/0/1/0/all/0/1\">Zhangir Azerbayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piotrowski_B/0/1/0/all/0/1\">Bartosz Piotrowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_H/0/1/0/all/0/1\">Hailey Schoelkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayers_E/0/1/0/all/0/1\">Edward W. Ayers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avigad_J/0/1/0/all/0/1\">Jeremy Avigad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUX-PLMs: Pre-training Language Models with Data Multiplexing. (arXiv:2302.12441v1 [cs.LG])","link":"http://arxiv.org/abs/2302.12441","description":"<p>Data multiplexing is a recently proposed method for improving a model's\ninference efficiency by processing multiple instances simultaneously using an\nordered representation mixture. Prior work on data multiplexing only used\ntask-specific Transformers without any pre-training, which limited their\naccuracy and generality. In this paper, we develop pre-trained multiplexed\nlanguage models (MUX-PLMs) that can be widely finetuned on any downstream task.\nOur approach includes a three-stage training procedure and novel multiplexing\nand demultiplexing modules for improving throughput and downstream task\naccuracy. We demonstrate our method on BERT and ELECTRA pre-training\nobjectives, with our MUX-BERT and MUX-ELECTRA models achieving 2x/5x inference\nspeedup with a 2-4 \\% drop in absolute performance on GLUE and 1-2 \\% drop on\ntoken-level tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murahari_V/0/1/0/all/0/1\">Vishvak Murahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_C/0/1/0/all/0/1\">Carlos E. Jimenez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafran_I/0/1/0/all/0/1\">Izhak Shafran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingqiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGL-PT: A Strong Graph Learner with Graph Prompt Tuning. (arXiv:2302.12449v1 [cs.LG])","link":"http://arxiv.org/abs/2302.12449","description":"<p>Recently, much exertion has been paid to design graph self-supervised methods\nto obtain generalized pre-trained models, and adapt pre-trained models onto\ndownstream tasks through fine-tuning. However, there exists an inherent gap\nbetween pretext and downstream graph tasks, which insufficiently exerts the\nability of pre-trained models and even leads to negative transfer. Meanwhile,\nprompt tuning has seen emerging success in natural language processing by\naligning pre-training and fine-tuning with consistent training objectives. In\nthis paper, we identify the challenges for graph prompt tuning: The first is\nthe lack of a strong and universal pre-training task across sundry pre-training\nmethods in graph domain. The second challenge lies in the difficulty of\ndesigning a consistent training objective for both pre-training and downstream\ntasks. To overcome above obstacles, we propose a novel framework named SGL-PT\nwhich follows the learning strategy ``Pre-train, Prompt, and Predict''.\nSpecifically, we raise a strong and universal pre-training task coined as SGL\nthat acquires the complementary merits of generative and contrastive\nself-supervised graph learning. And aiming for graph classification task, we\nunify pre-training and fine-tuning by designing a novel verbalizer-free\nprompting function, which reformulates the downstream task in a similar format\nas pretext task. Empirical results show that our method surpasses other\nbaselines under unsupervised setting, and our prompt tuning method can greatly\nfacilitate models on biological datasets over fine-tuning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianhao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. (arXiv:2302.12461v1 [cs.LG])","link":"http://arxiv.org/abs/2302.12461","description":"<p>Recent advancements in interpretability research made transformer language\nmodels more transparent. This progress led to a better understanding of their\ninner workings for toy and naturally occurring models. However, how these\nmodels internally process sentiment changes has yet to be sufficiently\nanswered. In this work, we introduce a new interpretability tool called PCP\nablation, where we replace modules with low-rank matrices based on the\nprincipal components of their activations, reducing model parameters and their\nbehavior to essentials. We demonstrate PCP ablations on MLP and attention\nlayers in backdoored toy, backdoored large, and naturally occurring models. We\ndetermine MLPs as most important for the backdoor mechanism and use this\nknowledge to remove, insert, and modify backdoor mechanisms with engineered\nreplacements via PCP ablation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lamparth_M/0/1/0/all/0/1\">Max Lamparth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuel_A/0/1/0/all/0/1\">Anka Reuel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Table-to-Text Generation with Prompt-based Adapter. (arXiv:2302.12468v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12468","description":"<p>Pre-trained language models (PLMs) have made remarkable progress in\ntable-to-text generation tasks. However, the topological gap between tabular\ndata and text and the lack of domain-specific knowledge make it difficult for\nPLMs to produce faithful text, especially in real-world applications with\nlimited resources. In this paper, we mitigate the above challenges by\nintroducing a novel augmentation method: Prompt-based Adapter (PA), which\ntargets table-to-text generation under few-shot conditions. The core insight\ndesign of the PA is to inject prompt templates for augmenting domain-specific\nknowledge and table-related representations into the model for bridging the\nstructural gap between tabular data and descriptions through adapters. Such\nprompt-based knowledge augmentation method brings at least two benefits: (1)\nenables us to fully use the large amounts of unlabelled domain-specific\nknowledge, which can alleviate the PLMs' inherent shortcomings of lacking\ndomain knowledge; (2) allows us to design different types of tasks supporting\nthe generative challenge. Extensive experiments and analyses are conducted on\nthree open-domain few-shot NLG datasets: Humans, Books, and Songs. Compared to\nprevious state-of-the-art approaches, our model achieves superior performance\nin terms of both fluency and accuracy as judged by human and automatic\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhixin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Minyxuan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiexing Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Ziwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guanjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Sentence Similarity Estimation for Unsupervised Extractive Summarization. (arXiv:2302.12490v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12490","description":"<p>Unsupervised extractive summarization aims to extract salient sentences from\na document as the summary without labeled data. Recent literatures mostly\nresearch how to leverage sentence similarity to rank sentences in the order of\nsalience. However, sentence similarity estimation using pre-trained language\nmodels mostly takes little account of document-level information and has a weak\ncorrelation with sentence salience ranking. In this paper, we proposed two\nnovel strategies to improve sentence similarity estimation for unsupervised\nextractive summarization. We use contrastive learning to optimize a\ndocument-level objective that sentences from the same document are more similar\nthan those from different documents. Moreover, we use mutual learning to\nenhance the relationship between sentence similarity estimation and sentence\nsalience ranking, where an extra signal amplifier is used to refine the pivotal\ninformation. Experimental results demonstrate the effectiveness of our\nstrategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Ruifeng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time-aware Multiway Adaptive Fusion Network for Temporal Knowledge Graph Question Answering. (arXiv:2302.12529v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12529","description":"<p>Knowledge graphs (KGs) have received increasing attention due to its wide\napplications on natural language processing. However, its use case on temporal\nquestion answering (QA) has not been well-explored. Most of existing methods\nare developed based on pre-trained language models, which might not be capable\nto learn \\emph{temporal-specific} presentations of entities in terms of\ntemporal KGQA task. To alleviate this problem, we propose a novel\n\\textbf{T}ime-aware \\textbf{M}ultiway \\textbf{A}daptive (\\textbf{TMA}) fusion\nnetwork. Inspired by the step-by-step reasoning behavior of humans. For each\ngiven question, TMA first extracts the relevant concepts from the KG, and then\nfeeds them into a multiway adaptive module to produce a\n\\emph{temporal-specific} representation of the question. This representation\ncan be incorporated with the pre-trained KG embedding to generate the final\nprediction. Empirical results verify that the proposed model achieves better\nperformance than the state-of-the-art models in the benchmark dataset. Notably,\nthe Hits@1 and Hits@10 results of TMA on the CronQuestions dataset's complex\nquestions are absolutely improved by 24\\% and 10\\% compared to the\nbest-performing baseline. Furthermore, we also show that TMA employing an\nadaptive fusion mechanism can provide interpretability by analyzing the\nproportion of information in question representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yonghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Di Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1\">Fang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rui Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Path Modeling for Semantic Matching by Perceiving Subtle Conflicts. (arXiv:2302.12530v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12530","description":"<p>Transformer-based pre-trained models have achieved great improvements in\nsemantic matching. However, existing models still suffer from insufficient\nability to capture subtle differences. The modification, addition and deletion\nof words in sentence pairs may make it difficult for the model to predict their\nrelationship. To alleviate this problem, we propose a novel Dual Path Modeling\nFramework to enhance the model's ability to perceive subtle differences in\nsentence pairs by separately modeling affinity and difference semantics. Based\non dual-path modeling framework we design the Dual Path Modeling Network\n(DPM-Net) to recognize semantic relations. And we conduct extensive experiments\non 10 well-studied semantic matching and robustness test datasets, and the\nexperimental results show that our proposed method achieves consistent\nimprovements over baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1\">Chao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Di Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Temporal Reasoning for Evidence-Based Fact-Checking. (arXiv:2302.12569v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12569","description":"<p>Leveraging contextual knowledge has become standard practice in automated\nclaim verification, yet the impact of temporal reasoning has been largely\noverlooked. Our study demonstrates that time positively influences the claim\nverification process of evidence-based fact-checking. The temporal aspects and\nrelations between claims and evidence are first established through grounding\non shared timelines, which are constructed using publication dates and time\nexpressions extracted from their text. Temporal information is then provided to\nRNN-based and Transformer-based classifiers before or after claim and evidence\nencoding. Our time-aware fact-checking models surpass base models by up to 9%\nMicro F1 (64.17%) and 15% Macro F1 (47.43%) on the MultiFC dataset. They also\noutperform prior methods that explicitly model temporal relations between\nevidence. Our findings show that the presence of temporal information and the\nmanner in which timelines are constructed greatly influence how fact-checking\nmodels determine the relevance and supporting or refuting character of evidence\ndocuments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allein_L/0/1/0/all/0/1\">Liesbeth Allein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saelens_M/0/1/0/all/0/1\">Marlon Saelens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cartuyvels_R/0/1/0/all/0/1\">Ruben Cartuyvels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness in Language Models Beyond English: Gaps and Challenges. (arXiv:2302.12578v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12578","description":"<p>With language models becoming increasingly ubiquitous, it has become\nessential to address their inequitable treatment of diverse demographic groups\nand factors. Most research on evaluating and mitigating fairness harms has been\nconcentrated on English, while multilingual models and non-English languages\nhave received comparatively little attention. In this paper, we survey\ndifferent aspects of fairness in languages beyond English and multilingual\ncontexts. This paper presents a survey of fairness in multilingual and\nnon-English contexts, highlighting the shortcomings of current research and the\ndifficulties faced by methods designed for English. We contend that the\nmultitude of diverse cultures and languages across the world makes it\ninfeasible to achieve comprehensive coverage in terms of constructing fairness\ndatasets. Thus, the measurement and mitigation of biases must evolve beyond the\ncurrent dataset-driven practices that are narrowly focused on specific\ndimensions and types of biases and, therefore, impossible to scale across\nlanguages and cultures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1\">Krithika Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VivesDebate-Speech: A Corpus of Spoken Argumentation to Leverage Audio Features for Argument Mining. (arXiv:2302.12584v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12584","description":"<p>In this paper, we describe VivesDebate-Speech, a corpus of spoken\nargumentation created to leverage audio features for argument mining tasks. The\ncreation of this corpus represents an important contribution to the\nintersection of speech processing and argument mining communities, and one of\nthe most complete publicly available resources in this topic. Moreover, we have\nperformed a set of first-of-their-kind experiments which show an improvement\nwhen integrating audio features into the argument mining pipeline. The provided\nresults can be used as a baseline for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Dolz_R/0/1/0/all/0/1\">Ramon Ruiz-Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iranzo_Sanchez_J/0/1/0/all/0/1\">Javier Iranzo-S&#xe1;nchez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CARE: Collaborative AI-Assisted Reading Environment. (arXiv:2302.12611v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12611","description":"<p>Recent years have seen impressive progress in AI-assisted writing, yet the\ndevelopments in AI-assisted reading are lacking. We propose inline commentary\nas a natural vehicle for AI-based reading assistance, and present CARE: the\nfirst open integrated platform for the study of inline commentary and reading.\nCARE facilitates data collection for inline commentaries in a commonplace\ncollaborative reading environment, and provides a framework for enhancing\nreading with NLP-based assistance, such as text classification, generation or\nquestion answering. The extensible behavioral logging allows unique insights\ninto the reading and commenting behavior, and flexible configuration makes the\nplatform easy to deploy in new scenarios. To evaluate CARE in action, we apply\nthe platform in a user study dedicated to scholarly peer review. CARE\nfacilitates the data collection and study of inline commentary in NLP,\nextrinsic evaluation of NLP assistance, and application prototyping. We invite\nthe community to explore and build upon the open source implementation of CARE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zyska_D/0/1/0/all/0/1\">Dennis Zyska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dycke_N/0/1/0/all/0/1\">Nils Dycke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchmann_J/0/1/0/all/0/1\">Jan Buchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1\">Ilia Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TUTORING: Instruction-Grounded Conversational Agent for Language Learners. (arXiv:2302.12623v1 [cs.AI])","link":"http://arxiv.org/abs/2302.12623","description":"<p>In this paper, we propose Tutoring bot, a generative chatbot trained on a\nlarge scale of tutor-student conversations for English-language learning. To\nmimic a human tutor's behavior in language education, the tutor bot leverages\ndiverse educational instructions and grounds to each instruction as additional\ninput context for the tutor response generation. As a single instruction\ngenerally involves multiple dialogue turns to give the student sufficient\nspeaking practice, the tutor bot is required to monitor and capture when the\ncurrent instruction should be kept or switched to the next instruction. For\nthat, the tutor bot is learned to not only generate responses but also infer\nits teaching action and progress on the current conversation simultaneously by\na multi-task learning scheme. Our Tutoring bot is deployed under a\nnon-commercial use license at https://tutoringai.com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chae_H/0/1/0/all/0/1\">Hyungjoo Chae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minjin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chaehyeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_W/0/1/0/all/0/1\">Wonseok Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyejoong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junmyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1\">Jinyoung Yeo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Depth Look at Word Filling Societal Bias Measures. (arXiv:2302.12640v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12640","description":"<p>Many measures of societal bias in language models have been proposed in\nrecent years. A popular approach is to use a set of word filling prompts to\nevaluate the behavior of the language models. In this work, we analyze the\nvalidity of two such measures -- StereoSet and CrowS-Pairs. We show that these\nmeasures produce unexpected and illogical results when appropriate control\ngroup samples are constructed. Based on this, we believe that they are\nproblematic and using them in the future should be reconsidered. We propose a\nway forward with an improved testing protocol. Finally, we also introduce a new\ngender bias dataset for Slovak.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pikuliak_M/0/1/0/all/0/1\">Mat&#xfa;&#x161; Pikuliak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benova_I/0/1/0/all/0/1\">Ivana Be&#x148;ov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachraty_V/0/1/0/all/0/1\">Viktor Bachrat&#xfd;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modelling Temporal Document Sequences for Clinical ICD Coding. (arXiv:2302.12666v1 [cs.LG])","link":"http://arxiv.org/abs/2302.12666","description":"<p>Past studies on the ICD coding problem focus on predicting clinical codes\nprimarily based on the discharge summary. This covers only a small fraction of\nthe notes generated during each hospital stay and leaves potential for\nimproving performance by analysing all the available clinical notes. We propose\na hierarchical transformer architecture that uses text across the entire\nsequence of clinical notes in each hospital stay for ICD coding, and\nincorporates embeddings for text metadata such as their position, time, and\ntype of note. While using all clinical notes increases the quantity of data\nsubstantially, superconvergence can be used to reduce training costs. We\nevaluate the model on the MIMIC-III dataset. Our model exceeds the prior\nstate-of-the-art when using only discharge summaries as input, and achieves\nfurther performance improvements when all clinical notes are used as input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ng_C/0/1/0/all/0/1\">Clarence Boon Liang Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_D/0/1/0/all/0/1\">Diogo Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Transformers and Language Models for Clinical Prediction in Immunotherapy. (arXiv:2302.12692v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12692","description":"<p>Clinical prediction is an essential task in the healthcare industry. However,\nthe recent success of transformers, on which large language models are built,\nhas not been extended to this domain. In this research, we explore the use of\ntransformers and language models in prognostic prediction for immunotherapy\nusing real-world patients' clinical data and molecular profiles. This paper\ninvestigates the potential of transformers to improve clinical prediction\ncompared to conventional machine learning approaches and addresses the\nchallenge of few-shot learning in predicting rare disease areas. The study\nbenchmarks the efficacy of baselines and language models on prognostic\nprediction across multiple cancer types and investigates the impact of\ndifferent pretrained language models under few-shot regimes. The results\ndemonstrate significant improvements in accuracy and highlight the potential of\nNLP in clinical research to improve early detection and intervention for\ndifferent diseases. Anonymous codes are available at\n\\url{https://anonymous.4open.science/r/table2text-88ED}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zekai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balan_M/0/1/0/all/0/1\">Mariann Micsinai Balan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_K/0/1/0/all/0/1\">Kevin Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Transfer of Cognitive Processing Complexity. (arXiv:2302.12695v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12695","description":"<p>When humans read a text, their eye movements are influenced by the structural\ncomplexity of the input sentences. This cognitive phenomenon holds across\nlanguages and recent studies indicate that multilingual language models utilize\nstructural similarities between languages to facilitate cross-lingual transfer.\nWe use sentence-level eye-tracking patterns as a cognitive indicator for\nstructural complexity and show that the multilingual model XLM-RoBERTa can\nsuccessfully predict varied patterns for 13 typologically diverse languages,\ndespite being fine-tuned only on English data. We quantify the sensitivity of\nthe model to structural complexity and distinguish a range of complexity\ncharacteristics. Our results indicate that the model develops a meaningful bias\ntowards sentence length but also integrates cross-lingual differences. We\nconduct a control experiment with randomized word order and find that the model\nseems to additionally capture more complex structural information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pouw_C/0/1/0/all/0/1\">Charlotte Pouw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollenstein_N/0/1/0/all/0/1\">Nora Hollenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beinborn_L/0/1/0/all/0/1\">Lisa Beinborn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spanish Built Factual Freectianary (Spanish-BFF): the first IA-generated free dictionary. (arXiv:2302.12746v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12746","description":"<p>Dictionaries are one of the oldest and most used linguistic resources.\nBuilding them is a complex task that, to the best of our knowledge, has yet to\nbe explored with generative Large Language Models (LLMs). We introduce the\n\"Spanish Built Factual Freectianary\" (Spanish-BFF) as the first Spanish\nIA-generated dictionary. This first-of-its-kind free dictionary uses GPT-3. We\nalso define future steps we aim to follow to improve this initial commitment to\nthe field, such as more additional languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sierra_O/0/1/0/all/0/1\">&#xd3;scar Garc&#xed;a Sierra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Martin_M/0/1/0/all/0/1\">Miguel Ortega-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardoiz_A/0/1/0/all/0/1\">Alfonso Ardoiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armenteros_J/0/1/0/all/0/1\">Juan Carlos Armenteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jorge &#xc1;lvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_A/0/1/0/all/0/1\">Adri&#xe1;n Alonso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble knowledge distillation of self-supervised speech models. (arXiv:2302.12757v1 [eess.AS])","link":"http://arxiv.org/abs/2302.12757","description":"<p>Distilled self-supervised models have shown competitive performance and\nefficiency in recent years. However, there is a lack of experience in jointly\ndistilling multiple self-supervised speech models. In our work, we performed\nEnsemble Knowledge Distillation (EKD) on various self-supervised speech models\nsuch as HuBERT, RobustHuBERT, and WavLM. We tried two different aggregation\ntechniques, layerwise-average and layerwise-concatenation, to the\nrepresentations of different teacher models and found that the former was more\neffective. On top of that, we proposed a multiple prediction head method for\nstudent models to predict different layer outputs of multiple teacher models\nsimultaneously. The experimental results show that our method improves the\nperformance of the distilled models on four downstream speech processing tasks,\nPhoneme Recognition, Speaker Identification, Emotion Recognition, and Automatic\nSpeech Recognition in the hidden-set track of the SUPERB benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Po Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_T/0/1/0/all/0/1\">Tzu-hsun Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yu-Kuan Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_T/0/1/0/all/0/1\">Tsu-Yuan Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yen_P/0/1/0/all/0/1\">Po-Chieh Yen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tseng_W/0/1/0/all/0/1\">Wei-Cheng Tseng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Driven Representation Learning for Robotics. (arXiv:2302.12766v1 [cs.RO])","link":"http://arxiv.org/abs/2302.12766","description":"<p>Recent work in visual representation learning for robotics demonstrates the\nviability of learning from large video datasets of humans performing everyday\ntasks. Leveraging methods such as masked autoencoding and contrastive learning,\nthese representations exhibit strong transfer to policy learning for visuomotor\ncontrol. But, robot learning encompasses a diverse set of problems beyond\ncontrol including grasp affordance prediction, language-conditioned imitation\nlearning, and intent scoring for human-robot collaboration, amongst others.\nFirst, we demonstrate that existing representations yield inconsistent results\nacross these tasks: masked autoencoding approaches pick up on low-level spatial\nfeatures at the cost of high-level semantics, while contrastive learning\napproaches capture the opposite. We then introduce Voltron, a framework for\nlanguage-driven representation learning from human videos and associated\ncaptions. Voltron trades off language-conditioned visual reconstruction to\nlearn low-level visual patterns, and visually-grounded language generation to\nencode high-level semantics. We also construct a new evaluation suite spanning\nfive distinct robot learning problems $\\unicode{x2013}$ a unified platform for\nholistically evaluating visual representations for robotics. Through\ncomprehensive, controlled experiments across all five problems, we find that\nVoltron's language-driven representations outperform the prior\nstate-of-the-art, especially on targeted problems requiring higher-level\nfeatures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karamcheti_S/0/1/0/all/0/1\">Siddharth Karamcheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Suraj Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Annie S. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollar_T/0/1/0/all/0/1\">Thomas Kollar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1\">Dorsa Sadigh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STA: Self-controlled Text Augmentation for Improving Text Classifications. (arXiv:2302.12784v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12784","description":"<p>Despite recent advancements in Machine Learning, many tasks still involve\nworking in low-data regimes which can make solving natural language problems\ndifficult. Recently, a number of text augmentation techniques have emerged in\nthe field of Natural Language Processing (NLP) which can enrich the training\ndata with new examples, though they are not without their caveats. For\ninstance, simple rule-based heuristic methods are effective, but lack variation\nin semantic content and syntactic structure with respect to the original text.\nOn the other hand, more complex deep learning approaches can cause extreme\nshifts in the intrinsic meaning of the text and introduce unwanted noise into\nthe training data. To more reliably control the quality of the augmented\nexamples, we introduce a state-of-the-art approach for Self-Controlled Text\nAugmentation (STA). Our approach tightly controls the generation process by\nintroducing a self-checking procedure to ensure that generated examples retain\nthe semantic content of the original text. Experimental results on multiple\nbenchmarking datasets demonstrate that STA substantially outperforms existing\nstate-of-the-art techniques, whilst qualitative analysis reveals that the\ngenerated examples are both lexically diverse and semantically reliable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Congcong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pontiveros_G/0/1/0/all/0/1\">Gonzalo Fiz Pontiveros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derby_S/0/1/0/all/0/1\">Steven Derby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_T/0/1/0/all/0/1\">Tri Kurniawan Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HULAT at SemEval-2023 Task 9: Data augmentation for pre-trained transformers applied to Multilingual Tweet Intimacy Analysis. (arXiv:2302.12794v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12794","description":"<p>This paper describes our participation in SemEval-2023 Task 9, Intimacy\nAnalysis of Multilingual Tweets. We fine-tune some of the most popular\ntransformer models with the training dataset and synthetic data generated by\ndifferent data augmentation techniques. During the development phase, our best\nresults were obtained by using XLM-T. Data augmentation techniques provide a\nvery slight improvement in the results. Our system ranked in the 27th position\nout of the 45 participating systems. Despite its modest results, our system\nshows promising results in languages such as Portuguese, English, and Dutch.\nAll our code is available in the repository\n\\url{https://github.com/isegura/hulat_intimacy}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Segura_Bedmar_I/0/1/0/all/0/1\">Isabel Segura-Bedmar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. (arXiv:2302.12813v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12813","description":"<p>Large language models (LLMs), such as ChatGPT, are able to generate\nhuman-like, fluent responses for many downstream tasks, e.g., task-oriented\ndialog and question answering. However, applying LLMs to real-world,\nmission-critical applications remains challenging mainly due to their tendency\nto generate hallucinations and inability to use external knowledge.This paper\nproposes a LLM-Augmenter system, which augments a black-box LLM with a set of\nplug-and-play modules. Our system makes the LLM generate responses grounded in\nconsolidated external knowledge, e.g., stored in task-specific databases. It\nalso iteratively revises LLM prompts to improve model responses using feedback\ngenerated by utility functions, e.g., the factuality score of a LLM-generated\nresponse. The effectiveness of LLM-Augmenter is empirically validated on two\ntypes of mission-critical scenarios, task-oriented dialog and open-domain\nquestion answering. LLM-Augmenter significantly reduces ChatGPT's\nhallucinations without sacrificing the fluency and informativeness of its\nresponses. We make the source code and models publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiuyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liden_L/0/1/0/all/0/1\">Lars Liden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data. (arXiv:2302.12822v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12822","description":"<p>Chain-of-thought prompting (CoT) advances the reasoning abilities of large\nlanguage models (LLMs) and achieves superior performance in arithmetic,\ncommonsense, and symbolic reasoning tasks. However, most CoT studies rely on\ncarefully designed human-annotated rational chains to prompt the language\nmodel, which poses challenges for real-world applications where labeled\ntraining data is available without human-annotated rational chains. This\ncreates barriers to applications of CoT prompting to these general tasks. This\npaper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and\nSelection with Chain-of-Thought), that can bypass human engineering of CoTs by\nautomatically augmenting rational chains from a small labeled dataset, and then\npruning low-quality chains to construct a candidate pool of machine-generated\nrationale chains based on the labels. Finally, it selects the optimal\ncombination of several rationale chains from the pool for CoT prompting by\nemploying a variance-reduced policy gradient strategy to estimate the\nsignificance of each example in a black-box language model. Automate-CoT\nenables a quick adaptation of the CoT technique to different tasks.\nExperimental results demonstrate the effectiveness of our method, where\nstate-of-the-art results are achieved on arithmetic reasoning (+2.7\\%),\ncommonsense reasoning (+3.4\\%), symbolic reasoning (+3.2\\%), and non-reasoning\ntasks (+2.5\\%). Our code will be available at\nhttps://github.com/shizhediao/automate-cot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shum_K/0/1/0/all/0/1\">KaShun Shum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1\">Shizhe Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Massively Multilingual ASR With Auxiliary CTC Objectives. (arXiv:2302.12829v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12829","description":"<p>Multilingual Automatic Speech Recognition (ASR) models have extended the\nusability of speech technologies to a wide variety of languages. With how many\nlanguages these models have to handle, however, a key to understanding their\nimbalanced performance across different languages is to examine if the model\nactually knows which language it should transcribe. In this paper, we introduce\nour work on improving performance on FLEURS, a 102-language open ASR benchmark,\nby conditioning the entire model on language identity (LID). We investigate\ntechniques inspired from recent Connectionist Temporal Classification (CTC)\nstudies to help the model handle the large number of languages, conditioning on\nthe LID predictions of auxiliary tasks. Our experimental results demonstrate\nthe effectiveness of our technique over standard CTC/Attention-based hybrid\nmod- els. Furthermore, our state-of-the-art systems using self-supervised\nmodels with the Conformer architecture improve over the results of prior work\non FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are\navailable at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">William Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiti_S/0/1/0/all/0/1\">Soumi Maiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Graphical Conventions in a Visual Communication Game. (arXiv:2111.14210v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.14210","description":"<p>Humans communicate with graphical sketches apart from symbolic languages.\nPrimarily focusing on the latter, recent studies of emergent communication\noverlook the sketches; they do not account for the evolution process through\nwhich symbolic sign systems emerge in the trade-off between iconicity and\nsymbolicity. In this work, we take the very first step to model and simulate\nthis process via two neural agents playing a visual communication game; the\nsender communicates with the receiver by sketching on a canvas. We devise a\nnovel reinforcement learning method such that agents are evolved jointly\ntowards successful communication and abstract graphical conventions. To inspect\nthe emerged conventions, we define three fundamental properties -- iconicity,\nsymbolicity, and semanticity -- and design evaluation methods accordingly. Our\nexperimental results under different controls are consistent with the\nobservation in studies of human graphical conventions. Of note, we find that\nevolved sketches can preserve the continuum of semantics under proper\nenvironmental pressures. More interestingly, co-evolved agents can switch\nbetween conventionalized and iconic communication based on their familiarity\nwith referents. We hope the present research can pave the path for studying\nemergent communication with the modality of sketches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shuwen Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sirui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lifeng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Dynamic Neural Networks for Natural Language Processing. (arXiv:2202.07101v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07101","description":"<p>Effectively scaling large Transformer models is a main driver of recent\nadvances in natural language processing. Dynamic neural networks, as an\nemerging research direction, are capable of scaling up neural networks with\nsub-linear increases in computation and time by dynamically adjusting their\ncomputational path based on the input. Dynamic neural networks could be a\npromising solution to the growing parameter numbers of pretrained language\nmodels, allowing both model pretraining with trillions of parameters and faster\ninference on mobile devices. In this survey, we summarize progress of three\ntypes of dynamic neural networks in NLP: skimming, mixture of experts, and\nearly exit. We also highlight current challenges in dynamic neural networks and\ndirections for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DHGE: Dual-view Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing. (arXiv:2207.08562v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2207.08562","description":"<p>In the field of representation learning on knowledge graphs (KGs), a\nhyper-relational fact consists of a main triple and several auxiliary\nattribute-value descriptions, which is considered more comprehensive and\nspecific than a triple-based fact. However, currently available\nhyper-relational KG embedding methods in a single view are limited in\napplication because they weaken the hierarchical structure that represents the\naffiliation between entities. To overcome this limitation, we propose a\ndual-view hyper-relational KG structure (DH-KG) that contains a\nhyper-relational instance view for entities and a hyper-relational ontology\nview for concepts that are abstracted hierarchically from the entities. This\npaper defines link prediction and entity typing tasks on DH-KG for the first\ntime and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and\nHTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding\nmodel based on GRAN encoders, HGNNs, and joint learning. DHGE outperforms\nbaseline models on DH-KG, according to experimental results. Finally, we\nprovide an example of how this technology can be used to treat hypertension.\nOur model and new datasets are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haoran Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+E_H/0/1/0/all/0/1\">Haihong E</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Ling Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Gengxian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Tianyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_K/0/1/0/all/0/1\">Kaiyang Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing through Forgetting -- Domain Generalization for Symptom Event Extraction in Clinical Notes. (arXiv:2209.09485v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.09485","description":"<p>Symptom information is primarily documented in free-text clinical notes and\nis not directly accessible for downstream applications. To address this\nchallenge, information extraction approaches that can handle clinical language\nvariation across different institutions and specialties are needed. In this\npaper, we present domain generalization for symptom extraction using\npretraining and fine-tuning data that differs from the target domain in terms\nof institution and/or specialty and patient population. We extract symptom\nevents using a transformer-based joint entity and relation extraction method.\nTo reduce reliance on domain-specific features, we propose a domain\ngeneralization method that dynamically masks frequent symptoms words in the\nsource domain. Additionally, we pretrain the transformer language model (LM) on\ntask-related unlabeled texts for better representation. Our experiments\nindicate that masking and adaptive pretraining methods can significantly\nimprove performance when the source domain is more distant from the target\ndomain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sitong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lybarger_K/0/1/0/all/0/1\">Kevin Lybarger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Temporal Article Grounding. (arXiv:2210.12444v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.12444","description":"<p>Given a long untrimmed video and natural language queries, video grounding\n(VG) aims to temporally localize the semantically-aligned video segments.\nAlmost all existing VG work holds two simple but unrealistic assumptions: 1)\nAll query sentences can be grounded in the corresponding video. 2) All query\nsentences for the same video are always at the same semantic scale.\nUnfortunately, both assumptions make today's VG models fail to work in\npractice. For example, in real-world multimodal assets (eg, news articles),\nmost of the sentences in the article can not be grounded in their affiliated\nvideos, and they typically have rich hierarchical relations (ie, at different\nsemantic scales). To this end, we propose a new challenging grounding task:\nWeakly-Supervised temporal Article Grounding (WSAG). Specifically, given an\narticle and a relevant video, WSAG aims to localize all ``groundable''\nsentences to the video, and these sentences are possibly at different semantic\nscales. Accordingly, we collect the first WSAG dataset to facilitate this task:\nYouwikiHow, which borrows the inherent multi-scale descriptions in wikiHow\narticles and plentiful YouTube videos. In addition, we propose a simple but\neffective method DualMIL for WSAG, which consists of a two-level MIL loss and a\nsingle-/cross- sentence constraint loss. These training objectives are\ncarefully designed for these relaxed assumptions. Extensive ablations have\nverified the effectiveness of DualMIL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yulei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1\">Christopher Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayyubi_H/0/1/0/all/0/1\">Hammad Ayyubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SentBS: Sentence-level Beam Search for Controllable Summarization. (arXiv:2210.14502v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14502","description":"<p>A wide range of control perspectives have been explored in controllable text\ngeneration. Structure-controlled summarization is recently proposed as a useful\nand interesting research direction. However, current structure-controlling\nmethods have limited effectiveness in enforcing the desired structure. To\naddress this limitation, we propose a sentence-level beam search generation\nmethod (SentBS), where evaluation is conducted throughout the generation\nprocess to select suitable sentences for subsequent generations. We experiment\nwith different combinations of decoding methods to be used as subcomponents by\nSentBS and evaluate results on the structure-controlled dataset MReD.\nExperiments show that all explored combinations for SentBS can improve the\nagreement between the generated text and the desired structure, with the best\nmethod significantly reducing the structural discrepancies suffered by the\nexisting model, by approximately 68%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenhui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models. (arXiv:2210.16433v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16433","description":"<p>Fully-parametric language models generally require a huge number of model\nparameters to store the necessary knowledge for solving multiple natural\nlanguage tasks in zero/few-shot settings. In addition, it is hard to adapt to\nthe evolving world knowledge without the costly model re-training. In this\npaper, we develop a novel semi-parametric language model architecture,\nKnowledge-in-Context (KiC), which empowers a parametric text-to-text language\nmodel with a knowledge-rich external memory. Specifically, the external memory\ncontains six different types of knowledge: entity, dictionary, commonsense,\nevent, script, and causality knowledge. For each input instance, the KiC model\nadaptively selects a knowledge type and retrieves the most helpful pieces of\nknowledge. The input instance along with its knowledge augmentation is fed into\na text-to-text model (e.g., T5) to generate the output answer, where both the\ninput and the output are in natural language forms after prompting.\nInterestingly, we find that KiC can be identified as a special\nmixture-of-experts (MoE) model, where the knowledge selector plays the role of\na router that is used to determine the sequence-to-expert assignment in MoE.\nThis key observation inspires us to develop a novel algorithm for training KiC\nwith an instance-adaptive knowledge selector. As a knowledge-rich\nsemi-parametric language model, KiC only needs a much smaller parametric part\nto achieve superior zero-shot performance on unseen tasks. By evaluating on 40+\ndifferent tasks, we show that KiC_Large with 770M parameters easily outperforms\nlarge language models (LMs) that are 4-39x larger by a large margin. We also\ndemonstrate that KiC exhibits emergent abilities at a much smaller model scale\ncompared to the fully-parametric models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiaoman Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing and Adversarial: Improve ASR with Speaker Labels. (arXiv:2211.06369v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2211.06369","description":"<p>ASR can be improved by multi-task learning (MTL) with domain enhancing or\ndomain adversarial training, which are two opposite objectives with the aim to\nincrease/decrease domain variance towards domain-aware/agnostic ASR,\nrespectively. In this work, we study how to best apply these two opposite\nobjectives with speaker labels to improve conformer-based ASR. We also propose\na novel adaptive gradient reversal layer for stable and effective adversarial\ntraining without tuning effort. Detailed analysis and experimental verification\nare conducted to show the optimal positions in the ASR neural network (NN) to\napply speaker enhancing and adversarial training. We also explore their\ncombination for further improvement, achieving the same performance as\ni-vectors plus adversarial training. Our best speaker-based MTL achieves 7\\%\nrelative improvement on the Switchboard Hub5'00 set. We also investigate the\neffect of such speaker-based MTL w.r.t. cleaner dataset and weaker ASR NN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Haotian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luscher_C/0/1/0/all/0/1\">Christoph L&#xfc;scher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of a Thermodynamics of Human Cognition and Human Culture. (arXiv:2212.12795v2 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2212.12795","description":"<p>Inspired by foundational studies in classical and quantum physics, and by\ninformation retrieval studies in quantum information theory, we prove that the\nnotions of 'energy' and 'entropy' can be consistently introduced in human\nlanguage and, more generally, in human culture. More explicitly, if energy is\nattributed to words according to their frequency of appearance in a text, then\nthe ensuing energy levels are distributed non-classically, namely, they obey\nBose-Einstein, rather than Maxwell-Boltzmann, statistics, as a consequence of\nthe genuinely 'quantum indistinguishability' of the words that appear in the\ntext. Secondly, the 'quantum entanglement' due to the way meaning is carried by\na text reduces the (von Neumann) entropy of the words that appear in the text,\na behaviour which cannot be explained within classical (thermodynamic or\ninformation) entropy. We claim here that this 'quantum-type behaviour is valid\nin general in human language', namely, any text is conceptually more concrete\nthan the words composing it, which entails that the entropy of the overall text\ndecreases. In addition, we provide examples taken from cognition, where\nquantization of energy appears in categorical perception, and from culture,\nwhere entities collaborate, thus 'entangle', to decrease overall entropy. We\nuse these findings to propose the development of a new 'non-classical\nthermodynamic theory' for human cognition, which also covers broad parts of\nhuman culture and its artefacts and bridges concepts with quantum physics\nentities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Aerts_D/0/1/0/all/0/1\">Diederik Aerts</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Arguelles_J/0/1/0/all/0/1\">Jonito Aerts Argu&#xeb;lles</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Beltran_L/0/1/0/all/0/1\">Lester Beltran</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sozzo_S/0/1/0/all/0/1\">Sandro Sozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge Memorization. (arXiv:2302.04415v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.04415","description":"<p>Pre-trained language models (PLM) have achieved remarkable advancement in\ntable-to-text generation tasks. However, the lack of labeled domain-specific\nknowledge and the topology gap between tabular data and text make it difficult\nfor PLMs to yield faithful text. Low-resource generation likewise faces unique\nchallenges in this domain. Inspired by how humans descript tabular data with\nprior knowledge, we suggest a new framework: PromptMize, which targets\ntable-to-text generation under few-shot settings. The design of our framework\nconsists of two aspects: a prompt planner and a knowledge adapter. The prompt\nplanner aims to generate a prompt signal that provides instance guidance for\nPLMs to bridge the topology gap between tabular data and text. Moreover, the\nknowledge adapter memorizes domain-specific knowledge from the unlabelled\ncorpus to supply essential information during generation. Extensive experiments\nand analyses are investigated on three open domain few-shot NLG datasets:\nhuman, song, and book. Compared with previous state-of-the-art approaches, our\nmodel achieves remarkable performance in generating quality as judged by human\nand automatic evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhixin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Minyxuan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiexing Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Ziwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guanjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Song of Ice and Fire: Analyzing Textual Autotelic Agents in ScienceWorld. (arXiv:2302.05244v5 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.05244","description":"<p>Building open-ended agents that can autonomously discover a diversity of\nbehaviours is one of the long-standing goals of artificial intelligence. This\nchallenge can be studied in the framework of autotelic RL agents, i.e. agents\nthat learn by selecting and pursuing their own goals, self-organizing a\nlearning curriculum. Recent work identified language as a key dimension of\nautotelic learning, in particular because it enables abstract goal sampling and\nguidance from social peers for hindsight relabelling. Within this perspective,\nwe study the following open scientific questions: What is the impact of\nhindsight feedback from a social peer (e.g. selective vs. exhaustive)? How can\nthe agent learn from very rare language goal examples in its experience replay?\nHow can multiple forms of exploration be combined, and take advantage of easier\ngoals as stepping stones to reach harder ones? To address these questions, we\nuse ScienceWorld, a textual environment with rich abstract and combinatorial\nphysics. We show the importance of selectivity from the social peer's feedback;\nthat experience replay needs to over-sample examples of rare goals; and that\nfollowing self-generated goal sequences where the agent's competence is\nintermediate leads to significant improvements in final performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_L/0/1/0/all/0/1\">Laetitia Teodorescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficiency 360: Efficient Vision Transformers. (arXiv:2302.08374v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2302.08374","description":"<p>Transformers are widely used for solving tasks in natural language\nprocessing, computer vision, speech, and music domains. In this paper, we talk\nabout the efficiency of transformers in terms of memory (the number of\nparameters), computation cost (number of floating points operations), and\nperformance of models, including accuracy, the robustness of the model, and\nfair \\&amp; bias-free features. We mainly discuss the vision transformer for the\nimage classification task. Our contribution is to introduce an efficient 360\nframework, which includes various aspects of the vision transformer, to make it\nmore efficient for industrial applications. By considering those applications,\nwe categorize them into multiple dimensions such as privacy, robustness,\ntransparency, fairness, inclusiveness, continual learning, probabilistic\nmodels, approximation, computational complexity, and spectral complexity. We\ncompare various vision transformer models based on their performance, the\nnumber of parameters, and the number of floating point operations (FLOPs) on\nmultiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patro_B/0/1/0/all/0/1\">Badri N. Patro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agneeswaran_V/0/1/0/all/0/1\">Vijay Srinivas Agneeswaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities. (arXiv:2302.11154v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2302.11154","description":"<p>Large-scale multi-modal pre-training models such as CLIP and PaLI exhibit\nstrong generalization on various visual domains and tasks. However, existing\nimage classification benchmarks often evaluate recognition on a specific domain\n(e.g., outdoor images) or a specific task (e.g., classifying plant species),\nwhich falls short of evaluating whether pre-trained foundational models are\nuniversal visual recognizers. To address this, we formally present the task of\nOpen-domain Visual Entity recognitioN (OVEN), where a model need to link an\nimage onto a Wikipedia entity with respect to a text query. We construct\nOVEN-Wiki by re-purposing 14 existing datasets with all labels grounded onto\none single label space: Wikipedia entities. OVEN challenges models to select\namong six million possible Wikipedia entities, making it a general visual\nrecognition benchmark with the largest number of labels. Our study on\nstate-of-the-art pre-trained models reveals large headroom in generalizing to\nthe massive-scale label space. We show that a PaLI-based auto-regressive visual\nrecognition model performs surprisingly well, even on Wikipedia entities that\nhave never been seen during fine-tuning. We also find existing pretrained\nmodels yield different strengths: while PaLI-based models obtain higher overall\nperformance, CLIP-based models are better at recognizing tail entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1\">Yi Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_U/0/1/0/all/0/1\">Urvashi Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLSP2022 EVJVQA Challenge: Multilingual Visual Question Answering. (arXiv:2302.11752v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.11752","description":"<p>Visual Question Answering (VQA) is a challenging task of natural language\nprocessing (NLP) and computer vision (CV), attracting significant attention\nfrom researchers. English is a resource-rich language that has witnessed\nvarious developments in datasets and models for visual question answering.\nVisual question answering in other languages also would be developed for\nresources and models. In addition, there is no multilingual dataset targeting\nthe visual content of a particular country with its own objects and cultural\ncharacteristics. To address the weakness, we provide the research community\nwith a benchmark dataset named EVJVQA, including 33,000+ pairs of\nquestion-answer over three languages: Vietnamese, English, and Japanese, on\napproximately 5,000 images taken from Vietnam for evaluating multilingual VQA\nsystems or models. EVJVQA is used as a benchmark dataset for the challenge of\nmultilingual visual question answering at the 9th Workshop on Vietnamese\nLanguage and Speech Processing (VLSP 2022). This task attracted 62 participant\nteams from various universities and organizations. In this article, we present\ndetails of the organization of the challenge, an overview of the methods\nemployed by shared-task participants, and the results. The highest performances\nare 0.4392 in F1-score and 0.4009 in BLUE on the private test set. The\nmultilingual QA systems proposed by the top 2 teams use ViT for the pre-trained\nvision model and mT5 for the pre-trained language model, a powerful pre-trained\nlanguage model based on the transformer architecture. EVJVQA is a challenging\ndataset that motivates NLP and CV researchers to further explore the\nmultilingual models or systems for visual question answering systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nghia Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Duong T.D Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khanh Quoc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProsAudit, a prosodic benchmark for self-supervised speech models. (arXiv:2302.12057v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12057","description":"<p>We present ProsAudit, a benchmark in English to assess structural prosodic\nknowledge in self-supervised learning (SSL) speech models. It consists of two\nsubtasks, their corresponding metrics, an evaluation dataset. In the\nprotosyntax task, the model must correctly identify strong versus weak prosodic\nboundaries. In the lexical task, the model needs to correctly distinguish\nbetween pauses inserted between words and within words. We also provide human\nevaluation scores on this benchmark. We evaluated a series of SSL models and\nfound that they were all able to perform above chance on both tasks, even when\ntrained on an unseen language. However, non-native models performed\nsignificantly worse than native ones on the lexical task, highlighting the\nimportance of lexical knowledge in this task. We also found a clear effect of\nsize with models trained on more data performing better in the two subtasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seyssel_M/0/1/0/all/0/1\">Maureen de Seyssel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavechin_M/0/1/0/all/0/1\">Marvin Lavechin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titeux_H/0/1/0/all/0/1\">Hadrien Titeux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1\">Arthur Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virlet_G/0/1/0/all/0/1\">Gwendal Virlet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revilla_A/0/1/0/all/0/1\">Andrea Santos Revilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisniewski_G/0/1/0/all/0/1\">Guillaume Wisniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludusan_B/0/1/0/all/0/1\">Bogdan Ludusan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook. (arXiv:2210.13623v2 [cs.AI] CROSS LISTED)","link":"http://arxiv.org/abs/2210.13623","description":"<p>In recent years, reinforcement learning and bandits have transformed a wide\nrange of real-world applications including healthcare, finance, recommendation\nsystems, robotics, and last but not least, the speech and natural language\nprocessing. While most speech and language applications of reinforcement\nlearning algorithms are centered around improving the training of deep neural\nnetworks with its flexible optimization properties, there are still many\ngrounds to explore to utilize the benefits of reinforcement learning, such as\nits reward-driven adaptability, state representations, temporal structures and\ngeneralizability. In this survey, we present an overview of recent advancements\nof reinforcement learning and bandits, and discuss how they can be effectively\nemployed to solve speech and natural language processing problems with models\nthat are adaptive, interactive and scalable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}