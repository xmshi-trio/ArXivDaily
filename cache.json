{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-29T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Linguistically Informed ChatGPT Prompts to Enhance Japanese-Chinese Machine Translation: A Case Study on Attributive Clauses. (arXiv:2303.15587v1 [cs.CL])","link":"http://arxiv.org/abs/2303.15587","description":"<p>In the field of Japanese-Chinese translation linguistics, the issue of\ncorrectly translating attributive clauses has persistently proven to be\nchallenging. Present-day machine translation tools often fail to accurately\ntranslate attributive clauses from Japanese to Chinese. In light of this, this\npaper investigates the linguistic problem underlying such difficulties, namely\nhow does the semantic role of the modified noun affect the selection of\ntranslation patterns for attributive clauses, from a linguistic perspective. To\nad-dress these difficulties, a pre-edit scheme is proposed, which aims to\nenhance the accuracy of translation. Furthermore, we propose a novel two-step\nprompt strategy, which combines this pre-edit scheme with ChatGPT, currently\nthe most widely used large language model. This prompt strategy is capable of\noptimizing translation input in zero-shot scenarios and has been demonstrated\nto improve the average translation accuracy score by over 35%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Wenshi Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Typhoon: Towards an Effective Task-Specific Masking Strategy for Pre-trained Language Models. (arXiv:2303.15619v1 [cs.CL])","link":"http://arxiv.org/abs/2303.15619","description":"<p>Through exploiting a high level of parallelism enabled by graphics processing\nunits, transformer architectures have enabled tremendous strides forward in the\nfield of natural language processing. In a traditional masked language model,\nspecial MASK tokens are used to prompt our model to gather contextual\ninformation from surrounding words to restore originally hidden information. In\nthis paper, we explore a task-specific masking framework for pre-trained large\nlanguage models that enables superior performance on particular downstream\ntasks on the datasets in the GLUE benchmark. We develop our own masking\nalgorithm, Typhoon, based on token input gradients, and compare this with other\nstandard baselines. We find that Typhoon offers performance competitive with\nwhole-word masking on the MRPC dataset. Our implementation can be found in a\npublic Github Repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdurrahman_M/0/1/0/all/0/1\">Muhammed Shahir Abdurrahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elezabi_H/0/1/0/all/0/1\">Hashem Elezabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bruce Changlong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT as a Factual Inconsistency Evaluator for Abstractive Text Summarization. (arXiv:2303.15621v1 [cs.CL])","link":"http://arxiv.org/abs/2303.15621","description":"<p>The performance of abstractive text summarization has been greatly boosted by\npre-trained language models recently. The main concern of existing abstractive\nsummarization methods is the factual inconsistency problem of their generated\nsummary. To alleviate the problem, many efforts have focused on developing\neffective factuality evaluation metrics based on natural language inference and\nquestion answering et al. However, they have limitations of high computational\ncomplexity and relying on annotated data. Most recently, large language models\nsuch as ChatGPT have shown strong ability in not only natural language\nunderstanding but also natural language inference. In this paper, we study the\nfactual inconsistency evaluation ability of ChatGPT under the zero-shot setting\nby evaluating it on the coarse-grained and fine-grained factuality evaluation\ntasks including binary natural language inference (NLI), summary ranking, and\nconsistency rating. Experimental results show that ChatGPT outperforms previous\nSOTA evaluation metrics on 6/9 datasets across three tasks, demonstrating its\ngreat potential for assessing factual inconsistency in the zero-shot setting.\nThe results also highlight the importance of prompt design and the need for\nfuture efforts to address ChatGPT's limitations on evaluation bias, wrong\nreasoning, and hallucination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zheheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning. (arXiv:2303.15647v1 [cs.CL])","link":"http://arxiv.org/abs/2303.15647","description":"<p>This paper presents a systematic overview and comparison of\nparameter-efficient fine-tuning methods covering over 40 papers published\nbetween February 2019 and February 2023. These methods aim to resolve the\ninfeasibility and impracticality of fine-tuning large language models by only\ntraining a small set of parameters. We provide a taxonomy that covers a broad\nrange of methods and present a detailed method comparison with a specific focus\non real-life efficiency and fine-tuning multibillion-scale language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1\">Vladislav Lialin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_V/0/1/0/all/0/1\">Vijeta Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint embedding in Hierarchical distance and semantic representation learning for link prediction. (arXiv:2303.15655v1 [cs.CL])","link":"http://arxiv.org/abs/2303.15655","description":"<p>The link prediction task aims to predict missing entities or relations in the\nknowledge graph and is essential for the downstream application. Existing\nwell-known models deal with this task by mainly focusing on representing\nknowledge graph triplets in the distance space or semantic space. However, they\ncan not fully capture the information of head and tail entities, nor even make\ngood use of hierarchical level information. Thus, in this paper, we propose a\nnovel knowledge graph embedding model for the link prediction task, namely,\nHIE, which models each triplet (\\textit{h}, \\textit{r}, \\textit{t}) into\ndistance measurement space and semantic measurement space, simultaneously.\nMoreover, HIE is introduced into hierarchical-aware space to leverage rich\nhierarchical information of entities and relations for better representation\nlearning. Specifically, we apply distance transformation operation on the head\nentity in distance space to obtain the tail entity instead of translation-based\nor rotation-based approaches. Experimental results of HIE on four real-world\ndatasets show that HIE outperforms several existing state-of-the-art knowledge\ngraph embedding methods on the link prediction task and deals with complex\nrelations accurately.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianye Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chongfeng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fengyu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT4PCG Competition: Character-like Level Generation for Science Birds. (arXiv:2303.15662v1 [cs.AI])","link":"http://arxiv.org/abs/2303.15662","description":"<p>This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE\nConference on Games. The objective of this competition is for participants to\ncreate effective prompts for ChatGPT--enabling it to generate Science Birds\nlevels with high stability and character-like qualities--fully using their\ncreativity as well as prompt engineering skills. ChatGPT is a conversational\nagent developed by OpenAI. Science Birds is selected as the competition\nplatform because designing an Angry Birds-like level is not a trivial task due\nto the in-game gravity; the playability of the levels is determined by their\nstability. To lower the entry barrier to the competition, we limit the task to\nthe generation of capitalized English alphabetical characters. Here, the\nquality of the generated levels is determined by their stability and similarity\nto the given characters. A sample prompt is provided to participants for their\nreference. An experiment is conducted to determine the effectiveness of its\nmodified versions on level stability and similarity by testing them on several\ncharacters. To the best of our knowledge, we believe that ChatGPT4PCG is the\nfirst competition of its kind and hope to inspire enthusiasm for prompt\nengineering in procedural content generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taveekitworachai_P/0/1/0/all/0/1\">Pittawat Taveekitworachai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_F/0/1/0/all/0/1\">Febri Abdullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dewantoro_M/0/1/0/all/0/1\">Mury F. Dewantoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thawonmas_R/0/1/0/all/0/1\">Ruck Thawonmas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1\">Julian Togelius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renz_J/0/1/0/all/0/1\">Jochen Renz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training Transformers for Knowledge Graph Completion. (arXiv:2303.15682v1 [cs.CL])","link":"http://arxiv.org/abs/2303.15682","description":"<p>Learning transferable representation of knowledge graphs (KGs) is challenging\ndue to the heterogeneous, multi-relational nature of graph structures. Inspired\nby Transformer-based pretrained language models' success on learning\ntransferable representation for texts, we introduce a novel inductive KG\nrepresentation model (iHT) for KG completion by large-scale pre-training. iHT\nconsists of a entity encoder (e.g., BERT) and a neighbor-aware relational\nscoring function both parameterized by Transformers. We first pre-train iHT on\na large KG dataset, Wikidata5M. Our approach achieves new state-of-the-art\nresults on matched evaluations, with a relative improvement of more than 25% in\nmean reciprocal rank over previous SOTA models. When further fine-tuned on\nsmaller KGs with either entity and relational shifts, pre-trained iHT\nrepresentations are shown to be transferable, significantly improving the\nperformance on FB15K-237 and WN18RR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model and Evaluation: Towards Fairness in Multilingual Text Classification. (arXiv:2303.15697v1 [cs.CL])","link":"http://arxiv.org/abs/2303.15697","description":"<p>Recently, more and more research has focused on addressing bias in text\nclassification models. However, existing research mainly focuses on the\nfairness of monolingual text classification models, and research on fairness\nfor multilingual text classification is still very limited. In this paper, we\nfocus on the task of multilingual text classification and propose a debiasing\nframework for multilingual text classification based on contrastive learning.\nOur proposed method does not rely on any external language resources and can be\nextended to any other languages. The model contains four modules: multilingual\ntext representation module, language fusion module, text debiasing module, and\ntext classification module. The multilingual text representation module uses a\nmultilingual pre-trained language model to represent the text, the language\nfusion module makes the semantic spaces of different languages tend to be\nconsistent through contrastive learning, and the text debiasing module uses\ncontrastive learning to make the model unable to identify sensitive attributes'\ninformation. The text classification module completes the basic tasks of\nmultilingual text classification. In addition, the existing research on the\nfairness of multilingual text classification is relatively simple in the\nevaluation mode. The evaluation method of fairness is the same as the\nmonolingual equality difference evaluation method, that is, the evaluation is\nperformed on a single language. We propose a multi-dimensional fairness\nevaluation framework for multilingual text classification, which evaluates the\nmodel's monolingual equality difference, multilingual equality difference,\nmultilingual equality performance difference, and destructiveness of the\nfairness strategy. We hope that our work can provide a more general debiasing\nmethod and a more comprehensive evaluation framework for multilingual text\nfairness tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhenghang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aimin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translate the Beauty in Songs: Jointly Learning to Align Melody and Translate Lyrics. (arXiv:2303.15705v1 [cs.CL])","link":"http://arxiv.org/abs/2303.15705","description":"<p>Song translation requires both translation of lyrics and alignment of music\nnotes so that the resulting verse can be sung to the accompanying melody, which\nis a challenging problem that has attracted some interests in different aspects\nof the translation process. In this paper, we propose Lyrics-Melody Translation\nwith Adaptive Grouping (LTAG), a holistic solution to automatic song\ntranslation by jointly modeling lyrics translation and lyrics-melody alignment.\nIt is a novel encoder-decoder framework that can simultaneously translate the\nsource lyrics and determine the number of aligned notes at each decoding step\nthrough an adaptive note grouping module. To address data scarcity, we\ncommissioned a small amount of training data annotated specifically for this\ntask and used large amounts of augmented data through back-translation.\nExperiments conducted on an English-Chinese song translation data set show the\neffectiveness of our model in both automatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1\">Kai Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1\">Jiajun Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias or Diversity? Unraveling Semantic Discrepancy in U.S. News Headlines. (arXiv:2303.15708v1 [cs.CL])","link":"http://arxiv.org/abs/2303.15708","description":"<p>There is a broad consensus that news media outlets incorporate ideological\nbiases in their news articles. However, prior studies on measuring the\ndiscrepancies among media outlets and further dissecting the origins of\nsemantic differences suffer from small sample sizes and limited scope. In this\nstudy, we collect a large dataset of 1.8 million news headlines from major U.S.\nmedia outlets spanning from 2014 to 2022 to thoroughly track and dissect the\nsemantic discrepancy in U.S. news media. We employ multiple correspondence\nanalysis (MCA) to quantify the semantic discrepancy relating to four prominent\ntopics - domestic politics, economic issues, social issues, and foreign\naffairs. Additionally, we compare the most frequent n-grams in media headlines\nto provide further qualitative insights into our analysis. Our findings\nindicate that on domestic politics and social issues, the discrepancy can be\nattributed to a certain degree of media bias. Meanwhile, the discrepancy in\nreporting foreign affairs is largely attributed to the diversity in individual\njournalistic styles. Finally, U.S. media outlets show consistency and high\nsimilarity in their coverage of economic issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinsheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Weihong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zichen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1\">Hanjia Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])","link":"http://arxiv.org/abs/2303.15714","description":"<p>Language models have been shown to perform remarkably well on a wide range of\nnatural language processing tasks. In this paper, we propose a novel system\nthat uses language models to perform multi-step logical reasoning. Our system\nincorporates explicit planning into its inference procedure, thus able to make\nmore informed reasoning decisions at each step by looking ahead into their\nfuture effects. In our experiments, our full system significantly outperforms\nother competing systems. On a multiple-choice question answering task, our\nsystem performs competitively compared to GPT-3-davinci despite having only\naround 1.5B parameters. We conduct several ablation studies to demonstrate that\nexplicit planning plays a crucial role in the system's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kangrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hongyuan Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of ChatGPT for NLP-based Mental Health Applications. (arXiv:2303.15727v1 [cs.CL])","link":"http://arxiv.org/abs/2303.15727","description":"<p>Large language models (LLM) have been successful in several natural language\nunderstanding tasks and could be relevant for natural language processing\n(NLP)-based mental health application research. In this work, we report the\nperformance of LLM-based ChatGPT (with gpt-3.5-turbo backend) in three\ntext-based mental health classification tasks: stress detection (2-class\nclassification), depression detection (2-class classification), and suicidality\ndetection (5-class classification). We obtained annotated social media posts\nfor the three classification tasks from public datasets. Then ChatGPT API\nclassified the social media posts with an input prompt for classification. We\nobtained F1 scores of 0.73, 0.86, and 0.37 for stress detection, depression\ndetection, and suicidality detection, respectively. A baseline model that\nalways predicted the dominant class resulted in F1 scores of 0.35, 0.60, and\n0.19. The zero-shot classification accuracy obtained with ChatGPT indicates a\npotential use of language models for mental health classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lamichhane_B/0/1/0/all/0/1\">Bishal Lamichhane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Soft-prompt tuning to predict lung cancer using primary care free-text Dutch medical notes. (arXiv:2303.15846v1 [cs.CL])","link":"http://arxiv.org/abs/2303.15846","description":"<p>We investigate different natural language processing (NLP) approaches based\non contextualised word representations for the problem of early prediction of\nlung cancer using free-text patient medical notes of Dutch primary care\nphysicians. Because lung cancer has a low prevalence in primary care, we also\naddress the problem of classification under highly imbalanced classes.\nSpecifically, we use large Transformer-based pretrained language models (PLMs)\nand investigate: 1) how \\textit{soft prompt-tuning} -- an NLP technique used to\nadapt PLMs using small amounts of training data -- compares to standard model\nfine-tuning; 2) whether simpler static word embedding models (WEMs) can be more\nrobust compared to PLMs in highly imbalanced settings; and 3) how models fare\nwhen trained on notes from a small number of patients. We find that 1)\nsoft-prompt tuning is an efficient alternative to standard model fine-tuning;\n2) PLMs show better discrimination but worse calibration compared to simpler\nstatic word embedding models as the classification problem becomes more\nimbalanced; and 3) results when training models on small number of patients are\nmixed and show no clear differences between PLMs and WEMs. All our code is\navailable open source in\n\\url{https://bitbucket.org/aumc-kik/prompt_tuning_cancer_prediction/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elfrink_A/0/1/0/all/0/1\">Auke Elfrink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vagliano_I/0/1/0/all/0/1\">Iacopo Vagliano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_Hanna_A/0/1/0/all/0/1\">Ameen Abu-Hanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calixto_I/0/1/0/all/0/1\">Iacer Calixto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Granularity Matching Attention Network for Query Intent Classification in E-commerce Retrieval. (arXiv:2303.15870v1 [cs.IR])","link":"http://arxiv.org/abs/2303.15870","description":"<p>Query intent classification, which aims at assisting customers to find\ndesired products, has become an essential component of the e-commerce search.\nExisting query intent classification models either design more exquisite models\nto enhance the representation learning of queries or explore label-graph and\nmulti-task to facilitate models to learn external information. However, these\nmodels cannot capture multi-granularity matching features from queries and\ncategories, which makes them hard to mitigate the gap in the expression between\ninformal queries and categories.\n</p>\n<p>This paper proposes a Multi-granularity Matching Attention Network (MMAN),\nwhich contains three modules: a self-matching module, a char-level matching\nmodule, and a semantic-level matching module to comprehensively extract\nfeatures from the query and a query-category interaction matrix. In this way,\nthe model can eliminate the difference in expression between queries and\ncategories for query intent classification. We conduct extensive offline and\nonline A/B experiments, and the results show that the MMAN significantly\noutperforms the strong baselines, which shows the superiority and effectiveness\nof MMAN. MMAN has been deployed in production and brings great commercial value\nfor our company.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chunyuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yiming Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haiqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Songlin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Sulong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Neural Topic Models Really Need Dropout? Analysis of the Effect of Dropout in Topic Modeling. (arXiv:2303.15973v1 [cs.CL])","link":"http://arxiv.org/abs/2303.15973","description":"<p>Dropout is a widely used regularization trick to resolve the overfitting\nissue in large feedforward neural networks trained on a small dataset, which\nperforms poorly on the held-out test subset. Although the effectiveness of this\nregularization trick has been extensively studied for convolutional neural\nnetworks, there is a lack of analysis of it for unsupervised models and in\nparticular, VAE-based neural topic models. In this paper, we have analyzed the\nconsequences of dropout in the encoder as well as in the decoder of the VAE\narchitecture in three widely used neural topic models, namely, contextualized\ntopic model (CTM), ProdLDA, and embedded topic model (ETM) using four publicly\navailable datasets. We characterize the dropout effect on these models in terms\nof the quality and predictive performance of the generated topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adhya_S/0/1/0/all/0/1\">Suman Adhya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_A/0/1/0/all/0/1\">Avishek Lahiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_D/0/1/0/all/0/1\">Debarshi Kumar Sanyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Experimental Study on Sentiment Classification of Moroccan dialect texts in the web. (arXiv:2303.15987v1 [cs.CL])","link":"http://arxiv.org/abs/2303.15987","description":"<p>With the rapid growth of the use of social media websites, obtaining the\nusers' feedback automatically became a crucial task to evaluate their\ntendencies and behaviors online. Despite this great availability of\ninformation, and the increasing number of Arabic users only few research has\nmanaged to treat Arabic dialects. The purpose of this paper is to study the\nopinion and emotion expressed in real Moroccan texts precisely in the YouTube\ncomments using some well-known and commonly used methods for sentiment\nanalysis. In this paper, we present our work of Moroccan dialect comments\nclassification using Machine Learning (ML) models and based on our collected\nand manually annotated YouTube Moroccan dialect dataset. By employing many text\npreprocessing and data representation techniques we aim to compare our\nclassification results utilizing the most commonly used supervised classifiers:\nk-nearest neighbors (KNN), Support Vector Machine (SVM), Naive Bayes (NB), and\ndeep learning (DL) classifiers such as Convolutional Neural Network (CNN) and\nLong Short-Term Memory (LTSM). Experiments were performed using both raw and\npreprocessed data to show the importance of the preprocessing. In fact, the\nexperimental results prove that DL models have a better performance for\nMoroccan Dialect than classical approaches and we achieved an accuracy of 90%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jbel_M/0/1/0/all/0/1\">Mouad Jbel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hafidi_I/0/1/0/all/0/1\">Imad Hafidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metrane_A/0/1/0/all/0/1\">Abdulmutallib Metrane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetically generated text for supervised text analysis. (arXiv:2303.16028v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16028","description":"<p>Supervised text models are a valuable tool for political scientists but\npresent several obstacles to their use, including the expense of hand-labeling\ndocuments, the difficulty of retrieving rare relevant documents for annotation,\nand copyright and privacy concerns involved in sharing annotated documents.\nThis article proposes a partial solution to these three issues, in the form of\ncontrolled generation of synthetic text with large language models. I provide a\nconceptual overview of text generation, guidance on when researchers should\nprefer different techniques for generating synthetic text, a discussion of\nethics, and a simple technique for improving the quality of synthetic text. I\ndemonstrate the usefulness of synthetic text with three applications:\ngenerating synthetic tweets describing the fighting in Ukraine, synthetic news\narticles describing specified political events for training an event detection\nsystem, and a multilingual corpus of populist manifesto statements for training\na sentence-level populism classifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Halterman_A/0/1/0/all/0/1\">Andrew Halterman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Carolina: a General Corpus of Contemporary Brazilian Portuguese with Provenance, Typology and Versioning Information. (arXiv:2303.16098v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16098","description":"<p>This paper presents the first publicly available version of the Carolina\nCorpus and discusses its future directions. Carolina is a large open corpus of\nBrazilian Portuguese texts under construction using web-as-corpus methodology\nenhanced with provenance, typology, versioning, and text integrality. The\ncorpus aims at being used both as a reliable source for research in Linguistics\nand as an important resource for Computer Science research on language models,\ncontributing towards removing Portuguese from the set of low-resource\nlanguages. Here we present the construction of the corpus methodology,\ncomparing it with other existing methodologies, as well as the corpus current\nstate: Carolina's first public version has $653,322,577$ tokens, distributed\nover $7$ broad types. Each text is annotated with several different metadata\ncategories in its header, which we developed using TEI annotation standards. We\nalso present ongoing derivative works and invite NLP researchers to contribute\nwith their own.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Crespo_M/0/1/0/all/0/1\">Maria Clara Ramos Morales Crespo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_M/0/1/0/all/0/1\">Maria Lina de Souza Jeannine Rocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sturzeneker_M/0/1/0/all/0/1\">Mariana Louren&#xe7;o Sturzeneker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serras_F/0/1/0/all/0/1\">Felipe Ribas Serras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_G/0/1/0/all/0/1\">Guilherme Lamartine de Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1\">Aline Silva Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palma_M/0/1/0/all/0/1\">Mayara Feliciano Palma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesquita_R/0/1/0/all/0/1\">Renata Morais Mesquita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guets_R/0/1/0/all/0/1\">Raquel de Paula Guets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">Mariana Marques da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finger_M/0/1/0/all/0/1\">Marcelo Finger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sousa_M/0/1/0/all/0/1\">Maria Clara Paix&#xe3;o de Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namiuti_C/0/1/0/all/0/1\">Cristiane Namiuti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monte_V/0/1/0/all/0/1\">Vanessa Martins do Monte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hallucinations in Large Multilingual Translation Models. (arXiv:2303.16104v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16104","description":"<p>Large-scale multilingual machine translation systems have demonstrated\nremarkable ability to translate directly between numerous languages, making\nthem increasingly appealing for real-world applications. However, when deployed\nin the wild, these models may generate hallucinated translations which have the\npotential to severely undermine user trust and raise safety concerns. Existing\nresearch on hallucinations has primarily focused on small bilingual models\ntrained on high-resource languages, leaving a gap in our understanding of\nhallucinations in massively multilingual models across diverse translation\nscenarios. In this work, we fill this gap by conducting a comprehensive\nanalysis on both the M2M family of conventional neural machine translation\nmodels and ChatGPT, a general-purpose large language model~(LLM) that can be\nprompted for translation. Our investigation covers a broad spectrum of\nconditions, spanning over 100 translation directions across various resource\nlevels and going beyond English-centric language pairs. We provide key insights\nregarding the prevalence, properties, and mitigation of hallucinations, paving\nthe way towards more responsible and reliable machine translation systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guerreiro_N/0/1/0/all/0/1\">Nuno M. Guerreiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alves_D/0/1/0/all/0/1\">Duarte Alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waldendorf_J/0/1/0/all/0/1\">Jonas Waldendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1\">Barry Haddow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models. (arXiv:2303.16133v1 [cs.CV])","link":"http://arxiv.org/abs/2303.16133","description":"<p>As general purpose vision models get increasingly effective at a wide set of\ntasks, it is imperative that they be consistent across the tasks they support.\nInconsistent AI models are considered brittle and untrustworthy by human users\nand are more challenging to incorporate into larger systems that take\ndependencies on their outputs. Measuring consistency between very heterogeneous\ntasks that might include outputs in different modalities is challenging since\nit is difficult to determine if the predictions are consistent with one\nanother. As a solution, we introduce a benchmark dataset, COCOCON, where we use\ncontrast sets created by modifying test instances for multiple tasks in small\nbut semantically meaningful ways to change the gold label, and outline metrics\nfor measuring if a model is consistent by ranking the original and perturbed\ninstances across tasks. We find that state-of-the-art systems suffer from a\nsurprisingly high degree of inconsistent behavior across tasks, especially for\nmore heterogeneous tasks. Finally, we propose using a rank correlation-based\nauxiliary objective computed over large automatically created cross-task\ncontrast sets to improve the multi-task consistency of large unified models,\nwhile retaining their original accuracy on downstream tasks. Project website\navailable at https://adymaharana.github.io/cococon/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maharana_A/0/1/0/all/0/1\">Adyasha Maharana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Amita Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_C/0/1/0/all/0/1\">Christopher Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16166","description":"<p>Despite its pivotal role in research experiments, code correctness is often\npresumed only on the basis of the perceived quality of the results. This comes\nwith the risk of erroneous outcomes and potentially misleading findings. To\naddress this issue, we posit that the current focus on result reproducibility\nshould go hand in hand with the emphasis on coding best practices. We bolster\nour call to the NLP community by presenting a case study, in which we identify\n(and correct) three bugs in widely used open-source implementations of the\nstate-of-the-art Conformer architecture. Through comparative experiments on\nautomatic speech recognition and translation in various language settings, we\ndemonstrate that the existence of bugs does not prevent the achievement of good\nand reproducible results and can lead to incorrect conclusions that potentially\nmisguide future research. In response to this, this study is a call to action\ntoward the adoption of coding best practices aimed at fostering correctness and\nimproving the quality of the developed software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilzer_A/0/1/0/all/0/1\">Andrea Pilzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Countering Essentialism through Social Bias Reasoning. (arXiv:2303.16173v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16173","description":"<p>Essentialist beliefs (i.e., believing that members of the same group are\nfundamentally alike) play a central role in social stereotypes and can lead to\nharm when left unchallenged. In our work, we conduct exploratory studies into\nthe task of countering essentialist beliefs (e.g., ``liberals are stupid'').\nDrawing on prior work from psychology and NLP, we construct five types of\ncounterstatements and conduct human studies on the effectiveness of these\ndifferent strategies. Our studies also investigate the role in choosing a\ncounterstatement of the level of explicitness with which an essentialist belief\nis conveyed. We find that statements that broaden the scope of a stereotype\n(e.g., to other groups, as in ``conservatives can also be stupid'') are the\nmost popular countering strategy. We conclude with a discussion of challenges\nand open questions for future work in this area (e.g., improving factuality,\nstudying community-specific variation) and we emphasize the importance of work\nat the intersection of NLP and psychology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allaway_E/0/1/0/all/0/1\">Emily Allaway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taneja_N/0/1/0/all/0/1\">Nina Taneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leslie_S/0/1/0/all/0/1\">Sarah-Jane Leslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])","link":"http://arxiv.org/abs/2303.16199","description":"<p>We present LLaMA-Adapter, a lightweight adaption method to efficiently\nfine-tune LLaMA into an instruction-following model. Using 52K self-instruct\ndemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon\nthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8\nA100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and\nprepend them to the input text tokens at higher transformer layers. Then, a\nzero-init attention mechanism with zero gating is proposed, which adaptively\ninjects the new instructional cues into LLaMA, while effectively preserves its\npre-trained knowledge. With efficient training, LLaMA-Adapter generates\nhigh-quality responses, comparable to Alpaca with fully fine-tuned 7B\nparameters. Furthermore, our approach can be simply extended to multi-modal\ninput, e.g., images, for image-conditioned LLaMA, which achieves superior\nreasoning capacity on ScienceQA. We release our code at\nhttps://github.com/ZrrSkywalker/LLaMA-Adapter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiaming Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aojun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiangfei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shilin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection. (arXiv:2112.11479v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.11479","description":"<p>Recent advancements in technology have led to a boost in social media usage\nwhich has ultimately led to large amounts of user-generated data which also\nincludes hateful and offensive speech. The language used in social media is\noften a combination of English and the native language in the region. In India,\nHindi is used predominantly and is often code-switched with English, giving\nrise to the Hinglish (Hindi+English) language. Various approaches have been\nmade in the past to classify the code-mixed Hinglish hate speech using\ndifferent machine learning and deep learning-based techniques. However, these\ntechniques make use of recurrence on convolution mechanisms which are\ncomputationally expensive and have high memory requirements. Past techniques\nalso make use of complex data processing making the existing techniques very\ncomplex and non-sustainable to change in data. Proposed work gives a much\nsimpler approach which is not only at par with these complex networks but also\nexceeds performance with the use of subword tokenization algorithms like BPE\nand Unigram, along with multi-head attention-based techniques, giving an\naccuracy of 87.41% and an F1 score of 0.851 on standard datasets. Efficient use\nof BPE and Unigram algorithms help handle the nonconventional Hinglish\nvocabulary making the proposed technique simple, efficient and sustainable to\nuse in the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shingi_G/0/1/0/all/0/1\">Geet Shingi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagh_V/0/1/0/all/0/1\">Vedangi Wagh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagh_K/0/1/0/all/0/1\">Kishor Wagh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagh_S/0/1/0/all/0/1\">Sharmila Wagh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SilverAlign: MT-Based Silver Data Algorithm For Evaluating Word Alignment. (arXiv:2210.06207v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06207","description":"<p>Word alignments are essential for a variety of NLP tasks. Therefore, choosing\nthe best approaches for their creation is crucial. However, the scarce\navailability of gold evaluation data makes the choice difficult. We propose\nSilverAlign, a new method to automatically create silver data for the\nevaluation of word aligners by exploiting machine translation and minimal\npairs. We show that performance on our silver data correlates well with gold\nbenchmarks for 9 language pairs, making our approach a valid resource for\nevaluation of different domains and languages when gold data are not available.\nThis addresses the important scenario of missing gold data alignments for\nlow-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Abdullatif K&#xf6;ksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Severini_S/0/1/0/all/0/1\">Silvia Severini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Tuning Makes a Good Classification Head. (arXiv:2210.16771v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16771","description":"<p>In recent years, pretrained models revolutionized the paradigm of natural\nlanguage understanding (NLU), where we append a randomly initialized\nclassification head after the pretrained backbone, e.g. BERT, and finetune the\nwhole model. As the pretrained backbone makes a major contribution to the\nimprovement, we naturally expect a good pretrained classification head can also\nbenefit the training. However, the final-layer output of the backbone, i.e. the\ninput of the classification head, will change greatly during finetuning, making\nthe usual head-only pretraining (LP-FT) ineffective. In this paper, we find\nthat parameter-efficient tuning makes a good classification head, with which we\ncan simply replace the randomly initialized heads for a stable performance\ngain. Our experiments demonstrate that the classification head jointly\npretrained with parameter-efficient tuning consistently improves the\nperformance on 9 tasks in GLUE and SuperGLUE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1\">Qingsong Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptCap: Prompt-Guided Task-Aware Image Captioning. (arXiv:2211.09699v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.09699","description":"<p>Knowledge-based visual question answering (VQA) involves questions that\nrequire world knowledge beyond the image to yield the correct answer. Large\nlanguage models (LMs) like GPT-3 are particularly helpful for this task because\nof their strong knowledge retrieval and reasoning capabilities. To enable LM to\nunderstand images, prior work uses a captioning model to convert images into\ntext. However, when summarizing an image in a single caption sentence, which\nvisual entities to describe are often underspecified. Generic image captions\noften miss visual details essential for the LM to answer visual questions\ncorrectly. To address this challenge, we propose PromptCap (Prompt-guided image\nCaptioning), a captioning model designed to serve as a better connector between\nimages and black-box LMs. Different from generic captions, PromptCap takes a\nnatural-language prompt to control the visual entities to describe in the\ngenerated caption. The prompt contains a question that the caption should aid\nin answering. To avoid extra annotation, PromptCap is trained by examples\nsynthesized with GPT-3 and existing datasets. We demonstrate PromptCap's\neffectiveness on an existing pipeline in which GPT-3 is prompted with image\ncaptions to carry out VQA. PromptCap outperforms generic captions by a large\nmargin and achieves state-of-the-art accuracy on knowledge-based VQA tasks\n(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that\nPromptCap generalizes well to unseen domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yushi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_H/0/1/0/all/0/1\">Hang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.06373","description":"<p>Current approaches to empathetic response generation typically encode the\nentire dialogue history directly and put the output into a decoder to generate\nfriendly feedback. These methods focus on modelling contextual information but\nneglect capturing the direct intention of the speaker. We argue that the last\nutterance in the dialogue empirically conveys the intention of the speaker.\nConsequently, we propose a novel model named InferEM for empathetic response\ngeneration. We separately encode the last utterance and fuse it with the entire\ndialogue through the multi-head attention based intention fusion module to\ncapture the speaker's intention. Besides, we utilize previous utterances to\npredict the last utterance, which simulates human's psychology to guess what\nthe interlocutor may speak in advance. To balance the optimizing rates of the\nutterance prediction and response generation, a multi-task learning strategy is\ndesigned for InferEM. Experimental results demonstrate the plausibility and\nvalidity of InferEM in improving empathetic expression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1\">Guoqing Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextDescriptives: A Python package for calculating a large variety of metrics from text. (arXiv:2301.02057v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.02057","description":"<p>TextDescriptives is a Python package for calculating a large variety of\nmetrics from text. It is built on top of spaCy and can be easily integrated\ninto existing workflows. The package has already been used for analysing the\nlinguistic stability of clinical texts, creating features for predicting\nneuropsychiatric conditions, and analysing linguistic goals of primary school\nstudents. This paper describes the package and its features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1\">Lasse Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsen_L/0/1/0/all/0/1\">Ludvig Renbo Olsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Enevoldsen_K/0/1/0/all/0/1\">Kenneth Enevoldsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opportunities and Challenges in Neural Dialog Tutoring. (arXiv:2301.09919v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.09919","description":"<p>Designing dialog tutors has been challenging as it involves modeling the\ndiverse and complex pedagogical strategies employed by human tutors. Although\nthere have been significant recent advances in neural conversational systems\nusing large language models (LLMs) and growth in available dialog corpora,\ndialog tutoring has largely remained unaffected by these advances. In this\npaper, we rigorously analyze various generative language models on two dialog\ntutoring datasets for language learning using automatic and human evaluations\nto understand the new opportunities brought by these advances as well as the\nchallenges we must overcome to build models that would be usable in real\neducational settings. We find that although current approaches can model\ntutoring in constrained learning scenarios when the number of concepts to be\ntaught and possible teacher strategies are small, they perform poorly in less\nconstrained scenarios. Our human quality evaluation shows that both models and\nground-truth annotations exhibit low performance in terms of equitable\ntutoring, which measures learning opportunities for students and how engaging\nthe dialog is. To understand the behavior of our models in a real tutoring\nsetting, we conduct a user study using expert annotators and find a\nsignificantly large number of model reasoning errors in 45% of conversations.\nFinally, we connect our findings to outline future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Macina_J/0/1/0/all/0/1\">Jakub Macina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daheim_N/0/1/0/all/0/1\">Nico Daheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_T/0/1/0/all/0/1\">Tanmay Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapur_M/0/1/0/all/0/1\">Manu Kapur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KNNs of Semantic Encodings for Rating Prediction. (arXiv:2302.00412v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00412","description":"<p>This paper explores a novel application of textual semantic similarity to\nuser-preference representation for rating prediction. The approach represents a\nuser's preferences as a graph of textual snippets from review text, where the\nedges are defined by semantic similarity. This textual, memory-based approach\nto rating prediction enables review-based explanations for recommendations. The\nmethod is evaluated quantitatively, highlighting that leveraging text in this\nway outperforms both strong memory-based and model-based collaborative\nfiltering baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laugier_L/0/1/0/all/0/1\">L&#xe9;o Laugier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vadapalli_R/0/1/0/all/0/1\">Raghuram Vadapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonald_T/0/1/0/all/0/1\">Thomas Bonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixon_L/0/1/0/all/0/1\">Lucas Dixon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models. (arXiv:2302.07027v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07027","description":"<p>Pretrained language models (PLMs) are trained on massive corpora, but often\nneed to specialize to specific domains. A parameter-efficient adaptation method\nsuggests training an adapter for each domain on the task of language modeling.\nThis leads to good in-domain scores but can be impractical for domain- or\nresource-restricted settings. A solution is to use a related-domain adapter for\nthe novel domain at test time. In this paper, we introduce AdapterSoup, an\napproach that performs weight-space averaging of adapters trained on different\ndomains. Our approach is embarrassingly parallel: first, we train a set of\ndomain-specific adapters; then, for each novel domain, we determine which\nadapters should be averaged at test time. We present extensive experiments\nshowing that AdapterSoup consistently improves performance to new domains\nwithout extra training. We also explore weight averaging of adapters trained on\nthe same domain with different hyper-parameters, and show that it preserves the\nperformance of a PLM on new domains while obtaining strong in-domain results.\nWe explore various approaches for choosing which adapters to combine, such as\ntext clustering and semantic similarity. We find that using clustering leads to\nthe most competitive results on novel domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chronopoulou_A/0/1/0/all/0/1\">Alexandra Chronopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-stage Pipeline for Multilingual Dialect Detection. (arXiv:2303.03487v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.03487","description":"<p>Dialect Identification is a crucial task for localizing various Large\nLanguage Models. This paper outlines our approach to the VarDial 2023 shared\ntask. Here we have to identify three or two dialects from three languages each\nwhich results in a 9-way classification for Track-1 and 6-way classification\nfor Track-2 respectively. Our proposed approach consists of a two-stage system\nand outperforms other participants' systems and previous works in this domain.\nWe achieve a score of 58.54% for Track-1 and 85.61% for Track-2. Our codebase\nis available publicly (https://github.com/ankit-vaidya19/EACL_VarDial2023).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaidya_A/0/1/0/all/0/1\">Ankit Vaidya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kane_A/0/1/0/all/0/1\">Aditya Kane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-augmented Prompt Tuning for Better Few-shot Learning. (arXiv:2303.12314v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.12314","description":"<p>Prompt tuning is a parameter-efficient method, which freezes all PLM\nparameters and only prepends some additional tunable tokens called soft prompts\nto the input text. However, soft prompts heavily rely on a better\ninitialization and may easily result in overfitting under few-shot settings,\nwhich causes prompt-tuning performing much worse than fine-tuning. To address\nthe above issues, this paper proposes a novel Self-sUpervised Meta-prompt\nlearning framework with MEtagradient Regularization for few shot generalization\n(SUMMER). We leverage self-supervised meta-learning to better initialize soft\nprompts and curriculum-based task augmentation is further proposed to enrich\nthe meta-task distribution. Besides, a novel meta-gradient regularization\nmethod is integrated into the meta-prompt learning framework, which meta-learns\nto transform the raw gradient during few-shot learning into a\ndomain-generalizable direction, thus alleviating the problem of overfitting.\nExtensive experiments show that SUMMER achieves better performance for\ndifferent few-shot downstream tasks, and also exhibits a stronger domain\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_K/0/1/0/all/0/1\">Kaihang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hongye Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4. (arXiv:2303.12712v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.12712","description":"<p>Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1\">S&#xe9;bastien Bubeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1\">Varun Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldan_R/0/1/0/all/0/1\">Ronen Eldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrke_J/0/1/0/all/0/1\">Johannes Gehrke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Peter Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yin Tat Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundberg_S/0/1/0/all/0/1\">Scott Lundberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_H/0/1/0/all/0/1\">Harsha Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marco Tulio Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering. (arXiv:2303.13284v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13284","description":"<p>In this work, we present an end-to-end Knowledge Graph Question Answering\n(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text\npre-trained language model. The model takes a question in natural language as\ninput and produces a simpler form of the intended SPARQL query. In the simpler\nform, the model does not directly produce entity and relation IDs. Instead, it\nproduces corresponding entity and relation labels. The labels are grounded to\nKG entity and relation IDs in a subsequent step. To further improve the\nresults, we instruct the model to produce a truncated version of the KG\nembedding for each entity. The truncated KG embedding enables a finer search\nfor disambiguation purposes. We find that T5 is able to learn the truncated KG\nembeddings without any change of loss function, improving KGQA performance. As\na result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata\ndatasets on end-to-end KGQA over Wikidata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debayan Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_P/0/1/0/all/0/1\">Pranav Ajit Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph. (arXiv:2303.13351v2 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2303.13351","description":"<p>In this work we create a question answering dataset over the DBLP scholarly\nknowledge graph (KG). DBLP is an on-line reference for bibliographic\ninformation on major computer science publications that indexes over 4.4\nmillion publications published by more than 2.2 million authors. Our dataset\nconsists of 10,000 question answer pairs with the corresponding SPARQL queries\nwhich can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD\nis the largest scholarly question answering dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debayan Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awale_S/0/1/0/all/0/1\">Sushil Awale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. (arXiv:2303.14070v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.14070","description":"<p>Recent large language models (LLMs) in the general domain, such as ChatGPT,\nhave shown remarkable success in following instructions and producing\nhuman-like responses. However, such language models have not been tailored to\nthe medical domain, resulting in poor answer accuracy and inability to give\nplausible recommendations for medical diagnosis, medications, etc. To address\nthis issue, we collected more than 700 diseases and their corresponding\nsymptoms, required medical tests, and recommended medications, from which we\ngenerated 5K doctor-patient conversations. By fine-tuning LLMs using these\ntailored doctor-patient conversations, the resulting models emerge with great\npotential to understand patients' needs, provide informed advice, and offer\nvaluable assistance in a variety of medical-related fields. The integration of\nthese advanced language models into healthcare can revolutionize the way\nhealthcare professionals and patients communicate, ultimately improving the\noverall efficiency and quality of patient care and outcomes. In addition, we\nmade public all the source codes, datasets, and model weights to facilitate the\nfurther development of dialogue models in the medical field. The training data,\ncodes, and weights of this project are available at:\nhttps://github.com/Kent0n-Li/ChatDoctor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yunxiang_L/0/1/0/all/0/1\">Li Yunxiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zihan_L/0/1/0/all/0/1\">Li Zihan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kai_Z/0/1/0/all/0/1\">Zhang Kai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruilong_D/0/1/0/all/0/1\">Dan Ruilong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zhang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SmartBook: AI-Assisted Situation Report Generation. (arXiv:2303.14337v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.14337","description":"<p>Emerging events, such as the COVID pandemic and the Ukraine Crisis, require a\ntime-sensitive comprehensive understanding of the situation to allow for\nappropriate decision-making and effective action response. Automated generation\nof situation reports can significantly reduce the time, effort, and cost for\ndomain experts when preparing their official human-curated reports. However, AI\nresearch toward this goal has been very limited, and no successful trials have\nyet been conducted to automate such report generation. We propose SmartBook, a\nnovel task formulation targeting situation report generation, which consumes\nlarge volumes of news data to produce a structured situation report with\nmultiple hypotheses (claims) summarized and grounded with rich links to factual\nevidence. We realize SmartBook for the Ukraine-Russia crisis by automatically\ngenerating intelligence analysis reports to assist expert analysts. The\nmachine-generated reports are structured in the form of timelines, with each\ntimeline organized by major events (or chapters), corresponding strategic\nquestions (or sections) and their grounded summaries (or section content). Our\nproposed framework automatically detects real-time event-related strategic\nquestions, which are more directed than manually-crafted analyst questions,\nwhich tend to be too complex, hard to parse, vague and high-level. Results from\nthorough qualitative evaluations show that roughly 82% of the questions in\nSmartbook have strategic importance, with at least 93% of the sections in the\nreport being tactically useful. Further, experiments show that expert analysts\ntend to add more information into the SmartBook reports, with only 2.3% of the\nexisting tokens being deleted, meaning SmartBook can serve as a useful\nfoundation for analysts to build upon when creating intelligence reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Revanth Gangi Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_Y/0/1/0/all/0/1\">Yi R. Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_P/0/1/0/all/0/1\">Paul Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Diverse Role-Players for Summarization Evaluation. (arXiv:2303.15078v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.15078","description":"<p>Text summarization has a wide range of applications in many scenarios. The\nevaluation of the quality of the generated text is a complex problem. A big\nchallenge to language evaluation is that there is a clear divergence between\nexisting metrics and human evaluation. For example, the quality of a document\nsummary can be measured by human annotators from both objective aspects, such\nas grammatical and semantic correctness, as well as subjective dimensions, such\nas comprehensiveness, succinctness, and interestingness. Most of the automatic\nevaluation methods like BLUE/ROUGE may be not able to capture the above\ndimensions well. In this paper, we propose a new evaluation framework based on\nLLMs, which provides a comprehensive evaluation framework by comparing\ngenerated text and reference text from both objective and subjective aspects.\nFirst, we propose to model objective and subjective dimensions of generated\ntext based on roleplayers prompting mechanism. Furthermore, we introduce a\ncontext-based prompting mechanism that is able to generate dynamic roleplayer\nprofiles based on input context. Finally, we design a multi-roleplayer\nprompting technology based on batch prompting to integrate multiple evaluation\nresults into evaluation results. Experimental results on two real datasets for\nsummarization show that our model is highly competitive and has a very high\nconsistency with human annotators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Ning Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shining Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v1 [cs.NE] CROSS LISTED)","link":"http://arxiv.org/abs/2302.14838","description":"<p>Given the recent impressive accomplishments of language models (LMs) for code\ngeneration, we explore the use of LMs as adaptive mutation and crossover\noperators for an evolutionary neural architecture search (NAS) algorithm. While\nNAS still proves too difficult a task for LMs to succeed at solely through\nprompting, we find that the combination of evolutionary prompt engineering with\nsoft prompt-tuning, a method we term EvoPrompting, consistently finds diverse\nand high performing models. We first demonstrate that EvoPrompting is effective\non the computationally efficient MNIST-1D dataset, where EvoPrompting produces\nconvolutional architecture variants that outperform both those designed by\nhuman experts and naive few-shot prompting in terms of accuracy and model size.\nWe then apply our method to searching for graph neural networks on the CLRS\nAlgorithmic Reasoning Benchmark, where EvoPrompting is able to design novel\narchitectures that outperform current state-of-the-art models on 21 out of 30\nalgorithmic reasoning tasks while maintaining similar model size. EvoPrompting\nis successful at designing accurate and efficient neural network architectures\nacross a variety of machine learning tasks, while also being general enough for\neasy adaptation to other tasks beyond neural network design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David M. Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1\">David R. So</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}