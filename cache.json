{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-07-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])","link":"http://arxiv.org/abs/2307.14361","description":"<p>This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and\nGloVe to classify gene mutations using Kaggle's Personalized Medicine:\nRedefining Cancer Treatment dataset. The results were compared against\nwell-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and\ntheir LSTM ensembles. Our model outperformed all other models in terms of\naccuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it\nalso needed less training time, resulting in a perfect combination of\nperformance and efficiency. This study demonstrates the utility of ensemble\nmodels for difficult tasks such as gene mutation classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Aburass_S/0/1/0/all/0/1\">Sanad Aburass</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Dorgham_O/0/1/0/all/0/1\">Osama Dorgham</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shaqsi_J/0/1/0/all/0/1\">Jamil Al Shaqsi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers. (arXiv:2307.14367v1 [q-bio.QM])","link":"http://arxiv.org/abs/2307.14367","description":"<p>The complex nature of big biological systems pushed some scientists to\nclassify its understanding under the inconceivable missions. Different leveled\nchallenges complicated this task, one of is the prediction of a protein's\nfunction. In recent years, significant progress has been made in this field\nthrough the development of various machine learning approaches. However, most\nexisting methods formulate the task as a multi-classification problem, i.e\nassigning predefined labels to proteins. In this work, we propose a novel\napproach, \\textbf{Prot2Text}, which predicts a protein function's in a free\ntext style, moving beyond the conventional binary or categorical\nclassifications. By combining Graph Neural Networks(GNNs) and Large Language\nModels(LLMs), in an encoder-decoder framework, our model effectively integrates\ndiverse data types including proteins' sequences, structures, and textual\nannotations. This multimodal approach allows for a holistic representation of\nproteins' functions, enabling the generation of detailed and accurate\ndescriptions. To evaluate our model, we extracted a multimodal protein dataset\nfrom SwissProt, and demonstrate empirically the effectiveness of Prot2Text.\nThese results highlight the transformative impact of multimodal models,\nspecifically the fusion of GNNs and LLMs, empowering researchers with powerful\ntools for more accurate prediction of proteins' functions. The code, the models\nand a demo will be publicly released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Abdine_H/0/1/0/all/0/1\">Hadi Abdine</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chatzianastasis_M/0/1/0/all/0/1\">Michail Chatzianastasis</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bouyioukos_C/0/1/0/all/0/1\">Costas Bouyioukos</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Can Large Language Models Help Humans in Design and Manufacturing?. (arXiv:2307.14377v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14377","description":"<p>The advancement of Large Language Models (LLMs), including GPT-4, provides\nexciting new opportunities for generative design. We investigate the\napplication of this tool across the entire design and manufacturing workflow.\nSpecifically, we scrutinize the utility of LLMs in tasks such as: converting a\ntext-based prompt into a design specification, transforming a design into\nmanufacturing instructions, producing a design space and design variations,\ncomputing the performance of a design, and searching for designs predicated on\nperformance. Through a series of examples, we highlight both the benefits and\nthe limitations of the current LLMs. By exposing these limitations, we aspire\nto catalyze the continued improvement and progression of these models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Makatura_L/0/1/0/all/0/1\">Liane Makatura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foshey_M/0/1/0/all/0/1\">Michael Foshey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+HahnLein_F/0/1/0/all/0/1\">Felix H&#xe4;hnLein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingchuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bolei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjandrasuwita_M/0/1/0/all/0/1\">Megan Tjandrasuwita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spielberg_A/0/1/0/all/0/1\">Andrew Spielberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owens_C/0/1/0/all/0/1\">Crystal Elaine Owens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peter Yichen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1\">Allan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1\">Amy Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norton_W/0/1/0/all/0/1\">Wil J Norton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_E/0/1/0/all/0/1\">Edward Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacob_J/0/1/0/all/0/1\">Joshua Jacob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1\">Adriana Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matusik_W/0/1/0/all/0/1\">Wojciech Matusik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Large Language Models for Mental Health Prediction via Online Text Data. (arXiv:2307.14385v1 [cs.HC])","link":"http://arxiv.org/abs/2307.14385","description":"<p>The recent technology boost of large language models (LLMs) has empowered a\nvariety of applications. However, there is very little research on\nunderstanding and improving LLMs' capability for the mental health domain. In\nthis work, we present the first comprehensive evaluation of multiple LLMs,\nincluding Alpaca, Alpaca-LoRA, and GPT-3.5, on various mental health prediction\ntasks via online text data. We conduct a wide range of experiments, covering\nzero-shot prompting, few-shot prompting, and instruction finetuning. The\nresults indicate the promising yet limited performance of LLMs with zero-shot\nand few-shot prompt designs for mental health tasks. More importantly, our\nexperiments show that instruction finetuning can significantly boost the\nperformance of LLMs for all tasks simultaneously. Our best-finetuned model,\nMental-Alpaca, outperforms GPT-3.5 (25 times bigger) by 16.7\\% on balanced\naccuracy and performs on par with the state-of-the-art task-specific model. We\nsummarize our findings into a set of action guidelines for future researchers,\nengineers, and practitioners on how to empower LLMs with better mental health\ndomain knowledge and become an expert in mental health prediction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuhai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingshen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuanzhe Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendler_J/0/1/0/all/0/1\">James Hendler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1\">Anind K. Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG. (arXiv:2307.14389v1 [eess.AS])","link":"http://arxiv.org/abs/2307.14389","description":"<p>Decoding EEG signals for imagined speech is a challenging task due to the\nhigh-dimensional nature of the data and low signal-to-noise ratio. In recent\nyears, denoising diffusion probabilistic models (DDPMs) have emerged as\npromising approaches for representation learning in various domains. Our study\nproposes a novel method for decoding EEG signals for imagined speech using\nDDPMs and a conditional autoencoder named Diff-E. Results indicate that Diff-E\nsignificantly improves the accuracy of decoding EEG signals for imagined speech\ncompared to traditional machine learning techniques and baseline models. Our\nfindings suggest that DDPMs can be an effective tool for EEG signal decoding,\nwith potential implications for the development of brain-computer interfaces\nthat enable communication through imagined speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Soowon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1\">Young-Eun Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Seo-Hyun Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models. (arXiv:2307.14430v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14430","description":"<p>The quality of training data impacts the performance of pre-trained large\nlanguage models (LMs). Given a fixed budget of tokens, we study how to best\nselect data that leads to good downstream model performance across tasks. We\ndevelop a new framework based on a simple hypothesis: just as humans acquire\ninterdependent skills in a deliberate order, language models also follow a\nnatural order when learning a set of skills from their training data. If such\nan order exists, it can be utilized for improved understanding of LMs and for\ndata-efficient training. Using this intuition, our framework formalizes the\nnotion of a skill and of an ordered set of skills in terms of the associated\ndata. First, using both synthetic and real data, we demonstrate that these\nordered skill sets exist, and that their existence enables more advanced skills\nto be learned with less data when we train on their prerequisite skills.\nSecond, using our proposed framework, we introduce an online data sampling\nalgorithm, Skill-It, over mixtures of skills for both continual pre-training\nand fine-tuning regimes, where the objective is to efficiently learn multiple\nskills in the former and an individual skill in the latter. On the LEGO\nsynthetic in the continual pre-training setting, Skill-It obtains 36.5 points\nhigher accuracy than random sampling. On the Natural Instructions dataset in\nthe fine-tuning setting, Skill-It reduces the validation loss on the target\nskill by 13.6% versus training on data associated with the target skill itself.\nWe apply our skills framework on the recent RedPajama dataset to continually\npre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation\nHarness with 1B tokens than the baseline approach of sampling uniformly over\ndata sources with 3B tokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mayee F. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1\">Nicholas Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_K/0/1/0/all/0/1\">Kush Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1\">Frederic Sala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Generation of Dialogue Acts for Dialogue Systems via Few-Shot Response Generation and Ranking. (arXiv:2307.14440v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14440","description":"<p>Dialogue systems need to produce responses that realize multiple types of\ndialogue acts (DAs) with high semantic fidelity. In the past, natural language\ngenerators (NLGs) for dialogue were trained on large parallel corpora that map\nfrom a domain-specific DA and its semantic attributes to an output utterance.\nRecent work shows that pretrained language models (LLMs) offer new\npossibilities for controllable NLG using prompt-based learning. Here we develop\na novel few-shot overgenerate-and-rank approach that achieves the controlled\ngeneration of DAs. We compare eight few-shot prompt styles that include a novel\nmethod of generating from textual pseudo-references using a textual style\ntransfer approach. We develop six automatic ranking functions that identify\noutputs with both the correct DA and high semantic accuracy at generation time.\nWe test our approach on three domains and four LLMs. To our knowledge, this is\nthe first work on NLG for dialogue that automatically ranks outputs using both\nDA and attribute accuracy. For completeness, we compare our results to\nfine-tuned few-shot models trained with 5 to 100 instances per DA. Our results\nshow that several prompt settings achieve perfect DA accuracy, and near perfect\nsemantic accuracy (99.81%) and perform better than few-shot fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_A/0/1/0/all/0/1\">Angela Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_K/0/1/0/all/0/1\">Karik Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juraska_J/0/1/0/all/0/1\">Juraj Juraska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_U/0/1/0/all/0/1\">Utkarsh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_M/0/1/0/all/0/1\">Marilyn A. Walker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing. (arXiv:2307.14500v1 [cs.HC])","link":"http://arxiv.org/abs/2307.14500","description":"<p>This study introduces and empirically tests a novel predictive model for\ndigital information engagement (IE) - the READ model, an acronym for the four\npivotal attributes of engaging information: Representativeness, Ease-of-use,\nAffect, and Distribution. Conceptualized within the theoretical framework of\nCumulative Prospect Theory, the model integrates key cognitive biases with\ncomputational linguistics and natural language processing to develop a\nmultidimensional perspective on information engagement. A rigorous testing\nprotocol was implemented, involving 50 randomly selected pairs of synonymous\nwords (100 words in total) from the WordNet database. These words' engagement\nlevels were evaluated through a large-scale online survey (n = 80,500) to\nderive empirical IE metrics. The READ attributes for each word were then\ncomputed and their predictive efficacy examined. The findings affirm the READ\nmodel's robustness, accurately predicting a word's IE level and distinguishing\nthe more engaging word from a pair of synonyms with an 84% accuracy rate. The\nREAD model's potential extends across various domains, including business,\neducation, government, and healthcare, where it could enhance content\nengagement and inform AI language model development and generative text work.\nFuture research should address the model's scalability and adaptability across\ndifferent domains and languages, thereby broadening its applicability and\nefficacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dvir_N/0/1/0/all/0/1\">Nimrod Dvir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_E/0/1/0/all/0/1\">Elaine Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Commuri_S/0/1/0/all/0/1\">Suraj Commuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+yang_F/0/1/0/all/0/1\">Fan yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romano_J/0/1/0/all/0/1\">Jennifer Romano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Words That Stick: Predicting Decision Making and Synonym Engagement Using Cognitive Biases and Computational Linguistics. (arXiv:2307.14511v1 [cs.HC])","link":"http://arxiv.org/abs/2307.14511","description":"<p>This research draws upon cognitive psychology and information systems studies\nto anticipate user engagement and decision-making on digital platforms. By\nemploying natural language processing (NLP) techniques and insights from\ncognitive bias research, we delve into user interactions with synonyms within\ndigital content. Our methodology synthesizes four cognitive\nbiasesRepresentativeness, Ease-of-use, Affect, and Distributioninto the READ\nmodel. Through a comprehensive user survey, we assess the model's ability to\npredict user engagement, discovering that synonyms that accurately represent\ncore ideas, are easy to understand, elicit emotional responses, and are\ncommonly encountered, promote greater user engagement. Crucially, our work\noffers a fresh lens on human-computer interaction, digital behaviors, and\ndecision-making processes. Our results highlight the promise of cognitive\nbiases as potent indicators of user engagement, underscoring their significance\nin designing effective digital content across fields like education and\nmarketing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dvir_N/0/1/0/all/0/1\">Nimrod Dvir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_E/0/1/0/all/0/1\">Elaine Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Commuri_S/0/1/0/all/0/1\">Suraj Commuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romano_J/0/1/0/all/0/1\">Jennifer Romano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions. (arXiv:2307.14522v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14522","description":"<p>A clinical trial is a study that evaluates new biomedical interventions. To\ndesign new trials, researchers draw inspiration from those current and\ncompleted. In 2022, there were on average more than 100 clinical trials\nsubmitted to ClinicalTrials.gov every day, with each trial having a mean of\napproximately 1500 words [1]. This makes it nearly impossible to keep up to\ndate. To mitigate this issue, we have created a batch clinical trial summarizer\ncalled CliniDigest using GPT-3.5. CliniDigest is, to our knowledge, the first\ntool able to provide real-time, truthful, and comprehensive summaries of\nclinical trials. CliniDigest can reduce up to 85 clinical trial descriptions\n(approximately 10,500 words) into a concise 200-word summary with references\nand limited hallucinations. We have tested CliniDigest on its ability to\nsummarize 457 trials divided across 27 medical subdomains. For each field,\nCliniDigest generates summaries of $\\mu=153,\\ \\sigma=69 $ words, each of which\nutilizes $\\mu=54\\%,\\ \\sigma=30\\% $ of the sources. A more comprehensive\nevaluation is planned and outlined in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+White_R/0/1/0/all/0/1\">Renee D. White</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1\">Tristan Peng</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sripitak_P/0/1/0/all/0/1\">Pann Sripitak</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Johansen_A/0/1/0/all/0/1\">Alexander Rosenberg Johansen</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Snyder_M/0/1/0/all/0/1\">Michael Snyder</a> (1) (1) <a href=\"http://arxiv.org/find/cs/1/au:+University_S/0/1/0/all/0/1\">Stanford University</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models. (arXiv:2307.14539v1 [cs.CR])","link":"http://arxiv.org/abs/2307.14539","description":"<p>The rapid growth and increasing popularity of incorporating additional\nmodalities (e.g., vision) into large language models (LLMs) has raised\nsignificant security concerns. This expansion of modality, akin to adding more\ndoors to a house, unintentionally creates multiple access points for\nadversarial attacks. In this paper, by introducing adversarial embedding space\nattacks, we emphasize the vulnerabilities present in multi-modal systems that\noriginate from incorporating off-the-shelf components like public pre-trained\nencoders in a plug-and-play manner into these systems. In contrast to existing\nwork, our approach does not require access to the multi-modal system's weights\nor parameters but instead relies on the huge under-explored embedding space of\nsuch pre-trained encoders. Our proposed embedding space attacks involve seeking\ninput images that reside within the dangerous or targeted regions of the\nextensive embedding space of these pre-trained components. These crafted\nadversarial images pose two major threats: 'Context Contamination' and 'Hidden\nPrompt Injection'-both of which can compromise multi-modal models like LLaVA\nand fully change the behavior of the associated language model. Our findings\nemphasize the need for a comprehensive examination of the underlying\ncomponents, particularly pre-trained encoders, before incorporating them into\nsystems in a plug-and-play manner to ensure robust security.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shayegani_E/0/1/0/all/0/1\">Erfan Shayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1\">Nael Abu-Ghazaleh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speed Reading Tool Powered by Artificial Intelligence for Students with ADHD, Dyslexia, or Short Attention Span. (arXiv:2307.14544v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14544","description":"<p>This paper presents a novel approach to assist students with dyslexia, ADHD,\nand short attention span in digesting any text-based information more\nefficiently. The proposed solution utilizes the Multilayer Perceptron (MLP)\nalgorithm for complex text processing and summarization tasks. The tool\nleverages the T5 (Text-to-Text Transfer Transformer) model from Hugging Face,\nwhich treats every NLP task as a text generation task. The model is fine-tuned\non specific tasks using a smaller dataset. The NLTK's Punkt Sentence Tokenizer\nis used to divide a text into a list of sentences. The application is served\nusing Flask, a lightweight web server and framework. The tool also applies\nprinciples from Bionic Reading to enhance readability, which includes a bolding\nfunction and adjustments to line, word, and character spacing. The paper\ndiscusses the methodology, implementation, and results of the AI-based speed\nreading tool.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamarozaman_M/0/1/0/all/0/1\">Megat Irfan Zackry Bin Ismail Ahmad Nazran bin Yusri Muhammad Hafizzul Bin Abdul Manap Muhammad Muizzuddin Bin Kamarozaman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metric-Based In-context Learning: A Case Study in Text Simplification. (arXiv:2307.14632v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14632","description":"<p>In-context learning (ICL) for large language models has proven to be a\npowerful approach for many natural language processing tasks. However,\ndetermining the best method to select examples for ICL is nontrivial as the\nresults can vary greatly depending on the quality, quantity, and order of\nexamples used. In this paper, we conduct a case study on text simplification\n(TS) to investigate how to select the best and most robust examples for ICL. We\npropose Metric-Based in-context Learning (MBL) method that utilizes commonly\nused TS metrics such as SARI, compression ratio, and BERT-Precision for\nselection. Through an extensive set of experiments with various-sized GPT\nmodels on standard TS benchmarks such as TurkCorpus and ASSET, we show that\nexamples selected by the top SARI scores perform the best on larger models such\nas GPT-175B, while the compression ratio generally performs better on smaller\nmodels such as GPT-13B and GPT-6.7B. Furthermore, we demonstrate that MBL is\ngenerally robust to example orderings and out-of-domain test sets, and\noutperforms strong baselines and state-of-the-art finetuned language models.\nFinally, we show that the behaviour of large GPT models can be implicitly\ncontrolled by the chosen metric. Our research provides a new framework for\nselecting examples in ICL, and demonstrates its effectiveness in text\nsimplification tasks, breaking new ground for more accurate and efficient NLG\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vadlamannati_S/0/1/0/all/0/1\">Subha Vadlamannati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_G/0/1/0/all/0/1\">G&#xf6;zde G&#xfc;l &#x15e;ahin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Natural Language Inference in Arabic using Transformer Models and Linguistically Informed Pre-Training. (arXiv:2307.14666v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14666","description":"<p>This paper addresses the classification of Arabic text data in the field of\nNatural Language Processing (NLP), with a particular focus on Natural Language\nInference (NLI) and Contradiction Detection (CD). Arabic is considered a\nresource-poor language, meaning that there are few data sets available, which\nleads to limited availability of NLP methods. To overcome this limitation, we\ncreate a dedicated data set from publicly available resources. Subsequently,\ntransformer-based machine learning models are being trained and evaluated. We\nfind that a language-specific model (AraBERT) performs competitively with\nstate-of-the-art multilingual approaches, when we apply linguistically informed\npre-training methods such as Named Entity Recognition (NER). To our knowledge,\nthis is the first large-scale evaluation for this task in Arabic, as well as\nthe first application of multi-task pre-training in this context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deen_M/0/1/0/all/0/1\">Mohammad Majd Saad Al Deen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pielka_M/0/1/0/all/0/1\">Maren Pielka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1\">J&#xf6;rn Hees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdou_B/0/1/0/all/0/1\">Bouthaina Soulef Abdou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifa_R/0/1/0/all/0/1\">Rafet Sifa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Generative Models for Graph-to-Text Generation. (arXiv:2307.14712v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14712","description":"<p>Large language models (LLMs) have been widely employed for graph-to-text\ngeneration tasks. However, the process of finetuning LLMs requires significant\ntraining resources and annotation work. In this paper, we explore the\ncapability of generative models to generate descriptive text from graph data in\na zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two\ngraph-to-text datasets and compare their performance with that of finetuned LLM\nmodels such as T5 and BART. Our results demonstrate that generative models are\ncapable of generating fluent and coherent text, achieving BLEU scores of 10.57\nand 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error\nanalysis reveals that generative models still struggle with understanding the\nsemantic relations between entities, and they also tend to generate text with\nhallucinations or irrelevant information. As a part of error analysis, we\nutilize BERT to detect machine-generated text and achieve high macro-F1 scores.\nWe have made the text generated by generative models publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Shuzhou Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farber_M/0/1/0/all/0/1\">Michael F&#xe4;rber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turning Whisper into Real-Time Transcription System. (arXiv:2307.14743v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14743","description":"<p>Whisper is one of the recent state-of-the-art multilingual speech recognition\nand translation models, however, it is not designed for real time\ntranscription. In this paper, we build on top of Whisper and create\nWhisper-Streaming, an implementation of real-time speech transcription and\ntranslation of Whisper-like models. Whisper-Streaming uses local agreement\npolicy with self-adaptive latency to enable streaming transcription. We show\nthat Whisper-Streaming achieves high quality and 3.3 seconds latency on\nunsegmented long-form speech transcription test set, and we demonstrate its\nrobustness and practical usability as a component in live transcription service\nat a multilingual conference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Machacek_D/0/1/0/all/0/1\">Dominik Mach&#xe1;&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset. (arXiv:2307.14783v1 [eess.AS])","link":"http://arxiv.org/abs/2307.14783","description":"<p>We present a new large-scale emotion-labeled symbolic music dataset\nconsisting of 12k MIDI songs. To create this dataset, we first trained emotion\nclassification models on the GoEmotions dataset, achieving state-of-the-art\nresults with a model half the size of the baseline. We then applied these\nmodels to lyrics from two large-scale MIDI datasets. Our dataset covers a wide\nrange of fine-grained emotions, providing a valuable resource to explore the\nconnection between music and emotions and, especially, to develop models that\ncan generate music based on specific emotions. Our code for inference, trained\nmodels, and datasets are available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sulun_S/0/1/0/all/0/1\">Serkan Sulun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_P/0/1/0/all/0/1\">Pedro Oliveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Viana_P/0/1/0/all/0/1\">Paula Viana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Aspect-Based Sentiment with End-to-End Semantic Role Labeling Model. (arXiv:2307.14785v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14785","description":"<p>This paper presents a series of approaches aimed at enhancing the performance\nof Aspect-Based Sentiment Analysis (ABSA) by utilizing extracted semantic\ninformation from a Semantic Role Labeling (SRL) model. We propose a novel\nend-to-end Semantic Role Labeling model that effectively captures most of the\nstructured semantic information within the Transformer hidden state. We believe\nthat this end-to-end model is well-suited for our newly proposed models that\nincorporate semantic information. We evaluate the proposed models in two\nlanguages, English and Czech, employing ELECTRA-small models. Our combined\nmodels improve ABSA performance in both languages. Moreover, we achieved new\nstate-of-the-art results on the Czech ABSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Priban_P/0/1/0/all/0/1\">Pavel P&#x159;ib&#xe1;&#x148;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1\">Ond&#x159;ej Pra&#x17e;&#xe1;k</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Models of reference production: How do they withstand the test of time?. (arXiv:2307.14817v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14817","description":"<p>In recent years, many NLP studies have focused solely on performance\nimprovement. In this work, we focus on the linguistic and scientific aspects of\nNLP. We use the task of generating referring expressions in context\n(REG-in-context) as a case study and start our analysis from GREC, a\ncomprehensive set of shared tasks in English that addressed this topic over a\ndecade ago. We ask what the performance of models would be if we assessed them\n(1) on more realistic datasets, and (2) using more advanced methods. We test\nthe models using different evaluation metrics and feature selection\nexperiments. We conclude that GREC can no longer be regarded as offering a\nreliable assessment of models' ability to mimic human reference production,\nbecause the results are highly impacted by the choice of corpus and evaluation\nmetrics. Our results also suggest that pre-trained language models are less\ndependent on the choice of corpus than classic Machine Learning models, and\ntherefore make more robust class predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Same_F/0/1/0/all/0/1\">Fahime Same</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes a Good Paraphrase: Do Automated Evaluations Work?. (arXiv:2307.14818v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14818","description":"<p>Paraphrasing is the task of expressing an essential idea or meaning in\ndifferent words. But how different should the words be in order to be\nconsidered an acceptable paraphrase? And can we exclusively use automated\nmetrics to evaluate the quality of a paraphrase? We attempt to answer these\nquestions by conducting experiments on a German data set and performing\nautomatic and expert linguistic evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moskvina_A/0/1/0/all/0/1\">Anna Moskvina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotnis_B/0/1/0/all/0/1\">Bhushan Kotnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catacata_C/0/1/0/all/0/1\">Chris Catacata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janz_M/0/1/0/all/0/1\">Michael Janz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saef_N/0/1/0/all/0/1\">Nasrin Saef</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turkish Native Language Identification. (arXiv:2307.14850v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14850","description":"<p>In this paper, we present the first application of Native Language\nIdentification (NLI) for the Turkish language. NLI involves predicting the\nwriter's first language by analysing their writing in different languages.\nWhile most NLI research has focused on English, our study extends its scope to\nTurkish. We used the recently constructed Turkish Learner Corpus and employed a\ncombination of three syntactic features (CFG production rules, part-of-speech\nn-grams and function words) with L2 texts to demonstrate their effectiveness in\nthis task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uluslu_A/0/1/0/all/0/1\">Ahmet Yavuz Uluslu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_G/0/1/0/all/0/1\">Gerold Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArcGPT: A Large Language Model Tailored for Real-world Archival Applications. (arXiv:2307.14852v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14852","description":"<p>Archives play a crucial role in preserving information and knowledge, and the\nexponential growth of such data necessitates efficient and automated tools for\nmanaging and utilizing archive information resources. Archival applications\ninvolve managing massive data that are challenging to process and analyze.\nAlthough LLMs have made remarkable progress in diverse domains, there are no\npublicly available archives tailored LLM. Addressing this gap, we introduce\nArcGPT, to our knowledge, the first general-purpose LLM tailored to the\narchival field. To enhance model performance on real-world archival tasks,\nArcGPT has been pre-trained on massive and extensive archival domain data.\nAlongside ArcGPT, we release AMBLE, a benchmark comprising four real-world\narchival tasks. Evaluation on AMBLE shows that ArcGPT outperforms existing\nstate-of-the-art models, marking a substantial step forward in effective\narchival data management. Ultimately, ArcGPT aims to better serve the archival\ncommunity, aiding archivists in their crucial role of preserving and harnessing\nour collective information and knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shitou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jingrui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Siyuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qibiao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners. (arXiv:2307.14856v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14856","description":"<p>In-context learning, which offers substantial advantages over fine-tuning, is\npredominantly observed in decoder-only models, while encoder-decoder (i.e.,\nseq2seq) models excel in methods that rely on weight updates. Recently, a few\nstudies have demonstrated the feasibility of few-shot learning with seq2seq\nmodels; however, this has been limited to tasks that align well with the\nseq2seq architecture, such as summarization and translation. Inspired by these\ninitial studies, we provide a first-ever extensive experiment comparing the\nin-context few-shot learning capabilities of decoder-only and encoder-decoder\nmodels on a broad range of tasks. Furthermore, we propose two methods to more\neffectively elicit in-context learning ability in seq2seq models:\nobjective-aligned prompting and a fusion-based approach. Remarkably, our\napproach outperforms a decoder-only model that is six times larger and exhibits\nsignificant performance improvements compared to conventional seq2seq models\nacross a variety of settings. We posit that, with the right configuration and\nprompt design, seq2seq models can be highly effective few-shot learners for a\nwide spectrum of applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dain Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1\">Doohae Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Boseop Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+On_K/0/1/0/all/0/1\">Kyoung-Woon On</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MESED: A Multi-modal Entity Set Expansion Dataset with Fine-grained Semantic Classes and Hard Negative Entities. (arXiv:2307.14878v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14878","description":"<p>The Entity Set Expansion (ESE) task aims to expand a handful of seed entities\nwith new entities belonging to the same semantic class. Conventional ESE\nmethods are based on mono-modality (i.e., literal modality), which struggle to\ndeal with complex entities in the real world such as: (1) Negative entities\nwith fine-grained semantic differences. (2) Synonymous entities. (3) Polysemous\nentities. (4) Long-tailed entities. These challenges prompt us to propose\nMulti-modal Entity Set Expansion (MESE), where models integrate information\nfrom multiple modalities to represent entities. Intuitively, the benefits of\nmulti-modal information for ESE are threefold: (1) Different modalities can\nprovide complementary information. (2) Multi-modal information provides a\nunified signal via common visual properties for the same semantic class or\nentity. (3) Multi-modal information offers robust alignment signal for\nsynonymous entities. To assess the performance of model in MESE and facilitate\nfurther research, we constructed the MESED dataset which is the first\nmulti-modal dataset for ESE with large-scale and elaborate manual calibration.\nA powerful multi-modal model MultiExpan is proposed which is pre-trained on\nfour multimodal pre-training tasks. The extensive experiments and analyses on\nMESED demonstrate the high quality of the dataset and the effectiveness of our\nMultiExpan, as well as pointing the direction for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tingwei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianyu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shulin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jun Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-based Text Selection for Addressing Class-Imbalanced Data in Classification. (arXiv:2307.14899v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14899","description":"<p>This paper addresses the problem of selecting of a set of texts for\nannotation in text classification using retrieval methods when there are limits\non the number of annotations due to constraints on human resources. An\nadditional challenge addressed is dealing with binary categories that have a\nsmall number of positive instances, reflecting severe class imbalance. In our\nsituation, where annotation occurs over a long time period, the selection of\ntexts to be annotated can be made in batches, with previous annotations guiding\nthe choice of the next set. To address these challenges, the paper proposes\nleveraging SHAP to construct a quality set of queries for Elasticsearch and\nsemantic search, to try to identify optimal sets of texts for annotation that\nwill help with class imbalance. The approach is tested on sets of cue texts\ndescribing possible future events, constructed by participants involved in\nstudies aimed to help with the management of obesity and diabetes. We introduce\nan effective method for selecting a small set of texts for annotation and\nbuilding high-quality classifiers. We integrate vector search, semantic search,\nand machine learning classifiers to yield a good solution. Our experiments\ndemonstrate improved F1 scores for the minority classes in binary\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1\">Sareh Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Aditya Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1\">Edward Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection. (arXiv:2307.14912v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14912","description":"<p>Fanfiction, a popular form of creative writing set within established\nfictional universes, has gained a substantial online following. However,\nensuring the well-being and safety of participants has become a critical\nconcern in this community. The detection of triggering content, material that\nmay cause emotional distress or trauma to readers, poses a significant\nchallenge. In this paper, we describe our approach for the Trigger Detection\nshared task at PAN CLEF 2023, where we want to detect multiple triggering\ncontent in a given Fanfiction document. For this, we build a hierarchical model\nthat uses recurrence over Transformer-based language models. In our approach,\nwe first split long documents into smaller sized segments and use them to\nfine-tune a Transformer model. Then, we extract feature embeddings from the\nfine-tuned Transformer model, which are used as input in the training of\nmultiple LSTM models for trigger detection in a multi-label setting. Our model\nachieves an F1-macro score of 0.372 and F1-micro score of 0.736 on the\nvalidation set, which are higher than the baseline results shared at PAN CLEF\n2023.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahin_U/0/1/0/all/0/1\">Umitcan Sahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucukkaya_I/0/1/0/all/0/1\">Izzet Emre Kucukkaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toraman_C/0/1/0/all/0/1\">Cagri Toraman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARC-NLP at PAN 2023: Transition-Focused Natural Language Inference for Writing Style Detection. (arXiv:2307.14913v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14913","description":"<p>The task of multi-author writing style detection aims at finding any\npositions of writing style change in a given text document. We formulate the\ntask as a natural language inference problem where two consecutive paragraphs\nare paired. Our approach focuses on transitions between paragraphs while\ntruncating input tokens for the task. As backbone models, we employ different\nTransformer-based encoders with warmup phase during training. We submit the\nmodel version that outperforms baselines and other proposed model versions in\nour experiments. For the easy and medium setups, we submit transition-focused\nnatural language inference based on DeBERTa with warmup training, and the same\nmodel without transition for the hard setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kucukkaya_I/0/1/0/all/0/1\">Izzet Emre Kucukkaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_U/0/1/0/all/0/1\">Umitcan Sahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toraman_C/0/1/0/all/0/1\">Cagri Toraman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback. (arXiv:2307.14936v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14936","description":"<p>Large Language Models for Code (Code LLM) are flourishing. New and powerful\nmodels are released on a weekly basis, demonstrating remarkable performance on\nthe code generation task. Various approaches have been proposed to boost the\ncode generation performance of pre-trained Code LLMs, such as supervised\nfine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we\npropose a novel RRTF (Rank Responses to align Test&amp;Teacher Feedback) framework,\nwhich can effectively and efficiently boost pre-trained large language models\nfor code generation. Under this framework, we present PanGu-Coder2, which\nachieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through\nan extensive evaluation on CoderEval and LeetCode benchmarks, we show that\nPanGu-Coder2 consistently outperforms all previous Code LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bo Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Taihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_D/0/1/0/all/0/1\">Daoguang Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_B/0/1/0/all/0/1\">Bing Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1\">An Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Muhan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Ailun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jichuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jingyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuenan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianxiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs. (arXiv:2307.14988v1 [cs.LG])","link":"http://arxiv.org/abs/2307.14988","description":"<p>Deep learning often faces the challenge of efficiently processing dynamic\ninputs, such as sensor data or user inputs. For example, an AI writing\nassistant is required to update its suggestions in real time as a document is\nedited. Re-running the model each time is expensive, even with compression\ntechniques like knowledge distillation, pruning, or quantization. Instead, we\ntake an incremental computing approach, looking to reuse calculations as the\ninputs change. However, the dense connectivity of conventional architectures\nposes a major obstacle to incremental computation, as even minor input changes\ncascade through the network and restrict information reuse. To address this, we\nuse vector quantization to discretize intermediate values in the network, which\nfilters out noisy and unnecessary modifications to hidden neurons, facilitating\nthe reuse of their values. We apply this approach to the transformers\narchitecture, creating an efficient incremental inference algorithm with\ncomplexity proportional to the fraction of the modified inputs. Our experiments\nwith adapting the OPT-125M pre-trained language model demonstrate comparable\naccuracy on document classification while requiring 12.1X (median) fewer\noperations for processing sequences of atomic edits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharir_O/0/1/0/all/0/1\">Or Sharir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling TransNormer to 175 Billion Parameters. (arXiv:2307.14995v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14995","description":"<p>We present TransNormerLLM, the first linear attention-based Large Language\nModel (LLM) that outperforms conventional softmax attention-based models in\nterms of both accuracy and efficiency. TransNormerLLM evolves from the previous\nlinear attention architecture TransNormer by making advanced modifications that\ninclude positional embedding, linear attention acceleration, gating mechanism,\ntensor normalization, inference acceleration and stabilization. Specifically,\nwe use LRPE together with an exponential decay to avoid attention dilution\nissues while allowing the model to retain global interactions between tokens.\nAdditionally, we propose Lightning Attention, a cutting-edge technique that\naccelerates linear attention by more than twice in runtime and reduces memory\nusage by a remarkable four times. To further enhance the performance of\nTransNormer, we leverage a gating mechanism to smooth training and a new tensor\nnormalization scheme to accelerate the model, resulting in an impressive\nacceleration of over 20%. Furthermore, we have developed a robust inference\nalgorithm that ensures numerical stability and consistent inference speed,\nregardless of the sequence length, showcasing superior efficiency during both\ntraining and inference stages. Scalability is at the heart of our model's\ndesign, enabling seamless deployment on large-scale clusters and facilitating\nexpansion to even more extensive models, all while maintaining outstanding\nperformance metrics. Rigorous validation of our model design is achieved\nthrough a series of comprehensive experiments on our self-collected corpus,\nboasting a size exceeding 6TB and containing over 2 trillion tokens. To ensure\ndata quality and relevance, we implement a new self-cleaning strategy to filter\nour collected data. Our pre-trained models will be released to foster community\nadvancements in efficient LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weigao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weixuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuyang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaodong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunshen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_B/0/1/0/all/0/1\">Baohong Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1\">Fei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gzip versus bag-of-words for text classification with KNN. (arXiv:2307.15002v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15002","description":"<p>The effectiveness of compression distance in KNN-based text classification\n('gzip') has recently garnered lots of attention. In this note, we show that\nsimilar or better effectiveness can be achieved with simpler means, and text\ncompression may not be necessary. Indeed, we find that a simple 'bag-of-words'\nmatching can achieve similar or better accuracy, and is more efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1\">Juri Opitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark. (arXiv:2307.15020v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15020","description":"<p>Large language models (LLMs) have shown the potential to be integrated into\nhuman daily lives. Therefore, user preference is the most critical criterion\nfor assessing LLMs' performance in real-world scenarios. However, existing\nbenchmarks mainly focus on measuring models' accuracy using multi-choice\nquestions, which limits the understanding of their capabilities in real\napplications. We fill this gap by proposing a comprehensive Chinese benchmark\nSuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE\nencompasses three sub-tasks: actual users' queries and ratings derived from an\nLLM battle platform (CArena), open-ended questions with single and\nmultiple-turn dialogues (OPEN), and closed-ended questions with the same stems\nas open-ended single-turn ones (CLOSE). Our study shows that accuracy on\nclosed-ended questions is insufficient to reflect human preferences achieved on\nopen-ended ones. At the same time, they can complement each other to predict\nactual user preferences. We also demonstrate that GPT-4 is a reliable judge to\nautomatically evaluate human preferences on open-ended questions in a Chinese\ncontext. Our benchmark will be released at https://www.CLUEbenchmarks.com\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Anqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hang Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Changtai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kangkang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haonan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Q/0/1/0/all/0/1\">Qiyue Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhenzhong Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal and Transferable Adversarial Attacks on Aligned Language Models. (arXiv:2307.15043v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15043","description":"<p>Because \"out-of-the-box\" large language models are capable of generating a\ngreat deal of objectionable content, recent work has focused on aligning these\nmodels in an attempt to prevent undesirable generation. While there has been\nsome success at circumventing these measures -- so-called \"jailbreaks\" against\nLLMs -- these attacks have required significant human ingenuity and are brittle\nin practice. In this paper, we propose a simple and effective attack method\nthat causes aligned language models to generate objectionable behaviors.\nSpecifically, our approach finds a suffix that, when attached to a wide range\nof queries for an LLM to produce objectionable content, aims to maximize the\nprobability that the model produces an affirmative response (rather than\nrefusing to answer). However, instead of relying on manual engineering, our\napproach automatically produces these adversarial suffixes by a combination of\ngreedy and gradient-based search techniques, and also improves over past\nautomatic prompt generation methods.\n</p>\n<p>Surprisingly, we find that the adversarial prompts generated by our approach\nare quite transferable, including to black-box, publicly released LLMs.\nSpecifically, we train an adversarial attack suffix on multiple prompts (i.e.,\nqueries asking for many different types of objectionable content), as well as\nmultiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting\nattack suffix is able to induce objectionable content in the public interfaces\nto ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,\nPythia, Falcon, and others. In total, this work significantly advances the\nstate-of-the-art in adversarial attacks against aligned language models,\nraising important questions about how such systems can be prevented from\nproducing objectionable information. Code is available at\ngithub.com/llm-attacks/llm-attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Andy Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1\">J. Zico Kolter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1\">Matt Fredrikson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15051","description":"<p>Clinical trials are vital in advancing drug development and evidence-based\nmedicine, but their success is often hindered by challenges in patient\nrecruitment. In this work, we investigate the potential of large language\nmodels (LLMs) to assist individual patients and referral physicians in\nidentifying suitable clinical trials from an extensive selection. Specifically,\nwe introduce TrialGPT, a novel architecture employing LLMs to predict\ncriterion-level eligibility with detailed explanations, which are then\naggregated for ranking and excluding candidate clinical trials based on\nfree-text patient notes. We evaluate TrialGPT on three publicly available\ncohorts of 184 patients and 18,238 annotated clinical trials. The experimental\nresults demonstrate several key findings: First, TrialGPT achieves high\ncriterion-level prediction accuracy with faithful explanations. Second, the\naggregated trial-level TrialGPT scores are highly correlated with expert\neligibility annotations. Third, these scores prove effective in ranking\nclinical trials and exclude ineligible candidates. Our error analysis suggests\nthat current LLMs still make some mistakes due to limited medical knowledge and\ndomain-specific context understanding. Nonetheless, we believe the explanatory\ncapabilities of LLMs are highly valuable. Future research is warranted on how\nsuch AI assistants can be integrated into the routine trial matching workflow\nin real-world settings to improve its efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Floudas_C/0/1/0/all/0/1\">Charalampos S. Floudas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Geometric Notion of Causal Probing. (arXiv:2307.15054v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15054","description":"<p>Large language models rely on real-valued representations of text to make\ntheir predictions. These representations contain information learned from the\ndata that the model has trained on, including knowledge of linguistic\nproperties and forms of demographic bias, e.g., based on gender. A growing body\nof work has considered information about concepts such as these using\northogonal projections onto subspaces of the representation space. We\ncontribute to this body of work by proposing a formal definition of intrinsic\ninformation in a subspace of a language model's representation space. We\npropose a counterfactual approach that avoids the failure mode of spurious\ncorrelations (Kumar et al., 2022) by treating components in the subspace and\nits orthogonal complement independently. We show that our counterfactual notion\nof information in a subspace is optimizing by an causal concept subspace.\nFurthermore, this intervention allows us to attempt concept controlled\ngeneration by manipulating the value of the conceptual component of a\nrepresentation. Empirically, we find that R-LACE (Ravfogel et al., 2022)\nreturns a one-dimensional subspace containing roughly half of total concept\ninformation under our framework. Our causal controlled intervention shows that,\nfor at least one model, the subspace returned by R-LACE can be used to\nmanipulate the concept value of the generated word with precision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guerner_C/0/1/0/all/0/1\">Cl&#xe9;ment Guerner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svete_A/0/1/0/all/0/1\">Anej Svete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1\">Alexander Warstadt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Subset Pruning of Transformer Heads. (arXiv:2108.04657v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04657","description":"<p>Multi-head attention, a collection of several attention mechanisms that\nindependently attend to different parts of the input, is the key ingredient in\nthe Transformer. Recent work has shown, however, that a large proportion of the\nheads in a Transformer's multi-head attention mechanism can be safely pruned\naway without significantly harming the performance of the model; such pruning\nleads to models that are noticeably smaller and faster in practice. Our work\nintroduces a new head pruning technique that we term differentiable subset\npruning. Intuitively, our method learns per-head importance variables and then\nenforces a user-specified hard constraint on the number of unpruned heads. The\nimportance variables are learned via stochastic gradient descent. We conduct\nexperiments on natural language inference and machine translation; we show that\ndifferentiable subset pruning performs comparably or better than previous works\nwhile offering precise control of the sparsity level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaoda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14704","description":"<p>Prompt learning approaches have made waves in natural language processing by\ninducing better few-shot performance while they still follow a parametric-based\nlearning paradigm; the oblivion and rote memorization problems in learning may\nencounter unstable generalization issues. Specifically, vanilla prompt learning\nmay struggle to utilize atypical instances by rote during fully-supervised\ntraining or overfit shallow patterns with low-shot data. To alleviate such\nlimitations, we develop RetroPrompt with the motivation of decoupling knowledge\nfrom memorization to help the model strike a balance between generalization and\nmemorization. In contrast with vanilla prompt learning, RetroPrompt constructs\nan open-book knowledge-store from training instances and implements a retrieval\nmechanism during the process of input, training and inference, thus equipping\nthe model with the ability to retrieve related contexts from the training\ncorpus as cues for enhancement. Extensive experiments demonstrate that\nRetroPrompt can obtain better performance in both few-shot and zero-shot\nsettings. Besides, we further illustrate that our proposed RetroPrompt can\nyield better generalization abilities with new datasets. Detailed analysis of\nmemorization indeed reveals RetroPrompt can reduce the reliance of language\nmodels on memorization; thus, improving generalization for downstream tasks.\nCode is available in\nhttps://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Struggle to Learn Long-Tail Knowledge. (arXiv:2211.08411v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08411","description":"<p>The Internet contains a wealth of knowledge -- from the birthdays of\nhistorical figures to tutorials on how to code -- all of which may be learned\nby language models. However, while certain pieces of information are ubiquitous\non the web, others appear extremely rarely. In this paper, we study the\nrelationship between the knowledge memorized by large language models and the\ninformation in pre-training datasets scraped from the web. In particular, we\nshow that a language model's ability to answer a fact-based question relates to\nhow many documents associated with that question were seen during pre-training.\nWe identify these relevant documents by entity linking pre-training datasets\nand counting documents that contain the same entities as a given\nquestion-answer pair. Our results demonstrate strong correlational and causal\nrelationships between accuracy and relevant document count for numerous\nquestion answering datasets (e.g., TriviaQA), pre-training corpora (e.g.,\nROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models\nare better at learning long-tail knowledge, we estimate that today's models\nmust be scaled by many orders of magnitude to reach competitive QA performance\non questions with little support in the pre-training data. Finally, we show\nthat retrieval-augmentation can reduce the dependence on relevant pre-training\ninformation, presenting a promising approach for capturing the long-tail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kandpal_N/0/1/0/all/0/1\">Nikhil Kandpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Haikang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1\">Eric Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11596","description":"<p>Large language models (LLMs) such as GPT-4 have recently demonstrated\nimpressive results across a wide range of tasks. LLMs are still limited,\nhowever, in that they frequently fail at complex reasoning, their reasoning\nprocesses are opaque, they are prone to 'hallucinate' facts, and there are\nconcerns about their underlying biases. Letting models verbalize reasoning\nsteps as natural language, a technique known as chain-of-thought prompting, has\nrecently been proposed as a way to address some of these issues. Here we\npresent ThoughtSource, a meta-dataset and software library for chain-of-thought\n(CoT) reasoning. The goal of ThoughtSource is to improve future artificial\nintelligence systems by facilitating qualitative understanding of CoTs,\nenabling empirical evaluations, and providing training data. This first release\nof ThoughtSource integrates seven scientific/medical, three general-domain and\nfive math word question answering datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ott_S/0/1/0/all/0/1\">Simon Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebenstreit_K/0/1/0/all/0/1\">Konstantin Hebenstreit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lievin_V/0/1/0/all/0/1\">Valentin Li&#xe9;vin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hother_C/0/1/0/all/0/1\">Christoffer Egeberg Hother</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Milad Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayrhauser_M/0/1/0/all/0/1\">Maximilian Mayrhauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Praas_R/0/1/0/all/0/1\">Robert Praas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Retrieval-Augmented Language Models. (arXiv:2302.00083v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00083","description":"<p>Retrieval-Augmented Language Modeling (RALM) methods, which condition a\nlanguage model (LM) on relevant documents from a grounding corpus during\ngeneration, were shown to significantly improve language modeling performance.\nIn addition, they can mitigate the problem of factually inaccurate text\ngeneration and provide natural source attribution mechanism. Existing RALM\napproaches focus on modifying the LM architecture in order to facilitate the\nincorporation of external information, significantly complicating deployment.\nThis paper considers a simple alternative, which we dub In-Context RALM:\nleaving the LM architecture unchanged and prepending grounding documents to the\ninput, without any further training of the LM. We show that In-Context RALM\nthat builds on off-the-shelf general purpose retrievers provides surprisingly\nlarge LM gains across model sizes and diverse corpora. We also demonstrate that\nthe document retrieval and ranking mechanism can be specialized to the RALM\nsetting to further boost performance. We conclude that In-Context RALM has\nconsiderable potential to increase the prevalence of LM grounding, particularly\nin settings where a pretrained LM must be used without modification or even via\nAPI access.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1\">Ori Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmedigos_I/0/1/0/all/0/1\">Itay Dalmedigos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhlgay_D/0/1/0/all/0/1\">Dor Muhlgay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leyton_Brown_K/0/1/0/all/0/1\">Kevin Leyton-Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoham_Y/0/1/0/all/0/1\">Yoav Shoham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Confidence-based Partial Label Learning Model for Crowd-Annotated Named Entity Recognition. (arXiv:2305.12485v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12485","description":"<p>Existing models for named entity recognition (NER) are mainly based on\nlarge-scale labeled datasets, which always obtain using crowdsourcing. However,\nit is hard to obtain a unified and correct label via majority voting from\nmultiple annotators for NER due to the large labeling space and complexity of\nthis task. To address this problem, we aim to utilize the original\nmulti-annotator labels directly. Particularly, we propose a Confidence-based\nPartial Label Learning (CPLL) method to integrate the prior confidence (given\nby annotators) and posterior confidences (learned by models) for\ncrowd-annotated NER. This model learns a token- and content-dependent\nconfidence via an Expectation-Maximization (EM) algorithm by minimizing\nempirical risk. The true posterior estimator and confidence estimator perform\niteratively to update the true posterior and confidence respectively. We\nconduct extensive experimental results on both real-world and synthetic\ndatasets, which show that our model can improve performance effectively\ncompared with strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Limao Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qunxi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanbin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond. (arXiv:2305.15299v3 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2305.15299","description":"<p>Large language models of artificial intelligence (AI), such as ChatGPT, find\nremarkable but controversial applicability in science and research. This paper\nreviews epistemological challenges, ethical and integrity risks in science\nconduct in the advent of generative AI. This is with the aim to lay new timely\nfoundations for a high-quality research ethics review. The role of AI language\nmodels as a research instrument and subject is scrutinized along with ethical\nimplications for scientists, participants and reviewers. New emerging practices\nfor research ethics review are discussed, concluding with ten recommendations\nthat shape a response for a more responsible research conduct in the era of AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pournaras_E/0/1/0/all/0/1\">Evangelos Pournaras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-aware attention layers coupled with optimal transport domain adaptation and multimodal fusion methods for recognizing dementia from spontaneous speech. (arXiv:2305.16406v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16406","description":"<p>Alzheimer's disease (AD) constitutes a complex neurocognitive disease and is\nthe main cause of dementia. Although many studies have been proposed targeting\nat diagnosing dementia through spontaneous speech, there are still limitations.\nExisting state-of-the-art approaches, which propose multimodal methods, train\nseparately language and acoustic models, employ majority-vote approaches, and\nconcatenate the representations of the different modalities either at the input\nlevel, i.e., early fusion, or during training. Also, some of them employ\nself-attention layers, which calculate the dependencies between representations\nwithout considering the contextual information. In addition, no prior work has\ntaken into consideration the model calibration. To address these limitations,\nwe propose some new methods for detecting AD patients, which capture the intra-\nand cross-modal interactions. First, we convert the audio files into log-Mel\nspectrograms, their delta, and delta-delta and create in this way an image per\naudio file consisting of three channels. Next, we pass each transcript and\nimage through BERT and DeiT models respectively. After that, context-based\nself-attention layers, self-attention layers with a gate model, and optimal\ntransport domain adaptation methods are employed for capturing the intra- and\ninter-modal interactions. Finally, we exploit two methods for fusing the self\nand cross-attention features. For taking into account the model calibration, we\napply label smoothing. We use both performance and calibration metrics.\nExperiments conducted on the ADReSS and ADReSSo Challenge datasets indicate the\nefficacy of our introduced approaches over existing research initiatives with\nour best performing model reaching Accuracy and F1-score up to 91.25% and\n91.06% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilias_L/0/1/0/all/0/1\">Loukas Ilias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1\">Dimitris Askounis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Emotion Experiencer Recognition. (arXiv:2305.16731v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16731","description":"<p>The most prominent subtask in emotion analysis is emotion classification; to\nassign a category to a textual unit, for instance a social media post. Many\nresearch questions from the social sciences do, however, not only require the\ndetection of the emotion of an author of a post but to understand who is\nascribed an emotion in text. This task is tackled by emotion role labeling\nwhich aims at extracting who is described in text to experience an emotion,\nwhy, and towards whom. This could, however, be considered overly sophisticated\nif the main question to answer is who feels which emotion. A targeted approach\nfor such setup is to classify emotion experiencer mentions (aka \"emoters\")\nregarding the emotion they presumably perceive. This task is similar to named\nentity recognition of person names with the difference that not every mentioned\nentity name is an emoter. While, very recently, data with emoter annotations\nhas been made available, no experiments have yet been performed to detect such\nmentions. With this paper, we provide baseline experiments to understand how\nchallenging the task is. We further evaluate the impact on experiencer-specific\nemotion categorization and appraisal detection in a pipeline, when gold\nmentions are not available. We show that experiencer detection in text is a\nchallenging task, with a precision of .82 and a recall of .56 (F1 =.66). These\nresults motivate future work of jointly modeling emoter spans and\nemotion/appraisal predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wegge_M/0/1/0/all/0/1\">Maximilian Wegge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19472","description":"<p>Procedural planning, which entails decomposing a high-level goal into a\nsequence of temporally ordered steps, is an important yet intricate task for\nmachines. It involves integrating common-sense knowledge to reason about\ncomplex contextualized situations that are often counterfactual, e.g.\n\"scheduling a doctor's appointment without a phone\". While current approaches\nshow encouraging results using large language models (LLMs), they are hindered\nby drawbacks such as costly API calls and reproducibility issues. In this\npaper, we advocate planning using smaller language models. We present PlaSma, a\nnovel two-pronged approach to endow small language models with procedural\nknowledge and (counterfactual) planning capabilities. More concretely, we\ndevelop symbolic procedural knowledge distillation to enhance the implicit\nknowledge in small language models and an inference-time algorithm to\nfacilitate more structured and accurate reasoning. In addition, we introduce a\nnovel task, Counterfactual Planning, that requires a revision of a plan to cope\nwith a counterfactual situation. In both the original and counterfactual\nsetting, we show that orders-of-magnitude smaller models (770M-11B parameters)\ncan compete and often surpass their larger teacher models' capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1\">Valentina Pyatkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lorraine Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arai_H/0/1/0/all/0/1\">Hirona J. Arai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00017","description":"<p>Large language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saba_W/0/1/0/all/0/1\">Walid S. Saba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets. (arXiv:2307.00610v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2307.00610","description":"<p>The option of sharing images, videos and audio files on social media opens up\nnew possibilities for distinguishing between false information and fake news on\nthe Internet. Due to the vast amount of data shared every second on social\nmedia, not all data can be verified by a computer or a human expert. Here, a\ncheck-worthiness analysis can be used as a first step in the fact-checking\npipeline and as a filtering mechanism to improve efficiency. This paper\nproposes a novel way of detecting the check-worthiness in multi-modal tweets.\nIt takes advantage of two classifiers, each trained on a single modality. For\nimage data, extracting the embedded text with an OCR analysis has shown to\nperform best. By combining the two classifiers, the proposed solution was able\nto place first in the CheckThat! 2023 Task 1A with an F1 score of 0.7297\nachieved on the private test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frick_R/0/1/0/all/0/1\">Raphael Frick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogel_I/0/1/0/all/0/1\">Inna Vogel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining Clues from Incomplete Utterance: A Query-enhanced Network for Incomplete Utterance Rewriting. (arXiv:2307.00866v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.00866","description":"<p>Incomplete utterance rewriting has recently raised wide attention. However,\nprevious works do not consider the semantic structural information between\nincomplete utterance and rewritten utterance or model the semantic structure\nimplicitly and insufficiently. To address this problem, we propose a\nQUEry-Enhanced Network (QUEEN). Firstly, our proposed query template explicitly\nbrings guided semantic structural knowledge between the incomplete utterance\nand the rewritten utterance making model perceive where to refer back to or\nrecover omitted tokens. Then, we adopt a fast and effective edit operation\nscoring network to model the relation between two tokens. Benefiting from\nproposed query template and the well-designed edit operation scoring network,\nQUEEN achieves state-of-the-art performance on several public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shuzheng Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shuang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fraunhofer SIT at CheckThat! 2023: Tackling Classification Uncertainty Using Model Souping on the Example of Check-Worthiness Classification. (arXiv:2307.02377v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.02377","description":"<p>This paper describes the second-placed approach developed by the Fraunhofer\nSIT team in the CLEF-2023 CheckThat! lab Task 1B for English. Given a text\nsnippet from a political debate, the aim of this task is to determine whether\nit should be assessed for check-worthiness. Detecting check-worthy statements\naims to facilitate manual fact-checking efforts by prioritizing the claims that\nfact-checkers should consider first. It can also be considered as primary step\nof a fact-checking system. Our best-performing method took advantage of an\nensemble classification scheme centered on Model Souping. When applied to the\nEnglish data set, our submitted model achieved an overall F1 score of 0.878 and\nwas ranked as the second-best model in the competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frick_R/0/1/0/all/0/1\">Raphael Frick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogel_I/0/1/0/all/0/1\">Inna Vogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jeong-Eun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a Good Personality Recognizer? A Preliminary Study. (arXiv:2307.03952v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.03952","description":"<p>In recent years, personality has been regarded as a valuable personal factor\nbeing incorporated into numerous tasks such as sentiment analysis and product\nrecommendation. This has led to widespread attention to text-based personality\nrecognition task, which aims to identify an individual's personality based on\ngiven text. Considering that ChatGPT has recently exhibited remarkable\nabilities on various natural language processing tasks, we provide a\npreliminary evaluation of ChatGPT on text-based personality recognition task\nfor generating effective personality data. Concretely, we employ a variety of\nprompting strategies to explore ChatGPT's ability in recognizing personality\nfrom given text, especially the level-oriented prompting strategy we designed\nfor guiding ChatGPT in analyzing given text at a specified level. The\nexperimental results on two representative real-world datasets reveal that\nChatGPT with zero-shot chain-of-thought prompting exhibits impressive\npersonality recognition ability and is capable to provide natural language\nexplanations through text-based logical reasoning. Furthermore, by employing\nthe level-oriented prompting strategy to optimize zero-shot chain-of-thought\nprompting, the performance gap between ChatGPT and corresponding\nstate-of-the-art model has been narrowed even more. However, we observe that\nChatGPT shows unfairness towards certain sensitive demographic attributes such\nas gender and age. Additionally, we discover that eliciting the personality\nrecognition ability of ChatGPT helps improve its performance on\npersonality-related downstream tasks such as sentiment classification and\nstress prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yu Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RRAML: Reinforced Retrieval Augmented Machine Learning. (arXiv:2307.12798v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.12798","description":"<p>The emergence of large language models (LLMs) has revolutionized machine\nlearning and related fields, showcasing remarkable abilities in comprehending,\ngenerating, and manipulating human language. However, their conventional usage\nthrough API-based text prompt submissions imposes certain limitations in terms\nof context constraints and external source availability. To address these\nchallenges, we propose a novel framework called Reinforced Retrieval Augmented\nMachine Learning (RRAML). RRAML integrates the reasoning capabilities of LLMs\nwith supporting information retrieved by a purpose-built retriever from a vast\nuser-provided database. By leveraging recent advancements in reinforcement\nlearning, our method effectively addresses several critical challenges.\nFirstly, it circumvents the need for accessing LLM gradients. Secondly, our\nmethod alleviates the burden of retraining LLMs for specific tasks, as it is\noften impractical or impossible due to restricted access to the model and the\ncomputational intensity involved. Additionally we seamlessly link the\nretriever's task with the reasoner, mitigating hallucinations and reducing\nirrelevant, and potentially damaging retrieved documents. We believe that the\nresearch agenda outlined in this paper has the potential to profoundly impact\nthe field of AI, democratizing access to and utilization of LLMs for a wide\nrange of entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_A/0/1/0/all/0/1\">Andrea Bacciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuconasu_F/0/1/0/all/0/1\">Florin Cuconasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siciliano_F/0/1/0/all/0/1\">Federico Siciliano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Fabrizio Silvestri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonellotto_N/0/1/0/all/0/1\">Nicola Tonellotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trappolini_G/0/1/0/all/0/1\">Giovanni Trappolini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empower Your Model with Longer and Better Context Comprehension. (arXiv:2307.13365v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.13365","description":"<p>Recently, with the emergence of numerous Large Language Models (LLMs), the\nimplementation of AI has entered a new era. Irrespective of these models' own\ncapacity and structure, there is a growing demand for LLMs to possess enhanced\ncomprehension of longer and more complex contexts with relatively smaller\nsizes. Models often encounter an upper limit when processing sequences of\nsentences that extend beyond their comprehension capacity and result in\noff-topic or even chaotic responses. While several recent works attempt to\naddress this issue in various ways, they rarely focus on \"why models are unable\nto compensate or strengthen their capabilities on their own\". In this paper, we\nthoroughly investigate the nature of information transfer within LLMs and\npropose a novel technique called Attention Transition. This technique empowers\nmodels to achieve longer and better context comprehension with minimal\nadditional training or impact on generation fluency. Our experiments are\nconducted on the challenging XSum dataset using LLaMa-7b model with context\ntoken length ranging from 800 to 1900. Results demonstrate that we achieve\nsubstantial improvements compared with the original generation results\nevaluated by GPT4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yifei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jun Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Longhua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Large Language Models for Radiology Natural Language Processing. (arXiv:2307.13693v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.13693","description":"<p>The rise of large language models (LLMs) has marked a pivotal shift in the\nfield of natural language processing (NLP). LLMs have revolutionized a\nmultitude of domains, and they have made a significant impact in the medical\nfield. Large language models are now more abundant than ever, and many of these\nmodels exhibit bilingual capabilities, proficient in both English and Chinese.\nHowever, a comprehensive evaluation of these models remains to be conducted.\nThis lack of assessment is especially apparent within the context of radiology\nNLP. This study seeks to bridge this gap by critically evaluating thirty two\nLLMs in interpreting radiology reports, a crucial component of radiology NLP.\nSpecifically, the ability to derive impressions from radiologic findings is\nassessed. The outcomes of this evaluation provide key insights into the\nperformance, strengths, and weaknesses of these LLMs, informing their practical\napplications within the medical domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1\">Tianyang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yutong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zihao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1\">Peixin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Chao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_P/0/1/0/all/0/1\">Peng Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yaonai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zuowei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holmes_J/0/1/0/all/0/1\">Jason Holmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shaochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haixing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_B/0/1/0/all/0/1\">Bao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xiaoyan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xintao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shijie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Quanzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongtu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition. (arXiv:2307.14132v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2307.14132","description":"<p>RNN-T models are widely used in ASR, which rely on the RNN-T loss to achieve\nlength alignment between input audio and target sequence. However, the\nimplementation complexity and the alignment-based optimization target of RNN-T\nloss lead to computational redundancy and a reduced role for predictor network,\nrespectively. In this paper, we propose a novel model named CIF-Transducer\n(CIF-T) which incorporates the Continuous Integrate-and-Fire (CIF) mechanism\nwith the RNN-T model to achieve efficient alignment. In this way, the RNN-T\nloss is abandoned, thus bringing a computational reduction and allowing the\npredictor network a more significant role. We also introduce Funnel-CIF,\nContext Blocks, Unified Gating and Bilinear Pooling joint network, and\nauxiliary training strategy to further improve performance. Experiments on the\n178-hour AISHELL-1 and 10000-hour WenetSpeech datasets show that CIF-T achieves\nstate-of-the-art results with lower computational overhead compared to RNN-T\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tian-Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dinghao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_G/0/1/0/all/0/1\">Guiping Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baoxiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-07-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}