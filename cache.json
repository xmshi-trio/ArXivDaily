{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Radically Lower Data-Labeling Costs for Visually Rich Document Extraction Models. (arXiv:2210.16391v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16391","description":"<p>A key bottleneck in building automatic extraction models for visually rich\ndocuments like invoices is the cost of acquiring the several thousand\nhigh-quality labeled documents that are needed to train a model with acceptable\naccuracy. We propose Selective Labeling to simplify the labeling task to\nprovide \"yes/no\" labels for candidate extractions predicted by a model trained\non partially labeled documents. We combine this with a custom active learning\nstrategy to find the predictions that the model is most uncertain about. We\nshow through experiments on document types drawn from 3 different domains that\nselective labeling can reduce the cost of acquiring labeled data by $10\\times$\nwith a negligible loss in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wendt_J/0/1/0/all/0/1\">James B. Wendt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potti_N/0/1/0/all/0/1\">Navneet Potti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tata_S/0/1/0/all/0/1\">Sandeep Tata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"System Demo: Tool and Infrastructure for Offensive Language Error Analysis (OLEA) in English. (arXiv:2210.16398v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16398","description":"<p>The automatic detection of offensive language is a pressing societal need.\nMany systems perform well on explicit offensive language but struggle to detect\nmore complex, nuanced, or implicit cases of offensive and hateful language.\nOLEA is an open-source Python library that provides easy-to-use tools for error\nanalysis in the context of detecting offensive language in English. OLEA also\nprovides an infrastructure for re-distribution of new datasets and analysis\nmethods requiring very little coding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grace_M/0/1/0/all/0/1\">Marie Grace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seabrum_X/0/1/0/all/0/1\">Xajavion &quot;Jay&quot; Seabrum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivas_D/0/1/0/all/0/1\">Dananjay Srinivas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_A/0/1/0/all/0/1\">Alexis Palmer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just-DREAM-about-it: Figurative Language Understanding with DREAM-FLUTE. (arXiv:2210.16407v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16407","description":"<p>Figurative language (e.g., \"he flew like the wind\") is challenging to\nunderstand, as it is hard to tell what implicit information is being conveyed\nfrom the surface form alone. We hypothesize that to perform this task well, the\nreader needs to mentally elaborate the scene being described to identify a\nsensible meaning of the language. We present DREAM-FLUTE, a figurative language\nunderstanding system that does this, first forming a \"mental model\" of\nsituations described in a premise and hypothesis before making an\nentailment/contradiction decision and generating an explanation. DREAM-FLUTE\nuses an existing scene elaboration model, DREAM, for constructing its \"mental\nmodel.\" In the FigLang2022 Shared Task evaluation, DREAM-FLUTE achieved (joint)\nfirst place (Acc@60=63.3%), and can perform even better with ensemble\ntechniques, demonstrating the effectiveness of this approach. More generally,\nthis work suggests that adding a reflective component to pretrained language\nmodels can improve their performance beyond standard fine-tuning (3.3%\nimprovement in Acc@60).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuling Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1\">Valentina Pyatkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magnusson_I/0/1/0/all/0/1\">Ian Magnusson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1\">Bhavana Dalvi Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Unifying Text Segmentation and Long Document Summarization. (arXiv:2210.16422v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16422","description":"<p>Text segmentation is important for signaling a document's structure. Without\nsegmenting a long document into topically coherent sections, it is difficult\nfor readers to comprehend the text, let alone find important information. The\nproblem is only exacerbated by a lack of segmentation in transcripts of\naudio/video recordings. In this paper, we explore the role that section\nsegmentation plays in extractive summarization of written and spoken documents.\nOur approach learns robust sentence representations by performing summarization\nand segmentation simultaneously, which is further enhanced by an\noptimization-based regularizer to promote selection of diverse summary\nsentences. We conduct experiments on multiple datasets ranging from scientific\narticles to spoken transcripts to evaluate the model's performance. Our\nfindings suggest that the model can not only achieve state-of-the-art\nperformance on publicly available benchmarks, but demonstrate better\ncross-genre transferability when equipped with text segmentation. We perform a\nseries of analyses to quantify the impact of section segmentation on\nsummarizing written and spoken documents of substantial length and complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sangwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaiqiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiMBERT: Learning Vision-Language Grounded Representations with Disentangled Multimodal-Attention. (arXiv:2210.16431v1 [cs.CV])","link":"http://arxiv.org/abs/2210.16431","description":"<p>Vision-and-language (V-L) tasks require the system to understand both vision\ncontent and natural language, thus learning fine-grained joint representations\nof vision and language (a.k.a. V-L representations) is of paramount importance.\nRecently, various pre-trained V-L models are proposed to learn V-L\nrepresentations and achieve improved results in many tasks. However, the\nmainstream models process both vision and language inputs with the same set of\nattention matrices. As a result, the generated V-L representations are\nentangled in one common latent space. To tackle this problem, we propose\nDiMBERT (short for Disentangled Multimodal-Attention BERT), which is a novel\nframework that applies separated attention spaces for vision and language, and\nthe representations of multi-modalities can thus be disentangled explicitly. To\nenhance the correlation between vision and language in disentangled spaces, we\nintroduce the visual concepts to DiMBERT which represent visual information in\ntextual format. In this manner, visual concepts help to bridge the gap between\nthe two modalities. We pre-train DiMBERT on a large amount of image-sentence\npairs on two tasks: bidirectional language modeling and sequence-to-sequence\nlanguage modeling. After pre-train, DiMBERT is further fine-tuned for the\ndownstream tasks. Experiments show that DiMBERT sets new state-of-the-art\nperformance on three tasks (over four datasets), including both generation\ntasks (image captioning and visual storytelling) and classification tasks\n(referring expressions). The proposed DiM (short for Disentangled\nMultimodal-Attention) module can be easily incorporated into existing\npre-trained V-L models to boost their performance, up to a 5% increase on the\nrepresentative task. Finally, we conduct a systematic analysis and demonstrate\nthe effectiveness of our DiM and the introduced visual concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models. (arXiv:2210.16433v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16433","description":"<p>Fully-parametric language models generally require a huge number of model\nparameters to store the necessary knowledge for solving multiple natural\nlanguage tasks in zero/few-shot settings. In addition, it is hard to adapt to\nthe evolving world knowledge without the costly model re-training. In this\npaper, we develop a novel semi-parametric language model architecture,\nKnowledge-in-Context (KiC), which empowers a parametric text-to-text language\nmodel with a knowledge-rich external memory. Specifically, the external memory\ncontains six different types of knowledge: entity, dictionary, commonsense,\nevent, script, and causality knowledge. For each input instance, the KiC model\nadaptively selects a knowledge type and retrieves the most helpful pieces of\nknowledge. The input instance along with its knowledge augmentation is fed into\na text-to-text model (e.g., T5) to generate the output answer, where both the\ninput and the output are in natural language forms after prompting.\nInterestingly, we find that KiC can be identified as a special\nmixture-of-experts (MoE) model, where the knowledge selector plays the role of\na router that is used to determine the sequence-to-expert assignment in MoE.\nThis key observation inspires us to develop a novel algorithm for training KiC\nwith an instance-adaptive knowledge selector. As a knowledge-rich\nsemi-parametric language model, KiC only needs a much smaller parametric part\nto achieve superior zero-shot performance on unseen tasks. By evaluating on 40+\ndifferent tasks, we show that KiC_Large with 770M parameters easily outperforms\nlarge language models (LMs) that are 4-39x larger by a large margin. We also\ndemonstrate that KiC exhibits emergent abilities at a much smaller model scale\ncompared to the fully-parametric models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiaoman Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Classification of Code-Switched Text using Pre-trained Multilingual Embeddings and Segmentation. (arXiv:2210.16461v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16461","description":"<p>With increasing globalization and immigration, various studies have estimated\nthat about half of the world population is bilingual. Consequently, individuals\nconcurrently use two or more languages or dialects in casual conversational\nsettings. However, most research is natural language processing is focused on\nmonolingual text. To further the work in code-switched sentiment analysis, we\npropose a multi-step natural language processing algorithm utilizing points of\ncode-switching in mixed text and conduct sentiment analysis around those\nidentified points. The proposed sentiment analysis algorithm uses semantic\nsimilarity derived from large pre-trained multilingual models with a\nhandcrafted set of positive and negative words to determine the polarity of\ncode-switched text. The proposed approach outperforms a comparable baseline\nmodel by 11.2% for accuracy and 11.64% for F1-score on a Spanish-English\ndataset. Theoretically, the proposed algorithm can be expanded for sentiment\nanalysis of multiple languages with limited human expertise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aryal_S/0/1/0/all/0/1\">Saurav K. Aryal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prioleau_H/0/1/0/all/0/1\">Howard Prioleau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_G/0/1/0/all/0/1\">Gloria Washington</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating RNN-T Training and Inference Using CTC guidance. (arXiv:2210.16481v1 [eess.AS])","link":"http://arxiv.org/abs/2210.16481","description":"<p>We propose a novel method to accelerate training and inference process of\nrecurrent neural network transducer (RNN-T) based on the guidance from a\nco-trained connectionist temporal classification (CTC) model. We made a key\nassumption that if an encoder embedding frame is classified as a blank frame by\nthe CTC model, it is likely that this frame will be aligned to blank for all\nthe partial alignments or hypotheses in RNN-T and it can be discarded from the\ndecoder input. We also show that this frame reduction operation can be applied\nin the middle of the encoder, which result in significant speed up for the\ntraining and inference in RNN-T. We further show that the CTC alignment, a\nby-product of the CTC decoder, can also be used to perform lattice reduction\nfor RNN-T during training. Our method is evaluated on the Librispeech and\nSpeechStew tasks. We demonstrate that the proposed method is able to accelerate\nthe RNN-T inference by 2.2 times with similar or slightly better word error\nrates (WER).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yongqiang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuai Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_C/0/1/0/all/0/1\">Chengjian Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haghani_P/0/1/0/all/0/1\">Parisa Haghani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STPrompt: Semantic-guided and Task-driven prompts for Effective Few-shot Classification. (arXiv:2210.16489v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16489","description":"<p>The effectiveness of prompt learning has been demonstrated in different\npre-trained language models. By formulating suitable template and choosing\nrepresentative label mapping, prompt learning can be used as an efficient\nknowledge probe. However, finding suitable prompt in existing methods requires\nmultiple experimental attempts or appropriate vector initialization on\nformulating suitable template and choosing representative label mapping, which\nit is more common in few-shot learning tasks. Motivating by PLM working\nprocess, we try to construct the prompt from task semantic perspective and thus\npropose the STPrompt -Semantic-guided and Task-driven Prompt model.\nSpecifically, two novel prompts generated from the semantic dependency tree\n(Dep-prompt) and task-specific metadata description (Meta-prompt), are firstly\nconstructed in a prompt augmented pool, and the proposed model would\nautomatically select a suitable semantic prompt to motivating the prompt\nlearning process. Our results show that the proposed model achieves the\nstate-of-the-art performance in five different datasets of few-shot text\nclassification tasks, which prove that more semantic and significant prompts\ncould assume as a better knowledge proving tool.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1\">Jinta Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huan_H/0/1/0/all/0/1\">Heyan Huan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two is Better than Many? Binary Classification as an Effective Approach to Multi-Choice Question Answering. (arXiv:2210.16495v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16495","description":"<p>We propose a simple refactoring of multi-choice question answering (MCQA)\ntasks as a series of binary classifications. The MCQA task is generally\nperformed by scoring each (question, answer) pair normalized over all the\npairs, and then selecting the answer from the pair that yield the highest\nscore. For n answer choices, this is equivalent to an n-class classification\nsetup where only one class (true answer) is correct. We instead show that\nclassifying (question, true answer) as positive instances and (question, false\nanswer) as negative instances is significantly more effective across various\nmodels and datasets. We show the efficacy of our proposed approach in different\ntasks -- abductive reasoning, commonsense question answering, science question\nanswering, and sentence completion. Our DeBERTa binary classification model\nreaches the top or close to the top performance on public leaderboards for\nthese tasks. The source code of the proposed approach is available at\nhttps://github.com/declare-lab/TEAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Data Augmentation for Contrastive Sentence Representation Learning. (arXiv:2210.16536v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16536","description":"<p>Fine-tuning a pre-trained language model via the contrastive learning\nframework with a large amount of unlabeled sentences or labeled sentence pairs\nis a common way to obtain high-quality sentence representations. Although the\ncontrastive learning framework has shown its superiority on sentence\nrepresentation learning over previous methods, the potential of such a\nframework is under-explored so far due to the simple method it used to\nconstruct positive pairs. Motivated by this, we propose a method that makes\nhard positives from the original training examples. A pivotal ingredient of our\napproach is the use of prefix that is attached to a pre-trained language model,\nwhich allows for differentiable data augmentation during contrastive learning.\nOur method can be summarized in two steps: supervised prefix-tuning followed by\njoint contrastive fine-tuning with unlabeled or labeled examples. Our\nexperiments confirm the effectiveness of our data augmentation approach. The\nproposed method yields significant improvements over existing methods under\nboth semi-supervised and supervised settings. Our experiments under a low\nlabeled data setting also show that our method is more label-efficient than the\nstate-of-the-art contrastive learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianduo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phonemic Representation and Transcription for Speech to Text Applications for Under-resourced Indigenous African Languages: The Case of Kiswahili. (arXiv:2210.16537v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16537","description":"<p>Building automatic speech recognition (ASR) systems is a challenging task,\nespecially for under-resourced languages that need to construct corpora nearly\nfrom scratch and lack sufficient training data. It has emerged that several\nAfrican indigenous languages, including Kiswahili, are technologically\nunder-resourced. ASR systems are crucial, particularly for the hearing-impaired\npersons who can benefit from having transcripts in their native languages.\nHowever, the absence of transcribed speech datasets has complicated efforts to\ndevelop ASR models for these indigenous languages. This paper explores the\ntranscription process and the development of a Kiswahili speech corpus, which\nincludes both read-out texts and spontaneous speech data from native Kiswahili\nspeakers. The study also discusses the vowels and consonants in Kiswahili and\nprovides an updated Kiswahili phoneme dictionary for the ASR model that was\ncreated using the CMU Sphinx speech recognition toolbox, an open-source speech\nrecognition toolkit. The ASR model was trained using an extended phonetic set\nthat yielded a WER and SER of 18.87% and 49.5%, respectively, an improved\nperformance than previous similar research for under-resourced languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Awino_E/0/1/0/all/0/1\">Ebbie Awino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanzare_L/0/1/0/all/0/1\">Lilian Wanzare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muchemi_L/0/1/0/all/0/1\">Lawrence Muchemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanjawa_B/0/1/0/all/0/1\">Barack Wanjawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ombui_E/0/1/0/all/0/1\">Edward Ombui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indede_F/0/1/0/all/0/1\">Florence Indede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McOnyango_O/0/1/0/all/0/1\">Owen McOnyango</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okal_B/0/1/0/all/0/1\">Benard Okal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting prompt learning with pre-trained language models for Alzheimer's Disease detection. (arXiv:2210.16539v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16539","description":"<p>Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating\npreventive care and to delay further progression. Speech based automatic AD\nscreening systems provide a non-intrusive and more scalable alternative to\nother clinical screening techniques. Textual embedding features produced by\npre-trained language models (PLMs) such as BERT are widely used in such\nsystems. However, PLM domain fine-tuning is commonly based on the masked word\nor sentence prediction costs that are inconsistent with the back-end AD\ndetection task. To this end, this paper investigates the use of prompt-based\nfine-tuning of PLMs that consistently uses AD classification errors as the\ntraining objective function. Disfluency features based on hesitation or pause\nfiller token frequencies are further incorporated into prompt phrases during\nPLM fine-tuning. The exploit of the complementarity between BERT or RoBERTa\nbased PLMs that are either prompt learning fine-tuned, or optimized using\nconventional masked word or sentence prediction costs, decision voting based\nsystem combination between them is further applied. Mean, standard deviation\nand the maximum among accuracy scores over 15 experiment runs are adopted as\nperformance measurements for the AD detection system. Mean detection accuracy\nof 84.20% (with std 2.09%, best 87.5%) and 82.64% (with std 4.0%, best 89.58%)\nwere obtained using manual and ASR speech transcripts respectively on the\nADReSS20 test set consisting of 48 elderly speakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianzi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shoukang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-centered Cross-document Relation Extraction. (arXiv:2210.16541v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16541","description":"<p>Relation Extraction (RE) is a fundamental task of information extraction,\nwhich has attracted a large amount of research attention. Previous studies\nfocus on extracting the relations within a sentence or document, while\ncurrently researchers begin to explore cross-document RE. However, current\ncross-document RE methods directly utilize text snippets surrounding target\nentities in multiple given documents, which brings considerable noisy and\nnon-relevant sentences. Moreover, they utilize all the text paths in a document\nbag in a coarse-grained way, without considering the connections between these\ntext paths.In this paper, we aim to address both of these shortages and push\nthe state-of-the-art for cross-document RE. First, we focus on input\nconstruction for our RE model and propose an entity-based document-context\nfilter to retain useful information in the given documents by using the bridge\nentities in the text paths. Second, we propose a cross-document RE model based\non cross-path entity relation attention, which allow the entity relations\nacross text paths to interact with each other. We compare our cross-document RE\nmethod with the state-of-the-art methods in the dataset CodRED. Our method\noutperforms them by at least 10% in F1, thus demonstrating its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fengqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengqiong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_F/0/1/0/all/0/1\">Fangfang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wenxuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_B/0/1/0/all/0/1\">Bo Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Spoken Language Understanding with Tree-constrained Pointer Generator. (arXiv:2210.16554v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16554","description":"<p>End-to-end spoken language understanding (SLU) suffers from the long-tail\nword problem. This paper exploits contextual biasing, a technique to improve\nthe speech recognition of rare words, in end-to-end SLU systems. Specifically,\na tree-constrained pointer generator (TCPGen), a powerful and efficient biasing\nmodel component, is studied, which leverages a slot shortlist with\ncorresponding entities to extract biasing lists. Meanwhile, to bias the SLU\nmodel output slot distribution, a slot probability biasing (SPB) mechanism is\nproposed to calculate a slot distribution from TCPGen. Experiments on the SLURP\ndataset showed consistent SLU-F1 improvements using TCPGen and SPB, especially\non unseen entities. On a new split by holding out 5 slot types for the test,\nTCPGen with SPB achieved zero-shot learning with an SLU-F1 score over 50%\ncompared to baselines which can not deal with it. In addition to slot filling,\nthe intent classification accuracy was also improved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Attribute-Entangled Controllable Text Generation: A Pilot Study of Blessing Generation. (arXiv:2210.16557v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16557","description":"<p>Controllable Text Generation (CTG) has obtained great success due to its\nfine-grained generation ability obtained by focusing on multiple attributes.\nHowever, most existing CTG researches overlook how to utilize the attribute\nentanglement to enhance the diversity of the controlled generated texts. Facing\nthis dilemma, we focus on a novel CTG scenario, i.e., blessing generation which\nis challenging because high-quality blessing texts require CTG models to\ncomprehensively consider the entanglement between multiple attributes (e.g.,\nobjects and occasions). To promote the research on blessing generation, we\npresent EBleT, a large-scale Entangled Blessing Text dataset containing 293K\nEnglish sentences annotated with multiple attributes. Furthermore, we propose\nnovel evaluation metrics to measure the quality of the blessing texts generated\nby the baseline models we designed. Our study opens a new research direction\nfor controllable text generation and enables the development of\nattribute-entangled CTG models. Our dataset and source codes are available at\n\\url{https://github.com/huangshulin123/Blessing-Generation}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shulin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shirong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shiyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NTULM: Enriching Social Media Text Representations with Non-Textual Units. (arXiv:2210.16586v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16586","description":"<p>On social media, additional context is often present in the form of\nannotations and meta-data such as the post's author, mentions, Hashtags, and\nhyperlinks. We refer to these annotations as Non-Textual Units (NTUs). We posit\nthat NTUs provide social context beyond their textual semantics and leveraging\nthese units can enrich social media text representations. In this work we\nconstruct an NTU-centric social heterogeneous network to co-embed NTUs. We then\nprincipally integrate these NTU embeddings into a large pretrained language\nmodel by fine-tuning with these additional units. This adds context to noisy\nshort-text social media. Experiments show that utilizing NTU-augmented text\nrepresentations significantly outperforms existing text-only baselines by 2-5\\%\nrelative points on many downstream tasks highlighting the importance of context\nto social media NLP. We also highlight that including NTU context into the\ninitial layers of language model alongside text is better than using it after\nthe text embedding is generated. Our work leads to the generation of holistic\ngeneral purpose social media content embedding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shubhanshu Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1\">Ahmed El-Kishky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sneha Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_V/0/1/0/all/0/1\">Vivek Kulkarni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Critical Reflection and Forward Perspective on Empathy and Natural Language Processing. (arXiv:2210.16604v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16604","description":"<p>We review the state of research on empathy in natural language processing and\nidentify the following issues: (1) empathy definitions are absent or abstract,\nwhich (2) leads to low construct validity and reproducibility. Moreover, (3)\nemotional empathy is overemphasized, skewing our focus to a narrow subset of\nsimplified tasks. We believe these issues hinder research progress and argue\nthat current directions will benefit from a clear conceptualization that\nincludes operationalizing cognitive empathy components. Our main objectives are\nto provide insight and guidance on empathy conceptualization for NLP research\nobjectives and to encourage researchers to pursue the overlooked opportunities\nin this area, highly relevant, e.g., for clinical and educational sectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lahnala_A/0/1/0/all/0/1\">Allison Lahnala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_C/0/1/0/all/0/1\">Charles Welch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1\">Lucie Flek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of Knowledge Distillation to Multi-task Speech Representation Learning. (arXiv:2210.16611v1 [eess.AS])","link":"http://arxiv.org/abs/2210.16611","description":"<p>Model architectures such as wav2vec 2.0 and HuBERT have been proposed to\nlearn speech representations from audio waveforms in a self-supervised manner.\nWhen these models are combined with downstream tasks such as speech\nrecognition, they have been shown to provide state-of-the-art performance.\nHowever, these models use a large number of parameters, the smallest version of\nwhich has about 95 million parameters. This constitutes a challenge for edge AI\ndevice deployments. In this paper, we use knowledge distillation to reduce the\noriginal model size by about 75% while maintaining similar performance levels.\nMoreover, we use wav2vec 2.0 and HuBERT models for distillation and present a\ncomprehensive performance analysis through our experiments where we fine-tune\nthe distilled models on single task and multi-task frameworks separately. In\nparticular, our experiments show that fine-tuning the distilled models on\nkeyword spotting and speaker verification tasks result in only 0.1% accuracy\nand 0.9% equal error rate degradations, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kerpicci_M/0/1/0/all/0/1\">Mine Kerpicci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_V/0/1/0/all/0/1\">Van Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shuhua Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Visser_E/0/1/0/all/0/1\">Erik Visser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Parallel Data Synthesis for Cross-Database Adaptation of Text-to-SQL Parsers. (arXiv:2210.16613v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16613","description":"<p>Text-to-SQL parsers typically struggle with databases unseen during the train\ntime. Adapting parsers to new databases is a challenging problem due to the\nlack of natural language queries in the new schemas. We present ReFill, a\nframework for synthesizing high-quality and textually diverse parallel datasets\nfor adapting a Text-to-SQL parser to a target schema. ReFill learns to\nretrieve-and-edit text queries from the existing schemas and transfers them to\nthe target schema. We show that retrieving diverse existing text, masking their\nschema-specific tokens, and refilling with tokens relevant to the target\nschema, leads to significantly more diverse text queries than achievable by\nstandard SQL-to-Text generation methods. Through experiments spanning multiple\ndatabases, we demonstrate that fine-tuning parsers on datasets synthesized\nusing ReFill consistently outperforms the prior data-augmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Awasthi_A/0/1/0/all/0/1\">Abhijeet Awasthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathe_A/0/1/0/all/0/1\">Ashutosh Sathe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Evaluation of Post-Training Quantization Methods for Language Tasks. (arXiv:2210.16621v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16621","description":"<p>Transformer-based architectures like BERT have achieved great success in a\nwide range of Natural Language tasks. Despite their decent performance, the\nmodels still have numerous parameters and high computational complexity,\nimpeding their deployment in resource-constrained environments. Post-Training\nQuantization (PTQ), which enables low-bit computations without extra training,\ncould be a promising tool. In this work, we conduct an empirical evaluation of\nthree PTQ methods on BERT-Base and BERT-Large: Linear Quantization (LQ),\nAnalytical Clipping for Integer Quantization (ACIQ), and Outlier Channel\nSplitting (OCS). OCS theoretically surpasses the others in minimizing the Mean\nSquare quantization Error and avoiding distorting the weights' outliers. That\nis consistent with the evaluation results of most language tasks of GLUE\nbenchmark and a reading comprehension task, SQuAD. Moreover, low-bit quantized\nBERT models could outperform the corresponding 32-bit baselines on several\nsmall language tasks, which we attribute to the alleviation of\nover-parameterization. We further explore the limit of quantization bit and\nshow that OCS could quantize BERT-Base and BERT-Large to 3-bits and retain 98%\nand 96% of the performance on the GLUE benchmark accordingly. Moreover, we\nconduct quantization on the whole BERT family, i.e., BERT models in different\nconfigurations, and comprehensively evaluate their performance on the GLUE\nbenchmark and SQuAD, hoping to provide valuable guidelines for their deployment\nin various computation environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Ting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1\">Christoph Meinel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haojin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations. (arXiv:2210.16637v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16637","description":"<p>Recent work has demonstrated that pre-trained language models (PLMs) are\nzero-shot learners. However, most existing zero-shot methods involve heavy\nhuman engineering or complicated self-training pipelines, hindering their\napplication to new situations. In this work, we show that zero-shot text\nclassification can be improved simply by clustering texts in the embedding\nspaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian\nGaussian Mixture Model after initializing cluster positions and shapes using\nclass names. Despite its simplicity, this approach achieves superior or\ncomparable performance on both topic and sentiment classification datasets and\noutperforms prior works significantly on unbalanced datasets. We further\nexplore the applicability of our clustering approach by evaluating it on 14\ndatasets with more diverse topics, text lengths, and numbers of classes. Our\napproach achieves an average of 20% absolute improvement over prompt-based\nzero-shot learning. Finally, we compare different PLM embedding spaces and find\nthat texts are well-clustered by topics even if the PLM is not explicitly\npre-trained to generate meaningful sentence embeddings. This work indicates\nthat PLM embeddings can categorize texts without task-specific fine-tuning,\nthus providing a new way to analyze and utilize their knowledge and zero-shot\nlearning ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_Y/0/1/0/all/0/1\">Yu Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_P/0/1/0/all/0/1\">Ping Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying the Discrete and Continuous Emotion labels for Speech Emotion Recognition. (arXiv:2210.16642v1 [cs.SD])","link":"http://arxiv.org/abs/2210.16642","description":"<p>Traditionally, in paralinguistic analysis for emotion detection from speech,\nemotions have been identified with discrete or dimensional (continuous-valued)\nlabels. Accordingly, models that have been proposed for emotion detection use\none or the other of these label types. However, psychologists like Russell and\nPlutchik have proposed theories and models that unite these views, maintaining\nthat these representations have shared and complementary information. This\npaper is an attempt to validate these viewpoints computationally. To this end,\nwe propose a model to jointly predict continuous and discrete emotional\nattributes and show how the relationship between these can be utilized to\nimprove the robustness and performance of emotion recognition tasks. Our\napproach comprises multi-task and hierarchical multi-task learning frameworks\nthat jointly model the relationships between continuous-valued and discrete\nemotion labels. Experimental results on two widely used datasets (IEMOCAP and\nMSPPodcast) for speech-based emotion recognition show that our model results in\nstatistically significant improvements in performance over strong baselines\nwith non-unified approaches. We also demonstrate that using one type of label\n(discrete or continuous-valued) for training improves recognition performance\nin tasks that use the other type of label. Experimental results and reasoning\nfor this approach (called the mismatched training approach) are also presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Roshan Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamyal_H/0/1/0/all/0/1\">Hira Dhamyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XNOR-FORMER: Learning Accurate Approximations in Long Speech Transformers. (arXiv:2210.16643v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16643","description":"<p>Transformers are among the state of the art for many tasks in speech, vision,\nand natural language processing, among others. Self-attentions, which are\ncrucial contributors to this performance have quadratic computational\ncomplexity, which makes training on longer input sequences challenging. Prior\nwork has produced state-of-the-art transformer variants with linear attention,\nhowever, current models sacrifice performance to achieve efficient\nimplementations. In this work, we develop a novel linear transformer by\nexamining the properties of the key-query product within self-attentions. Our\nmodel outperforms state of the art approaches on speech recognition and speech\nsummarization, resulting in 1 % absolute WER improvement on the Librispeech-100\nspeech recognition benchmark and a new INTERVIEW speech recognition benchmark,\nand 5 points on ROUGE for summarization with How2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Roshan Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Dependencies of Discrete Speech Representations with Neural Hidden Markov Models. (arXiv:2210.16659v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16659","description":"<p>While discrete latent variable models have had great success in\nself-supervised learning, most models assume that frames are independent. Due\nto the segmental nature of phonemes in speech perception, modeling dependencies\namong latent variables at the frame level can potentially improve the learned\nrepresentations on phonetic-related tasks. In this work, we assume Markovian\ndependencies among latent variables, and propose to learn speech\nrepresentations with neural hidden Markov models. Our general framework allows\nus to compare to self-supervised models that assume independence, while keeping\nthe number of parameters fixed. The added dependencies improve the\naccessibility of phonetic information, phonetic segmentation, and the cluster\npurity of phones, showcasing the benefit of the assumed dependencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_S/0/1/0/all/0/1\">Sung-Lin Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model. (arXiv:2210.16663v1 [eess.AS])","link":"http://arxiv.org/abs/2210.16663","description":"<p>This paper presents BERT-CTC, a novel formulation of end-to-end speech\nrecognition that adapts BERT for connectionist temporal classification (CTC).\nOur formulation relaxes the conditional independence assumptions used in\nconventional CTC and incorporates linguistic knowledge through the explicit\noutput dependency obtained by BERT contextual embedding. BERT-CTC attends to\nthe full contexts of the input and hypothesized output sequences via the\nself-attention mechanism. This mechanism encourages a model to learn\ninner/inter-dependencies between the audio and token representations while\nmaintaining CTC's training efficiency. During inference, BERT-CTC combines a\nmask-predict algorithm with CTC decoding, which iteratively refines an output\nsequence. The experimental results reveal that BERT-CTC improves over\nconventional approaches across variations in speaking styles and languages.\nFinally, we show that the semantic representations in BERT-CTC are beneficial\ntowards downstream spoken language understanding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_Y/0/1/0/all/0/1\">Yosuke Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ogawa_T/0/1/0/all/0/1\">Tetsuji Ogawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kobayashi_T/0/1/0/all/0/1\">Tetsunori Kobayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Far are We from Robust Long Abstractive Summarization?. (arXiv:2210.16732v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16732","description":"<p>Abstractive summarization has made tremendous progress in recent years. In\nthis work, we perform fine-grained human annotations to evaluate long document\nabstractive summarization systems (i.e., models and metrics) with the aim of\nimplementing them to generate reliable summaries. For long document abstractive\nmodels, we show that the constant strive for state-of-the-art ROUGE results can\nlead us to generate more relevant summaries but not factual ones. For long\ndocument evaluation metrics, human evaluation results show that ROUGE remains\nthe best at evaluating the relevancy of a summary. It also reveals important\nlimitations of factuality metrics in detecting different types of factual\nerrors and the reasons behind the effectiveness of BARTScore. We then suggest\npromising directions in the endeavor of developing factual consistency metrics.\nFinally, we release our annotated long document dataset with the hope that it\ncan contribute to the development of metrics across a broader range of\nsummarization settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koh_H/0/1/0/all/0/1\">Huan Yee Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1\">Jiaxin Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuDe: Dual-Decoder Multilingual ASR for Indian Languages using Common Label Set. (arXiv:2210.16739v1 [eess.AS])","link":"http://arxiv.org/abs/2210.16739","description":"<p>In a multilingual country like India, multilingual Automatic Speech\nRecognition (ASR) systems have much scope. Multilingual ASR systems exhibit\nmany advantages like scalability, maintainability, and improved performance\nover the monolingual ASR systems. However, building multilingual systems for\nIndian languages is challenging since different languages use different scripts\nfor writing. On the other hand, Indian languages share a lot of common sounds.\nCommon Label Set (CLS) exploits this idea and maps graphemes of various\nlanguages with similar sounds to common labels. Since Indian languages are\nmostly phonetic, building a parser to convert from native script to CLS is\neasy. In this paper, we explore various approaches to build multilingual ASR\nmodels. We also propose a novel architecture called Encoder-Decoder-Decoder for\nbuilding multilingual systems that use both CLS and native script labels. We\nalso analyzed the effectiveness of CLS-based multilingual systems combined with\nmachine transliteration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+A_A/0/1/0/all/0/1\">Arunkumar A</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Batra_M/0/1/0/all/0/1\">Mudit Batra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+S_U/0/1/0/all/0/1\">Umesh S</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text. (arXiv:2210.16755v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16755","description":"<p>Self-supervised pre-training has been successful in both text and speech\nprocessing. Speech and text offer different but complementary information. The\nquestion is whether we are able to perform a speech-text joint pre-training on\nunpaired speech and text. In this paper, we take the idea of self-supervised\npre-training one step further and propose token2vec, a novel joint pre-training\nframework for unpaired speech and text based on discrete representations of\nspeech. Firstly, due to the distinct characteristics between speech and text\nmodalities, where speech is continuous while text is discrete, we first\ndiscretize speech into a sequence of discrete speech tokens to solve the\nmodality mismatch problem. Secondly, to solve the length mismatch problem,\nwhere the speech sequence is usually much longer than text sequence, we convert\nthe words of text into phoneme sequences and randomly repeat each phoneme in\nthe sequences. Finally, we feed the discrete speech and text tokens into a\nmodality-agnostic Transformer encoder and pre-train with token-level masking\nlanguage modeling (tMLM). Experiments show that token2vec is significantly\nsuperior to various speech-only pre-training baselines, with up to 17.7%\nrelative WER reduction. Token2vec model is also validated on a non-ASR task,\ni.e., spoken intent classification, and shows good transferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xianghu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_J/0/1/0/all/0/1\">Junyi Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiaoxue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Tuning Makes a Good Classification Head. (arXiv:2210.16771v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16771","description":"<p>In recent years, pretrained models revolutionized the paradigm of natural\nlanguage understanding (NLU), where we append a randomly initialized\nclassification head after the pretrained backbone, e.g. BERT, and finetune the\nwhole model. As the pretrained backbone makes a major contribution to the\nimprovement, we naturally expect a good pretrained classification head can also\nbenefit the training. However, the final-layer output of the backbone, i.e. the\ninput of the classification head, will change greatly during finetuning, making\nthe usual head-only pretraining (LP-FT) ineffective. In this paper, we find\nthat parameter-efficient tuning makes a good classification head, with which we\ncan simply replace the randomly initialized heads for a stable performance\ngain. Our experiments demonstrate that the classification head jointly\npretrained with parameter-efficient tuning consistently improves the\nperformance on 9 tasks in GLUE and SuperGLUE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1\">Qingsong Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks. (arXiv:2210.16773v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16773","description":"<p>Access to external knowledge is essential for many natural language\nprocessing tasks, such as question answering and dialogue. Existing methods\noften rely on a parametric model that stores knowledge in its parameters, or\nuse a retrieval-augmented model that has access to an external knowledge\nsource. Parametric and retrieval-augmented models have complementary strengths\nin terms of computational efficiency and predictive accuracy. To combine the\nstrength of both approaches, we propose the Efficient Memory-Augmented\nTransformer (EMAT) -- it encodes external knowledge into a key-value memory and\nexploits the fast maximum inner product search for memory querying. We also\nintroduce pre-training tasks that allow EMAT to encode informative key-value\nrepresentations, and to learn an implicit strategy to integrate multiple memory\nslots into the transformer. Experiments on various knowledge-intensive tasks\nsuch as question answering and dialogue datasets show that, simply augmenting\nparametric models (T5-base) using our method produces more accurate results\n(e.g., 25.8 -&gt; 44.3 EM on NQ) while retaining a high throughput (e.g., 1000\nqueries/s on NQ). Compared to retrieval-augmented models, EMAT runs\nsubstantially faster across the board and produces more accurate results on WoW\nand ELI5. Our code and datasets are available at https://github.\ncom/uclnlp/EMAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generate, Discriminate and Contrast: A Semi-Supervised Sentence Representation Learning Framework. (arXiv:2210.16798v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16798","description":"<p>Most sentence embedding techniques heavily rely on expensive human-annotated\nsentence pairs as the supervised signals. Despite the use of large-scale\nunlabeled data, the performance of unsupervised methods typically lags far\nbehind that of the supervised counterparts in most downstream tasks. In this\nwork, we propose a semi-supervised sentence embedding framework, GenSE, that\neffectively leverages large-scale unlabeled data. Our method include three\nparts: 1) Generate: A generator/discriminator model is jointly trained to\nsynthesize sentence pairs from open-domain unlabeled corpus; 2) Discriminate:\nNoisy sentence pairs are filtered out by the discriminator to acquire\nhigh-quality positive and negative sentence pairs; 3) Contrast: A prompt-based\ncontrastive approach is presented for sentence representation learning with\nboth annotated and synthesized data. Comprehensive experiments show that GenSE\nachieves an average correlation score of 85.19 on the STS datasets and\nconsistent performance improvement on four domain adaptation tasks,\nsignificantly surpassing the state-of-the-art methods and convincingly\ncorroborating its effectiveness and generalization ability.Code, Synthetic data\nand Models available at https://github.com/MatthewCYM/GenSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zuozhu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Data Augmentation via Perspective Transition for Open-Domain Dialogues. (arXiv:2210.16838v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16838","description":"<p>The construction of open-domain dialogue systems requires high-quality\ndialogue datasets. The dialogue data admits a wide variety of responses for a\ngiven dialogue history, especially responses with different semantics. However,\ncollecting high-quality such a dataset in most scenarios is labor-intensive and\ntime-consuming. In this paper, we propose a data augmentation method to\nautomatically augment high-quality responses with different semantics by\ncounterfactual inference. Specifically, given an observed dialogue, our\ncounterfactual generation model first infers semantically different responses\nby replacing the observed reply perspective with substituted ones. Furthermore,\nour data selection method filters out detrimental augmented responses.\nExperimental results show that our data augmentation method can augment\nhigh-quality responses with different semantics for a given dialogue history,\nand can outperform competitive baselines on multiple downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1\">Jiao Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Actionable Phrase Detection using NLP. (arXiv:2210.16841v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16841","description":"<p>Actionable sentences are terms that, in the most basic sense, imply the\nnecessity of taking a specific action. In Linguistic terms, they are steps to\nachieve an operation, often through the usage of action verbs. For example, the\nsentence, `Get your homework finished by tomorrow` qualifies as actionable\nsince it demands a specific action (In this case, finishing homework) to be\ntaken. In contrast, a simple sentence such as, `I like to play the guitar` does\nnot qualify as an actionable phrase since it simply states a personal choice of\nthe person instead of demanding a task to be finished.\n</p>\n<p>In this paper, the aim is to explore if Actionables can be extracted from raw\ntext using Linguistic filters designed from scratch. These filters are\nspecially catered to identifying actionable text using Transfer Learning as the\nlead role. Actionable Detection can be used in detecting emergency tasks during\na crisis, Instruction accuracy for First aid and can also be used to make\nproductivity tools like automatic ToDo list generators from conferences. To\naccomplish this, we use the Enron Email Dataset and apply our Linguistic\nfilters on the cleaned textual data. We then use Transfer Learning with the\nUniversal Sentence Encoder to train a model to classify whether a given string\nof raw text is actionable or not.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magotra_A/0/1/0/all/0/1\">Adit Magotra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings. (arXiv:2210.16848v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16848","description":"<p>Although contextualized embeddings generated from large-scale pre-trained\nmodels perform well in many tasks, traditional static embeddings (e.g.,\nSkip-gram, Word2Vec) still play an important role in low-resource and\nlightweight settings due to their low computational cost, ease of deployment,\nand stability. In this paper, we aim to improve word embeddings by 1)\nincorporating more contextual information from existing pre-trained models into\nthe Skip-gram framework, which we call Context-to-Vec; 2) proposing a\npost-processing retrofitting method for static embeddings independent of\ntraining by employing priori synonym knowledge and weighted vector\ndistribution. Through extrinsic and intrinsic tasks, our methods are well\nproven to outperform the baselines by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiangbin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yile Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guojiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Codes Prediction from Clinical Notes: From Human Coders to Machines. (arXiv:2210.16850v1 [cs.LG])","link":"http://arxiv.org/abs/2210.16850","description":"<p>Prediction of medical codes from clinical notes is a practical and essential\nneed for every healthcare delivery organization within current medical systems.\nAutomating annotation will save significant time and excessive effort that\nhuman coders spend today. However, the biggest challenge is directly\nidentifying appropriate medical codes from several thousands of\nhigh-dimensional codes from unstructured free-text clinical notes. This complex\nmedical codes prediction problem from clinical notes has received substantial\ninterest in the NLP community, and several recent studies have shown the\nstate-of-the-art code prediction results of full-fledged deep learning-based\nmethods. This progress raises the fundamental question of how far automated\nmachine learning systems are from human coders' working performance, as well as\nthe important question of how well current explainability methods apply to\nadvanced neural network models such as transformers. This is to predict correct\ncodes and present references in clinical notes that support code prediction, as\nthis level of explainability and accuracy of the prediction outcomes is\ncritical to gaining trust from professional medical coders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byung-Hak Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts. (arXiv:2210.16865v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16865","description":"<p>Explicit decomposition modeling, which involves breaking down complex tasks\ninto more straightforward and often more interpretable sub-tasks, has long been\na central theme in developing robust and interpretable NLU systems. However,\ndespite the many datasets and resources built as part of this effort, the\nmajority have small-scale annotations and limited scope, which is insufficient\nto solve general decomposition tasks. In this paper, we look at large-scale\nintermediate pre-training of decomposition-based transformers using distant\nsupervision from comparable texts, particularly large-scale parallel news. We\nshow that with such intermediate pre-training, developing robust\ndecomposition-based models for a diverse range of tasks becomes more feasible.\nFor example, on semantic parsing, our model, DecompT5, improves 20% to 30% on\ntwo datasets, Overnight and TORQUE, over the baseline language model. We\nfurther use DecompT5 to build a novel decomposition-based QA system named\nDecompEntail, improving over state-of-the-art models, including GPT-3, on both\nHotpotQA and StrategyQA by 8% and 4%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Ben Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_K/0/1/0/all/0/1\">Kyle Richardson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaodong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffusER: Discrete Diffusion via Edit-based Reconstruction. (arXiv:2210.16886v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16886","description":"<p>In text generation, models that generate text from scratch one token at a\ntime are currently the dominant paradigm. Despite being performant, these\nmodels lack the ability to revise existing text, which limits their usability\nin many practical scenarios. We look to address this, with DiffusER (Diffusion\nvia Edit-based Reconstruction), a new edit-based generative model for text\nbased on denoising diffusion models -- a class of models that use a Markov\nchain of denoising steps to incrementally generate data. DiffusER is not only a\nstrong generative model in general, rivalling autoregressive models on several\ntasks spanning machine translation, summarization, and style transfer; it can\nalso perform other varieties of generation that standard autoregressive models\nare not well-suited for. For instance, we demonstrate that DiffusER makes it\npossible for a user to condition generation on a prototype, or an incomplete\nsequence, and continue revising based on previous edit steps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1\">Vincent J. Hellendoorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising Large-Scale Image Captioning from Alt-text Data using Content Selection Models. (arXiv:2009.05175v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.05175","description":"<p>Training large-scale image captioning (IC) models demands access to a rich\nand diverse set of training examples, gathered from the wild, often from noisy\nalt-text data. However, recent modeling approaches to IC often fall short in\nterms of performance in this case, because they assume a clean annotated\ndataset (as opposed to the noisier alt-text--based annotations), and employ an\nend-to-end generation approach, which often lacks both controllability and\ninterpretability. We address these problems by breaking down the task into two\nsimpler, more controllable tasks -- skeleton prediction and skeleton-based\ncaption generation. Specifically, we show that selecting content words as\nskeletons} helps in generating improved and denoised captions when leveraging\nrich yet noisy alt-text--based uncurated datasets. We also show that the\npredicted English skeletons can be further cross-lingually leveraged to\ngenerate non-English captions, and present experimental results covering\ncaption generation in French, Italian, German, Spanish and Hindi. We also show\nthat skeleton-based prediction allows for better control of certain caption\nproperties, such as length, content, and gender expression, providing a handle\nto perform human-in-the-loop semi-automatic corrections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1\">Khyathi Raghavi Chandu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thapliyal_A/0/1/0/all/0/1\">Ashish Thapliyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v8 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08315","description":"<p>Large language models have shown promising results in zero-shot settings\n(Brown et al.,2020; Radford et al., 2019). For example, they can perform\nmultiple choice tasks simply by conditioning on a question and selecting the\nanswer with the highest probability.\n</p>\n<p>However, ranking by string probability can be problematic due to surface form\ncompetition-wherein different surface forms compete for probability mass, even\nif they represent the same underlying concept, e.g. \"computer\" and \"PC.\" Since\nprobability mass is finite, this lowers the probability of the correct answer,\ndue to competition from other strings that are valid answers (but not one of\nthe multiple choice options).\n</p>\n<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative\nscoring function that directly compensates for surface form competition by\nsimply reweighing each option according to a term that is proportional to its a\npriori likelihood within the context of the specific zero-shot task. It\nachieves consistent gains in zero-shot performance over both calibrated (Zhao\net al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models\nover a variety of multiple choice datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let the CAT out of the bag: Contrastive Attributed explanations for Text. (arXiv:2109.07983v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07983","description":"<p>Contrastive explanations for understanding the behavior of black box models\nhas gained a lot of attention recently as they provide potential for recourse.\nIn this paper, we propose a method Contrastive Attributed explanations for Text\n(CAT) which provides contrastive explanations for natural language text data\nwith a novel twist as we build and exploit attribute classifiers leading to\nmore semantically meaningful explanations. To ensure that our contrastive\ngenerated text has the fewest possible edits with respect to the original text,\nwhile also being fluent and close to a human generated contrastive, we resort\nto a minimal perturbation approach regularized using a BERT language model and\nattribute classifiers trained on available attributes. We show through\nqualitative examples and a user study that our method not\nonlyconveysmoreinsightbecauseoftheseattributes,butalsoleadstobetterquality(contrastive)text.\nQuantitatively, we show that our method outperforms other state-of-the-art\nmethods across four data sets on four benchmark metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chemmengath_S/0/1/0/all/0/1\">Saneem Chemmengath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azad_A/0/1/0/all/0/1\">Amar Prakash Azad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luss_R/0/1/0/all/0/1\">Ronny Luss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhurandhar_A/0/1/0/all/0/1\">Amit Dhurandhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SlovakBERT: Slovak Masked Language Model. (arXiv:2109.15254v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.15254","description":"<p>We introduce a new Slovak masked language model called SlovakBERT. This is to\nour best knowledge the first paper discussing Slovak transformers-based\nlanguage models. We evaluate our model on several NLP tasks and achieve\nstate-of-the-art results. This evaluation is likewise the first attempt to\nestablish a benchmark for Slovak language models. We publish the masked\nlanguage model, as well as the fine-tuned models for part-of-speech tagging,\nsentiment analysis and semantic textual similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pikuliak_M/0/1/0/all/0/1\">Mat&#xfa;&#x161; Pikuliak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grivalsky_S/0/1/0/all/0/1\">&#x160;tefan Grivalsk&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopka_M/0/1/0/all/0/1\">Martin Kon&#xf4;pka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blstak_M/0/1/0/all/0/1\">Miroslav Bl&#x161;t&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamajka_M/0/1/0/all/0/1\">Martin Tamajka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachraty_V/0/1/0/all/0/1\">Viktor Bachrat&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simko_M/0/1/0/all/0/1\">Mari&#xe1;n &#x160;imko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balazik_P/0/1/0/all/0/1\">Pavol Bal&#xe1;&#x17e;ik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trnka_M/0/1/0/all/0/1\">Michal Trnka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uhlarik_F/0/1/0/all/0/1\">Filip Uhl&#xe1;rik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rerunning OCR: A Machine Learning Approach to Quality Assessment and Enhancement Prediction. (arXiv:2110.01661v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01661","description":"<p>Iterating with new and improved OCR solutions enforces decision making when\nit comes to targeting the right candidates for reprocessing. This especially\napplies when the underlying data collection is of considerable size and rather\ndiverse in terms of fonts, languages, periods of publication and consequently\nOCR quality. This article captures the efforts of the National Library of\nLuxembourg to support those targeting decisions. They are crucial in order to\nguarantee low computational overhead and reduced quality degradation risks,\ncombined with a more quantifiable OCR improvement. In particular, this work\nexplains the methodology of the library with respect to text block level\nquality assessment. Through extension of this technique, a regression model,\nthat is able to take into account the enhancement potential of a new OCR\nengine, is also presented. They both mark promising approaches, especially for\ncultural institutions dealing with historical data of lower quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pit Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurer_Y/0/1/0/all/0/1\">Yves Maurer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Retriever-Ranker for dense text retrieval. (arXiv:2110.03611v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03611","description":"<p>Current dense text retrieval models face two typical challenges. First, they\nadopt a siamese dual-encoder architecture to encode queries and documents\nindependently for fast indexing and searching, while neglecting the\nfiner-grained term-wise interactions. This results in a sub-optimal recall\nperformance. Second, their model training highly relies on a negative sampling\ntechnique to build up the negative documents in their contrastive losses. To\naddress these challenges, we present Adversarial Retriever-Ranker (AR2), which\nconsists of a dual-encoder retriever plus a cross-encoder ranker. The two\nmodels are jointly optimized according to a minimax adversarial objective: the\nretriever learns to retrieve negative documents to cheat the ranker, while the\nranker learns to rank a collection of candidates including both the\nground-truth and the retrieved ones, as well as providing progressive direct\nfeedback to the dual-encoder retriever. Through this adversarial game, the\nretriever gradually produces harder negative documents to train a better\nranker, whereas the cross-encoder ranker provides progressive feedback to\nimprove retriever. We evaluate AR2 on three benchmarks. Experimental results\nshow that AR2 consistently and significantly outperforms existing dense\nretriever methods and achieves new state-of-the-art results on all of them.\nThis includes the improvements on Natural Questions R@5 to 77.9%(+2.1%),\nTriviaQA R@5 to 78.2%(+1.4), and MS-MARCO MRR@10 to 39.5%(+1.3%). Code and\nmodels are available at https://github.com/microsoft/AR2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MemPrompt: Memory-assisted Prompt Editing with User Feedback. (arXiv:2201.06009v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06009","description":"<p>Large LMs such as GPT-3 are powerful, but can commit mistakes that are\nobvious to humans. For example, GPT-3 would mistakenly interpret \"What word is\nsimilar to good?\" to mean a homophone, while the user intended a synonym. Our\ngoal is to effectively correct such errors via user interactions with the\nsystem but without retraining, which will be prohibitively costly. We pair\nGPT-3 with a growing memory of recorded cases where the model misunderstood the\nuser's intents, along with user feedback for clarification. Such a memory\nallows our system to produce enhanced prompts for any new query based on the\nuser feedback for error correction on similar cases in the past. On four tasks\n(two lexical tasks, two advanced ethical reasoning tasks), we show how a\n(simulated) user can interactively teach a deployed GPT-3, substantially\nincreasing its accuracy over the queries with different kinds of\nmisunderstandings by the GPT-3. Our approach is a step towards the low-cost\nutility enhancement for very large pre-trained LMs. Code, data, and\ninstructions to implement MEMPROMPT for a new task at\nhttps://www.memprompt.com/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SQUIRE: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph Reasoning. (arXiv:2201.06206v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06206","description":"<p>Multi-hop knowledge graph (KG) reasoning has been widely studied in recent\nyears to provide interpretable predictions on missing links with evidential\npaths. Most previous works use reinforcement learning (RL) based methods that\nlearn to navigate the path towards the target entity. However, these methods\nsuffer from slow and poor convergence, and they may fail to infer a certain\npath when there is a missing edge along the path. Here we present SQUIRE, the\nfirst Sequence-to-sequence based multi-hop reasoning framework, which utilizes\nan encoder-decoder Transformer structure to translate the query to a path. Our\nframework brings about two benefits: (1) It can learn and predict in an\nend-to-end fashion, which gives better and faster convergence; (2) Our\nTransformer model does not rely on existing edges to generate the path, and has\nthe flexibility to complete missing edges along the path, especially in sparse\nKGs. Experiments on standard and sparse KGs show that our approach yields\nsignificant improvement over prior methods, while converging 4x-7x faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yushi Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yincen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zelin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization. (arXiv:2201.06910v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.06910","description":"<p>We propose a multitask pretraining approach ZeroPrompt for zero-shot\ngeneralization, focusing on task scaling and zero-shot prompting. While\nprevious models are trained on only a few dozen tasks, we scale to 1,000 tasks\nfor the first time using real-world data. This leads to a crucial discovery\nthat task scaling can be an efficient alternative to model scaling; i.e., the\nmodel size has little impact on performance with an extremely large number of\ntasks. Our results show that task scaling can substantially improve training\nefficiency by 30 times in FLOPs. Moreover, we present a prompting method that\nincorporates a genetic algorithm to automatically search for the best prompt\nfor unseen tasks, along with a few other improvements. Empirically, ZeroPrompt\nsubstantially improves both the efficiency and the performance of zero-shot\nlearning across a variety of academic and production datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hanwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yujun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yulun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_N/0/1/0/all/0/1\">Nan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haiyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal data matters: language model pre-training over structured and unstructured electronic health records. (arXiv:2201.10113v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.10113","description":"<p>As two important textual modalities in electronic health records (EHR), both\nstructured data (clinical codes) and unstructured data (clinical narratives)\nhave recently been increasingly applied to the healthcare domain. Most existing\nEHR-oriented studies, however, either focus on a particular modality or\nintegrate data from different modalities in a straightforward manner, which\nusually treats structured and unstructured data as two independent sources of\ninformation about patient admission and ignore the intrinsic interactions\nbetween them. In fact, the two modalities are documented during the same\nencounter where structured data inform the documentation of unstructured data\nand vice versa. In this paper, we proposed a Medical Multimodal Pre-trained\nLanguage Model, named MedM-PLM, to learn enhanced EHR representations over\nstructured and unstructured data and explore the interaction of two modalities.\nIn MedM-PLM, two Transformer-based neural network components are firstly\nadopted to learn representative characteristics from each modality. A\ncross-modal module is then introduced to model their interactions. We\npre-trained MedM-PLM on the MIMIC-III dataset and verified the effectiveness of\nthe model on three downstream clinical tasks, i.e., medication recommendation,\n30-day readmission prediction and ICD coding. Extensive experiments demonstrate\nthe power of MedM-PLM compared with state-of-the-art methods. Further analyses\nand visualizations show the robustness of our model, which could potentially\nprovide more comprehensive interpretations for clinical decision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sicen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yongshuai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Has Been Enhanced in my Knowledge-Enhanced Language Model?. (arXiv:2202.00964v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00964","description":"<p>We show that existing model interpretation methods such as linear probes and\nprompts have some key limitations in answering these questions. We revisit KI\nfrom an information-theoretic view and propose a new theoretically sound probe\ncalled Graph Convolution Simulator (GCS) for KI interpretation. GCS uses graph\nattention on the corresponding knowledge graph for interpretation. In our\nexperiments we verify that GCS can provide reasonable interpretation results\nfor two well-known knowledge-enhanced LMs: ERNIE and K-Adapter. We also find\nthat only a marginal amount of knowledge is successfully integrated in these\nmodels, and simply increasing the size of the KI corpus may not lead to better\nknowledge-enhanced LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yifan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guoji Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Trained Language Models for Interactive Decision-Making. (arXiv:2202.01771v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01771","description":"<p>Language model (LM) pre-training is useful in many language processing tasks.\nBut can pre-trained LMs be further leveraged for more general machine learning\nproblems? We propose an approach for using LMs to scaffold learning and\ngeneralization in general sequential decision-making problems. In this\napproach, goals and observations are represented as a sequence of embeddings,\nand a policy network initialized with a pre-trained LM predicts the next\naction. We demonstrate that this framework enables effective combinatorial\ngeneralization across different environments and supervisory modalities. We\nbegin by assuming access to a set of expert demonstrations, and show that\ninitializing policies with LMs and fine-tuning them via behavior cloning\nimproves task completion rates by 43.6% in the VirtualHome environment. Next,\nwe integrate an active data gathering procedure in which agents iteratively\ninteract with the environment, relabel past \"failed\" experiences with new\ngoals, and update their policies in a self-supervised loop. Active data\ngathering further improves combinatorial generalization, outperforming the best\nbaseline by 25.1%. Finally, we explain these results by investigating three\npossible factors underlying the effectiveness of the LM-based policy. We find\nthat sequential input representations (vs. fixed-dimensional feature vectors)\nand LM-based weight initialization are both important for generalization.\nSurprisingly, however, the format of the policy inputs encoding (e.g. as a\nnatural language string vs. an arbitrary sequential encoding) has little\ninfluence. Together, these results suggest that language modeling induces\nrepresentations that are useful for modeling not just language, but also goals\nand plans; these representations can aid learning and generalization even\noutside of language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puig_X/0/1/0/all/0/1\">Xavier Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Clinton Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">De-An Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1\">Ekin Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation. (arXiv:2202.13363v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13363","description":"<p>In this paper, we propose a variational autoencoder with disentanglement\npriors, VAE-DPRIOR, for task-specific natural language generation with none or\na handful of task-specific labeled examples. In order to tackle compositional\ngeneralization across tasks, our model performs disentangled representation\nlearning by introducing a conditional prior for the latent content space and\nanother conditional prior for the latent label space. Both types of priors\nsatisfy a novel property called $\\epsilon$-disentangled. We show both\nempirically and theoretically that the novel priors can disentangle\nrepresentations even without specific regularizations as in the prior work. The\ncontent prior enables directly sampling diverse content representations from\nthe content space learned from the seen tasks, and fuse them with the\nrepresentations of novel tasks for generating semantically diverse texts in the\nlow-resource settings. Our extensive experiments demonstrate the superior\nperformance of our model over competitive baselines in terms of i) data\naugmentation in continuous zero/few-shot learning, and ii) text style transfer\nin the few-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiongkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongtong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_T/0/1/0/all/0/1\">Tianyang Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation. (arXiv:2203.09553v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2203.09553","description":"<p>Federated learning (FL) can be essential in knowledge representation,\nreasoning, and data mining applications over multi-source knowledge graphs\n(KGs). A recent study FedE first proposes an FL framework that shares entity\nembeddings of KGs across all clients. However, entity embedding sharing from\nFedE would incur a severe privacy leakage. Specifically, the known entity\nembedding can be used to infer whether a specific relation between two entities\nexists in a private client. In this paper, we introduce a novel attack method\nthat aims to recover the original data based on the embedding information,\nwhich is further used to evaluate the vulnerabilities of FedE. Furthermore, we\npropose a Federated learning paradigm with privacy-preserving Relation\nembedding aggregation (FedR) to tackle the privacy issue in FedE. Besides,\nrelation embedding sharing can significantly reduce the communication cost due\nto its smaller size of queries. We conduct extensive experiments to evaluate\nFedR with five different KG embedding models and three datasets. Compared to\nFedE, FedR achieves similar utility and significant improvements regarding\nprivacy-preserving effect and communication efficiency on the link prediction\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoregressive Co-Training for Learning Discrete Speech Representations. (arXiv:2203.15840v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.15840","description":"<p>While several self-supervised approaches for learning discrete speech\nrepresentation have been proposed, it is unclear how these seemingly similar\napproaches relate to each other. In this paper, we consider a generative model\nwith discrete latent variables that learns a discrete representation for\nspeech. The objective of learning the generative model is formulated as\ninformation-theoretic co-training. Besides the wide generality, the objective\ncan be optimized with several approaches, subsuming HuBERT-like training and\nvector quantization for learning discrete representation. Empirically, we find\nthat the proposed approach learns discrete representation that is highly\ncorrelated with phonetic units, more correlated than HuBERT-like training and\nvector quantization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_S/0/1/0/all/0/1\">Sung-Lin Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation. (arXiv:2204.13031v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13031","description":"<p>Dialog response generation in open domain is an important research topic\nwhere the main challenge is to generate relevant and diverse responses. In this\npaper, we propose a new dialog pre-training framework called DialogVED, which\nintroduces continuous latent variables into the enhanced encoder-decoder\npre-training framework to increase the relevance and diversity of responses.\nWith the help of a large dialog corpus (Reddit), we pre-train the model using\nthe following 4 tasks adopted in language models (LMs) and variational\nautoencoders (VAEs): 1) masked language model; 2) response generation; 3)\nbag-of-words prediction; and 4) KL divergence reduction. We also add additional\nparameters to model the turn structure in dialogs to improve the performance of\nthe pre-trained model. We conduct experiments on PersonaChat, DailyDialog, and\nDSTC7-AVSD benchmarks for response generation. Experimental results show that\nour model achieves the new state-of-the-art results on all these datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bolun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Weizhen Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bartuer Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Biao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02357","description":"<p>Multimodal Knowledge Graphs (MKGs), which organize visual-text factual\nknowledge, have recently been successfully applied to tasks such as information\nretrieval, question answering, and recommendation system. Since most MKGs are\nfar from complete, extensive knowledge graph completion studies have been\nproposed focusing on the multimodal entity, relation extraction and link\nprediction. However, different tasks and modalities require changes to the\nmodel architecture, and not all images/objects are relevant to text input,\nwhich hinders the applicability to diverse real-world scenarios. In this paper,\nwe propose a hybrid transformer with multi-level fusion to address those\nissues. Specifically, we leverage a hybrid transformer architecture with\nunified input-output for diverse multimodal knowledge graph completion tasks.\nMoreover, we propose multi-level fusion, which integrates visual and text\nrepresentation via coarse-grained prefix-guided interaction and fine-grained\ncorrelation-aware fusion modules. We conduct extensive experiments to validate\nthat our MKGformer can obtain SOTA performance on four datasets of multimodal\nlink prediction, multimodal RE, and multimodal NER. Code is available in\nhttps://github.com/zjunlp/MKGformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Literal and Implied Subquestions to Fact-check Complex Claims. (arXiv:2205.06938v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.06938","description":"<p>Verifying complex political claims is a challenging task, especially when\npoliticians use various tactics to subtly misrepresent the facts. Automatic\nfact-checking systems fall short here, and their predictions like \"half-true\"\nare not very useful in isolation, since we have no idea which parts of the\nclaim are true and which are not. In this work, we focus on decomposing a\ncomplex claim into a comprehensive set of yes-no subquestions whose answers\ninfluence the veracity of the claim. We present ClaimDecomp, a dataset of\ndecompositions for over 1000 claims. Given a claim and its verification\nparagraph written by fact-checkers, our trained annotators write subquestions\ncovering both explicit propositions of the original claim and its implicit\nfacets, such as asking about additional political context that changes our view\nof the claim's veracity. We study whether state-of-the-art models can generate\nsuch subquestions, showing that these models generate reasonable questions to\nask, but predicting the comprehensive set of subquestions from the original\nclaim without evidence remains challenging. We further show that these\nsubquestions can help identify relevant evidence to fact-check the full claim\nand derive the veracity through their answers, suggesting that they can be\nuseful pieces of a fact-checking pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriram_A/0/1/0/all/0/1\">Aniruddh Sriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Supervised Contrastive Learning for Fair Text Classification. (arXiv:2205.11485v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11485","description":"<p>Contrastive representation learning has gained much attention due to its\nsuperior performance in learning representations from both image and sequential\ndata. However, the learned representations could potentially lead to\nperformance disparities in downstream tasks, such as increased silencing of\nunderrepresented groups in toxicity comment classification. In light of this\nchallenge, in this work, we study learning fair representations that satisfy a\nnotion of fairness known as equalized odds for text classification via\ncontrastive learning. Specifically, we first theoretically analyze the\nconnections between learning representations with a fairness constraint and\nconditional supervised contrastive objectives, and then propose to use\nconditional supervised contrastive objectives to learn fair representations for\ntext classification. We conduct experiments on two text datasets to demonstrate\nthe effectiveness of our approaches in balancing the trade-offs between task\nperformance and bias mitigation among existing baselines for text\nclassification. Furthermore, we also show that the proposed methods are stable\nin different hyperparameter settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_J/0/1/0/all/0/1\">Jianfeng Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shand_W/0/1/0/all/0/1\">William Shand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yaodong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Formulating Few-shot Fine-tuning Towards Language Model Pre-training: A Pilot Study on Named Entity Recognition. (arXiv:2205.11799v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11799","description":"<p>Fine-tuning pre-trained language models has recently become a common practice\nin building NLP models for various tasks, especially few-shot tasks. We argue\nthat under the few-shot setting, formulating fine-tuning closer to the\npre-training objectives shall be able to unleash more benefits from the\npre-trained language models. In this work, we take few-shot named entity\nrecognition (NER) for a pilot study, where existing fine-tuning strategies are\nmuch different from pre-training. We propose a novel few-shot fine-tuning\nframework for NER, FFF-NER. Specifically, we introduce three new types of\ntokens, \"is-entity\", \"which-type\" and bracket, so we can formulate the NER\nfine-tuning as (masked) token prediction or generation, depending on the choice\nof pre-trained language models. In our experiments, we apply FFF-NER to\nfine-tune both BERT and BART for few-shot NER on several benchmark datasets and\nobserve significant improvements over existing fine-tuning strategies,\nincluding sequence labeling, prototype meta-learning, and prompt-based\napproaches. We further perform a series of ablation studies, showing few-shot\nNER performance is strongly correlated with the similarity between fine-tuning\nand pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kewen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeDef: Weakly Supervised Backdoor Defense for Text Classification. (arXiv:2205.11803v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11803","description":"<p>Existing backdoor defense methods are only effective for limited trigger\ntypes. To defend different trigger types at once, we start from the\nclass-irrelevant nature of the poisoning process and propose a novel weakly\nsupervised backdoor defense framework WeDef. Recent advances in weak\nsupervision make it possible to train a reasonably accurate text classifier\nusing only a small number of user-provided, class-indicative seed words. Such\nseed words shall be considered independent of the triggers. Therefore, a weakly\nsupervised text classifier trained by only the poisoned documents without their\nlabels will likely have no backdoor. Inspired by this observation, in WeDef, we\ndefine the reliability of samples based on whether the predictions of the weak\nclassifier agree with their labels in the poisoned training set. We further\nimprove the results through a two-phase sanitization: (1) iteratively refine\nthe weak classifier based on the reliable samples and (2) train a binary poison\nclassifier by distinguishing the most unreliable samples from the most reliable\nsamples. Finally, we train the sanitized model on the samples that the poison\nclassifier predicts as benign. Extensive experiments show that WeDefis\neffective against popular trigger-based attacks (e.g., words, sentences, and\nparaphrases), outperforming existing defense methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lesheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuned Language Models are Continual Learners. (arXiv:2205.12393v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12393","description":"<p>Recent work on large language models relies on the intuition that most\nnatural language processing tasks can be described via natural language\ninstructions. Language models trained on these instructions show strong\nzero-shot performance on several standard datasets. However, these models even\nthough impressive still perform poorly on a wide range of tasks outside of\ntheir respective training and evaluation sets. To address this limitation, we\nargue that a model should be able to keep extending its knowledge and\nabilities, without forgetting previous skills. In spite of the limited success\nof Continual Learning we show that Language Models can be continual learners.\nWe empirically investigate the reason for this success and conclude that\nContinual Learning emerges from self-supervision pre-training. Our resulting\nmodel Continual-T0 (CT0) is able to learn diverse new tasks, while still\nmaintaining good performance on previous tasks, spanning remarkably through 70\ndatasets in total. Finally, we show that CT0 is able to combine instructions in\nways it was never trained for, demonstrating some compositionality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Locality in Abstractive Text Summarization. (arXiv:2205.12476v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12476","description":"<p>Neural attention models have achieved significant improvements on many\nnatural language processing tasks. However, the quadratic memory complexity of\nthe self-attention module with respect to the input length hinders their\napplications in long text summarization. Instead of designing more efficient\nattention modules, we approach this problem by investigating if models with a\nrestricted context can have competitive performance compared with the\nmemory-efficient attention models that maintain a global context by treating\nthe input as a single sequence. Our model is applied to individual pages which\ncontain parts of inputs grouped by the principle of locality during both\nencoding and decoding. We empirically investigated three kinds of locality in\ntext summarization at different levels of granularity, ranging from sentences\nto documents. Our experimental results show that our model has a better\nperformance compared with strong baselines with efficient attention modules,\nand our analysis provides further insights into our locality-aware modeling\nstrategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Linyong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1\">Budhaditya Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed H. Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Language-neutral Sub-networks in Multilingual Language Models. (arXiv:2205.12672v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12672","description":"<p>Multilingual pre-trained language models transfer remarkably well on\ncross-lingual downstream tasks. However, the extent to which they learn\nlanguage-neutral representations (i.e., shared representations that encode\nsimilar phenomena across languages), and the effect of such representations on\ncross-lingual transfer performance, remain open questions. In this work, we\nconceptualize language neutrality of multilingual models as a function of the\noverlap between language-encoding sub-networks of these models. We employ the\nlottery ticket hypothesis to discover sub-networks that are individually\noptimized for various languages and tasks. Our evaluation across three distinct\ntasks and eleven typologically-diverse languages demonstrates that sub-networks\nfor different languages are topologically similar (i.e., language-neutral),\nmaking them effective initializations for cross-lingual transfer with limited\nperformance degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foroutan_N/0/1/0/all/0/1\">Negar Foroutan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banaei_M/0/1/0/all/0/1\">Mohammadreza Banaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebret_R/0/1/0/all/0/1\">Remi Lebret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberer_K/0/1/0/all/0/1\">Karl Aberer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning. (arXiv:2206.06522v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.06522","description":"<p>Fine-tuning large pre-trained models on downstream tasks has been adopted in\na variety of domains recently. However, it is costly to update the entire\nparameter set of large pre-trained models. Although recently proposed\nparameter-efficient transfer learning (PETL) techniques allow updating a small\nsubset of parameters (e.g. only using 2% of parameters) inside a pre-trained\nbackbone network for a new task, they only reduce the training memory\nrequirement by up to 30%. This is because the gradient computation for the\ntrainable parameters still requires backpropagation through the large\npre-trained backbone model. To address this, we propose Ladder Side-Tuning\n(LST), a new PETL technique that can reduce training memory requirements by\nmore substantial amounts. Unlike existing parameter-efficient methods that\ninsert additional parameters inside backbone networks, we train a ladder side\nnetwork, a small and separate network that takes intermediate activations as\ninput via shortcut connections (called ladders) from backbone networks and\nmakes predictions. LST has significantly lower memory requirements than\nprevious methods, because it does not require backpropagation through the\nbackbone network, but instead only through the side network and ladder\nconnections. We evaluate our method with various models (T5 and CLIP-T5) on\nboth NLP (GLUE) and vision-and-language (VQA, GQA, NLVR2 , MSCOCO) tasks. LST\nsaves 69% of the memory costs to fine-tune the whole network, while other\nmethods only save 26% of that in similar parameter usages (hence, 2.7x more\nmemory savings). Moreover, LST achieves higher accuracy than Adapter and LoRA\nin a low-memory regime. To further show the advantage of this better memory\nefficiency, we also apply LST to larger T5 models, attaining better GLUE\nperformance than full fine-tuning and other PETL methods. The\naccuracy-efficiency trade-off also holds on VL tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yi-Lin Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks. (arXiv:2206.06565v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.06565","description":"<p>Fine-tuning pretrained language models (LMs) without making any architectural\nchanges has become a norm for learning various language downstream tasks.\nHowever, for non-language downstream tasks, a common practice is to employ\ntask-specific designs for input, output layers, and loss functions. For\ninstance, it is possible to fine-tune an LM into an MNIST classifier by\nreplacing the word embedding layer with an image patch embedding layer, the\nword token output layer with a 10-way output layer, and the word prediction\nloss with a 10-way classification loss, respectively. A natural question\narises: Can LM fine-tuning solve non-language downstream tasks without changing\nthe model architecture or loss function? To answer this, we propose\nLanguage-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations\nby conducting an extensive empirical study on a suite of non-language\nclassification and regression tasks. LIFT does not make any changes to the\nmodel architecture or loss function, and it solely relies on the natural\nlanguage interface, enabling \"no-code machine learning with LMs.\" We find that\nLIFT performs comparably well across a wide range of low-dimensional\nclassification and regression tasks, matching the performances of the best\nbaselines in many cases, especially for the classification tasks. We also\nreport experimental results on the fundamental properties of LIFT, including\ninductive bias, robustness, and sample complexity. We also analyze the effect\nof pretraining on LIFT and a few properties/techniques specific to LIFT, e.g.,\ncontext-aware learning via appropriate prompting, calibrated predictions, data\ngeneration, and two-stage fine-tuning. Our code is available at\nhttps://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tuan Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yuchen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruisu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Ziqian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gira_M/0/1/0/all/0/1\">Michael Gira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajput_S/0/1/0/all/0/1\">Shashank Rajput</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1\">Jy-yong Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1\">Dimitris Papailiopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kangwook Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change). (arXiv:2206.10498v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.10498","description":"<p>Recent advances in large language models (LLMs) have transformed the field of\nnatural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art\nperformance on natural language tasks is being pushed forward with every new\nlarge language model. Along with natural language abilities, there has been a\nsignificant interest in understanding whether such models exhibit reasoning\ncapabilities with the use of reasoning benchmarks. However, even though results\nare seemingly positive, these benchmarks prove to be simplistic in nature and\nthe performance of LLMs on these benchmarks cannot be used as evidence to\nsupport, many a times outlandish, claims being made about LLMs' reasoning\ncapabilities. Further, these only represent a very limited set of simple\nreasoning tasks and we need to look at more sophisticated reasoning problems if\nwe are to measure the true limits of such LLM-based systems. Motivated by this,\nwe propose an extensible assessment framework to test the capabilities of LLMs\non reasoning about actions and change, a central aspect of human intelligence.\nWe provide multiple test cases that are more involved than any of the\npreviously established benchmarks and each test case evaluates a different\naspect of reasoning about actions and change. Results on GPT-3 (davinci),\nInstruct-GPT3 (text-davinci-002) and BLOOM (176B), showcase subpar performance\non such reasoning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valmeekam_K/0/1/0/all/0/1\">Karthik Valmeekam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olmo_A/0/1/0/all/0/1\">Alberto Olmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreedharan_S/0/1/0/all/0/1\">Sarath Sreedharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1\">Subbarao Kambhampati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Principal Phrase Mining. (arXiv:2206.13748v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.13748","description":"<p>Extracting frequent words from a collection of texts is commonly performed in\nmany subjects. However, as useful as it is to obtain a collection of commonly\noccurring words from texts, there is a need for more specific information to be\nobtained from texts in the form of most commonly occurring phrases. Despite\nthis need, extracting frequent phrases is not commonly done due to inherent\ncomplications, the most significant being double-counting. Double-counting\noccurs when words or phrases are counted when they appear inside longer phrases\nthat themselves are also counted, resulting in a selection of mostly\nmeaningless phrases that are frequent only because they occur inside frequent\nsuper phrases. Several papers have been written on phrase mining that describe\nsolutions to this issue; however, they either require a list of so-called\nquality phrases to be available to the extracting process, or they require\nhuman interaction to identify those quality phrases during the process. We\npresent here a method that eliminates double-counting via a unique\nrectification process that does not require lists of quality phrases. In the\ncontext of a set of texts, we define a principal phrase as a phrase that does\nnot cross punctuation marks, does not start with a stop word, with the\nexception of the stop words \"not\" and \"no\", does not end with a stop word, is\nfrequent within those texts without being double counted, and is meaningful to\nthe user. Our method identifies such principal phrases independently without\nhuman input, and enables their extraction from any texts within a reasonable\namount of time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Small_E/0/1/0/all/0/1\">Ellie Small</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabrera_J/0/1/0/all/0/1\">Javier Cabrera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models and the Reverse Turing Test. (arXiv:2207.14382v8 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.14382","description":"<p>Large Language Models (LLMs) have been transformative. They are pre-trained\nfoundational models that are self-supervised and can be adapted with fine\ntuning to a wide range of natural language tasks, each of which previously\nwould have required a separate network model. This is one step closer to the\nextraordinary versatility of human language. GPT-3 and more recently LaMDA can\ncarry on dialogs with humans on many topics after minimal priming with a few\nexamples. However, there has been a wide range of reactions and debate on\nwhether these LLMs understand what they are saying or exhibit signs of\nintelligence. This high variance is exhibited in three interviews with LLMs\nreaching wildly different conclusions. A new possibility was uncovered that\ncould explain this divergence. What appears to be intelligence in LLMs may in\nfact be a mirror that reflects the intelligence of the interviewer, a\nremarkable twist that could be considered a Reverse Turing Test. If so, then by\nstudying interviews we may be learning more about the intelligence and beliefs\nof the interviewer than the intelligence of the LLMs. As LLMs become more\ncapable they may transform the way we interact with machines and how they\ninteract with each other. Increasingly, LLMs are being coupled with\nsensorimotor devices. LLMs can talk the talk, but can they walk the walk? A\nroad map for achieving artificial general autonomy is outlined with seven major\nimprovements inspired by brain systems. LLMs could be used to uncover new\ninsights into brain function by downloading brain data during natural\nbehaviors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sejnowski_T/0/1/0/all/0/1\">Terrence Sejnowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Sleuth: From Unlabeled Text to a Classifier in a Few Hours. (arXiv:2208.01483v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.01483","description":"<p>Text classification can be useful in many real-world scenarios, saving a lot\nof time for end users. However, building a custom classifier typically requires\ncoding skills and ML knowledge, which poses a significant barrier for many\npotential users. To lift this barrier, we introduce Label Sleuth, a free open\nsource system for labeling and creating text classifiers. This system is unique\nfor (a) being a no-code system, making NLP accessible to non-experts, (b)\nguiding users through the entire labeling process until they obtain a custom\nclassifier, making the process efficient -- from cold start to classifier in a\nfew hours, and (c) being open for configuration and extension by developers. By\nopen sourcing Label Sleuth we hope to build a community of users and developers\nthat will broaden the utilization of NLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shnarch_E/0/1/0/all/0/1\">Eyal Shnarch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halfon_A/0/1/0/all/0/1\">Alon Halfon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gera_A/0/1/0/all/0/1\">Ariel Gera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danilevsky_M/0/1/0/all/0/1\">Marina Danilevsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsis_Y/0/1/0/all/0/1\">Yannis Katsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_M/0/1/0/all/0/1\">Martin Santillan Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Epelboim_D/0/1/0/all/0/1\">Dina Epelboim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yip_L/0/1/0/all/0/1\">Lucy Yip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ein_Dor_L/0/1/0/all/0/1\">Liat Ein-Dor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dankin_L/0/1/0/all/0/1\">Lena Dankin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shnayderman_I/0/1/0/all/0/1\">Ilya Shnayderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharonov_R/0/1/0/all/0/1\">Ranit Aharonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liberman_N/0/1/0/all/0/1\">Naftali Liberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slesarev_P/0/1/0/all/0/1\">Philip Levin Slesarev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newton_G/0/1/0/all/0/1\">Gwilym Newton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofek_Koifman_S/0/1/0/all/0/1\">Shila Ofek-Koifman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SupervisorBot: NLP-Annotated Real-Time Recommendations of Psychotherapy Treatment Strategies with Deep Reinforcement Learning. (arXiv:2208.13077v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.13077","description":"<p>We propose a recommendation system that suggests treatment strategies to a\ntherapist during the psychotherapy session in real-time. Our system uses a\nturn-level rating mechanism that predicts the therapeutic outcome by computing\na similarity score between the deep embedding of a scoring inventory, and the\ncurrent sentence that the patient is speaking. The system automatically\ntranscribes a continuous audio stream and separates it into turns of the\npatient and of the therapist and perform real-time inference of their\ntherapeutic working alliance. The dialogue pairs along with their computed\nworking alliance as ratings are then fed into a deep reinforcement learning\nrecommendation system where the sessions are treated as users and the topics\nare treated as items. Other than evaluating the empirical advantages of the\ncore components on an existing dataset of psychotherapy sessions, we\ndemonstrate the effectiveness of this system in a web app.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cecchi_G/0/1/0/all/0/1\">Guillermo Cecchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1\">Djallel Bouneffouf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual and Cross-Domain Crisis Classification for Low-Resource Scenarios. (arXiv:2209.02139v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.02139","description":"<p>Social media data has emerged as a useful source of timely information about\nreal-world crisis events. One of the main tasks related to the use of social\nmedia for disaster management is the automatic identification of crisis-related\nmessages. Most of the studies on this topic have focused on the analysis of\ndata for a particular type of event in a specific language. This limits the\npossibility of generalizing existing approaches because models cannot be\ndirectly applied to new types of events or other languages. In this work, we\nstudy the task of automatically classifying messages that are related to crisis\nevents by leveraging cross-language and cross-domain labeled data. Our goal is\nto make use of labeled data from high-resource languages to classify messages\nfrom other (low-resource) languages and/or of new (previously unseen) types of\ncrisis situations. For our study we consolidated from the literature a large\nunified dataset containing multiple crisis events and languages. Our empirical\nfindings show that it is indeed possible to leverage data from crisis events in\nEnglish to classify the same type of event in other languages, such as Spanish\nand Italian (80.0% F1-score). Furthermore, we achieve good performance for the\ncross-domain task (80.0% F1-score) in a cross-lingual setting. Overall, our\nwork contributes to improving the data scarcity problem that is so important\nfor multilingual crisis classification. In particular, mitigating cold-start\nsituations in emergency events, when time is of essence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_C/0/1/0/all/0/1\">Cinthia S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarmiento_H/0/1/0/all/0/1\">Hernan Sarmiento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abeliuk_A/0/1/0/all/0/1\">Andres Abeliuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Jorge P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poblete_B/0/1/0/all/0/1\">Barbara Poblete</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Scene-based Topic Channel Construction System for E-Commerce. (arXiv:2210.02643v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02643","description":"<p>Scene marketing that well demonstrates user interests within a certain\nscenario has proved effective for offline shopping. To conduct scene marketing\nfor e-commerce platforms, this work presents a novel product form, scene-based\ntopic channel which typically consists of a list of diverse products belonging\nto the same usage scenario and a topic title that describes the scenario with\nmarketing words. As manual construction of channels is time-consuming due to\nbillions of products as well as dynamic and diverse customers' interests, it is\nnecessary to leverage AI techniques to automatically construct channels for\ncertain usage scenarios and even discover novel topics. To be specific, we\nfirst frame the channel construction task as a two-step problem, i.e.,\nscene-based topic generation and product clustering, and propose an E-commerce\nScene-based Topic Channel construction system (i.e., ESTC) to achieve automated\nproduction, consisting of scene-based topic generation model for the e-commerce\ndomain, product clustering on the basis of topic similarity, as well as quality\ncontrol based on automatic model filtering and human screening. Extensive\noffline experiments and online A/B test validates the effectiveness of such a\nnovel product form as well as the proposed system. In addition, we also\nintroduce the experience of deploying the proposed system on a real-world\ne-commerce recommendation platform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Peng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yanyan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval Augmented Visual Question Answering with Outside Knowledge. (arXiv:2210.03809v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03809","description":"<p>Outside-Knowledge Visual Question Answering (OK-VQA) is a challenging VQA\ntask that requires retrieval of external knowledge to answer questions about\nimages. Recent OK-VQA systems use Dense Passage Retrieval (DPR) to retrieve\ndocuments from external knowledge bases, such as Wikipedia, but with DPR\ntrained separately from answer generation, introducing a potential limit on the\noverall system performance. Instead, we propose a joint training scheme which\nincludes differentiable DPR integrated with answer generation so that the\nsystem can be trained in an end-to-end fashion. Our experiments show that our\nscheme outperforms recent OK-VQA systems with strong DPR for retrieval. We also\nintroduce new diagnostic metrics to analyze how retrieval and generation\ninteract. The strong retrieval ability of our model significantly reduces the\nnumber of retrieved documents needed in training, yielding significant benefits\nin answer quality and computation required for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weizhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1\">Bill Byrne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of Soft and Hard Target RNN-T Distillation for Large-scale ASR. (arXiv:2210.05793v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.05793","description":"<p>Knowledge distillation is an effective machine learning technique to transfer\nknowledge from a teacher model to a smaller student model, especially with\nunlabeled data. In this paper, we focus on knowledge distillation for the RNN-T\nmodel, which is widely used in state-of-the-art (SoTA) automatic speech\nrecognition (ASR). Specifically, we compared using soft and hard target\ndistillation to train large-scaleRNN-T models on the LibriSpeech/LibriLight\npublic dataset (60k hours) and our in-house data (600k hours). We found that\nhard tar-gets are more effective when the teacher and student have different\narchitecture, such as large teacher and small streaming student. On the other\nhand, soft target distillation works better in self-training scenario like\niterative large teacher training. For a large model with0.6B weights, we\nachieve a new SoTA word error rate (WER) on LibriSpeech (8% relative\nimprovement on dev-other) using Noisy Student Training with soft target\ndistillation. It also allows our production teacher to adapt new data domain\ncontinuously.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers. (arXiv:2210.07904v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07904","description":"<p>Transformer-based pre-trained language models are vocabulary-dependent,\nmapping by default each token to its corresponding embedding. This one-to-one\nmapping results into embedding matrices that occupy a lot of memory (i.e.\nmillions of parameters) and grow linearly with the size of the vocabulary.\nPrevious work on on-device transformers dynamically generate token embeddings\non-the-fly without embedding matrices using locality-sensitive hashing over\nmorphological information. These embeddings are subsequently fed into\ntransformer layers for text classification. However, these methods are not\npre-trained. Inspired by this line of work, we propose HashFormers, a new\nfamily of vocabulary-independent pre-trained transformers that support an\nunlimited vocabulary (i.e. all possible tokens in a corpus) given a\nsubstantially smaller fixed-sized embedding matrix. We achieve this by first\nintroducing computationally cheap hashing functions that bucket together\nindividual tokens to embeddings. We also propose three variants that do not\nrequire an embedding matrix at all, further reducing the memory requirements.\nWe empirically demonstrate that HashFormers are more memory efficient compared\nto standard pre-trained transformers while achieving comparable predictive\nperformance when fine-tuned on multiple text classification tasks. For example,\nour most efficient HashFormer variant has a negligible performance degradation\n(0.4\\% on GLUE) using only 99.1K parameters for representing the embeddings\ncompared to 12.3-38M parameters of state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Huiyin Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Invariant Representation and Risk Minimized for Unsupervised Accent Domain Adaptation. (arXiv:2210.08182v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2210.08182","description":"<p>Unsupervised representation learning for speech audios attained impressive\nperformances for speech recognition tasks, particularly when annotated speech\nis limited. However, the unsupervised paradigm needs to be carefully designed\nand little is known about what properties these representations acquire. There\nis no guarantee that the model learns meaningful representations for valuable\ninformation for recognition. Moreover, the adaptation ability of the learned\nrepresentations to other domains still needs to be estimated. In this work, we\nexplore learning domain-invariant representations via a direct mapping of\nspeech representations to their corresponding high-level linguistic\ninformations. Results prove that the learned latents not only capture the\narticulatory feature of each phoneme but also enhance the adaptation ability,\noutperforming the baseline largely on accented benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chendong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoyang Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of Self-Supervised Speech Representation Learning. (arXiv:2210.08634v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08634","description":"<p>We present the SUPERB challenge at SLT 2022, which aims at learning\nself-supervised speech representation for better performance, generalization,\nand efficiency. The challenge builds upon the SUPERB benchmark and implements\nmetrics to measure the computation requirements of self-supervised learning\n(SSL) representation and to evaluate its generalizability and performance\nacross the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive\ncoverage of popular speech processing tasks, from speech and speaker\nrecognition to audio generation and semantic understanding. As SSL has gained\ninterest in the speech community and showed promising outcomes, we envision the\nchallenge to uplevel the impact of SSL techniques by motivating more practical\ndesigns of techniques beyond task performance. We summarize the results of 14\nsubmitted models in this paper. We also discuss the main findings from those\nsubmissions and the future directions of SSL research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Tzu-hsun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_A/0/1/0/all/0/1\">Annie Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Ching-Feng Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tzu-Quan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zili Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haibin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-Based Multilingual Label Propagation for Low-Resource Part-of-Speech Tagging. (arXiv:2210.09840v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09840","description":"<p>Part-of-Speech (POS) tagging is an important component of the NLP pipeline,\nbut many low-resource languages lack labeled data for training. An established\nmethod for training a POS tagger in such a scenario is to create a labeled\ntraining set by transferring from high-resource languages. In this paper, we\npropose a novel method for transferring labels from multiple high-resource\nsource to low-resource target languages. We formalize POS tag projection as\ngraph-based label propagation. Given translations of a sentence in multiple\nlanguages, we create a graph with words as nodes and alignment links as edges\nby aligning words for all language pairs. We then propagate node labels from\nsource to target using a Graph Neural Network augmented with transformer\nlayers. We show that our propagation creates training sets that allow us to\ntrain POS taggers for a diverse set of languages. When combined with enhanced\ncontextualized embeddings, our method achieves a new state-of-the-art for\nunsupervised POS tagging of low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imani_A/0/1/0/all/0/1\">Ayyoob Imani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Severini_S/0/1/0/all/0/1\">Silvia Severini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1\">Masoud Jalili Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Museformer: Transformer with Fine- and Coarse-Grained Attention for Music Generation. (arXiv:2210.10349v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2210.10349","description":"<p>Symbolic music generation aims to generate music scores automatically. A\nrecent trend is to use Transformer or its variants in music generation, which\nis, however, suboptimal, because the full attention cannot efficiently model\nthe typically long music sequences (e.g., over 10,000 tokens), and the existing\nmodels have shortcomings in generating musical repetition structures. In this\npaper, we propose Museformer, a Transformer with a novel fine- and\ncoarse-grained attention for music generation. Specifically, with the\nfine-grained attention, a token of a specific bar directly attends to all the\ntokens of the bars that are most relevant to music structures (e.g., the\nprevious 1st, 2nd, 4th and 8th bars, selected via similarity statistics); with\nthe coarse-grained attention, a token only attends to the summarization of the\nother bars rather than each token of them so as to reduce the computational\ncost. The advantages are two-fold. First, it can capture both music\nstructure-related correlations via the fine-grained attention, and other\ncontextual information via the coarse-grained attention. Second, it is\nefficient and can model over 3X longer music sequences compared to its\nfull-attention counterpart. Both objective and subjective experimental results\ndemonstrate its ability to generate long music sequences with high quality and\nbetter structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Botao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Peiling Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models. (arXiv:2210.11498v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11498","description":"<p>Traditional (fickle) adversarial examples involve finding a small\nperturbation that does not change an input's true label but confuses the\nclassifier into outputting a different prediction. Conversely, obstinate\nadversarial examples occur when an adversary finds a small perturbation that\npreserves the classifier's prediction but changes the true label of an input.\nAdversarial training and certified robust training have shown some\neffectiveness in improving the robustness of machine learnt models to fickle\nadversarial examples. We show that standard adversarial training methods\nfocused on reducing vulnerability to fickle adversarial examples may make a\nmodel more vulnerable to obstinate adversarial examples, with experiments for\nboth natural language inference and paraphrase identification tasks. To counter\nthis phenomenon, we introduce Balanced Adversarial Training, which incorporates\ncontrastive learning to increase robustness against both fickle and obstinate\nadversarial examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hannah Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_D/0/1/0/all/0/1\">David Evans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design a Sustainable Micro-mobility Future: Trends and Challenges in the United States and European Union Using Natural Language Processing Techniques. (arXiv:2210.11714v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11714","description":"<p>Micro-mobility is promising to contribute to sustainable cities in the future\nwith its efficiency and low cost. To better design such a sustainable future,\nit is necessary to understand the trends and challenges. Thus, we examined\npeople's opinions on micro-mobility in the US and the EU using Tweets. We used\ntopic modeling based on advanced natural language processing techniques and\ncategorized the data into seven topics: promotion and service, mobility,\ntechnical features, acceptance, recreation, infrastructure and regulations.\nFurthermore, using sentiment analysis, we investigated people's positive and\nnegative attitudes towards specific aspects of these topics and compared the\npatterns of the trends and challenges in the US and the EU. We found that 1)\npromotion and service included the majority of Twitter discussions in the both\nregions, 2) the EU had more positive opinions than the US, 3) micro-mobility\ndevices were more widely used for utilitarian mobility and recreational\npurposes in the EU than in the US, and 4) compared to the EU, people in the US\nhad many more concerns related to infrastructure and regulation issues. These\nfindings help us understand the trends and challenges and prioritize different\naspects in micro-mobility to improve their safety and experience across the two\nareas for designing a more sustainable micro-mobility future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avetisyan_L/0/1/0/all/0/1\">Lilit Avetisyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Sue Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pari_E/0/1/0/all/0/1\">Ehsan Moradi Pari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fred Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Shan Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploration of the Usage of Color Terms by Color-blind Participants in Online Discussion Platforms. (arXiv:2210.11905v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11905","description":"<p>Prominent questions about the role of sensory vs. linguistic input in the way\nwe acquire and use language have been extensively studied in the\npsycholinguistic literature. However, the relative effect of various factors in\na person's overall experience on their linguistic system remains unclear. We\nstudy this question by making a step forward towards a better understanding of\nthe conceptual perception of colors by color-blind individuals, as reflected in\ntheir spontaneous linguistic productions. Using a novel and carefully curated\ndataset, we show that red-green color-blind speakers use the \"red\" and \"green\"\ncolor terms in less predictable contexts, and in linguistic environments\nevoking mental image to a lower extent, when compared to their normal-sighted\ncounterparts. These findings shed some new and interesting light on the role of\nsensory experience on our linguistic system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabinovich_E/0/1/0/all/0/1\">Ella Rabinovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmeli_B/0/1/0/all/0/1\">Boaz Carmeli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graphemic Normalization of the Perso-Arabic Script. (arXiv:2210.12273v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12273","description":"<p>Since its original appearance in 1991, the Perso-Arabic script representation\nin Unicode has grown from 169 to over 440 atomic isolated characters spread\nover several code pages representing standard letters, various diacritics and\npunctuation for the original Arabic and numerous other regional orthographic\ntraditions. This paper documents the challenges that Perso-Arabic presents\nbeyond the best-documented languages, such as Arabic and Persian, building on\nearlier work by the expert community. We particularly focus on the situation in\nnatural language processing (NLP), which is affected by multiple, often\nneglected, issues such as the use of visually ambiguous yet canonically\nnonequivalent letters and the mixing of letters from different orthographies.\nAmong the contributing conflating factors are the lack of input methods, the\ninstability of modern orthographies, insufficient literacy, and loss or lack of\northographic tradition. We evaluate the effects of script normalization on\neight languages from diverse language families in the Perso-Arabic script\ndiaspora on machine translation and statistical language modeling tasks. Our\nresults indicate statistically significant improvements in performance in most\nconditions for all the languages considered when normalization is applied. We\nargue that better understanding and representation of Perso-Arabic script\nvariation within regional orthographic traditions, where those are present, is\ncrucial for further progress of modern computational NLP techniques especially\nfor languages with a paucity of resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doctor_R/0/1/0/all/0/1\">Raiomond Doctor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutkin_A/0/1/0/all/0/1\">Alexander Gutkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johny_C/0/1/0/all/0/1\">Cibu Johny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roark_B/0/1/0/all/0/1\">Brian Roark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sproat_R/0/1/0/all/0/1\">Richard Sproat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling. (arXiv:2210.12378v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12378","description":"<p>Abstractive summarization models often generate inconsistent summaries\ncontaining factual errors or hallucinated content. Recent works focus on\ncorrecting factual errors in generated summaries via post-editing. Such\ncorrection models are trained using adversarial non-factual summaries\nconstructed using heuristic rules for injecting errors. However, generating\nnon-factual summaries using heuristics often does not generalize well to actual\nmodel errors. In this work, we propose to generate hard, representative\nsynthetic examples of non-factual summaries through infilling language models.\nWith this data, we train a more robust fact-correction model to post-edit the\nsummaries to improve factual consistency. Through quantitative and qualitative\nexperiments on two popular summarization datasets -- CNN/DM and XSum -- we show\nthat our approach vastly outperforms prior methods in correcting erroneous\nsummaries. Our model -- FactEdit -- improves factuality scores by over ~11\npoints on CNN/DM and over ~31 points on XSum on average across multiple\nsummarization models, producing more factual summaries while maintaining\ncompetitive summarization quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1\">Vidhisha Balachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?. (arXiv:2210.12770v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12770","description":"<p>Pre-trained language models (PLMs) have been deployed in many natural\nlanguage processing (NLP) tasks and in various domains. Language model\npre-training from general or mixed domain rich data plus fine-tuning using\nsmall amounts of available data in a low resource domain demonstrated\nbeneficial results by researchers. In this work, we question this statement and\nverify if BERT-based PLMs from the biomedical domain can perform well in\nclinical text mining tasks via fine-tuning. We test the state-of-the-art\nmodels, i.e. Bioformer which is pre-trained on a large amount of biomedical\ndata from PubMed corpus. We use a historical n2c2 clinical NLP challenge\ndataset for fine-tuning its task-adapted version (BioformerApt), and show that\ntheir performances are actually very low. We also present our own end-to-end\nmodel, TransformerCRF, which is developed using Transformer and conditional\nrandom fields (CRFs) as encoder and decoder. We further create a new variation\nmodel by adding a CRF layer on top of PLM Bioformer (BioformerCRF). We\ninvestigate the performances of TransformerCRF on clinical text mining tasks by\ntraining from scratch using a limited amount of data, as well as the model\nBioformerCRF. Experimental evaluation shows that, in a \\textit{constrained\nsetting}, all tested models are \\textit{far from ideal} regarding extreme\nlow-frequency special token recognition, even though they can achieve\nrelatively higher accuracy on overall text tagging. Our models including source\ncodes will be hosted at \\url{https://github.com/poethan/TransformerCRF}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuping Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonini_V/0/1/0/all/0/1\">Valerio Antonini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks. (arXiv:2210.12786v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12786","description":"<p>Humans can reason compositionally whilst grounding language utterances to the\nreal world. Recent benchmarks like ReaSCAN use navigation tasks grounded in a\ngrid world to assess whether neural models exhibit similar capabilities. In\nthis work, we present a simple transformer-based model that outperforms\nspecialized architectures on ReaSCAN and a modified version of gSCAN. On\nanalyzing the task, we find that identifying the target location in the grid\nworld is the main challenge for the models. Furthermore, we show that a\nparticular split in ReaSCAN, which tests depth generalization, is unfair. On an\namended version of this split, we show that transformers can generalize to\ndeeper input structures. Finally, we design a simpler grounded compositional\ngeneralization task, RefEx, to investigate how transformers reason\ncompositionally. We show that a single self-attention layer with a single head\ngeneralizes to novel combinations of object attributes. Moreover, we derive a\nprecise mathematical construction of the transformer's computations from the\nlearned network. Overall, we provide valuable insights about the grounded\ncompositional generalization task and the behaviour of transformers on it,\nwhich would be useful for researchers working in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sikarwar_A/0/1/0/all/0/1\">Ankur Sikarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Arkil Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Navin Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Automated Essay Scoring using Transformer Models. (arXiv:2210.12809v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12809","description":"<p>Automated essay scoring is one of the most important problem in Natural\nLanguage Processing. It has been explored for a number of years, and it remains\npartially solved. In addition to its economic and educational usefulness, it\npresents research problems. Transfer learning has proved to be beneficial in\nNLP. Data augmentation techniques have also helped build state-of-the-art\nmodels for automated essay scoring. Many works in the past have attempted to\nsolve this problem by using RNNs, LSTMs, etc. This work examines the\ntransformer models like BERT, RoBERTa, etc. We empirically demonstrate the\neffectiveness of transformer models and data augmentation for automated essay\ngrading across many topics using a single model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kshitij Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Information Alleviates Hallucinations in Abstractive Summarization. (arXiv:2210.13210v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13210","description":"<p>Despite significant progress in the quality of language generated from\nabstractive summarization models, these models still exhibit the tendency to\nhallucinate, i.e., output content not supported by the source document. A\nnumber of works have tried to fix--or at least uncover the source of--the\nproblem with limited success. In this paper, we identify a simple criterion\nunder which models are significantly more likely to assign more probability to\nhallucinated content during generation: high model uncertainty. This finding\noffers a potential explanation for hallucinations: models default to favoring\ntext with high marginal probability, i.e., high-frequency occurrences in the\ntraining set, when uncertain about a continuation. It also motivates possible\nroutes for real-time intervention during decoding to prevent such\nhallucinations. We propose a decoding strategy that switches to optimizing for\npointwise mutual information of the source and target token--rather than purely\nthe probability of the target token--when the model exhibits uncertainty.\nExperiments on the XSum dataset show that our method decreases the probability\nof hallucinated tokens while maintaining the Rouge and BertS scores of\ntop-performing decoding strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poel_L/0/1/0/all/0/1\">Liam van der Poel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation. (arXiv:2210.13832v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13832","description":"<p>Recent model-based reference-free metrics for open-domain dialogue evaluation\nexhibit promising correlations with human judgment. However, they either\nperform turn-level evaluation or look at a single dialogue quality dimension.\nOne would expect a good evaluation metric to assess multiple quality dimensions\nat the dialogue level. To this end, we are motivated to propose a\nmulti-dimensional dialogue-level metric, which consists of three sub-metrics\nwith each targeting a specific dimension. The sub-metrics are trained with\nnovel self-supervised objectives and exhibit strong correlations with human\njudgment for their respective dimensions. Moreover, we explore two approaches\nto combine the sub-metrics: metric ensemble and multitask learning. Both\napproaches yield a holistic metric that significantly outperforms individual\nsub-metrics. Compared to the existing state-of-the-art metric, the combined\nmetrics achieve around 16% relative improvement on average across three\nhigh-quality dialogue-level evaluation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DHaro_L/0/1/0/all/0/1\">Luis Fernando D&#x27;Haro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrichs_T/0/1/0/all/0/1\">Thomas Friedrichs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From exemplar to copy: the scribal appropriation of a Hadewijch manuscript computationally explored. (arXiv:2210.14061v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14061","description":"<p>This study is devoted to two of the oldest known manuscripts in which the\noeuvre of the medieval mystical author Hadewijch has been preserved: Brussels,\nKBR, 2879-2880 (ms. A) and Brussels, KBR, 2877-2878 (ms. B). On the basis of\ncodicological and contextual arguments, it is assumed that the scribe who\nproduced B used A as an exemplar. While the similarities in both layout and\ncontent between the two manuscripts are striking, the present article seeks to\nidentify the differences. After all, regardless of the intention to produce a\ncopy that closely follows the exemplar, subtle linguistic variation is\napparent. Divergences relate to spelling conventions, but also to the way in\nwhich words are abbreviated (and the extent to which abbreviations occur). The\npresent study investigates the spelling profiles of the scribes who produced\nmss. A and B in a computational way. In the first part of this study, we will\npresent both manuscripts in more detail, after which we will consider prior\nresearch carried out on scribal profiling. The current study both builds and\nexpands on Kestemont (2015). Next, we outline the methodology used to analyse\nand measure the degree of scribal appropriation that took place when ms. B was\ncopied off the exemplar ms. A. After this, we will discuss the results\nobtained, focusing on the scribal variation that can be found both at the level\nof individual words and n-grams. To this end, we use machine learning to\nidentify the most distinctive features that separate manuscript A from B.\nFinally, we look at possible diachronic trends in the appropriation by B's\nscribe of his exemplar. We argue that scribal takeovers in the exemplar impacts\nthe practice of the copying scribe, while transitions to a different content\nmatter cause little to no effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haverals_W/0/1/0/all/0/1\">Wouter Haverals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kestemont_M/0/1/0/all/0/1\">Mike Kestemont</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Search Is What You Need For Neural Text Generation. (arXiv:2210.14140v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14140","description":"<p>Generating text with autoregressive language models (LMs) is of great\nimportance to many natural language processing (NLP) applications. Previous\nsolutions for this task often produce text that contains degenerative\nexpressions or lacks semantic consistency. Recently, Su et al. introduced a new\ndecoding method, contrastive search, based on the isotropic representation\nspace of the language model and obtained new state of the art on various\nbenchmarks. Additionally, Su et al. argued that the representations of\nautoregressive LMs (e.g. GPT-2) are intrinsically anisotropic which is also\nshared by previous studies. Therefore, to ensure the language model follows an\nisotropic distribution, Su et al. proposed a contrastive learning scheme,\nSimCTG, which calibrates the language model's representations through\nadditional training.\n</p>\n<p>In this study, we first answer the question: \"Are autoregressive LMs really\nanisotropic?\". To this end, we extensively evaluate the isotropy of LMs across\n16 major languages. Surprisingly, we find that the anisotropic problem only\nexists in the two specific English GPT-2-small/medium models. On the other\nhand, all other evaluated LMs are naturally isotropic which is in contrast to\nthe conclusion drawn by previous studies. Based on our findings, we further\nassess the contrastive search decoding method using off-the-shelf LMs on four\ngeneration tasks across 16 languages. Our experimental results demonstrate that\ncontrastive search significantly outperforms previous decoding methods without\nany additional training. More notably, on 12 out of the 16 evaluated languages,\ncontrastive search performs comparably with human-level performances as judged\nby human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"arXivEdits: Understanding the Human Revision Process in Scientific Writing. (arXiv:2210.15067v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15067","description":"<p>Scientific publications are the primary means to communicate research\ndiscoveries, where the writing quality is of crucial importance. However, prior\nwork studying the human editing process in this domain mainly focused on the\nabstract or introduction sections, resulting in an incomplete picture. In this\nwork, we provide a complete computational framework for studying text revision\nin scientific writing. We first introduce arXivEdits, a new annotated corpus of\n751 full papers from arXiv with gold sentence alignment across their multiple\nversions of revision, as well as fine-grained span-level edits and their\nunderlying intentions for 1,000 sentence pairs. It supports our data-driven\nanalysis to unveil the common strategies practiced by researchers for revising\ntheir papers. To scale up the analysis, we also develop automatic methods to\nextract revision at document-, sentence-, and word-levels. A neural CRF\nsentence alignment model trained on our corpus achieves 93.8 F1, enabling the\nreliable matching of sentences between different versions. We formulate the\nedit extraction task as a span alignment problem, and our proposed method\nextracts more fine-grained and explainable edits, compared to the commonly used\ndiff algorithm. An intention classifier trained on our dataset achieves 78.9 F1\non the fine-grained intent classification task. Our data and system are\nreleased at tiny.one/arxivedits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1\">Samuel Stevens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Language-driven Scientific AI. (arXiv:2210.15327v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15327","description":"<p>Inspired by recent and revolutionary developments in AI, particularly in\nlanguage understanding and generation, we set about designing AI systems that\nare able to address complex scientific tasks that challenge human capabilities\nto make new discoveries. Central to our approach is the notion of natural\nlanguage as core representation, reasoning, and exchange format between\nscientific AI and human scientists. In this paper, we identify and discuss some\nof the main research challenges to accomplish such vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Perez_J/0/1/0/all/0/1\">Jos&#xe9; Manuel G&#xf3;mez-P&#xe9;rez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance. (arXiv:2210.16031v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.16031","description":"<p>Diffusion generative models have recently greatly improved the power of\ntext-conditioned image generation. Existing image generation models mainly\ninclude text conditional diffusion model and cross-modal guided diffusion\nmodel, which are good at small scene image generation and complex scene image\ngeneration respectively. In this work, we propose a simple yet effective\napproach, namely UPainting, to unify simple and complex scene image generation,\nas shown in Figure 1. Based on architecture improvements and diverse guidance\nschedules, UPainting effectively integrates cross-modal guidance from a\npretrained image-text matching model into a text conditional diffusion model\nthat utilizes a pretrained Transformer language model as the text encoder. Our\nkey findings is that combining the power of large-scale Transformer language\nmodel in understanding language and image-text matching model in capturing\ncross-modal semantics and style, is effective to improve sample fidelity and\nimage-text alignment of image generation. In this way, UPainting has a more\ngeneral image generation capability, which can generate images of both simple\nand complex scenes more effectively. To comprehensively compare text-to-image\nmodels, we further create a more general benchmark, UniBench, with well-written\nChinese and English prompts in both simple and complex scenes. We compare\nUPainting with recent models and find that UPainting greatly outperforms other\nmodels in terms of caption similarity and image fidelity in both simple and\ncomplex scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xue Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhanpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhifan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qiaoqiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yajuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}