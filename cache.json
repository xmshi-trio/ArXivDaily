{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Mortality Prediction Models with Clinical Notes Using Sparse Attention at the Word and Sentence Levels. (arXiv:2212.06267v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06267","description":"<p>Intensive Care in-hospital mortality prediction has various clinical\napplications. Neural prediction models, especially when capitalising on\nclinical notes, have been put forward as improvement on currently existing\nmodels. However, to be acceptable these models should be performant and\ntransparent. This work studies different attention mechanisms for clinical\nneural prediction models in terms of their discrimination and calibration.\nSpecifically, we investigate sparse attention as an alternative to dense\nattention weights in the task of in-hospital mortality prediction from clinical\nnotes. We evaluate the attention mechanisms based on: i) local self-attention\nover words in a sentence, and ii) global self-attention with a transformer\narchitecture across sentences. We demonstrate that the sparse mechanism\napproach outperforms the dense one for the local self-attention in terms of\npredictive performance with a publicly available dataset, and puts higher\nattention to prespecified relevant directive words. The performance at the\nsentence level, however, deteriorates as sentences including the influential\ndirective words tend to be dropped all together.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rios_M/0/1/0/all/0/1\">Miguel Rios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_Hanna_A/0/1/0/all/0/1\">Ameen Abu-Hanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Despite \"super-human\" performance, current LLMs are unsuited for decisions about ethics and safety. (arXiv:2212.06295v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06295","description":"<p>Large language models (LLMs) have exploded in popularity in the past few\nyears and have achieved undeniably impressive results on benchmarks as varied\nas question answering and text summarization. We provide a simple new prompting\nstrategy that leads to yet another supposedly \"super-human\" result, this time\noutperforming humans at common sense ethical reasoning (as measured by accuracy\non a subset of the ETHICS dataset). Unfortunately, we find that relying on\naverage performance to judge capabilities can be highly misleading. LLM errors\ndiffer systematically from human errors in ways that make it easy to craft\nadversarial examples, or even perturb existing examples to flip the output\nlabel. We also observe signs of inverse scaling with model size on some\nexamples, and show that prompting models to \"explain their reasoning\" often\nleads to alarming justifications of unethical actions. Our results highlight\nhow human-like performance does not necessarily imply human-like understanding\nor reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_J/0/1/0/all/0/1\">Joshua Albrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitanidis_E/0/1/0/all/0/1\">Ellie Kitanidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetterman_A/0/1/0/all/0/1\">Abraham J. Fetterman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Massively Multilingual Natural Language Understanding 2022 (MMNLU-22) Workshop and Competition. (arXiv:2212.06346v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06346","description":"<p>Despite recent progress in Natural Language Understanding (NLU), the creation\nof multilingual NLU systems remains a challenge. It is common to have NLU\nsystems limited to a subset of languages due to lack of available data. They\nalso often vary widely in performance. We launch a three-phase approach to\naddress the limitations in NLU and help propel NLU technology to new heights.\nWe release a 52 language dataset called the Multilingual Amazon SLU resource\npackage (SLURP) for Slot-filling, Intent classification, and Virtual assistant\nEvaluation, or MASSIVE, in an effort to address parallel data availability for\nvoice assistants. We organize the Massively Multilingual NLU 2022 Challenge to\nprovide a competitive environment and push the state-of-the art in the\ntransferability of models into other languages. Finally, we host the first\nMassively Multilingual NLU workshop which brings these components together. The\nMMNLU workshop seeks to advance the science behind multilingual NLU by\nproviding a platform for the presentation of new research in the field and\nconnecting teams working on this research direction. This paper summarizes the\ndataset, workshop and the competition and the findings of each phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hench_C/0/1/0/all/0/1\">Christopher Hench</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peris_C/0/1/0/all/0/1\">Charith Peris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+FitzGerald_J/0/1/0/all/0/1\">Jack FitzGerald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_K/0/1/0/all/0/1\">Kay Rottmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technical Report -- Competition Solution for Prompt Tuning using Pretrained Language Model. (arXiv:2212.06369v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06369","description":"<p>Prompt tuning recently becomes a hot-spot in the applications of large\npretrained language models on specific downstream tasks. Regarding the Language\nModel as a Service (LMaaS), black-box tuning using derivative-free optimization\n(DFO) provides a novel approach to expand the practical scenarios of pretrained\nmodels and enrich the researches of few-shot learning. In this report, we\npresent our solution in this competition that is based on the LMaaS scenario.\nOur solution consists of several modifications to BBTv2, including multiple\nlabel words, selection of P0, rolling update strategy, multi-task loss from MLP\nclassifier, and finally using the ensemble method to further improve\ngeneralization ability. We also shared some strategies that we tried but didn't\nuse in the final submission for further discussion. In the end we raised a\nquestion about the SNLI dataset and the impact on the results, as well as our\nconcerns about the competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiang-Long Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wu-He Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiao-Lei Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06373","description":"<p>Current approaches to empathetic response generation typically encode the\nentire dialogue history directly and put the output into a decoder to generate\nfriendly feedback. These methods focus on modelling contextual information but\nneglect capturing the direct intention of the speaker. We argue that the last\nutterance in the dialogue empirically conveys the intention of the speaker.\nConsequently, we propose a novel model named InferEM for empathetic response\ngeneration. We separately encode the last utterance and fuse it with the entire\ndialogue through multi-head attention based intention fusion module to capture\nthe speaker's intention. Besides, we utilize previous utterances to predict the\nlast utterance, which simulates human's psychology to guess what the\ninterlocutor may speak in advance. To balance the optimizing rates of the\nutterance prediction and response generation, a multi-task learning strategy is\ndesigned for InferEM. Experimental results demonstrate the plausibility and\nvalidity of InferEM in improving empathetic expression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1\">Guoqing Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhigang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a general purpose machine translation system for Sranantongo. (arXiv:2212.06383v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06383","description":"<p>Machine translation for Sranantongo (Sranan, srn), a low-resource Creole\nlanguage spoken predominantly in Surinam, is virgin territory. In this study we\ncreate a general purpose machine translation system for srn. In order to\nfacilitate this research, we introduce the SRNcorpus, a collection of parallel\nDutch (nl) to srn and monolingual srn data. We experiment with a wide range of\nproven machine translation methods. Our results demonstrate a strong baseline\nmachine translation system for srn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zwennicker_J/0/1/0/all/0/1\">Just Zwennicker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stap_D/0/1/0/all/0/1\">David Stap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models of Different Modalities. (arXiv:2212.06385v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06385","description":"<p>Recently, the success of pre-training in text domain has been fully extended\nto vision, audio, and cross-modal scenarios. The proposed pre-training models\nof different modalities are showing a rising trend of homogeneity in their\nmodel structures, which brings the opportunity to implement different\npre-training models within a uniform framework. In this paper, we present\nTencentPretrain, a toolkit supporting pre-training models of different\nmodalities. The core feature of TencentPretrain is the modular design. The\ntoolkit uniformly divides pre-training models into 5 components: embedding,\nencoder, target embedding, decoder, and target. As almost all of common modules\nare provided in each component, users can choose the desired modules from\ndifferent components to build a complete pre-training model. The modular design\nenables users to efficiently reproduce existing pre-training models or build\nbrand-new one. We test the toolkit on text, vision, and audio benchmarks and\nshow that it can match the performance of the original implementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yudong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1\">Cheng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1\">Rong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weijie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiren Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_N/0/1/0/all/0/1\">Ningyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haoyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Weiquan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weigang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Taiqiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wenhang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liqun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feifei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoshuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xingwu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhanhui Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiaoyong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Kimmo Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style-Label-Free: Cross-Speaker Style Transfer by Quantized VAE and Speaker-wise Normalization in Speech Synthesis. (arXiv:2212.06397v1 [cs.SD])","link":"http://arxiv.org/abs/2212.06397","description":"<p>Cross-speaker style transfer in speech synthesis aims at transferring a style\nfrom source speaker to synthesised speech of a target speaker's timbre. Most\nprevious approaches rely on data with style labels, but manually-annotated\nlabels are expensive and not always reliable. In response to this problem, we\npropose Style-Label-Free, a cross-speaker style transfer method, which can\nrealize the style transfer from source speaker to target speaker without style\nlabels. Firstly, a reference encoder structure based on quantized variational\nautoencoder (Q-VAE) and style bottleneck is designed to extract discrete style\nrepresentations. Secondly, a speaker-wise batch normalization layer is proposed\nto reduce the source speaker leakage. In order to improve the style extraction\nability of the reference encoder, a style invariant and contrastive data\naugmentation method is proposed. Experimental results show that the method\noutperforms the baseline. We provide a website with audio samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiang_C/0/1/0/all/0/1\">Chunyu Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Peng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1\">Hao Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaorui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lisan: Yemenu, Irqi, Libyan, and Sudanese Arabic Dialect Copora with Morphological Annotations. (arXiv:2212.06468v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06468","description":"<p>This article presents morphologically-annotated Yemeni, Sudanese, Iraqi, and\nLibyan Arabic dialects Lisan corpora. Lisan features around 1.2 million tokens.\nWe collected the content of the corpora from several social media platforms.\nThe Yemeni corpus (~ 1.05M tokens) was collected automatically from Twitter.\nThe corpora of the other three dialects (~ 50K tokens each) came manually from\nFacebook and YouTube posts and comments.\n</p>\n<p>Thirty five (35) annotators who are native speakers of the target dialects\ncarried out the annotations. The annotators segemented all words in the four\ncorpora into prefixes, stems and suffixes and labeled each with different\nmorphological features such as part of speech, lemma, and a gloss in English.\nAn Arabic Dialect Annotation Toolkit ADAT was developped for the purpose of the\nannation. The annotators were trained on a set of guidelines and on how to use\nADAT. We developed ADAT to assist the annotators and to ensure compatibility\nwith SAMA and Curras tagsets. The tool is open source, and the four corpora are\nalso available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jarrar_M/0/1/0/all/0/1\">Mustafa Jarrar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaraket_F/0/1/0/all/0/1\">Fadi A Zaraket</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammouda_T/0/1/0/all/0/1\">Tymaa Hammouda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alavi_D/0/1/0/all/0/1\">Daanish Masood Alavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waahlisch_M/0/1/0/all/0/1\">Martin Waahlisch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distantly-Supervised Named Entity Recognition with Adaptive Teacher Learning and Fine-grained Student Ensemble. (arXiv:2212.06522v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06522","description":"<p>Distantly-Supervised Named Entity Recognition (DS-NER) effectively alleviates\nthe data scarcity problem in NER by automatically generating training samples.\nUnfortunately, the distant supervision may induce noisy labels, thus\nundermining the robustness of the learned models and restricting the practical\napplication. To relieve this problem, recent works adopt self-training\nteacher-student frameworks to gradually refine the training labels and improve\nthe generalization ability of NER models. However, we argue that the\nperformance of the current self-training frameworks for DS-NER is severely\nunderestimated by their plain designs, including both inadequate student\nlearning and coarse-grained teacher updating. Therefore, in this paper, we make\nthe first attempt to alleviate these issues by proposing: (1) adaptive teacher\nlearning comprised of joint training of two teacher-student networks and\nconsidering both consistent and inconsistent predictions between two teachers,\nthus promoting comprehensive student learning. (2) fine-grained student\nensemble that updates each fragment of the teacher model with a temporal moving\naverage of the corresponding fragment of the student, which enhances consistent\npredictions on each model fragment against noise. To verify the effectiveness\nof our proposed method, we conduct experiments on four DS-NER datasets. The\nexperimental results demonstrate that our method significantly surpasses\nprevious SOTA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1\">Baoxing Huai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modelling Stance Detection as Textual Entailment Recognition and Leveraging Measurement Knowledge from Social Sciences. (arXiv:2212.06543v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06543","description":"<p>Stance detection (SD) can be considered a special case of textual entailment\nrecognition (TER), a generic natural language task. Modelling SD as TER may\noffer benefits like more training data and a more general learning scheme. In\nthis paper, we present an initial empirical analysis of this approach. We apply\nit to a difficult but relevant test case where no existing labelled SD dataset\nis available, because this is where modelling SD as TER may be especially\nhelpful. We also leverage measurement knowledge from social sciences to improve\nmodel performance. We discuss our findings and suggest future research\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qixiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giachanou_A/0/1/0/all/0/1\">Anastasia Giachanou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagheri_A/0/1/0/all/0/1\">Ayoub Bagheri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localized Latent Updates for Fine-Tuning Vision-Language Models. (arXiv:2212.06556v1 [cs.CV])","link":"http://arxiv.org/abs/2212.06556","description":"<p>Although massive pre-trained vision-language models like CLIP show impressive\ngeneralization capabilities for many tasks, still it often remains necessary to\nfine-tune them for improved performance on specific datasets. When doing so, it\nis desirable that updating the model is fast and that the model does not lose\nits capabilities on data outside of the dataset, as is often the case with\nclassical fine-tuning approaches. In this work we suggest a lightweight\nadapter, that only updates the models predictions close to seen datapoints. We\ndemonstrate the effectiveness and speed of this relatively simple approach in\nthe context of few-shot learning, where our results both on classes seen and\nunseen during training are comparable with or improve on the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ibing_M/0/1/0/all/0/1\">Moritz Ibing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_I/0/1/0/all/0/1\">Isaak Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobbelt_L/0/1/0/all/0/1\">Leif Kobbelt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Fake News Detection with Heterogeneous Social Media Context Graphs. (arXiv:2212.06560v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06560","description":"<p>Fake news detection has become a research area that goes way beyond a purely\nacademic interest as it has direct implications on our society as a whole.\nRecent advances have primarily focused on textbased approaches. However, it has\nbecome clear that to be effective one needs to incorporate additional,\ncontextual information such as spreading behaviour of news articles and user\ninteraction patterns on social media. We propose to construct heterogeneous\nsocial context graphs around news articles and reformulate the problem as a\ngraph classification task. Exploring the incorporation of different types of\ninformation (to get an idea as to what level of social context is most\neffective) and using different graph neural network architectures indicates\nthat this approach is highly effective with robust results on a common\nbenchmark dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Donabauer_G/0/1/0/all/0/1\">Gregor Donabauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruschwitz_U/0/1/0/all/0/1\">Udo Kruschwitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Category Theory for Quantum Natural Language Processing. (arXiv:2212.06615v1 [math.CT])","link":"http://arxiv.org/abs/2212.06615","description":"<p>This thesis introduces quantum natural language processing (QNLP) models\nbased on a simple yet powerful analogy between computational linguistics and\nquantum mechanics: grammar as entanglement. The grammatical structure of text\nand sentences connects the meaning of words in the same way that entanglement\nstructure connects the states of quantum systems. Category theory allows to\nmake this language-to-qubit analogy formal: it is a monoidal functor from\ngrammar to vector spaces. We turn this abstract analogy into a concrete\nalgorithm that translates the grammatical structure onto the architecture of\nparameterised quantum circuits. We then use a hybrid classical-quantum\nalgorithm to train the model so that evaluating the circuits computes the\nmeaning of sentences in data-driven tasks.\n</p>\n<p>The implementation of QNLP models motivated the development of DisCoPy\n(Distributional Compositional Python), the toolkit for applied category theory\nof which the first chapter gives a comprehensive overview. String diagrams are\nthe core data structure of DisCoPy, they allow to reason about computation at a\nhigh level of abstraction. We show how they can encode both grammatical\nstructures and quantum circuits, but also logical formulae, neural networks or\narbitrary Python code. Monoidal functors allow to translate these abstract\ndiagrams into concrete computation, interfacing with optimised task-specific\nlibraries.\n</p>\n<p>The second chapter uses DisCopy to implement QNLP models as parameterised\nfunctors from grammar to quantum circuits. It gives a first proof-of-concept\nfor the more general concept of functorial learning: generalising machine\nlearning from functions to functors by learning from diagram-like data. In\norder to learn optimal functor parameters via gradient descent, we introduce\nthe notion of diagrammatic differentiation: a graphical calculus for computing\nthe gradients of parameterised diagrams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Toumi_A/0/1/0/all/0/1\">Alexis Toumi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Categorical Tools for Natural Language Processing. (arXiv:2212.06636v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06636","description":"<p>This thesis develops the translation between category theory and\ncomputational linguistics as a foundation for natural language processing. The\nthree chapters deal with syntax, semantics and pragmatics. First, string\ndiagrams provide a unified model of syntactic structures in formal grammars.\nSecond, functors compute semantics by turning diagrams into logical, tensor,\nneural or quantum computation. Third, the resulting functorial models can be\ncomposed to form games where equilibria are the solutions of language\nprocessing tasks. This framework is implemented as part of DisCoPy, the Python\nlibrary for computing with string diagrams. We describe the correspondence\nbetween categorical, linguistic and computational structures, and demonstrate\ntheir applications in compositional natural language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Felice_G/0/1/0/all/0/1\">Giovanni de Felice</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Text-to-Text Multi-Task Learners Suffer from Task Conflict?. (arXiv:2212.06645v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06645","description":"<p>Traditional multi-task learning architectures train a single model across\nmultiple tasks through a shared encoder followed by task-specific decoders.\nLearning these models often requires specialized training algorithms that\naddress task-conflict in the shared parameter updates, which otherwise can lead\nto negative transfer. A new type of multi-task learning within NLP homogenizes\nmulti-task architectures as a shared encoder and language model decoder, which\ndoes surprisingly well across a range of diverse tasks. Does this new\narchitecture suffer from task-conflicts that require specialized training\nalgorithms? We study how certain factors in the shift towards text-to-text\nmodels affects multi-task conflict and negative transfer, finding that both\ndirectional conflict and transfer are surprisingly constant across\narchitectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mueller_D/0/1/0/all/0/1\">David Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_N/0/1/0/all/0/1\">Nicholas Andrews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Text-based Personality Computing: Challenges and Future Directions. (arXiv:2212.06711v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06711","description":"<p>Text-based personality computing (TPC) has gained many research interests in\nNLP. In this paper, we describe 15 challenges that we consider deserving the\nattention of the research community. These challenges are organized by the\nfollowing topics: personality taxonomies, measurement quality, datasets,\nperformance evaluation, modelling choices, as well as ethics and fairness. When\naddressing each challenge, not only do we combine perspectives from both NLP\nand social sciences, but also offer concrete suggestions towards more valid and\nreliable TPC research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qixiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giachanou_A/0/1/0/all/0/1\">Anastasia Giachanou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagheri_A/0/1/0/all/0/1\">Ayoub Bagheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boeschoten_L/0/1/0/all/0/1\">Laura Boeschoten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesteren_E/0/1/0/all/0/1\">Erik-Jan van Kesteren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamalabad_M/0/1/0/all/0/1\">Mahdi Shafiee Kamalabad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberski_D/0/1/0/all/0/1\">Daniel L Oberski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Prompting: Scaling In-Context Learning to 1,000 Examples. (arXiv:2212.06713v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06713","description":"<p>Large language models have exhibited intriguing in-context learning\ncapability, achieving promising zero- and few-shot performance without updating\nthe parameters. However, conventional in-context learning is usually restricted\nby length constraints, rendering it ineffective to absorb supervision from a\nlarge number of examples. In order to go beyond few shots, we introduce\nstructured prompting that breaks the length limit and scales in-context\nlearning to thousands of examples. Specifically, demonstration examples are\nseparately encoded with well-designed position embeddings, and then they are\njointly attended by the test example using a rescaled attention mechanism. So\nwe can scale the number of exemplars with linear complexity instead of\nquadratic complexity with respect to length. Experimental results on a diverse\nset of tasks show that our approach improves end-task performance and reduces\nevaluation variance over conventional in-context learning as the number of\ndemonstration examples increases. Code has been released at\nhttps://aka.ms/structured-prompting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yaru Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yutao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhixiong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Turing Deception. (arXiv:2212.06721v1 [cs.LG])","link":"http://arxiv.org/abs/2212.06721","description":"<p>This research revisits the classic Turing test and compares recent large\nlanguage models such as ChatGPT for their abilities to reproduce human-level\ncomprehension and compelling text generation. Two task challenges --\nsummarization, and question answering -- prompt ChatGPT to produce original\ncontent (98-99%) from a single text entry and also sequential questions\noriginally posed by Turing in 1950. The question of a machine fooling a human\njudge recedes in this work relative to the question of \"how would one prove\nit?\" The original contribution of the work presents a metric and simple\ngrammatical set for understanding the writing mechanics of chatbots in\nevaluating their readability and statistical clarity, engagement, delivery, and\noverall quality. While Turing's original prose scores at least 14% below the\nmachine-generated output, the question of whether an algorithm displays hints\nof Turing's truly original thoughts (the \"Lovelace 2.0\" test) remains\nunanswered and potentially unanswerable for now.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciolino_M/0/1/0/all/0/1\">Matt Ciolino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages. (arXiv:2212.06742v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06742","description":"<p>Software engineers working with the same programming language (PL) may speak\ndifferent natural languages (NLs) and vice versa, erecting huge barriers to\ncommunication and working efficiency. Recent studies have demonstrated the\neffectiveness of generative pre-training in computer programs, yet they are\nalways English-centric. In this work, we step towards bridging the gap between\nmultilingual NLs and multilingual PLs for large language models (LLMs). We\nrelease ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs.\nWe employ two methods for universal cross-lingual pre-training: span-corruption\nlanguage modeling that learns patterns from monolingual NL or PL; and\npivot-based translation language modeling that relies on parallel data of many\nNLs and PLs. Extensive results show that ERNIE-Code outperforms previous\nmultilingual LLMs for PL or NL across a wide range of end tasks of code\nintelligence, including multilingual code-to-text, text-to-code, code-to-code,\nand text-to-text generation. We further show its advantage of zero-shot\nprompting on multilingual code summarization and text-to-text translation. We\nwill make our code and pre-trained models publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yekun Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_C/0/1/0/all/0/1\">Chao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Earthquake Impact Analysis Based on Text Mining and Social Media Analytics. (arXiv:2212.06765v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06765","description":"<p>Earthquakes have a deep impact on wide areas, and emergency rescue operations\nmay benefit from social media information about the scope and extent of the\ndisaster. Therefore, this work presents a text miningbased approach to collect\nand analyze social media data for early earthquake impact analysis. First,\ndisasterrelated microblogs are collected from the Sina microblog based on\ncrawler technology. Then, after data cleaning a series of analyses are\nconducted including (1) the hot words analysis, (2) the trend of the number of\nmicroblogs, (3) the trend of public opinion sentiment, and (4) a keyword and\nrule-based text classification for earthquake impact analysis. Finally, two\nrecent earthquakes with the same magnitude and focal depth in China are\nanalyzed to compare their impacts. The results show that the public opinion\ntrend analysis and the trend of public opinion sentiment can estimate the\nearthquake's social impact at an early stage, which will be helpful to\ndecision-making and rescue management.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hong-Zheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu-Cheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xin-Zheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jia-Rui Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Demonstrations Improve In-context Compositional Generalization. (arXiv:2212.06800v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06800","description":"<p>In-context learning has shown great success in i.i.d semantic parsing splits,\nwhere the training and test sets are drawn from the same distribution. In this\nsetup, models are typically prompted with demonstrations that are similar to\nthe input question. However, in the setup of compositional generalization,\nwhere models are tested on outputs with structures that are absent from the\ntraining set, selecting similar demonstrations is insufficient, as often no\nexample will be similar enough to the input. In this work, we propose a method\nto select diverse demonstrations that aims to collectively cover all of the\nstructures required in the output program, in order to encourage the model to\ngeneralize to new structures from these demonstrations. We empirically show\nthat combining diverse demonstrations with in-context learning substantially\nimproves performance across three compositional generalization semantic parsing\ndatasets in the pure in-context learning setup and when combined with\nfinetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levy_I/0/1/0/all/0/1\">Itay Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogin_B/0/1/0/all/0/1\">Ben Bogin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A fine-grained comparison of pragmatic language understanding in humans and language models. (arXiv:2212.06801v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06801","description":"<p>Pragmatics is an essential part of communication, but it remains unclear what\nmechanisms underlie human pragmatic communication and whether NLP systems\ncapture pragmatic language understanding. To investigate both these questions,\nwe perform a fine-grained comparison of language models and humans on seven\npragmatic phenomena, using zero-shot prompting on an expert-curated set of\nEnglish materials. We ask whether models (1) select pragmatic interpretations\nof speaker utterances, (2) make similar error patterns as humans, and (3) use\nsimilar linguistic cues as humans to solve the tasks. We find that the largest\nmodels achieve high accuracy and match human error patterns: within incorrect\nresponses, models favor the literal interpretation of an utterance over\nheuristic-based distractors. We also find evidence that models and humans are\nsensitive to similar linguistic cues. Our results suggest that even\nparadigmatic pragmatic phenomena may be solved without explicit representations\nof other agents' mental states, and that artificial models can be used to gain\nmechanistic insights into human pragmatic processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jennifer Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Floyd_S/0/1/0/all/0/1\">Sammy Floyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouravlev_O/0/1/0/all/0/1\">Olessia Jouravlev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1\">Evelina Fedorenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibson_E/0/1/0/all/0/1\">Edward Gibson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RT-1: Robotics Transformer for Real-World Control at Scale. (arXiv:2212.06817v1 [cs.RO])","link":"http://arxiv.org/abs/2212.06817","description":"<p>By transferring knowledge from large, diverse, task-agnostic datasets, modern\nmachine learning models can solve specific downstream tasks either zero-shot or\nwith small task-specific datasets to a high level of performance. While this\ncapability has been demonstrated in other fields such as computer vision,\nnatural language processing or speech recognition, it remains to be shown in\nrobotics, where the generalization capabilities of the models are particularly\ncritical due to the difficulty of collecting real-world robotic data. We argue\nthat one of the keys to the success of such general robotic models lies with\nopen-ended task-agnostic training, combined with high-capacity architectures\nthat can absorb all of the diverse, robotic data. In this paper, we present a\nmodel class, dubbed Robotics Transformer, that exhibits promising scalable\nmodel properties. We verify our conclusions in a study of different model\nclasses and their ability to generalize as a function of the data size, model\nsize, and data diversity based on a large-scale data collection on real robots\nperforming real-world tasks. The project's website and videos can be found at\nrobotics-transformer.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brohan_A/0/1/0/all/0/1\">Anthony Brohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Noah Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbajal_J/0/1/0/all/0/1\">Justice Carbajal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chebotar_Y/0/1/0/all/0/1\">Yevgen Chebotar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabis_J/0/1/0/all/0/1\">Joseph Dabis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Keerthana Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1\">Karol Hausman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzog_A/0/1/0/all/0/1\">Alex Herzog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_J/0/1/0/all/0/1\">Jasmine Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibarz_J/0/1/0/all/0/1\">Julian Ibarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irpan_A/0/1/0/all/0/1\">Alex Irpan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_T/0/1/0/all/0/1\">Tomas Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jesmonth_S/0/1/0/all/0/1\">Sally Jesmonth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nikhil J Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Julian_R/0/1/0/all/0/1\">Ryan Julian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalashnikov_D/0/1/0/all/0/1\">Dmitry Kalashnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Y/0/1/0/all/0/1\">Yuheng Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_I/0/1/0/all/0/1\">Isabel Leal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kuang-Huei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malla_U/0/1/0/all/0/1\">Utsav Malla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunath_D/0/1/0/all/0/1\">Deeksha Manjunath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1\">Ofir Nachum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parada_C/0/1/0/all/0/1\">Carolina Parada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peralta_J/0/1/0/all/0/1\">Jodilyn Peralta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Emily Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pertsch_K/0/1/0/all/0/1\">Karl Pertsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quiambao_J/0/1/0/all/0/1\">Jornell Quiambao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">Kanishka Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salazar_G/0/1/0/all/0/1\">Grecia Salazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanketi_P/0/1/0/all/0/1\">Pannag Sanketi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayed_K/0/1/0/all/0/1\">Kevin Sayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jaspiar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sontakke_S/0/1/0/all/0/1\">Sumedh Sontakke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_A/0/1/0/all/0/1\">Austin Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Clayton Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Huong Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanhoucke_V/0/1/0/all/0/1\">Vincent Vanhoucke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vega_S/0/1/0/all/0/1\">Steve Vega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_Q/0/1/0/all/0/1\">Quan Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Ted Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Sichun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitkovich_B/0/1/0/all/0/1\">Brianna Zitkovich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering. (arXiv:2104.06378v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06378","description":"<p>The problem of answering questions using knowledge from pre-trained language\nmodels (LMs) and knowledge graphs (KGs) presents two challenges: given a QA\ncontext (question and answer choice), methods need to (i) identify relevant\nknowledge from large KGs, and (ii) perform joint reasoning over the QA context\nand KG. In this work, we propose a new model, QA-GNN, which addresses the above\nchallenges through two key innovations: (i) relevance scoring, where we use LMs\nto estimate the importance of KG nodes relative to the given QA context, and\n(ii) joint reasoning, where we connect the QA context and KG to form a joint\ngraph, and mutually update their representations through graph neural networks.\nWe evaluate our model on QA benchmarks in the commonsense (CommonsenseQA,\nOpenBookQA) and biomedical (MedQA-USMLE) domains. QA-GNN outperforms existing\nLM and LM+KG models, and exhibits capabilities to perform interpretable and\nstructured reasoning, e.g., correctly handling negation in questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textless Speech Emotion Conversion using Discrete and Decomposed Representations. (arXiv:2111.07402v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07402","description":"<p>Speech emotion conversion is the task of modifying the perceived emotion of a\nspeech utterance while preserving the lexical content and speaker identity. In\nthis study, we cast the problem of emotion conversion as a spoken language\ntranslation task. We use a decomposition of the speech signal into discrete\nlearned representations, consisting of phonetic-content units, prosodic\nfeatures, speaker, and emotion. First, we modify the speech content by\ntranslating the phonetic-content units to a target emotion, and then predict\nthe prosodic features based on these units. Finally, the speech waveform is\ngenerated by feeding the predicted representations into a neural vocoder. Such\na paradigm allows us to go beyond spectral and parametric changes of the\nsignal, and model non-verbal vocalizations, such as laughter insertion, yawning\nremoval, etc. We demonstrate objectively and subjectively that the proposed\nmethod is vastly superior to current approaches and even beats text-based\nsystems in terms of perceived emotion and audio quality. We rigorously evaluate\nall components of such a complex system and conclude with an extensive model\nanalysis and ablation study to better emphasize the architectural choices,\nstrengths and weaknesses of the proposed method. Samples are available under\nthe following link: https://speechbot.github.io/emotion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreuk_F/0/1/0/all/0/1\">Felix Kreuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu-Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1\">Morgane Rivi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Tuning Transformers: Vocabulary Transfer. (arXiv:2112.14569v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.14569","description":"<p>Transformers are responsible for the vast majority of recent advances in\nnatural language processing. The majority of practical natural language\nprocessing applications of these models are typically enabled through transfer\nlearning. This paper studies if corpus-specific tokenization used for\nfine-tuning improves the resulting performance of the model. Through a series\nof experiments, we demonstrate that such tokenization combined with the\ninitialization and fine-tuning strategy for the vocabulary tokens speeds up the\ntransfer and boosts the performance of the fine-tuned model. We call this\naspect of transfer facilitation vocabulary transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mosin_V/0/1/0/all/0/1\">Vladislav Mosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samenko_I/0/1/0/all/0/1\">Igor Samenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozlovskii_B/0/1/0/all/0/1\">Borislav Kozlovskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HighMMT: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning. (arXiv:2203.01311v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.01311","description":"<p>Many real-world problems are inherently multimodal, from the communicative\nmodalities humans use to express social and emotional states to the force,\nproprioception, and visual sensors ubiquitous on robots. While there has been\nan explosion of interest in multimodal representation learning, these methods\nare still largely focused on a small set of modalities, primarily in the\nlanguage, vision, and audio space. In order to accelerate generalization\ntowards diverse and understudied modalities, this paper studies efficient\nrepresentation learning for high-modality scenarios. Since adding new models\nfor every new modality or task becomes prohibitively expensive, a critical\ntechnical challenge is heterogeneity quantification: how can we measure which\nmodalities encode similar information and interactions in order to permit\nparameter sharing with previous modalities? We propose two new\ninformation-theoretic metrics for heterogeneity quantification: (1) modality\nheterogeneity studies how similar 2 modalities $\\{X_1,X_2\\}$ are by measuring\nhow much information can be transferred from $X_1$ to $X_2$, while (2)\ninteraction heterogeneity studies how similarly pairs of modalities\n$\\{X_1,X_2\\}, \\{X_3,X_4\\}$ interact by measuring how much interaction\ninformation can be transferred from $\\{X_1,X_2\\}$ to $\\{X_3,X_4\\}$. We show the\nimportance of these proposed metrics in high-modality scenarios as a way to\nautomatically prioritize the fusion of modalities that contain unique\ninformation or interactions. The result is a single model, HighMMT, that scales\nup to $10$ modalities and $15$ tasks from $5$ different research areas. Not\nonly does HighMMT outperform prior methods on the tradeoff between performance\nand efficiency, it also demonstrates a crucial scaling behavior: performance\ncontinues to improve with each modality added, and transfers to entirely new\nmodalities and tasks during fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaw_J/0/1/0/all/0/1\">Jeffrey Tsaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yudong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shentong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CORAL: Contextual Response Retrievability Loss Function for Training Dialog Generation Models. (arXiv:2205.10558v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10558","description":"<p>Natural Language Generation (NLG) represents a large collection of tasks in\nthe field of NLP. While many of these tasks have been tackled well by the\ncross-entropy (CE) loss, the task of dialog generation poses a few unique\nchallenges for this loss function. First, CE loss assumes that for any given\ninput, the only possible output is the one available as the ground truth in the\ntraining dataset. In general, this is not true for any task, as there can be\nmultiple semantically equivalent sentences, each with a different surface form.\nThis problem gets exaggerated further for the dialog generation task, as there\ncan be multiple valid responses (for a given context) that not only have\ndifferent surface forms but are also not semantically equivalent. Second, CE\nloss does not take the context into consideration while processing the response\nand, hence, it treats all ground truths with equal importance irrespective of\nthe context. But, we may want our final agent to avoid certain classes of\nresponses (e.g. bland, non-informative or biased responses) and give relatively\nhigher weightage for more context-specific responses. To circumvent these\nshortcomings of the CE loss, in this paper, we propose a novel loss function,\nCORAL, that directly optimizes recently proposed estimates of human preference\nfor generated responses. Using CORAL, we can train dialog generation models\nwithout assuming non-existence of response other than the ground-truth. Also,\nthe CORAL loss is computed based on both the context and the response.\nExtensive comparisons on two benchmark datasets show that the proposed methods\noutperform strong state-of-the-art baseline models of different sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santra_B/0/1/0/all/0/1\">Bishal Santra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadia_R/0/1/0/all/0/1\">Ravi Ghadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_A/0/1/0/all/0/1\">Arpit Dwivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Round-Trip Translation for Machine Translation Evaluation. (arXiv:2209.07351v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.07351","description":"<p>Automatic evaluation on low-resource language translation suffers from a\ndeficiency of parallel corpora. Round-trip translation could be served as a\nclever and straightforward technique to alleviate the requirement of the\nparallel evaluation corpus. However, there was an observation of obscure\ncorrelations between the evaluation scores by forward and round-trip\ntranslations in the era of statistical machine translation (SMT). In this\npaper, we report the surprising finding that round-trip translation can be used\nfor automatic evaluation without the references. Firstly, our revisit on the\nround-trip translation in SMT evaluation unveils that its long-standing\nmisunderstanding is essentially caused by copying mechanism. After removing\ncopying mechanism in SMT, round-trip translation scores can appropriately\nreflect the forward translation performance. Then, we demonstrate the\nrectification is overdue as round-trip translation could benefit multiple\nmachine translation evaluation tasks. To be more specific, round-trip\ntranslation could be used i) to predict corresponding forward translation\nscores; ii) to improve the performance of the recently advanced quality\nestimation model; and iii) to identify adversarial competitors in shared tasks\nvia cross-system verification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiongkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanli He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-Tuning Can Be Much Better Than Fine-Tuning on Cross-lingual Understanding With Multilingual Language Models. (arXiv:2210.12360v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12360","description":"<p>Pre-trained multilingual language models show significant performance gains\nfor zero-shot cross-lingual model transfer on a wide range of natural language\nunderstanding (NLU) tasks. Previously, for zero-shot cross-lingual evaluation,\npre-trained models are only fine-tuned on English data and tested on a variety\nof target languages. In this paper, we do cross-lingual evaluation on various\nNLU tasks (sentence classification, sequence labeling, question answering)\nusing prompt-tuning and compare it with fine-tuning. The results show that\nprompt tuning achieves much better cross-lingual transfer than fine-tuning\nacross datasets, with only 0.1% to 0.3% tuned parameters. Additionally, we\ndemonstrate through the analysis that prompt tuning can have better\ncross-lingual transferability of representations on downstream tasks with\nbetter aligned decision boundaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1\">Lifu Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Reuse Distractors to support Multiple Choice Question Generation in Education. (arXiv:2210.13964v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13964","description":"<p>Multiple choice questions (MCQs) are widely used in digital learning systems,\nas they allow for automating the assessment process. However, due to the\nincreased digital literacy of students and the advent of social media\nplatforms, MCQ tests are widely shared online, and teachers are continuously\nchallenged to create new questions, which is an expensive and time-consuming\ntask. A particularly sensitive aspect of MCQ creation is to devise relevant\ndistractors, i.e., wrong answers that are not easily identifiable as being\nwrong. This paper studies how a large existing set of manually created answers\nand distractors for questions over a variety of domains, subjects, and\nlanguages can be leveraged to help teachers in creating new MCQs, by the smart\nreuse of existing distractors. We built several data-driven models based on\ncontext-aware question and distractor representations, and compared them with\nstatic feature-based models. The proposed models are evaluated with automated\nmetrics and in a realistic user test with teachers. Both automatic and human\nevaluations indicate that context-aware models consistently outperform a static\nfeature-based approach. For our best-performing context-aware model, on average\n3 distractors out of the 10 shown to teachers were rated as high-quality\ndistractors. We create a performance benchmark, and make it public, to enable\ncomparison between different approaches and to introduce a more standardized\nevaluation of the task. The benchmark contains a test of 298 educational\nquestions covering multiple subjects &amp; languages and a 77k multilingual pool of\ndistractor vocabulary for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bitew_S/0/1/0/all/0/1\">Semere Kiros Bitew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadifar_A/0/1/0/all/0/1\">Amir Hadifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sterckx_L/0/1/0/all/0/1\">Lucas Sterckx</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deleu_J/0/1/0/all/0/1\">Johannes Deleu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Math Word Problem via Cooperative Reasoning induced Language Models. (arXiv:2210.16257v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16257","description":"<p>Large-scale pre-trained language models (PLMs) bring new opportunities to\nchallenge problems, especially those that need high-level intelligence, such as\nthe math word problem (MWPs). However, directly applying existing PLMs to MWPs\ncan fail as the generation process lacks sufficient supervision and thus lacks\nfast adaptivity as humans. We notice that human reasoning has a dual reasoning\nframework that consists of an immediate reaction system (system 1) and a\ndelicate reasoning system (system 2), where the entire reasoning is determined\nby their interaction. This inspires us to develop a cooperative\nreasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe),\nresulting in a human-like reasoning architecture with system 1 as the generator\nand system 2 as the verifier. In our approach, the generator is responsible for\ngenerating reasoning paths, and the verifiers are used to supervise the\nevaluation in order to obtain reliable feedback for the generator. We evaluate\nour CoRe framework on several mathematical reasoning datasets and achieve\ndecent improvement over state-of-the-art methods, up to 9.8% increase over best\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_R/0/1/0/all/0/1\">Ruyi Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effect of Multiple Replies for Natural Language Generation Chatbots. (arXiv:2210.17209v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2210.17209","description":"<p>In this research, by responding to users' utterances with multiple replies to\ncreate a group chat atmosphere, we alleviate the problem that Natural Language\nGeneration chatbots might reply with inappropriate content, thus causing a bad\nuser experience. Because according to our findings, users tend to pay attention\nto appropriate replies and ignore inappropriate replies. We conducted a 2\n(single reply vs. five replies) x 2 (anonymous avatar vs. anime avatar)\nrepeated measures experiment to compare the chatting experience in different\nconditions. The result shows that users will have a better chatting experience\nwhen receiving multiple replies at once from the NLG model compared to the\nsingle reply. Furthermore, according to the effect size of our result, to\nimprove the chatting experience for NLG chatbots which is single reply and\nanonymous avatar, providing five replies will have more benefits than setting\nan anime avatar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Eason Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Building Text-To-Speech Systems for the Next Billion Users. (arXiv:2211.09536v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.09536","description":"<p>Deep learning based text-to-speech (TTS) systems have been evolving rapidly\nwith advances in model architectures, training methodologies, and\ngeneralization across speakers and languages. However, these advances have not\nbeen thoroughly investigated for Indian language speech synthesis. Such\ninvestigation is computationally expensive given the number and diversity of\nIndian languages, relatively lower resource availability, and the diverse set\nof advances in neural TTS that remain untested. In this paper, we evaluate the\nchoice of acoustic models, vocoders, supplementary loss functions, training\nschedules, and speaker and language diversity for Dravidian and Indo-Aryan\nlanguages. Based on this, we identify monolingual models with FastPitch and\nHiFi-GAN V1, trained jointly on male and female speakers to perform the best.\nWith this setup, we train and evaluate TTS models for 13 languages and find our\nmodels to significantly improve upon existing models in all languages as\nmeasured by mean opinion scores. We open-source all models on the Bhashini\nplatform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gokul Karthik Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_P/0/1/0/all/0/1\">Praveen S V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandakumar_K/0/1/0/all/0/1\">Karthik Nandakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and Prefix-Tuning. (arXiv:2211.09783v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.09783","description":"<p>The diverse demands of different summarization tasks and their high\nannotation costs are driving a need for few-shot summarization. However,\ndespite the emergence of many summarization tasks and datasets, the current\ntraining paradigm for few-shot summarization systems ignores potentially\nshareable knowledge in heterogeneous datasets. To this end, we propose\n\\textsc{UniSumm}, a unified few-shot summarization model pre-trained with\nmultiple summarization tasks and can be prefix-tuned to excel at any few-shot\nsummarization datasets. Meanwhile, to better evaluate few-shot summarization\nsystems, under the principles of diversity and robustness, we assemble and\npublicize a new benchmark \\textsc{SummZoo}. It consists of $8$ diverse\nsummarization tasks with multiple sets of few-shot samples for each task,\ncovering both monologue and dialogue domains. Experimental results and ablation\nstudies show that \\textsc{UniSumm} outperforms strong baseline systems by a\nlarge margin across all tasks in \\textsc{SummZoo} under both automatic and\nhuman evaluations. We release our code and benchmark at\n\\url{https://github.com/microsoft/UniSumm}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PCRED: Zero-shot Relation Triplet Extraction with Potential Candidate Relation Selection and Entity Boundary Detection. (arXiv:2211.14477v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.14477","description":"<p>Zero-shot relation triplet extraction (ZeroRTE) aims to extract relation\ntriplets from unstructured texts under the zero-shot setting, where the\nrelation sets at the training and testing stages are disjoint. Previous\nstate-of-the-art method handles this challenging task by leveraging pretrained\nlanguage models to generate data as additional training samples, which\nincreases the training cost and severely constrains the model performance. To\naddress the above issues, we propose a novel method named PCRED for ZeroRTE\nwith Potential Candidate Relation Selection and Entity Boundary Detection. The\nremarkable characteristic of PCRED is that it does not rely on additional data\nand still achieves promising performance. The model adopts a relation-first\nparadigm, recognizing unseen relations through candidate relation selection.\nWith this approach, the semantics of relations are naturally infused in the\ncontext. Entities are extracted based on the context and the semantics of\nrelations subsequently. We evaluate our model on two ZeroRTE datasets. The\nexperiment results show that our method consistently outperforms previous\nworks. Our code will be available at https://anonymous.4open.science/r/PCRED.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yuquan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keywords Reinforcement LM: Improving End-to-End Response Generation in Task Oriented Dialog. (arXiv:2211.16773v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.16773","description":"<p>In task-oriented dialogs such as MultiWoZ (Budzianowski et al., 2018), an\ninformative and successful system response needs to include key information\nsuch as the phone number of a hotel. Therefore, we hypothesize that by asking\nthe model to focus on generating more key quantities correctly, it can achieve\nbetter overall performance. In this paper, we propose a new training algorithm,\nKeywords Reinforcement Language Modeling (KRLM), that aims to use a\nfine-grained reward function for each token and a new per-token Reinforcement\nLearning procedure to help the model learn keywords generation more robustly\nduring inference. Empirical results show that our proposed KRLM training\nalgorithm can achieve state-of-the-art performance on the inform rate, success\nrate, and combined score in the MultiWoZ benchmark dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning for On-Device Speech Recognition using Disentangled Conformers. (arXiv:2212.01393v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2212.01393","description":"<p>Automatic speech recognition research focuses on training and evaluating on\nstatic datasets. Yet, as speech models are increasingly deployed on personal\ndevices, such models encounter user-specific distributional shifts. To simulate\nthis real-world scenario, we introduce LibriContinual, a continual learning\nbenchmark for speaker-specific domain adaptation derived from LibriVox\naudiobooks, with data corresponding to 118 individual speakers and 6 train\nsplits per speaker of different sizes. Additionally, current speech recognition\nmodels and continual learning algorithms are not optimized to be\ncompute-efficient. We adapt a general-purpose training algorithm NetAug for ASR\nand create a novel Conformer variant called the DisConformer (Disentangled\nConformer). This algorithm produces ASR models consisting of a frozen 'core'\nnetwork for general-purpose use and several tunable 'augment' networks for\nspeaker-specific tuning. Using such models, we propose a novel\ncompute-efficient continual learning algorithm called DisentangledCL. Our\nexperiments show that the DisConformer models significantly outperform\nbaselines on general ASR i.e. LibriSpeech (15.58% rel. WER on test-other). On\nspeaker-specific LibriContinual they significantly outperform\ntrainable-parameter-matched baselines (by 20.65% rel. WER on test) and even\nmatch fully finetuned baselines in some settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Diwan_A/0/1/0/all/0/1\">Anuj Diwan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeh_C/0/1/0/all/0/1\">Ching-Feng Yeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tomasello_P/0/1/0/all/0/1\">Paden Tomasello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning. (arXiv:2212.03760v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2212.03760","description":"<p>Recent studies have proposed unified user modeling frameworks that leverage\nuser behavior data from various applications. Many of them benefit from\nutilizing users' behavior sequences as plain texts, representing rich\ninformation in any domain or system without losing generality. Hence, a\nquestion arises: Can language modeling for user history corpus help improve\nrecommender systems? While its versatile usability has been widely investigated\nin many domains, its applications to recommender systems still remain\nunderexplored. We show that language modeling applied directly to task-specific\nuser histories achieves excellent results on diverse recommendation tasks.\nAlso, leveraging additional task-agnostic user histories delivers significant\nperformance benefits. We further demonstrate that our approach can provide\npromising transfer learning capabilities for a broad spectrum of real-world\nrecommender systems, even on unseen domains and services.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1\">Kyuyong Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Hanock Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Wonjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jisu Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seungjae Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Min Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndicXTREME: A Multi-Task Benchmark For Evaluating Indic Languages. (arXiv:2212.05409v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05409","description":"<p>In this work, we introduce IndicXTREME, a benchmark consisting of nine\ndiverse tasks covering 18 languages from the Indic sub-continent belonging to\nfour different families. Across languages and tasks, IndicXTREME contains a\ntotal of 103 evaluation sets, of which 51 are new contributions to the\nliterature. To maintain high quality, we only use human annotators to curate or\ntranslate our datasets. To the best of our knowledge, this is the first effort\ntoward creating a standard benchmark for Indic languages that aims to test the\nzero-shot capabilities of pretrained language models. We also release IndicCorp\nv2, an updated and much larger version of IndicCorp that contains 20.9 billion\ntokens in 24 languages. We pretrain IndicBERT v2 on IndicCorp v2 and evaluate\nit on IndicXTREME to show that it outperforms existing multilingual language\nmodels such as XLM-R and MuRIL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aralikatte_R/0/1/0/all/0/1\">Rahul Aralikatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_G/0/1/0/all/0/1\">Gowtham Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Shreya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned Receipt Images. (arXiv:2212.05525v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05525","description":"<p>Digitization of scanned receipts aims to extract text from receipt images and\nsave it into structured documents. This is usually split into two sub-tasks:\ntext localization and optical character recognition (OCR). Most existing OCR\nmodels only focus on the cropped text instance images, which require the\nbounding box information provided by a text region detection model. Introducing\nan additional detector to identify the text instance images in advance is\ninefficient, however instance-level OCR models have very low accuracy when\nprocessing the whole image for the document-level OCR, such as receipt images\ncontaining multiple text lines arranged in various layouts. To this end, we\npropose a localization-free document-level OCR model for transcribing all the\ncharacters in a receipt image into an ordered sequence end-to-end.\nSpecifically, we finetune the pretrained Transformer-based instance-level model\nTrOCR with randomly cropped image chunks, and gradually increase the image\nchunk size to generalize the recognition ability from instance images to\nfull-page images. In our experiments on the SROIE receipt OCR dataset, the\nmodel finetuned with our strategy achieved 64.4 F1-score and a 22.8% character\nerror rates (CER) on the word-level and character-level metrics, respectively,\nwhich outperforms the baseline results with 48.5 F1-score and 50.6% CER. The\nbest model, which splits the full image into 15 equally sized chunks, gives\n87.8 F1-score and 4.98% CER with minimal additional pre or post-processing of\nthe output. Moreover, the characters in the generated document-level sequences\nare arranged in the reading order, which is practical for real-world\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongkuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whittaker_E/0/1/0/all/0/1\">Edward Whittaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitagishi_I/0/1/0/all/0/1\">Ikuo Kitagishi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning over Different Types of Knowledge Graphs: Static, Temporal and Multi-Modal. (arXiv:2212.05767v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.05767","description":"<p>Knowledge graph reasoning (KGR), aiming to deduce new facts from existing\nfacts based on mined logic rules underlying knowledge graphs (KGs), has become\na fast-growing research direction. It has been proven to significantly benefit\nthe usage of KGs in many AI applications, such as question answering and\nrecommendation systems, etc. According to the graph types, the existing KGR\nmodels can be roughly divided into three categories, i.e., static models,\ntemporal models, and multi-modal models. The early works in this domain mainly\nfocus on static KGR and tend to directly apply general knowledge graph\nembedding models to the reasoning task. However, these models are not suitable\nfor more complex but practical tasks, such as inductive static KGR, temporal\nKGR, and multi-modal KGR. To this end, multiple works have been developed\nrecently, but no survey papers and open-source repositories comprehensively\nsummarize and discuss models in this important direction. To fill the gap, we\nconduct a survey for knowledge graph reasoning tracing from static to temporal\nand then to multi-modal KGs. Concretely, the preliminaries, summaries of KGR\nmodels, and typical datasets are introduced and discussed consequently.\nMoreover, we discuss the challenges and potential opportunities. The\ncorresponding open-source repository is shared on GitHub:\nhttps://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Ke Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingyuan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wenxuan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sihang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated ICD Coding using Extreme Multi-label Long Text Transformer-based Models. (arXiv:2212.05857v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05857","description":"<p>Background: Encouraged by the success of pretrained Transformer models in\nmany natural language processing tasks, their use for International\nClassification of Diseases (ICD) coding tasks is now actively being explored.\nIn this study, we investigate three types of Transformer-based models, aiming\nto address the extreme label set and long text classification challenges that\nare posed by automated ICD coding tasks. Methods: The Transformer-based model\nPLM-ICD achieved the current state-of-the-art (SOTA) performance on the ICD\ncoding benchmark dataset MIMIC-III. It was chosen as our baseline model to be\nfurther optimised. XR-Transformer, the new SOTA model in the general extreme\nmulti-label text classification domain, and XR-LAT, a novel adaptation of the\nXR-Transformer model, were also trained on the MIMIC-III dataset. XR-LAT is a\nrecursively trained model chain on a predefined hierarchical code tree with\nlabel-wise attention, knowledge transferring and dynamic negative sampling\nmechanisms. Results: Our optimised PLM-ICD model, which was trained with longer\ntotal and chunk sequence lengths, significantly outperformed the current SOTA\nPLM-ICD model, and achieved the highest micro-F1 score of 60.8%. The\nXR-Transformer model, although SOTA in the general domain, did not perform well\nacross all metrics. The best XR-LAT based model obtained results that were\ncompetitive with the current SOTA PLM-ICD model, including improving the\nmacro-AUC by 2.1%. Conclusion: Our optimised PLM-ICD model is the new SOTA\nmodel for automated ICD coding on the MIMIC-III dataset, while our novel XR-LAT\nmodel performs competitively with the previous SOTA PLM-ICD model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Leibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Concha_O/0/1/0/all/0/1\">Oscar Perez-Concha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anthony Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_V/0/1/0/all/0/1\">Vicki Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorm_L/0/1/0/all/0/1\">Louisa Jorm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technological taxonomies for hypernym and hyponym retrieval in patent texts. (arXiv:2212.06039v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2212.06039","description":"<p>This paper presents an automatic approach to creating taxonomies of technical\nterms based on the Cooperative Patent Classification (CPC). The resulting\ntaxonomy contains about 170k nodes in 9 separate technological branches and is\nfreely available. We also show that a Text-to-Text Transfer Transformer (T5)\nmodel can be fine-tuned to generate hypernyms and hyponyms with relatively high\nprecision, confirming the manually assessed quality of the resource. The T5\nmodel opens the taxonomy to any new technological terms for which a hypernym\ncan be generated, thus making the resource updateable with new terms, an\nessential feature for the constantly evolving field of technological\nterminology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Y/0/1/0/all/0/1\">You Zuo</a> (ALMAnaCH), <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1\">Alma Parias Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerdes_K/0/1/0/all/0/1\">Kim Gerdes</a> (LISN)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-12-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}