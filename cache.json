{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-02T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Reward Design with Language Models. (arXiv:2303.00001v1 [cs.LG])","link":"http://arxiv.org/abs/2303.00001","description":"<p>Reward design in reinforcement learning (RL) is challenging since specifying\nhuman notions of desired behavior may be difficult via reward functions or\nrequire many expert demonstrations. Can we instead cheaply design rewards using\na natural language interface? This paper explores how to simplify reward design\nby prompting a large language model (LLM) such as GPT-3 as a proxy reward\nfunction, where the user provides a textual prompt containing a few examples\n(few-shot) or a description (zero-shot) of the desired behavior. Our approach\nleverages this proxy reward function in an RL framework. Specifically, users\nspecify a prompt once at the beginning of training. During training, the LLM\nevaluates an RL agent's behavior against the desired behavior described by the\nprompt and outputs a corresponding reward signal. The RL agent then uses this\nreward to update its behavior. We evaluate whether our approach can train\nagents aligned with user objectives in the Ultimatum Game, matrix games, and\nthe DealOrNoDeal negotiation task. In all three tasks, we show that RL agents\ntrained with our framework are well-aligned with the user's objectives and\noutperform RL agents trained with reward functions learned via supervised\nlearning\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1\">Minae Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bullard_K/0/1/0/all/0/1\">Kalesha Bullard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1\">Dorsa Sadigh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClArTTS: An Open-Source Classical Arabic Text-to-Speech Corpus. (arXiv:2303.00069v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00069","description":"<p>At present, Text-to-speech (TTS) systems that are trained with high-quality\ntranscribed speech data using end-to-end neural models can generate speech that\nis intelligible, natural, and closely resembles human speech. These models are\ntrained with relatively large single-speaker professionally recorded audio,\ntypically extracted from audiobooks. Meanwhile, due to the scarcity of freely\navailable speech corpora of this kind, a larger gap exists in Arabic TTS\nresearch and development. Most of the existing freely available Arabic speech\ncorpora are not suitable for TTS training as they contain multi-speaker casual\nspeech with variations in recording conditions and quality, whereas the corpus\ncurated for speech synthesis are generally small in size and not suitable for\ntraining state-of-the-art end-to-end models. In a move towards filling this gap\nin resources, we present a speech corpus for Classical Arabic Text-to-Speech\n(ClArTTS) to support the development of end-to-end TTS systems for Arabic. The\nspeech is extracted from a LibriVox audiobook, which is then processed,\nsegmented, and manually transcribed and annotated. The final ClArTTS corpus\ncontains about 12 hours of speech from a single male speaker sampled at 40100\nkHz. In this paper, we describe the process of corpus creation and provide\ndetails of corpus statistics and a comparison with existing resources.\nFurthermore, we develop two TTS systems based on Grad-TTS and Glow-TTS and\nillustrate the performance of the resulting systems via subjective and\nobjective evaluations. The corpus will be made publicly available at\nwww.clartts.com for research purposes, along with the baseline TTS systems\ndemo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Ajinkya Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Atharva Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shatnawi_S/0/1/0/all/0/1\">Sara Abedalmonem Mohammad Shatnawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldarmaki_H/0/1/0/all/0/1\">Hanan Aldarmaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond the limitations of any imaginable mechanism: large language models and psycholinguistics. (arXiv:2303.00077v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00077","description":"<p>Large language models are not detailed models of human linguistic processing.\nThey are, however, extremely successful at their primary task: providing a\nmodel for language. For this reason and because there are no animal models for\nlanguage, large language models are important in psycholinguistics: they are\nuseful as a practical tool, as an illustrative comparative, and\nphilosophically, as a basis for recasting the relationship between language and\nthought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Houghton_C/0/1/0/all/0/1\">Conor Houghton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazanina_N/0/1/0/all/0/1\">Nina Kazanina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukumaran_P/0/1/0/all/0/1\">Priyanka Sukumaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Medical Speech-to-Text Accuracy with Vision-Language Pre-training Model. (arXiv:2303.00091v1 [eess.AS])","link":"http://arxiv.org/abs/2303.00091","description":"<p>Automatic Speech Recognition (ASR) is a technology that converts spoken words\ninto text, facilitating interaction between humans and machines. One of the\nmost common applications of ASR is Speech-To-Text (STT) technology, which\nsimplifies user workflows by transcribing spoken words into text. In the\nmedical field, STT has the potential to significantly reduce the workload of\nclinicians who rely on typists to transcribe their voice recordings. However,\ndeveloping an STT model for the medical domain is challenging due to the lack\nof sufficient speech and text datasets. To address this issue, we propose a\nmedical-domain text correction method that modifies the output text of a\ngeneral STT system using the Vision Language Pre-training (VLP) method. VLP\ncombines textual and visual information to correct text based on image\nknowledge. Our extensive experiments demonstrate that the proposed method\noffers quantitatively and clinically significant improvements in STT\nperformance in the medical field. We further show that multi-modal\nunderstanding of image and text information outperforms single-modal\nunderstanding using only text information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huh_J/0/1/0/all/0/1\">Jaeyoung Huh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Sangjoon Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Jeong Eun Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning for COVID-19 topic modelling via Twitter: Alpha, Delta and Omicron. (arXiv:2303.00135v1 [cs.LG])","link":"http://arxiv.org/abs/2303.00135","description":"<p>Topic modelling with innovative deep learning methods has gained interest for\na wide range of applications that includes COVID-19. Topic modelling can\nprovide, psychological, social and cultural insights for understanding human\nbehaviour in extreme events such as the COVID-19 pandemic. In this paper, we\nuse prominent deep learning-based language models for COVID-19 topic modelling\ntaking into account data from emergence (Alpha) to the Omicron variant. We\napply topic modeling to review the public behaviour across the first, second\nand third waves based on Twitter dataset from India. Our results show that the\ntopics extracted for the subsequent waves had certain overlapping themes such\nas covers governance, vaccination, and pandemic management while novel issues\naroused in political, social and economic situation during COVID-19 pandemic.\nWe also found a strong correlation of the major topics qualitatively to news\nmedia prevalent at the respective time period. Hence, our framework has the\npotential to capture major issues arising during different phases of the\nCOVID-19 pandemic which can be extended to other countries and regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lande_J/0/1/0/all/0/1\">Janhavi Lande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pillay_A/0/1/0/all/0/1\">Arti Pillay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohitash Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIFFQG: Generating Questions to Summarize Factual Changes. (arXiv:2303.00242v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00242","description":"<p>Identifying the difference between two versions of the same article is useful\nto update knowledge bases and to understand how articles evolve. Paired texts\noccur naturally in diverse situations: reporters write similar news stories and\nmaintainers of authoritative websites must keep their information up to date.\nWe propose representing factual changes between paired documents as\nquestion-answer pairs, where the answer to the same question differs between\ntwo versions. We find that question-answer pairs can flexibly and concisely\ncapture the updated contents. Provided with paired documents, annotators\nidentify questions that are answered by one passage but answered differently or\ncannot be answered by the other. We release DIFFQG which consists of 759 QA\npairs and 1153 examples of paired passages with no factual change. These\nquestions are intended to be both unambiguous and information-seeking and\ninvolve complex edits, pushing beyond the capabilities of current question\ngeneration and factual change detection systems. Our dataset summarizes the\nchanges between two versions of the document as questions and answers, studying\nautomatic update summarization in a novel way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1\">Jeremy R. Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Palak Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1\">Julian Martin Eisenschlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Michael J.Q. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hidden Markov Transformer for Simultaneous Machine Translation. (arXiv:2303.00257v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00257","description":"<p>Simultaneous machine translation (SiMT) outputs the target sequence while\nreceiving the source sequence, and hence learning when to start translating\neach target token is the core challenge for SiMT task. However, it is\nnon-trivial to learn the optimal moment among many possible moments of starting\ntranslating, as the moments of starting translating always hide inside the\nmodel and can only be supervised with the observed target sequence. In this\npaper, we propose a Hidden Markov Transformer (HMT), which treats the moments\nof starting translating as hidden events and the target sequence as the\ncorresponding observed events, thereby organizing them as a hidden Markov\nmodel. HMT explicitly models multiple moments of starting translating as the\ncandidate hidden events, and then selects one to generate the target token.\nDuring training, by maximizing the marginal likelihood of the target sequence\nover multiple moments of starting translating, HMT learns to start translating\nat the moments that target tokens can be generated more accurately. Experiments\non multiple SiMT benchmarks show that HMT outperforms strong baselines and\nachieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Covid-19 Segmentation via Vision-Language Alignment. (arXiv:2303.00279v1 [eess.IV])","link":"http://arxiv.org/abs/2303.00279","description":"<p>Segmentation of COVID-19 lesions can assist physicians in better diagnosis\nand treatment of COVID-19. However, there are few relevant studies due to the\nlack of detailed information and high-quality annotation in the COVID-19\ndataset. To solve the above problem, we propose C2FVL, a Coarse-to-Fine\nsegmentation framework via Vision-Language alignment to merge text information\ncontaining the number of lesions and specific locations of image information.\nThe introduction of text information allows the network to achieve better\nprediction results on challenging datasets. We conduct extensive experiments on\ntwo COVID-19 datasets including chest X-ray and CT, and the results demonstrate\nthat our proposed method outperforms other state-of-the-art segmentation\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shan_D/0/1/0/all/0/1\">Dandan Shan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zihan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1\">Wentao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qingde Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_J/0/1/0/all/0/1\">Jie Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hong_Q/0/1/0/all/0/1\">Qingqi Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks. (arXiv:2303.00293v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00293","description":"<p>The GPT-3.5 models have demonstrated impressive performance in various\nNatural Language Processing (NLP) tasks, showcasing their strong understanding\nand reasoning capabilities. However, their robustness and abilities to handle\nvarious complexities of the open world have yet to be explored, which is\nespecially crucial in assessing the stability of models and is a key aspect of\ntrustworthy AI. In this study, we perform a comprehensive experimental analysis\nof GPT-3.5, exploring its robustness using 21 datasets (about 116K test\nsamples) with 66 text transformations from TextFlint that cover 9 popular\nNatural Language Understanding (NLU) tasks. Our findings indicate that while\nGPT-3.5 outperforms existing fine-tuned models on some tasks, it still\nencounters significant robustness degradation, such as its average performance\ndropping by up to 35.74\\% and 43.59\\% in natural language inference and\nsentiment analysis tasks, respectively. We also show that GPT-3.5 faces some\nspecific robustness challenges, including robustness instability, prompt\nsensitivity, and number sensitivity. These insights are valuable for\nunderstanding its limitations and guiding future research in addressing these\nchallenges to enhance GPT-3.5's overall performance and generalization\nabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zu_C/0/1/0/all/0/1\">Can Zu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Nuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Minlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Multiple User Interests using Hierarchical Knowledge for Conversational Recommender System. (arXiv:2303.00311v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00311","description":"<p>A conversational recommender system (CRS) is a practical application for item\nrecommendation through natural language conversation. Such a system estimates\nuser interests for appropriate personalized recommendations. Users sometimes\nhave various interests in different categories or genres, but existing studies\nassume a unique user interest that can be covered by closely related items. In\nthis work, we propose to model such multiple user interests in CRS. We\ninvestigated its effects in experiments using the ReDial dataset and found that\nthe proposed method can recommend a wider variety of items than that of the\nbaseline CR-Walker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Okuda_Y/0/1/0/all/0/1\">Yuka Okuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1\">Katsuhito Sudoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinagawa_S/0/1/0/all/0/1\">Seitaro Shinagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Competence-Based Analysis of Language Models. (arXiv:2303.00333v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00333","description":"<p>Despite the recent success of large pretrained language models (LMs) on a\nvariety of prompting tasks, these models can be alarmingly brittle to small\nchanges in inputs or application contexts. To better understand such behavior\nand motivate the design of more robust LMs, we propose a general experimental\nframework, CALM (Competence-based Analysis of Language Models), where targeted\ncausal interventions are utilized to damage an LM's internal representation of\nvarious linguistic properties in order to evaluate its use of each\nrepresentation in performing a given task. We implement these interventions as\ngradient-based adversarial attacks, which (in contrast to prior causal probing\nmethodologies) are able to target arbitrarily-encoded representations of\nrelational properties, and carry out a case study of this approach to analyze\nhow BERT-like LMs use representations of several relational properties in\nperforming associated relation prompting tasks. We find that, while the\nrepresentations LMs leverage in performing each task are highly entangled, they\nmay be meaningfully interpreted in terms of the tasks where they are most\nutilized; and more broadly, that CALM enables an expanded scope of inquiry in\nLM analysis that may be useful in predicting and explaining weaknesses of\nexisting LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davies_A/0/1/0/all/0/1\">Adam Davies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jize Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inline Citation Classification using Peripheral Context and Time-evolving Augmentation. (arXiv:2303.00344v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00344","description":"<p>Citation plays a pivotal role in determining the associations among research\narticles. It portrays essential information in indicative, supportive, or\ncontrastive studies. The task of inline citation classification aids in\nextrapolating these relationships; However, existing studies are still immature\nand demand further scrutiny. Current datasets and methods used for inline\ncitation classification only use citation-marked sentences constraining the\nmodel to turn a blind eye to domain knowledge and neighboring contextual\nsentences. In this paper, we propose a new dataset, named 3Cext, which along\nwith the cited sentences, provides discourse information using the vicinal\nsentences to analyze the contrasting and entailing relationships as well as\ndomain information. We propose PeriCite, a Transformer-based deep neural\nnetwork that fuses peripheral sentences and domain knowledge. Our model\nachieves the state-of-the-art on the 3Cext dataset by +0.09 F1 against the best\nbaseline. We conduct extensive ablations to analyze the efficacy of the\nproposed dataset and model fusion methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Priyanshi Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1\">Yash Kumar Atri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagvenkar_A/0/1/0/all/0/1\">Apurva Nagvenkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Sourish Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Persian Benchmark for Joint Intent Detection and Slot Filling. (arXiv:2303.00408v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00408","description":"<p>Natural Language Understanding (NLU) is important in today's technology as it\nenables machines to comprehend and process human language, leading to improved\nhuman-computer interactions and advancements in fields such as virtual\nassistants, chatbots, and language-based AI systems. This paper highlights the\nsignificance of advancing the field of NLU for low-resource languages. With\nintent detection and slot filling being crucial tasks in NLU, the widely used\ndatasets ATIS and SNIPS have been utilized in the past. However, these datasets\nonly cater to the English language and do not support other languages. In this\nwork, we aim to address this gap by creating a Persian benchmark for joint\nintent detection and slot filling based on the ATIS dataset. To evaluate the\neffectiveness of our benchmark, we employ state-of-the-art methods for intent\ndetection and slot filling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akbari_M/0/1/0/all/0/1\">Masoud Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimi_A/0/1/0/all/0/1\">Amir Hossein Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeedi_T/0/1/0/all/0/1\">Tayyebeh Saeedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeidi_Z/0/1/0/all/0/1\">Zeinab Saeidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghezelbash_K/0/1/0/all/0/1\">Kiana Ghezelbash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsezat_F/0/1/0/all/0/1\">Fatemeh Shamsezat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbari_M/0/1/0/all/0/1\">Mohammad Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohades_A/0/1/0/all/0/1\">Ali Mohades</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space. (arXiv:2303.00456v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00456","description":"<p>Error correction models form an important part of Automatic Speech\nRecognition (ASR) post-processing to improve the readability and quality of\ntranscriptions. Most prior works use the 1-best ASR hypothesis as input and\ntherefore can only perform correction by leveraging the context within one\nsentence. In this work, we propose a novel N-best T5 model for this task, which\nis fine-tuned from a T5 model and utilizes ASR N-best lists as model input. By\ntransferring knowledge from the pre-trained language model and obtaining richer\ninformation from the ASR decoding space, the proposed approach outperforms a\nstrong Conformer-Transducer baseline. Another issue with standard error\ncorrection is that the generation process is not well-guided. To address this a\nconstrained decoding process, either based on the N-best list or an ASR\nlattice, is used which allows additional information to be propagated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J F Gales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knill_K/0/1/0/all/0/1\">Kate Knill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1\">Mengjie Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uzbek text summarization based on TF-IDF. (arXiv:2303.00461v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00461","description":"<p>The volume of information is increasing at an incredible rate with the rapid\ndevelopment of the Internet and electronic information services. Due to time\nconstraints, we don't have the opportunity to read all this information. Even\nthe task of analyzing textual data related to one field requires a lot of work.\nThe text summarization task helps to solve these problems. This article\npresents an experiment on summarization task for Uzbek language, the\nmethodology was based on text abstracting based on TF-IDF algorithm. Using this\ndensity function, semantically important parts of the text are extracted. We\nsummarize the given text by applying the n-gram method to important parts of\nthe whole text. The authors used a specially handcrafted corpus called \"School\ncorpus\" to evaluate the performance of the proposed method. The results show\nthat the proposed approach is effective in extracting summaries from Uzbek\nlanguage text and can potentially be used in various applications such as\ninformation retrieval and natural language processing. Overall, this research\ncontributes to the growing body of work on text summarization in\nunder-resourced languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madatov_K/0/1/0/all/0/1\">Khabibulla Madatov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekchanov_S/0/1/0/all/0/1\">Shukurla Bekchanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vicic_J/0/1/0/all/0/1\">Jernej Vi&#x10d;i&#x10d;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uzbek text's correspondence with the educational potential of pupils: a case study of the School corpus. (arXiv:2303.00465v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00465","description":"<p>One of the major challenges of an educational system is choosing appropriate\ncontent considering pupils' age and intellectual potential. In this article the\nexperiment of primary school grades (from 1st to 4th grades) is considered for\nautomatically determining the correspondence of an educational materials\nrecommended for pupils by using the School corpus where it includes the dataset\nof 25 school textbooks confirmed by the Ministry of preschool and school\neducation of the Republic of Uzbekistan. In this case, TF-IDF scores of the\ntexts are determined, they are converted into a vector representation, and the\ngiven educational materials are compared with the corresponding class of the\nSchool corpus using the cosine similarity algorithm. Based on the results of\nthe calculation, it is determined whether the given educational material is\nappropriate or not appropriate for the pupils' educational potential.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madatov_K/0/1/0/all/0/1\">Khabibulla Madatov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matlatipov_S/0/1/0/all/0/1\">Sanatbek Matlatipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aripov_M/0/1/0/all/0/1\">Mersaid Aripov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training. (arXiv:2303.00534v1 [cs.CV])","link":"http://arxiv.org/abs/2303.00534","description":"<p>Vision-and-language multi-modal pretraining and fine-tuning have shown great\nsuccess in visual question answering (VQA). Compared to general domain VQA, the\nperformance of biomedical VQA suffers from limited data. In this paper, we\npropose a retrieval-augmented pretrain-and-finetune paradigm named RAMM for\nbiomedical VQA to overcome the data limitation issue. Specifically, we collect\na new biomedical dataset named PMCPM which offers patient-based image-text\npairs containing diverse patient situations from PubMed. Then, we pretrain the\nbiomedical multi-modal model to learn visual and textual representation for\nimage-text pairs and align these representations with image-text contrastive\nobjective (ITC). Finally, we propose a retrieval-augmented method to better use\nthe limited data. We propose to retrieve similar image-text pairs based on ITC\nfrom pretraining datasets and introduce a novel retrieval-attention module to\nfuse the representation of the image and the question with the retrieved images\nand texts. Experiments demonstrate that our retrieval-augmented\npretrain-and-finetune paradigm obtains state-of-the-art performance on\nMed-VQA2019, Med-VQA2021, VQARAD, and SLAKE datasets. Further analysis shows\nthat the proposed RAMM and PMCPM can enhance biomedical VQA performance\ncompared with previous resources and methods. We will open-source our dataset,\ncodes, and pretrained model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhengyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Universal Question-Answering Platform for Knowledge Graphs. (arXiv:2303.00595v1 [cs.AI])","link":"http://arxiv.org/abs/2303.00595","description":"<p>Knowledge from diverse application domains is organized as knowledge graphs\n(KGs) that are stored in RDF engines accessible in the web via SPARQL\nendpoints. Expressing a well-formed SPARQL query requires information about the\ngraph structure and the exact URIs of its components, which is impractical for\nthe average user. Question answering (QA) systems assist by translating natural\nlanguage questions to SPARQL. Existing QA systems are typically based on\napplication-specific human-curated rules, or require prior information,\nexpensive pre-processing and model adaptation for each targeted KG. Therefore,\nthey are hard to generalize to a broad set of applications and KGs.\n</p>\n<p>In this paper, we propose KGQAn, a universal QA system that does not need to\nbe tailored to each target KG. Instead of curated rules, KGQAn introduces a\nnovel formalization of question understanding as a text generation problem to\nconvert a question into an intermediate abstract representation via a neural\nsequence-to-sequence model. We also develop a just-in-time linker that maps at\nquery time the abstract representation to a SPARQL query for a specific KG,\nusing only the publicly accessible APIs and the existing indices of the RDF\nstore, without requiring any pre-processing. Our experiments with several real\nKGs demonstrate that KGQAn is easily deployed and outperforms by a large margin\nthe state-of-the-art in terms of quality of answers and processing time,\nespecially for arbitrary KGs, unseen during the training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Omar_R/0/1/0/all/0/1\">Reham Omar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhall_I/0/1/0/all/0/1\">Ishika Dhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalnis_P/0/1/0/all/0/1\">Panos Kalnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_E/0/1/0/all/0/1\">Essam Mansour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation. (arXiv:2303.00628v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00628","description":"<p>We introduce MuAViC, a multilingual audio-visual corpus for robust speech\nrecognition and robust speech-to-text translation providing 1200 hours of\naudio-visual speech in 9 languages. It is fully transcribed and covers 6\nEnglish-to-X translation as well as 6 X-to-English translation directions. To\nthe best of our knowledge, this is the first open benchmark for audio-visual\nspeech-to-text translation and the largest open benchmark for multilingual\naudio-visual speech recognition. Our baseline results show that MuAViC is\neffective for building noise-robust speech recognition and translation models.\nWe make the corpus available at https://github.com/facebookresearch/muavic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anwar_M/0/1/0/all/0/1\">Mohamed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1\">Vedanuj Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Analysis of Vocabulary and BPE Settings for Optimal Fine-tuning of NMT: A Case Study of In-domain Translation. (arXiv:2303.00722v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00722","description":"<p>The effectiveness of Neural Machine Translation (NMT) models largely depends\non the vocabulary used at training; small vocabularies can lead to\nout-of-vocabulary problems -- large ones, to memory issues. Subword (SW)\ntokenization has been successfully employed to mitigate these issues. The\nchoice of vocabulary and SW tokenization has a significant impact on both\ntraining and fine-tuning an NMT model. Fine-tuning is a common practice in\noptimizing an MT model with respect to new data. However, new data potentially\nintroduces new words (or tokens), which, if not taken into consideration, may\nlead to suboptimal performance. In addition, the distribution of tokens in the\nnew data can differ from the distribution of the original data. As such, the\noriginal SW tokenization model could be less suitable for the new data. Through\na systematic empirical evaluation, in this work we compare different strategies\nfor SW tokenization and vocabulary generation with the ultimate goal to uncover\nan optimal setting for fine-tuning a domain-specific model. Furthermore, we\ndeveloped several (in-domain) models, the best of which achieves 6 BLEU points\nimprovement over the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharami_J/0/1/0/all/0/1\">J. Pourmostafa Roshan Sharami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shterionov_D/0/1/0/all/0/1\">D. Shterionov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spronck_P/0/1/0/all/0/1\">P. Spronck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks. (arXiv:2303.00733v1 [eess.AS])","link":"http://arxiv.org/abs/2303.00733","description":"<p>Prompt tuning is a technology that tunes a small set of parameters to steer a\npre-trained language model (LM) to directly generate the output for downstream\ntasks. Recently, prompt tuning has demonstrated its storage and computation\nefficiency in both natural language processing (NLP) and speech processing\nfields. These advantages have also revealed prompt tuning as a candidate\napproach to serving pre-trained LM for multiple tasks in a unified manner. For\nspeech processing, SpeechPrompt shows its high parameter efficiency and\ncompetitive performance on a few speech classification tasks. However, whether\nSpeechPrompt is capable of serving a large number of tasks is unanswered. In\nthis work, we propose SpeechPrompt v2, a prompt tuning framework capable of\nperforming a wide variety of speech classification tasks, covering multiple\nlanguages and prosody-related tasks. The experiment result shows that\nSpeechPrompt v2 achieves performance on par with prior works with less than\n0.15M trainable parameters in a unified framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Kai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1\">Hua Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_I/0/1/0/all/0/1\">Iu-thing Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tseng_W/0/1/0/all/0/1\">Wei-Cheng Tseng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Transformers know symbolic rules, and would we know if they did?. (arXiv:2203.00162v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.00162","description":"<p>To improve the explainability of leading Transformer networks used in NLP, it\nis important to tease apart genuine symbolic rules from merely associative\ninput-output patterns. However, we identify several inconsistencies in how\n``symbolicity'' has been construed in recent NLP literature. To mitigate this\nproblem, we propose two criteria to be the most relevant, one pertaining to a\nsystem's internal architecture and the other to the dissociation between\nabstract rules and specific input identities. From this perspective, we\ncritically examine prior work on the symbolic capacities of Transformers, and\ndeem the results to be fundamentally inconclusive for reasons inherent in\nexperiment design. We further maintain that there is no simple fix to this\nproblem, since it arises -- to an extent -- in all end-to-end settings.\nNonetheless, we emphasize the need for more robust evaluation of whether\nnon-symbolic explanations exist for success in seemingly symbolic tasks. To\nfacilitate this, we experiment on four sequence modelling tasks on the T5\nTransformer in two experiment settings: zero-shot generalization, and\ngeneralization across class-specific vocabularies flipped between the training\nand test set. We observe that T5's generalization is markedly stronger in\nsequence-to-sequence tasks than in comparable classification tasks. Based on\nthis, we propose a thus far overlooked analysis, where the Transformer itself\ndoes not need to be symbolic to be part of a symbolic architecture as the\nprocessor, operating on the input and output as external memory components.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grondahl_T/0/1/0/all/0/1\">Tommi Gr&#xf6;ndahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yujia Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asokan_N/0/1/0/all/0/1\">N. Asokan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ontology-Driven and Weakly Supervised Rare Disease Identification from Clinical Notes. (arXiv:2205.05656v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05656","description":"<p>Computational text phenotyping is the practice of identifying patients with\ncertain disorders and traits from clinical notes. Rare diseases are challenging\nto be identified due to few cases available for machine learning and the need\nfor data annotation from domain experts. We propose a method using ontologies\nand weak supervision, with recent pre-trained contextual representations from\nBi-directional Transformers (e.g. BERT). The ontology-based framework includes\ntwo steps: (i) Text-to-UMLS, extracting phenotypes by contextually linking\nmentions to concepts in Unified Medical Language System (UMLS), with a Named\nEntity Recognition and Linking (NER+L) tool, SemEHR, and weak supervision with\ncustomised rules and contextual mention representation; (ii) UMLS-to-ORDO,\nmatching UMLS concepts to rare diseases in Orphanet Rare Disease Ontology\n(ORDO). The weakly supervised approach is proposed to learn a phenotype\nconfirmation model to improve Text-to-UMLS linking, without annotated data from\ndomain experts. We evaluated the approach on three clinical datasets, MIMIC-III\ndischarge summaries, MIMIC-III radiology reports, and NHS Tayside brain imaging\nreports from two institutions in the US and the UK, with annotations. The\nimprovements in the precision were pronounced (by over 30% to 50% absolute\nscore for Text-to-UMLS linking), with almost no loss of recall compared to the\nexisting NER+L tool, SemEHR. Results on radiology reports from MIMIC-III and\nNHS Tayside were consistent with the discharge summaries. The overall pipeline\nprocessing clinical notes can extract rare disease cases, mostly uncaptured in\nstructured data (manually assigned ICD codes). We discuss the usefulness of the\nweak supervision approach and propose directions for future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1\">V&#xed;ctor Su&#xe1;rez-Paniagua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huayu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casey_A/0/1/0/all/0/1\">Arlene Casey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_E/0/1/0/all/0/1\">Emma Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alex_B/0/1/0/all/0/1\">Beatrice Alex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteley_W/0/1/0/all/0/1\">William Whiteley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Similarity is More Valuable than Character Similarity: An Empirical Study for Chinese Spell Checking. (arXiv:2207.09217v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.09217","description":"<p>Chinese Spell Checking (CSC) task aims to detect and correct Chinese spelling\nerrors. Recently, related researches focus on introducing character similarity\nfrom confusion set to enhance the CSC models, ignoring the context of\ncharacters that contain richer information. To make better use of contextual\ninformation, we propose a simple yet effective Curriculum Learning (CL)\nframework for the CSC task. With the help of our model-agnostic CL framework,\nexisting CSC models will be trained from easy to difficult as humans learn\nChinese characters and achieve further performance improvements. Extensive\nexperiments and detailed analyses on widely used SIGHAN datasets show that our\nmethod outperforms previous state-of-the-art methods. More instructively, our\nstudy empirically suggests that contextual similarity is more valuable than\ncharacter similarity for the CSC task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Ding Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shirong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning. (arXiv:2208.04202v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2208.04202","description":"<p>We present Bit Diffusion: a simple and generic approach for generating\ndiscrete data with continuous state and continuous time diffusion models. The\nmain idea behind our approach is to first represent the discrete data as binary\nbits, and then train a continuous diffusion model to model these bits as real\nnumbers which we call analog bits. To generate samples, the model first\ngenerates the analog bits, which are then thresholded to obtain the bits that\nrepresent the discrete variables. We further propose two simple techniques,\nnamely Self-Conditioning and Asymmetric Time Intervals, which lead to a\nsignificant improvement in sample quality. Despite its simplicity, the proposed\napproach can achieve strong performance in both discrete image generation and\nimage captioning tasks. For discrete image generation, we significantly improve\nprevious state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens)\nand ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the\nbest autoregressive model in both sample quality (measured by FID) and\nefficiency. For image captioning on MS-COCO dataset, our approach achieves\ncompetitive results compared to autoregressive models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruixiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1\">Geoffrey Hinton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Analogical Reasoning over Knowledge Graphs. (arXiv:2210.00312v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.00312","description":"<p>Analogical reasoning is fundamental to human cognition and holds an important\nplace in various fields. However, previous studies mainly focus on single-modal\nanalogical reasoning and ignore taking advantage of structure knowledge.\nNotably, the research in cognitive psychology has demonstrated that information\nfrom multimodal sources always brings more powerful cognitive transfer than\nsingle modality sources. To this end, we introduce the new task of multimodal\nanalogical reasoning over knowledge graphs, which requires multimodal reasoning\nability with the help of background knowledge. Specifically, we construct a\nMultimodal Analogical Reasoning dataSet (MARS) and a multimodal knowledge graph\nMarKG. We evaluate with multimodal knowledge graph embedding and pre-trained\nTransformer baselines, illustrating the potential challenges of the proposed\ntask. We further propose a novel model-agnostic Multimodal analogical reasoning\nframework with Transformer (MarT) motivated by the structure mapping theory,\nwhich can obtain better performance. Code and datasets are available in\nhttps://github.com/zjunlp/MKG_Analogy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization. (arXiv:2210.01241v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01241","description":"<p>We tackle the problem of aligning pre-trained large language models (LMs)\nwith human preferences. If we view text generation as a sequential\ndecision-making problem, reinforcement learning (RL) appears to be a natural\nconceptual framework. However, using RL for LM-based generation faces empirical\nchallenges, including training instability due to the combinatorial action\nspace, as well as a lack of open-source libraries and benchmarks customized for\nLM alignment. Thus, a question rises in the research community: is RL a\npractical paradigm for NLP?\n</p>\n<p>To help answer this, we first introduce an open-source modular library,\nRL4LMs (Reinforcement Learning for Language Models), for optimizing language\ngenerators with RL. The library consists of on-policy RL algorithms that can be\nused to train any encoder or encoder-decoder LM in the HuggingFace library\n(Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE\n(General Reinforced-language Understanding Evaluation) benchmark, a set of 6\nlanguage generation tasks which are supervised not by target strings, but by\nreward functions which capture automated measures of human preference.GRUE is\nthe first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally,\nwe introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language\nPolicy Optimization)} that learns to effectively reduce the combinatorial\naction space in language generation. We show 1) that RL techniques are\ngenerally better than supervised methods at aligning LMs to human preferences;\nand 2) that NLPO exhibits greater stability and performance than previous\npolicy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both\nautomatic and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramamurthy_R/0/1/0/all/0/1\">Rajkumar Ramamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1\">Kiant&#xe9; Brantley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifa_R/0/1/0/all/0/1\">Rafet Sifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauckhage_C/0/1/0/all/0/1\">Christian Bauckhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Binding Language Models in Symbolic Languages. (arXiv:2210.02875v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02875","description":"<p>Though end-to-end neural approaches have recently been dominating NLP tasks\nin both performance and ease-of-use, they lack interpretability and robustness.\nWe propose Binder, a training-free neural-symbolic framework that maps the task\ninput to a program, which (1) allows binding a unified API of language model\n(LM) functionalities to a programming language (e.g., SQL, Python) to extend\nits grammar coverage and thus tackle more diverse questions, (2) adopts an LM\nas both the program parser and the underlying model called by the API during\nexecution, and (3) requires only a few in-context exemplar annotations.\nSpecifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only\na few in-context exemplars, Codex is able to identify the part of the task\ninput that cannot be answerable by the original programming language, correctly\ngenerate API calls to prompt Codex to solve the unanswerable part, and identify\nwhere to place the API calls while being compatible with the original grammar.\nIn the execution stage, Codex can perform versatile functionalities (e.g.,\ncommonsense QA, information extraction) given proper prompts in the API calls.\nBinder achieves state-of-the-art results on WikiTableQuestions and TabFact\ndatasets, with explicit output programs that benefit human debugging. Note that\nprevious best systems are all finetuned on tens of thousands of task-specific\nsamples, while Binder only uses dozens of annotations as in-context exemplars\nwithout any training. Our code is available at https://github.com/HKUNLP/Binder .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhoujun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianbao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengzu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadkarni_R/0/1/0/all/0/1\">Rahul Nadkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yushi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning. (arXiv:2210.12587v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12587","description":"<p>Prompt tuning approaches, which learn task-specific soft prompts for a\ndownstream task conditioning on frozen pre-trained models, have attracted\ngrowing interest due to its parameter efficiency. With large language models\nand sufficient training data, prompt tuning performs comparably to full-model\ntuning. However, with limited training samples in few-shot settings, prompt\ntuning fails to match the performance of full-model fine-tuning. In this work,\nwe focus on improving the few-shot performance of prompt tuning by transferring\nknowledge from soft prompts of source tasks. Recognizing the good\ngeneralization capabilities of ensemble methods in low-data regime, we first\nexperiment and show that a simple ensemble of model predictions based on\ndifferent source prompts, outperforms existing multi-prompt knowledge transfer\napproaches such as source prompt fusion in the few-shot setting. Motivated by\nthis observation, we further investigate model ensembles and propose\nSample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the\ncontribution of each source model for each target sample separately when\nensembling source model outputs. Through this way, SESoM inherits the superior\ngeneralization of model ensemble approaches and simultaneously captures the\nsample-specific competence of each source prompt. We conduct experiments across\na diverse set of eight NLP tasks using models of different scales (T5-{base,\nlarge, XL}) and find that SESoM consistently outperforms the existing models of\nthe same as well as larger parametric scale by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choubey_P/0/1/0/all/0/1\">Prafulla Kumar Choubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Noisy Student Training on Non-target Domain Data for Automatic Speech Recognition. (arXiv:2211.04717v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2211.04717","description":"<p>Noisy Student Training (NST) has recently demonstrated extremely strong\nperformance in Automatic Speech Recognition(ASR). In this paper, we propose a\ndata selection strategy named LM Filter to improve the performance of NST on\nnon-target domain data in ASR tasks. Hypotheses with and without a Language\nModel are generated and the CER differences between them are utilized as a\nfilter threshold. Results reveal that significant improvements of 10.4%\ncompared with no data filtering baselines. We can achieve 3.31% CER in\nAISHELL-1 test set, which is best result from our knowledge without any other\nsupervised data. We also perform evaluations on the supervised 1000 hour\nAISHELL-2 dataset and competitive results of 4.73% CER can be achieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Junjie Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards continually learning new languages. (arXiv:2211.11703v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11703","description":"<p>Multilingual speech recognition with neural networks is often implemented\nwith batch-learning, when all of the languages are available before training.\nAn ability to add new languages after the prior training sessions can be\neconomically beneficial, but the main challenge is catastrophic forgetting. In\nthis work, we combine the qualities of weight factorization and elastic weight\nconsolidation in order to counter catastrophic forgetting and facilitate\nlearning new languages quickly. Such combination allowed us to eliminate\ncatastrophic forgetting while still achieving performance for the new languages\ncomparable with having all languages at once, in experiments of learning from\nan initial 10 languages to achieve 26 languages without catastrophic forgetting\nand a reasonable performance compared to training all languages from scratch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Ngoc-Quan Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph. (arXiv:2212.00959v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.00959","description":"<p>Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the\nanswer entities that are multiple hops away from the topic entities mentioned\nin a natural language question on a large-scale Knowledge Graph (KG). To cope\nwith the vast search space, existing work usually adopts a two-stage approach:\nit first retrieves a relatively small subgraph related to the question and then\nperforms the reasoning on the subgraph to find the answer entities accurately.\nAlthough these two stages are highly related, previous work employs very\ndifferent technical solutions for developing the retrieval and reasoning\nmodels, neglecting their relatedness in task essence. In this paper, we propose\nUniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and\nreasoning in both model architecture and parameter learning. For model\narchitecture, UniKGQA consists of a semantic matching module based on a\npre-trained language model~(PLM) for question-relation semantic matching, and a\nmatching information propagation module to propagate the matching information\nalong the directed edges on KGs. For parameter learning, we design a shared\npre-training task based on question-relation matching for both retrieval and\nreasoning models, and then propose retrieval- and reasoning-oriented\nfine-tuning strategies. Compared with previous studies, our approach is more\nunified, tightly relating the retrieval and reasoning stages. Extensive\nexperiments on three benchmark datasets have demonstrated the effectiveness of\nour method on the multi-hop KGQA task. Our codes and data are publicly\navailable at~\\url{https://github.com/RUCAIBox/UniKGQA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Avoiding spurious correlations via logit correction. (arXiv:2212.01433v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.01433","description":"<p>Empirical studies suggest that machine learning models trained with empirical\nrisk minimization (ERM) often rely on attributes that may be spuriously\ncorrelated with the class labels. Such models typically lead to poor\nperformance during inference for data lacking such correlations. In this work,\nwe explicitly consider a situation where potential spurious correlations are\npresent in the majority of training data. In contrast with existing approaches,\nwhich use the ERM model outputs to detect the samples without spurious\ncorrelations and either heuristically upweight or upsample those samples, we\npropose the logit correction (LC) loss, a simple yet effective improvement on\nthe softmax cross-entropy loss, to correct the sample logit. We demonstrate\nthat minimizing the LC loss is equivalent to maximizing the group-balanced\naccuracy, so the proposed LC could mitigate the negative impacts of spurious\ncorrelations. Our extensive experimental results further reveal that the\nproposed LC loss outperforms state-of-the-art solutions on multiple popular\nbenchmarks by a large margin, an average 5.5\\% absolute improvement, without\naccess to spurious attribute labels. LC is also competitive with oracle methods\nthat make use of the attribute labels. Code is available at\nhttps://github.com/shengliu66/LC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekhar_N/0/1/0/all/0/1\">Nitesh Sekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_P/0/1/0/all/0/1\">Prateek Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis. (arXiv:2212.05032v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2212.05032","description":"<p>Large-scale diffusion models have achieved state-of-the-art results on\ntext-to-image synthesis (T2I) tasks. Despite their ability to generate\nhigh-quality yet creative images, we observe that attribution-binding and\ncompositional capabilities are still considered major challenging issues,\nespecially when involving multiple objects. In this work, we improve the\ncompositional skills of T2I models, specifically more accurate attribute\nbinding and better image compositions. To do this, we incorporate linguistic\nstructures with the diffusion guidance process based on the controllable\nproperties of manipulating cross-attention layers in diffusion-based T2I\nmodels. We observe that keys and values in cross-attention layers have strong\nsemantic meanings associated with object layouts and content. Therefore, we can\nbetter preserve the compositional semantics in the generated image by\nmanipulating the cross-attention representations based on linguistic insights.\nBuilt upon Stable Diffusion, a SOTA T2I model, our structured cross-attention\ndesign is efficient that requires no additional training samples. We achieve\nbetter compositional skills in qualitative and quantitative results, leading to\na 5-8% advantage in head-to-head user comparison studies. Lastly, we conduct an\nin-depth analysis to reveal potential causes of incorrect image compositions\nand justify the properties of cross-attention layers in the generation process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Weixi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuehai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tsu-Jui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akula_A/0/1/0/all/0/1\">Arjun Akula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayana_P/0/1/0/all/0/1\">Pradyumna Narayana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Sugato Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training language models to summarize narratives improves brain alignment. (arXiv:2212.10898v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10898","description":"<p>Building systems that achieve a deeper understanding of language is one of\nthe central goals of natural language processing (NLP). Towards this goal,\nrecent works have begun to train language models on narrative datasets which\nrequire extracting the most critical information by integrating across long\ncontexts. However, it is still an open question whether these models are\nlearning a deeper understanding of the text, or if the models are simply\nlearning a heuristic to complete the task. This work investigates this further\nby turning to the one language processing system that truly understands complex\nlanguage: the human brain. We show that training language models for deeper\nnarrative understanding results in richer representations that have improved\nalignment to human brain activity. We further find that the improvements in\nbrain alignment are larger for character names than for other discourse\nfeatures, which indicates that these models are learning important narrative\nelements. Taken together, these results suggest that this type of training can\nindeed lead to deeper language understanding. These findings have consequences\nboth for cognitive neuroscience by revealing some of the significant factors\nbehind brain-NLP alignment, and for NLP by highlighting that understanding of\nlong-range context can be improved beyond language modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aw_K/0/1/0/all/0/1\">Khai Loong Aw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toneva_M/0/1/0/all/0/1\">Mariya Toneva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling. (arXiv:2301.00591v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.00591","description":"<p>This work profoundly analyzes discrete self-supervised speech representations\n(units) through the eyes of Generative Spoken Language Modeling (GSLM).\nFollowing the findings of such an analysis, we propose practical improvements\nto the discrete unit for the GSLM. First, we start comprehending these units by\nanalyzing them in three axes: interpretation, visualization, and resynthesis.\nOur analysis finds a high correlation between the speech units to phonemes and\nphoneme families, while their correlation with speaker or gender is weaker.\nAdditionally, we found redundancies in the extracted units and claim that one\nreason may be the units' context. Following this analysis, we propose a new,\nunsupervised metric to measure unit redundancies. Finally, we use this metric\nto develop new methods that improve the robustness of units' clustering and\nshow significant improvement considering zero-resource speech metrics such as\nABX. Code and analysis tools are available under the following link:\nhttps://github.com/slp-rl/SLM-Discrete-Representations\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sicherman_A/0/1/0/all/0/1\">Amitay Sicherman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt Learning for Automatic Scoring in Science Education. (arXiv:2301.08771v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.08771","description":"<p>Developing models to automatically score students' written responses to\nscience problems is critical for science education. However, collecting and\nlabeling sufficient student responses for training models is time and\ncost-consuming. Recent studies suggest that pre-trained language models (PLMs)\ncan be adapted to downstream tasks without fine-tuning with prompts. However,\nno research has employed such a prompt approach in science education. As\nstudent responses are presented with natural language, aligning the scoring\nprocedure as the next sentence prediction task using prompts can skip the\ncostly fine-tuning stage. In this study, we developed a zero-shot approach to\nautomatically score student responses via Matching Exemplars as Next Sentence\nPrediction (MeNSP). This approach employs no training samples. We first apply\nMeNSP in scoring three assessment tasks of scientific argumentation and found\nmachine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and\nF1 score ranges from 0.54 to 0.81. To improve the performance, we extend our\nresearch to the few-shots setting, either randomly selecting labeled student\nresponses or manually constructing responses to fine-tune the models. We find\nthat one task's performance is improved with more samples, Cohen's Kappa from\n0.30 to 0.38, and F1 score from 0.54 to 0.59; for the two others, scoring\nperformance is not improved. We also find that randomly selected few-shots\nperform better than the human expert-crafted approach. This study suggests that\nMeNSP can yield referable automatic scoring for student responses while\nsignificantly reducing the cost of model training. This method can benefit\nlow-stakes classroom assessment practices in science education. Future research\nshould further explore the applicability of the MeNSP in different types of\nassessment tasks in science education and improve the model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuansheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xinyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaoming Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UzbekTagger: The rule-based POS tagger for Uzbek language. (arXiv:2301.12711v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12711","description":"<p>This research paper presents a part-of-speech (POS) annotated dataset and\ntagger tool for the low-resource Uzbek language. The dataset includes 12 tags,\nwhich were used to develop a rule-based POS-tagger tool. The corpus text used\nin the annotation process was made sure to be balanced over 20 different fields\nin order to ensure its representativeness. Uzbek being an agglutinative\nlanguage so the most of the words in an Uzbek sentence are formed by adding\nsuffixes. This nature of it makes the POS-tagging task difficult to find the\nstems of words and the right part-of-speech they belong to. The methodology\nproposed in this research is the stemming of the words with an affix/suffix\nstripping approach including database of the stem forms of the words in the\nUzbek language. The tagger tool was tested on the annotated dataset and showed\nhigh accuracy in identifying and tagging parts of speech in Uzbek text. This\nnewly presented dataset and tagger tool can be used for a variety of natural\nlanguage processing tasks such as language modeling, machine translation, and\ntext-to-speech synthesis. The presented dataset is the first of its kind to be\nmade publicly available for Uzbek, and the POS-tagger tool created can also be\nused as a pivot to use as a base for other closely-related Turkic languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharipov_M/0/1/0/all/0/1\">Maksud Sharipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuriyozov_E/0/1/0/all/0/1\">Elmurod Kuriyozov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuldashev_O/0/1/0/all/0/1\">Ollabergan Yuldashev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sobirov_O/0/1/0/all/0/1\">Ogabek Sobirov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E2E Spoken Entity Extraction for Virtual Agents. (arXiv:2302.10186v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2302.10186","description":"<p>This paper reimagines some aspects of speech processing using speech\nencoders, specifically about extracting entities directly from speech, with no\nintermediate textual representation. In human-computer conversations,\nextracting entities such as names, postal addresses and email addresses from\nspeech is a challenging task. In this paper, we study the impact of fine-tuning\npre-trained speech encoders on extracting spoken entities in human-readable\nform directly from speech without the need for text transcription. We\nillustrate that such a direct approach optimizes the encoder to transcribe only\nthe entity relevant portions of speech, ignoring the superfluous portions such\nas carrier phrases and spellings of entities. In the context of dialogs from an\nenterprise virtual agent, we demonstrate that the 1-step approach outperforms\nthe typical 2-step cascade of first generating lexical transcriptions followed\nby text-based entity extraction for identifying spoken entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Singla_K/0/1/0/all/0/1\">Karan Singla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_Y/0/1/0/all/0/1\">Yeon-Jun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Price_R/0/1/0/all/0/1\">Ryan Price</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jalalvand_S/0/1/0/all/0/1\">Shahab Jalalvand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bangalore_S/0/1/0/all/0/1\">Srinivas Bangalore</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. (arXiv:2302.12813v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12813","description":"<p>Large language models (LLMs), such as ChatGPT, are able to generate\nhuman-like, fluent responses for many downstream tasks, e.g., task-oriented\ndialog and question answering. However, applying LLMs to real-world,\nmission-critical applications remains challenging mainly due to their tendency\nto generate hallucinations and inability to use external knowledge.This paper\nproposes a LLM-Augmenter system, which augments a black-box LLM with a set of\nplug-and-play modules. Our system makes the LLM generate responses grounded in\nconsolidated external knowledge, e.g., stored in task-specific databases. It\nalso iteratively revises LLM prompts to improve model responses using feedback\ngenerated by utility functions, e.g., the factuality score of a LLM-generated\nresponse. The effectiveness of LLM-Augmenter is empirically validated on two\ntypes of mission-critical scenarios, task-oriented dialog and open-domain\nquestion answering. LLM-Augmenter significantly reduces ChatGPT's\nhallucinations without sacrificing the fluency and informativeness of its\nresponses. We make the source code and models publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiuyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liden_L/0/1/0/all/0/1\">Lars Liden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HULAT at SemEval-2023 Task 10: Data augmentation for pre-trained transformers applied to the detection of sexism in social media. (arXiv:2302.12840v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12840","description":"<p>This paper describes our participation in SemEval-2023 Task 10, whose goal is\nthe detection of sexism in social media. We explore some of the most popular\ntransformer models such as BERT, DistilBERT, RoBERTa, and XLNet. We also study\ndifferent data augmentation techniques to increase the training dataset. During\nthe development phase, our best results were obtained by using RoBERTa and data\naugmentation for tasks B and C. However, the use of synthetic data does not\nimprove the results for task C. We participated in the three subtasks. Our\napproach still has much room for improvement, especially in the two\nfine-grained classifications. All our code is available in the repository\nhttps://github.com/isegura/hulat_edos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Segura_Bedmar_I/0/1/0/all/0/1\">Isabel Segura-Bedmar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Is Not All You Need: Aligning Perception with Language Models. (arXiv:2302.14045v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.14045","description":"<p>A big convergence of language, multimodal perception, action, and world\nmodeling is a key step toward artificial general intelligence. In this work, we\nintroduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive\ngeneral modalities, learn in context (i.e., few-shot), and follow instructions\n(i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale\nmultimodal corpora, including arbitrarily interleaved text and images,\nimage-caption pairs, and text data. We evaluate various settings, including\nzero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range\nof tasks without any gradient updates or finetuning. Experimental results show\nthat Kosmos-1 achieves impressive performance on (i) language understanding,\ngeneration, and even OCR-free NLP (directly fed with document images), (ii)\nperception-language tasks, including multimodal dialogue, image captioning,\nvisual question answering, and (iii) vision tasks, such as image recognition\nwith descriptions (specifying classification via text instructions). We also\nshow that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge\nfrom language to multimodal, and from multimodal to language. In addition, we\nintroduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning\ncapability of MLLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yaru Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_O/0/1/0/all/0/1\">Owais Khan Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Barun Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1\">Kriti Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bjorck_J/0/1/0/all/0/1\">Johan Bjorck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Som_S/0/1/0/all/0/1\">Subhojit Som</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}