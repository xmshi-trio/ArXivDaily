{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-22T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Social media mining for toxicovigilance of prescription medications: End-to-end pipeline, challenges and future work. (arXiv:2211.10443v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10443","description":"<p>Substance use, substance use disorder, and overdoses related to substance use\nare major public health problems globally and in the United States. A key\naspect of addressing these problems from a public health standpoint is improved\nsurveillance. Traditional surveillance systems are laggy, and social media are\npotentially useful sources of timely data. However, mining knowledge from\nsocial media is challenging, and requires the development of advanced\nartificial intelligence, specifically natural language processing (NLP) and\nmachine learning methods. We developed a sophisticated end-to-end pipeline for\nmining information about nonmedical prescription medication use from social\nmedia, namely Twitter and Reddit. Our pipeline employs supervised machine\nlearning and NLP for filtering out noise and characterizing the chatter. In\nthis paper, we describe our end-to-end pipeline developed over four years. In\naddition to describing our data mining infrastructure, we discuss existing\nchallenges in social media mining for toxicovigilance, and possible future\nresearch directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarker_A/0/1/0/all/0/1\">Abeed Sarker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Refinement based on Triplet BERT-Networks. (arXiv:2211.10460v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10460","description":"<p>Knowledge graph embedding techniques are widely used for knowledge graph\nrefinement tasks such as graph completion and triple classification. These\ntechniques aim at embedding the entities and relations of a Knowledge Graph\n(KG) in a low dimensional continuous feature space. This paper adopts a\ntransformer-based triplet network creating an embedding space that clusters the\ninformation about an entity or relation in the KG. It creates textual sequences\nfrom facts and fine-tunes a triplet network of pre-trained transformer-based\nlanguage models. It adheres to an evaluation paradigm that relies on an\nefficient spatial semantic search technique. We show that this evaluation\nprotocol is more adapted to a few-shot setting for the relation prediction\ntask. Our proposed GilBERT method is evaluated on triplet classification and\nrelation prediction tasks on multiple well-known benchmark knowledge graphs\nsuch as FB13, WN11, and FB15K. We show that GilBERT achieves better or\ncomparable results to the state-of-the-art performance on these two refinement\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nassiri_A/0/1/0/all/0/1\">Armita Khajeh Nassiri</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Pernelle_N/0/1/0/all/0/1\">Nathalie Pernelle</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Sais_F/0/1/0/all/0/1\">Fatiha Sais</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Quercini_G/0/1/0/all/0/1\">Gianluca Quercini</a> (1) ((1) LISN, CNRS UMR 9015, University of Paris Saclay (2) LIPN, CNRS UMR 7030, University of Sorbonne Paris Nord)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Generation From Text. (arXiv:2211.10511v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10511","description":"<p>In this work we propose a novel end-to-end multi-stage Knowledge Graph (KG)\ngeneration system from textual inputs, separating the overall process into two\nstages. The graph nodes are generated first using pretrained language model,\nfollowed by a simple edge construction head, enabling efficient KG extraction\nfrom the text. For each stage we consider several architectural choices that\ncan be used depending on the available training resources. We evaluated the\nmodel on a recent WebNLG 2020 Challenge dataset, matching the state-of-the-art\nperformance on text-to-RDF generation task, as well as on New York Times (NYT)\nand a large-scale TekGen datasets, showing strong overall performance,\noutperforming the existing baselines. We believe that the proposed system can\nserve as a viable KG construction alternative to the existing linearization or\nsampling-based graph generation approaches. Our code can be found at\nhttps://github.com/IBM/Grapher\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_I/0/1/0/all/0/1\">Igor Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dognin_P/0/1/0/all/0/1\">Pierre Dognin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bipartite-play Dialogue Collection for Practical Automatic Evaluation of Dialogue Systems. (arXiv:2211.10596v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10596","description":"<p>Automation of dialogue system evaluation is a driving force for the efficient\ndevelopment of dialogue systems. This paper introduces the bipartite-play\nmethod, a dialogue collection method for automating dialogue system evaluation.\nIt addresses the limitations of existing dialogue collection methods: (i)\ninability to compare with systems that are not publicly available, and (ii)\nvulnerability to cheating by intentionally selecting systems to be compared.\nExperimental results show that the automatic evaluation using the\nbipartite-play method mitigates these two drawbacks and correlates as strongly\nwith human subjectivity as existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sato_S/0/1/0/all/0/1\">Shiki Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kishinami_Y/0/1/0/all/0/1\">Yosuke Kishinami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_H/0/1/0/all/0/1\">Hiroaki Sugiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akama_R/0/1/0/all/0/1\">Reina Akama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokuhisa_R/0/1/0/all/0/1\">Ryoko Tokuhisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1\">Jun Suzuki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-Assisted Language Models for Identifying Check-worthy Sentences. (arXiv:2211.10678v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10678","description":"<p>We propose a new uniform framework for text classification and ranking that\ncan automate the process of identifying check-worthy sentences in political\ndebates and speech transcripts. Our framework combines the semantic analysis of\nthe sentences, with additional entity embeddings obtained through the\nidentified entities within the sentences. In particular, we analyse the\nsemantic meaning of each sentence using state-of-the-art neural language models\nsuch as BERT, ALBERT, and RoBERTa, while embeddings for entities are obtained\nfrom knowledge graph (KG) embedding models. Specifically, we instantiate our\nframework using five different language models, entity embeddings obtained from\nsix different KG embedding models, as well as two combination methods leading\nto several Entity-Assisted neural language models. We extensively evaluate the\neffectiveness of our framework using two publicly available datasets from the\nCLEF' 2019 &amp; 2020 CheckThat! Labs. Our results show that the neural language\nmodels significantly outperform traditional TF.IDF and LSTM methods. In\naddition, we show that the ALBERT model is consistently the most effective\nmodel among all the tested neural language models. Our entity embeddings\nsignificantly outperform other existing approaches from the literature that are\nbased on similarity and relatedness scores between the entities in a sentence,\nwhen used alongside a KG embedding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_T/0/1/0/all/0/1\">Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macdonald_C/0/1/0/all/0/1\">Craig Macdonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ounis_I/0/1/0/all/0/1\">Iadh Ounis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pairwise Instance Relation Augmentation for Long-tailed Multi-label Text Classification. (arXiv:2211.10685v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10685","description":"<p>Multi-label text classification (MLTC) is one of the key tasks in natural\nlanguage processing. It aims to assign multiple target labels to one document.\nDue to the uneven popularity of labels, the number of documents per label\nfollows a long-tailed distribution in most cases. It is much more challenging\nto learn classifiers for data-scarce tail labels than for data-rich head\nlabels. The main reason is that head labels usually have sufficient\ninformation, e.g., a large intra-class diversity, while tail labels do not. In\nresponse, we propose a Pairwise Instance Relation Augmentation Network (PIRAN)\nto augment tailed-label documents for balancing tail labels and head labels.\nPIRAN consists of a relation collector and an instance generator. The former\naims to extract the document pairwise relations from head labels. Taking these\nrelations as perturbations, the latter tries to generate new document instances\nin high-level feature space around the limited given tailed-label instances.\nMeanwhile, two regularizers (diversity and consistency) are designed to\nconstrain the generation process. The consistency-regularizer encourages the\nvariance of tail labels to be close to head labels and further balances the\nwhole datasets. And diversity-regularizer makes sure the generated instances\nhave diversity and avoids generating redundant instances. Extensive\nexperimental results on three benchmark datasets demonstrate that PIRAN\nconsistently outperforms the SOTA methods, and dramatically improves the\nperformance of tail labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReInform: Selecting paths with reinforcement learning for contextualized link prediction. (arXiv:2211.10688v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10688","description":"<p>We propose to use reinforcement learning to inform transformer-based\ncontextualized link prediction models by providing paths that are most useful\nfor predicting the correct answer. This is in contrast to previous approaches,\nthat either used reinforcement learning (RL) to directly search for the answer,\nor based their prediction on limited or randomly selected context. Our\nexperiments on WN18RR and FB15k-237 show that contextualized link prediction\nmodels consistently outperform RL-based answer search, and that additional\nimprovements (of up to 13.5\\% MRR) can be gained by combining RL with a link\nprediction model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Speranskaya_M/0/1/0/all/0/1\">Marina Speranskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Methias_S/0/1/0/all/0/1\">Sameh Methias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Suffering from Vaccines or from Government? : Partisan Bias in COVID-19 Vaccine Adverse Events Coverage. (arXiv:2211.10707v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10707","description":"<p>Vaccine adverse events have been presumed to be a relatively objective\nmeasure that is immune to political polarization. The real-world data, however,\nshows the correlation between presidential disapproval ratings and the\nsubjective severity of adverse events. This paper investigates the partisan\nbias in COVID vaccine adverse events coverage with language models that can\nclassify the topic of vaccine-related articles and the political disposition of\nnews comments. Based on 90K news articles from 52 major newspaper companies, we\nfound that conservative media are inclined to report adverse events more\nfrequently than their liberal counterparts, while the coverage itself was\nstatistically uncorrelated with the severity of real-world adverse events. The\nusers who support the conservative opposing party were more likely to write the\npopular comments from 2.3K random sampled articles on news platforms. This\nresearch implies that bipartisanship can still play a significant role in\nforming public opinion on the COVID vaccine even after the majority of the\npopulation's vaccination\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1\">TaeYoung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hanbin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metaphorical Language Change Is Self-Organized Criticality. (arXiv:2211.10709v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10709","description":"<p>One way to resolve the actuation problem of metaphorical language change is\nto provide a statistical profile of metaphorical constructions and generative\nrules with antecedent conditions. Based on arguments from the view of language\nas complex systems and the dynamic view of metaphor, this paper argues that\nmetaphorical language change qualifies as a self-organized criticality state\nand the linguistic expressions of a metaphor can be profiled as a fractal with\nspatio-temporal correlations. Synchronously, these metaphorical expressions\nself-organize into a self-similar, scale-invariant fractal that follows a\npower-law distribution; temporally, long range inter-dependence constrains the\nself-organization process by the way of transformation rules that are intrinsic\nof a language system. This argument is verified in the paper with statistical\nanalyses of twelve randomly selected Chinese verb metaphors in a large-scale\ndiachronic corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuri Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Huifang Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArtELingo: A Million Emotion Annotations of WikiArt with Emphasis on Diversity over Language and Culture. (arXiv:2211.10780v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10780","description":"<p>This paper introduces ArtELingo, a new benchmark and dataset, designed to\nencourage work on diversity across languages and cultures. Following ArtEmis, a\ncollection of 80k artworks from WikiArt with 0.45M emotion labels and\nEnglish-only captions, ArtELingo adds another 0.79M annotations in Arabic and\nChinese, plus 4.8K in Spanish to evaluate \"cultural-transfer\" performance. More\nthan 51K artworks have 5 annotations or more in 3 languages. This diversity\nmakes it possible to study similarities and differences across languages and\ncultures. Further, we investigate captioning tasks, and find diversity improves\nthe performance of baseline models. ArtELingo is publicly available at\nhttps://www.artelingo.org/ with standard splits and baseline models. We hope\nour work will help ease future research on multilinguality and culturally-aware\nAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_Y/0/1/0/all/0/1\">Youssef Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelfattah_M/0/1/0/all/0/1\">Mohamed Abdelfattah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhuwaider_S/0/1/0/all/0/1\">Shyma Alhuwaider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1\">Kenneth Ward Church</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation. (arXiv:2211.10797v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10797","description":"<p>In the study, we empirically compare the two recently proposed decoding\nmethods, i.e. Contrastive Search (CS) and Contrastive Decoding (CD), for\nopen-ended text generation. The automatic evaluation results suggest that,\nwhile CS performs worse than CD on the MAUVE metric, it substantially surpasses\nCD on the diversity and coherence metrics. More notably, extensive human\nevaluations across three different domains demonstrate that human annotators\nare universally more in favor of CS over CD with substantial margins.\n</p>\n<p>The contradicted results between MAUVE and human evaluations reveal that\nMAUVE does not accurately reflect human preferences. Therefore, we call upon\nthe research community to develop better evaluation metrics for open-ended text\ngeneration. To ensure the reproducibility of our work, we have open-sourced all\nour code, evaluation results, as well as human annotations at\nhttps://github.com/yxuansu/Contrastive_Search_versus_Contrastive_Decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jialu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining State-of-the-Art Models with Maximal Marginal Relevance for Few-Shot and Zero-Shot Multi-Document Summarization. (arXiv:2211.10808v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10808","description":"<p>In Natural Language Processing, multi-document summarization (MDS) poses many\nchallenges to researchers above those posed by single-document summarization\n(SDS). These challenges include the increased search space and greater\npotential for the inclusion of redundant information. While advancements in\ndeep learning approaches have led to the development of several advanced\nlanguage models capable of summarization, the variety of training data specific\nto the problem of MDS remains relatively limited. Therefore, MDS approaches\nwhich require little to no pretraining, known as few-shot or zero-shot\napplications, respectively, could be beneficial additions to the current set of\ntools available in summarization. To explore one possible approach, we devise a\nstrategy for combining state-of-the-art models' outputs using maximal marginal\nrelevance (MMR) with a focus on query relevance rather than document diversity.\nOur MMR-based approach shows improvement over some aspects of the current\nstate-of-the-art results in both few-shot and zero-shot MDS applications while\nmaintaining a state-of-the-art standard of output by all available metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adams_D/0/1/0/all/0/1\">David Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suri_G/0/1/0/all/0/1\">Gandharv Suri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chali_Y/0/1/0/all/0/1\">Yllias Chali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeDR: Segment Representation Learning for Long Documents Dense Retrieval. (arXiv:2211.10841v1 [cs.IR])","link":"http://arxiv.org/abs/2211.10841","description":"<p>Recently, Dense Retrieval (DR) has become a promising solution to document\nretrieval, where document representations are used to perform effective and\nefficient semantic search. However, DR remains challenging on long documents,\ndue to the quadratic complexity of its Transformer-based encoder and the finite\ncapacity of a low-dimension embedding. Current DR models use suboptimal\nstrategies such as truncating or splitting-and-pooling to long documents\nleading to poor utilization of whole document information. In this work, to\ntackle this problem, we propose Segment representation learning for long\ndocuments Dense Retrieval (SeDR). In SeDR, Segment-Interaction Transformer is\nproposed to encode long documents into document-aware and segment-sensitive\nrepresentations, while it holds the complexity of splitting-and-pooling and\noutperforms other segment-interaction patterns on DR. Since GPU memory\nrequirements for long document encoding causes insufficient negatives for DR\ntraining, Late-Cache Negative is further proposed to provide additional cache\nnegatives for optimizing representation learning. Experiments on MS MARCO and\nTREC-DL datasets show that SeDR achieves superior performance among DR models,\nand confirm the effectiveness of SeDR on long document retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongfang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yutao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mulco: Recognizing Chinese Nested Named Entities Through Multiple Scopes. (arXiv:2211.10854v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10854","description":"<p>Nested Named Entity Recognition (NNER) has been a long-term challenge to\nresearchers as an important sub-area of Named Entity Recognition. NNER is where\none entity may be part of a longer entity, and this may happen on multiple\nlevels, as the term nested suggests. These nested structures make traditional\nsequence labeling methods unable to properly recognize all entities. While\nrecent researches focus on designing better recognition methods for NNER in a\nvariety of languages, the Chinese NNER (CNNER) still lacks attention, where a\nfree-for-access, CNNER-specialized benchmark is absent. In this paper, we aim\nto solve CNNER problems by providing a Chinese dataset and a learning-based\nmodel to tackle the issue. To facilitate the research on this task, we release\nChiNesE, a CNNER dataset with 20,000 sentences sampled from online passages of\nmultiple domains, containing 117,284 entities failing in 10 categories, where\n43.8 percent of those entities are nested. Based on ChiNesE, we propose Mulco,\na novel method that can recognize named entities in nested structures through\nmultiple scopes. Each scope use a designed scope-based sequence labeling\nmethod, which predicts an anchor and the length of a named entity to recognize\nit. Experiment results show that Mulco has outperformed several baseline\nmethods with the different recognizing schemes on ChiNesE. We also conduct\nextensive experiments on ACE2005 Chinese corpus, where Mulco has achieved the\nbest performance compared with the baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiuding Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jinwen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weidong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jerry Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Interrogation for Attributing Language Models. (arXiv:2211.10877v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10877","description":"<p>This paper presents solutions to the Machine Learning Model Attribution\nchallenge (MLMAC) collectively organized by MITRE, Microsoft, Schmidt-Futures,\nRobust-Intelligence, Lincoln-Network, and Huggingface community. The challenge\nprovides twelve open-sourced base versions of popular language models developed\nby well-known organizations and twelve fine-tuned language models for text\ngeneration. The names and architecture details of fine-tuned models were kept\nhidden, and participants can access these models only through the rest APIs\ndeveloped by the organizers. Given these constraints, the goal of the contest\nis to identify which fine-tuned models originated from which base model. To\nsolve this challenge, we have assumed that fine-tuned models and their\ncorresponding base versions must share a similar vocabulary set with a matching\nsyntactical writing style that resonates in their generated outputs. Our\nstrategy is to develop a set of queries to interrogate base and fine-tuned\nmodels. And then perform one-to-many pairing between them based on similarities\nin their generated responses, where more than one fine-tuned model can pair\nwith a base model but not vice-versa. We have employed four distinct approaches\nfor measuring the resemblance between the responses generated from the models\nof both sets. The first approach uses evaluation metrics of the machine\ntranslation, and the second uses a vector space model. The third approach uses\nstate-of-the-art multi-class text classification, Transformer models. Lastly,\nthe fourth approach uses a set of Transformer based binary text classifiers,\none for each provided base model, to perform multi-class text classification in\na one-vs-all fashion. This paper reports implementation details, comparison,\nand experimental studies, of these approaches along with the final obtained\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhanani_F/0/1/0/all/0/1\">Farhan Dhanani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafi_M/0/1/0/all/0/1\">Muhammad Rafi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Error correction and extraction in request dialogs. (arXiv:2004.04243v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.04243","description":"<p>We propose a dialog system utility component that gets the two last\nutterances of a user and can detect whether the last utterance is an error\ncorrection of the second last utterance. If yes, it corrects the second last\nutterance according to the error correction in the last utterance. In addition,\nthe proposed component outputs the extracted pairs of reparandum and repair\nentity. This component offers two advantages, learning the concept of\ncorrections to avoid collecting corrections for every new domain and extracting\nreparandum and repair pairs, which offers the possibility to learn out of it.\n</p>\n<p>For the error correction one sequence labeling and two sequence to sequence\napproaches are presented. For the error correction detection these three error\ncorrection approaches can also be used and in addition, we present a sequence\nclassification approach. One error correction detection and one error\ncorrection approach can be combined to a pipeline or the error correction\napproaches can be trained and used end-to-end to avoid two components. We\nmodified the EPIC-KITCHENS-100 dataset to evaluate the approaches for\ncorrecting entity phrases in request dialogs. For error correction detection\nand correction, we got an accuracy of 97.54 % on synthetic validation data and\nan accuracy of 69.27 % on human-created real-world test data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Constantin_S/0/1/0/all/0/1\">Stefan Constantin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Attention with Performers. (arXiv:2009.14794v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.14794","description":"<p>We introduce Performers, Transformer architectures which can estimate regular\n(softmax) full-rank-attention Transformers with provable accuracy, but using\nonly linear (as opposed to quadratic) space and time complexity, without\nrelying on any priors such as sparsity or low-rankness. To approximate softmax\nattention-kernels, Performers use a novel Fast Attention Via positive\nOrthogonal Random features approach (FAVOR+), which may be of independent\ninterest for scalable kernel methods. FAVOR+ can be also used to efficiently\nmodel kernelizable attention mechanisms beyond softmax. This representational\npower is crucial to accurately compare softmax with other kernels for the first\ntime on large-scale tasks, beyond the reach of regular Transformers, and\ninvestigate optimal attention-kernels. Performers are linear architectures\nfully compatible with regular Transformers and with strong theoretical\nguarantees: unbiased or nearly-unbiased estimation of the attention matrix,\nuniform convergence and low estimation variance. We tested Performers on a rich\nset of tasks stretching from pixel-prediction through text models to protein\nsequence modeling. We demonstrate competitive results with other examined\nefficient sparse and dense attention methods, showcasing effectiveness of the\nnovel attention-learning paradigm leveraged by Performers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1\">Krzysztof Choromanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhosherstov_V/0/1/0/all/0/1\">Valerii Likhosherstov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyou Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gane_A/0/1/0/all/0/1\">Andreea Gane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarlos_T/0/1/0/all/0/1\">Tamas Sarlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_P/0/1/0/all/0/1\">Peter Hawkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">Jared Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohiuddin_A/0/1/0/all/0/1\">Afroz Mohiuddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_L/0/1/0/all/0/1\">Lukasz Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belanger_D/0/1/0/all/0/1\">David Belanger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colwell_L/0/1/0/all/0/1\">Lucy Colwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v9 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08315","description":"<p>Large language models have shown promising results in zero-shot settings\n(Brown et al.,2020; Radford et al., 2019). For example, they can perform\nmultiple choice tasks simply by conditioning on a question and selecting the\nanswer with the highest probability.\n</p>\n<p>However, ranking by string probability can be problematic due to surface form\ncompetition-wherein different surface forms compete for probability mass, even\nif they represent the same underlying concept, e.g. \"computer\" and \"PC.\" Since\nprobability mass is finite, this lowers the probability of the correct answer,\ndue to competition from other strings that are valid answers (but not one of\nthe multiple choice options).\n</p>\n<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative\nscoring function that directly compensates for surface form competition by\nsimply reweighing each option according to a term that is proportional to its a\npriori likelihood within the context of the specific zero-shot task. It\nachieves consistent gains in zero-shot performance over both calibrated (Zhao\net al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models\nover a variety of multiple choice datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Entity Abstraction Help Generative Transformers Reason?. (arXiv:2201.01787v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.01787","description":"<p>We study the utility of incorporating entity type abstractions into\npre-trained Transformers and test these methods on four NLP tasks requiring\ndifferent forms of logical reasoning: (1) compositional language understanding\nwith text-based relational reasoning (CLUTRR), (2) abductive reasoning\n(ProofWriter), (3) multi-hop question answering (HotpotQA), and (4)\nconversational question answering (CoQA). We propose and empirically explore\nthree ways to add such abstraction: (i) as additional input embeddings, (ii) as\na separate sequence to encode, and (iii) as an auxiliary prediction task for\nthe model. Overall, our analysis demonstrates that models with abstract entity\nknowledge performs better than without it. The best abstraction aware models\nachieved an overall accuracy of 88.8% and 91.8% compared to the baseline model\nachieving 62.9% and 89.8% on CLUTRR and ProofWriter respectively. However, for\nHotpotQA and CoQA, we find that F1 scores improve by only 0.5% on average. Our\nresults suggest that the benefit of explicit abstraction is significant in\nformally defined logical reasoning settings requiring many reasoning hops, but\npoint to the notion that it is less beneficial for NLP tasks having less formal\nlogical structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gontier_N/0/1/0/all/0/1\">Nicolas Gontier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GUDN: A novel guide network with label reinforcement strategy for extreme multi-label text classification. (arXiv:2201.11582v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11582","description":"<p>In natural language processing, extreme multi-label text classification is an\nemerging but essential task. The problem of extreme multi-label text\nclassification (XMTC) is to recall some of the most relevant labels for a text\nfrom an extremely large label set. Large-scale pre-trained models have brought\na new trend to this problem. Though the large-scale pre-trained models have\nmade significant achievements on this problem, the valuable fine-tuned methods\nhave yet to be studied. Though label semantics have been introduced in XMTC,\nthe vast semantic gap between texts and labels has yet to gain enough\nattention. This paper builds a new guide network (GUDN) to help fine-tune the\npre-trained model to instruct classification later. Furthermore, GUDN uses raw\nlabel semantics combined with a helpful label reinforcement strategy to\neffectively explore the latent space between texts and labels, narrowing the\nsemantic gap, which can further improve predicted accuracy. Experimental\nresults demonstrate that GUDN outperforms state-of-the-art methods on Eurlex-4k\nand has competitive results on other popular datasets. In an additional\nexperiment, we investigated the input lengths' influence on the\nTransformer-based model's accuracy. Our source code is released at\nhttps://t.hk.uy/aFSH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jia Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_H/0/1/0/all/0/1\">Hongji Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asamoah_K/0/1/0/all/0/1\">Kwame Omono Asamoah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianyang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Cong Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UDAAN: Machine Learning based Post-Editing tool for Document Translation. (arXiv:2203.01644v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.01644","description":"<p>We introduce UDAAN, an open-source post-editing tool that can reduce manual\nediting efforts to quickly produce publishable-standard documents in several\nIndic languages. UDAAN has an end-to-end Machine Translation (MT) plus\npost-editing pipeline wherein users can upload a document to obtain raw MT\noutput. Further, users can edit the raw translations using our tool. UDAAN\noffers several advantages: a) Domain-aware, vocabulary-based lexical\nconstrained MT. b) source-target and target-target lexicon suggestions for\nusers. Replacements are based on the source and target texts lexicon alignment.\nc) Translation suggestions are based on logs created during user interaction.\nd) Source-target sentence alignment visualisation that reduces the cognitive\nload of users during editing. e) Translated outputs from our tool are available\nin multiple formats: docs, latex, and PDF. We also provide the facility to use\naround 100 in-domain dictionaries for lexicon-aware machine translation.\nAlthough we limit our experiments to English-to-Hindi translation, our tool is\nindependent of the source and target languages. Experimental results based on\nthe usage of the tools and users feedback show that our tool speeds up the\ntranslation time by approximately a factor of three compared to the baseline\nmethod of translating documents from scratch. Our tool is available for both\nWindows and Linux platforms. The tool is open-source under MIT license, and the\nsource code can be accessed from our website at https://www.udaanproject.org.\nDemonstration and tutorial videos for various features of our tool can be\naccessed at https://www.youtube.com/channel/UClfK7iC8J7b22bj3GwAUaCw. Our MT\npipeline can be accessed at https://udaaniitb.aicte-india.org/udaan/translate/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1\">Ayush Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravindran_A/0/1/0/all/0/1\">Ajay Ravindran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_V/0/1/0/all/0/1\">Venkatapathy Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building for Tomorrow: Assessing the Temporal Persistence of Text Classifiers. (arXiv:2205.05435v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05435","description":"<p>Performance of text classification models tends to drop over time due to\nchanges in data, which limits the lifetime of a pretrained model. Therefore an\nability to predict a model's ability to persist over time can help design\nmodels that can be effectively used over a longer period of time. In this\npaper, we provide a thorough discussion into the problem, establish an\nevaluation setup for the task. We look at this problem from a practical\nperspective by assessing the ability of a wide range of language models and\nclassification algorithms to persist over time, as well as how dataset\ncharacteristics can help predict the temporal stability of different models. We\nperform longitudinal classification experiments on three datasets spanning\nbetween 6 and 19 years, and involving diverse tasks and types of data. By\nsplitting the longitudinal datasets into years, we perform a comprehensive set\nof experiments by training and testing across data that are different numbers\nof years apart from each other, both in the past and in the future. This\nenables a gradual investigation into the impact of the temporal gap between\ntraining and test sets on the classification performance, as well as measuring\nthe extent of the persistence over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alkhalifa_R/0/1/0/all/0/1\">Rabab Alkhalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochkina_E/0/1/0/all/0/1\">Elena Kochkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaVAE: Exploring Adaptive GPT-2s in Variational Auto-Encoders for Language Modeling. (arXiv:2205.05862v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05862","description":"<p>Variational Auto-Encoder (VAE) has become the de-facto learning paradigm in\nachieving representation learning and generation for natural language at the\nsame time. Nevertheless, existing VAE-based language models either employ\nelementary RNNs, which is not powerful to handle complex works in the\nmulti-task situation, or fine-tunes two pre-trained language models (PLMs) for\nany downstream task, which is a huge drain on resources. In this paper, we\npropose the first VAE framework empowered with adaptive GPT-2s (AdaVAE).\nDifferent from existing systems, we unify both the encoder\\&amp;decoder of the VAE\nmodel using GPT-2s with adaptive parameter-efficient components, and further\nintroduce Latent Attention operation to better construct latent space from\ntransformer models. Experiments from multiple dimensions validate that AdaVAE\nis competent to effectively organize language in three related tasks (language\nmodeling, representation modeling and guided text generation) even with less\nthan $15\\%$ activated parameters in training. Our code is available at\n\\url{https://github.com/ImKeTT/AdaVAE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_H/0/1/0/all/0/1\">Haoqin Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhongliang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinshuai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LeRaC: Learning Rate Curriculum. (arXiv:2205.09180v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.09180","description":"<p>Most curriculum learning methods require an approach to sort the data samples\nby difficulty, which is often cumbersome to perform. In this work, we propose a\nnovel curriculum learning approach termed Learning Rate Curriculum (LeRaC),\nwhich leverages the use of a different learning rate for each layer of a neural\nnetwork to create a data-free curriculum during the initial training epochs.\nMore specifically, LeRaC assigns higher learning rates to neural layers closer\nto the input, gradually decreasing the learning rates as the layers are placed\nfarther away from the input. The learning rates increase at various paces\nduring the first training iterations, until they all reach the same value. From\nthis point on, the neural model is trained as usual. This creates a model-level\ncurriculum learning strategy that does not require sorting the examples by\ndifficulty and is compatible with any neural network, generating higher\nperformance levels regardless of the architecture. We conduct comprehensive\nexperiments on eight datasets from the computer vision (CIFAR-10, CIFAR-100,\nTiny ImageNet), language (BoolQ, QNLI, RTE) and audio (ESC-50, CREMA-D)\ndomains, considering various convolutional (ResNet-18, Wide-ResNet-50,\nDenseNet-121), recurrent (LSTM) and transformer (CvT, BERT, SepTr)\narchitectures, comparing our approach with the conventional training regime.\nMoreover, we also compare with Curriculum by Smoothing (CBS), a\nstate-of-the-art data-free curriculum learning approach. Unlike CBS, our\nperformance improvements over the standard training regime are consistent\nacross all datasets and models. Furthermore, we significantly surpass CBS in\nterms of training time (there is no additional cost over the standard training\nregime for LeRaC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Croitoru_F/0/1/0/all/0/1\">Florinel-Alin Croitoru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Entity Linking with Entity Representation by Multiple Embeddings. (arXiv:2205.10498v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10498","description":"<p>We propose a simple and practical method for named entity linking (NEL),\nbased on entity representation by multiple embeddings. To explore this method,\nand to review its dependency on parameters, we measure its performance on\nNamesakes, a highly challenging dataset of ambiguously named entities. Our\nobservations suggest that the minimal number of mentions required to create a\nknowledge base (KB) entity is very important for NEL performance. The number of\nembeddings is less important and can be kept small, within as few as 10 or\nless. We show that our representations of KB entities can be adjusted using\nonly KB data, and the adjustment can improve NEL performance. We also compare\nNEL performance of embeddings obtained from tuning language model on diverse\nnews texts as opposed to tuning on more uniform texts from public datasets\nXSum, CNN / Daily Mail. We found that tuning on diverse news provides better\nembeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasilyev_O/0/1/0/all/0/1\">Oleg Vasilyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dauenhauer_A/0/1/0/all/0/1\">Alex Dauenhauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharnidharka_V/0/1/0/all/0/1\">Vedant Dharnidharka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohannon_J/0/1/0/all/0/1\">John Bohannon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Domain Sign Language Translation Learned from Online Video. (arXiv:2205.12870v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12870","description":"<p>Existing work on sign language translation - that is, translation from sign\nlanguage videos into sentences in a written language - has focused mainly on\n(1) data collected in a controlled environment or (2) data in a specific\ndomain, which limits the applicability to real-world settings. In this paper,\nwe introduce OpenASL, a large-scale American Sign Language (ASL) - English\ndataset collected from online video sites (e.g., YouTube). OpenASL contains 288\nhours of ASL videos in multiple domains from over 200 signers and is the\nlargest publicly available ASL translation dataset to date. To tackle the\nchallenges of sign language translation in realistic settings and without\nglosses, we propose a set of techniques including sign search as a pretext task\nfor pre-training and fusion of mouthing and handshape features. The proposed\ntechniques produce consistent and large improvements in translation quality,\nover baseline models based on prior work. Our data and code are publicly\navailable at https://github.com/chevalierNoir/OpenASL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brentari_D/0/1/0/all/0/1\">Diane Brentari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1\">Greg Shakhnarovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation. (arXiv:2205.16001v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.16001","description":"<p>A good automatic evaluation metric for language generation ideally correlates\nhighly with human judgements of text quality. Yet, there is a dearth of such\nmetrics, which inhibits the rapid and efficient progress of language\ngenerators. One exception is the recently proposed Mauve. In theory, Mauve\nmeasures an information-theoretic divergence between two probability\ndistributions over strings: one representing the language generator under\nevaluation; the other representing the true natural language distribution.\nMauve's authors argue that its success comes from the qualitative properties of\ntheir proposed divergence. Yet in practice, as this divergence is uncomputable,\nMauve approximates it by measuring the divergence between multinomial\ndistributions over clusters instead, where cluster assignments are attained by\ngrouping strings based on a pre-trained language model's embeddings. As we\nshow, however, this is not a tight approximation -- in either theory or\npractice. This begs the question: why does Mauve work so well? In this work, we\nshow that Mauve was right for the wrong reasons, and that its newly proposed\ndivergence is not necessary for its high performance. In fact, classical\ndivergences paired with its proposed cluster-based approximation may actually\nserve as better evaluation metrics. We finish the paper with a probing\nanalysis; this analysis leads us to conclude that -- by encoding syntactic- and\ncoherence-level features of text, while ignoring surface-level features -- such\ncluster-based substitutes to string distributions may simply be better for\nevaluating state-of-the-art language generators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models (Mostly) Know What They Know. (arXiv:2207.05221v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.05221","description":"<p>We study whether language models can evaluate the validity of their own\nclaims and predict which questions they will be able to answer correctly. We\nfirst show that larger models are well-calibrated on diverse multiple choice\nand true/false questions when they are provided in the right format. Thus we\ncan approach self-evaluation on open-ended sampling tasks by asking models to\nfirst propose answers, and then to evaluate the probability \"P(True)\" that\ntheir answers are correct. We find encouraging performance, calibration, and\nscaling for P(True) on a diverse array of tasks. Performance at self-evaluation\nfurther improves when we allow models to consider many of their own samples\nbefore predicting the validity of one specific possibility. Next, we\ninvestigate whether models can be trained to predict \"P(IK)\", the probability\nthat \"I know\" the answer to a question, without reference to any particular\nproposed answer. Models perform well at predicting P(IK) and partially\ngeneralize across tasks, though they struggle with calibration of P(IK) on new\ntasks. The predicted P(IK) probabilities also increase appropriately in the\npresence of relevant source materials in the context, and in the presence of\nhints towards the solution of mathematical word problems. We hope these\nobservations lay the groundwork for training more honest models, and for\ninvestigating how honesty generalizes to cases where models are trained on\nobjectives other than the imitation of human writing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conerly_T/0/1/0/all/0/1\">Tom Conerly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1\">Amanda Askell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henighan_T/0/1/0/all/0/1\">Tom Henighan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1\">Nicholas Schiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatfield_Dodds_Z/0/1/0/all/0/1\">Zac Hatfield-Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1\">Nova DasSarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Johnson_E/0/1/0/all/0/1\">Eli Tran-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnston_S/0/1/0/all/0/1\">Scott Johnston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Showk_S/0/1/0/all/0/1\">Sheer El-Showk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_A/0/1/0/all/0/1\">Andy Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhage_N/0/1/0/all/0/1\">Nelson Elhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hume_T/0/1/0/all/0/1\">Tristan Hume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anna Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuntao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Sam Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1\">Deep Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1\">Danny Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_J/0/1/0/all/0/1\">Josh Jacobson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kernion_J/0/1/0/all/0/1\">Jackson Kernion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1\">Shauna Kravec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovitt_L/0/1/0/all/0/1\">Liane Lovitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1\">Kamal Ndousse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsson_C/0/1/0/all/0/1\">Catherine Olsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ringer_S/0/1/0/all/0/1\">Sam Ringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amodei_D/0/1/0/all/0/1\">Dario Amodei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1\">Tom Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jack Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_B/0/1/0/all/0/1\">Ben Mann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1\">Sam McCandlish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olah_C/0/1/0/all/0/1\">Chris Olah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSLAT: Open Set Label Attention Transformer for Medical Entity Retrieval and Span Extraction. (arXiv:2207.05817v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.05817","description":"<p>Medical entity span extraction and linking are critical steps for many\nhealthcare NLP tasks. Most existing entity extraction methods either have a\nfixed vocabulary of medical entities or require span annotations. In this\npaper, we propose a method for linking an open set of entities that does not\nrequire any span annotations. Our method, Open Set Label Attention Transformer\n(OSLAT), uses the label-attention mechanism to learn candidate-entity\ncontextualized text representations. We find that OSLAT can not only link\nentities but is also able to implicitly learn spans associated with entities.\nWe evaluate OSLAT on two tasks: (1) span extraction trained without explicit\nspan annotations, and (2) entity linking trained without span-level annotation.\nWe test the generalizability of our method by training two separate models on\ntwo datasets with low entity overlap and comparing cross-dataset performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valmianski_I/0/1/0/all/0/1\">Ilya Valmianski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Li Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amatriain_X/0/1/0/all/0/1\">Xavier Amatriain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1\">Anitha Kannan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing for Systems Engineering: Automatic Generation of Systems Modelling Language Diagrams. (arXiv:2208.05008v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.05008","description":"<p>The design of complex engineering systems is an often long and articulated\nprocess that highly relies on engineers' expertise and professional judgment.\nAs such, the typical pitfalls of activities involving the human factor often\nmanifest themselves in terms of lack of completeness or exhaustiveness of the\nanalysis, inconsistencies across design choices or documentation, as well as an\nimplicit degree of subjectivity. An approach is proposed to assist systems\nengineers in the automatic generation of systems diagrams from unstructured\nnatural language text. Natural Language Processing (NLP) techniques are used to\nextract entities and their relationships from textual resources (e.g.,\nspecifications, manuals, technical reports, maintenance reports) available\nwithin an organisation, and convert them into Systems Modelling Language\n(SysML) diagrams, with particular focus on structure and requirement diagrams.\nThe intention is to provide the users with a more standardised, comprehensive\nand automated starting point onto which subsequently refine and adapt the\ndiagrams according to their needs. The proposed approach is flexible and\nopen-domain. It consists of six steps which leverage open-access tools, and it\nleads to an automatic generation of SysML diagrams without intermediate\nmodelling requirement, but through the specification of a set of parameters by\nthe user. The applicability and benefits of the proposed approach are shown\nthrough six case studies having different textual sources as inputs, and\nbenchmarked against manually defined diagram elements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Shaohong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarinci_A/0/1/0/all/0/1\">Andrea Scarinci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicirello_A/0/1/0/all/0/1\">Alice Cicirello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Embeddings for Text. (arXiv:2208.08386v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.08386","description":"<p>We propose a new kind of embedding for natural language text that deeply\nrepresents semantic meaning. Standard text embeddings use the outputs from\nhidden layers of a pretrained language model. In our method, we let a language\nmodel learn from the text and then literally pick its brain, taking the actual\nweights of the model's neurons to generate a vector. We call this\nrepresentation of the text a neural embedding. We confirm the ability of this\nrepresentation to reflect semantics of the text by an analysis of its behavior\non several datasets, and by a comparison of neural embedding with state of the\nart sentence embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasilyev_O/0/1/0/all/0/1\">Oleg Vasilyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohannon_J/0/1/0/all/0/1\">John Bohannon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Grounding of Inter-lingual Word-Embeddings. (arXiv:2209.03714v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.03714","description":"<p>Visual grounding of Language aims at enriching textual representations of\nlanguage with multiple sources of visual knowledge such as images and videos.\nAlthough visual grounding is an area of intense research, inter-lingual aspects\nof visual grounding have not received much attention. The present study\ninvestigates the inter-lingual visual grounding of word embeddings. We propose\nan implicit alignment technique between the two spaces of vision and language\nin which inter-lingual textual information interacts in order to enrich\npre-trained textual word embeddings. We focus on three languages in our\nexperiments, namely, English, Arabic, and German. We obtained visually grounded\nvector representations for these languages and studied whether visual grounding\non one or multiple languages improved the performance of embeddings on word\nsimilarity and categorization benchmarks. Our experiments suggest that\ninter-lingual knowledge improves the performance of grounded embeddings in\nsimilar languages such as German and English. However, inter-lingual grounding\nof German or English with Arabic led to a slight degradation in performance on\nword similarity benchmarks. On the other hand, we observed an opposite trend on\ncategorization benchmarks where Arabic had the most improvement on English. In\nthe discussion section, several reasons for those findings are laid out. We\nhope that our experiments provide a baseline for further research on\ninter-lingual visual grounding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_W/0/1/0/all/0/1\">Wafaa Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahmohammadi_H/0/1/0/all/0/1\">Hassan Shahmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1\">Hendrik P. A. Lensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baayen_R/0/1/0/all/0/1\">R. Harald Baayen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Language Model Prompting in Support of Semi-autonomous Task Learning. (arXiv:2209.07636v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2209.07636","description":"<p>Language models (LLMs) offer potential as a source of knowledge for agents\nthat need to acquire new task competencies within a performance environment. We\ndescribe efforts toward a novel agent capability that can construct cues (or\n\"prompts\") that result in useful LLM responses for an agent learning a new\ntask. Importantly, responses must not only be \"reasonable\" (a measure used\ncommonly in research on knowledge extraction from LLMs) but also specific to\nthe agent's task context and in a form that the agent can interpret given its\nnative language capacities. We summarize a series of empirical investigations\nof prompting strategies and evaluate responses against the goals of targeted\nand actionable responses for task learning. Our results demonstrate that\nactionable task knowledge can be obtained from LLMs in support of online agent\ntask learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_J/0/1/0/all/0/1\">James R. Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wray_R/0/1/0/all/0/1\">Robert E. Wray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindes_P/0/1/0/all/0/1\">Peter Lindes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laird_J/0/1/0/all/0/1\">John E. Laird</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-contextualizing Fairness in NLP: The Case of India. (arXiv:2209.12226v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.12226","description":"<p>Recent research has revealed undesirable biases in NLP data and models.\nHowever, these efforts focus on social disparities in West, and are not\ndirectly portable to other geo-cultural contexts. In this paper, we focus on\nNLP fair-ness in the context of India. We start with a brief account of the\nprominent axes of social disparities in India. We build resources for fairness\nevaluation in the Indian context and use them to demonstrate prediction biases\nalong some of the axes. We then delve deeper into social stereotypes for Region\nandReligion, demonstrating its prevalence in corpora and models. Finally, we\noutline a holistic research agenda to re-contextualize NLP fairness research\nfor the Indian context, ac-counting for Indian societal context, bridging\ntechnological gaps in NLP capabilities and re-sources, and adapting to Indian\ncultural values. While we focus on India, this framework can be generalized to\nother geo-cultural contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1\">Shaily Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1\">Shachi Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Surprising Computational Power of Nondeterministic Stack RNNs. (arXiv:2210.01343v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01343","description":"<p>Traditional recurrent neural networks (RNNs) have a fixed, finite number of\nmemory cells. In theory (assuming bounded range and precision), this limits\ntheir formal language recognition power to regular languages, and in practice,\nRNNs have been shown to be unable to learn many context-free languages (CFLs).\nIn order to expand the class of languages RNNs recognize, prior work has\naugmented RNNs with a nondeterministic stack data structure, putting them on\npar with pushdown automata and increasing their language recognition power to\nCFLs. Nondeterminism is needed for recognizing all CFLs (not just deterministic\nCFLs), but in this paper, we show that nondeterminism and the neural controller\ninteract to produce two more unexpected abilities. First, the nondeterministic\nstack RNN can recognize not only CFLs, but also many non-context-free\nlanguages. Second, it can recognize languages with much larger alphabet sizes\nthan one might expect given the size of its stack alphabet. Finally, to\nincrease the information capacity in the stack and allow it to solve more\ncomplicated tasks with large alphabet sizes, we propose a new version of the\nnondeterministic stack that simulates stacks of vectors rather than discrete\nsymbols. We demonstrate perplexity improvements with this new model on the Penn\nTreebank language modeling benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DuSell_B/0/1/0/all/0/1\">Brian DuSell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Sentiment Analysis By Emotion Lexicon Approach on Vietnamese Texts. (arXiv:2210.02063v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02063","description":"<p>The sentiment analysis task has various applications in practice. In the\nsentiment analysis task, words and phrases that represent positive and negative\nemotions are important. Finding out the words that represent the emotion from\nthe text can improve the performance of the classification models for the\nsentiment analysis task. In this paper, we propose a methodology that combines\nthe emotion lexicon with the classification model for enhancing the accuracy of\nthe models. Our experimental results show that the emotion lexicon combined\nwith the classification model improves the performance of models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doan_A/0/1/0/all/0/1\">An Long Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ask Me Anything: A simple strategy for prompting language models. (arXiv:2210.02441v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02441","description":"<p>Large language models (LLMs) transfer well to new tasks out-of-the-box simply\ngiven a natural language prompt that demonstrates how to perform the task and\nno additional training. Prompting is a brittle process wherein small\nmodifications to the prompt can cause large variations in the model\npredictions, and therefore significant effort is dedicated towards designing a\npainstakingly \"perfect prompt\" for a task. To mitigate the high degree of\neffort involved in prompt-design, we instead ask whether producing multiple\neffective, yet imperfect, prompts and aggregating them can lead to a high\nquality prompting strategy. Our observations motivate our proposed prompting\nmethod, ASK ME ANYTHING (AMA). We first develop an understanding of the\neffective prompt formats, finding that question-answering (QA) prompts, which\nencourage open-ended generation (\"Who went to the park?\") tend to outperform\nthose that restrict the model outputs (\"John went to the park. Output True or\nFalse.\"). Our approach recursively uses the LLM itself to transform task inputs\nto the effective QA format. We apply the collected prompts to obtain several\nnoisy votes for the input's true label. We find that the prompts can have very\ndifferent accuracies and complex dependencies and thus propose to use weak\nsupervision, a procedure for combining the noisy predictions, to produce the\nfinal predictions for the inputs. We evaluate AMA across open-source model\nfamilies (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B\nparameters), demonstrating an average performance lift of 10.2% over the\nfew-shot baseline. This simple strategy enables the open-source GPT-J-6B model\nto match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular\nbenchmarks. Averaged across these tasks, the GPT-J-6B model outperforms\nfew-shot GPT3-175B. We release our code here:\nhttps://github.com/HazyResearch/ama_prompting\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Simran Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_A/0/1/0/all/0/1\">Avanika Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mayee F. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orr_L/0/1/0/all/0/1\">Laurel Orr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_N/0/1/0/all/0/1\">Neel Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_K/0/1/0/all/0/1\">Kush Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chami_I/0/1/0/all/0/1\">Ines Chami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1\">Frederic Sala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods. (arXiv:2210.07321v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07321","description":"<p>Advances in natural language generation (NLG) have resulted in machine\ngenerated text that is increasingly difficult to distinguish from human\nauthored text. Powerful open-source models are freely available, and\nuser-friendly tools democratizing access to generative models are\nproliferating. The great potential of state-of-the-art NLG systems is tempered\nby the multitude of avenues for abuse. Detection of machine generated text is a\nkey countermeasure for reducing abuse of NLG models, with significant technical\nchallenges and numerous open problems. We provide a survey that includes both\n1) an extensive analysis of threat models posed by contemporary NLG systems,\nand 2) the most complete review of machine generated text detection methods to\ndate. This survey places machine generated text within its cybersecurity and\nsocial context, and provides strong guidance for future work addressing the\nmost critical threat models, and ensuring detection systems themselves\ndemonstrate trustworthiness through fairness, robustness, and accountability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Crothers_E/0/1/0/all/0/1\">Evan Crothers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Japkowicz_N/0/1/0/all/0/1\">Nathalie Japkowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viktor_H/0/1/0/all/0/1\">Herna Viktor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpanProto: A Two-stage Span-based Prototypical Network for Few-shot Named Entity Recognition. (arXiv:2210.09049v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09049","description":"<p>Few-shot Named Entity Recognition (NER) aims to identify named entities with\nvery little annotated data. Previous methods solve this problem based on\ntoken-wise classification, which ignores the information of entity boundaries,\nand inevitably the performance is affected by the massive non-entity tokens. To\nthis end, we propose a seminal span-based prototypical network (SpanProto) that\ntackles few-shot NER via a two-stage approach, including span extraction and\nmention classification. In the span extraction stage, we transform the\nsequential tags into a global boundary matrix, enabling the model to focus on\nthe explicit boundary information. For mention classification, we leverage\nprototypical learning to capture the semantic representations for each labeled\nspan and make the model better adapt to novel-class entities. To further\nimprove the model performance, we split out the false positives generated by\nthe span extractor but not labeled in the current episode set, and then present\na margin-based loss to separate them from each prototype region. Experiments\nover multiple benchmarks demonstrate that our model outperforms strong\nbaselines by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chengcheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Ming Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots. (arXiv:2210.11060v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11060","description":"<p>This paper introduces Doc2Bot, a novel dataset for building machines that\nhelp users seek information via conversations. This is of particular interest\nfor companies and organizations that own a large number of manuals or\ninstruction books. Despite its potential, the nature of our task poses several\nchallenges: (1) documents contain various structures that hinder the ability of\nmachines to comprehend, and (2) user information needs are often\nunderspecified. Compared to prior datasets that either focus on a single\nstructural type or overlook the role of questioning to uncover user needs, the\nDoc2Bot dataset is developed to target such challenges systematically. Our\ndataset contains over 100,000 turns based on Chinese documents from five\ndomains, larger than any prior document-grounded dialog dataset for information\nseeking. We propose three tasks in Doc2Bot: (1) dialog state tracking to track\nuser intentions, (2) dialog policy learning to plan system actions and\ncontents, and (3) response generation which generates responses based on the\noutputs of the dialog policy. Baseline methods based on the latest deep\nlearning models are presented, indicating that our proposed tasks are\nchallenging and worthy of further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Haomin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yeqin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cam-Tu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning. (arXiv:2210.12587v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12587","description":"<p>Prompt tuning approaches, which learn task-specific soft prompts for a\ndownstream task conditioning on frozen pre-trained models, have attracted\ngrowing interest due to its parameter efficiency. With large language models\nand sufficient training data, prompt tuning performs comparably to full-model\ntuning. However, with limited training samples in few-shot settings, prompt\ntuning fails to match the performance of full-model fine-tuning. In this work,\nwe focus on improving the few-shot performance of prompt tuning by transferring\nknowledge from soft prompts of source tasks. Recognizing the good\ngeneralization capabilities of ensemble methods in low-data regime, we first\nexperiment and show that a simple ensemble of model predictions based on\ndifferent source prompts, outperforms existing multi-prompt knowledge transfer\napproaches such as source prompt fusion in the few-shot setting. Motivated by\nthis observation, we further investigate model ensembles and propose\nSample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the\ncontribution of each source model for each target sample separately when\nensembling source model outputs. Through this way, SESoM inherits the superior\ngeneralization of model ensemble approaches and simultaneously captures the\nsample-specific competence of each source prompt. We conduct experiments across\na diverse set of eight NLP tasks using models of different scales (T5-{base,\nlarge, XL}) and find that SESoM consistently outperforms the existing models of\nthe same as well as larger parametric scale by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choubey_P/0/1/0/all/0/1\">Prafulla Kumar Choubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SentBS: Sentence-level Beam Search for Controllable Summarization. (arXiv:2210.14502v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14502","description":"<p>A wide range of control perspectives have been explored in controllable text\ngeneration. Structure-controlled summarization is recently proposed as a useful\nand interesting research direction. However, current structure-controlling\nmethods have limited effectiveness in enforcing the desired structure. To\naddress this limitation, we propose a sentence-level beam search generation\nmethod (SentBS), where evaluation is conducted throughout the generation\nprocess to select suitable sentences for subsequent generations. We experiment\nwith different combinations of decoding methods to be used as subcomponents by\nSentBS and evaluate results on the structure-controlled dataset MReD.\nExperiments show that all explored combinations for SentBS can improve the\nagreement between the generated text and the desired structure, with the best\nmethod significantly reducing the structural discrepancies suffered by the\nexisting model, by approximately 68%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenhui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fixing Model Bugs with Natural Language Patches. (arXiv:2211.03318v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.03318","description":"<p>Current approaches for fixing systematic problems in NLP models (e.g. regex\npatches, finetuning on more data) are either brittle, or labor-intensive and\nliable to shortcuts. In contrast, humans often provide corrections to each\nother through natural language. Taking inspiration from this, we explore\nnatural language patches -- declarative statements that allow developers to\nprovide corrective feedback at the right level of abstraction, either\noverriding the model (``if a review gives 2 stars, the sentiment is negative'')\nor providing additional information the model may lack (``if something is\ndescribed as the bomb, then it is good''). We model the task of determining if\na patch applies separately from the task of integrating patch information, and\nshow that with a small amount of synthetic data, we can teach models to\neffectively use real patches on real data -- 1 to 7 patches improve accuracy by\n~1-4 accuracy points on different slices of a sentiment analysis dataset, and\nF1 by 7 points on a relation extraction dataset. Finally, we show that\nfinetuning on as many as 100 labeled examples may be needed to match the\nperformance of a small set of language patches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murty_S/0/1/0/all/0/1\">Shikhar Murty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundberg_S/0/1/0/all/0/1\">Scott Lundberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marco Tulio Ribeiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiaASQ : A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis. (arXiv:2211.05705v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05705","description":"<p>The rapid development of aspect-based sentiment analysis (ABSA) within recent\ndecades shows great potential for real-world society. The current ABSA works,\nhowever, are mostly limited to the scenario of a single text piece, leaving the\nstudy in dialogue contexts unexplored. In this work, we introduce a novel task\nof conversational aspect-based sentiment quadruple analysis, namely DiaASQ,\naiming to detect the sentiment quadruple of\n\\emph{target-aspect-opinion-sentiment} in a dialogue. DiaASQ bridges the gap\nbetween fine-grained sentiment analysis and conversational opinion mining. We\nmanually construct a large-scale high-quality DiaASQ dataset in both Chinese\nand English languages. We deliberately develop a neural model to benchmark the\ntask, which advances in effectively performing end-to-end quadruple prediction,\nand manages to incorporate rich dialogue-specific and discourse feature\nrepresentations for better cross-utterance quadruple extraction. We finally\npoint out several potential future works to facilitate the follow-up research\nof this new task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinsong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengqiong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lizi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-game Toxic Language Detection: Shared Task and Attention Residuals. (arXiv:2211.05995v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05995","description":"<p>In-game toxic language becomes the hot potato in the gaming industry and\ncommunity. There have been several online game toxicity analysis frameworks and\nmodels proposed. However, it is still challenging to detect toxicity due to the\nnature of in-game chat, which has extremely short length. In this paper, we\ndescribe how the in-game toxic language shared task has been established using\nthe real-world in-game chat data. In addition, we propose and introduce the\nmodel/framework for toxic language token tagging (slot filling) from the\nin-game chat. The data and code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yuanzhe Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weixuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_F/0/1/0/all/0/1\">Feiqi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collecting Interactive Multi-modal Datasets for Grounded Language Understanding. (arXiv:2211.06552v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.06552","description":"<p>Human intelligence can remarkably adapt quickly to new tasks and\nenvironments. Starting from a very young age, humans acquire new skills and\nlearn how to solve new tasks either by imitating the behavior of others or by\nfollowing provided natural language instructions. To facilitate research which\ncan enable similar capabilities in machines, we made the following\ncontributions (1) formalized the collaborative embodied agent using natural\nlanguage task; (2) developed a tool for extensive and scalable data collection;\nand (3) collected the first dataset for interactive grounded language\nunderstanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1\">Shrestha Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabzadeh_N/0/1/0/all/0/1\">Negar Arabzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teruel_M/0/1/0/all/0/1\">Milagro Teruel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zholus_A/0/1/0/all/0/1\">Artem Zholus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrynnik_A/0/1/0/all/0/1\">Alexey Skrynnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail Burtsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinet_K/0/1/0/all/0/1\">Kavya Srinet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1\">Aleksandr Panov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities. (arXiv:2211.06679v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.06679","description":"<p>In this work, we present a conceptually simple and effective method to train\na strong bilingual/multilingual multimodal representation model. Starting from\nthe pre-trained multimodal representation model CLIP released by OpenAI, we\naltered its text encoder with a pre-trained multilingual text encoder XLM-R,\nand aligned both languages and image representations by a two-stage training\nschema consisting of teacher learning and contrastive learning. We validate our\nmethod through evaluations of a wide range of tasks. We set new\nstate-of-the-art performances on a bunch of tasks including ImageNet-CN,\nFlicker30k-CN, COCO-CN and XTD. Further, we obtain very close performances with\nCLIP on almost all tasks, suggesting that one can simply alter the text encoder\nin CLIP for extended capabilities such as multilingual understanding. Our\nmodels and code are available at https://github.com/FlagAI-Open/FlagAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhongzhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo-Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fulong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qinghong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Ledell Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Parsing as Tagging. (arXiv:2211.07344v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07344","description":"<p>There have been many proposals to reduce constituency parsing to tagging in\nthe literature. To better understand what these approaches have in common, we\ncast several existing proposals into a unifying pipeline consisting of three\nsteps: linearization, learning, and decoding. In particular, we show how to\nreduce tetratagging, a state-of-the-art constituency tagger, to shift--reduce\nparsing by performing a right-corner transformation on the grammar and making a\nspecific independence assumption. Furthermore, we empirically evaluate our\ntaxonomy of tagging pipelines with different choices of linearizers, learners,\nand decoders. Based on the results in English and a set of 8 typologically\ndiverse languages, we conclude that the linearization of the derivation tree\nand its alignment with the input sequence is the most critical factor in\nachieving accurate taggers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Afra Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward expanding the scope of radiology report summarization to multiple anatomies and modalities. (arXiv:2211.08584v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08584","description":"<p>Radiology report summarization is a growing area of research. Given the\nFindings and/or Background sections of a radiology report, the goal is to\ngenerate a summary (called an Impression section) that highlights the key\nobservations and conclusions of the radiology study. Recent efforts have\nreleased systems that achieve promising performance as measured by widely used\nsummarization metrics such as BLEU and ROUGE. However, the research area of\nradiology report summarization currently faces important limitations. First,\nmost of the results are reported on private datasets. This limitation prevents\nthe ability to reproduce results and fairly compare different systems and\nsolutions. Secondly, to the best of our knowledge, most research is carried out\non chest X-rays. Sometimes, studies even omit to mention the concerned modality\nand anatomy in the radiology reports used for their experiments. To palliate\nthese limitations, we propose a new dataset of six different modalities and\nanatomies based on the MIMIC-III database. We further release our results and\nthe data splits used to carry out our experiments. Finally, we propose a simple\nreport summarization system that outperforms the previous replicable research\non the existing dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delbrouck_J/0/1/0/all/0/1\">Jean-Benoit Delbrouck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Maya Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and Prefix-Tuning. (arXiv:2211.09783v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.09783","description":"<p>The diverse demands of different summarization tasks and their high\nannotation costs are driving a need for few-shot summarization. However,\ndespite the emergence of many summarization tasks and datasets, the current\ntraining paradigm for few-shot summarization systems ignores potentially\nshareable knowledge in heterogeneous datasets. To this end, we propose\n\\textsc{UniSumm}, a unified few-shot summarization model pre-trained with\nmultiple summarization tasks and can be prefix-tuned to excel at any few-shot\nsummarization datasets. Meanwhile, to better evaluate few-shot summarization\nsystems, under the principles of diversity and robustness, we assemble and\npublicize a new benchmark \\textsc{SummZoo}. It consists of $8$ diverse\nsummarization tasks with multiple sets of few-shot samples for each task,\ncovering both monologue and dialogue domains. Experimental results and ablation\nstudies show that \\textsc{UniSumm} outperforms strong baseline systems by a\nlarge margin across all tasks in \\textsc{SummZoo} under both automatic and\nhuman evaluations. We release our code and benchmark at\n\\url{https://github.com/microsoft/UniSumm}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}