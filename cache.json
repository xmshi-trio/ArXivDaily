{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-10-31T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"COMET-QE and Active Learning for Low-Resource Machine Translation. (arXiv:2210.15696v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15696","description":"<p>Active learning aims to deliver maximum benefit when resources are scarce. We\nuse COMET-QE, a reference-free evaluation metric, to select sentences for\nlow-resource neural machine translation. Using Swahili, Kinyarwanda and Spanish\nfor our experiments, we show that COMET-QE significantly outperforms two\nvariants of Round Trip Translation Likelihood (RTTL) and random sentence\nselection by up to 5 BLEU points for 20k sentences selected by Active Learning\non a 30k baseline. This suggests that COMET-QE is a powerful tool for sentence\nselection in the very low-resource limit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chimoto_E/0/1/0/all/0/1\">Everlyn Asiko Chimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassett_B/0/1/0/all/0/1\">Bruce A. Bassett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulating realistic speech overlaps improves multi-talker ASR. (arXiv:2210.15715v1 [eess.AS])","link":"http://arxiv.org/abs/2210.15715","description":"<p>Multi-talker automatic speech recognition (ASR) has been studied to generate\ntranscriptions of natural conversation including overlapping speech of multiple\nspeakers. Due to the difficulty in acquiring real conversation data with\nhigh-quality human transcriptions, a na\\\"ive simulation of multi-talker speech\nby randomly mixing multiple utterances was conventionally used for model\ntraining. In this work, we propose an improved technique to simulate\nmulti-talker overlapping speech with realistic speech overlaps, where an\narbitrary pattern of speech overlaps is represented by a sequence of discrete\ntokens. With this representation, speech overlapping patterns can be learned\nfrom real conversations based on a statistical language model, such as N-gram,\nwhich can be then used to generate multi-talker speech for training. In our\nexperiments, multi-talker ASR models trained with the proposed method show\nconsistent improvement on the word error rates across multiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sivasankaran_S/0/1/0/all/0/1\">Sunit Sivasankaran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation. (arXiv:2210.15718v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15718","description":"<p>Large Language Models (LLMs) have shown impressive results on a variety of\ntext understanding tasks. Search queries though pose a unique challenge, given\ntheir short-length and lack of nuance or context. Complicated feature\nengineering efforts do not always lead to downstream improvements as their\nperformance benefits may be offset by increased complexity of knowledge\ndistillation. Thus, in this paper we make the following contributions: (1) We\ndemonstrate that Retrieval Augmentation of queries provides LLMs with valuable\nadditional context enabling improved understanding. While Retrieval\nAugmentation typically increases latency of LMs (thus hurting distillation\nefficacy), (2) we provide a practical and effective way of distilling Retrieval\nAugmentation LLMs. Specifically, we use a novel two-stage distillation approach\nthat allows us to carry over the gains of retrieval augmentation, without\nsuffering the increased compute typically associated with it. (3) We\ndemonstrate the benefits of the proposed approach (QUILL) on a billion-scale,\nreal-world query understanding system resulting in huge gains. Via extensive\nexperiments, including on public benchmarks, we believe this work offers a\nrecipe for practical use of retrieval-augmented query understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_K/0/1/0/all/0/1\">Krishna Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_K/0/1/0/all/0/1\">Karthik Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samanta_A/0/1/0/all/0/1\">Anupam Samanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lingrui Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertelli_L/0/1/0/all/0/1\">Luca Bertelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Mike Bendersky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token-level Sequence Labeling for Spoken Language Understanding using Compositional End-to-End Models. (arXiv:2210.15734v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15734","description":"<p>End-to-end spoken language understanding (SLU) systems are gaining popularity\nover cascaded approaches due to their simplicity and ability to avoid error\npropagation. However, these systems model sequence labeling as a sequence\nprediction task causing a divergence from its well-established token-level\ntagging formulation. We build compositional end-to-end SLU systems that\nexplicitly separate the added complexity of recognizing spoken mentions in SLU\nfrom the NLU task of sequence labeling. By relying on intermediate decoders\ntrained for ASR, our end-to-end systems transform the input modality from\nspeech to token-level representations that can be used in the traditional\nsequence labeling framework. This composition of ASR and NLU formulations in\nour end-to-end SLU system offers direct compatibility with pre-trained ASR and\nNLU systems, allows performance monitoring of individual components and enables\nthe use of globally normalized losses like CRF, making them attractive in\npractical scenarios. Our models outperform both cascaded and direct end-to-end\nmodels on a labeling task of named entity recognition across SLU benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised language learning from raw audio: Lessons from the Zero Resource Speech Challenge. (arXiv:2210.15759v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15759","description":"<p>Recent progress in self-supervised or unsupervised machine learning has\nopened the possibility of building a full speech processing system from raw\naudio without using any textual representations or expert labels such as\nphonemes, dictionaries or parse trees. The contribution of the Zero Resource\nSpeech Challenge series since 2015 has been to break down this long-term\nobjective into four well-defined tasks -- Acoustic Unit Discovery, Spoken Term\nDiscovery, Discrete Resynthesis, and Spoken Language Modeling -- and introduce\nassociated metrics and benchmarks enabling model comparison and cumulative\nprogress. We present an overview of the six editions of this challenge series\nsince 2015, discuss the lessons learned, and outline the areas which need more\nwork or give puzzling results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dunbar_E/0/1/0/all/0/1\">Ewan Dunbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilakis_N/0/1/0/all/0/1\">Nicolas Hamilakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nearest Neighbor Language Models for Stylistic Controllable Generation. (arXiv:2210.15762v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15762","description":"<p>Recent language modeling performance has been greatly improved by the use of\nexternal memory. This memory encodes the context so that similar contexts can\nbe recalled during decoding. This similarity depends on how the model learns to\nencode context, which can be altered to include other attributes, such as\nstyle. We construct and evaluate an architecture for this purpose, using\ncorpora annotated for politeness, formality, and toxicity. Through extensive\nexperiments and human evaluation we demonstrate the potential of our method to\ngenerate text while controlling style. We find that style-specific datastores\nimprove generation performance, though results vary greatly across styles, and\nthe effect of pretraining data and specific styles should be explored in future\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trotta_S/0/1/0/all/0/1\">Severino Trotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1\">Lucie Flek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_C/0/1/0/all/0/1\">Charles Welch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating context-invariance in unsupervised speech representations. (arXiv:2210.15775v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15775","description":"<p>Unsupervised speech representations have taken off, with benchmarks (SUPERB,\nZeroSpeech) demonstrating major progress on semi-supervised speech recognition,\nspeech synthesis, and speech-only language modelling. Inspiration comes from\nthe promise of ``discovering the phonemes'' of a language or a similar\nlow-bitrate encoding. However, one of the critical properties of phoneme\ntranscriptions is context-invariance: the phonetic context of a speech sound\ncan have massive influence on the way it is pronounced, while the text remains\nstable. This is what allows tokens of the same word to have the same\ntranscriptions -- key to language understanding. Current benchmarks do not\nmeasure context-invariance. We develop a new version of the ZeroSpeech ABX\nbenchmark that measures context-invariance, and apply it to recent\nself-supervised representations. We demonstrate that the context-independence\nof representations is predictive of the stability of word-level\nrepresentations. We suggest research concentrate on improving\ncontext-independence of self-supervised and unsupervised representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hallap_M/0/1/0/all/0/1\">Mark Hallap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunbar_E/0/1/0/all/0/1\">Ewan Dunbar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforced Question Rewriting for Conversational Question Answering. (arXiv:2210.15777v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15777","description":"<p>Conversational Question Answering (CQA) aims to answer questions contained\nwithin dialogues, which are not easily interpretable without context.\nDeveloping a model to rewrite conversational questions into self-contained ones\nis an emerging solution in industry settings as it allows using existing\nsingle-turn QA systems to avoid training a CQA model from scratch. Previous\nwork trains rewriting models using human rewrites as supervision. However, such\nobjectives are disconnected with QA models and therefore more human-like\nrewrites do not guarantee better QA performance. In this paper we propose using\nQA feedback to supervise the rewriting model with reinforcement learning.\nExperiments show that our approach can effectively improve QA performance over\nbaselines for both extractive and retrieval QA. Furthermore, human evaluation\nshows that our method can generate more accurate and detailed rewrites when\ncompared to human annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_A/0/1/0/all/0/1\">Anjie Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetahu_B/0/1/0/all/0/1\">Besnik Fetahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rokhlenko_O/0/1/0/all/0/1\">Oleg Rokhlenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malmasi_S/0/1/0/all/0/1\">Shervin Malmasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AmberNet: A Compact End-to-End Model for Spoken Language Identification. (arXiv:2210.15781v1 [eess.AS])","link":"http://arxiv.org/abs/2210.15781","description":"<p>We present AmberNet, a compact end-to-end neural network for Spoken Language\nIdentification. AmberNet consists of 1D depth-wise separable convolutions and\nSqueeze-and-Excitation layers with global context, followed by statistics\npooling and linear layers. AmberNet achieves performance similar to\nstate-of-the-art(SOTA) models on VoxLingua107 dataset, while being 10x smaller.\nAmberNet can be adapted to unseen languages and new acoustic conditions with\nsimple finetuning. It attains SOTA accuracy of 75.8% on FLEURS benchmark. We\nshow the model is easily scalable to achieve a better trade-off between\naccuracy and speed. We further inspect the model's sensitivity to input length\nand show that AmberNet performs well even on short utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jia_F/0/1/0/all/0/1\">Fei Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koluguri_N/0/1/0/all/0/1\">Nithin Rao Koluguri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balam_J/0/1/0/all/0/1\">Jagadeesh Balam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Label Correlations in a Multi-label Setting: A Case Study in Emotion. (arXiv:2210.15842v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15842","description":"<p>Detecting emotions expressed in text has become critical to a range of\nfields. In this work, we investigate ways to exploit label correlations in\nmulti-label emotion recognition models to improve emotion detection. First, we\ndevelop two modeling approaches to the problem in order to capture word\nassociations of the emotion words themselves, by either including the emotions\nin the input, or by leveraging Masked Language Modeling (MLM). Second, we\nintegrate pairwise constraints of emotion representations as regularization\nterms alongside the classification loss of the models. We split these terms\ninto two categories, local and global. The former dynamically change based on\nthe gold labels, while the latter remain static during training. We demonstrate\nstate-of-the-art performance across Spanish, English, and Arabic in SemEval\n2018 Task 1 E-c using monolingual BERT-based models. On top of better\nperformance, we also demonstrate improved robustness. Code is available at\nhttps://github.com/gchochla/Demux-MEmo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chochlakis_G/0/1/0/all/0/1\">Georgios Chochlakis</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_G/0/1/0/all/0/1\">Gireesh Mahajan</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Baruah_S/0/1/0/all/0/1\">Sabyasachee Baruah</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_K/0/1/0/all/0/1\">Keith Burghardt</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a> (1 and 2) ((1) Signal Analysis and Interpretation Lab, University of Southern California, (2) Information Science Institute, University of Southern California, (3) Microsoft Cognitive Services)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction. (arXiv:2210.15843v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15843","description":"<p>Recently, prompt-tuning has attracted growing interests in event argument\nextraction (EAE). However, the existing prompt-tuning methods have not achieved\nsatisfactory performance due to the lack of consideration of entity\ninformation. In this paper, we propose a bi-directional iterative prompt-tuning\nmethod for EAE, where the EAE task is treated as a cloze-style task to take\nfull advantage of entity information and pre-trained language models (PLMs).\nFurthermore, our method explores event argument interactions by introducing the\nargument roles of contextual entities into prompt construction. Since template\nand verbalizer are two crucial components in a cloze-style prompt, we propose\nto utilize the role label semantic knowledge to construct a semantic verbalizer\nand design three kinds of templates for the EAE task. Experiments on the ACE\n2005 English dataset with standard and low-resource settings show that the\nproposed method significantly outperforms the peer state-of-the-art methods.\nOur code is available at https://github.com/HustMinsLab/BIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Lu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1\">Yijun Mo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Zero-Shot Multilingual Translation with Universal Representations and Cross-Mappings. (arXiv:2210.15851v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15851","description":"<p>The many-to-many multilingual neural machine translation can translate\nbetween language pairs unseen during training, i.e., zero-shot translation.\nImproving zero-shot translation requires the model to learn universal\nrepresentations and cross-mapping relationships to transfer the knowledge\nlearned on the supervised directions to the zero-shot directions. In this work,\nwe propose the state mover's distance based on the optimal theory to model the\ndifference of the representations output by the encoder. Then, we bridge the\ngap between the semantic-equivalent representations of different languages at\nthe token level by minimizing the proposed distance to learn universal\nrepresentations. Besides, we propose an agreement-based training scheme, which\ncan help the model make consistent predictions based on the semantic-equivalent\nsentences to learn universal cross-mapping relationships for all translation\ndirections. The experimental results on diverse multilingual datasets show that\nour method can improve consistently compared with the baseline system and other\ncontrast methods. The analysis proves that our method can better align the\nsemantic space and improve the prediction consistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuhao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You can't pick your neighbors, or can you? When and how to rely on retrieval in the $k$NN-LM. (arXiv:2210.15859v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15859","description":"<p>Retrieval-enhanced language models (LMs), which condition their predictions\non text retrieved from large external datastores, have recently shown\nsignificant perplexity improvements compared to standard LMs. One such\napproach, the $k$NN-LM, interpolates any existing LM's predictions with the\noutput of a $k$-nearest neighbors model and requires no additional training. In\nthis paper, we explore the importance of lexical and semantic matching in the\ncontext of items retrieved by $k$NN-LM. We find two trends: (1) the presence of\nlarge overlapping $n$-grams between the datastore and evaluation set plays an\nimportant factor in strong performance, even when the datastore is derived from\nthe training data; and (2) the $k$NN-LM is most beneficial when retrieved items\nhave high semantic similarity with the query. Based on our analysis, we define\na new formulation of the $k$NN-LM that uses retrieval quality to assign the\ninterpolation coefficient. We empirically measure the effectiveness of our\napproach on two English language modeling datasets, Wikitext-103 and PG-19. Our\nre-formulation of the $k$NN-LM is beneficial in both cases, and leads to nearly\n4% improvement in perplexity on the Wikitext-103 test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Drozdov_A/0/1/0/all/0/1\">Andrew Drozdov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shufan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_R/0/1/0/all/0/1\">Razieh Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation of Machine Translation with Crowdworkers. (arXiv:2210.15861v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15861","description":"<p>Although a machine translation model trained with a large in-domain parallel\ncorpus achieves remarkable results, it still works poorly when no in-domain\ndata are available. This situation restricts the applicability of machine\ntranslation when the target domain's data are limited. However, there is great\ndemand for high-quality domain-specific machine translation models for many\ndomains. We propose a framework that efficiently and effectively collects\nparallel sentences in a target domain from the web with the help of\ncrowdworkers. With the collected parallel data, we can quickly adapt a machine\ntranslation model to the target domain. Our experiments show that the proposed\nmethod can collect target-domain parallel data over a few days at a reasonable\ncost. We tested it with five domains, and the domain-adapted model improved the\nBLEU scores to +19.7 by an average of +7.8 points compared to a general-purpose\ntranslation model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morishita_M/0/1/0/all/0/1\">Makoto Morishita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1\">Jun Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagata_M/0/1/0/all/0/1\">Masaaki Nagata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Adapters for Few-Shot Text-to-Speech Speaker Adaptation. (arXiv:2210.15868v1 [cs.SD])","link":"http://arxiv.org/abs/2210.15868","description":"<p>Adapting a neural text-to-speech (TTS) model to a target speaker typically\ninvolves fine-tuning most if not all of the parameters of a pretrained\nmulti-speaker backbone model. However, serving hundreds of fine-tuned neural\nTTS models is expensive as each of them requires significant footprint and\nseparate computational resources (e.g., accelerators, memory). To scale speaker\nadapted neural TTS voices to hundreds of speakers while preserving the\nnaturalness and speaker similarity, this paper proposes a parameter-efficient\nfew-shot speaker adaptation, where the backbone model is augmented with\ntrainable lightweight modules called residual adapters. This architecture\nallows the backbone model to be shared across different target speakers.\nExperimental results show that the proposed approach can achieve competitive\nnaturalness and speaker similarity compared to the full fine-tuning approaches,\nwhile requiring only $\\sim$0.1% of the backbone model parameters for each\nspeaker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morioka_N/0/1/0/all/0/1\">Nobuyuki Morioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zen_H/0/1/0/all/0/1\">Heiga Zen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nanxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifan Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"It's Not Just Hate'': A Multi-Dimensional Perspective on Detecting Harmful Speech Online. (arXiv:2210.15870v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15870","description":"<p>Well-annotated data is a prerequisite for good Natural Language Processing\nmodels. Too often, though, annotation decisions are governed by optimizing time\nor annotator agreement. We make a case for nuanced efforts in an\ninterdisciplinary setting for annotating offensive online speech. Detecting\noffensive content is rapidly becoming one of the most important real-world NLP\ntasks. However, most datasets use a single binary label, e.g., for hate or\nincivility, even though each concept is multi-faceted. This modeling choice\nseverely limits nuanced insights, but also performance. We show that a more\nfine-grained multi-label approach to predicting incivility and hateful or\nintolerant content addresses both conceptual and performance issues. We release\na novel dataset of over 40,000 tweets about immigration from the US and UK,\nannotated with six labels for different aspects of incivility and intolerance.\nOur dataset not only allows for a more nuanced understanding of harmful speech\nonline, models trained on it also outperform or match performance on benchmark\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hills_S/0/1/0/all/0/1\">Stefanie Anja Hills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossini_P/0/1/0/all/0/1\">Patricia Rossini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tromble_R/0/1/0/all/0/1\">Rebekah Tromble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tintarev_N/0/1/0/all/0/1\">Nava Tintarev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving short-video speech recognition using random utterance concatenation. (arXiv:2210.15876v1 [eess.AS])","link":"http://arxiv.org/abs/2210.15876","description":"<p>One of the limitations in end-to-end automatic speech recognition framework\nis its performance would be compromised if train-test utterance lengths are\nmismatched. In this paper, we propose a random utterance concatenation (RUC)\nmethod to alleviate train-test utterance length mismatch issue for short-video\nspeech recognition task. Specifically, we are motivated by observations our\nhuman-transcribed training utterances tend to be much shorter for short-video\nspontaneous speech (~3 seconds on average), while our test utterance generated\nfrom voice activity detection front-end is much longer (~10 seconds on\naverage). Such a mismatch can lead to sub-optimal performance. Experimentally,\nby using the proposed RUC method, the best word error rate reduction (WERR) can\nbe achieved with around three fold training data size increase as well as two\nutterance concatenation for each. In practice, the proposed method consistently\noutperforms the strong baseline models, where 3.64% average WERR is achieved on\n14 languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_H/0/1/0/all/0/1\">Haihua Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pham_V/0/1/0/all/0/1\">Van Tung Pham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khassanov_Y/0/1/0/all/0/1\">Yerbolat Khassanov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Yist Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_T/0/1/0/all/0/1\">Tao Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chong_T/0/1/0/all/0/1\">Tze Yuan Chong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yi He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Current Explainability Help Provide References in Clinical Notes to Support Humans Annotate Medical Codes?. (arXiv:2210.15882v1 [cs.LG])","link":"http://arxiv.org/abs/2210.15882","description":"<p>The medical codes prediction problem from clinical notes has received\nsubstantial interest in the NLP community, and several recent studies have\nshown the state-of-the-art (SOTA) code prediction results of full-fledged deep\nlearning-based methods. However, most previous SOTA works based on deep\nlearning are still in early stages in terms of providing textual references and\nexplanations of the predicted codes, despite the fact that this level of\nexplainability of the prediction outcomes is critical to gaining trust from\nprofessional medical coders. This raises the important question of how well\ncurrent explainability methods apply to advanced neural network models such as\ntransformers to predict correct codes and present references in clinical notes\nthat support code prediction. First, we present an explainable Read, Attend,\nand Code (xRAC) framework and assess two approaches, attention score-based\nxRAC-ATTN and model-agnostic knowledge-distillation-based xRAC-KD, through\nsimplified but thorough human-grounded evaluations with SOTA transformer-based\nmodel, RAC. We find that the supporting evidence text highlighted by xRAC-ATTN\nis of higher quality than xRAC-KD whereas xRAC-KD has potential advantages in\nproduction deployment scenarios. More importantly, we show for the first time\nthat, given the current state of explainability methodologies, using the SOTA\nmedical codes prediction system still requires the expertise and competencies\nof professional coders, even though its prediction accuracy is superior to that\nof human coders. This, we believe, is a very meaningful step toward developing\nexplainable and accurate machine learning systems for fully autonomous medical\ncode prediction from clinical notes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byung-Hak Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhongfen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganapathi_V/0/1/0/all/0/1\">Varun Ganapathi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad Responses into Good Labels. (arXiv:2210.15893v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15893","description":"<p>Deployed dialogue agents have the potential to integrate human feedback to\ncontinuously improve themselves. However, humans may not always provide\nexplicit signals when the chatbot makes mistakes during interactions. In this\nwork, we propose Juicer, a framework to make use of both binary and free-form\ntextual human feedback. It works by: (i) extending sparse binary feedback by\ntraining a satisfaction classifier to label the unlabeled data; and (ii)\ntraining a reply corrector to map the bad replies to good ones. We find that\naugmenting training with model-corrected replies improves the final dialogue\nmodel, and we can further improve performance by using both positive and\nnegative replies through the recently proposed Director model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiyan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1\">Emily Dinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Use of Modality-Specific Large-Scale Pre-Trained Encoders for Multimodal Sentiment Analysis. (arXiv:2210.15937v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15937","description":"<p>This paper investigates the effectiveness and implementation of\nmodality-specific large-scale pre-trained encoders for multimodal sentiment\nanalysis~(MSA). Although the effectiveness of pre-trained encoders in various\nfields has been reported, conventional MSA methods employ them for only\nlinguistic modality, and their application has not been investigated. This\npaper compares the features yielded by large-scale pre-trained encoders with\nconventional heuristic features. One each of the largest pre-trained encoders\npublicly available for each modality are used; CLIP-ViT, WavLM, and BERT for\nvisual, acoustic, and linguistic modalities, respectively. Experiments on two\ndatasets reveal that methods with domain-specific pre-trained encoders attain\nbetter performance than those with conventional features in both unimodal and\nmultimodal scenarios. We also find it better to use the outputs of the\nintermediate layers of the encoders than those of the output layer. The codes\nare available at https://github.com/ando-hub/MSA_Pretrain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ando_A/0/1/0/all/0/1\">Atsushi Ando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1\">Ryo Masumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1\">Akihiko Takashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_S/0/1/0/all/0/1\">Satoshi Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1\">Naoki Makishima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1\">Keita Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moriya_T/0/1/0/all/0/1\">Takafumi Moriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashihara_T/0/1/0/all/0/1\">Takanori Ashihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_H/0/1/0/all/0/1\">Hiroshi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoChBert: Towards Robust BERT Fine-tuning for Chinese. (arXiv:2210.15944v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15944","description":"<p>Despite of the superb performance on a wide range of tasks, pre-trained\nlanguage models (e.g., BERT) have been proved vulnerable to adversarial texts.\nIn this paper, we present RoChBERT, a framework to build more Robust BERT-based\nmodels by utilizing a more comprehensive adversarial graph to fuse Chinese\nphonetic and glyph features into pre-trained representations during\nfine-tuning. Inspired by curriculum learning, we further propose to augment the\ntraining dataset with adversarial texts in combination with intermediate\nsamples. Extensive experiments demonstrate that RoChBERT outperforms previous\nmethods in significant ways: (i) robust -- RoChBERT greatly improves the model\nrobustness without sacrificing accuracy on benign texts. Specifically, the\ndefense lowers the success rates of unlimited and limited attacks by 59.43% and\n39.33% respectively, while remaining accuracy of 93.30%; (ii) flexible --\nRoChBERT can easily extend to various language models to solve different\ndownstream tasks with excellent performance; and (iii) efficient -- RoChBERT\ncan be directly applied to the fine-tuning stage without pre-training language\nmodel from scratch, and the proposed data augmentation method is also low-cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1\">Ning Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Donghong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stanceosaurus: Classifying Stance Towards Multilingual Misinformation. (arXiv:2210.15954v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15954","description":"<p>We present Stanceosaurus, a new corpus of 28,033 tweets in English, Hindi,\nand Arabic annotated with stance towards 251 misinformation claims. As far as\nwe are aware, it is the largest corpus annotated with stance towards\nmisinformation claims. The claims in Stanceosaurus originate from 15\nfact-checking sources that cover diverse geographical regions and cultures.\nUnlike existing stance datasets, we introduce a more fine-grained 5-class\nlabeling strategy with additional subcategories to distinguish implicit stance.\nPre-trained transformer-based stance classifiers that are fine-tuned on our\ncorpus show good generalization on unseen claims and regional claims from\ncountries outside the training data. Cross-lingual experiments demonstrate\nStanceosaurus' capability of training multi-lingual models, achieving 53.1 F1\non Hindi and 50.4 F1 on Arabic without any target-language fine-tuning.\nFinally, we show how a domain adaptation method can be used to improve\nperformance on Stanceosaurus using additional RumourEval-2019 data. We make\nStanceosaurus publicly available to the research community and hope it will\nencourage further work on misinformation identification across languages and\ncultures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jonathan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baheti_A/0/1/0/all/0/1\">Ashutosh Baheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naous_T/0/1/0/all/0/1\">Tarek Naous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEBERT: Efficient and robust binary ensemble BERT. (arXiv:2210.15976v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15976","description":"<p>Pre-trained BERT models have achieved impressive accuracy on natural language\nprocessing (NLP) tasks. However, their excessive amount of parameters hinders\nthem from efficient deployment on edge devices. Binarization of the BERT models\ncan significantly alleviate this issue but comes with a severe accuracy drop\ncompared with their full-precision counterparts. In this paper, we propose an\nefficient and robust binary ensemble BERT (BEBERT) to bridge the accuracy gap.\nTo the best of our knowledge, this is the first work employing ensemble\ntechniques on binary BERTs, yielding BEBERT, which achieves superior accuracy\nwhile retaining computational efficiency. Furthermore, we remove the knowledge\ndistillation procedures during ensemble to speed up the training process\nwithout compromising accuracy. Experimental results on the GLUE benchmark show\nthat the proposed BEBERT significantly outperforms the existing binary BERT\nmodels in accuracy and robustness with a 2x speedup on training time. Moreover,\nour BEBERT has only a negligible accuracy loss of 0.3% compared to the\nfull-precision baseline while saving 15x and 13x in FLOPs and model size,\nrespectively. In addition, BEBERT also outperforms other compressed BERTs in\naccuracy by up to 6.7%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jiayi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of a rule-based lemmatization algorithm through Finite State Machine for Uzbek language. (arXiv:2210.16006v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16006","description":"<p>Lemmatization is one of the core concepts in natural language processing,\nthus creating a lemmatization tool is an important task. This paper discusses\nthe construction of a lemmatization algorithm for the Uzbek language. The main\npurpose of the work is to remove affixes of words in the Uzbek language by\nmeans of the finite state machine and to identify a lemma (a word that can be\nfound in the dictionary) of the word. The process of removing affixes uses a\ndatabase of affixes and part of speech knowledge. This lemmatization consists\nof the general rules and a part of speech data of the Uzbek language, affixes,\nclassification of affixes, removing affixes on the basis of the finite state\nmachine for each class, as well as a definition of this word lemma.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharipov_M/0/1/0/all/0/1\">Maksud Sharipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sobirov_O/0/1/0/all/0/1\">Ogabek Sobirov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UzbekStemmer: Development of a Rule-Based Stemming Algorithm for Uzbek Language. (arXiv:2210.16011v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16011","description":"<p>In this paper we present a rule-based stemming algorithm for the Uzbek\nlanguage. Uzbek is an agglutinative language, so many words are formed by\nadding suffixes, and the number of suffixes is also large. For this reason, it\nis difficult to find a stem of words. The methodology is proposed for doing the\nstemming of the Uzbek words with an affix stripping approach whereas not\nincluding any database of the normal word forms of the Uzbek language. Word\naffixes are classified into fifteen classes and designed as finite state\nmachines (FSMs) for each class according to morphological rules. We created\nfifteen FSMs and linked them together to create the Basic FSM. A lexicon of\naffixes in XML format was created and a stemming application for Uzbek words\nhas been developed based on the FSMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharipov_M/0/1/0/all/0/1\">Maksud Sharipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuldashov_O/0/1/0/all/0/1\">Ollabergan Yuldashov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Phrase Break of ESL speech with Pre-trained Language Models. (arXiv:2210.16029v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16029","description":"<p>This work introduces an approach to assessing phrase break in ESL learners'\nspeech with pre-trained language models (PLMs). Different with traditional\nmethods, this proposal converts speech to token sequences, and then leverages\nthe power of PLMs. There are two sub-tasks: overall assessment of phrase break\nfor a speech clip; fine-grained assessment of every possible phrase break\nposition. Speech input is first force-aligned with texts, then pre-processed to\na token sequence, including words and associated phrase break information. The\ntoken sequence is then fed into the pre-training and fine-tuning pipeline. In\npre-training, a replaced break token detection module is trained with token\ndata where each token has a certain percentage chance to be randomly replaced.\nIn fine-tuning, overall and fine-grained scoring are optimized with text\nclassification and sequence labeling pipeline, respectively. With the\nintroduction of PLMs, the dependence on labeled training data has been greatly\nreduced, and performance has improved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shaoguang Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yan Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance. (arXiv:2210.16031v1 [cs.CV])","link":"http://arxiv.org/abs/2210.16031","description":"<p>Diffusion generative models have recently greatly improved the power of\ntext-conditioned image generation. Existing image generation models mainly\ninclude text conditional diffusion model and cross-modal guided diffusion\nmodel, which are good at small scene image generation and complex scene image\ngeneration respectively. In this work, we propose a simple yet effective\napproach, namely UPainting, to unify simple and complex scene image generation,\nas shown in Figure~\\ref{fig:leading_samples}. Based on architecture\nimprovements and diverse guidance schedules, UPainting effectively integrates\ncross-modal guidance from a pretrained image-text matching model into a text\nconditional diffusion model that utilizes a pretrained Transformer language\nmodel as the text encoder. Our key findings is that combining the power of\nlarge-scale Transformer language model in understanding language and image-text\nmatching model in capturing cross-modal semantics and style, is effective to\nimprove sample fidelity and image-text alignment of image generation. In this\nway, UPainting has a more general image generation capability, which can\ngenerate images of both simple and complex scenes more effectively. %On the\nCOCO dataset, UPainting achieves much better performance than Stable Diffusion,\none of the state-of-the-art text-to-image diffusion models. To comprehensively\ncompare text-to-image models, we further create a more general benchmark,\nUniBench, with well-written Chinese and English prompts in both simple and\ncomplex scenes. We compare UPainting with recent models and find that UPainting\ngreatly outperforms other models in terms of caption similarity and image\nfidelity in both simple and complex scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xue Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhanpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhifan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qiaoqiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yajuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Acoustic Word Embeddings from Pre-trained Self-supervised Speech Models. (arXiv:2210.16043v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16043","description":"<p>Given the strong results of self-supervised models on various tasks, there\nhave been surprisingly few studies exploring self-supervised representations\nfor acoustic word embeddings (AWE), fixed-dimensional vectors representing\nvariable-length spoken word segments. In this work, we study several\npre-trained models and pooling methods for constructing AWEs with\nself-supervised representations. Owing to the contextualized nature of\nself-supervised representations, we hypothesize that simple pooling methods,\nsuch as averaging, might already be useful for constructing AWEs. When\nevaluating on a standard word discrimination task, we find that HuBERT\nrepresentations with mean-pooling rival the state of the art on English AWEs.\nMore surprisingly, despite being trained only on English, HuBERT\nrepresentations evaluated on Xitsonga, Mandarin, and French consistently\noutperform the multilingual model XLSR-53 (as well as Wav2Vec 2.0 trained on\nEnglish).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1\">Ramon Sanabria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwater_S/0/1/0/all/0/1\">Sharon Goldwater</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards zero-shot Text-based voice editing using acoustic context conditioning, utterance embeddings, and reference encoders. (arXiv:2210.16045v1 [cs.SD])","link":"http://arxiv.org/abs/2210.16045","description":"<p>Text-based voice editing (TBVE) uses synthetic output from text-to-speech\n(TTS) systems to replace words in an original recording. Recent work has used\nneural models to produce edited speech that is similar to the original speech\nin terms of clarity, speaker identity, and prosody. However, one limitation of\nprior work is the usage of finetuning to optimise performance: this requires\nfurther model training on data from the target speaker, which is a costly\nprocess that may incorporate potentially sensitive data into server-side\nmodels. In contrast, this work focuses on the zero-shot approach which avoids\nfinetuning altogether, and instead uses pretrained speaker verification\nembeddings together with a jointly trained reference encoder to encode\nutterance-level information that helps capture aspects such as speaker identity\nand prosody. Subjective listening tests find that both utterance embeddings and\na reference encoder improve the continuity of speaker identity and prosody\nbetween the edited synthetic speech and unedited original recording in the\nzero-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fong_J/0/1/0/all/0/1\">Jason Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Prabhav Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manohar_V/0/1/0/all/0/1\">Vimal Manohar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jilong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohler_T/0/1/0/all/0/1\">Thilo K&#xf6;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Network based Formation of Cognitive Maps of Semantic Spaces and the Emergence of Abstract Concepts. (arXiv:2210.16062v1 [q-bio.NC])","link":"http://arxiv.org/abs/2210.16062","description":"<p>The hippocampal-entorhinal complex plays a major role in the organization of\nmemory and thought. The formation of and navigation in cognitive maps of\narbitrary mental spaces via place and grid cells can serve as a representation\nof memories and experiences and their relations to each other. The multi-scale\nsuccessor representation is proposed to be the mathematical principle\nunderlying place and grid cell computations. Here, we present a neural network,\nwhich learns a cognitive map of a semantic space based on 32 different animal\nspecies encoded as feature vectors. The neural network successfully learns the\nsimilarities between different animal species, and constructs a cognitive map\nof 'animal space' based on the principle of successor representations with an\naccuracy of around 30% which is near to the theoretical maximum regarding the\nfact that all animal species have more than one possible successor, i.e.\nnearest neighbor in feature space. Furthermore, a hierarchical structure, i.e.\ndifferent scales of cognitive maps, can be modeled based on multi-scale\nsuccessor representations. We find that, in fine-grained cognitive maps, the\nanimal vectors are evenly distributed in feature space. In contrast, in\ncoarse-grained maps, animal vectors are highly clustered according to their\nbiological class, i.e. amphibians, mammals and insects. This could be a\npossible mechanism explaining the emergence of new abstract semantic concepts.\nFinally, even completely new or incomplete input can be represented by\ninterpolation of the representations from the cognitive map with remarkable\nhigh accuracy of up to 95%. We conclude that the successor representation can\nserve as a weighted pointer to past memories and experiences, and may therefore\nbe a crucial building block for future machine learning to include prior\nknowledge, and to derive context knowledge from novel input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Stoewer_P/0/1/0/all/0/1\">Paul Stoewer</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Schilling_A/0/1/0/all/0/1\">Achim Schilling</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Krauss_P/0/1/0/all/0/1\">Patrick Krauss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DORE: Document Ordered Relation Extraction based on Generative Framework. (arXiv:2210.16064v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16064","description":"<p>In recent years, there is a surge of generation-based information extraction\nwork, which allows a more direct use of pre-trained language models and\nefficiently captures output dependencies. However, previous generative methods\nusing lexical representation do not naturally fit document-level relation\nextraction (DocRE) where there are multiple entities and relational facts. In\nthis paper, we investigate the root cause of the underwhelming performance of\nthe existing generative DocRE models and discover that the culprit is the\ninadequacy of the training paradigm, instead of the capacities of the models.\nWe propose to generate a symbolic and ordered sequence from the relation matrix\nwhich is deterministic and easier for model to learn. Moreover, we design a\nparallel row generation method to process overlong target sequences. Besides,\nwe introduce several negative sampling strategies to improve the performance\nwith balanced signals. Experimental results on four datasets show that our\nproposed method can improve the performance of the generative DocRE models. We\nhave released our code at https://github.com/ayyyq/DORE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiasing Masks: A New Framework for Shortcut Mitigation in NLU. (arXiv:2210.16079v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16079","description":"<p>Debiasing language models from unwanted behaviors in Natural Language\nUnderstanding tasks is a topic with rapidly increasing interest in the NLP\ncommunity. Spurious statistical correlations in the data allow models to\nperform shortcuts and avoid uncovering more advanced and desirable linguistic\nfeatures. A multitude of effective debiasing approaches has been proposed, but\nflexibility remains a major issue. For the most part, models must be retrained\nto find a new set of weights with debiased behavior. We propose a new debiasing\nmethod in which we identify debiased pruning masks that can be applied to a\nfinetuned model. This enables the selective and conditional application of\ndebiasing behaviors. We assume that bias is caused by a certain subset of\nweights in the network; our method is, in essence, a mask search to identify\nand remove biased weights. Our masks show equivalent or superior performance to\nthe standard counterparts, while offering important benefits. Pruning masks can\nbe stored with high efficiency in memory, and it becomes possible to switch\namong several debiasing behaviors (or revert back to the original biased model)\nat inference time. Finally, it opens the doors to further research on how\nbiases are acquired by studying the generated masks. For example, we observed\nthat the early layers and attention heads were pruned more aggressively,\npossibly hinting towards the location in which biases may be encoded.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meissner_J/0/1/0/all/0/1\">Johannes Mario Meissner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1\">Saku Sugawara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stop Measuring Calibration When Humans Disagree. (arXiv:2210.16133v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16133","description":"<p>Calibration is a popular framework to evaluate whether a classifier knows\nwhen it does not know - i.e., its predictive probabilities are a good\nindication of how likely a prediction is to be correct. Correctness is commonly\nestimated against the human majority class. Recently, calibration to human\nmajority has been measured on tasks where humans inherently disagree about\nwhich class applies. We show that measuring calibration to human majority given\ninherent disagreements is theoretically problematic, demonstrate this\nempirically on the ChaosNLI dataset, and derive several instance-level measures\nof calibration that capture key statistical properties of human judgements -\nclass frequency, ranking and entropy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baan_J/0/1/0/all/0/1\">Joris Baan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1\">Raquel Fernandez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling structure-building in the brain with CCG parsing and large language models. (arXiv:2210.16147v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16147","description":"<p>To model behavioral and neural correlates of language comprehension in\nnaturalistic environments, researchers have turned to broad-coverage tools from\nnatural-language processing and machine learning. Where syntactic structure is\nexplicitly modeled, prior work has relied predominantly on context-free\ngrammars (CFG), yet such formalisms are not sufficiently expressive for human\nlanguages. Combinatory Categorial Grammars (CCGs) are sufficiently expressive\ndirectly compositional models of grammar with flexible constituency that\naffords incremental interpretation. In this work we evaluate whether a more\nexpressive CCG provides a better model than a CFG for human neural signals\ncollected with fMRI while participants listen to an audiobook story. We further\ntest between variants of CCG that differ in how they handle optional adjuncts.\nThese evaluations are carried out against a baseline that includes estimates of\nnext-word predictability from a Transformer neural network language model. Such\na comparison reveals unique contributions of CCG structure-building\npredominantly in the left posterior temporal lobe: CCG-derived measures offer a\nsuperior fit to neural signals compared to those derived from a CFG. These\neffects are spatially distinct from bilateral superior temporal effects that\nare unique to predictability. Neural effects for structure-building are thus\nseparable from predictability during naturalistic listening, and those effects\nare best characterized by a grammar whose expressive power is motivated on\nindependent linguistic grounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stanojevic_M/0/1/0/all/0/1\">Milo&#x161; Stanojevi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brennan_J/0/1/0/all/0/1\">Jonathan R. Brennan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunagan_D/0/1/0/all/0/1\">Donald Dunagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_J/0/1/0/all/0/1\">John T. Hale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Neural Topic Models Broken?. (arXiv:2210.16162v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16162","description":"<p>Recently, the relationship between automated and human evaluation of topic\nmodels has been called into question. Method developers have staked the\nefficacy of new topic model variants on automated measures, and their failure\nto approximate human preferences places these models on uncertain ground.\nMoreover, existing evaluation paradigms are often divorced from real-world use.\n</p>\n<p>Motivated by content analysis as a dominant real-world use case for topic\nmodeling, we analyze two related aspects of topic models that affect their\neffectiveness and trustworthiness in practice for that purpose: the stability\nof their estimates and the extent to which the model's discovered categories\nalign with human-determined categories in the data. We find that neural topic\nmodels fare worse in both respects compared to an established classical method.\nWe take a step toward addressing both issues in tandem by demonstrating that a\nstraightforward ensembling method can reliably outperform the members of the\nensemble.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoyle_A/0/1/0/all/0/1\">Alexander Hoyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1\">Pranav Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_R/0/1/0/all/0/1\">Rupak Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resnik_P/0/1/0/all/0/1\">Philip Resnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Engineering vs BERT on Twitter Data. (arXiv:2210.16168v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16168","description":"<p>In this paper, we compare the performances of traditional machine learning\nmodels using feature engineering and word vectors and the state-of-the-art\nlanguage model BERT using word embeddings on three datasets. We also consider\nthe time and cost efficiency of feature engineering compared to BERT. From our\nresults we conclude that the use of the BERT model was only worth the time and\ncost trade-off for one of the three datasets we used for comparison, where the\nBERT model significantly outperformed any kind of traditional classifier that\nuses feature vectors, instead of embeddings. Using the BERT model for the other\ndatasets only achieved an increase of 0.03 and 0.05 of accuracy and F1 score\nrespectively, which could be argued makes its use not worth the time and cost\nof GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gani_R/0/1/0/all/0/1\">Ryiaadh Gani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalaguine_L/0/1/0/all/0/1\">Lisa Chalaguine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing for targeted syntactic knowledge through grammatical error detection. (arXiv:2210.16228v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16228","description":"<p>Targeted studies testing knowledge of subject-verb agreement (SVA) indicate\nthat pre-trained language models encode syntactic information. We assert that\nif models robustly encode subject-verb agreement, they should be able to\nidentify when agreement is correct and when it is incorrect. To that end, we\npropose grammatical error detection as a diagnostic probe to evaluate\ntoken-level contextual representations for their knowledge of SVA. We evaluate\ncontextual representations at each layer from five pre-trained English language\nmodels: BERT, XLNet, GPT-2, RoBERTa, and ELECTRA. We leverage public annotated\ntraining data from both English second language learners and Wikipedia edits,\nand report results on manually crafted stimuli for subject-verb agreement. We\nfind that masked language models linearly encode information relevant to the\ndetection of SVA errors, while the autoregressive models perform on par with\nour baseline. However, we also observe a divergence in performance when probes\nare trained on different training sets, and when they are evaluated on\ndifferent syntactic constructions, suggesting the information pertaining to SVA\nerror detection is not robustly encoded.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davis_C/0/1/0/all/0/1\">Christopher Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryant_C/0/1/0/all/0/1\">Christopher Bryant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caines_A/0/1/0/all/0/1\">Andrew Caines</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buttery_P/0/1/0/all/0/1\">Paula Buttery</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Math Word Problem via Cooperative Reasoning induced Language Models. (arXiv:2210.16257v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16257","description":"<p>Large-scale pre-trained language models (PLMs) bring new opportunities to\nchallenge problems, especially those that need high-level intelligence, such as\nthe math word problem (MWPs). However, directly applying existing PLMs to MWPs\ncan fail as the generation process lacks sufficient supervision and thus lacks\nfast adaptivity as humans. We notice that human reasoning has a dual reasoning\nframework that consists of an immediate reaction system (system 1) and a\ndelicate reasoning system (system 2), where the entire reasoning is determined\nby their interaction. This inspires us to develop a cooperative\nreasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe),\nresulting in a human-like reasoning architecture with system 1 as the generator\nand system 2 as the verifier. In our approach, the generator is responsible for\ngenerating reasoning paths, and the verifiers are used to supervise the\nevaluation in order to obtain reliable feedback for the generator. We evaluate\nour CoRe framework on several mathematical reasoning datasets and achieve\ndecent improvement over state-of-the-art methods, up to 9.8% increase over best\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_R/0/1/0/all/0/1\">Ruyi Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Speech Translation with Dynamic Latent Perceivers. (arXiv:2210.16264v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16264","description":"<p>Transformers have been the dominant architecture for Speech Translation in\nrecent years, achieving significant improvements in translation quality. Since\nspeech signals are longer than their textual counterparts, and due to the\nquadratic complexity of the Transformer, a down-sampling step is essential for\nits adoption in Speech Translation. Instead, in this research, we propose to\nease the complexity by using a Perceiver encoder to map the speech inputs to a\nfixed-length latent representation. Furthermore, we introduce a novel way of\ntraining Perceivers, with Dynamic Latent Access (DLA), unlocking larger latent\nspaces without any additional computational overhead. Speech-to-Text Perceivers\nwith DLA can match the performance of a Transformer baseline across three\nlanguage pairs in MuST-C. Finally, a DLA-trained model is easily adaptable to\nDLA at inference, and can be flexibly deployed with various computational\nbudgets, without significant drops in translation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsiamas_I/0/1/0/all/0/1\">Ioannis Tsiamas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonollosa_J/0/1/0/all/0/1\">Jos&#xe9; A. R. Fonollosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe1;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Ensemble Methods for Model Robustness Improvement of Text Classifiers. (arXiv:2210.16298v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16298","description":"<p>Large pre-trained language models have shown remarkable performance over the\npast few years. These models, however, sometimes learn superficial features\nfrom the dataset and cannot generalize to the distributions that are dissimilar\nto the training scenario. There have been several approaches proposed to reduce\nmodel's reliance on these bias features which can improve model robustness in\nthe out-of-distribution setting. However, existing methods usually use a fixed\nlow-capacity model to deal with various bias features, which ignore the\nlearnability of those features. In this paper, we analyze a set of existing\nbias features and demonstrate there is no single model that works best for all\nthe cases. We further show that by choosing an appropriate bias model, we can\nobtain a better robustness result than baselines with a more sophisticated\nmodel design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jieyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jilin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AGReE: A system for generating Automated Grammar Reading Exercises. (arXiv:2210.16302v1 [cs.CL])","link":"http://arxiv.org/abs/2210.16302","description":"<p>We describe the AGReE system, which takes user-submitted passages as input\nand automatically generates grammar practice exercises that can be completed\nwhile reading. Multiple-choice practice items are generated for a variety of\ndifferent grammar constructs: punctuation, articles, conjunctions, pronouns,\nprepositions, verbs, and nouns. We also conducted a large-scale human\nevaluation with around 4,500 multiple-choice practice items. We notice for 95%\nof items, a majority of raters out of five were able to identify the correct\nanswer and for 85% of cases, raters agree that there is only one correct answer\namong the choices. Finally, the error analysis shows that raters made the most\nmistakes for punctuation and conjunctions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Sophia Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somasundaran_S/0/1/0/all/0/1\">Swapna Somasundaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Debanjan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengxuan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concadia: Towards Image-Based Text Generation with a Purpose. (arXiv:2104.08376v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08376","description":"<p>Current deep learning models often achieve excellent results on benchmark\nimage-to-text datasets but fail to generate texts that are useful in practice.\nWe argue that to close this gap, it is vital to distinguish descriptions from\ncaptions based on their distinct communicative roles. Descriptions focus on\nvisual features and are meant to replace an image (often to increase\naccessibility), whereas captions appear alongside an image to supply additional\ninformation. To motivate this distinction and help people put it into practice,\nwe introduce the publicly available Wikipedia-based dataset Concadia consisting\nof 96,918 images with corresponding English-language descriptions, captions,\nand surrounding context. Using insights from Concadia, models trained on it,\nand a preregistered human-subjects experiment with human- and model-generated\ntexts, we characterize the commonalities and differences between descriptions\nand captions. In addition, we show that, for generating both descriptions and\ncaptions, it is useful to augment image-to-text models with representations of\nthe textual context in which the image appeared.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreiss_E/0/1/0/all/0/1\">Elisa Kreiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1\">Fei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Attention Mechanisms More Robust and Interpretable with Virtual Adversarial Training. (arXiv:2104.08763v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08763","description":"<p>Adversarial training (AT) for attention mechanisms has successfully reduced\nsuch drawbacks by considering adversarial perturbations. However, this\ntechnique requires label information, and thus, its use is limited to\nsupervised settings. In this study, we explore the concept of incorporating\nvirtual AT (VAT) into the attention mechanisms, by which adversarial\nperturbations can be computed even from unlabeled data. To realize this\napproach, we propose two general training techniques, namely VAT for attention\nmechanisms (Attention VAT) and \"interpretable\" VAT for attention mechanisms\n(Attention iVAT), which extend AT for attention mechanisms to a semi-supervised\nsetting. In particular, Attention iVAT focuses on the differences in attention;\nthus, it can efficiently learn clearer attention and improve model\ninterpretability, even with unlabeled data. Empirical experiments based on six\npublic datasets revealed that our techniques provide better prediction\nperformance than conventional AT-based as well as VAT-based techniques, and\nstronger agreement with evidence that is provided by humans in detecting\nimportant words in sentences. Moreover, our proposal offers these advantages\nwithout needing to add the careful selection of unlabeled data. That is, even\nif the model using our VAT-based technique is trained on unlabeled data from a\nsource other than the target task, both the prediction performance and model\ninterpretability can be improved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kitada_S/0/1/0/all/0/1\">Shunsuke Kitada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyatomi_H/0/1/0/all/0/1\">Hitoshi Iyatomi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-NER : Contextual Phrase Generation at Scale. (arXiv:2109.08079v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2109.08079","description":"<p>NLP research has been focused on NER extraction and how to efficiently\nextract them from a sentence. However, generating relevant context of entities\nfrom a sentence has remained under-explored. In this work we introduce the task\nContext-NER in which relevant context of an entity has to be generated. The\nextracted context may not be found exactly as a substring in the sentence. We\nalso introduce the EDGAR10-Q dataset for the same, which is a corpus of 1,500\npublicly traded companies. It is a manually created complex corpus and one of\nthe largest in terms of number of sentences and entities (1 M and 2.8 M). We\nintroduce a baseline approach that leverages phrase generation algorithms and\nuses the pre-trained BERT model to get 33% ROUGE-L score. We also do a one shot\nevaluation with GPT-3 and get 39% score, signifying the hardness and future\nscope of this task. We hope that addition of this dataset and our study will\npave the way for further research in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1\">Shreyas Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_T/0/1/0/all/0/1\">Tarun Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_T/0/1/0/all/0/1\">Tamanna Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badugu_A/0/1/0/all/0/1\">Amogh Badugu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_H/0/1/0/all/0/1\">Himanshu Sharad Bhatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Need One Model for Open-domain Question Answering. (arXiv:2112.07381v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07381","description":"<p>Recent approaches to Open-domain Question Answering refer to an external\nknowledge base using a retriever model, optionally rerank passages with a\nseparate reranker model and generate an answer using another reader model.\nDespite performing related tasks, the models have separate parameters and are\nweakly-coupled during training. We propose casting the retriever and the\nreranker as internal passage-wise attention mechanisms applied sequentially\nwithin the transformer architecture and feeding computed representations to the\nreader, with the hidden representations progressively refined at each stage.\nThis allows us to use a single question answering model trained end-to-end,\nwhich is a more efficient use of model capacity and also leads to better\ngradient flow. We present a pre-training method to effectively train this\narchitecture and evaluate our model on the Natural Questions and TriviaQA open\ndatasets. For a fixed parameter budget, our model outperforms the previous\nstate-of-the-art model by 1.0 and 0.7 exact match scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Haejun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kedia_A/0/1/0/all/0/1\">Akhil Kedia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongwon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_A/0/1/0/all/0/1\">Ashwin Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_K/0/1/0/all/0/1\">Kyoung-Gu Woo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning. (arXiv:2112.08558v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08558","description":"<p>Compared to standard retrieval tasks, passage retrieval for conversational\nquestion answering (CQA) poses new challenges in understanding the current user\nquestion, as each question needs to be interpreted within the dialogue context.\nMoreover, it can be expensive to re-train well-established retrievers such as\nsearch engines that are originally developed for non-conversational queries. To\nfacilitate their use, we develop a query rewriting model CONQRR that rewrites a\nconversational question in the context into a standalone question. It is\ntrained with a novel reward function to directly optimize towards retrieval\nusing reinforcement learning and can be adapted to any off-the-shelf retriever.\nCONQRR achieves state-of-the-art results on a recent open-domain CQA dataset\ncontaining conversations from three different sources, and is effective for two\ndifferent off-the-shelf retrievers. Our extensive analysis also shows the\nrobustness of CONQRR to out-of-domain dialogues as well as to zero query\nrewriting supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zeqiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1\">Yi Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashkin_H/0/1/0/all/0/1\">Hannah Rashkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1\">David Reitter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomar_G/0/1/0/all/0/1\">Gaurav Singh Tomar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Deduction through Search over Statement Compositions. (arXiv:2201.06028v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06028","description":"<p>In settings from fact-checking to question answering, we frequently want to\nknow whether a collection of evidence (premises) entails a hypothesis. Existing\nmethods primarily focus on the end-to-end discriminative version of this task,\nbut less work has treated the generative version in which a model searches over\nthe space of statements entailed by the premises to constructively derive the\nhypothesis. We propose a system for doing this kind of deductive reasoning in\nnatural language by decomposing the task into separate steps coordinated by a\nsearch procedure, producing a tree of intermediate conclusions that faithfully\nreflects the system's reasoning process. Our experiments on the EntailmentBank\ndataset (Dalvi et al., 2021) demonstrate that the proposed system can\nsuccessfully prove true statements while rejecting false ones. Moreover, it\nproduces natural language explanations with a 17% absolute higher step validity\nthan those produced by an end-to-end T5 model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bostrom_K/0/1/0/all/0/1\">Kaj Bostrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sprague_Z/0/1/0/all/0/1\">Zayne Sprague</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Swarat Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SQUIRE: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph Reasoning. (arXiv:2201.06206v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06206","description":"<p>Multi-hop knowledge graph (KG) reasoning has been widely studied in recent\nyears to provide interpretable predictions on missing links with evidential\npaths. Most previous works use reinforcement learning (RL) based methods that\nlearn to navigate the path towards the target entity. However, these methods\nsuffer from slow and poor convergence, and they may fail to infer a certain\npath when there is a missing edge along the path. Here we present SQUIRE, the\nfirst Sequence-to-sequence based multi-hop reasoning framework, which utilizes\nan encoder-decoder Transformer structure to translate the query to a path. Our\nframework brings about two benefits: (1) It can learn and predict in an\nend-to-end fashion, which gives better and faster convergence; (2) Our\nTransformer model does not rely on existing edges to generate the path, and has\nthe flexibility to complete missing edges along the path, especially in sparse\nKGs. Experiments on standard and sparse KGs show that our approach yields\nsignificant improvement over prior methods, while converging 4x-7x faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yushi Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yincen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zelin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Sentence Grounding in Videos: A Survey and Future Directions. (arXiv:2201.08071v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08071","description":"<p>Temporal sentence grounding in videos (TSGV), \\aka natural language video\nlocalization (NLVL) or video moment retrieval (VMR), aims to retrieve a\ntemporal moment that semantically corresponds to a language query from an\nuntrimmed video. Connecting computer vision and natural language, TSGV has\ndrawn significant attention from researchers in both communities. This survey\nattempts to provide a summary of fundamental concepts in TSGV and current\nresearch status, as well as future research directions. As the background, we\npresent a common structure of functional components in TSGV, in a tutorial\nstyle: from feature extraction from raw video and language query, to answer\nprediction of the target moment. Then we review the techniques for multimodal\nunderstanding and interaction, which is the key focus of TSGV for effective\nalignment between the two modalities. We construct a taxonomy of TSGV\ntechniques and elaborate the methods in different categories with their\nstrengths and weaknesses. Lastly, we discuss issues with the current TSGV\nresearch and share our insights about promising research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_W/0/1/0/all/0/1\">Wei Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Identifying Social Bias in Dialog Systems: Frame, Datasets, and Benchmarks. (arXiv:2202.08011v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08011","description":"<p>The research of open-domain dialog systems has been greatly prospered by\nneural models trained on large-scale corpora, however, such corpora often\nintroduce various safety problems (e.g., offensive languages, biases, and toxic\nbehaviors) that significantly hinder the deployment of dialog systems in\npractice. Among all these unsafe issues, addressing social bias is more complex\nas its negative impact on marginalized populations is usually expressed\nimplicitly, thus requiring normative reasoning and rigorous analysis. In this\npaper, we focus our investigation on social bias detection of dialog safety\nproblems. We first propose a novel Dial-Bias Frame for analyzing the social\nbias in conversations pragmatically, which considers more comprehensive\nbias-related analyses rather than simple dichotomy annotations. Based on the\nproposed framework, we further introduce CDail-Bias Dataset that, to our\nknowledge, is the first well-annotated Chinese social bias dialog dataset. In\naddition, we establish several dialog bias detection benchmarks at different\nlabel granularities and input types (utterance-level and context-level). We\nshow that the proposed in-depth analyses together with these benchmarks in our\nDial-Bias Frame are necessary and essential to bias detection tasks and can\nbenefit building safe dialog systems in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiawen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification. (arXiv:2203.07216v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07216","description":"<p>Many recent deep learning-based solutions have widely adopted the\nattention-based mechanism in various tasks of the NLP discipline. However, the\ninherent characteristics of deep learning models and the flexibility of the\nattention mechanism increase the models' complexity, thus leading to challenges\nin model explainability. In this paper, to address this challenge, we propose a\nnovel practical framework by utilizing a two-tier attention architecture to\ndecouple the complexity of explanation and the decision-making process. We\napply it in the context of a news article classification task. The experiments\non two large-scaled news corpora demonstrate that the proposed model can\nachieve competitive performance with many state-of-the-art alternatives and\nillustrate its appropriateness from an explainability perspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dairui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greene_D/0/1/0/all/0/1\">Derek Greene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zoom Out and Observe: News Environment Perception for Fake News Detection. (arXiv:2203.10885v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10885","description":"<p>Fake news detection is crucial for preventing the dissemination of\nmisinformation on social media. To differentiate fake news from real ones,\nexisting methods observe the language patterns of the news post and \"zoom in\"\nto verify its content with knowledge sources or check its readers' replies.\nHowever, these methods neglect the information in the external news environment\nwhere a fake news post is created and disseminated. The news environment\nrepresents recent mainstream media opinion and public attention, which is an\nimportant inspiration of fake news fabrication because fake news is often\ndesigned to ride the wave of popular events and catch public attention with\nunexpected novel content for greater exposure and spread. To capture the\nenvironmental signals of news posts, we \"zoom out\" to observe the news\nenvironment and propose the News Environment Perception Framework (NEP). For\neach post, we construct its macro and micro news environment from recent\nmainstream news. Then we design a popularity-oriented and a novelty-oriented\nmodule to perceive useful signals and further assist final prediction.\nExperiments on our newly built datasets show that the NEP can efficiently\nimprove the performance of basic fake news detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Qiang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xueyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rundong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danding Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularization-based Pruning of Irrelevant Weights in Deep Neural Architectures. (arXiv:2204.04977v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04977","description":"<p>Deep neural networks exploiting millions of parameters are nowadays the norm\nin deep learning applications. This is a potential issue because of the great\namount of computational resources needed for training, and of the possible loss\nof generalization performance of overparametrized networks. We propose in this\npaper a method for learning sparse neural topologies via a regularization\ntechnique which identifies non relevant weights and selectively shrinks their\nnorm, while performing a classic update for relevant ones. This technique,\nwhich is an improvement of classical weight decay, is based on the definition\nof a regularization term which can be added to any loss functional regardless\nof its form, resulting in a unified general framework exploitable in many\ndifferent contexts. The actual elimination of parameters identified as\nirrelevant is handled by an iterative pruning algorithm. We tested the proposed\ntechnique on different image classification and Natural language generation\ntasks, obtaining results on par or better then competitors in terms of sparsity\nand metrics, while achieving strong models compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonetta_G/0/1/0/all/0/1\">Giovanni Bonetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribero_M/0/1/0/all/0/1\">Matteo Ribero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cancelliere_R/0/1/0/all/0/1\">Rossella Cancelliere</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Role of Pre-trained Language Models in Word Ordering: A Case Study with BART. (arXiv:2204.07367v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07367","description":"<p>Word ordering is a constrained language generation task taking unordered\nwords as input. Existing work uses linear models and neural networks for the\ntask, yet pre-trained language models have not been studied in word ordering,\nlet alone why they help. We use BART as an instance and show its effectiveness\nin the task. To explain why BART helps word ordering, we extend analysis with\nprobing and empirically identify that syntactic dependency knowledge in BART is\na reliable explanation. We also report performance gains with BART in the\nrelated partial tree linearization task, which readily extends our analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zebin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"I'm sorry to hear that\": Finding New Biases in Language Models with a Holistic Descriptor Dataset. (arXiv:2205.09209v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09209","description":"<p>As language models grow in popularity, it becomes increasingly important to\nclearly measure all possible markers of demographic identity in order to avoid\nperpetuating existing societal harms. Many datasets for measuring bias\ncurrently exist, but they are restricted in their coverage of demographic axes\nand are commonly used with preset bias tests that presuppose which types of\nbiases models can exhibit. In this work, we present a new, more inclusive bias\nmeasurement dataset, HolisticBias, which includes nearly 600 descriptor terms\nacross 13 different demographic axes. HolisticBias was assembled in a\nparticipatory process including experts and community members with lived\nexperience of these terms. These descriptors combine with a set of bias\nmeasurement templates to produce over 450,000 unique sentence prompts, which we\nuse to explore, identify, and reduce novel forms of bias in several generative\nmodels. We demonstrate that HolisticBias is effective at measuring previously\nundetectable biases in token likelihoods from language models, as well as in an\noffensiveness classifier. We will invite additions and amendments to the\ndataset, which we hope will serve as a basis for more easy-to-use and\nstandardized methods for evaluating bias in NLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1\">Eric Michael Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_M/0/1/0/all/0/1\">Melissa Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kambadur_M/0/1/0/all/0/1\">Melanie Kambadur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Presani_E/0/1/0/all/0/1\">Eleonora Presani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twist Decoding: Diverse Generators Guide Each Other. (arXiv:2205.09273v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09273","description":"<p>Many language generation models are now available for a wide range of\ngeneration tasks, including machine translation and summarization. Combining\nsuch diverse models may lead to further progress, but ensembling generation\nmodels is challenging during inference: conventional ensembling methods (e.g.,\nshallow fusion) require that the models share vocabulary/tokenization schemes.\nWe introduce Twist decoding, a simple and general text generation algorithm\nthat benefits from diverse models at inference time. Our method does not assume\nthe vocabulary, tokenization or even generation order is shared. Our extensive\nevaluations on machine translation and scientific paper summarization\ndemonstrate that Twist decoding substantially outperforms each model decoded in\nisolation over various scenarios, including cases where domain-specific and\ngeneral-purpose models are both available. Twist decoding also consistently\noutperforms the popular reranking heuristic where output candidates from one\nmodel are rescored by another. We hope that our work will encourage researchers\nand practitioners to examine generation models collectively, not just\nindependently, and to seek out models with complementary strengths to the\ncurrently available models. Our code is available at\nhttps://github.com/jungokasai/twist_decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SNaC: Coherence Error Detection for Narrative Summarization. (arXiv:2205.09641v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09641","description":"<p>Progress in summarizing long texts is inhibited by the lack of appropriate\nevaluation frameworks. When a long summary must be produced to appropriately\ncover the facets of that text, that summary needs to present a coherent\nnarrative to be understandable by a reader, but current automatic and human\nevaluation methods fail to identify gaps in coherence. In this work, we\nintroduce SNaC, a narrative coherence evaluation framework rooted in\nfine-grained annotations for long summaries. We develop a taxonomy of coherence\nerrors in generated narrative summaries and collect span-level annotations for\n6.6k sentences across 150 book and movie screenplay summaries. Our work\nprovides the first characterization of coherence errors generated by\nstate-of-the-art summarization models and a protocol for eliciting coherence\njudgments from crowd annotators. Furthermore, we show that the collected\nannotations allow us to train a strong classifier for automatically localizing\ncoherence errors in generated summaries as well as benchmarking past work in\ncoherence modeling. Finally, our SNaC framework can support future work in long\ndocument summarization and coherence evaluation, including improved\nsummarization modeling and post-hoc summary correction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translating Hanja Historical Documents to Contemporary Korean and English. (arXiv:2205.10019v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10019","description":"<p>The Annals of Joseon Dynasty (AJD) contain the daily records of the Kings of\nJoseon, the 500-year kingdom preceding the modern nation of Korea. The Annals\nwere originally written in an archaic Korean writing system, `Hanja', and were\ntranslated into Korean from 1968 to 1993. The resulting translation was however\ntoo literal and contained many archaic Korean words; thus, a new expert\ntranslation effort began in 2012. Since then, the records of only one king have\nbeen completed in a decade. In parallel, expert translators are working on\nEnglish translation, also at a slow pace and produced only one king's records\nin English so far. Thus, we propose H2KE, a neural machine translation model,\nthat translates historical documents in Hanja to more easily understandable\nKorean and to English. Built on top of multilingual neural machine translation,\nH2KE learns to translate a historical document written in Hanja, from both a\nfull dataset of outdated Korean translation and a small dataset of more\nrecently translated contemporary Korean and English. We compare our method\nagainst two baselines: a recent model that simultaneously learns to restore and\ntranslate Hanja historical document and a Transformer based model trained only\non newly translated corpora. The experiments reveal that our method\nsignificantly outperforms the baselines in terms of BLEU scores for both\ncontemporary Korean and English translations. We further conduct extensive\nhuman evaluation which shows that our translation is preferred over the\noriginal expert translations by both experts and non-expert Korean speakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1\">Juhee Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiho Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_H/0/1/0/all/0/1\">Haneul Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bak_J/0/1/0/all/0/1\">JinYeong Bak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Matters for Image Descriptions for Accessibility: Challenges for Referenceless Evaluation Metrics. (arXiv:2205.10646v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10646","description":"<p>Few images on the Web receive alt-text descriptions that would make them\naccessible to blind and low vision (BLV) users. Image-based NLG systems have\nprogressed to the point where they can begin to address this persistent\nsocietal problem, but these systems will not be fully successful unless we\nevaluate them on metrics that guide their development correctly. Here, we argue\nagainst current referenceless metrics -- those that don't rely on\nhuman-generated ground-truth descriptions -- on the grounds that they do not\nalign with the needs of BLV users. The fundamental shortcoming of these metrics\nis that they do not take context into account, whereas contextual information\nis highly valued by BLV users. To substantiate these claims, we present a study\nwith BLV participants who rated descriptions along a variety of dimensions. An\nin-depth analysis reveals that the lack of context-awareness makes current\nreferenceless metrics inadequate for advancing image accessibility. As a\nproof-of-concept, we provide a contextual version of the referenceless metric\nCLIPScore which begins to address the disconnect to the BLV data. An accessible\nHTML version of this paper is available at\nhttps://elisakreiss.github.io/contextual-description-evaluation/paper/reflessmetrics.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreiss_E/0/1/0/all/0/1\">Elisa Kreiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_C/0/1/0/all/0/1\">Cynthia Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooshmand_S/0/1/0/all/0/1\">Shayan Hooshmand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelikman_E/0/1/0/all/0/1\">Eric Zelikman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_M/0/1/0/all/0/1\">Meredith Ringel Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation. (arXiv:2205.12206v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12206","description":"<p>Formal verse poetry imposes strict constraints on the meter and rhyme scheme\nof poems. Most prior work on generating this type of poetry uses existing poems\nfor supervision, which are difficult to obtain for most languages and poetic\nforms. In this work, we propose an unsupervised approach to generate poems\nfollowing any given meter and rhyme scheme, without requiring any poetic text\nfor training. Our method works by splitting a regular, non-poetic corpus into\nphrases, prepending control codes that describe the length and end rhyme of\neach phrase, and training a transformer language model in the augmented corpus.\nDuring inference, we build control codes for the desired meter and rhyme\nscheme, and condition our language model on them to generate formal verse\npoetry. Experiments in Spanish and Basque show that our approach is able to\ngenerate valid poems, which are often comparable in quality to those written by\nhumans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ormazabal_A/0/1/0/all/0/1\">Aitor Ormazabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirrezabal_M/0/1/0/all/0/1\">Manex Agirrezabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1\">Aitor Soroa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Your Transformer May Not be as Powerful as You Expect. (arXiv:2205.13401v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.13401","description":"<p>Relative Positional Encoding (RPE), which encodes the relative distance\nbetween any pair of tokens, is one of the most successful modifications to the\noriginal Transformer. As far as we know, theoretical understanding of the\nRPE-based Transformers is largely unexplored. In this work, we mathematically\nanalyze the power of RPE-based Transformers regarding whether the model is\ncapable of approximating any continuous sequence-to-sequence functions. One may\nnaturally assume the answer is in the affirmative -- RPE-based Transformers are\nuniversal function approximators. However, we present a negative result by\nshowing there exist continuous sequence-to-sequence functions that RPE-based\nTransformers cannot approximate no matter how deep and wide the neural network\nis. One key reason lies in that most RPEs are placed in the softmax attention\nthat always generates a right stochastic matrix. This restricts the network\nfrom capturing positional information in the RPEs and limits its capacity. To\novercome the problem and make the model more powerful, we first present\nsufficient conditions for RPE-based Transformers to achieve universal function\napproximation. With the theoretical guidance, we develop a novel attention\nmodule, called Universal RPE-based (URPE) Attention, which satisfies the\nconditions. Therefore, the corresponding URPE-based Transformers become\nuniversal function approximators. Extensive experiments covering typical\narchitectures and tasks demonstrate that our model is parameter-efficient and\ncan achieve superior performance to strong baselines in a wide range of\napplications. The code will be made publicly available at\nhttps://github.com/lsj2408/URPE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shanda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SBERT studies Meaning Representations: Decomposing Sentence Embeddings into Explainable Semantic Features. (arXiv:2206.07023v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.07023","description":"<p>Models based on large-pretrained language models, such as S(entence)BERT,\nprovide effective and efficient sentence embeddings that show high correlation\nto human similarity ratings, but lack interpretability. On the other hand,\ngraph metrics for graph-based meaning representations (e.g., Abstract Meaning\nRepresentation, AMR) can make explicit the semantic aspects in which two\nsentences are similar. However, such metrics tend to be slow, rely on parsers,\nand do not reach state-of-the-art performance when rating sentence similarity.\n</p>\n<p>In this work, we aim at the best of both worlds, by learning to induce\n$S$emantically $S$tructured $S$entence BERT embeddings (S$^3$BERT). Our\nS$^3$BERT embeddings are composed of explainable sub-embeddings that emphasize\nvarious semantic sentence features (e.g., semantic roles, negation, or\nquantification). We show how to i) learn a decomposition of the sentence\nembeddings into semantic features, through approximation of a suite of\ninterpretable AMR graph metrics, and how to ii) preserve the overall power of\nthe neural embeddings by controlling the decomposition learning process with a\nsecond objective that enforces consistency with the similarity ratings of an\nSBERT teacher model. In our experimental studies, we show that our approach\noffers interpretability -- while fully preserving the effectiveness and\nefficiency of the neural sentence embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1\">Juri Opitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models. (arXiv:2206.08325v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.08325","description":"<p>Large language models produce human-like text that drive a growing number of\napplications. However, recent literature and, increasingly, real world\nobservations, have demonstrated that these models can generate language that is\ntoxic, biased, untruthful or otherwise harmful. Though work to evaluate\nlanguage model harms is under way, translating foresight about which harms may\narise into rigorous benchmarks is not straightforward. To facilitate this\ntranslation, we outline six ways of characterizing harmful text which merit\nexplicit consideration when designing new benchmarks. We then use these\ncharacteristics as a lens to identify trends and gaps in existing benchmarks.\nFinally, we apply them in a case study of the Perspective API, a toxicity\nclassifier that is widely used in harm benchmarks. Our characteristics provide\none piece of the bridge that translates between foresight and effective\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rauh_M/0/1/0/all/0/1\">Maribeth Rauh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mellor_J/0/1/0/all/0/1\">John Mellor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uesato_J/0/1/0/all/0/1\">Jonathan Uesato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Sen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welbl_J/0/1/0/all/0/1\">Johannes Welbl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weidinger_L/0/1/0/all/0/1\">Laura Weidinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dathathri_S/0/1/0/all/0/1\">Sumanth Dathathri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaese_A/0/1/0/all/0/1\">Amelia Glaese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1\">Geoffrey Irving</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_I/0/1/0/all/0/1\">Iason Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isaac_W/0/1/0/all/0/1\">William Isaac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1\">Lisa Anne Hendricks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task Active Learning for Pre-trained Transformer-based Models. (arXiv:2208.05379v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.05379","description":"<p>Multi-task learning, in which several tasks are jointly learned by a single\nmodel, allows NLP models to share information from multiple annotations and may\nfacilitate better predictions when the tasks are inter-related. This technique,\nhowever, requires annotating the same text with multiple annotation schemes\nwhich may be costly and laborious. Active learning (AL) has been demonstrated\nto optimize annotation processes by iteratively selecting unlabeled examples\nwhose annotation is most valuable for the NLP model. Yet, multi-task active\nlearning (MT-AL) has not been applied to state-of-the-art pre-trained\nTransformer-based NLP models. This paper aims to close this gap. We explore\nvarious multi-task selection criteria in three realistic multi-task scenarios,\nreflecting different relations between the participating tasks, and demonstrate\nthe effectiveness of multi-task compared to single-task selection. Our results\nsuggest that MT-AL can be effectively used in order to minimize annotation\nefforts for multi-task NLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rotman_G/0/1/0/all/0/1\">Guy Rotman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Interpersonal Conflict Types and their Impact on Perception Classification. (arXiv:2208.08758v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.08758","description":"<p>Studies on interpersonal conflict have a long history and contain many\nsuggestions for conflict typology. We use this as the basis of a novel\nannotation scheme and release a new dataset of situations and conflict aspect\nannotations. We then build a classifier to predict whether someone will\nperceive the actions of one individual as right or wrong in a given situation.\nOur analyses include conflict aspects, but also generated clusters, which are\nhuman validated, and show differences in conflict content based on the\nrelationship of participants to the author. Our findings have important\nimplications for understanding conflict and social norms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Welch_C/0/1/0/all/0/1\">Charles Welch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plepi_J/0/1/0/all/0/1\">Joan Plepi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neuendorf_B/0/1/0/all/0/1\">B&#xe9;la Neuendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1\">Lucie Flek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VAuLT: Augmenting the Vision-and-Language Transformer for Sentiment Classification on Social Media. (arXiv:2208.09021v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2208.09021","description":"<p>We propose the Vision-and-Augmented-Language Transformer (VAuLT). VAuLT is an\nextension of the popular Vision-and-Language Transformer (ViLT), and improves\nperformance on vision-and-language (VL) tasks that involve more complex text\ninputs than image captions while having minimal impact on training and\ninference efficiency. ViLT, importantly, enables efficient training and\ninference in VL tasks, achieved by encoding images using a linear projection of\npatches instead of an object detector. However, it is pretrained on captioning\ndatasets, where the language input is simple, literal, and descriptive,\ntherefore lacking linguistic diversity. So, when working with multimedia data\nin the wild, such as multimodal social media data, there is a notable shift\nfrom captioning language data, as well as diversity of tasks. We indeed find\nevidence that the language capacity of ViLT is lacking. The key insight of\nVAuLT is to propagate the output representations of a large language model (LM)\nlike BERT to the language input of ViLT. We show that joint training of the LM\nand ViLT in VAuLT can yield relative improvements up to 20% over ViLT on VL\ntasks involving richer language inputs and affective constructs, such as for\nTarget-Oriented Sentiment Classification in TWITTER-2015 and TWITTER-2017, and\nSentiment Classification in MVSA-Single and MVSA-Multiple. Our code is\navailable at https://github.com/gchochla/VAuLT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chochlakis_G/0/1/0/all/0/1\">Georgios Chochlakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_T/0/1/0/all/0/1\">Tejas Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a> (University of Southern California)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUN: Exploring Intrinsic Uncertainties in Text-to-SQL Parsers. (arXiv:2209.06442v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.06442","description":"<p>This paper aims to improve the performance of text-to-SQL parsing by\nexploring the intrinsic uncertainties in the neural network based approaches\n(called SUN). From the data uncertainty perspective, it is indisputable that a\nsingle SQL can be learned from multiple semantically-equivalent\nquestions.Different from previous methods that are limited to one-to-one\nmapping, we propose a data uncertainty constraint to explore the underlying\ncomplementary semantic information among multiple semantically-equivalent\nquestions (many-to-one) and learn the robust feature representations with\nreduced spurious associations. In this way, we can reduce the sensitivity of\nthe learned representations and improve the robustness of the parser. From the\nmodel uncertainty perspective, there is often structural information\n(dependence) among the weights of neural networks. To improve the\ngeneralizability and stability of neural text-to-SQL parsers, we propose a\nmodel uncertainty constraint to refine the query representations by enforcing\nthe output representations of different perturbed encoding networks to be\nconsistent with each other. Extensive experiments on five benchmark datasets\ndemonstrate that our method significantly outperforms strong competitors and\nachieves new state-of-the-art results. For reproducibility, we release our code\nand data at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/sunsql.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bowen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiangpeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Locate Visual Answer in Video Corpus Using Question. (arXiv:2210.05423v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.05423","description":"<p>We introduce a new task, named video corpus visual answer localization\n(VCVAL), which aims to locate the visual answer in a large collection of\nuntrimmed instructional videos using a natural language question. This task\nrequires a range of skills - the interaction between vision and language, video\nretrieval, passage comprehension, and visual answer localization. In this\npaper, we propose a cross-modal contrastive global-span (CCGS) method for the\nVCVAL, jointly training the video corpus retrieval and visual answer\nlocalization subtasks with the global-span matrix. We have reconstructed a\ndataset named MedVidCQA, on which the VCVAL task is benchmarked. Experimental\nresults show that the proposed method outperforms other competitive methods\nboth in the video corpus retrieval and visual answer localization subtasks.\nMost importantly, we perform detailed analyses on extensive experiments, paving\na new path for understanding the instructional videos, which ushers in further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STAR: SQL Guided Pre-Training for Context-dependent Text-to-SQL Parsing. (arXiv:2210.11888v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11888","description":"<p>In this paper, we propose a novel SQL guided pre-training framework STAR for\ncontext-dependent text-to-SQL parsing, which leverages contextual information\nto enrich natural language (NL) utterance and table schema representations for\ntext-to-SQL conversations. Concretely, we propose two novel pre-training\nobjectives which respectively explore the context-dependent interactions of NL\nutterances and SQL queries within each text-to-SQL conversation: (i) schema\nstate tracking (SST) objective that tracks and explores the schema states of\ncontext-dependent SQL queries in the form of schema-states by predicting and\nupdating the value of each schema slot during interaction; (ii) utterance\ndependency tracking (UDT) objective that employs weighted contrastive learning\nto pull together two semantically similar NL utterances and push away the\nrepresentations of semantically dissimilar NL utterances within each\nconversation. In addition, we construct a high-quality large-scale\ncontext-dependent text-to-SQL conversation corpus to pre-train STAR. Extensive\nexperiments show that STAR achieves new state-of-the-art performance on two\ndownstream benchmarks (SParC and CoSQL), significantly outperforming previous\npre-training methods and ranking first on the leaderboard. We believe the\nrelease of the constructed corpus, codebase and pre-trained STAR checkpoints\nwould push forward the research in this area. For reproducibility, we release\nour code and data at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/star.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zefeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation. (arXiv:2210.13304v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13304","description":"<p>We study the text generation task under the approach of pre-trained language\nmodels (PLMs). Typically, an auto-regressive (AR) method is adopted for\ngenerating texts in a token-by-token manner. Despite many advantages of AR\ngeneration, it usually suffers from inefficient inference. Therefore,\nnon-autoregressive (NAR) models are proposed to generate all target tokens\nsimultaneously. However, NAR models usually generate texts of lower quality due\nto the absence of token dependency in the output text. In this paper, we\npropose ELMER: an efficient and effective PLM for NAR text generation to\nexplicitly model the token dependency during NAR generation. By leveraging the\nearly exit technique, ELMER enables the token generations at different layers,\naccording to their prediction confidence (a more confident token will exit at a\nlower layer). Besides, we propose a novel pre-training objective, Layer\nPermutation Language Modeling, to pre-train ELMER by permuting the exit layer\nfor each token in sequences. Experiments on three text generation tasks show\nthat ELMER significantly outperforms NAR models and further narrows the\nperformance gap with AR PLMs (\\eg ELMER (29.92) vs BART (30.61) ROUGE-L in\nXSUM) while achieving over 10 times inference speedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Knowledge Graph Construction and Event-centric Knowledge Infusion for Scientific NLI. (arXiv:2210.15248v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15248","description":"<p>With the advance of natural language inference (NLI), a rising demand for NLI\nis to handle scientific texts. Existing methods depend on pre-trained models\n(PTM) which lack domain-specific knowledge. To tackle this drawback, we\nintroduce a scientific knowledge graph to generalize PTM to scientific domain.\nHowever, existing knowledge graph construction approaches suffer from some\ndrawbacks, i.e., expensive labeled data, failure to apply in other domains,\nlong inference time and difficulty extending to large corpora. Therefore, we\npropose an unsupervised knowledge graph construction method to build a\nscientific knowledge graph (SKG) without any labeled data. Moreover, to\nalleviate noise effect from SKG and complement knowledge in sentences better,\nwe propose an event-centric knowledge infusion method to integrate external\nknowledge into each event that is a fine-grained semantic unit in sentences.\nExperimental results show that our method achieves state-of-the-art performance\nand the effectiveness and reliability of SKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaodong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/"}}]}]}