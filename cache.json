{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ArchiSound: Audio Generation with Diffusion. (arXiv:2301.13267v1 [cs.SD])","link":"http://arxiv.org/abs/2301.13267","description":"<p>The recent surge in popularity of diffusion models for image generation has\nbrought new attention to the potential of these models in other areas of media\ngeneration. One area that has yet to be fully explored is the application of\ndiffusion models to audio generation. Audio generation requires an\nunderstanding of multiple aspects, such as the temporal dimension, long term\nstructure, multiple layers of overlapping sounds, and the nuances that only\ntrained listeners can detect. In this work, we investigate the potential of\ndiffusion models for audio generation. We propose a set of models to tackle\nmultiple aspects, including a new method for text-conditional latent audio\ndiffusion with stacked 1D U-Nets, that can generate multiple minutes of music\nfrom a textual description. For each model, we make an effort to maintain\nreasonable inference speed, targeting real-time on a single consumer GPU. In\naddition to trained models, we provide a collection of open source libraries\nwith the hope of simplifying future work in the field. Samples can be found at\nhttps://bit.ly/audio-diffusion. Codes are at\nhttps://github.com/archinetai/audio-diffusion-pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_F/0/1/0/all/0/1\">Flavio Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Dynamic Prompting for Response Generation in Task-oriented Dialog Systems. (arXiv:2301.13268v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13268","description":"<p>Response generation is one of the critical components in task-oriented dialog\nsystems. Existing studies have shown that large pre-trained language models can\nbe adapted to this task. The typical paradigm of adapting such extremely large\nlanguage models would be by fine-tuning on the downstream tasks which is not\nonly time-consuming but also involves significant resources and access to\nfine-tuning data. Prompting \\citep{schick2020exploiting} has been an\nalternative to fine-tuning in many NLP tasks. In our work, we explore the idea\nof using prompting for response generation in task-oriented dialog systems.\nSpecifically, we propose an approach that performs \\textit{contextual dynamic\nprompting} where the prompts are learnt from dialog contexts. We aim to distill\nuseful prompting signals from the dialog context. On experiments with MultiWOZ\n2.2 dataset \\cite{zang2020multiwoz}, we show that contextual dynamic prompts\nimprove response generation in terms of \\textit{combined score}\n\\cite{mehri-etal-2019-structured} by 3 absolute points, and a massive 20 points\nwhen dialog states are incorporated. Furthermore, human annotation on these\nconversations found that agents which incorporate context were preferred over\nagents with vanilla prefix-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Swamy_S/0/1/0/all/0/1\">Sandesh Swamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabari_N/0/1/0/all/0/1\">Narges Tabari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chacha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangadharaiah_R/0/1/0/all/0/1\">Rashmi Gangadharaiah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Machine Translation with Large Language Models. (arXiv:2301.13294v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13294","description":"<p>Consistency is a key requirement of high-quality translation. It is\nespecially important to adhere to pre-approved terminology and corrected\ntranslations in domain-specific projects. Machine translation (MT) has achieved\nsignificant progress in the area of domain adaptation. However, real-time\nadaptation remains challenging. Large-scale language models (LLMs) have\nrecently shown interesting capabilities of in-context learning, where they\nlearn to replicate certain input-output text generation patterns, without\nfurther fine-tuning. By feeding an LLM with a prompt that consists of a list of\ntranslation pairs, it can then simulate the domain and style characteristics at\ninference time. This work aims to investigate how we can utilize in-context\nlearning to improve real-time adaptive MT. Our extensive experiments show\npromising results at translation time. For example, GPT-3.5 can adapt to a set\nof in-domain sentence pairs and/or terminology while translating a new\nsentence. We observe that the translation quality with few-shot in-context\nlearning can surpass that of strong encoder-decoder MT systems, especially for\nhigh-resource languages. Moreover, we investigate whether we can combine MT\nfrom strong encoder-decoder models with fuzzy matches, which can further\nimprove the translation, especially for less supported languages. We conduct\nour experiments across five diverse languages, namely English-to-Arabic\n(EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR),\nEnglish-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES) language pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moslem_Y/0/1/0/all/0/1\">Yasmin Moslem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_R/0/1/0/all/0/1\">Rejwanul Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Way_A/0/1/0/all/0/1\">Andy Way</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization. (arXiv:2301.13298v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13298","description":"<p>While human evaluation remains best practice for accurately judging the\nfaithfulness of automatically-generated summaries, few solutions exist to\naddress the increased difficulty and workload when evaluating long-form\nsummaries. Through a survey of 162 papers on long-form summarization, we first\nshed light on current human evaluation practices surrounding long-form\nsummaries. We find that 73% of these papers do not perform any human evaluation\non model-generated summaries, while other works face new difficulties that\nmanifest when dealing with long documents (e.g., low inter-annotator\nagreement). Motivated by our survey, we present LongEval, a set of guidelines\nfor human evaluation of faithfulness in long-form summaries that addresses the\nfollowing challenges: (1) How can we achieve high inter-annotator agreement on\nfaithfulness scores? (2) How can we minimize annotator workload while\nmaintaining accurate faithfulness scores? and (3) Do humans benefit from\nautomated alignment between summary and source snippets? We deploy LongEval in\nannotation studies on two long-form summarization datasets in different domains\n(SQuALITY and PubMed), and we find that switching to a finer granularity of\njudgment (e.g., clause-level) reduces inter-annotator variance in faithfulness\nscores (e.g., std-dev from 18.5 to 6.8). We also show that scores from a\npartial annotation of fine-grained units highly correlates with scores from a\nfull annotation workload (0.89 Kendall's tau using 50% judgments). We release\nour human judgments, annotation templates, and our software as a Python library\nfor future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bransom_E/0/1/0/all/0/1\">Erin Bransom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasigi_P/0/1/0/all/0/1\">Pradeep Dasigi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alternating Updates for Efficient Transformers. (arXiv:2301.13310v1 [cs.LG])","link":"http://arxiv.org/abs/2301.13310","description":"<p>It is well established that increasing scale in deep transformer networks\nleads to improved quality and performance. This increase in scale often comes\nwith an increase in compute cost and inference latency. Consequently, research\ninto methods which help realize the benefits of increased scale without leading\nto an increase in the compute cost becomes important. We introduce Alternating\nUpdates (AltUp), a simple-to-implement method to increase a model's capacity\nwithout the computational burden. AltUp enables the widening of the learned\nrepresentation without increasing the computation time by working on a subblock\nof the representation at each layer. Our experiments on various transformer\nmodels and language tasks demonstrate the consistent effectiveness of\nalternating updates on a diverse set of benchmarks. Finally, we present\nextensions of AltUp to the sequence dimension, and demonstrate how AltUp can be\nsynergistically combined with existing approaches, such as Sparse\nMixture-of-Experts models, to obtain efficient models with even higher\ncapacity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baykal_C/0/1/0/all/0/1\">Cenk Baykal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cutler_D/0/1/0/all/0/1\">Dylan Cutler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dikkala_N/0/1/0/all/0/1\">Nishanth Dikkala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_N/0/1/0/all/0/1\">Nikhil Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panigrahy_R/0/1/0/all/0/1\">Rina Panigrahy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proxy-based Zero-Shot Entity Linking by Effective Candidate Retrieval. (arXiv:2301.13318v1 [cs.LG])","link":"http://arxiv.org/abs/2301.13318","description":"<p>A recent advancement in the domain of biomedical Entity Linking is the\ndevelopment of powerful two-stage algorithms, an initial candidate retrieval\nstage that generates a shortlist of entities for each mention, followed by a\ncandidate ranking stage. However, the effectiveness of both stages are\ninextricably dependent on computationally expensive components. Specifically,\nin candidate retrieval via dense representation retrieval it is important to\nhave hard negative samples, which require repeated forward passes and nearest\nneighbour searches across the entire entity label set throughout training. In\nthis work, we show that pairing a proxy-based metric learning loss with an\nadversarial regularizer provides an efficient alternative to hard negative\nsampling in the candidate retrieval stage. In particular, we show competitive\nperformance on the recall@1 metric, thereby providing the option to leave out\nthe expensive candidate ranking step. Finally, we demonstrate how the model can\nbe used in a zero-shot setting to discover out of knowledge base biomedical\nentities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiatrak_M/0/1/0/all/0/1\">Maciej Wiatrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arvaniti_E/0/1/0/all/0/1\">Eirini Arvaniti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brayne_A/0/1/0/all/0/1\">Angus Brayne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetterle_J/0/1/0/all/0/1\">Jonas Vetterle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_A/0/1/0/all/0/1\">Aaron Sim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Entailment for Parameter Efficient Few Shot Learning. (arXiv:2301.13345v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13345","description":"<p>Few-shot learning allows pre-trained language models to adapt to downstream\ntasks while using a limited number of training examples. However, practical\napplications are limited when all model parameters must be optimized. In this\nwork we apply a new technique for parameter efficient few shot learning while\nadopting a strict definition of parameter efficiency. Our training method\ncombines 1) intermediate training by reformulating natural language tasks as\nentailment tasks \\cite{wang_entailment_2021} and 2) differentiable optimization\nof template and label tokens \\cite{zhang_differentiable_2021}. We quantify the\ntradeoff between parameter efficiency and performance in the few-shot regime\nand propose a simple model agnostic approach that can be extended to any task\nBy achieving competitive performance while only optimizing 3\\% of a model's\nparameters and allowing for batched inference, we allow for more efficient\npractical deployment of models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Ethan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jerry Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Identification with BOS and EOS Label Combinations. (arXiv:2301.13352v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13352","description":"<p>The sentence is a fundamental unit in many NLP applications. Sentence\nsegmentation is widely used as the first preprocessing task, where an input\ntext is split into consecutive sentences considering the end of the sentence\n(EOS) as their boundaries. This task formulation relies on a strong assumption\nthat the input text consists only of sentences, or what we call the sentential\nunits (SUs). However, real-world texts often contain non-sentential units\n(NSUs) such as metadata, sentence fragments, nonlinguistic markers, etc. which\nare unreasonable or undesirable to be treated as a part of an SU. To tackle\nthis issue, we formulate a novel task of sentence identification, where the\ngoal is to identify SUs while excluding NSUs in a given text. To conduct\nsentence identification, we propose a simple yet effective method which\ncombines the beginning of the sentence (BOS) and EOS labels to determine the\nmost probable SUs and NSUs based on dynamic programming. To evaluate this task,\nwe design an automatic, language-independent procedure to convert the Universal\nDependencies corpora into sentence identification benchmarks. Finally, our\nexperiments on the sentence identification task demonstrate that our proposed\nmethod generally outperforms sentence segmentation baselines which only utilize\nEOS labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Udagawa_T/0/1/0/all/0/1\">Takuma Udagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanayama_H/0/1/0/all/0/1\">Hiroshi Kanayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_I/0/1/0/all/0/1\">Issei Yoshida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Open-Domain Dialogue Evaluation with a Causal Inference Model. (arXiv:2301.13372v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13372","description":"<p>Effective evaluation methods remain a significant challenge for research on\nopen-domain conversational dialogue systems. Explicit satisfaction ratings can\nbe elicited from users, but users often do not provide ratings when asked, and\nthose they give can be highly subjective. Post-hoc ratings by experts are an\nalternative, but these can be both expensive and complex to collect. Here, we\nexplore the creation of automated methods for predicting both expert and user\nratings of open-domain dialogues. We compare four different approaches. First,\nwe train a baseline model using an end-to-end transformer to predict ratings\ndirectly from the raw dialogue text. The other three methods are variants of a\ntwo-stage approach in which we first extract interpretable features at the turn\nlevel that capture, among other aspects, user dialogue behaviors indicating\ncontradiction, repetition, disinterest, compliments, or criticism. We project\nthese features to the dialogue level and train a dialogue-level MLP regression\nmodel, a dialogue-level LSTM, and a novel causal inference model called\ncounterfactual-LSTM (CF-LSTM) to predict ratings. The proposed CF-LSTM is a\nsequential model over turn-level features which predicts ratings using multiple\nregressors depending on hypotheses derived from the turn-level features. As a\ncausal inference model, CF-LSTM aims to learn the underlying causes of a\nspecific event, such as a low rating. We also bin the user ratings and perform\nclassification experiments with all four models. In evaluation experiments on\nconversational data from the Alexa Prize SocialBot, we show that the CF-LSTM\nachieves the best performance for predicting dialogue ratings and\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1\">Cat P. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Luke Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnston_M/0/1/0/all/0/1\">Michael Johnston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_M/0/1/0/all/0/1\">Marilyn Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanadan_R/0/1/0/all/0/1\">Reza Ghanadan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithful Chain-of-Thought Reasoning. (arXiv:2301.13379v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13379","description":"<p>While Chain-of-Thought (CoT) prompting boosts Language Models' (LM)\nperformance on a gamut of complex reasoning tasks, the generated reasoning\nchain does not necessarily reflect how the model arrives at the answer (aka.\nfaithfulness). We propose Faithful CoT, a faithful-by-construction framework\nthat decomposes a reasoning task into two stages: Translation (Natural Language\nquery $\\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning\nchain $\\rightarrow$ answer), using an LM and a deterministic solver\nrespectively. We demonstrate the efficacy of our approach on 10 reasoning\ndatasets from 4 diverse domains. It outperforms traditional CoT prompting on 9\nout of the 10 datasets, with an average accuracy gain of 4.4 on Math Word\nProblems, 1.9 on Planning, 4.0 on Multi-hop Question Answering (QA), and 18.1\non Logical Inference, under greedy decoding. Together with self-consistency\ndecoding, we achieve new state-of-the-art few-shot performance on 7 out of the\n10 datasets, showing a strong synergy between faithfulness and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1\">Shreya Havaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_A/0/1/0/all/0/1\">Adam Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1\">Delip Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1\">Eric Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1\">Marianna Apidianaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models. (arXiv:2301.13382v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13382","description":"<p>Large language models (LLM) such as OpenAI's ChatGPT and GPT-3 offer unique\ntestbeds for exploring the translation challenges of turning literacy into\nnumeracy. Previous publicly-available transformer models from eighteen months\nprior and 1000 times smaller failed to provide basic arithmetic. The\nstatistical analysis of four complex datasets described here combines\narithmetic manipulations that cannot be memorized or encoded by simple rules.\nThe work examines whether next-token prediction succeeds from sentence\ncompletion into the realm of actual numerical understanding. For example, the\nwork highlights cases for descriptive statistics on in-memory datasets that the\nLLM initially loads from memory or generates randomly using python libraries.\nThe resulting exploratory data analysis showcases the model's capabilities to\ngroup by or pivot categorical sums, infer feature importance, derive\ncorrelations, and predict unseen test cases using linear regression. To extend\nthe model's testable range, the research deletes and appends random rows such\nthat recall alone cannot explain emergent numeracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKee_F/0/1/0/all/0/1\">Forrest McKee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZhichunRoad at Amazon KDD Cup 2022: MultiTask Pre-Training for E-Commerce Product Search. (arXiv:2301.13455v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13455","description":"<p>In this paper, we propose a robust multilingual model to improve the quality\nof search results. Our model not only leverage the processed class-balanced\ndataset, but also benefit from multitask pre-training that leads to more\ngeneral representations. In pre-training stage, we adopt mlm task,\nclassification task and contrastive learning task to achieve considerably\nperformance. In fine-tuning stage, we use confident learning, exponential\nmoving average method (EMA), adversarial training (FGM) and regularized dropout\nstrategy (R-Drop) to improve the model's generalization and robustness.\nMoreover, we use a multi-granular semantic unit to discover the queries and\nproducts textual metadata for enhancing the representation of the model. Our\napproach obtained competitive results and ranked top-8 in three tasks. We\nrelease the source code and pre-trained models associated with this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xuange Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wei Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Songlin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Archive TimeLine Summarization (ATLS): Conceptual Framework for Timeline Generation over Historical Document Collections. (arXiv:2301.13479v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13479","description":"<p>Archive collections are nowadays mostly available through search engines\ninterfaces, which allow a user to retrieve documents by issuing queries. The\nstudy of these collections may be, however, impaired by some aspects of search\nengines, such as the overwhelming number of documents returned or the lack of\ncontextual knowledge provided. New methods that could work independently or in\ncombination with search engines are then required to access these collections.\nIn this position paper, we propose to extend TimeLine Summarization (TLS)\nmethods on archive collections to assist in their studies. We provide an\noverview of existing TLS methods and we describe a conceptual framework for an\nArchive TimeLine Summarization (ATLS) system, which aims to generate\ninformative, readable and interpretable timelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutehrle_N/0/1/0/all/0/1\">Nicolas Gutehrl&#xe9;</a> (CRIT), <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Antoine Doucet</a> (L3I), <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physarum Inspired Bicycle Lane Network Design in a Congested Mega City. (arXiv:2301.13609v1 [physics.soc-ph])","link":"http://arxiv.org/abs/2301.13609","description":"<p>Mobility is a key factor in urban life and transport network plays a vital\nrole in mobility. Worse transport network having less mobility is one of the\nkey reasons to decline the living standard in any unplanned mega city.\nTransport mobility enhancement in an unplanned mega city is always challenging\ndue to various constraints including complex design and high cost involvement.\nThe aim of this thesis is to enhance transport mobility in a megacity\nintroducing a bicycle lane. To design the bicycle lane natural Physarum,\nbrainless single celled multi-nucleated protist, is studied and modified for\nbetter optimization. Recently Physarum inspired techniques are drawn\nsignificant attention to the construction of effective networks. Exiting\nPhysarum inspired models effectively and efficiently solves different problems\nincluding transport network design and modification and implication for bicycle\nlane is the unique contribution of this study. Central area of Dhaka, the\ncapital city of Bangladesh, is considered to analyze and design the bicycle\nlane network bypassing primary roads.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Habib_M/0/1/0/all/0/1\">Md. Ahsan Habib</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Akhand_M/0/1/0/all/0/1\">M. A. H. Akhand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TopoBERT: Plug and Play Toponym Recognition Module Harnessing Fine-tuned BERT. (arXiv:2301.13631v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13631","description":"<p>Extracting precise geographical information from textual contents is crucial\nin a plethora of applications. For example, during hazardous events, a robust\nand unbiased toponym extraction framework can provide an avenue to tie the\nlocation concerned to the topic discussed by news media posts and pinpoint\nhumanitarian help requests or damage reports from social media. Early studies\nhave leveraged rule-based, gazetteer-based, deep learning, and hybrid\napproaches to address this problem. However, the performance of existing tools\nis deficient in supporting operations like emergency rescue, which relies on\nfine-grained, accurate geographic information. The emerging pretrained language\nmodels can better capture the underlying characteristics of text information,\nincluding place names, offering a promising pathway to optimize toponym\nrecognition to underpin practical applications. In this paper, TopoBERT, a\ntoponym recognition module based on a one dimensional Convolutional Neural\nNetwork (CNN1D) and Bidirectional Encoder Representation from Transformers\n(BERT), is proposed and fine-tuned. Three datasets (CoNLL2003-Train,\nWikipedia3000, WNUT2017) are leveraged to tune the hyperparameters, discover\nthe best training strategy, and train the model. Another two datasets\n(CoNLL2003-Test and Harvey2017) are used to evaluate the performance. Three\ndistinguished classifiers, linear, multi-layer perceptron, and CNN1D, are\nbenchmarked to determine the optimal model architecture. TopoBERT achieves\nstate-of-the-art performance (f1-score=0.865) compared to the other five\nbaseline models and can be applied to diverse toponym recognition tasks without\nadditional training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Lei Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yingjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_Y/0/1/0/all/0/1\">Yi Qiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Sentiment and Hate Speech Analysis of Facebook Data by Employing Multilingual Transformer Models. (arXiv:2301.13668v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13668","description":"<p>In recent years, there has been a heightened consensus within academia and in\nthe public discourse that Social Media Platforms (SMPs), amplify the spread of\nhateful and negative sentiment content. Researchers have identified how hateful\ncontent, political propaganda, and targeted messaging contributed to real-world\nharms including insurrections against democratically elected governments,\ngenocide, and breakdown of social cohesion due to heightened negative discourse\ntowards certain communities in parts of the world. To counter these issues,\nSMPs have created semi-automated systems that can help identify toxic speech.\nIn this paper we analyse the statistical distribution of hateful and negative\nsentiment contents within a representative Facebook dataset (n= 604,703)\nscrapped through 648 public Facebook pages which identify themselves as\nproponents (and followers) of far-right Hindutva actors. These pages were\nidentified manually using keyword searches on Facebook and on CrowdTangleand\nclassified as far-right Hindutva pages based on page names, page descriptions,\nand discourses shared on these pages. We employ state-of-the-art, open-source\nXLM-T multilingual transformer-based language models to perform sentiment and\nhate speech analysis of the textual contents shared on these pages over a\nperiod of 5.5 years. The result shows the statistical distributions of the\npredicted sentiment and the hate speech labels; top actors, and top page\ncategories. We further discuss the benchmark performances and limitations of\nthese pre-trained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manuvie_R/0/1/0/all/0/1\">Ritumbra Manuvie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Saikat Chatterjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Friend-training: Learning from Models of Different but Related Tasks. (arXiv:2301.13683v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13683","description":"<p>Current self-training methods such as standard self-training, co-training,\ntri-training, and others often focus on improving model performance on a single\ntask, utilizing differences in input features, model architectures, and\ntraining processes. However, many tasks in natural language processing are\nabout different but related aspects of language, and models trained for one\ntask can be great teachers for other related tasks. In this work, we propose\nfriend-training, a cross-task self-training framework, where models trained to\ndo different tasks are used in an iterative training, pseudo-labeling, and\nretraining process to help each other for better selection of pseudo-labels.\nWith two dialogue understanding tasks, conversational semantic role labeling\nand dialogue rewriting, chosen for a case study, we show that the models\ntrained with the friend-training framework achieve the best performance\ncompared to strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lifeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linfeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiabing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Flan Collection: Designing Data and Methods for Effective Instruction Tuning. (arXiv:2301.13688v1 [cs.AI])","link":"http://arxiv.org/abs/2301.13688","description":"<p>We study the design decisions of publicly available instruction tuning\nmethods, and break down the development of Flan 2022 (Chung et al., 2022).\nThrough careful ablation studies on the Flan Collection of tasks and methods,\nwe tease apart the effect of design decisions which enable Flan-T5 to\noutperform prior work by 3-17%+ across evaluation settings. We find task\nbalancing and enrichment techniques are overlooked but critical to effective\ninstruction tuning, and in particular, training with mixed prompt settings\n(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)\nperformance in all settings. In further experiments, we show Flan-T5 requires\nless finetuning to converge higher and faster than T5 on single downstream\ntasks, motivating instruction-tuned models as more computationally-efficient\nstarting checkpoints for new tasks. Finally, to accelerate research on\ninstruction tuning, we make the Flan 2022 collection of datasets, templates,\nand methods publicly available at\nhttps://github.com/google-research/FLAN/tree/main/flan/v2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursive Neural Networks with Bottlenecks Diagnose (Non-)Compositionality. (arXiv:2301.13714v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13714","description":"<p>A recent line of work in NLP focuses on the (dis)ability of models to\ngeneralise compositionally for artificial languages. However, when considering\nnatural language tasks, the data involved is not strictly, or locally,\ncompositional. Quantifying the compositionality of data is a challenging task,\nwhich has been investigated primarily for short utterances. We use recursive\nneural models (Tree-LSTMs) with bottlenecks that limit the transfer of\ninformation between nodes. We illustrate that comparing data's representations\nin models with and without the bottleneck can be used to produce a\ncompositionality metric. The procedure is applied to the evaluation of\narithmetic expressions using synthetic data, and sentiment classification using\nnatural language data. We demonstrate that compression through a bottleneck\nimpacts non-compositional examples disproportionately and then use the\nbottleneck compositionality metric (BCM) to distinguish compositional from\nnon-compositional samples, yielding a compositionality ranking over a dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dankers_V/0/1/0/all/0/1\">Verna Dankers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot cross-lingual transfer language selection using linguistic similarity. (arXiv:2301.13720v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13720","description":"<p>We study the selection of transfer languages for different Natural Language\nProcessing tasks, specifically sentiment analysis, named entity recognition and\ndependency parsing. In order to select an optimal transfer language, we propose\nto utilize different linguistic similarity metrics to measure the distance\nbetween languages and make the choice of transfer language based on this\ninformation instead of relying on intuition. We demonstrate that linguistic\nsimilarity correlates with cross-lingual transfer performance for all of the\nproposed tasks. We also show that there is a statistically significant\ndifference in choosing the optimal language as the transfer source instead of\nEnglish. This allows us to select a more suitable transfer language which can\nbe used to better leverage knowledge from high-resource languages in order to\nimprove the performance of language applications lacking data. For the study,\nwe used datasets from eight different languages from three language families.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eronen_J/0/1/0/all/0/1\">Juuso Eronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ptaszynski_M/0/1/0/all/0/1\">Michal Ptaszynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masui_F/0/1/0/all/0/1\">Fumito Masui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers. (arXiv:2301.13741v1 [cs.CV])","link":"http://arxiv.org/abs/2301.13741","description":"<p>Real-world data contains a vast amount of multimodal information, among which\nvision and language are the two most representative modalities. Moreover,\nincreasingly heavier models, e.g., Transformers, have attracted the attention\nof researchers to model compression. However, how to compress multimodal\nmodels, especially vison-language Transformers, is still under-explored. This\npaper proposes the \\textbf{U}nified and \\textbf{P}r\\textbf{o}gressive\n\\textbf{P}runing (UPop) as a universal vison-language Transformer compression\nframework, which incorporates 1) unifiedly searching multimodal subnets in a\ncontinuous optimization space from the original model, which enables automatic\nassignment of pruning ratios among compressible modalities and structures; 2)\nprogressively searching and retraining the subnet, which maintains convergence\nbetween the search and retrain to attain higher compression ratios. Experiments\non multiple generative and discriminative vision-language tasks, including\nVisual Reasoning, Image Caption, Visual Question Answer, Image-Text Retrieval,\nText-Image Retrieval, and Image Classification, demonstrate the effectiveness\nand versatility of the proposed UPop framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dachuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chaofan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhendong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Scheduled Sampling with Imitation Loss for Neural Text Generation. (arXiv:2301.13753v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13753","description":"<p>State-of-the-art neural text generation models are typically trained to\nmaximize the likelihood of each token in the ground-truth sequence conditioned\non the previous target tokens. However, during inference, the model needs to\nmake a prediction conditioned on the tokens generated by itself. This\ntrain-test discrepancy is referred to as exposure bias. Scheduled sampling is a\ncurriculum learning strategy that gradually exposes the model to its own\npredictions during training to mitigate this bias. Most of the proposed\napproaches design a scheduler based on training steps, which generally requires\ncareful tuning depending on the training setup. In this work, we introduce\nDynamic Scheduled Sampling with Imitation Loss (DySI), which maintains the\nschedule based solely on the training time accuracy, while enhancing the\ncurriculum learning by introducing an imitation loss, which attempts to make\nthe behavior of the decoder indistinguishable from the behavior of a\nteacher-forced decoder. DySI is universally applicable across training setups\nwith minimal tuning. Extensive experiments and analysis show that DySI not only\nachieves notable improvements on standard machine translation benchmarks, but\nalso significantly improves the robustness of other text generation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jwalapuram_P/0/1/0/all/0/1\">Prathyusha Jwalapuram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Touch\\'e23-ValueEval Dataset for Identifying Human Values behind Arguments. (arXiv:2301.13771v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13771","description":"<p>We present the Touch\\'e23-ValueEval Dataset for Identifying Human Values\nbehind Arguments. To investigate approaches for the automated detection of\nhuman values behind arguments, we collected 9324 arguments from 6 diverse\nsources, covering religious texts, political discussions, free-text arguments,\nnewspaper editorials, and online democracy platforms. Each argument was\nannotated by 3 crowdworkers for 54 values. The Touch\\'e23-ValueEval dataset\nextends the Webis-ArgValues-22. In comparison to the previous dataset, the\neffectiveness of a 1-Baseline decreases, but that of an out-of-the-box BERT\nmodel increases. Therefore, though the classification difficulty increased as\nper the label distribution, the larger dataset allows for training better\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirzakhmedova_N/0/1/0/all/0/1\">Nailia Mirzakhmedova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiesel_J/0/1/0/all/0/1\">Johannes Kiesel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshomary_M/0/1/0/all/0/1\">Milad Alshomary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinrich_M/0/1/0/all/0/1\">Maximilian Heinrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Handke_N/0/1/0/all/0/1\">Nicolas Handke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xiaoni Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentin_B/0/1/0/all/0/1\">Barriere Valentin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dastgheib_D/0/1/0/all/0/1\">Doratossadat Dastgheib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghahroodi_O/0/1/0/all/0/1\">Omid Ghahroodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadraei_M/0/1/0/all/0/1\">Mohammad Ali Sadraei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgari_E/0/1/0/all/0/1\">Ehsaneddin Asgari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaletz_L/0/1/0/all/0/1\">Lea Kawaletz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning. (arXiv:2301.13808v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13808","description":"<p>Table-based reasoning has shown remarkable progress in combining deep models\nwith discrete reasoning, which requires reasoning over both free-form natural\nlanguage (NL) questions and structured tabular data. However, previous\ntable-based reasoning solutions usually suffer from significant performance\ndegradation on huge evidence (tables). In addition, most existing methods\nstruggle to reason over complex questions since the required information is\nscattered in different places. To alleviate the above challenges, we exploit\nlarge language models (LLMs) as decomposers for effective table-based\nreasoning, which (i) decompose huge evidence (a huge table) into sub-evidence\n(a small table) to mitigate the interference of useless information for table\nreasoning; and (ii) decompose complex questions into simpler sub-questions for\ntext reasoning. Specifically, we first use the LLMs to break down the evidence\n(tables) involved in the current question, retaining the relevant evidence and\nexcluding the remaining irrelevant evidence from the huge table. In addition,\nwe propose a \"parsing-execution-filling\" strategy to alleviate the\nhallucination dilemma of the chain of thought by decoupling logic and numerical\ncomputation in each step. Extensive experiments show that our method can\neffectively leverage decomposed evidence and questions and outperforms the\nstrong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably,\nour model outperforms human performance for the first time on the TabFact\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yunhu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v1 [cs.LG])","link":"http://arxiv.org/abs/2301.13816","description":"<p>The utilization of programming language (PL) models, pretrained on\nlarge-scale code corpora, as a means of automating software engineering\nprocesses has demonstrated considerable potential in streamlining various code\ngeneration tasks such as code completion, code translation, and program\nsynthesis. However, current approaches mainly rely on supervised fine-tuning\nobjectives borrowed from text generation, neglecting specific sequence-level\nfeatures of code, including but not limited to compilability as well as\nsyntactic and functional correctness. To address this limitation, we propose\nPPOCoder, a new framework for code generation that combines pretrained PL\nmodels with Proximal Policy Optimization (PPO) deep reinforcement learning and\nemploys execution feedback as the external source of knowledge into the model\noptimization. PPOCoder is transferable across different code generation tasks\nand PLs. Extensive experiments on three code generation tasks demonstrate the\neffectiveness of our proposed approach compared to SOTA methods, improving the\nsuccess rate of compilation and functional correctness over different PLs. Our\ncode can be found at https://github.com/reddy-lab-code-research/PPOCoder .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shojaee_P/0/1/0/all/0/1\">Parshin Shojaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aneesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tipirneni_S/0/1/0/all/0/1\">Sindhu Tipirneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal-Discovery Performance of ChatGPT in the context of Neuropathic Pain Diagnosis. (arXiv:2301.13819v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13819","description":"<p>ChatGPT has demonstrated exceptional proficiency in natural language\nconversation, e.g., it can answer a wide range of questions while no previous\nlarge language models can. Thus, we would like to push its limit and explore\nits ability to answer causal discovery questions by using a medical benchmark\n(Tu et al. 2019) in causal discovery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1\">Ruibo Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract). (arXiv:2301.13820v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13820","description":"<p>While large language models (LLMs) have demonstrated strong capability in\nstructured prediction tasks such as semantic parsing, few amounts of research\nhave explored the underlying mechanisms of their success. Our work studies\ndifferent methods for explaining an LLM-based semantic parser and qualitatively\ndiscusses the explained model behaviors, hoping to inspire future research\ntoward better understanding them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rai_D/0/1/0/all/0/1\">Daking Rai</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yilun Zhou</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Ziyu Yao</a> (1) ((1) George Mason University, (2) Massachusetts Institute of Technology)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Language Models to Images for Multimodal Generation. (arXiv:2301.13823v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13823","description":"<p>We propose an efficient method to ground pretrained text-only language models\nto the visual domain, enabling them to process and generate arbitrarily\ninterleaved image-and-text data. Our method leverages the abilities of language\nmodels learnt from large scale text-only pretraining, such as in-context\nlearning and free-form text generation. We keep the language model frozen, and\nfinetune input and output linear layers to enable cross-modality interactions.\nThis allows our model to process arbitrarily interleaved image-and-text inputs,\nand generate free-form text interleaved with retrieved images. We achieve\nstrong zero-shot performance on grounded tasks such as contextual image\nretrieval and multimodal dialogue, and showcase compelling interactive\nabilities. Our approach works with any off-the-shelf language model and paves\nthe way towards an effective, general solution for leveraging pretrained\nlanguage models in visually grounded settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models. (arXiv:2301.13826v1 [cs.CV])","link":"http://arxiv.org/abs/2301.13826","description":"<p>Recent text-to-image generative models have demonstrated an unparalleled\nability to generate diverse and creative imagery guided by a target text\nprompt. While revolutionary, current state-of-the-art diffusion models may\nstill fail in generating images that fully convey the semantics in the given\ntext prompt. We analyze the publicly available Stable Diffusion model and\nassess the existence of catastrophic neglect, where the model fails to generate\none or more of the subjects from the input prompt. Moreover, we find that in\nsome cases the model also fails to correctly bind attributes (e.g., colors) to\ntheir corresponding subjects. To help mitigate these failure cases, we\nintroduce the concept of Generative Semantic Nursing (GSN), where we seek to\nintervene in the generative process on the fly during inference time to improve\nthe faithfulness of the generated images. Using an attention-based formulation\nof GSN, dubbed Attend-and-Excite, we guide the model to refine the\ncross-attention units to attend to all subject tokens in the text prompt and\nstrengthen - or excite - their activations, encouraging the model to generate\nall subjects described in the text prompt. We compare our approach to\nalternative approaches and demonstrate that it conveys the desired concepts\nmore faithfully across a range of text prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chefer_H/0/1/0/all/0/1\">Hila Chefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alaluf_Y/0/1/0/all/0/1\">Yuval Alaluf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinker_Y/0/1/0/all/0/1\">Yael Vinker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Multi-Document Summarization Models Synthesize?. (arXiv:2301.13844v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13844","description":"<p>Multi-document summarization entails producing concise synopses of\ncollections of inputs. For some applications, the synopsis should accurately\n\\emph{synthesize} inputs with respect to a key property or aspect. For example,\na synopsis of film reviews all written about a particular movie should reflect\nthe average critic consensus. As a more consequential example, consider\nnarrative summaries that accompany biomedical \\emph{systematic reviews} of\nclinical trial results. These narratives should fairly summarize the\npotentially conflicting results from individual trials.\n</p>\n<p>In this paper we ask: To what extent do modern multi-document summarization\nmodels implicitly perform this type of synthesis? To assess this we perform a\nsuite of experiments that probe the degree to which conditional generation\nmodels trained for summarization using standard methods yield outputs that\nappropriately synthesize inputs. We find that existing models do partially\nperform synthesis, but do so imperfectly. In particular, they are\nover-sensitive to changes in input ordering and under-sensitive to changes in\ninput compositions (e.g., the ratio of positive to negative movie reviews). We\npropose a simple, general method for improving model synthesis capabilities by\ngenerating an explicitly diverse set of candidate outputs, and then selecting\nfrom these the string best aligned with the expected aggregate measure for the\ninputs, or \\emph{abstaining} when the model produces no good candidate. This\napproach improves model synthesis performance. We hope highlighting the need\nfor synthesis (in some summarization settings), motivates further research into\nmulti-document summarization methods and learning objectives that explicitly\naccount for the need to synthesize.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DeYoung_J/0/1/0/all/0/1\">Jay DeYoung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_S/0/1/0/all/0/1\">Stephanie C. Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_I/0/1/0/all/0/1\">Iain J. Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Large Language Models for News Summarization. (arXiv:2301.13848v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13848","description":"<p>Large language models (LLMs) have shown promise for automatic summarization\nbut the reasons behind their successes are poorly understood. By conducting a\nhuman evaluation on ten LLMs across different pretraining methods, prompts, and\nmodel scales, we make two important observations. First, we find instruction\ntuning, and not model size, is the key to the LLM's zero-shot summarization\ncapability. Second, existing studies have been limited by low-quality\nreferences, leading to underestimates of human performance and lower few-shot\nand finetuning performance. To better evaluate LLMs, we perform human\nevaluation over high-quality summaries we collect from freelance writers.\nDespite major stylistic differences such as the amount of paraphrasing, we find\nthat LMM summaries are judged to be on par with human written summaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori B. Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT or Human? Detect and Explain. Explaining Decisions of Machine Learning Model for Detecting Short ChatGPT-generated Text. (arXiv:2301.13852v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13852","description":"<p>ChatGPT has the ability to generate grammatically flawless and\nseemingly-human replies to different types of questions from various domains.\nThe number of its users and of its applications is growing at an unprecedented\nrate. Unfortunately, use and abuse come hand in hand. In this paper, we study\nwhether a machine learning model can be effectively trained to accurately\ndistinguish between original human and seemingly human (that is,\nChatGPT-generated) text, especially when this text is short. Furthermore, we\nemploy an explainable artificial intelligence framework to gain insight into\nthe reasoning behind the model trained to differentiate between\nChatGPT-generated and human-generated text. The goal is to analyze model's\ndecisions and determine if any specific patterns or characteristics can be\nidentified. Our study focuses on short online reviews, conducting two\nexperiments comparing human-generated and ChatGPT-generated text. The first\nexperiment involves ChatGPT text generated from custom queries, while the\nsecond experiment involves text generated by rephrasing original\nhuman-generated reviews. We fine-tune a Transformer-based model and use it to\nmake predictions, which are then explained using SHAP. We compare our model\nwith a perplexity score-based approach and find that disambiguation between\nhuman and ChatGPT-generated reviews is more challenging for the ML model when\nusing rephrased text. However, our proposed approach still achieves an accuracy\nof 79%. Using explainability, we observe that ChatGPT's writing is polite,\nwithout specific details, using fancy and atypical vocabulary, impersonal, and\ntypically it does not express feelings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitrovic_S/0/1/0/all/0/1\">Sandra Mitrovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreoletti_D/0/1/0/all/0/1\">Davide Andreoletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayoub_O/0/1/0/all/0/1\">Omran Ayoub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAGAN: Deep Semi-Supervised Linguistic-Anthropology Classification with Conditional Generative Adversarial Neural Network. (arXiv:2301.13853v1 [cs.CL])","link":"http://arxiv.org/abs/2301.13853","description":"<p>Education is a right of all, however, every individual is different than\nothers. Teachers in post-communism era discover inherent individualism to\nequally train all towards job market of fourth industrial revolution. We can\nconsider scenario of ethnic minority education in academic practices. Ethnic\nminority group has grown in their own culture and would prefer to be taught in\ntheir native way. We have formulated such linguistic anthropology(how people\nlearn)based engagement as semi-supervised problem. Then, we have developed an\nconditional deep generative adversarial network algorithm namely LA-GAN to\nclassify linguistic ethnographic features in student engagement. Theoretical\njustification proves the objective, regularization and loss function of our\nsemi-supervised adversarial model. Survey questions are prepared to reach some\nform of assumptions about z-generation and ethnic minority group, whose\nlearning style, learning approach and preference are our main area of interest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamal_R/0/1/0/all/0/1\">Rossi Kamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubincova_Z/0/1/0/all/0/1\">Zuzana Kubincova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mathematical Capabilities of ChatGPT. (arXiv:2301.13867v1 [cs.LG])","link":"http://arxiv.org/abs/2301.13867","description":"<p>We investigate the mathematical capabilities of ChatGPT by testing it on\npublicly available datasets, as well as hand-crafted ones, and measuring its\nperformance against other models trained on a mathematical corpus, such as\nMinerva. We also test whether ChatGPT can be a useful assistant to professional\nmathematicians by emulating various use cases that come up in the daily\nprofessional activities of mathematicians (question answering, theorem\nsearching). In contrast to formal mathematics, where large databases of formal\nproofs are available (e.g., the Lean Mathematical Library), current datasets of\nnatural-language mathematics, used to benchmark language models, only cover\nelementary mathematics. We address this issue by introducing a new dataset:\nGHOSTS. It is the first natural-language dataset made and curated by working\nresearchers in mathematics that (1) aims to cover graduate-level mathematics\nand (2) provides a holistic overview of the mathematical capabilities of\nlanguage models. We benchmark ChatGPT on GHOSTS and evaluate performance\nagainst fine-grained criteria. We make this new dataset publicly available to\nassist a community-driven comparison of ChatGPT with (future) large language\nmodels in terms of advanced mathematical comprehension. We conclude that\ncontrary to many positive reports in the media (a potential case of selection\nbias), ChatGPT's mathematical abilities are significantly below those of an\naverage mathematics graduate student. Our results show that ChatGPT often\nunderstands the question but fails to provide correct solutions. Hence, if your\ngoal is to use it to pass a university exam, you would be better off copying\nfrom your average peer!\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frieder_S/0/1/0/all/0/1\">Simon Frieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinchetti_L/0/1/0/all/0/1\">Luca Pinchetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1\">Ryan-Rhys Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvatori_T/0/1/0/all/0/1\">Tommaso Salvatori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_P/0/1/0/all/0/1\">Philipp Christian Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chevalier_A/0/1/0/all/0/1\">Alexis Chevalier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berner_J/0/1/0/all/0/1\">Julius Berner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PADL: Language-Directed Physics-Based Character Control. (arXiv:2301.13868v1 [cs.LG])","link":"http://arxiv.org/abs/2301.13868","description":"<p>Developing systems that can synthesize natural and life-like motions for\nsimulated characters has long been a focus for computer animation. But in order\nfor these systems to be useful for downstream applications, they need not only\nproduce high-quality motions, but must also provide an accessible and versatile\ninterface through which users can direct a character's behaviors. Natural\nlanguage provides a simple-to-use and expressive medium for specifying a user's\nintent. Recent breakthroughs in natural language processing (NLP) have\ndemonstrated effective use of language-based interfaces for applications such\nas image generation and program synthesis. In this work, we present PADL, which\nleverages recent innovations in NLP in order to take steps towards developing\nlanguage-directed controllers for physics-based character animation. PADL\nallows users to issue natural language commands for specifying both high-level\ntasks and low-level skills that a character should perform. We present an\nadversarial imitation learning approach for training policies to map high-level\nlanguage commands to low-level controls that enable a character to perform the\ndesired task and skill specified by a user's commands. Furthermore, we propose\na multi-task aggregation method that leverages a language-based multiple-choice\nquestion-answering approach to determine high-level task objectives from\nlanguage commands. We show that our framework can be applied to effectively\ndirect a simulated humanoid character to perform a diverse array of complex\nmotor skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Juravsky_J/0/1/0/all/0/1\">Jordan Juravsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yunrong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xue Bin Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Influencers: Unboxing the Mystique. (arXiv:2012.12311v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.12311","description":"<p>Influencer marketing has become a very popular tool to reach customers.\nDespite the rapid growth in influencer videos, there has been little research\non the effectiveness of their constituent elements in explaining video\nengagement. We study YouTube influencers and analyze their unstructured video\ndata across text, audio and images using a novel \"interpretable deep learning\"\nframework that accomplishes both goals of prediction and interpretation. Our\nprediction-based approach analyzes unstructured data and finds that \"what is\nsaid\" in words (text) is more influential than \"how it is said\" in imagery\n(images) followed by acoustics (audio). Our interpretation-based approach is\nimplemented after completion of model prediction by analyzing the same source\nof unstructured data to measure importance attributed to the video elements. We\neliminate several spurious and confounded relationships, and identify a smaller\nsubset of theory-based relationships. We uncover novel findings that establish\ndistinct effects for measures of shallow and deep engagement which are based on\nthe dual-system framework of human thinking. Our approach is validated using\nsimulated data, and we discuss the learnings from our findings for influencers\nand brands.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajaram_P/0/1/0/all/0/1\">Prashant Rajaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manchanda_P/0/1/0/all/0/1\">Puneet Manchanda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning From How Humans Correct. (arXiv:2102.00225v12 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.00225","description":"<p>In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and re-label\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we re-label the\nnoisy data in our dataset for our industry application. The experiment result\nshows that our method improve the classification accuracy from 91.7% to 92.5%.\nThe 91.7% accuracy is trained on the corrected dataset, which improve the\nbaseline from 83.3% to 91.7%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability. (arXiv:2109.05327v5 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2109.05327","description":"<p>Explainable AI was born as a pathway to allow humans to explore and\nunderstand the inner working of complex systems. However, establishing what is\nan explanation and objectively evaluating explainability are not trivial tasks.\nThis paper presents a new model-agnostic metric to measure the Degree of\nExplainability of information in an objective way. We exploit a specific\ntheoretical model from Ordinary Language Philosophy called the Achinstein's\nTheory of Explanations, implemented with an algorithm relying on deep language\nmodels for knowledge graph extraction and information retrieval. To understand\nwhether this metric can measure explainability, we devised a few experiments\nand user studies involving more than 190 participants, evaluating two realistic\nsystems for healthcare and finance using famous AI technology, including\nArtificial Neural Networks and TreeSHAP. The results we obtained are\nstatistically significant (with P values lower than .01), suggesting that our\nproposed metric for measuring the Degree of Explainability is robust in several\nscenarios, and it aligns with concrete expectations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sovrano_F/0/1/0/all/0/1\">Francesco Sovrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitali_F/0/1/0/all/0/1\">Fabio Vitali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards More Efficient Insertion Transformer with Fractional Positional Encoding. (arXiv:2112.06295v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06295","description":"<p>Auto-regressive neural sequence models have been shown to be effective across\ntext generation tasks. However, their left-to-right decoding order prevents\ngeneration from being parallelized. Insertion Transformer (Stern et al., 2019)\nis an attractive alternative that allows outputting multiple tokens in a single\ngeneration step. Nevertheless, due to the incompatibility between absolute\npositional encoding and insertion-based generation schemes, it needs to refresh\nthe encoding of every token in the generated partial hypothesis at each step,\nwhich could be costly. We design a novel reusable positional encoding scheme\nfor Insertion Transformers called Fractional Positional Encoding (FPE), which\nallows reusing representations calculated in previous steps. Empirical studies\non various text generation tasks demonstrate the effectiveness of FPE, which\nleads to floating-point operation reduction and latency improvements on batched\ndecoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhisong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolan_B/0/1/0/all/0/1\">Bill Dolan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One size does not fit all: Investigating strategies for differentially-private learning across NLP tasks. (arXiv:2112.08159v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08159","description":"<p>Preserving privacy in contemporary NLP models allows us to work with\nsensitive data, but unfortunately comes at a price. We know that stricter\nprivacy guarantees in differentially-private stochastic gradient descent\n(DP-SGD) generally degrade model performance. However, previous research on the\nefficiency of DP-SGD in NLP is inconclusive or even counter-intuitive. In this\nshort paper, we provide an extensive analysis of different privacy preserving\nstrategies on seven downstream datasets in five different `typical' NLP tasks\nwith varying complexity using modern neural models based on BERT and\nXtremeDistil architectures. We show that unlike standard non-private approaches\nto solving NLP tasks, where bigger is usually better, privacy-preserving\nstrategies do not exhibit a winning pattern, and each task and privacy regime\nrequires a special treatment to achieve adequate performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Senge_M/0/1/0/all/0/1\">Manuel Senge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igamberdiev_T/0/1/0/all/0/1\">Timour Igamberdiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemSup: Semantic Supervision for Simple and Scalable Zero-shot Generalization. (arXiv:2202.13100v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.13100","description":"<p>Zero-shot learning is the problem of predicting instances over classes not\nseen during training. One approach to zero-shot learning is providing auxiliary\nclass information to the model. Prior work along this vein have largely used\nexpensive per-instance annotation or singular class-level descriptions, but\nper-instance descriptions are hard to scale and single class descriptions may\nnot be rich enough. Furthermore, these works have used natural-language\ndescriptions exclusively, simple bi-encoders models, and modality or\ntask-specific methods. These approaches have several limitations: text\nsupervision may not always be available or optimal and bi-encoders may only\nlearn coarse relations between inputs and class descriptions. In this work, we\npresent SemSup, a novel approach that uses (1) a scalable multiple description\nsampling method which improves performance over single descriptions, (2)\nalternative description formats such as JSON that are easy to generate and\noutperform text on certain settings, and (3) hybrid lexical-semantic similarity\nto leverage fine-grained information in class descriptions. We demonstrate the\neffectiveness of SemSup across four datasets, two modalities, and three\ngeneralization settings. For example, across text and image datasets, SemSup\nincreases unseen class generalization accuracy by 15 points on average compared\nto the closest baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanjie_A/0/1/0/all/0/1\">Austin W. Hanjie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding Recycling for Language Models. (arXiv:2207.04993v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.04993","description":"<p>Real-world applications of neural language models often involve running many\ndifferent models over the same corpus. The high computational cost of these\nruns has led to interest in techniques that can reuse the contextualized\nembeddings produced in previous runs to speed training and inference of future\nones. We refer to this approach as embedding recycling (ER). While multiple ER\ntechniques have been proposed, their practical effectiveness is still unknown\nbecause existing evaluations consider very few models and do not adequately\naccount for overhead costs. We perform an extensive evaluation of ER across\neight different models (17 to 900 million parameters) and fourteen tasks in\nEnglish. We show how a simple ER technique that caches activations from an\nintermediate layer of a pretrained model, and learns task-specific adapters on\nthe later layers, is broadly effective. For the best-performing baseline in our\nexperiments (DeBERTa-v2 XL), adding a precomputed cache results in a &gt;90%\nspeedup during training and 87-91% speedup for inference, with negligible\nimpact on accuracy. Our analysis reveals important areas of future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saad_Falcon_J/0/1/0/all/0/1\">Jon Saad-Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DArcy_M/0/1/0/all/0/1\">Mike D&#x27;Arcy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PCC: Paraphrasing with Bottom-k Sampling and Cyclic Learning for Curriculum Data Augmentation. (arXiv:2208.08110v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.08110","description":"<p>Curriculum Data Augmentation (CDA) improves neural models by presenting\nsynthetic data with increasing difficulties from easy to hard. However,\ntraditional CDA simply treats the ratio of word perturbation as the difficulty\nmeasure and goes through the curriculums only once. This paper presents\n\\textbf{PCC}: \\textbf{P}araphrasing with Bottom-k Sampling and \\textbf{C}yclic\nLearning for \\textbf{C}urriculum Data Augmentation, a novel CDA framework via\nparaphrasing, which exploits the textual paraphrase similarity as the\ncurriculum difficulty measure. We propose a curriculum-aware paraphrase\ngeneration module composed of three units: a paraphrase candidate generator\nwith bottom-k sampling, a filtering mechanism and a difficulty measure. We also\npropose a cyclic learning strategy that passes through the curriculums multiple\ntimes. The bottom-k sampling is proposed to generate super-hard instances for\nthe later curriculums. Experimental results on few-shot text classification as\nwell as dialogue generation indicate that PCC surpasses competitive baselines.\nHuman evaluation and extensive case studies indicate that bottom-k sampling\neffectively generates super-hard instances, and PCC significantly improves the\nbaseline dialogue agent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Efficacy of Self-Supervised Speech Models for Audio Representations. (arXiv:2209.12900v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2209.12900","description":"<p>Self-supervised learning (SSL) speech models, which can serve as powerful\nupstream models to extract meaningful speech representations, have achieved\nunprecedented success in speech representation learning. However, their\neffectiveness on non-speech datasets is relatively less explored. In this work,\nwe propose an ensemble framework, with a combination of ensemble techniques, to\nfuse SSL speech models' embeddings. Extensive experiments on speech and\nnon-speech audio datasets are conducted to investigate the representation\nabilities of our ensemble method and its single constituent model. Ablation\nstudies are carried out to evaluate the performances of different ensemble\ntechniques, such as feature averaging and concatenation. All experiments are\nconducted during NeurIPS 2021 HEAR Challenge as a standard evaluation pipeline\nprovided by competition officials. Results demonstrate SSL speech models'\nstrong abilities on various non-speech tasks, while we also note that they fail\nto deal with fine-grained music tasks, such as pitch classification and note\nonset detection. In addition, feature ensemble is shown to have great potential\non producing more holistic representations, as our proposed framework generally\nsurpasses state-of-the-art SSL speech/audio models and has superior performance\non various datasets compared with other teams in HEAR Challenge. Our code is\navailable at https://github.com/tony10101105/HEAR-2021-NeurIPS-Challenge --\nNTU-GURA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tung-Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen-An Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tzu-Han Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_T/0/1/0/all/0/1\">Tsu-Yuan Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayes risk CTC: Controllable CTC alignment in Sequence-to-Sequence tasks. (arXiv:2210.07499v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07499","description":"<p>Sequence-to-Sequence (seq2seq) tasks transcribe the input sequence to a\ntarget sequence. The Connectionist Temporal Classification (CTC) criterion is\nwidely used in multiple seq2seq tasks. Besides predicting the target sequence,\na side product of CTC is to predict the alignment, which is the most probable\ninput-long sequence that specifies a hard aligning relationship between the\ninput and target units. As there are multiple potential aligning sequences\n(called paths) that are equally considered in CTC formulation, the choice of\nwhich path will be most probable and become the predicted alignment is always\nuncertain. In addition, it is usually observed that the alignment predicted by\nvanilla CTC will drift compared with its reference and rarely provides\npractical functionalities. Thus, the motivation of this work is to make the CTC\nalignment prediction controllable and thus equip CTC with extra\nfunctionalities. The Bayes risk CTC (BRCTC) criterion is then proposed in this\nwork, in which a customizable Bayes risk function is adopted to enforce the\ndesired characteristics of the predicted alignment. With the risk function, the\nBRCTC is a general framework to adopt some customizable preference over the\npaths in order to concentrate the posterior into a particular subset of the\npaths. In applications, we explore one particular preference which yields\nmodels with the down-sampling ability and reduced inference costs. By using\nBRCTC with another preference for early emissions, we obtain an improved\nperformance-latency trade-off for online models. Experimentally, the proposed\nBRCTC reduces the inference cost of offline models by up to 47% without\nperformance degradation and cuts down the overall latency of online systems to\nan unseen level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jinchuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1\">Chao Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences. (arXiv:2210.11794v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.11794","description":"<p>Efficient Transformers have been developed for long sequence modeling, due to\ntheir subquadratic memory and time complexity. Sparse Transformer is a popular\napproach to improving the efficiency of Transformers by restricting\nself-attention to locations specified by the predefined sparse patterns.\nHowever, leveraging sparsity may sacrifice expressiveness compared to\nfull-attention, when important token correlations are multiple hops away. To\ncombine advantages of both the efficiency of sparse transformer and the\nexpressiveness of full-attention Transformer, we propose \\textit{Diffuser}, a\nnew state-of-the-art efficient Transformer. Diffuser incorporates all token\ninteractions within one attention layer while maintaining low computation and\nmemory costs. The key idea is to expand the receptive field of sparse attention\nusing Attention Diffusion, which computes multi-hop token correlations based on\nall paths between corresponding disconnected tokens, besides attention among\nneighboring tokens. Theoretically, we show the expressiveness of Diffuser as a\nuniversal sequence approximator for sequence-to-sequence modeling, and\ninvestigate its ability to approximate full-attention by analyzing the graph\nexpander property from the spectral perspective. Experimentally, we investigate\nthe effectiveness of Diffuser with extensive evaluations, including language\nmodeling, image modeling, and Long Range Arena (LRA). Evaluation results show\nthat Diffuser achieves improvements by an average of 0.94% on text\nclassification tasks and 2.30% on LRA, with 1.67$\\times$ memory savings\ncompared to state-of-the-art benchmarks, which demonstrates superior\nperformance of Diffuser in both expressiveness and efficiency aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1\">Aosong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Better Few-Shot and Finetuning Performance with Forgetful Causal Language Models. (arXiv:2210.13432v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13432","description":"<p>Large language models (LLM) trained using the next-token-prediction\nobjective, such as GPT3 and PaLM, have revolutionized natural language\nprocessing in recent years by showing impressive zero-shot and few-shot\ncapabilities across a wide range of tasks. In this work, we propose a simple\ntechnique that significantly boosts the performance of LLMs without adding\ncomputational cost. Our key observation is that, by performing the next token\nprediction task with randomly selected past tokens masked out, we can improve\nthe quality of the learned representations for downstream language\nunderstanding tasks. We hypothesize that randomly masking past tokens prevents\nover-attending to recent tokens and encourages attention to tokens in the\ndistant past. We find that our method, Forgetful Causal Masking (FCM),\nsignificantly improves both few-shot and finetuning performance of PaLM. We\nfurther consider a simple extension, T-FCM, which introduces bidirectional\ncontext to causal language model without altering the sequence order, and\nfurther improves finetuning performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xinyang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lisa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Unsupervised Text Classification: Zero-shot and Similarity-based Approaches. (arXiv:2211.16285v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.16285","description":"<p>Text classification of unseen classes is a challenging Natural Language\nProcessing task and is mainly attempted using two different types of\napproaches. Similarity-based approaches attempt to classify instances based on\nsimilarities between text document representations and class description\nrepresentations. Zero-shot text classification approaches aim to generalize\nknowledge gained from a training task by assigning appropriate labels of\nunknown classes to text documents. Although existing studies have already\ninvestigated individual approaches to these categories, the experiments in\nliterature do not provide a consistent comparison. This paper addresses this\ngap by conducting a systematic evaluation of different similarity-based and\nzero-shot approaches for text classification of unseen classes. Different\nstate-of-the-art approaches are benchmarked on four text classification\ndatasets, including a new dataset from the medical domain. Additionally, novel\nSimCSE and SBERT-based baselines are proposed, as other baselines used in\nexisting work yield weak classification results and are easily outperformed.\nFinally, the novel similarity-based Lbl2TransformerVec approach is presented,\nwhich outperforms previous state-of-the-art approaches in unsupervised text\nclassification. Our experiments show that similarity-based approaches\nsignificantly outperform zero-shot approaches in most cases. Additionally,\nusing SimCSE or SBERT embeddings instead of simpler text representations\nincreases similarity-based classification results even further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1\">Tim Schopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_D/0/1/0/all/0/1\">Daniel Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers. (arXiv:2212.04325v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2212.04325","description":"<p>Recently, RNN-Transducers have achieved remarkable results on various\nautomatic speech recognition tasks. However, lattice-free sequence\ndiscriminative training methods, which obtain superior performance in hybrid\nmodes, are rarely investigated in RNN-Transducers. In this work, we propose\nthree lattice-free training objectives, namely lattice-free maximum mutual\ninformation, lattice-free segment-level minimum Bayes risk, and lattice-free\nminimum Bayes risk, which are used for the final posterior output of the\nphoneme-based neural transducer with a limited context dependency. Compared to\ncriteria using N-best lists, lattice-free methods eliminate the decoding step\nfor hypotheses generation during training, which leads to more efficient\ntraining. Experimental results show that lattice-free methods gain up to 6.5%\nrelative improvement in word error rate compared to a sequence-level\ncross-entropy trained model. Compared to the N-best-list based minimum Bayes\nrisk objectives, lattice-free methods gain 40% - 70% relative training time\nspeedup with a small degradation in performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Zijian Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Difformer: Empowering Diffusion Models on the Embedding Space for Text Generation. (arXiv:2212.09412v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09412","description":"<p>Diffusion models have achieved state-of-the-art synthesis quality on both\nvisual and audio tasks, and recent works further adapt them to textual data by\ndiffusing on the embedding space. In this paper, we conduct systematic studies\nand analyze the challenges between the continuous data space and the embedding\nspace which have not been carefully explored. Firstly, the data distribution is\nlearnable for embeddings, which may lead to the collapse of the loss function.\nSecondly, as the norm of embeddings varies between popular and rare words,\nadding the same noise scale will lead to sub-optimal results. In addition, we\nfind the normal level of noise causes insufficient training of the model. To\naddress the above challenges, we propose Difformer, an embedding diffusion\nmodel based on Transformer, which consists of three essential modules including\nan additional anchor loss function, a layer normalization module for\nembeddings, and a noise factor to the Gaussian noise. Experiments on two\nseminal text generation tasks including machine translation and text\nsummarization show the superiority of Difformer over compared embedding\ndiffusion baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhujin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongxin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Linli Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated speech- and text-based classification of neuropsychiatric conditions in a multidiagnostic setting. (arXiv:2301.06916v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.06916","description":"<p>Speech patterns have been identified as potential diagnostic markers for\nneuropsychiatric conditions. However, most studies only compare a single\nclinical group to healthy controls, whereas clinical practice often requires\ndifferentiating between multiple potential diagnoses (multiclass settings). To\naddress this, we assembled a dataset of repeated recordings from 420\nparticipants (67 with major depressive disorder, 106 with schizophrenia and 46\nwith autism, as well as matched controls), and tested the performance of a\nrange of conventional machine learning models and advanced Transformer models\non both binary and multiclass classification, based on voice and text features.\n</p>\n<p>While binary models performed comparably to previous research (F1 scores\nbetween 0.54-0.75 for autism spectrum disorder, ASD; 0.67-0.92 for major\ndepressive disorder, MDD; and 0.71-0.83 for schizophrenia); when\ndifferentiating between multiple diagnostic groups performance decreased\nmarkedly (F1 scores between 0.35-0.44 for ASD, 0.57-0.75 for MDD, 0.15-0.66 for\nschizophrenia, and 0.38-0.52 macro F1). Combining voice and text-based models\nyielded increased performance, suggesting that they capture complementary\ndiagnostic information.\n</p>\n<p>Our results indicate that models trained on binary classification may learn\nto rely on markers of generic differences between clinical and non-clinical\npopulations, or markers of clinical features that overlap across conditions,\nrather than identifying markers specific to individual conditions. We provide\nrecommendations for future research in the field, suggesting increased focus on\ndeveloping larger transdiagnostic datasets that include more fine-grained\nclinical features, and that can support the development of models that better\ncapture the complexity of neuropsychiatric conditions and naturalistic\ndiagnostic assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1\">Lasse Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocca_R/0/1/0/all/0/1\">Roberta Rocca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonsen_A/0/1/0/all/0/1\">Arndis Simonsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parola_A/0/1/0/all/0/1\">Alberto Parola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bliksted_V/0/1/0/all/0/1\">Vibeke Bliksted</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladegaard_N/0/1/0/all/0/1\">Nicolai Ladegaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_D/0/1/0/all/0/1\">Dan Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tylen_K/0/1/0/all/0/1\">Kristian Tyl&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weed_E/0/1/0/all/0/1\">Ethan Weed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostergaard_S/0/1/0/all/0/1\">S&#xf8;ren Dinesen &#xd8;stergaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fusaroli_R/0/1/0/all/0/1\">Riccardo Fusaroli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT A Good Translator? A Preliminary Study. (arXiv:2301.08745v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.08745","description":"<p>This report provides a preliminary evaluation of ChatGPT for machine\ntranslation, including translation prompt, multilingual translation, and\ntranslation robustness. We adopt the prompts advised by ChatGPT to trigger its\ntranslation ability and find that the candidate prompts generally work well and\nshow minor performance differences. By evaluating on a number of benchmark test\nsets, we find that ChatGPT performs competitively with commercial translation\nproducts (e.g., Google Translate) on high-resource European languages but lags\nbehind significantly on low-resource or distant languages. For distant\nlanguages, we explore an interesting strategy named $\\mathbf{pivot~prompting}$\nthat asks ChatGPT to translate the source sentence into a high-resource pivot\nlanguage before into the target language, which improves the translation\nperformance significantly. As for the translation robustness, ChatGPT does not\nperform as well as the commercial systems on biomedical abstracts or Reddit\ncomments but is potentially a good translator for spoken language. Scripts and\ndata: https://github.com/wxjiao/Is-ChatGPT-A-Good-Translator\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource Compositional Semantic Parsing with Concept Pretraining. (arXiv:2301.09809v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.09809","description":"<p>Semantic parsing plays a key role in digital voice assistants such as Alexa,\nSiri, and Google Assistant by mapping natural language to structured meaning\nrepresentations. When we want to improve the capabilities of a voice assistant\nby adding a new domain, the underlying semantic parsing model needs to be\nretrained using thousands of annotated examples from the new domain, which is\ntime-consuming and expensive. In this work, we present an architecture to\nperform such domain adaptation automatically, with only a small amount of\nmetadata about the new domain and without any new training data (zero-shot) or\nwith very few examples (few-shot). We use a base seq2seq (sequence-to-sequence)\narchitecture and augment it with a concept encoder that encodes intent and slot\ntags from the new domain. We also introduce a novel decoder-focused approach to\npretrain seq2seq models to be concept aware using Wikidata and use it to help\nour model learn important concepts and perform well in low-resource settings.\nWe report few-shot and zero-shot results for compositional semantic parsing on\nthe TOPv2 dataset and show that our model outperforms prior approaches in\nfew-shot settings for the TOPv2 and SNIPS datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rongali_S/0/1/0/all/0/1\">Subendhu Rongali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_M/0/1/0/all/0/1\">Mukund Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Haidar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arkoudas_K/0/1/0/all/0/1\">Konstantine Arkoudas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamza_W/0/1/0/all/0/1\">Wael Hamza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP as a Lens for Causal Analysis and Perception Mining to Infer Mental Health on Social Media. (arXiv:2301.11004v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11004","description":"<p>Interactions among humans on social media often convey intentions behind\ntheir actions, yielding a psychological language resource for Mental Health\nAnalysis (MHA) of online users. The success of Computational Intelligence\nTechniques (CIT) for inferring mental illness from such social media resources\npoints to NLP as a lens for causal analysis and perception mining. However, we\nargue that more consequential and explainable research is required for optimal\nimpact on clinical psychology practice and personalized mental healthcare. To\nbridge this gap, we posit two significant dimensions: (1) Causal analysis to\nillustrate a cause and effect relationship in the user generated text; (2)\nPerception mining to infer psychological perspectives of social effects on\nonline users intentions. Within the scope of Natural Language Processing (NLP),\nwe further explore critical areas of inquiry associated with these two\ndimensions, specifically through recent advancements in discourse analysis.\nThis position paper guides the community to explore solutions in this space and\nadvance the state of practice in developing conversational agents for inferring\nmental health from social media. We advocate for a more explainable approach\ntoward modeling computational psychology problems through the lens of language\nas we observe an increased number of research contributions in dataset and\nproblem formulation for causal relation extraction and perception enhancements\nwhile inferring mental states.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Muskan Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_C/0/1/0/all/0/1\">Chandni Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseem_U/0/1/0/all/0/1\">Usman Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorr_B/0/1/0/all/0/1\">Bonnie J Dorr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Knowledge into Document Summarization: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11719","description":"<p>Despite the great development of document summarization techniques nowadays,\nfactual inconsistencies between the generated summaries and the original text\nstill occur from time to time. This paper proposes a prefix-tuning-based\napproach that uses a set of trainable continuous prefix prompt together with\ndiscrete prompts to aid model generation, which makes a significant impact on\nboth CNN/Daily Mail and XSum summaries generated using GPT-2. The improvements\non fact preservation in the generated summaries indicates the effectiveness of\nadopting this prefix-tuning-based method in knowledge-enhanced document\nsummarization, and also shows a great potential on other natural language\nprocessing tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Emma Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_A/0/1/0/all/0/1\">Alireza Seyed Shakeri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time out of Mind: Generating Rate of Speech conditioned on emotion and speaker. (arXiv:2301.12331v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12331","description":"<p>Voice synthesis has seen significant improvements in the past decade\nresulting in highly intelligible voices. Further investigations have resulted\nin models that can produce variable speech, including conditional emotional\nexpression. The problem lies, however, in a focus on phrase-level modifications\nand prosodic vocal features. Using the CREMA-D dataset we have trained a GAN\nconditioned on emotion to generate worth lengths for a given input text. These\nword lengths are relative to neutral speech and can be provided, through speech\nsynthesis markup language (SSML) to a text-to-speech (TTS) system to generate\nmore expressive speech. Additionally, a generative model is also trained using\nimplicit maximum likelihood estimation (IMLE) and a comparative analysis with\nGANs is included. We were able to achieve better performances on objective\nmeasures for neutral speech, and better time alignment for happy speech when\ncompared to an out-of-box model. However, further investigation of subjective\nevaluation is required.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaur_N/0/1/0/all/0/1\">Navjot Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuttosi_P/0/1/0/all/0/1\">Paige Tuttosi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2301.11916","description":"<p>In recent years, pre-trained large language models have demonstrated\nremarkable efficiency in achieving an inference-time few-shot learning\ncapability known as in-context learning. However, existing literature has\nhighlighted the sensitivity of this capability to the selection of few-shot\ndemonstrations. The underlying mechanisms by which this capability arises from\nregular language model pretraining objectives remain poorly understood. In this\nstudy, we aim to examine the in-context learning phenomenon through a Bayesian\nlens, viewing large language models as topic models that implicitly infer\ntask-related information from demonstrations. On this premise, we propose an\nalgorithm for selecting optimal demonstrations from a set of annotated data and\ndemonstrate a significant 12.5% improvement relative to the random selection\nbaseline, averaged over eight GPT2 and GPT3 models on eight different\nreal-world text classification datasets. Our empirical findings support our\nhypothesis that large language models implicitly infer a latent concept\nvariable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}