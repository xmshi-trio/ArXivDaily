{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Inference of Partial Colexifications from Multilingual Wordlists. (arXiv:2302.00739v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00739","description":"<p>The past years have seen a drastic rise in studies devoted to the\ninvestigation of colexification patterns in individual languages families in\nparticular and the languages of the world in specific. Specifically\ncomputational studies have profited from the fact that colexification as a\nscientific construct is easy to operationalize, enabling scholars to infer\ncolexification patterns for large collections of cross-linguistic data. Studies\ndevoted to partial colexifications -- colexification patterns that do not\ninvolve entire words, but rather various parts of words--, however, have been\nrarely conducted so far. This is not surprising, since partial colexifications\nare less easy to deal with in computational approaches and may easily suffer\nfrom all kinds of noise resulting from false positive matches. In order to\naddress this problem, this study proposes new approaches to the handling of\npartial colexifications by (1) proposing new models with which partial\ncolexification patterns can be represented, (2) developing new efficient\nmethods and workflows which help to infer various types of partial\ncolexification patterns from multilingual wordlists, and (3) illustrating how\ninferred patterns of partial colexifications can be computationally analyzed\nand interactively visualized.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+List_J/0/1/0/all/0/1\">Johann-Mattis List</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AmbiCoref: Evaluating Human and Model Sensitivity to Ambiguous Coreference. (arXiv:2302.00762v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00762","description":"<p>Given a sentence \"Abby told Brittney that she upset Courtney\", one would\nstruggle to understand who \"she\" refers to, and ask for clarification. However,\nif the word \"upset\" were replaced with \"hugged\", \"she\" unambiguously refers to\nAbby. We study if modern coreference resolution models are sensitive to such\npronominal ambiguity. To this end, we construct AmbiCoref, a diagnostic corpus\nof minimal sentence pairs with ambiguous and unambiguous referents. Our\nexamples generalize psycholinguistic studies of human perception of ambiguity\naround particular arrangements of verbs and their arguments. Analysis shows\nthat (1) humans are less sure of referents in ambiguous AmbiCoref examples than\nunambiguous ones, and (2) most coreference models show little difference in\noutput between ambiguous and unambiguous pairs. We release AmbiCoref as a\ndiagnostic corpus for testing whether models treat ambiguity similarly to\nhuman.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuewei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malaviya_C/0/1/0/all/0/1\">Chaitanya Malaviya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yatskar_M/0/1/0/all/0/1\">Mark Yatskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborating with language models for embodied reasoning. (arXiv:2302.00763v1 [cs.LG])","link":"http://arxiv.org/abs/2302.00763","description":"<p>Reasoning in a complex and ambiguous environment is a key goal for\nReinforcement Learning (RL) agents. While some sophisticated RL agents can\nsuccessfully solve difficult tasks, they require a large amount of training\ndata and often struggle to generalize to new unseen environments and new tasks.\nOn the other hand, Large Scale Language Models (LSLMs) have exhibited strong\nreasoning ability and the ability to to adapt to new tasks through in-context\nlearning. However, LSLMs do not inherently have the ability to interrogate or\nintervene on the environment. In this work, we investigate how to combine these\ncomplementary abilities in a single system consisting of three parts: a\nPlanner, an Actor, and a Reporter. The Planner is a pre-trained language model\nthat can issue commands to a simple embodied agent (the Actor), while the\nReporter communicates with the Planner to inform its next command. We present a\nset of tasks that require reasoning, test this system's ability to generalize\nzero-shot and investigate failure cases, and demonstrate how components of this\nsystem can be trained with reinforcement-learning to improve performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaeser_Chen_C/0/1/0/all/0/1\">Christine Kaeser-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marino_K/0/1/0/all/0/1\">Kenneth Marino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_A/0/1/0/all/0/1\">Arun Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babayan_S/0/1/0/all/0/1\">Sheila Babayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1\">Rob Fergus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually Grounded Keyword Detection and Localisation for Low-Resource Languages. (arXiv:2302.00765v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00765","description":"<p>This study investigates the use of Visually Grounded Speech (VGS) models for\nkeyword localisation in speech. The study focusses on two main research\nquestions: (1) Is keyword localisation possible with VGS models and (2) Can\nkeyword localisation be done cross-lingually in a real low-resource setting?\nFour methods for localisation are proposed and evaluated on an English dataset,\nwith the best-performing method achieving an accuracy of 57%. A new dataset\ncontaining spoken captions in Yoruba language is also collected and released\nfor cross-lingual keyword localisation. The cross-lingual model obtains a\nprecision of 16% in actual keyword localisation and this performance can be\nimproved by initialising from a model pretrained on English data. The study\npresents a detailed analysis of the model's success and failure modes and\nhighlights the challenges of using VGS models for keyword localisation in\nlow-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olaleye_K/0/1/0/all/0/1\">Kayode Kolawole Olaleye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging task dependency and contrastive learning for Legal Judgement Prediction on the European Court of Human Rights. (arXiv:2302.00768v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00768","description":"<p>We report on an experiment in legal judgement prediction on European Court of\nHuman Rights cases where our model first learns to predict the convention\narticles allegedly violated by the state from case facts descriptions, and\nsubsequently utilizes that information to predict a finding of a violation by\nthe court. We assess the dependency between these two tasks at the feature and\noutcome level. Furthermore, we leverage a hierarchical contrastive loss to pull\ntogether article specific representations of cases at the higher level level,\nleading to distinctive article clusters, and further pulls the cases in each\narticle cluster based on their outcome leading to sub-clusters of cases with\nsimilar outcomes. Our experiment results demonstrate that, given a static\npre-trained encoder, our models produce a small but consistent improvement in\nprediction performance over single-task and joint models without contrastive\nloss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1\">Santosh T.Y.S.S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blas_M/0/1/0/all/0/1\">Marcel Perez San Blas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemper_P/0/1/0/all/0/1\">Phillip Kemper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabmair_M/0/1/0/all/0/1\">Matthias Grabmair</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User Study for Improving Tools for Bible Translation. (arXiv:2302.00778v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00778","description":"<p>Technology has increasingly become an integral part of the Bible translation\nprocess. Over time, both the translation process and relevant technology have\nevolved greatly. More recently, the field of Natural Language Processing (NLP)\nhas made great progress in solving some problems previously thought\nimpenetrable. Through this study we endeavor to better understand and\ncommunicate about a segment of the current landscape of the Bible translation\nprocess as it relates to technology and identify pertinent issues. We conduct\nseveral interviews with individuals working in different levels of the Bible\ntranslation process from multiple organizations to identify gaps and\nbottlenecks where technology (including recent advances in AI) could\npotentially play a pivotal role in reducing translation time and improving\noverall quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Joel Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermjakob_U/0/1/0/all/0/1\">Ulf Hermjakob</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Entity Alignment for Temporal Knowledge Graphs. (arXiv:2302.00796v1 [cs.IR])","link":"http://arxiv.org/abs/2302.00796","description":"<p>Entity alignment (EA) is a fundamental data integration task that identifies\nequivalent entities between different knowledge graphs (KGs). Temporal\nKnowledge graphs (TKGs) extend traditional knowledge graphs by introducing\ntimestamps, which have received increasing attention. State-of-the-art\ntime-aware EA studies have suggested that the temporal information of TKGs\nfacilitates the performance of EA. However, existing studies have not\nthoroughly exploited the advantages of temporal information in TKGs. Also, they\nperform EA by pre-aligning entity pairs, which can be labor-intensive and thus\ninefficient.\n</p>\n<p>In this paper, we present DualMatch which effectively fuses the relational\nand temporal information for EA. DualMatch transfers EA on TKGs into a weighted\ngraph matching problem. More specifically, DualMatch is equipped with an\nunsupervised method, which achieves EA without necessitating seed alignment.\nDualMatch has two steps: (i) encoding temporal and relational information into\nembeddings separately using a novel label-free encoder, Dual-Encoder; and (ii)\nfusing both information and transforming it into alignment using a novel\ngraph-matching-based decoder, GM-Decoder. DualMatch is able to perform EA on\nTKGs with or without supervision, due to its capability of effectively\ncapturing temporal information. Extensive experiments on three real-world TKG\ndatasets offer the insight that DualMatch outperforms the state-of-the-art\nmethods in terms of H@1 by 2.4% - 10.7% and MRR by 1.7% - 7.6%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunjun Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Rare Words Recognition through Homophone Extension and Unified Writing for Low-resource Cantonese Speech Recognition. (arXiv:2302.00836v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00836","description":"<p>Homophone characters are common in tonal syllable-based languages, such as\nMandarin and Cantonese. The data-intensive end-to-end Automatic Speech\nRecognition (ASR) systems are more likely to mis-recognize homophone characters\nand rare words under low-resource settings. For the problem of lowresource\nCantonese speech recognition, this paper presents a novel homophone extension\nmethod to integrate human knowledge of the homophone lexicon into the beam\nsearch decoding process with language model re-scoring. Besides, we propose an\nautomatic unified writing method to merge the variants of Cantonese characters\nand standardize speech annotation guidelines, which enables more efficient\nutilization of labeled utterances by providing more samples for the merged\ncharacters. We empirically show that both homophone extension and unified\nwriting improve the recognition performance significantly on both in-domain and\nout-of-domain test sets, with an absolute Character Error Rate (CER) decrease\nof around 5% and 18%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">HoLam Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu1_P/0/1/0/all/0/1\">Pengfei Liu1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_W/0/1/0/all/0/1\">Wai-Kim Leung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xixin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"idT5: Indonesian Version of Multilingual T5 Transformer. (arXiv:2302.00856v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00856","description":"<p>Indonesian language is spoken by almost 200 million people and is the 10th\nmost spoken language in the world, but it is under-represented in NLP (Natural\nLanguage Processing) research. A sparsity of language resources has hampered\nprevious work on Indonesian. The Transformer is a new architecture rapidly\nbecoming dominant for NLP, surpassing alternatives like convolutional and\nrecurrent neural networks. T5 (Text-to-Text Transfer Transformer) is a\nTransformer model that converts all text-based language problems to\ntext-to-text format for English. The multilingual variant is mT5 (multilingual\nT5) which has shown promising results on many NLP tasks across languages.\nHowever, the size of this multilingual model is a drawback for its application\nin real production applications, which sometimes require only one language. In\nthis study, the mT5 model was adapted for only one language, Indonesian,\nresulting in a pre-trained T5 model that was specific only for Indonesian with\na smaller size. For performance comparison, we fine-tuned this model and the\nmT5 model to the Sentiment Analysis (SA), Question Generation (QG), and\nQuestion Answering (QA) tasks with the exact mechanism and dataset. Fine-tuned\nmodel based on our model achieved 77.18% accuracy on SA, 8% higher than the\nmT5-based model, and obtained nearly the same score as the mT5-based model on\nQG and QA. The results confirm that it is possible to produce a smaller\npre-trained model that maintains comparable yields while reducing the model\nsize by up to 58%. In addition, the resulting model requires less memory, loads\nfaster, and inference times faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fuadi_M/0/1/0/all/0/1\">Mukhlish Fuadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wibawa_A/0/1/0/all/0/1\">Adhi Dharma Wibawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumpeno_S/0/1/0/all/0/1\">Surya Sumpeno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using In-Context Learning to Improve Dialogue Safety. (arXiv:2302.00871v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00871","description":"<p>While large neural-based conversational models have become increasingly\nproficient as dialogue agents, recent work has highlighted safety issues with\nthese systems. For example, these systems can be goaded into generating toxic\ncontent, which often perpetuates social biases or stereotypes. We investigate a\nretrieval-based framework for reducing bias and toxicity in responses generated\nfrom neural-based chatbots. It uses in-context learning to steer a model\ntowards safer generations. Concretely, to generate a response to an unsafe\ndialogue context, we retrieve demonstrations of safe model responses to similar\ndialogue contexts. We find our proposed approach performs competitively with\nstrong baselines which use fine-tuning. For instance, using automatic\nevaluation, we find our best fine-tuned baseline only generates safe responses\nto unsafe dialogue contexts from DiaSafety 2.92% more than our approach.\nFinally, we also propose a straightforward re-ranking procedure which can\nfurther improve response safeness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meade_N/0/1/0/all/0/1\">Nicholas Meade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gella_S/0/1/0/all/0/1\">Spandana Gella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Prakhar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-T&#xfc;r</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to choose \"Good\" Samples for Text Data Augmentation. (arXiv:2302.00894v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00894","description":"<p>Deep learning-based text classification models need abundant labeled data to\nobtain competitive performance. Unfortunately, annotating large-size corpus is\ntime-consuming and laborious. To tackle this, multiple researches try to use\ndata augmentation to expand the corpus size. However, data augmentation may\npotentially produce some noisy augmented samples. There are currently no works\nexploring sample selection for augmented samples in nature language processing\nfield. In this paper, we propose a novel self-training selection framework with\ntwo selectors to select the high-quality samples from data augmentation.\nSpecifically, we firstly use an entropy-based strategy and the model prediction\nto select augmented samples. Considering some samples with high quality at the\nabove step may be wrongly filtered, we propose to recall them from two\nperspectives of word overlap and semantic similarity. Experimental results show\nthe effectiveness and simplicity of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaotian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment. (arXiv:2302.00902v1 [cs.LG])","link":"http://arxiv.org/abs/2302.00902","description":"<p>Recent progress in scaling up large language models has shown impressive\ncapabilities in performing few-shot learning across a wide range of text-based\ntasks. However, a key limitation is that these language models fundamentally\nlack visual perception - a crucial attribute needed to extend these models to\nbe able to interact with the real world and solve vision tasks, such as in\nvisual-question answering and robotics. Prior works have largely connected\nimage to text through pretraining and/or fine-tuning on curated image-text\ndatasets, which can be a costly and expensive process. In order to resolve this\nlimitation, we propose a simple yet effective approach called\nLanguage-Quantized AutoEncoder (LQAE), a modification of VQ-VAE that learns to\nalign text-image data in an unsupervised manner by leveraging pretrained\nlanguage models (e.g., BERT, RoBERTa). Our main idea is to encode image as\nsequences of text tokens by directly quantizing image embeddings using a\npretrained language codebook. We then apply random masking followed by a BERT\nmodel, and have the decoder reconstruct the original image from BERT predicted\ntext token embeddings. By doing so, LQAE learns to represent similar images\nwith similar clusters of text tokens, thereby aligning these two modalities\nwithout the use of aligned text-image pairs. This enables few-shot image\nclassification with large language models (e.g., GPT-3) as well as linear\nclassification of images based on BERT text features. To the best of our\nknowledge, our work is the first work that uses unaligned images for multimodal\ntasks by leveraging the power of pretrained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wilson Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"History-Aware Hierarchical Transformer for Multi-session Open-domain Dialogue System. (arXiv:2302.00907v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00907","description":"<p>With the evolution of pre-trained language models, current open-domain\ndialogue systems have achieved great progress in conducting one-session\nconversations. In contrast, Multi-Session Conversation (MSC), which consists of\nmultiple sessions over a long term with the same user, is under-investigated.\nIn this paper, we propose History-Aware Hierarchical Transformer (HAHT) for\nmulti-session open-domain dialogue. HAHT maintains a long-term memory of\nhistory conversations and utilizes history information to understand current\nconversation context and generate well-informed and context-relevant responses.\nSpecifically, HAHT first encodes history conversation sessions hierarchically\ninto a history memory. Then, HAHT leverages historical information to\nfacilitate the understanding of the current conversation context by encoding\nthe history memory together with the current context with attention-based\nmechanisms. Finally, to explicitly utilize historical information, HAHT uses a\nhistory-aware response generator that switches between a generic vocabulary and\na history-aware vocabulary. Experimental results on a large-scale MSC dataset\nsuggest that the proposed HAHT model consistently outperforms baseline models.\nHuman evaluation results support that HAHT generates more human-like,\ncontext-relevant and history-relevant responses than baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhiwei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lizhen Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Chain-of-Thought Reasoning in Language Models. (arXiv:2302.00923v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00923","description":"<p>Large language models (LLMs) have shown impressive performance on complex\nreasoning by leveraging chain-of-thought (CoT) prompting to generate\nintermediate reasoning chains as the rationale to infer the answer. However,\nexisting CoT studies are mostly isolated in the language modality with LLMs,\nwhere LLMs are hard to deploy. To elicit CoT reasoning in multimodality, a\npossible solution is to fine-tune small language models by fusing the vision\nand language features to perform CoT reasoning. The key challenge is that those\nlanguage models tend to generate hallucinated reasoning chains that mislead the\nanswer inference. To mitigate the effect of such mistakes, we propose\nMultimodal-CoT that incorporates vision features in a decoupled training\nframework. The framework separates the rationale generation and answer\ninference into two stages. By incorporating the vision features in both stages,\nthe model is able to generate effective rationales that contribute to answer\ninference. With Multimodal-CoT, our model under 1 billion parameters\noutperforms the previous state-of-the-art LLM (GPT-3.5) by 16% (75.17%-&gt;91.68%)\non the ScienceQA benchmark and even surpasses human performance. Code is\npublicly available at https://github.com/amazon-science/mm-cot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alex Smola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Fewer Splits are Better: Deconstructing Readability in Sentence Splitting. (arXiv:2302.00937v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00937","description":"<p>In this work, we focus on sentence splitting, a subfield of text\nsimplification, motivated largely by an unproven idea that if you divide a\nsentence in pieces, it should become easier to understand. Our primary goal in\nthis paper is to find out whether this is true. In particular, we ask, does it\nmatter whether we break a sentence into two or three? We report on our findings\nbased on Amazon Mechanical Turk.\n</p>\n<p>More specifically, we introduce a Bayesian modeling framework to further\ninvestigate to what degree a particular way of splitting the complex sentence\naffects readability, along with a number of other parameters adopted from\ndiverse perspectives, including clinical linguistics, and cognitive\nlinguistics. The Bayesian modeling experiment provides clear evidence that\nbisecting the sentence leads to enhanced readability to a degree greater than\nwhat we create by trisection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nomoto_T/0/1/0/all/0/1\">Tadashi Nomoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransFool: An Adversarial Attack against Neural Machine Translation Models. (arXiv:2302.00944v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00944","description":"<p>Deep neural networks have been shown to be vulnerable to small perturbations\nof their inputs, known as adversarial attacks. In this paper, we investigate\nthe vulnerability of Neural Machine Translation (NMT) models to adversarial\nattacks and propose a new attack algorithm called TransFool. To fool NMT\nmodels, TransFool builds on a multi-term optimization problem and a gradient\nprojection step. By integrating the embedding representation of a language\nmodel, we generate fluent adversarial examples in the source language that\nmaintain a high level of semantic similarity with the clean samples.\nExperimental results demonstrate that, for different translation tasks and NMT\narchitectures, our white-box attack can severely degrade the translation\nquality while the semantic similarity between the original and the adversarial\nsentences stays high. Moreover, we show that TransFool is transferable to\nunknown target models. Finally, based on automatic and human evaluations,\nTransFool leads to improvement in terms of success rate, semantic similarity,\nand fluency compared to the existing attacks both in white-box and black-box\nsettings. Thus, TransFool permits us to better characterize the vulnerability\nof NMT models and outlines the necessity to design strong defense mechanisms\nand more robust NMT systems for real-life applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadrizadeh_S/0/1/0/all/0/1\">Sahar Sadrizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolamic_L/0/1/0/all/0/1\">Ljiljana Dolamic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curriculum-guided Abstractive Summarization for Mental Health Online Posts. (arXiv:2302.00954v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00954","description":"<p>Automatically generating short summaries from users' online mental health\nposts could save counselors' reading time and reduce their fatigue so that they\ncan provide timely responses to those seeking help for improving their mental\nstate. Recent Transformers-based summarization models have presented a\npromising approach to abstractive summarization. They go beyond sentence\nselection and extractive strategies to deal with more complicated tasks such as\nnovel word generation and sentence paraphrasing. Nonetheless, these models have\na prominent shortcoming; their training strategy is not quite efficient, which\nrestricts the model's performance. In this paper, we include a curriculum\nlearning approach to reweigh the training samples, bringing about an efficient\nlearning procedure. We apply our model on extreme summarization dataset of\nMentSum posts -- a dataset of mental health related posts from Reddit social\nmedia. Compared to the state-of-the-art model, our proposed method makes\nsubstantial gains in terms of Rouge and Bertscore evaluation metrics, yielding\n3.5% (Rouge-1), 10.4% (Rouge-2), and 4.7% (Rouge-L), 1.5% (Bertscore) relative\nimprovements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sotudeh_S/0/1/0/all/0/1\">Sajad Sotudeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goharian_N/0/1/0/all/0/1\">Nazli Goharian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deilamsalehy_H/0/1/0/all/0/1\">Hanieh Deilamsalehy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predefined domain specific embeddings of food concepts and recipes: A case study on heterogeneous recipe datasets. (arXiv:2302.01005v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01005","description":"<p>Although recipe data are very easy to come by nowadays, it is really hard to\nfind a complete recipe dataset - with a list of ingredients, nutrient values\nper ingredient, and per recipe, allergens, etc. Recipe datasets are usually\ncollected from social media websites where users post and publish recipes.\nUsually written with little to no structure, using both standardized and\nnon-standardized units of measurement. We collect six different recipe\ndatasets, publicly available, in different formats, and some including data in\ndifferent languages. Bringing all of these datasets to the needed format for\napplying a machine learning (ML) pipeline for nutrient prediction [1], [2],\nincludes data normalization using dictionary-based named entity recognition\n(NER), rule-based NER, as well as conversions using external domain-specific\nresources. From the list of ingredients, domain-specific embeddings are created\nusing the same embedding space for all recipes - one ingredient dataset is\ngenerated. The result from this normalization process is two corpora - one with\npredefined ingredient embeddings and one with predefined recipe embeddings. On\nall six recipe datasets, the ML pipeline is evaluated. The results from this\nuse case also confirm that the embeddings merged using the domain heuristic\nyield better results than the baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ispirova_G/0/1/0/all/0/1\">Gordana Ispirova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eftimov_T/0/1/0/all/0/1\">Tome Eftimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seljak_B/0/1/0/all/0/1\">Barbara Korou&#x161;i&#x107; Seljak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Coherence Markers for the Early Diagnosis of the Alzheimer Disease. (arXiv:2302.01025v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01025","description":"<p>In this work we explore how language models can be employed to analyze\nlanguage and discriminate between mentally impaired and healthy subjects\nthrough the perplexity metric. Perplexity was originally conceived as an\ninformation-theoretic measure to assess how much a given language model is\nsuited to predict a text sequence or, equivalently, how much a word sequence\nfits into a specific language model. We carried out an extensive\nexperimentation with the publicly available data, and employed language models\nas diverse as N-grams, from 2-grams to 5-grams, and GPT-2, a transformer-based\nlanguage model. We investigated whether perplexity scores may be used to\ndiscriminate between the transcripts of healthy subjects and subjects suffering\nfrom Alzheimer Disease (AD). Our best performing models achieved full accuracy\nand F-score (1.00 in both precision/specificity and recall/sensitivity) in\ncategorizing subjects from both the AD class and control subjects. These\nresults suggest that perplexity can be a valuable analytical metrics with\npotential application to supporting early diagnosis of symptoms of mental\ndisorders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colla_D/0/1/0/all/0/1\">Davide Colla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delsanto_M/0/1/0/all/0/1\">Matteo Delsanto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agosto_M/0/1/0/all/0/1\">Marco Agosto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitiello_B/0/1/0/all/0/1\">Benedetto Vitiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radicioni_D/0/1/0/all/0/1\">Daniele Paolo Radicioni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New Linear-time Algorithm for SubTree Kernel Computation based on Root-Weighted Tree Automata. (arXiv:2302.01097v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01097","description":"<p>Tree kernels have been proposed to be used in many areas as the automatic\nlearning of natural language applications. In this paper, we propose a new\nlinear time algorithm based on the concept of weighted tree automata for\nSubTree kernel computation. First, we introduce a new class of weighted tree\nautomata, called Root-Weighted Tree Automata, and their associated formal tree\nseries. Then we define, from this class, the SubTree automata that represent\ncompact computational models for finite tree languages. This allows us to\ndesign a theoretically guaranteed linear-time algorithm for computing the\nSubTree Kernel based on weighted tree automata intersection. The key idea\nbehind the proposed algorithm is to replace DAG reduction and nodes sorting\nsteps used in previous approaches by states equivalence classes computation\nallowed in the weighted tree automata approach. Our approach has three major\nadvantages: it is output-sensitive, it is free sensitive from the tree types\n(ordered trees versus unordered trees), and it is well adapted to any\nincremental tree kernel based learning methods. Finally, we conduct a variety\nof comparative experiments on a wide range of synthetic tree languages datasets\nadapted for a deep algorithm analysis. The obtained results show that the\nproposed algorithm outperforms state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mignot_L/0/1/0/all/0/1\">Ludovic Mignot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouardi_F/0/1/0/all/0/1\">Faissal Ouardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziadi_D/0/1/0/all/0/1\">Djelloul Ziadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Deep Neural Reranking and Unsupervised Extraction for Multi-Query Focused Summarization. (arXiv:2302.01148v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01148","description":"<p>The CrisisFACTS Track aims to tackle challenges such as multi-stream\nfact-finding in the domain of event tracking; participants' systems extract\nimportant facts from several disaster-related events while incorporating the\ntemporal order. We propose a combination of retrieval, reranking, and the\nwell-known Integer Linear Programming (ILP) and Maximal Marginal Relevance\n(MMR) frameworks. In the former two modules, we explore various methods\nincluding an entity-based baseline, pre-trained and fine-tuned Question\nAnswering systems, and ColBERT. We then use the latter module as an extractive\nsummarization component by taking diversity and novelty criteria into account.\nThe automatic scoring runs show strong results across the evaluation setups but\nalso reveal shortcomings and challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seeberger_P/0/1/0/all/0/1\">Philipp Seeberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedhammer_K/0/1/0/all/0/1\">Korbinian Riedhammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How learners produce data from text in classifying clickbait. (arXiv:2302.01292v1 [cs.CY])","link":"http://arxiv.org/abs/2302.01292","description":"<p>Text provides a compelling example of unstructured data that can be used to\nmotivate and explore classification problems. Challenges arise regarding the\nrepresentation of features of text and student linkage between text\nrepresentations as character strings and identification of features that embed\nconnections with underlying phenomena. In order to observe how students reason\nwith text data in scenarios designed to elicit certain aspects of the domain,\nwe employed a task-based interview method using a structured protocol with six\npairs of undergraduate students. Our goal was to shed light on students'\nunderstanding of text as data using a motivating task to classify headlines as\n\"clickbait\" or \"news\". Three types of features (function, content, and form)\nsurfaced, the majority from the first scenario. Our analysis of the interviews\nindicates that this sequence of activities engaged the participants in thinking\nat both the human-perception level and the computer-extraction level and\nconceptualizing connections between them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horton_N/0/1/0/all/0/1\">Nicholas J. Horton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_J/0/1/0/all/0/1\">Jie Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_P/0/1/0/all/0/1\">Phebe Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finzer_W/0/1/0/all/0/1\">William Finzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Language Reveals about Perception: Distilling Psychophysical Knowledge from Large Language Models. (arXiv:2302.01308v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01308","description":"<p>Understanding the extent to which the perceptual world can be recovered from\nlanguage is a fundamental problem in cognitive science. We reformulate this\nproblem as that of distilling psychophysical information from text and show how\nthis can be done by combining large language models (LLMs) with a classic\npsychophysical method based on similarity judgments. Specifically, we use the\nprompt auto-completion functionality of GPT3, a state-of-the-art LLM, to elicit\nsimilarity scores between stimuli and then apply multidimensional scaling to\nuncover their underlying psychological space. We test our approach on six\nperceptual domains and show that the elicited judgments strongly correlate with\nhuman data and successfully recover well-known psychophysical structures such\nas the color wheel and pitch spiral. We also explore meaningful divergences\nbetween LLM and human representations. Our work showcases how combining\nstate-of-the-art machine models with well-known cognitive paradigms can shed\nnew light on fundamental questions in perception and language research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1\">Raja Marjieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sucholutsky_I/0/1/0/all/0/1\">Ilia Sucholutsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijn_P/0/1/0/all/0/1\">Pol van Rijn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v1 [cs.LG])","link":"http://arxiv.org/abs/2302.01313","description":"<p>This work provides a formalization of Knowledge Graphs (KGs) as a new class\nof graphs that we denote doubly exchangeable attributed graphs, where node and\npairwise (joint 2-node) representations must be equivariant to permutations of\nboth node ids and edge (&amp; node) attributes (relations &amp; node features).\nDouble-permutation equivariant KG representations open a new research direction\nin KGs. We show that this equivariance imposes a structural representation of\nrelations that allows neural networks to perform complex logical reasoning\ntasks in KGs. Finally, we introduce a general blueprint for such equivariant\nrepresentations and test a simple GNN-based double-permutation equivariant\nneural architecture that achieve 100% Hits@10 test accuracy in both the\nWN18RRv1 and NELL995v1 inductive KG completion tasks, and can accurately\nperform logical reasoning tasks that no existing methods can perform, to the\nbest of our knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yangze Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1\">Bruno Ribeiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Large Language Model Decoding with Speculative Sampling. (arXiv:2302.01318v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01318","description":"<p>We present speculative sampling, an algorithm for accelerating transformer\ndecoding by enabling the generation of multiple tokens from each transformer\ncall. Our algorithm relies on the observation that the latency of parallel\nscoring of short continuations, generated by a faster but less powerful draft\nmodel, is comparable to that of sampling a single token from the larger target\nmodel. This is combined with a novel modified rejection sampling scheme which\npreserves the distribution of the target model within hardware numerics. We\nbenchmark speculative sampling with Chinchilla, a 70 billion parameter language\nmodel, achieving a 2-2.5x decoding speedup in a distributed setup, without\ncompromising the sample quality or making modifications to the model itself.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Charlie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1\">Geoffrey Irving</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lespiau_J/0/1/0/all/0/1\">Jean-Baptiste Lespiau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifre_L/0/1/0/all/0/1\">Laurent Sifre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jumper_J/0/1/0/all/0/1\">John Jumper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$IC^3$: Image Captioning by Committee Consensus. (arXiv:2302.01328v1 [cs.CV])","link":"http://arxiv.org/abs/2302.01328","description":"<p>If you ask a human to describe an image, they might do so in a thousand\ndifferent ways. Traditionally, image captioning models are trained to\napproximate the reference distribution of image captions, however, doing so\nencourages captions that are viewpoint-impoverished. Such captions often focus\non only a subset of the possible details, while ignoring potentially useful\ninformation in the scene. In this work, we introduce a simple, yet novel,\nmethod: \"Image Captioning by Committee Consensus\" ($IC^3$), designed to\ngenerate a single caption that captures high-level details from several\nviewpoints. Notably, humans rate captions produced by $IC^3$ at least as\nhelpful as baseline SOTA models more than two thirds of the time, and $IC^3$\ncaptions can improve the performance of SOTA automated recall systems by up to\n84%, indicating significant material improvements over existing SOTA approaches\nfor visual description. Our code is publicly available at\nhttps://github.com/DavidMChan/caption-by-committee\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1\">David M. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myers_A/0/1/0/all/0/1\">Austin Myers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayanarasimhan_S/0/1/0/all/0/1\">Sudheendra Vijayanarasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1\">David A. Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canny_J/0/1/0/all/0/1\">John Canny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08063","description":"<p>Knowledge Extraction (KE), aiming to extract structural information from\nunstructured texts, often suffers from data scarcity and emerging unseen types,\ni.e., low-resource scenarios. Many neural approaches to low-resource KE have\nbeen widely investigated and achieved impressive performance. In this paper, we\npresent a literature review towards KE in low-resource scenarios, and\nsystematically categorize existing works into three paradigms: (1) exploiting\nhigher-resource data, (2) exploiting stronger models, and (3) exploiting data\nand models together. In addition, we highlight promising applications and\noutline some potential directions for future research. We hope that our survey\ncan help both the academic and industrial communities to better understand this\nfield, inspire more ideas, and boost broader applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GausSetExpander: A Simple Approach for Entity Set Expansion. (arXiv:2202.13649v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13649","description":"<p>Entity Set Expansion is an important NLP task that aims at expanding a small\nset of entities into a larger one with items from a large pool of candidates.\nIn this paper, we propose GausSetExpander, an unsupervised approach based on\noptimal transport techniques. We propose to re-frame the problem as choosing\nthe entity that best completes the seed set. For this, we interpret a set as an\nelliptical distribution with a centroid which represents the mean and a spread\nthat is represented by the scale parameter. The best entity is the one that\nincreases the spread of the set the least. We demonstrate the validity of our\napproach by comparing to state-of-the art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diallo_A/0/1/0/all/0/1\">A&#xef;ssatou Diallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1\">Johannes F&#xfc;rnkranz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08657","description":"<p>Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a deep cross-modal encoder, or feed the\nlast-layer uni-modal representations from the deep pre-trained uni-modal\nencoders into the top cross-modal encoder. Both approaches potentially restrict\nvision-language representation learning and limit model performance. In this\npaper, we propose BridgeTower, which introduces multiple bridge layers that\nbuild a connection between the top layers of uni-modal encoders and each layer\nof the cross-modal encoder. This enables effective bottom-up cross-modal\nalignment and fusion between visual and textual representations of different\nsemantic levels of pre-trained uni-modal encoders in the cross-modal encoder.\nPre-trained with only 4M images, BridgeTower achieves state-of-the-art\nperformance on various downstream vision-language tasks. In particular, on the\nVQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming\nthe previous state-of-the-art model METER by 1.09% with the same pre-training\ndata and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, BridgeTower achieves an accuracy of\n81.15%, surpassing models that are pre-trained on orders-of-magnitude larger\ndatasets. Code and checkpoints are available at\nhttps://github.com/microsoft/BridgeTower.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenman_S/0/1/0/all/0/1\">Shachar Rosenman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search. (arXiv:2207.09068v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.09068","description":"<p>While contextualized word embeddings have been a de-facto standard, learning\ncontextualized phrase embeddings is less explored and being hindered by the\nlack of a human-annotated benchmark that tests machine understanding of phrase\nsemantics given a context sentence or paragraph (instead of phrases alone). To\nfill this gap, we propose PiC -- a dataset of ~28K of noun phrases accompanied\nby their contextual Wikipedia pages and a suite of three tasks for training and\nevaluating phrase embeddings. Training on PiC improves ranking models' accuracy\nand remarkably pushes span-selection (SS) models (i.e., predicting the start\nand end index of the target phrase) near-human accuracy, which is 95% Exact\nMatch (EM) on semantic search given a query phrase and a passage.\nInterestingly, we find evidence that such impressive performance is because the\nSS models learn to better capture the common meaning of a phrase regardless of\nits actual context. SotA models perform poorly in distinguishing two senses of\nthe same phrase in two contexts (~60% EM) and in estimating the similarity\nbetween two different phrases in the same context (~70% EM).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thang M. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarization Programs: Interpretable Abstractive Summarization with Neural Modular Trees. (arXiv:2209.10492v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.10492","description":"<p>Current abstractive summarization models either suffer from a lack of clear\ninterpretability or provide incomplete rationales by only highlighting parts of\nthe source document. To this end, we propose the Summarization Program (SP), an\ninterpretable modular framework consisting of an (ordered) list of binary\ntrees, each encoding the step-by-step generative process of an abstractive\nsummary sentence from the source document. A Summarization Program contains one\nroot node per summary sentence, and a distinct tree connects each summary\nsentence (root node) to the document sentences (leaf nodes) from which it is\nderived, with the connecting nodes containing intermediate generated sentences.\nEdges represent different modular operations involved in summarization such as\nsentence fusion, compression, and paraphrasing. We first propose an efficient\nbest-first search method over neural modules, SP-Search that identifies SPs for\nhuman summaries by directly optimizing for ROUGE scores. Next, using these\nprograms as automatic supervision, we propose seq2seq models that generate\nSummarization Programs, which are then executed to obtain final summaries. We\ndemonstrate that SP-Search effectively represents the generative process behind\nhuman summaries using modules that are typically faithful to their intended\nbehavior. We also conduct a simulation study to show that Summarization\nPrograms improve the interpretability of summarization models by allowing\nhumans to better simulate model reasoning. Summarization Programs constitute a\npromising step toward interpretable and modular abstractive summarization, a\ncomplex task previously addressed primarily through blackbox end-to-end neural\nsystems. Supporting code available at\nhttps://github.com/swarnaHub/SummarizationPrograms\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Swarnadeep Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESIE-BERT: Enriching Sub-words Information Explicitly with BERT for Joint Intent Classification and SlotFilling. (arXiv:2211.14829v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.14829","description":"<p>Natural language understanding (NLU) has two core tasks: intent\nclassification and slot filling. The success of pre-training language models\nresulted in a significant breakthrough in the two tasks. One of the promising\nsolutions called BERT can jointly optimize the two tasks. We note that\nBERT-based models convert each complex token into multiple sub-tokens by\nwordpiece algorithm, which generates a mismatch between the lengths of the\ntokens and the labels. This leads to BERT-based models do not do well in label\nprediction which limits model performance improvement. Many existing models can\nbe compatible with this issue but some hidden semantic information is discarded\nin the fine-tuning process. We address the problem by introducing a novel joint\nmethod on top of BERT which explicitly models the multiple sub-tokens features\nafter wordpiece tokenization, thereby contributing to the two tasks. Our method\ncan well extract the contextual features from complex tokens by the proposed\nsub-words attention adapter (SAA), which preserves overall utterance\ninformation. Additionally, we propose an intent attention adapter (IAA) to\nobtain the full sentence features to aid users to predict intent. Experimental\nresults confirm that our proposed model is significantly improved on two public\nbenchmark datasets. In particular, the slot filling F1 score is improved from\n96.1 to 98.2 (2.1% absolute) on the Airline Travel Information Systems (ATIS)\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhilong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huangen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leilei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Huaming Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shaopeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Table-and-Text HybridQA: Concepts, Methods, Challenges and Future Directions. (arXiv:2212.13465v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.13465","description":"<p>Table-and-text hybrid question answering (HybridQA) is a widely used and\nchallenging NLP task commonly applied in the financial and scientific domain.\nThe early research focuses on migrating other QA task methods to HybridQA,\nwhile with further research, more and more HybridQA-specific methods have been\npresent. With the rapid development of HybridQA, the systematic survey is still\nunder-explored to summarize the main techniques and advance further research.\nSo we present this work to summarize the current HybridQA benchmarks and\nmethods, then analyze the challenges and future directions of this task. The\ncontributions of this paper can be summarized in three folds: (1) first survey,\nto our best knowledge, including benchmarks, methods and challenges for\nHybridQA; (2) systematic investigation with the reasonable comparison of the\nexisting systems to articulate their advantages and shortcomings; (3) detailed\nanalysis of challenges in four important dimensions to shed light on future\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingzirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_L/0/1/0/all/0/1\">Longxu Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammar construction methods for extended deterministic expressions. (arXiv:2301.01621v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.01621","description":"<p>Extended regular expressions with counting and interleaving are widely used\nin practice. However the related theoretical studies for this kind of\nexpressions currently cannot meet the need of practical work. This paper\ndevelops syntax definitions for extended deterministic expressions and their\nsubclasses, hope to completely solve the long-standing problem that there are\nno syntax definitions for this kind of expressions, which has become an\nimportant reason for restricting the use of extended expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mou_X/0/1/0/all/0/1\">Xiaoying Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithful Chain-of-Thought Reasoning. (arXiv:2301.13379v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.13379","description":"<p>While Chain-of-Thought (CoT) prompting boosts Language Models' (LM)\nperformance on a gamut of complex reasoning tasks, the generated reasoning\nchain does not necessarily reflect how the model arrives at the answer (aka.\nfaithfulness). We propose Faithful CoT, a faithful-by-construction framework\nthat decomposes a reasoning task into two stages: Translation (Natural Language\nquery $\\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning\nchain $\\rightarrow$ answer), using an LM and a deterministic solver\nrespectively. We demonstrate the efficacy of our approach on 10 reasoning\ndatasets from 4 diverse domains. It outperforms traditional CoT prompting on 9\nout of the 10 datasets, with an average accuracy gain of 4.4 on Math Word\nProblems, 1.9 on Planning, 4.0 on Multi-hop Question Answering (QA), and 18.1\non Logical Inference, under greedy decoding. Together with self-consistency\ndecoding, we achieve new state-of-the-art few-shot performance on 7 out of the\n10 datasets, showing a strong synergy between faithfulness and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1\">Shreya Havaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_A/0/1/0/all/0/1\">Adam Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1\">Delip Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1\">Eric Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1\">Marianna Apidianaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection. (arXiv:1910.11621v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/1910.11621","description":"<p>Event detection (ED), a sub-task of event extraction, involves identifying\ntriggers and categorizing event mentions. Existing methods primarily rely upon\nsupervised learning and require large-scale labeled event datasets which are\nunfortunately not readily available in many real-life applications. In this\npaper, we consider and reformulate the ED task with limited labeled data as a\nFew-Shot Learning problem. We propose a Dynamic-Memory-Based Prototypical\nNetwork (DMB-PN), which exploits Dynamic Memory Network (DMN) to not only learn\nbetter prototypes for event types, but also produce more robust sentence\nencodings for event mentions. Differing from vanilla prototypical networks\nsimply computing event prototypes by averaging, which only consume event\nmentions once, our model is more robust and is capable of distilling contextual\ninformation from event mentions for multiple times due to the multi-hop\nmechanism of DMNs. The experiments show that DMB-PN not only deals with sample\nscarcity better than a series of baseline models but also performs more\nrobustly when the variety of event types is relatively large and the instance\nquantity is extremely small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jiaojian Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OntoED: Low-resource Event Detection with Ontology Embedding. (arXiv:2105.10922v4 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/2105.10922","description":"<p>Event Detection (ED) aims to identify event trigger words from a given text\nand classify it into an event type. Most of current methods to ED rely heavily\non training instances, and almost ignore the correlation of event types. Hence,\nthey tend to suffer from data scarcity and fail to handle new unseen event\ntypes. To address these problems, we formulate ED as a process of event\nontology population: linking event instances to pre-defined event types in\nevent ontology, and propose a novel ED framework entitled OntoED with ontology\nembedding. We enrich event ontology with linkages among event types, and\nfurther induce more event-event correlations. Based on the event ontology,\nOntoED can leverage and propagate correlation knowledge, particularly from\ndata-rich to data-poor event types. Furthermore, OntoED can be applied to new\nunseen event types, by establishing linkages to existing ones. Experiments\nindicate that OntoED is more predominant and robust than previous approaches to\nED, especially in data-scarce scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction and Applications of Billion-Scale Pre-trained Multimodal Business Knowledge Graph. (arXiv:2209.15214v3 [cs.AI] CROSS LISTED)","link":"http://arxiv.org/abs/2209.15214","description":"<p>Business Knowledge Graphs (KGs) are important to many enterprises today,\nproviding factual knowledge and structured data that steer many products and\nmake them more intelligent. Despite their promising benefits, building business\nKG necessitates solving prohibitive issues of deficient structure and multiple\nmodalities. In this paper, we advance the understanding of the practical\nchallenges related to building KG in non-trivial real-world systems. We\nintroduce the process of building an open business knowledge graph (OpenBG)\nderived from a well-known enterprise, Alibaba Group. Specifically, we define a\ncore ontology to cover various abstract products and consumption demands, with\nfine-grained taxonomy and multimodal facts in deployed applications. OpenBG is\nan open business KG of unprecedented scale: 2.6 billion triples with more than\n88 million entities covering over 1 million core classes/concepts and 2,681\ntypes of relations. We release all the open resources (OpenBG benchmarks)\nderived from it for the community and report experimental results of KG-centric\ntasks. We also run up an online competition based on OpenBG benchmarks, and has\nattracted thousands of teams. We further pre-train OpenBG and apply it to many\nKG- enhanced downstream tasks in business scenarios, demonstrating the\neffectiveness of billion-scale multimodal knowledge for e-commerce. All the\nresources with codes have been released at\n\\url{https://github.com/OpenBGBenchmark/OpenBG}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zelin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hehong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}