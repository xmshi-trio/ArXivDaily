{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-26T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Multi-Lingual DALL-E Storytime. (arXiv:2212.11985v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11985","description":"<p>While recent advancements in artificial intelligence (AI) language models\ndemonstrate cutting-edge performance when working with English texts,\nequivalent models do not exist in other languages or do not reach the same\nperformance level. This undesired effect of AI advancements increases the gap\nbetween access to new technology from different populations across the world.\nThis unsought bias mainly discriminates against individuals whose English\nskills are less developed, e.g., non-English speakers children. Following\nsignificant advancements in AI research in recent years, OpenAI has recently\npresented DALL-E: a powerful tool for creating images based on English text\nprompts. While DALL-E is a promising tool for many applications, its decreased\nperformance when given input in a different language, limits its audience and\ndeepens the gap between populations. An additional limitation of the current\nDALL-E model is that it only allows for the creation of a few images in\nresponse to a given input prompt, rather than a series of consecutive coherent\nframes that tell a story or describe a process that changes over time. Here, we\npresent an easy-to-use automatic DALL-E storytelling framework that leverages\nthe existing DALL-E model to enable fast and coherent visualizations of\nnon-English songs and stories, pushing the limit of the one-step-at-a-time\noption DALL-E currently offers. We show that our framework is able to\neffectively visualize stories from non-English texts and portray the changes in\nthe plot over time. It is also able to create a narrative and maintain\ninterpretable changes in the description across frames. Additionally, our\nframework offers users the ability to specify constraints on the story\nelements, such as a specific location or context, and to maintain a consistent\nstyle throughout the visualization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mudrik_N/0/1/0/all/0/1\">Noga Mudrik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charles_A/0/1/0/all/0/1\">Adam S. Charles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization. (arXiv:2212.12017v1 [cs.CL])","link":"http://arxiv.org/abs/2212.12017","description":"<p>Recent work has shown that fine-tuning large pre-trained language models on a\ncollection of tasks described via instructions, a.k.a. instruction-tuning,\nimproves their zero and few-shot generalization to unseen tasks. However, there\nis a limited understanding of the performance trade-offs of different decisions\nmade during the instruction-tuning process. These decisions include the scale\nand diversity of the instruction-tuning benchmark, different task sampling\nstrategies, fine-tuning with and without demonstrations, training using\nspecialized datasets for reasoning and dialogue, and finally, the fine-tuning\nobjectives themselves. In this paper, we characterize the effect of\ninstruction-tuning decisions on downstream task performance when scaling both\nmodel and benchmark sizes. To this end, we create OPT-IML Bench: a large\nbenchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated\ninto task categories from 8 existing benchmarks, and prepare an evaluation\nframework to measure three types of model generalizations: to tasks from fully\nheld-out categories, to held-out tasks from seen categories, and to held-out\ninstances from seen tasks. Through the lens of this framework, we first present\ninsights about instruction-tuning decisions as applied to OPT-30B and further\nexploit these insights to train OPT-IML 30B and 175B, which are\ninstruction-tuned versions of OPT. OPT-IML demonstrates all three\ngeneralization abilities at both scales on four different evaluation benchmarks\nwith diverse tasks and input formats -- PromptSource, FLAN,\nSuper-NaturalInstructions, and UnifiedSKG. Not only does it significantly\noutperform OPT on all benchmarks but is also highly competitive with existing\nmodels fine-tuned on each specific benchmark. We release OPT-IML at both\nscales, together with the OPT-IML Bench evaluation framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Srinivasan Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simig_D/0/1/0/all/0/1\">Daniel Simig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Ping Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koura_P/0/1/0/all/0/1\">Punit Singh Koura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OHoro_B/0/1/0/all/0/1\">Brian O&#x27;Horo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereyra_G/0/1/0/all/0/1\">Gabriel Pereyra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jeff Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dewan_C/0/1/0/all/0/1\">Christopher Dewan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Ves Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When are Lemons Purple? The Concept Association Bias of CLIP. (arXiv:2212.12043v1 [cs.CV])","link":"http://arxiv.org/abs/2212.12043","description":"<p>Large-scale vision-language models such as CLIP have shown impressive\nperformance on zero-shot image classification and image-to-text retrieval.\nHowever, such zero-shot performance of CLIP-based models does not realize in\ntasks that require a finer-grained correspondence between vision and language,\nsuch as Visual Question Answering (VQA). We investigate why this is the case,\nand report an interesting phenomenon of CLIP, which we call the Concept\nAssociation Bias (CAB), as a potential cause of the difficulty of applying CLIP\nto VQA and similar tasks. CAB is especially apparent when two concepts are\npresent in the given image while a text prompt only contains a single concept.\nIn such a case, we find that CLIP tends to treat input as a bag of concepts and\nattempts to fill in the other missing concept crossmodally, leading to an\nunexpected zero-shot prediction. For example, when asked for the color of a\nlemon in an image, CLIP predicts ``purple'' if the image contains a lemon and\nan eggplant. We demonstrate the Concept Association Bias of CLIP by showing\nthat CLIP's zero-shot classification performance greatly suffers when there is\na strong concept association between an object (e.g. lemon) and an attribute\n(e.g. its color). On the other hand, when the association between object and\nattribute is weak, we do not see this phenomenon. Furthermore, we show that CAB\nis significantly mitigated when we enable CLIP to learn deeper structure across\nimage and text embeddings by adding an additional Transformer on top of CLIP\nand fine-tuning it on VQA. We find that across such fine-tuned variants of\nCLIP, the strength of CAB in a model predicts how well it performs on VQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamada_Y/0/1/0/all/0/1\">Yutaro Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yingtian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yildirim_I/0/1/0/all/0/1\">Ilker Yildirim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing the performances of ASR models on English and Spanish accents. (arXiv:2212.12048v1 [cs.CL])","link":"http://arxiv.org/abs/2212.12048","description":"<p>Speech to text models tend to be trained and evaluated against a single\ntarget accent. This is especially true for English for which native speakers\nfrom the United States became the main benchmark. In this work, we are going to\nshow how two simple methods: pre-trained embeddings and auxiliary\nclassification losses can improve the performance of ASR systems. We are\nlooking for upgrades as universal as possible and therefore we will explore\ntheir impact on several models architectures and several languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chitkara_P/0/1/0/all/0/1\">Pooja Chitkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1\">Morgane Riviere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Frank Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1\">Yatharth Saraf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification. (arXiv:2212.12061v1 [cs.CL])","link":"http://arxiv.org/abs/2212.12061","description":"<p>This article presents a dataset of 10,917 news articles with hierarchical\nnews categories collected between January 1st 2019, and December 31st 2019. We\nmanually labelled the articles based on a hierarchical taxonomy with 17\nfirst-level and 109 second-level categories. This dataset can be used to train\nmachine learning models for automatically classifying news articles by topic.\nThis dataset can be helpful for researchers working on news structuring,\nclassification, and predicting future events based on released news.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petukhova_A/0/1/0/all/0/1\">Alina Petukhova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fachada_N/0/1/0/all/0/1\">Nuno Fachada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?. (arXiv:2212.12131v1 [cs.CL])","link":"http://arxiv.org/abs/2212.12131","description":"<p>This work presents a detailed linguistic analysis into why larger\nTransformer-based pre-trained language models with more parameters and lower\nperplexity nonetheless yield surprisal estimates that are less predictive of\nhuman reading times. First, regression analyses show a strictly monotonic,\npositive log-linear relationship between perplexity and fit to reading times\nfor the more recently released five GPT-Neo variants and eight OPT variants on\ntwo separate datasets, replicating earlier results limited to just GPT-2 (Oh et\nal., 2022). Subsequently, analysis of residual errors reveals a systematic\ndeviation of the larger variants, such as underpredicting reading times of\nnamed entities and making compensatory overpredictions for reading times of\nfunction words such as modals and conjunctions. These results suggest that the\npropensity of larger Transformer-based models to 'memorize' sequences during\ntraining makes their surprisal estimates diverge from humanlike expectations,\nwhich warrants caution in using pre-trained language models to study human\nlanguage processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_B/0/1/0/all/0/1\">Byung-Doh Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuler_W/0/1/0/all/0/1\">William Schuler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing. (arXiv:2212.12137v1 [cs.CL])","link":"http://arxiv.org/abs/2212.12137","description":"<p>We investigate how humans perform the task of dubbing video content from one\nlanguage into another, leveraging a novel corpus of 319.57 hours of video from\n54 professionally produced titles. This is the first such large-scale study we\nare aware of. The results challenge a number of assumptions commonly made in\nboth qualitative literature on human dubbing and machine-learning literature on\nautomatic dubbing, arguing for the importance of vocal naturalness and\ntranslation quality over commonly emphasized isometric (character length) and\nlip-sync constraints, and for a more qualified view of the importance of\nisochronic (timing) constraints. We also find substantial influence of the\nsource-side audio on human dubs through channels other than the words of the\ntranslation, pointing to the need for research on ways to preserve speech\ncharacteristics, as well as semantic transfer such as emphasis/emotion, in\nautomatic dubbing systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brannon_W/0/1/0/all/0/1\">William Brannon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virkar_Y/0/1/0/all/0/1\">Yogesh Virkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1\">Brian Thompson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Generate Questions by Enhancing Text Generation with Sentence Selection. (arXiv:2212.12192v1 [cs.CL])","link":"http://arxiv.org/abs/2212.12192","description":"<p>We introduce an approach for the answer-aware question generation problem.\nInstead of only relying on the capability of strong pre-trained language\nmodels, we observe that the information of answers and questions can be found\nin some relevant sentences in the context. Based on that, we design a model\nwhich includes two modules: a selector and a generator. The selector forces the\nmodel to more focus on relevant sentences regarding an answer to provide\nimplicit local information. The generator generates questions by implicitly\ncombining local information from the selector and global information from the\nwhole context encoded by the encoder. The model is trained jointly to take\nadvantage of latent interactions between the two modules. Experimental results\non two benchmark datasets show that our model is better than strong pre-trained\nmodels for the question generation task. The code is also available\n(shorturl.at/lV567).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Do Hoang Thai Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_N/0/1/0/all/0/1\">Nguyen Hong Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Tien Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuning for Sarcasm Detection with a Pruned Dataset. (arXiv:2212.12213v1 [cs.CL])","link":"http://arxiv.org/abs/2212.12213","description":"<p>Sarcasm is a form of irony that involves saying or writing something that is\nopposite or opposite to what one really means, often in a humorous or mocking\nway. It is often used to mock or mock someone or something, or to be humorous\nor amusing. Sarcasm is usually conveyed through tone of voice, facial\nexpressions, or other forms of nonverbal communication, but it can also be\nindicated by the use of certain words or phrases that are typically associated\nwith irony or humor. Sarcasm detection is difficult because it relies on\ncontext and non-verbal cues. It can also be culturally specific, subjective and\nambiguous. In this work, we fine-tune the RoBERTa based sarcasm detection model\npresented in Abaskohi et al. [2022] to get to within 0.02 F1 of the\nstate-of-the-art (Hercog et al. [2022]) on the iSarcasm dataset (Oprea and\nMagdy [2019]). This performance is achieved by augmenting iSarcasm with a\npruned version of the Self Annotated Reddit Corpus (SARC) (Khodak et al.\n[2017]). Our pruned version is 100 times smaller than the subset of SARC used\nto train the state-of-the-art model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_I/0/1/0/all/0/1\">Ishita Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandia_P/0/1/0/all/0/1\">Priyank Bhandia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dulam_S/0/1/0/all/0/1\">Sanjana Dulam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Judgement's Premises Towards Key Points. (arXiv:2212.12238v1 [cs.CL])","link":"http://arxiv.org/abs/2212.12238","description":"<p>Key Point Analysis(KPA) is a relatively new task in NLP that combines\nsummarization and classification by extracting argumentative key points (KPs)\nfor a topic from a collection of texts and categorizing their closeness to the\ndifferent arguments. In our work, we focus on the legal domain and develop\nmethods that identify and extract KPs from premises derived from texts of\njudgments. The first method is an adaptation to an existing state-of-the-art\nmethod, and the two others are new methods that we developed from scratch. We\npresent our methods and examples of their outputs, as well a comparison between\nthem. The full evaluation of our results is done in the matching task -- match\nbetween the generated KPs to arguments (premises).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sultan_O/0/1/0/all/0/1\">Oren Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhahri_R/0/1/0/all/0/1\">Rayen Dhahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardan_Y/0/1/0/all/0/1\">Yauheni Mardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eder_T/0/1/0/all/0/1\">Tobias Eder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Customer-Service Dialog System with Semi-Supervised Learning and Coarse-to-Fine Intent Detection. (arXiv:2212.12363v1 [cs.CL])","link":"http://arxiv.org/abs/2212.12363","description":"<p>Task-oriented dialog(TOD) aims to assist users in achieving specific goals\nthrough multi-turn conversation. Recently, good results have been obtained\nbased on large pre-trained models. However, the labeled-data scarcity hinders\nthe efficient development of TOD systems at scale. In this work, we constructed\na weakly supervised dataset based on a teacher/student paradigm that leverages\na large collection of unlabelled dialogues. Furthermore, we built a modular\ndialogue system and integrated coarse-to-fine grained classification for user\nintent detection. Experiments show that our method can reach the dialog goal\nwith a higher success rate and generate more coherent responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhitong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text classification in shipping industry using unsupervised models and Transformer based supervised models. (arXiv:2212.12407v1 [cs.CL])","link":"http://arxiv.org/abs/2212.12407","description":"<p>Obtaining labelled data in a particular context could be expensive and time\nconsuming. Although different algorithms, including unsupervised learning,\nsemi-supervised learning, self-learning have been adopted, the performance of\ntext classification varies with context. Given the lack of labelled dataset, we\nproposed a novel and simple unsupervised text classification model to classify\ncargo content in international shipping industry using the Standard\nInternational Trade Classification (SITC) codes. Our method stems from\nrepresenting words using pretrained Glove Word Embeddings and finding the most\nlikely label using Cosine Similarity. To compare unsupervised text\nclassification model with supervised classification, we also applied several\nTransformer models to classify cargo content. Due to lack of training data, the\nSITC numerical codes and the corresponding textual descriptions were used as\ntraining data. A small number of manually labelled cargo content data was used\nto evaluate the classification performances of the unsupervised classification\nand the Transformer based supervised classification. The comparison reveals\nthat unsupervised classification significantly outperforms Transformer based\nsupervised classification even after increasing the size of the training\ndataset by 30%. Lacking training data is a key bottleneck that prohibits deep\nlearning models (such as Transformers) from successful practical applications.\nUnsupervised classification can provide an alternative efficient and effective\nmethod to classify text when there is scarce training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Ying Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dongping Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alignment Entropy Regularization. (arXiv:2212.12442v1 [cs.CL])","link":"http://arxiv.org/abs/2212.12442","description":"<p>Existing training criteria in automatic speech recognition(ASR) permit the\nmodel to freely explore more than one time alignments between the feature and\nlabel sequences. In this paper, we use entropy to measure a model's\nuncertainty, i.e. how it chooses to distribute the probability mass over the\nset of allowed alignments. Furthermore, we evaluate the effect of entropy\nregularization in encouraging the model to distribute the probability mass only\non a smaller subset of allowed alignments. Experiments show that entropy\nregularization enables a much simpler decoding method without sacrificing word\nerror rate, and provides better time alignment quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Variani_E/0/1/0/all/0/1\">Ehsan Variani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Ke Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybach_D/0/1/0/all/0/1\">David Rybach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allauzen_C/0/1/0/all/0/1\">Cyril Allauzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riley_M/0/1/0/all/0/1\">Michael Riley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizable Natural Language Processing Framework for Migraine Reporting from Social Media. (arXiv:2212.12454v1 [cs.CL])","link":"http://arxiv.org/abs/2212.12454","description":"<p>Migraine is a high-prevalence and disabling neurological disorder. However,\ninformation migraine management in real-world settings could be limited to\ntraditional health information sources. In this paper, we (i) verify that there\nis substantial migraine-related chatter available on social media (Twitter and\nReddit), self-reported by migraine sufferers; (ii) develop a\nplatform-independent text classification system for automatically detecting\nself-reported migraine-related posts, and (iii) conduct analyses of the\nself-reported posts to assess the utility of social media for studying this\nproblem. We manually annotated 5750 Twitter posts and 302 Reddit posts. Our\nsystem achieved an F1 score of 0.90 on Twitter and 0.93 on Reddit. Analysis of\ninformation posted by our 'migraine cohort' revealed the presence of a plethora\nof relevant information about migraine therapies and patient sentiments\nassociated with them. Our study forms the foundation for conducting an in-depth\nanalysis of migraine-related information using social media data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuting Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajwal_S/0/1/0/all/0/1\">Swati Rajwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakamana_S/0/1/0/all/0/1\">Sahithi Lakamana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1\">Chia-Chun Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menell_P/0/1/0/all/0/1\">Paul C. Menell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahid_A/0/1/0/all/0/1\">Adnan H. Shahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Chieh Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_N/0/1/0/all/0/1\">Nikita Chhabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wan-Ju Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_C/0/1/0/all/0/1\">Chieh-Ju Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwedt_T/0/1/0/all/0/1\">Todd J. Schwedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_I/0/1/0/all/0/1\">Imon Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_A/0/1/0/all/0/1\">Abeed Sarker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Content Rating Classification for Fan Fiction. (arXiv:2212.12496v1 [cs.CL])","link":"http://arxiv.org/abs/2212.12496","description":"<p>Content ratings can enable audiences to determine the suitability of various\nmedia products. With the recent advent of fan fiction, the critical issue of\nfan fiction content ratings has emerged. Whether fan fiction content ratings\nare done voluntarily or required by regulation, there is the need to automate\nthe content rating classification. The problem is to take fan fiction text and\ndetermine the appropriate content rating. Methods for other domains, such as\nonline books, have been attempted though none have been applied to fan fiction.\nWe propose natural language processing techniques, including traditional and\ndeep learning methods, to automatically determine the content rating. We show\nthat these methods produce poor accuracy results for multi-classification. We\nthen demonstrate that treating the problem as a binary classification problem\nproduces better accuracy. Finally, we believe and provide some evidence that\nthe current approach of self-annotating has led to incorrect labels limiting\nclassification results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pope_J/0/1/0/all/0/1\">James Pope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MicroBERT: Effective Training of Low-resource Monolingual BERTs through Parameter Reduction and Multitask Learning. (arXiv:2212.12510v1 [cs.CL])","link":"http://arxiv.org/abs/2212.12510","description":"<p>Transformer language models (TLMs) are critical for most NLP tasks, but they\nare difficult to create for low-resource languages because of how much\npretraining data they require. In this work, we investigate two techniques for\ntraining monolingual TLMs in a low-resource setting: greatly reducing TLM size,\nand complementing the masked language modeling objective with two\nlinguistically rich supervised tasks (part-of-speech tagging and dependency\nparsing). Results from 7 diverse languages indicate that our model, MicroBERT,\nis able to produce marked improvements in downstream task evaluations relative\nto a typical monolingual TLM pretraining approach. Specifically, we find that\nmonolingual MicroBERT models achieve gains of up to 18% for parser LAS and 11%\nfor NER F1 compared to a multilingual baseline, mBERT, while having less than\n1% of its parameter count. We conclude reducing TLM parameter count and using\nlabeled data for pretraining low-resource TLMs can yield large quality benefits\nand in some cases produce models that outperform multilingual approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gessler_L/0/1/0/all/0/1\">Luke Gessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitask Pointer Network for Multi-Representational Parsing. (arXiv:2009.09730v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.09730","description":"<p>We propose a transition-based approach that, by training a single model, can\nefficiently parse any input sentence with both constituent and dependency\ntrees, supporting both continuous/projective and discontinuous/non-projective\nsyntactic structures. To that end, we develop a Pointer Network architecture\nwith two separate task-specific decoders and a common encoder, and follow a\nmultitask learning strategy to jointly train them. The resulting quadratic\nsystem, not only becomes the first parser that can jointly produce both\nunrestricted constituent and dependency trees from a single model, but also\nproves that both syntactic formalisms can benefit from each other during\ntraining, achieving state-of-the-art accuracies in several widely-used\nbenchmarks such as the continuous English and Chinese Penn Treebanks, as well\nas the discontinuous German NEGRA and TIGER datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1\">Daniel Fern&#xe1;ndez-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discontinuous Grammar as a Foreign Language. (arXiv:2110.10431v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.10431","description":"<p>In order to achieve deep natural language understanding, syntactic\nconstituent parsing is a vital step, highly demanded by many artificial\nintelligence systems to process both text and speech. One of the most recent\nproposals is the use of standard sequence-to-sequence models to perform\nconstituent parsing as a machine translation task, instead of applying\ntask-specific parsers. While they show a competitive performance, these\ntext-to-parse transducers are still lagging behind classic techniques in terms\nof accuracy, coverage and speed. To close the gap, we here extend the framework\nof sequence-to-sequence models for constituent parsing, not only by providing a\nmore powerful neural architecture for improving their performance, but also by\nenlarging their coverage to handle the most complex syntactic phenomena:\ndiscontinuous structures. To that end, we design several novel linearizations\nthat can fully produce discontinuities and, for the first time, we test a\nsequence-to-sequence model on the main discontinuous benchmarks, obtaining\ncompetitive results on par with task-specific discontinuous constituent parsers\nand achieving state-of-the-art scores on the (discontinuous) English Penn\nTreebank.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1\">Daniel Fern&#xe1;ndez-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InterHT: Knowledge Graph Embeddings by Interaction between Head and Tail Entities. (arXiv:2202.04897v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.04897","description":"<p>Knowledge graph embedding (KGE) models learn the representation of entities\nand relations in knowledge graphs. Distance-based methods show promising\nperformance on link prediction task, which predicts the result by the distance\nbetween two entity representations. However, most of these methods represent\nthe head entity and tail entity separately, which limits the model capacity. We\npropose two novel distance-based methods named InterHT and InterHT+ that allow\nthe head and tail entities to interact better and get better entity\nrepresentation. Experimental results show that our proposed method achieves the\nbest results on ogbl-wikikg2 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baoxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qingye Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Honghong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dayong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language. (arXiv:2205.02364v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02364","description":"<p>The need for Question Answering datasets in low resource languages is the\nmotivation of this research, leading to the development of Kencorpus Swahili\nQuestion Answering Dataset, KenSwQuAD. This dataset is annotated from raw story\ntexts of Swahili low resource language, which is a predominantly spoken in\nEastern African and in other parts of the world. Question Answering (QA)\ndatasets are important for machine comprehension of natural language for tasks\nsuch as internet search and dialog systems. Machine learning systems need\ntraining data such as the gold standard Question Answering set developed in\nthis research. The research engaged annotators to formulate QA pairs from\nSwahili texts collected by the Kencorpus project, a Kenyan languages corpus.\nThe project annotated 1,445 texts from the total 2,585 texts with at least 5 QA\npairs each, resulting into a final dataset of 7,526 QA pairs. A quality\nassurance set of 12.5% of the annotated texts confirmed that the QA pairs were\nall correctly annotated. A proof of concept on applying the set to the QA task\nconfirmed that the dataset can be usable for such tasks. KenSwQuAD has also\ncontributed to resourcing of the Swahili language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wanjawa_B/0/1/0/all/0/1\">Barack W. Wanjawa</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wanzare_L/0/1/0/all/0/1\">Lilian D.A. Wanzare</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Indede_F/0/1/0/all/0/1\">Florence Indede</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+McOnyango_O/0/1/0/all/0/1\">Owen McOnyango</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Muchemi_L/0/1/0/all/0/1\">Lawrence Muchemi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ombui_E/0/1/0/all/0/1\">Edward Ombui</a> (3) ((1) University of Nairobi Kenya, (2) Maseno University Kenya (3) Africa Nazarene University Kenya)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Foundation Models Talk Causality?. (arXiv:2206.10591v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2206.10591","description":"<p>Foundation models are subject to an ongoing heated debate, leaving open the\nquestion of progress towards AGI and dividing the community into two camps: the\nones who see the arguably impressive results as evidence to the scaling\nhypothesis, and the others who are worried about the lack of interpretability\nand reasoning capabilities. By investigating to which extent causal\nrepresentations might be captured by these large scale language models, we make\na humble efforts towards resolving the ongoing philosophical conflicts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Willig_M/0/1/0/all/0/1\">Moritz Willig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zecevic_M/0/1/0/all/0/1\">Matej Ze&#x10d;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1\">Devendra Singh Dhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Interdisciplinary Topic Detection Model for Research Proposal Classification. (arXiv:2209.13519v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2209.13519","description":"<p>The peer merit review of research proposals has been the major mechanism for\ndeciding grant awards. However, research proposals have become increasingly\ninterdisciplinary. It has been a longstanding challenge to assign\ninterdisciplinary proposals to appropriate reviewers, so proposals are fairly\nevaluated. One of the critical steps in reviewer assignment is to generate\naccurate interdisciplinary topic labels for proposal-reviewer matching.\nExisting systems mainly collect topic labels manually generated by principal\ninvestigators. However, such human-reported labels can be non-accurate,\nincomplete, labor intensive, and time costly. What role can AI play in\ndeveloping a fair and precise proposal reviewer assignment system? In this\nstudy, we collaborate with the National Science Foundation of China to address\nthe task of automated interdisciplinary topic path detection. For this purpose,\nwe develop a deep Hierarchical Interdisciplinary Research Proposal\nClassification Network (HIRPCN). Specifically, we first propose a hierarchical\ntransformer to extract the textual semantic information of proposals. We then\ndesign an interdisciplinary graph and leverage GNNs for learning\nrepresentations of each discipline in order to extract interdisciplinary\nknowledge. After extracting the semantic and interdisciplinary knowledge, we\ndesign a level-wise prediction component to fuse the two types of knowledge\nrepresentations and detect interdisciplinary topic paths for each proposal. We\nconduct extensive experiments and expert evaluations on three real-world\ndatasets to demonstrate the effectiveness of our proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Ziyue Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanjie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanchun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Experiments on Turkish ASR with Self-Supervised Speech Representation Learning. (arXiv:2210.07323v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07323","description":"<p>While the Turkish language is listed among low-resource languages, literature\non Turkish automatic speech recognition (ASR) is relatively old. In this\nreport, we present our findings on Turkish ASR with speech representation\nlearning using HUBERT. We investigate pre-training HUBERT for Turkish with\nlarge-scale data curated from online resources. We pre-train our model using\n6,500 hours of speech data from YouTube. The results show that the models are\nnot ready for commercial use since they are not robust against disturbances\nthat typically occur in real-world settings such as variations in accents,\nslang, background noise and interference. We analyze typical errors and the\nlimitations of the models for use in commercial settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safaya_A/0/1/0/all/0/1\">Ali Safaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erzin_E/0/1/0/all/0/1\">Engin Erzin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Knowledge-Grounded Pre-training for Task-Oriented Dialog Systems. (arXiv:2210.08873v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08873","description":"<p>Recent advances in neural approaches greatly improve task-oriented dialogue\n(TOD) systems which assist users to accomplish their goals. However, such\nsystems rely on costly manually labeled dialogs which are not available in\npractical scenarios. In this paper, we present our models for Track 2 of the\nSereTOD 2022 challenge, which is the first challenge of building\nsemi-supervised and reinforced TOD systems on a large-scale real-world Chinese\nTOD dataset MobileCS. We build a knowledge-grounded dialog model to formulate\ndialog history and local KB as input and predict the system response. And we\nperform semi-supervised pre-training both on the labeled and unlabeled data.\nOur system achieves the first place both in the automatic evaluation and human\ninteraction, especially with higher BLEU (+7.64) and Success (+13.6\\%) than the\nsecond place.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Weihao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Keqing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zechen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Dayuan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guanting Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_R/0/1/0/all/0/1\">Ruotong Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chaobo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiran Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADEPT: A DEbiasing PrompT Framework. (arXiv:2211.05414v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05414","description":"<p>Several works have proven that finetuning is an applicable approach for\ndebiasing contextualized word embeddings. Similarly, discrete prompts with\nsemantic meanings have shown to be effective in debiasing tasks. With unfixed\nmathematical representation at the token level, continuous prompts usually\nsurpass discrete ones at providing a pre-trained language model (PLM) with\nadditional task-specific information. Despite this, relatively few efforts have\nbeen made to debias PLMs by prompt tuning with continuous prompts compared to\nits discrete counterpart. Furthermore, for most debiasing methods that alter a\nPLM's original parameters, a major problem is the need to not only decrease the\nbias in the PLM but also to ensure that the PLM does not lose its\nrepresentation ability. Finetuning methods typically have a hard time\nmaintaining this balance, as they tend to violently remove meanings of\nattribute words. In this paper, we propose ADEPT, a method to debias PLMs using\nprompt tuning while maintaining the delicate balance between removing biases\nand ensuring representation ability. To achieve this, we propose a new training\ncriterion inspired by manifold learning and equip it with an explicit debiasing\nterm to optimize prompt tuning. In addition, we conduct several experiments\nwith regard to the reliability, quality, and quantity of a previously proposed\nattribute training corpus in order to obtain a clearer prototype of a certain\nattribute, which indicates the attribute's position and relative distances to\nother words on the manifold. We evaluate ADEPT on several widely acknowledged\ndebiasing benchmarks and downstream tasks, and find that it achieves\ncompetitive results while maintaining (and in some cases even improving) the\nPLM's representation ability. We further visualize words' correlation before\nand after debiasing a PLM, and give some possible explanations for the visible\neffects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Ke Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Charles Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_Y/0/1/0/all/0/1\">Yi Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-12-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}