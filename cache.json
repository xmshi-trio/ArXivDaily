{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-04T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ClusTop: An unsupervised and integrated text clustering and topic extraction framework. (arXiv:2301.00818v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00818","description":"<p>Text clustering and topic extraction are two important tasks in text mining.\nUsually, these two tasks are performed separately. For topic extraction to\nfacilitate clustering, we can first project texts into a topic space and then\nperform a clustering algorithm to obtain clusters. To promote topic extraction\nby clustering, we can first obtain clusters with a clustering algorithm and\nthen extract cluster-specific topics. However, this naive strategy ignores the\nfact that text clustering and topic extraction are strongly correlated and\nfollow a chicken-and-egg relationship. Performing them separately fails to make\nthem mutually benefit each other to achieve the best overall performance. In\nthis paper, we propose an unsupervised text clustering and topic extraction\nframework (ClusTop) which integrates text clustering and topic extraction into\na unified framework and can achieve high-quality clustering result and extract\ntopics from each cluster simultaneously. Our framework includes four\ncomponents: enhanced language model training, dimensionality reduction,\nclustering and topic extraction, where the enhanced language model can be\nviewed as a bridge between clustering and topic extraction. On one hand, it\nprovides text embeddings with a strong cluster structure which facilitates\neffective text clustering; on the other hand, it pays high attention on the\ntopic related words for topic extraction because of its self-attention\narchitecture. Moreover, the training of enhanced language model is\nunsupervised. Experiments on two datasets demonstrate the effectiveness of our\nframework and provide benchmarks for different model combinations in this\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhongtao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_C/0/1/0/all/0/1\">Chenghu Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duo_S/0/1/0/all/0/1\">Siwei Duo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingfei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yatong Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kannudi -- A Reference Editor for Kannada. (arXiv:2301.00836v1 [cs.HC])","link":"http://arxiv.org/abs/2301.00836","description":"<p>Kannudi is a reference editor for Kannada based on OPOK! and OHOK!\nprinciples, and domain knowledge. It introduces a method of input for Kannada,\ncalled OHOK!, that is, Ottu Haku Ottu Kodu! (apply pressure and give ottu).\nThis is especially suited for pressure sensitive input devices, though the\ncurrent online implementation uses the regular mechanical keyboard. OHOK! has\nthree possible modes, namely, sva-ottu (self-conjunct), kandante (as you see),\nand andante (as you say). It may be noted that kandante mode does not follow\nthe phonetic order. However, this mode may work well for those who are inclined\nto visualize as they type rather than vocalizing the sounds.\n</p>\n<p>Kannudi also demonstrates how domain knowledge can be effectively used to\npotentially increase speed, accuracy, and user friendliness. For example,\nselection of a default vowel, automatic shunyification, and arkification. Also\nimplemented are four types Deletes that are necessary for phono-syllabic\nlanguages like Kannada.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dixit_V/0/1/0/all/0/1\">Vishweshwar V. Dixit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Follow the Timeline! Generating Abstractive and Extractive Timeline Summary in Chronological Order. (arXiv:2301.00867v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00867","description":"<p>Nowadays, time-stamped web documents related to a general news query floods\nspread throughout the Internet, and timeline summarization targets concisely\nsummarizing the evolution trajectory of events along the timeline. Unlike\ntraditional document summarization, timeline summarization needs to model the\ntime series information of the input events and summarize important events in\nchronological order. To tackle this challenge, in this paper, we propose a\nUnified Timeline Summarizer (UTS) that can generate abstractive and extractive\ntimeline summaries in time order. Concretely, in the encoder part, we propose a\ngraph-based event encoder that relates multiple events according to their\ncontent dependency and learns a global representation of each event. In the\ndecoder part, to ensure the chronological order of the abstractive summary, we\npropose to extract the feature of event-level attention in its generation\nprocess with sequential information remained and use it to simulate the\nevolutionary attention of the ground truth summary. The event-level attention\ncan also be used to assist in extracting summary, where the extracted summary\nalso comes in time sequence. We augment the previous Chinese large-scale\ntimeline summarization dataset and collect a new English timeline dataset.\nExtensive experiments conducted on these datasets and on the out-of-domain\nTimeline 17 dataset show that UTS achieves state-of-the-art performance in\nterms of both automatic and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_Z/0/1/0/all/0/1\">Zhangming Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding. (arXiv:2301.00876v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00876","description":"<p>Reading comprehension of legal text can be a particularly challenging task\ndue to the length and complexity of legal clauses and a shortage of\nexpert-annotated datasets. To address this challenge, we introduce the Merger\nAgreement Understanding Dataset (MAUD), an expert-annotated reading\ncomprehension dataset based on the American Bar Association's 2021 Public\nTarget Deal Points Study, with over 39,000 examples and over 47,000 total\nannotations. Our fine-tuned Transformer baselines show promising results, with\nmodels performing well above random on most questions. However, on a large\nsubset of questions, there is still room for significant improvement. As the\nonly expert-annotated merger agreement dataset, MAUD is valuable as a benchmark\nfor both the legal profession and the NLP community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Steven H. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scardigli_A/0/1/0/all/0/1\">Antoine Scardigli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Leonard Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levkin_D/0/1/0/all/0/1\">Dimitry Levkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ball_S/0/1/0/all/0/1\">Spencer Ball</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodside_T/0/1/0/all/0/1\">Thomas Woodside</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_O/0/1/0/all/0/1\">Oliver Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Political Polarisation using Language Models: A dataset and method. (arXiv:2301.00891v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00891","description":"<p>Our paper aims to analyze political polarization in US political system using\nLanguage Models, and thereby help candidates make an informed decision. The\navailability of this information will help voters understand their candidates\nviews on the economy, healthcare, education and other social issues. Our main\ncontributions are a dataset extracted from Wikipedia that spans the past 120\nyears and a Language model based method that helps analyze how polarized a\ncandidate is. Our data is divided into 2 parts, background information and\npolitical information about a candidate, since our hypothesis is that the\npolitical views of a candidate should be based on reason and be independent of\nfactors such as birthplace, alma mater, etc. We further split this data into 4\nphases chronologically, to help understand if and how the polarization amongst\ncandidates changes. This data has been cleaned to remove biases. To understand\nthe polarization we begin by showing results from some classical language\nmodels in Word2Vec and Doc2Vec. And then use more powerful techniques like the\nLongformer, a transformer based encoder, to assimilate more information and\nfind the nearest neighbors of each candidate based on their political view and\ntheir background.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gode_S/0/1/0/all/0/1\">Samiran Gode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bare_S/0/1/0/all/0/1\">Supreeth Bare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_H/0/1/0/all/0/1\">Hyungon Yoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EZInterviewer: To Improve Job Interview Performance with Mock Interview Generator. (arXiv:2301.00972v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00972","description":"<p>Interview has been regarded as one of the most crucial step for recruitment.\nTo fully prepare for the interview with the recruiters, job seekers usually\npractice with mock interviews between each other. However, such a mock\ninterview with peers is generally far away from the real interview experience:\nthe mock interviewers are not guaranteed to be professional and are not likely\nto behave like a real interviewer. Due to the rapid growth of online\nrecruitment in recent years, recruiters tend to have online interviews, which\nmakes it possible to collect real interview data from real interviewers. In\nthis paper, we propose a novel application named EZInterviewer, which aims to\nlearn from the online interview data and provides mock interview services to\nthe job seekers. The task is challenging in two ways: (1) the interview data\nare now available but still of low-resource; (2) to generate meaningful and\nrelevant interview dialogs requires thorough understanding of both resumes and\njob descriptions. To address the low-resource challenge, EZInterviewer is\ntrained on a very small set of interview dialogs. The key idea is to reduce the\nnumber of parameters that rely on interview dialogs by disentangling the\nknowledge selector and dialog generator so that most parameters can be trained\nwith ungrounded dialogs as well as the resume data that are not low-resource.\nEvaluation results on a real-world job interview dialog dataset indicate that\nwe achieve promising results to generate mock interviews. With the help of\nEZInterviewer, we hope to make mock interview practice become easier for job\nseekers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Weiheng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analogical Inference Enhanced Knowledge Graph Embedding. (arXiv:2301.00982v1 [cs.AI])","link":"http://arxiv.org/abs/2301.00982","description":"<p>Knowledge graph embedding (KGE), which maps entities and relations in a\nknowledge graph into continuous vector spaces, has achieved great success in\npredicting missing links in knowledge graphs. However, knowledge graphs often\ncontain incomplete triples that are difficult to inductively infer by KGEs. To\naddress this challenge, we resort to analogical inference and propose a novel\nand general self-supervised framework AnKGE to enhance KGE models with\nanalogical inference capability. We propose an analogical object retriever that\nretrieves appropriate analogical objects from entity-level, relation-level, and\ntriple-level. And in AnKGE, we train an analogy function for each level of\nanalogical inference with the original element embedding from a well-trained\nKGE model as input, which outputs the analogical object embedding. In order to\ncombine inductive inference capability from the original KGE model and\nanalogical inference capability enhanced by AnKGE, we interpolate the analogy\nscore with the base model score and introduce the adaptive weights in the score\nfunction for prediction. Through extensive experiments on FB15k-237 and WN18RR\ndatasets, we show that AnKGE achieves competitive results on link prediction\ntask and well performs analogical inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhen_Y/0/1/0/all/0/1\">Yao Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mingyang_C/0/1/0/all/0/1\">Chen Mingyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yufeng_H/0/1/0/all/0/1\">Huang Yufeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Y/0/1/0/all/0/1\">Yang Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huajun_C/0/1/0/all/0/1\">Chen Huajun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Structured Object Sequence Encoders. (arXiv:2301.01015v1 [cs.CV])","link":"http://arxiv.org/abs/2301.01015","description":"<p>In this paper we explore the task of modeling (semi) structured object\nsequences; in particular we focus our attention on the problem of developing a\nstructure-aware input representation for such sequences. In such sequences, we\nassume that each structured object is represented by a set of key-value pairs\nwhich encode the attributes of the structured object. Given a universe of keys,\na sequence of structured objects can then be viewed as an evolution of the\nvalues for each key, over time. We encode and construct a sequential\nrepresentation using the values for a particular key (Temporal Value Modeling -\nTVM) and then self-attend over the set of key-conditioned value sequences to a\ncreate a representation of the structured object sequence (Key Aggregation -\nKA). We pre-train and fine-tune the two components independently and present an\ninnovative training schedule that interleaves the training of both modules with\nshared attention heads. We find that this iterative two part-training results\nin better performance than a unified network with hierarchical encoding as well\nas over, other methods that use a {\\em record-view} representation of the\nsequence \\cite{de2021transformers4rec} or a simple {\\em flattened}\nrepresentation of the sequence. We conduct experiments using real-world data to\ndemonstrate the advantage of interleaving TVM-KA on multiple tasks and detailed\nablation studies motivating our modeling choices. We find that our approach\nperforms better than flattening sequence objects and also allows us to operate\non significantly larger sequences than existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1\">Rudra Murthy V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Riyaz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasekara_C/0/1/0/all/0/1\">Chulaka Gunasekara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Hui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamecha_T/0/1/0/all/0/1\">Tejas Indulal Dhamecha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contractor_D/0/1/0/all/0/1\">Danish Contractor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danilevsky_M/0/1/0/all/0/1\">Marina Danilevsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Acoustic Embeddings And Their Transferability Across Languages. (arXiv:2301.01020v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01020","description":"<p>In speech recognition, it is essential to model the phonetic content of the\ninput signal while discarding irrelevant factors such as speaker variations and\nnoise, which is challenging in low-resource settings. Self-supervised\npre-training has been proposed as a way to improve both supervised and\nunsupervised speech recognition, including frame-level feature representations\nand Acoustic Word Embeddings (AWE) for variable-length segments. However,\nself-supervised models alone cannot learn perfect separation of the linguistic\ncontent as they are trained to optimize indirect objectives. In this work, we\nexperiment with different pre-trained self-supervised features as input to AWE\nmodels and show that they work best within a supervised framework. Models\ntrained on English can be transferred to other languages with no adaptation and\noutperform self-supervised models trained solely on the target languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ram_S/0/1/0/all/0/1\">Sreepratha Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldarmaki_H/0/1/0/all/0/1\">Hanan Aldarmaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIE-QG: Paraphrased Information Extraction for Unsupervised Question Generation from Small Corpora. (arXiv:2301.01064v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01064","description":"<p>Supervised Question Answering systems (QA systems) rely on domain-specific\nhuman-labeled data for training. Unsupervised QA systems generate their own\nquestion-answer training pairs, typically using secondary knowledge sources to\nachieve this outcome. Our approach (called PIE-QG) uses Open Information\nExtraction (OpenIE) to generate synthetic training questions from paraphrased\npassages and uses the question-answer pairs as training data for a language\nmodel for a state-of-the-art QA system based on BERT. Triples in the form of\n&lt;subject, predicate, object&gt; are extracted from each passage, and questions are\nformed with subjects (or objects) and predicates while objects (or subjects)\nare considered as answers. Experimenting on five extractive QA datasets\ndemonstrates that our technique achieves on-par performance with existing\nstate-of-the-art QA systems with the benefit of being trained on an order of\nmagnitude fewer documents and without any recourse to external reference data\nsources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagumothu_D/0/1/0/all/0/1\">Dinesh Nagumothu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofoghi_B/0/1/0/all/0/1\">Bahadorreza Ofoghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guangyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eklund_P/0/1/0/all/0/1\">Peter W. Eklund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Knowledge-Intensive Text-to-SQL Semantic Parsing with Formulaic Knowledge. (arXiv:2301.01067v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01067","description":"<p>In this paper, we study the problem of knowledge-intensive text-to-SQL, in\nwhich domain knowledge is necessary to parse expert questions into SQL queries\nover domain-specific tables. We formalize this scenario by building a new\nChinese benchmark KnowSQL consisting of domain-specific questions covering\nvarious domains. We then address this problem by presenting formulaic\nknowledge, rather than by annotating additional data examples. More concretely,\nwe construct a formulaic knowledge bank as a domain knowledge base and propose\na framework (ReGrouP) to leverage this formulaic knowledge during parsing.\nExperiments using ReGrouP demonstrate a significant 28.2% improvement overall\non KnowSQL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_L/0/1/0/all/0/1\">Longxu Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1\">Mingyang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingzirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">Dechen Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ring That Bell: A Corpus and Method for Multimodal Metaphor Detection in Videos. (arXiv:2301.01134v1 [cs.MM])","link":"http://arxiv.org/abs/2301.01134","description":"<p>We present the first openly available multimodal metaphor annotated corpus.\nThe corpus consists of videos including audio and subtitles that have been\nannotated by experts. Furthermore, we present a method for detecting metaphors\nin the new dataset based on the textual content of the videos. The method\nachieves a high F1-score (62\\%) for metaphorical labels. We also experiment\nwith other modalities and multimodal methods; however, these methods did not\nout-perform the text-based model. In our error analysis, we do identify that\nthere are cases where video could help in disambiguating metaphors, however,\nthe visual cues are too subtle for our model to capture. The data is available\non Zenodo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models are Drummers: Drum Composition with Natural Language Pre-Training. (arXiv:2301.01162v1 [cs.SD])","link":"http://arxiv.org/abs/2301.01162","description":"<p>Automatic music generation with artificial intelligence typically requires a\nlarge amount of data which is hard to obtain for many less common genres and\nmusical instruments. To tackle this issue, we present ongoing work and\npreliminary findings on the possibility for deep models to transfer knowledge\nfrom language to music, by finetuning large language models pre-trained on a\nmassive text corpus on only hundreds of MIDI files of drum performances. We\nshow that by doing so, one of the largest, state-of-the-art models (GPT3) is\ncapable of generating reasonable drum grooves, while models that are not\npre-trained (Transformer) shows no such ability beyond naive repetition.\nEvaluating generated music is a challenging task, more so is evaluating drum\ngrooves with little precedence in literature. Hence, we propose a tailored\nstructural evaluation method and analyze drum grooves produced by GPT3 compared\nto those played by human professionals, exposing the strengths and weaknesses\nof such generation by language-to-music transfer. Our findings suggest that\nlanguage-to-music transfer learning with large language models is viable and\npromising.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Based Geocoding. (arXiv:2301.01170v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01170","description":"<p>In this paper, we formulate the problem of predicting a geolocation from free\ntext as a sequence-to-sequence problem. Using this formulation, we obtain a\ngeocoding model by training a T5 encoder-decoder transformer model using free\ntext as an input and geolocation as an output. The geocoding model was trained\non geo-tagged wikidump data with adaptive cell partitioning for the geolocation\nrepresentation. All of the code including Rest-based application, dataset and\nmodel checkpoints used in this work are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Solaz_Y/0/1/0/all/0/1\">Yuval Solaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalumov_V/0/1/0/all/0/1\">Vitaly Shalumov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey On Few-shot Knowledge Graph Completion with Structural and Commonsense Knowledge. (arXiv:2301.01172v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01172","description":"<p>Knowledge graphs (KG) have served as the key component of various natural\nlanguage processing applications. Commonsense knowledge graphs (CKG) are a\nspecial type of KG, where entities and relations are composed of free-form\ntext. However, previous works in KG completion and CKG completion suffer from\nlong-tail relations and newly-added relations which do not have many know\ntriples for training. In light of this, few-shot KG completion (FKGC), which\nrequires the strengths of graph representation learning and few-shot learning,\nhas been proposed to challenge the problem of limited annotated data. In this\npaper, we comprehensively survey previous attempts on such tasks in the form of\na series of methods and applications. Specifically, we first introduce FKGC\nchallenges, commonly used KGs, and CKGs. Then we systematically categorize and\nsummarize existing works in terms of the type of KGs and the methods. Finally,\nwe present applications of FKGC models on prediction tasks in different areas\nand share our thoughts on future research directions of FKGC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haodi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daisy Zhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models as Corporate Lobbyists. (arXiv:2301.01181v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01181","description":"<p>We demonstrate a proof-of-concept of a large language model conducting\ncorporate lobbying related activities. We use an autoregressive large language\nmodel (OpenAI's text-davinci-003) to determine if proposed U.S. Congressional\nbills are relevant to specific public companies and provide explanations and\nconfidence levels. For the bills the model deems as relevant, the model drafts\na letter to the sponsor of the bill in an attempt to persuade the\ncongressperson to make changes to the proposed legislation. We use hundreds of\nground-truth labels of the relevance of a bill to a company to benchmark the\nperformance of the model, which outperforms the baseline of predicting the most\ncommon outcome of irrelevance. However, we test the ability to determine the\nrelevance of a bill with the previous OpenAI GPT-3 model (text-davinci-002),\nwhich was state-of-the-art on many language tasks until text-davinci-003 was\nreleased on November 28, 2022. The performance of text-davinci-002 is worse\nthan simply always predicting that a bill is irrelevant to a company. These\nresults suggest that, as large language models continue to improve core natural\nlanguage understanding capabilities, performance on corporate lobbying related\ntasks will continue to improve. We then discuss why this could be problematic\nfor societal-AI alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nay_J/0/1/0/all/0/1\">John J. Nay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Average Is Not Enough: Caveats of Multilingual Evaluation. (arXiv:2301.01269v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01269","description":"<p>This position paper discusses the problem of multilingual evaluation. Using\nsimple statistics, such as average language performance, might inject\nlinguistic biases in favor of dominant language families into evaluation\nmethodology. We argue that a qualitative analysis informed by comparative\nlinguistics is needed for multilingual results to detect this kind of bias. We\nshow in our case study that results in published works can indeed be\nlinguistically biased and we demonstrate that visualization based on URIEL\ntypological database can detect it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pikuliak_M/0/1/0/all/0/1\">Mat&#xfa;&#x161; Pikuliak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simko_M/0/1/0/all/0/1\">Mari&#xe1;n &#x160;imko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v15 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1906.02358","description":"<p>Sinhala is the native language of the Sinhalese people who make up the\nlargest ethnic group of Sri Lanka. The language belongs to the globe-spanning\nlanguage tree, Indo-European. However, due to poverty in both linguistic and\neconomic capital, Sinhala, in the perspective of Natural Language Processing\ntools and research, remains a resource-poor language which has neither the\neconomic drive its cousin English has nor the sheer push of the law of numbers\na language such as Chinese has. A number of research groups from Sri Lanka have\nnoticed this dearth and the resultant dire need for proper tools and research\nfor Sinhala natural language processing. However, due to various reasons, these\nattempts seem to lack coordination and awareness of each other. The objective\nof this paper is to fill that gap of a comprehensive literature survey of the\npublicly available Sinhala natural language tools and research so that the\nresearchers working in this field can better utilize contributions of their\npeers. As such, we shall be uploading this paper to arXiv and perpetually\nupdate it periodically to reflect the advances made in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability. (arXiv:2109.05327v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2109.05327","description":"<p>Explainable AI was born as a pathway to allow humans to explore and\nunderstand the inner working of complex systems. But establishing what is an\nexplanation and objectively evaluating explainability, are not trivial tasks.\nWith this paper, we present a new model-agnostic metric to measure the Degree\nof Explainability of information in an objective way, exploiting a specific\ntheoretical model from Ordinary Language Philosophy called the Achinstein's\nTheory of Explanations, implemented with an algorithm relying on deep language\nmodels for knowledge graph extraction and information retrieval. To understand\nwhether this metric is actually able to measure explainability, we devised a\nfew experiments and user studies involving more than 190 participants,\nevaluating two realistic systems for healthcare and finance using famous AI\ntechnology including Artificial Neural Networks and TreeSHAP. The results we\nobtained are statistically significant (with p-values lower than .01),\nsuggesting that our proposed metric for measuring the Degree of Explainability\nis robust on several scenarios and it aligns with concrete expectations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sovrano_F/0/1/0/all/0/1\">Francesco Sovrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitali_F/0/1/0/all/0/1\">Fabio Vitali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v7 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2203.13366","description":"<p>For a long time, different recommendation tasks typically require designing\ntask-specific architectures and training objectives. As a result, it is hard to\ntransfer the learned knowledge and representations from one task to another,\nthus restricting the generalization ability of existing recommendation\napproaches, e.g., a sequential recommendation model can hardly be applied or\ntransferred to a review generation method. To deal with such issues,\nconsidering that language can describe almost anything and language grounding\nis a powerful medium to represent various problems or tasks, we present a\nflexible and unified text-to-text paradigm called \"Pretrain, Personalized\nPrompt, and Predict Paradigm\" (P5) for recommendation, which unifies various\nrecommendation tasks in a shared framework. In P5, all data such as user-item\ninteractions, user descriptions, item metadata, and user reviews are converted\nto a common format -- natural language sequences. The rich information from\nnatural language assists P5 to capture deeper semantics for personalization and\nrecommendation. Specifically, P5 learns different tasks with the same language\nmodeling objective during pretraining. Thus, it serves as the foundation model\nfor various downstream recommendation tasks, allows easy integration with other\nmodalities, and enables instruction-based recommendation based on prompts. P5\nadvances recommender systems from shallow model to deep model to big model, and\nwill revolutionize the technical form of recommender systems towards universal\nrecommendation engine. With adaptive personalized prompt for different users,\nP5 is able to make predictions in a zero-shot or few-shot manner and largely\nreduces the necessity for extensive fine-tuning. On several recommendation\nbenchmarks, we conduct experiments to show the effectiveness of P5. We release\nthe source code at https://github.com/jeykigung/P5.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zuohui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StarGraph: Knowledge Representation Learning based on Incomplete Two-hop Subgraph. (arXiv:2205.14209v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14209","description":"<p>Conventional representation learning algorithms for knowledge graphs (KG) map\neach entity to a unique embedding vector, ignoring the rich information\ncontained in the neighborhood. We propose a method named StarGraph, which gives\na novel way to utilize the neighborhood information for large-scale knowledge\ngraphs to obtain entity representations. An incomplete two-hop neighborhood\nsubgraph for each target node is at first generated, then processed by a\nmodified self-attention network to obtain the entity representation, which is\nused to replace the entity embedding in conventional methods. We achieved SOTA\nperformance on ogbl-wikikg2 and got competitive results on fb15k-237. The\nexperimental results proves that StarGraph is efficient in parameters, and the\nimprovement made on ogbl-wikikg2 demonstrates its great effectiveness of\nrepresentation learning on large-scale knowledge graphs. The code is now\navailable at \\url{https://github.com/hzli-ucas/StarGraph}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiangrui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Linhui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yafeng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuhui Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Self-Attention for Language Understanding. (arXiv:2206.12608v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.12608","description":"<p>Deep neural models (e.g. Transformer) naturally learn spurious features,\nwhich create a ``shortcut'' between the labels and inputs, thus impairing the\ngeneralization and robustness. This paper advances self-attention mechanism to\nits robust variant for Transformer-based pre-trained language models (e.g.\nBERT). We propose \\textit{Adversarial Self-Attention} mechanism (ASA), which\nadversarially biases the attentions to effectively suppress the model reliance\non features (e.g. specific keywords) and encourage its exploration of broader\nsemantics. We conduct comprehensive evaluation across a wide range of tasks for\nboth pre-training and fine-tuning stages. For pre-training, ASA unfolds\nremarkable performance gain compared to naive training for longer steps. For\nfine-tuning, ASA-empowered models outweigh naive models by a large margin\nconsidering both generalization and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongqiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Ruixue Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Once-for-All Sequence Compression for Self-Supervised Speech Models. (arXiv:2211.02332v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.02332","description":"<p>The sequence length along the time axis is often the dominant factor of the\ncomputational cost of self-supervised speech models. Works have been proposed\nto reduce the sequence length for lowering the computational cost. However,\ndifferent downstream tasks have different tolerance of sequence compressing, so\na model that produces a fixed compressing rate may not fit all tasks. In this\nwork, we introduce a once-for-all (OFA) sequence compression framework for\nself-supervised speech models that supports a continuous range of compressing\nrates. The framework is evaluated on various tasks, showing marginal\ndegradation compared to the fixed compressing rate variants with a smooth\nperformance-efficiency trade-off. We further explore adaptive compressing rate\nlearning, demonstrating the ability to select task-specific preferred frame\nperiods without needing a grid search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsuan-Jui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yen Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}