{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Continual Learning for On-Device Speech Recognition using Disentangled Conformers. (arXiv:2212.01393v1 [eess.AS])","link":"http://arxiv.org/abs/2212.01393","description":"<p>Automatic speech recognition research focuses on training and evaluating on\nstatic datasets. Yet, as speech models are increasingly deployed on personal\ndevices, such models encounter user-specific distributional shifts. To simulate\nthis real-world scenario, we introduce LibriContinual, a continual learning\nbenchmark for speaker-specific domain adaptation derived from LibriVox\naudiobooks, with data corresponding to 118 individual speakers and 6 train\nsplits per speaker of different sizes. Additionally, current speech recognition\nmodels and continual learning algorithms are not optimized to be\ncompute-efficient. We adapt a general-purpose training algorithm NetAug for ASR\nand create a novel Conformer variant called the DisConformer (Disentangled\nConformer). This algorithm produces ASR models consisting of a frozen 'core'\nnetwork for general-purpose use and several tunable 'augment' networks for\nspeaker-specific tuning. Using such models, we propose a novel\ncompute-efficient continual learning algorithm called DisentangledCL. Our\nexperiments show that the DisConformer models significantly outperform\nbaselines on general ASR i.e. LibriSpeech (15.58% rel. WER on test-other). On\nspeaker-specific LibriContinual they significantly outperform\ntrainable-parameter-matched baselines (by 20.65% rel. WER on test) and even\nmatch fully finetuned baselines in some settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Diwan_A/0/1/0/all/0/1\">Anuj Diwan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeh_C/0/1/0/all/0/1\">Ching-Feng Yeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tomasello_P/0/1/0/all/0/1\">Paden Tomasello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thread With Caution: Proactively Helping Users Assess and Deescalate Tension in Their Online Discussions. (arXiv:2212.01401v1 [cs.HC])","link":"http://arxiv.org/abs/2212.01401","description":"<p>Incivility remains a major challenge for online discussion platforms, to such\nan extent that even conversations between well-intentioned users can often\nderail into uncivil behavior. Traditionally, platforms have relied on\nmoderators to -- with or without algorithmic assistance -- take corrective\nactions such as removing comments or banning users. In this work we propose a\ncomplementary paradigm that directly empowers users by proactively enhancing\ntheir awareness about existing tension in the conversation they are engaging in\nand actively guides them as they are drafting their replies to avoid further\nescalation.\n</p>\n<p>As a proof of concept for this paradigm, we design an algorithmic tool that\nprovides such proactive information directly to users, and conduct a user study\nin a popular discussion platform. Through a mixed methods approach combining\nsurveys with a randomized controlled experiment, we uncover qualitative and\nquantitative insights regarding how the participants utilize and react to this\ninformation. Most participants report finding this proactive paradigm valuable,\nnoting that it helps them to identify tension that they may have otherwise\nmissed and prompts them to further reflect on their own replies and to revise\nthem. These effects are corroborated by a comparison of how the participants\ndraft their reply when our tool warns them that their conversation is at risk\nof derailing into uncivil behavior versus in a control condition where the tool\nis disabled. These preliminary findings highlight the potential of this\nuser-centered paradigm and point to concrete directions for future\nimplementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jonathan P. Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluger_C/0/1/0/all/0/1\">Charlotte Schluger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danescu_Niculescu_Mizil_C/0/1/0/all/0/1\">Cristian Danescu-Niculescu-Mizil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Avoiding spurious correlations via logit correction. (arXiv:2212.01433v1 [cs.LG])","link":"http://arxiv.org/abs/2212.01433","description":"<p>Empirical studies suggest that machine learning models trained with empirical\nrisk minimization (ERM) often rely on attributes that may be spuriously\ncorrelated with the class labels. Such models typically lead to poor\nperformance during inference for data lacking such correlations. In this work,\nwe explicitly consider a situation where potential spurious correlations are\npresent in the majority of training data. In contrast with existing approaches,\nwhich use the ERM model outputs to detect the samples without spurious\ncorrelations, and either heuristically upweighting or upsampling those samples;\nwe propose the logit correction (LC) loss, a simple yet effective improvement\non the softmax cross-entropy loss, to correct the sample logit. We demonstrate\nthat minimizing the LC loss is equivalent to maximizing the group-balanced\naccuracy, so the proposed LC could mitigate the negative impacts of spurious\ncorrelations. Our extensive experimental results further reveal that the\nproposed LC loss outperforms the SoTA solutions on multiple popular benchmarks\nby a large margin, an average 5.5% absolute improvement, without access to\nspurious attribute labels. LC is also competitive with oracle methods that make\nuse of the attribute labels. Code is available at\nhttps://github.com/shengliu66/LC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekhar_N/0/1/0/all/0/1\">Nitesh Sekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_P/0/1/0/all/0/1\">Prateek Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twitter Data Analysis: Izmir Earthquake Case. (arXiv:2212.01453v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01453","description":"<p>T\\\"urkiye is located on a fault line; earthquakes often occur on a large and\nsmall scale. There is a need for effective solutions for gathering current\ninformation during disasters. We can use social media to get insight into\npublic opinion. This insight can be used in public relations and disaster\nmanagement. In this study, Twitter posts on Izmir Earthquake that took place on\nOctober 2020 are analyzed. We question if this analysis can be used to make\nsocial inferences on time. Data mining and natural language processing (NLP)\nmethods are used for this analysis. NLP is used for sentiment analysis and\ntopic modelling. The latent Dirichlet Allocation (LDA) algorithm is used for\ntopic modelling. We used the Bidirectional Encoder Representations from\nTransformers (BERT) model working with Transformers architecture for sentiment\nanalysis. It is shown that the users shared their goodwill wishes and aimed to\ncontribute to the initiated aid activities after the earthquake. The users\ndesired to make their voices heard by competent institutions and organizations.\nThe proposed methods work effectively. Future studies are also discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrali_O/0/1/0/all/0/1\">&#xd6;zg&#xfc;r Agrali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokun_H/0/1/0/all/0/1\">Hakan S&#xf6;k&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaarslan_E/0/1/0/all/0/1\">Enis Karaarslan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Modeling on Clinical Social Work Notes for Exploring Social Determinants of Health Factors. (arXiv:2212.01462v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01462","description":"<p>Most research studying social determinants of health (SDoH) has focused on\nphysician notes or structured elements of the electronic medical record (EMR).\nWe hypothesize that clinical notes from social workers, whose role is to\nameliorate social and economic factors, might provide a richer source of data\non SDoH. We sought to perform topic modeling to identify robust topics of\ndiscussion within a large cohort of social work notes. We retrieved a diverse,\ndeidentified corpus of 0.95 million clinical social work notes from 181,644\npatients at the University of California, San Francisco. We used word frequency\nanalysis and Latent Dirichlet Allocation (LDA) topic modeling analysis to\ncharacterize this corpus and identify potential topics of discussion. Word\nfrequency analysis identified both medical and non-medical terms associated\nwith specific ICD10 chapters. The LDA topic modeling analysis extracted 11\ntopics related to social determinants of health risk factors including\nfinancial status, abuse history, social support, risk of death, and mental\nhealth. In addition, the topic modeling approach captured the variation between\ndifferent types of social work notes and across patients with different types\nof diseases or conditions. We demonstrated that social work notes contain rich,\nunique, and otherwise unobtainable information on an individual's SDoH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shenghuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zack_T/0/1/0/all/0/1\">Travis Zack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sushil_M/0/1/0/all/0/1\">Madhumita Sushil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butte_A/0/1/0/all/0/1\">Atul J. Butte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NarraSum: A Large-Scale Dataset for Abstractive Narrative Summarization. (arXiv:2212.01476v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01476","description":"<p>Narrative summarization aims to produce a distilled version of a narrative to\ndescribe its most salient events and characters. Summarizing a narrative is\nchallenging as it requires an understanding of event causality and character\nbehaviors. To encourage research in this direction, we propose NarraSum, a\nlarge-scale narrative summarization dataset. It contains 122K narrative\ndocuments, which are collected from plot descriptions of movies and TV episodes\nwith diverse genres, and their corresponding abstractive summaries. Experiments\nshow that there is a large performance gap between humans and the\nstate-of-the-art summarization models on NarraSum. We hope that this dataset\nwill promote future research in summarization, as well as broader studies of\nnatural language understanding and generation. The dataset is available at\nhttps://github.com/zhaochaocs/narrasum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaiqiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event knowledge in large language models: the gap between the impossible and the unlikely. (arXiv:2212.01488v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01488","description":"<p>People constantly use language to learn about the world. Computational\nlinguists have capitalized on this fact to build large language models (LLMs)\nthat acquire co-occurrence-based knowledge from language corpora. LLMs achieve\nimpressive performance on many tasks, but the robustness of their world\nknowledge has been questioned. Here, we ask: do LLMs acquire generalized\nknowledge about real-world events? Using curated sets of minimal sentence pairs\n(n=1215), we tested whether LLMs are more likely to generate plausible event\ndescriptions compared to their implausible counterparts. We found that LLMs\nsystematically distinguish possible and impossible events (The teacher bought\nthe laptop vs. The laptop bought the teacher) but fall short of human\nperformance when distinguishing likely and unlikely events (The nanny tutored\nthe boy vs. The boy tutored the nanny). In follow-up analyses, we show that (i)\nLLM scores are driven by both plausibility and surface-level sentence features,\n(ii) LLMs generalize well across syntactic sentence variants (active vs\npassive) but less well across semantic sentence variants (synonymous\nsentences), (iii) some, but not all LLM deviations from ground-truth labels\nalign with crowdsourced human judgments, and (iv) explicit event plausibility\ninformation emerges in middle LLM layers and remains high thereafter. Overall,\nour analyses reveal a gap in LLMs' event knowledge, highlighting their\nlimitations as generalized knowledge bases. We conclude by speculating that the\ndifferential performance on impossible vs. unlikely events is not a temporary\nsetback but an inherent property of LLMs, reflecting a fundamental difference\nbetween linguistic knowledge and world knowledge in intelligent systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kauf_C/0/1/0/all/0/1\">Carina Kauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanova_A/0/1/0/all/0/1\">Anna A. Ivanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambelli_G/0/1/0/all/0/1\">Giulia Rambelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1\">Emmanuele Chersoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_J/0/1/0/all/0/1\">Jingyuan S. She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_Z/0/1/0/all/0/1\">Zawad Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1\">Evelina Fedorenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1\">Alessandro Lenci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Orders Are Unwanted: Dynamic Deep Graph Convolutional Network for Personality Detection. (arXiv:2212.01515v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01515","description":"<p>Predicting personality traits based on online posts has emerged as an\nimportant task in many fields such as social network analysis. One of the\nchallenges of this task is assembling information from various posts into an\noverall profile for each user. While many previous solutions simply concatenate\nthe posts into a long document and then encode the document by sequential or\nhierarchical models, they introduce unwarranted orders for the posts, which may\nmislead the models. In this paper, we propose a dynamic deep graph\nconvolutional network (D-DGCN) to overcome the above limitation. Specifically,\nwe design a learn-to-connect approach that adopts a dynamic multi-hop structure\ninstead of a deterministic structure, and combine it with a DGCN module to\nautomatically learn the connections between posts. The modules of post encoder,\nlearn-to-connect, and DGCN are jointly trained in an end-to-end manner.\nExperimental results on the Kaggle and Pandora datasets show the superior\nperformance of D-DGCN to state-of-the-art baselines. Our code is available at\nhttps://github.com/djz233/D-DGCN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jinghao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The RoyalFlush System for the WMT 2022 Efficiency Task. (arXiv:2212.01543v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01543","description":"<p>This paper describes the submission of the RoyalFlush neural machine\ntranslation system for the WMT 2022 translation efficiency task. Unlike the\ncommonly used autoregressive translation system, we adopted a two-stage\ntranslation paradigm called Hybrid Regression Translation (HRT) to combine the\nadvantages of autoregressive and non-autoregressive translation. Specifically,\nHRT first autoregressively generates a discontinuous sequence (e.g., make a\nprediction every $k$ tokens, $k&gt;1$) and then fills in all previously skipped\ntokens at once in a non-autoregressive manner. Thus, we can easily trade off\nthe translation quality and speed by adjusting $k$. In addition, by integrating\nother modeling techniques (e.g., sequence-level knowledge distillation and\ndeep-encoder-shallow-decoder layer allocation strategy) and a mass of\nengineering efforts, HRT improves 80\\% inference speed and achieves equivalent\ntranslation performance with the same-capacity AT counterpart. Our fastest\nsystem reaches 6k+ words/second on the GPU latency setting, estimated to be\nabout 3.1x faster than the last year's winner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_A/0/1/0/all/0/1\">Aixin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shuqin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Ming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Learning for Few-Shot Medical Text Classification. (arXiv:2212.01552v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01552","description":"<p>Medical professionals frequently work in a data constrained setting to\nprovide insights across a unique demographic. A few medical observations, for\ninstance, informs the diagnosis and treatment of a patient. This suggests a\nunique setting for meta-learning, a method to learn models quickly on new\ntasks, to provide insights unattainable by other methods. We investigate the\nuse of meta-learning and robustness techniques on a broad corpus of benchmark\ntext and medical data. To do this, we developed new data pipelines, combined\nlanguage models with meta-learning approaches, and extended existing\nmeta-learning algorithms to minimize worst case loss. We find that\nmeta-learning on text is a suitable framework for text-based data, providing\nbetter data efficiency and comparable performance to few-shot language models\nand can be successfully applied to medical note data. Furthermore,\nmeta-learning models coupled with DRO can improve worst case loss across\ndisease codes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pankaj Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qureshi_I/0/1/0/all/0/1\">Imran Qureshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Label Correlations for Ultra-Fine Entity Typing with Neural Pairwise Conditional Random Field. (arXiv:2212.01581v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01581","description":"<p>Ultra-fine entity typing (UFET) aims to predict a wide range of type phrases\nthat correctly describe the categories of a given entity mention in a sentence.\nMost recent works infer each entity type independently, ignoring the\ncorrelations between types, e.g., when an entity is inferred as a president, it\nshould also be a politician and a leader. To this end, we use an undirected\ngraphical model called pairwise conditional random field (PCRF) to formulate\nthe UFET problem, in which the type variables are not only unarily influenced\nby the input but also pairwisely relate to all the other type variables. We use\nvarious modern backbones for entity typing to compute unary potentials, and\nderive pairwise potentials from type phrase representations that both capture\nprior semantic information and facilitate accelerated inference. We use\nmean-field variational inference for efficient type inference on very large\ntype sets and unfold it as a neural network module to enable end-to-end\ntraining. Experiments on UFET show that the Neural-PCRF consistently\noutperforms its backbones with little cost and results in a competitive\nperformance against cross-encoder based SOTA while being thousands of times\nfaster. We also find Neural- PCRF effective on a widely used fine-grained\nentity typing dataset with a smaller type set. We pack Neural-PCRF as a network\nmodule that can be plugged onto multi-label type classifiers with ease and\nrelease it in https://github.com/modelscope/adaseq/tree/master/examples/NPCRF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chengyue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weiqi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RHO ($\\rho$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding. (arXiv:2212.01588v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01588","description":"<p>Dialogue systems can leverage large pre-trained language models and knowledge\nto generate fluent and informative responses. However, these models are still\nprone to produce hallucinated responses not supported by the input source,\nwhich greatly hinders their application. The heterogeneity between external\nknowledge and dialogue context challenges representation learning and source\nintegration, and further contributes to unfaithfulness. To handle this\nchallenge and generate more faithful responses, this paper presents RHO\n($\\rho$) utilizing the representations of linked entities and relation\npredicates from a knowledge graph (KG). We propose (1) local knowledge\ngrounding to combine textual embeddings with the corresponding KG embeddings;\nand (2) global knowledge grounding to equip RHO with multi-hop reasoning\nabilities via the attention mechanism. In addition, we devise a response\nre-ranking technique based on walks over KG sub-graphs for better\nconversational reasoning. Experimental results on OpenDialKG show that our\napproach significantly outperforms state-of-the-art methods on both automatic\nand human evaluation by a large margin, especially in hallucination reduction\n(17.54% in FeQA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Min Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoP: Factual Inconsistency Detection by Controlling the Preference. (arXiv:2212.01611v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01611","description":"<p>Abstractive summarization is the process of generating a summary given a\ndocument as input. Although significant progress has been made, the factual\ninconsistency between the document and the generated summary still limits its\npractical applications. Previous work found that the probabilities assigned by\nthe generation model reflect its preferences for the generated summary,\nincluding the preference for factual consistency, and the preference for the\nlanguage or knowledge prior as well. To separate the preference for factual\nconsistency, we propose an unsupervised framework named CoP by controlling the\npreference of the generation model with the help of prompt. More specifically,\nthe framework performs an extra inference step in which a text prompt is\nintroduced as an additional input. In this way, another preference is described\nby the generation probability of this extra inference process. The difference\nbetween the above two preferences, i.e. the difference between the\nprobabilities, could be used as measurements for detecting factual\ninconsistencies. Interestingly, we found that with the properly designed\nprompt, our framework could evaluate specific preferences and serve as\nmeasurements for fine-grained categories of inconsistency, such as\nentity-related inconsistency, coreference-related inconsistency, etc. Moreover,\nour framework could also be extended to the supervised setting to learn better\nprompt from the labeled data as well. Experiments show that our framework\nachieves new SOTA results on three factual inconsistency detection tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+She_S/0/1/0/all/0/1\">Shuaijie She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Entity and Relation Extraction with Multi-Modal Retrieval. (arXiv:2212.01612v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01612","description":"<p>Multi-modal named entity recognition (NER) and relation extraction (RE) aim\nto leverage relevant image information to improve the performance of NER and\nRE. Most existing efforts largely focused on directly extracting potentially\nuseful information from images (such as pixel-level features, identified\nobjects, and associated captions). However, such extraction processes may not\nbe knowledge aware, resulting in information that may not be highly relevant.\nIn this paper, we propose a novel Multi-modal Retrieval based framework (MoRe).\nMoRe contains a text retrieval module and an image-based retrieval module,\nwhich retrieve related knowledge of the input text and image in the knowledge\ncorpus respectively. Next, the retrieval results are sent to the textual and\nvisual models respectively for predictions. Finally, a Mixture of Experts (MoE)\nmodule combines the predictions from the two models to make the final decision.\nOur experiments show that both our textual model and visual model can achieve\nstate-of-the-art performance on four multi-modal NER datasets and one\nmulti-modal RE dataset. With MoE, the model performance can be further improved\nand our analysis demonstrates the benefits of integrating both textual and\nvisual cues for such tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intermediate Entity-based Sparse Interpretable Representation Learning. (arXiv:2212.01641v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01641","description":"<p>Interpretable entity representations (IERs) are sparse embeddings that are\n\"human-readable\" in that dimensions correspond to fine-grained entity types and\nvalues are predicted probabilities that a given entity is of the corresponding\ntype. These methods perform well in zero-shot and low supervision settings.\nCompared to standard dense neural embeddings, such interpretable\nrepresentations may permit analysis and debugging. However, while fine-tuning\nsparse, interpretable representations improves accuracy on downstream tasks, it\ndestroys the semantics of the dimensions which were enforced in pre-training.\nCan we maintain the interpretable semantics afforded by IERs while improving\npredictive performance on downstream tasks? Toward this end, we propose\nIntermediate enTity-based Sparse Interpretable Representation Learning\n(ItsIRL). ItsIRL realizes improved performance over prior IERs on biomedical\ntasks, while maintaining \"interpretability\" generally and their ability to\nsupport model debugging specifically. The latter is enabled in part by the\nability to perform \"counterfactual\" fine-grained entity type manipulation,\nwhich we explore in this work. Finally, we propose a method to construct entity\ntype based class prototypes for revealing global semantic properties of classes\nlearned by our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Olano_D/0/1/0/all/0/1\">Diego Garcia-Olano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onoe_Y/0/1/0/all/0/1\">Yasumasa Onoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_J/0/1/0/all/0/1\">Joydeep Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global memory transformer for processing long documents. (arXiv:2212.01650v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01650","description":"<p>Transformer variants dominate the state-of-the-art in different natural\nlanguage processing tasks such as translation, reading comprehension and\nsummarization. Our paper is more directed to use general memory slots added to\nthe inputs and studying the results of adding these slots. This paper is a go\non study of general memory slots rule that were added to the input of the\nproposed model in previous work. We have two main tasks;1) pretraining task\nusing masked language modeling and b) fine tuning task using HotpotQA . This\nstudy aims to verify the ability of the proposed model to handle chunks as if\nthey were one chunk comparing with the base model. As baseline we used T5\ntransformer. We studied the rule of memory slots augmented to each input chunk\nand studied the model performance without selector. We found that adding memory\nto input chunks helped the proposed model to overcome the baseline on Masked\nlanguage modeling task with specific training parameters. Ablation study\nreveals the ability of using the compressed input chunks with a degradation in\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adel_A/0/1/0/all/0/1\">Arij Al Adel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Fine-Tuning Data Selection for ASR Using Self-Supervised Speech Models. (arXiv:2212.01661v1 [eess.AS])","link":"http://arxiv.org/abs/2212.01661","description":"<p>Self-supervised learning (SSL) has been able to leverage unlabeled data to\nboost the performance of automatic speech recognition (ASR) models when we have\naccess to only a small amount of transcribed speech data. However, this raises\nthe question of which subset of the available unlabeled data should be selected\nfor transcription. Our work investigates different unsupervised data selection\ntechniques for fine-tuning the HuBERT model under a limited transcription\nbudget. We investigate the impact of speaker diversity, gender bias, and topic\ndiversity on the downstream ASR performance. We also devise two novel\ntechniques for unsupervised data selection: pre-training loss based data\nselection and the perplexity of byte pair encoded clustered units (PBPE) and we\nshow how these techniques compare to pure random data selection. Finally, we\nanalyze the correlations between the inherent characteristics of the selected\nfine-tuning subsets as well as how these characteristics correlate with the\nresultant word error rate. We demonstrate the importance of token diversity,\nspeaker diversity, and topic diversity in achieving the best performance in\nterms of WER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gody_R/0/1/0/all/0/1\">Reem Gody</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"T-STAR: Truthful Style Transfer using AMR Graph as Intermediate Representation. (arXiv:2212.01667v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01667","description":"<p>Unavailability of parallel corpora for training text style transfer (TST)\nmodels is a very challenging yet common scenario. Also, TST models implicitly\nneed to preserve the content while transforming a source sentence into the\ntarget style. To tackle these problems, an intermediate representation is often\nconstructed that is devoid of style while still preserving the meaning of the\nsource sentence. In this work, we study the usefulness of Abstract Meaning\nRepresentation (AMR) graph as the intermediate style agnostic representation.\nWe posit that semantic notations like AMR are a natural choice for an\nintermediate representation. Hence, we propose T-STAR: a model comprising of\ntwo components, text-to-AMR encoder and a AMR-to-text decoder. We propose\nseveral modeling improvements to enhance the style agnosticity of the generated\nAMR. To the best of our knowledge, T-STAR is the first work that uses AMR as an\nintermediate representation for TST. With thorough experimental evaluation we\nshow T-STAR significantly outperforms state of the art techniques by achieving\non an average 15.2% higher content preservation with negligible loss (3%\napprox.) in style accuracy. Through detailed human evaluation with 90,000\nratings, we also show that T-STAR has up to 50% lesser hallucinations compared\nto state of the art TST models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jangra_A/0/1/0/all/0/1\">Anubhav Jangra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nema_P/0/1/0/all/0/1\">Preksha Nema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghuveer_A/0/1/0/all/0/1\">Aravindan Raghuveer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Medical Document Summarization. (arXiv:2212.01669v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01669","description":"<p>The internet has had a dramatic effect on the healthcare industry, allowing\ndocuments to be saved, shared, and managed digitally. This has made it easier\nto locate and share important data, improving patient care and providing more\nopportunities for medical studies. As there is so much data accessible to\ndoctors and patients alike, summarizing it has become increasingly necessary -\nthis has been supported through the introduction of deep learning and\ntransformer-based networks, which have boosted the sector significantly in\nrecent years. This paper gives a comprehensive survey of the current techniques\nand trends in medical summarization\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Raghav Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jangra_A/0/1/0/all/0/1\">Anubhav Jangra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sriparna Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as Agent Models. (arXiv:2212.01681v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01681","description":"<p>Language models (LMs) are trained on collections of documents, written by\nindividual human agents to achieve specific goals in an outside world. During\ntraining, LMs have access only to text of these documents, with no direct\nevidence of the internal states of the agents that produced them -- a fact\noften used to argue that LMs are incapable of modeling goal-directed aspects of\nhuman language production and comprehension. Can LMs trained on text learn\nanything at all about the relationship between language and use? I argue that\nLMs are models of intentional communication in a specific, narrow sense. When\nperforming next word prediction given a textual context, an LM can infer and\nrepresent properties of an agent likely to have produced that context. These\nrepresentations can in turn influence subsequent LM generation in the same way\nthat agents' communicative intentions influence their language. I survey\nfindings from the recent literature showing that -- even in today's non-robust\nand error-prone models -- LMs infer and use representations of fine-grained\ncommunicative intentions and more abstract beliefs and goals. Despite the\nlimited nature of their training data, they can thus serve as building blocks\nfor systems that communicate and act intentionally.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What is Not in the Context? Evaluation of Few-shot Learners with Informative Demonstrations. (arXiv:2212.01692v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01692","description":"<p>Large language models demonstrate an emergent ability to learn a new task\nfrom a small number of input-output demonstrations, referred to as in-context\nfew-shot learning. However, recent work shows that in such settings, models\nmainly learn to mimic the new task distribution, instead of the mechanics of\nthe new task. We argue that the commonly-used evaluation settings of few-shot\nmodels utilizing a random selection of in-context demonstrations is not able to\ndisentangle models' ability to learn new skills from demonstrations, as most of\nthe such-selected demonstrations are not informative for prediction beyond\nexposing the new task's input and output distribution.\n</p>\n<p>Therefore, we introduce an evaluation technique that disentangles few-shot\nlearners' gain from in-context learning by picking the demonstrations sharing a\nspecific, informative concept with the predicted sample, in addition to the\nperformance reached by mainly non-informative samples. We find that regardless\nof the model size, existing few-shot learners are not able to benefit from\nobserving such informative concepts in demonstrations. We also find that such\nability may not be obtained trivially by exposing the informative\ndemonstrations in the training process, leaving the challenge of training true\nin-context learners open.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadlcik_M/0/1/0/all/0/1\">Marek Kadl&#x10d;&#xed;k</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust NLG Bias Evaluation with Syntactically-diverse Prompts. (arXiv:2212.01700v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01700","description":"<p>We present a robust methodology for evaluating biases in natural language\ngeneration(NLG) systems. Previous works use fixed hand-crafted prefix templates\nwith mentions of various demographic groups to prompt models to generate\ncontinuations for bias analysis. These fixed prefix templates could themselves\nbe specific in terms of styles or linguistic structures, which may lead to\nunreliable fairness conclusions that are not representative of the general\ntrends from tone varying prompts. To study this problem, we paraphrase the\nprompts with different syntactic structures and use these to evaluate\ndemographic bias in NLG systems. Our results suggest similar overall bias\ntrends but some syntactic structures lead to contradictory conclusions compared\nto past works. We show that our methodology is more robust and that some\nsyntactic structures prompt more toxic content while others could prompt less\nbiased generation. This suggests the importance of not relying on a fixed\nsyntactic structure and using tone-invariant prompts. Introducing\nsyntactically-diverse prompts can achieve more robust NLG (bias) evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_A/0/1/0/all/0/1\">Arshiya Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic Constructs as the Representation of the Domain Model in an Intelligent Language Tutoring System. (arXiv:2212.01711v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01711","description":"<p>This paper presents the development of an AI-based language learning platform\nRevita. It is a freely available intelligent online tutor, developed to support\nlearners of multiple languages, from low-intermediate to advanced levels. It\nhas been in pilot use by hundreds of students at several universities, whose\nfeedback and needs are shaping the development. One of the main emerging\nfeatures of Revita is the introduction of a system of linguistic constructs as\nthe representation of domain knowledge. The system of constructs is developed\nin close collaboration with experts in language teaching. Constructs define the\ntypes of exercises, the content of the feedback, and enable the detailed\nmodeling and evaluation of learning progress.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katinskaia_A/0/1/0/all/0/1\">Anisia Katinskaia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jue Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_A/0/1/0/all/0/1\">Anh-Duc Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yangarber_R/0/1/0/all/0/1\">Roman Yangarber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KPT: Keyword-guided Pre-training for Grounded Dialog Generation. (arXiv:2212.01739v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01739","description":"<p>Incorporating external knowledge into the response generation process is\nessential to building more helpful and reliable dialog agents. However,\ncollecting knowledge-grounded conversations is often costly, calling for a\nbetter pre-trained model for grounded dialog generation that generalizes well\nw.r.t. different types of knowledge. In this work, we propose KPT\n(Keyword-guided Pre-Training), a novel self-supervised pre-training method for\ngrounded dialog generation without relying on extra knowledge annotation.\nSpecifically, we use a pre-trained language model to extract the most uncertain\ntokens in the dialog as keywords. With these keywords, we construct two kinds\nof knowledge and pre-train a knowledge-grounded response generation model,\naiming at handling two different scenarios: (1) the knowledge should be\nfaithfully grounded; (2) it can be selectively used. For the former, the\ngrounding knowledge consists of keywords extracted from the response. For the\nlatter, the grounding knowledge is additionally augmented with keywords\nextracted from other utterances in the same dialog. Since the knowledge is\nextracted from the dialog itself, KPT can be easily performed on a large volume\nand variety of dialogue data. We considered three data sources (open-domain,\ntask-oriented, conversational QA) with a total of 2.5M dialogues. We conduct\nextensive experiments on various few-shot knowledge-grounded generation tasks,\nincluding grounding on dialog acts, knowledge graphs, persona descriptions, and\nWikipedia passages. Our comprehensive experiments and analyses demonstrate that\nKPT consistently outperforms state-of-the-art methods on these tasks with\ndiverse grounding knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Languages You Know Influence Those You Learn: Impact of Language Characteristics on Multi-Lingual Text-to-Text Transfer. (arXiv:2212.01757v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01757","description":"<p>Multi-lingual language models (LM), such as mBERT, XLM-R, mT5, mBART, have\nbeen remarkably successful in enabling natural language tasks in low-resource\nlanguages through cross-lingual transfer from high-resource ones. In this work,\nwe try to better understand how such models, specifically mT5, transfer *any*\nlinguistic and semantic knowledge across languages, even though no explicit\ncross-lingual signals are provided during pre-training. Rather, only\nunannotated texts from each language are presented to the model separately and\nindependently of one another, and the model appears to implicitly learn\ncross-lingual connections. This raises several questions that motivate our\nstudy, such as: Are the cross-lingual connections between every language pair\nequally strong? What properties of source and target language impact the\nstrength of cross-lingual transfer? Can we quantify the impact of those\nproperties on the cross-lingual transfer?\n</p>\n<p>In our investigation, we analyze a pre-trained mT5 to discover the attributes\nof cross-lingual connections learned by the model. Through a statistical\ninterpretation framework over 90 language pairs across three tasks, we show\nthat transfer performance can be modeled by a few linguistic and data-derived\nfeatures. These observations enable us to interpret cross-lingual understanding\nof the mT5 model. Through these observations, one can favorably choose the best\nsource language for a task, and can anticipate its training data demands. A key\nfinding of this work is that similarity of syntax, morphology and phonology are\ngood predictors of cross-lingual transfer, significantly more than just the\nlexical similarity of languages. For a given language, we are able to predict\nzero-shot performance, that increases on a logarithmic scale with the number of\nfew-shot target language data points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1\">Benjamin Muller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwardhan_S/0/1/0/all/0/1\">Siddharth Patwardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fauconnier_J/0/1/0/all/0/1\">Jean-Philippe Fauconnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandyke_D/0/1/0/all/0/1\">David Vandyke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sachin Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving End-to-end Speech Translation by Leveraging Auxiliary Speech and Text Data. (arXiv:2212.01778v1 [eess.AS])","link":"http://arxiv.org/abs/2212.01778","description":"<p>We present a method for introducing a text encoder into pre-trained\nend-to-end speech translation systems. It enhances the ability of adapting one\nmodality (i.e., source-language speech) to another (i.e., source-language\ntext). Thus, the speech translation model can learn from both unlabeled and\nlabeled data, especially when the source-language text data is abundant. Beyond\nthis, we present a denoising method to build a robust text encoder that can\ndeal with both normal and noisy text data. Our system sets new\nstate-of-the-arts on the MuST-C En-De, En-Fr, and LibriSpeech En-Fr tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_B/0/1/0/all/0/1\">Bojie Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chunliang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MiLMo:Minority Multilingual Pre-trained Language Model. (arXiv:2212.01779v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01779","description":"<p>Pre-trained language models are trained on large-scale unsupervised data, and\nthey can be fine-tuned on small-scale labeled datasets and achieve good\nresults. Multilingual pre-trained language models can be trained on multiple\nlanguages and understand multiple languages at the same time. At present, the\nresearch on pre-trained models mainly focuses on rich-resource language, while\nthere is relatively little research on low-resource languages such as minority\nlanguages, and the public multilingual pre-trained language model can not work\nwell for minority languages. Therefore, this paper constructs a multilingual\npre-trained language model named MiLMo that performs better on minority\nlanguage tasks, including Mongolian, Tibetan, Uyghur, Kazakh and Korean. To\nsolve the problem of scarcity of datasets on minority languages and verify the\neffectiveness of the MiLMo model, this paper constructs a minority multilingual\ntext classification dataset named MiTC, and trains a word2vec model for each\nlanguage. By comparing the word2vec model and the pre-trained model in the text\nclassification task, this paper provides an optimal scheme for the downstream\ntask research of minority languages. The final experimental results show that\nthe performance of the pre-trained model is better than that of the word2vec\nmodel, and it has achieved the best results in minority multilingual text\nclassification. The multilingual pre-trained language model MiLMo, multilingual\nword2vec model and multilingual text classification dataset MiTC are published\non https://milmo.cmli-nlp.com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hanru Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sisi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1\">Wugedele Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaobing Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An LSTM model for Twitter Sentiment Analysis. (arXiv:2212.01791v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01791","description":"<p>Sentiment analysis on social media such as Twitter provides organizations and\nindividuals an effective way to monitor public emotions towards them and their\ncompetitors. As a result, sentiment analysis has become an important and\nchallenging task. In this work, we have collected seven publicly available and\nmanually annotated twitter sentiment datasets. We create a new training and\ntesting dataset from the collected datasets. We develop an LSTM model to\nclassify sentiment of a tweet and evaluate the model with the new dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mollah_M/0/1/0/all/0/1\">Md Parvez Mollah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Highly Inductive Contexts for Dialogue Safety through Controllable Reverse Generation. (arXiv:2212.01810v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01810","description":"<p>Large pretrained language models can easily produce toxic or biased content,\nwhich is prohibitive for practical use. In order to detect such toxic\ngenerations, existing methods rely on templates, real-world data extraction,\ncrowdsourcing workers, or automatic generation to construct adversarial\ncontexts that are likely to induce toxic generations. However, what type of\ncontext is more likely to induce unsafe responses is still under-explored. In\nthis paper, we identify that context toxicity and context category (e.g.,\n\\textit{profanity}, \\textit{insult}, \\textit{drugs}, etc.) are two important\nfactors to cause safety issues in response generation. Hence, we propose a\nmethod called \\emph{reverse generation} to construct adversarial contexts\nconditioned on a given response, with the flexibility to control category,\ntoxicity level, and inductivity of the generated contexts. Via reverse\ngeneration, we augment the existing BAD dataset and construct a new dataset\nBAD+ which contains more than 120K diverse and highly inductive contexts in 12\ncategories. We test three popular pretrained dialogue models (Blender,\nDialoGPT, and Plato2) and find that BAD+ can largely expose their safety\nproblems. Furthermore, we show that BAD+ can greatly enhance the safety of\ngeneration and reveal the key factors of safety improvement. Our code and\ndataset is available at \\url{https://github.com/thu-coai/Reverse_Generation}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhexin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiale Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiawen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pair-Based Joint Encoding with Relational Graph Convolutional Networks for Emotion-Cause Pair Extraction. (arXiv:2212.01844v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01844","description":"<p>Emotion-cause pair extraction (ECPE) aims to extract emotion clauses and\ncorresponding cause clauses, which have recently received growing attention.\nPrevious methods sequentially encode features with a specified order. They\nfirst encode the emotion and cause features for clause extraction and then\ncombine them for pair extraction. This lead to an imbalance in inter-task\nfeature interaction where features extracted later have no direct contact with\nthe former. To address this issue, we propose a novel Pair-Based Joint Encoding\n(PBJE) network, which generates pairs and clauses features simultaneously in a\njoint feature encoding manner to model the causal relationship in clauses. PBJE\ncan balance the information flow among emotion clauses, cause clauses and\npairs. From a multi-relational perspective, we construct a heterogeneous\nundirected graph and apply the Relational Graph Convolutional Network (RGCN) to\ncapture the various relationship between clauses and the relationship between\npairs and clauses. Experimental results show that PBJE achieves\nstate-of-the-art performance on the Chinese benchmark corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junlong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1\">Xichen Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianli Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Efficient Language Model Pretraining and Downstream Adaptation via Self-Evolution: A Case Study on SuperGLUE. (arXiv:2212.01853v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01853","description":"<p>This technical report briefly describes our JDExplore d-team's Vega v2\nsubmission on the SuperGLUE leaderboard. SuperGLUE is more challenging than the\nwidely used general language understanding evaluation (GLUE) benchmark,\ncontaining eight difficult language understanding tasks, including question\nanswering, natural language inference, word sense disambiguation, coreference\nresolution, and reasoning. [Method] Instead of arbitrarily increasing the size\nof a pretrained language model (PLM), our aim is to 1) fully extract knowledge\nfrom the input pretraining data given a certain parameter budget, e.g., 6B, and\n2) effectively transfer this knowledge to downstream tasks. To achieve goal 1),\nwe propose self-evolution learning for PLMs to wisely predict the informative\ntokens that should be masked, and supervise the masked language modeling (MLM)\nprocess with rectified smooth labels. For goal 2), we leverage the prompt\ntransfer technique to improve the low-resource tasks by transferring the\nknowledge from the foundation model and related downstream tasks to the target\ntask. [Results] According to our submission record (Oct. 2022), with our\noptimized pretraining and fine-tuning strategies, our 6B Vega method achieved\nnew state-of-the-art performance on 4/8 tasks, sitting atop the SuperGLUE\nleaderboard on Oct. 8, 2022, with an average score of 91.3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qihuang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding How Model Size Affects Few-shot Instruction Prompting. (arXiv:2212.01907v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01907","description":"<p>Large Language Models are affected by the phenomena of memorizing and\nforgetting their training data. But how do these vary by model size? We work\ntowards this question by investigating how the model size affects the model's\nability to discriminate a word's meaning in a given context. We introduce a\ndataset called DeltaWords, which evaluates a model's ability to follow\ninstructions to select a sentence which replaces the target word with its\nantonym. We show a weak inverse scaling trend, where task accuracy degrades as\nmodel size increase, under extremely few-shot prompting regimes. We show that\nincreasing the number of examples tend to disproportionately benefit larger\nmodels than smaller models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joaquin_A/0/1/0/all/0/1\">Ayrton San Joaquin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haroen_A/0/1/0/all/0/1\">Ardy Haroen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Similarity of Multilingual Representations Revisited. (arXiv:2212.01924v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01924","description":"<p>Related works used indexes like CKA and variants of CCA to measure the\nsimilarity of cross-lingual representations in multilingual language models. In\nthis paper, we argue that assumptions of CKA/CCA align poorly with one of the\nmotivating goals of cross-lingual learning analysis, i.e., explaining zero-shot\ncross-lingual transfer. We highlight what valuable aspects of cross-lingual\nsimilarity these indexes fail to capture and provide a motivating case study\n\\textit{demonstrating the problem empirically}. Then, we introduce\n\\textit{Average Neuron-Wise Correlation (ANC)} as a straightforward alternative\nthat is exempt from the difficulties of CKA/CCA and is good specifically in a\ncross-lingual context. Finally, we use ANC to construct evidence that the\npreviously introduced ``first align, then predict'' pattern takes place not\nonly in masked language models (MLMs) but also in multilingual models with\n\\textit{causal language modeling} objectives (CLMs). Moreover, we show that the\npattern extends to the \\textit{scaled versions} of the MLMs and CLMs (up to 85x\noriginal mBERT).\\footnote{Our code is publicly available at\n\\url{https://github.com/TartuNLP/xsim}}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Del_M/0/1/0/all/0/1\">Maksym Del</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fishel_M/0/1/0/all/0/1\">Mark Fishel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer-wise Analysis of a Self-supervised Speech Representation Model. (arXiv:2107.04734v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.04734","description":"<p>Recently proposed self-supervised learning approaches have been successful\nfor pre-training speech representation models. The utility of these learned\nrepresentations has been observed empirically, but not much has been studied\nabout the type or extent of information encoded in the pre-trained\nrepresentations themselves. Developing such insights can help understand the\ncapabilities and limits of these models and enable the research community to\nmore efficiently develop their usage for downstream applications. In this work,\nwe begin to fill this gap by examining one recent and successful pre-trained\nmodel (wav2vec 2.0), via its intermediate representation vectors, using a suite\nof analysis tools. We use the metrics of canonical correlation, mutual\ninformation, and performance on simple downstream tasks with non-parametric\nprobes, in order to (i) query for acoustic and linguistic information content,\n(ii) characterize the evolution of information across model layers, and (iii)\nunderstand how fine-tuning the model for automatic speech recognition (ASR)\naffects these observations. Our findings motivate modifying the fine-tuning\nprotocol for ASR, which produces improved word error rates in a low-resource\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1\">Ju-Chieh Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with Self-training. (arXiv:2112.01404v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.01404","description":"<p>Natural language generation from structured data mainly focuses on\nsurface-level descriptions, suffering from uncontrollable content selection and\nlow fidelity. Previous works leverage logical forms to facilitate logical\nknowledge-conditioned text generation. Though achieving remarkable progress,\nthey are data-hungry, which makes the adoption for real-world applications\nchallenging with limited data. To this end, this paper proposes a unified\nframework for logical knowledge-conditioned text generation in the few-shot\nsetting. With only a few seeds logical forms (e.g., 20/100 shot), our approach\nleverages self-training and samples pseudo logical forms based on content and\nstructure consistency. Experimental results demonstrate that our approach can\nobtain better few-shot performance than baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiacheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantics-Preserved Distortion for Personal Privacy Protection in Information Management. (arXiv:2201.00965v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.00965","description":"<p>Although machine learning and especially deep learning methods have played an\nimportant role in the field of information management, privacy protection is an\nimportant and concerning topic in current machine learning models. In\ninformation management field, a large number of texts containing personal\ninformation are produced by users every day. As the model training on\ninformation from users is likely to invade personal privacy, many methods have\nbeen proposed to block the learning and memorizing of the sensitive data in raw\ntexts. In this paper, we try to do this more linguistically via distorting the\ntext while preserving the semantics. In practice, we leverage a recently our\nproposed metric, Neighboring Distribution Divergence, to evaluate the semantic\npreservation during the distortion. Based on the metric, we propose two\nframeworks for semantics-preserved distortion, a generative one and a\nsubstitutive one. We conduct experiments on named entity recognition,\nconstituency parsing, and machine reading comprehension tasks. Results from our\nexperiments show the plausibility and efficiency of our distortion as a method\nfor personal privacy protection. Moreover, we also evaluate the attribute\nattack on three privacy-related tasks in the current natural language\nprocessing field, and the results show the simplicity and effectiveness of our\ndata-based improvement approach compared to the structural improvement\napproach. Further, we also investigate the effects of privacy protection in\nspecific medical information management in this work and show that the medical\ninformation pre-training model using our approach can effectively reduce the\nmemory of patients and symptoms, which fully demonstrates the practicality of\nour approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiajia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letian Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xueyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Question Generation with Continual Lifelong Learning. (arXiv:2201.09696v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.09696","description":"<p>Question Generation (QG), as a challenging Natural Language Processing task,\naims at generating questions based on given answers and context. Existing QG\nmethods mainly focus on building or training models for specific QG datasets.\nThese works are subject to two major limitations: (1) They are dedicated to\nspecific QG formats (e.g., answer-extraction or multi-choice QG), therefore, if\nwe want to address a new format of QG, a re-design of the QG model is required.\n(2) Optimal performance is only achieved on the dataset they were just trained\non. As a result, we have to train and keep various QG models for different QG\ndatasets, which is resource-intensive and ungeneralizable.\n</p>\n<p>To solve the problems, we propose a model named Unified-QG based on lifelong\nlearning techniques, which can continually learn QG tasks across different\ndatasets and formats. Specifically, we first build a format-convert encoding to\ntransform different kinds of QG formats into a unified representation. Then, a\nmethod named \\emph{STRIDER} (\\emph{S}imilari\\emph{T}y \\emph{R}egular\\emph{I}zed\n\\emph{D}ifficult \\emph{E}xample \\emph{R}eplay) is built to alleviate\ncatastrophic forgetting in continual QG learning. Extensive experiments were\nconducted on $8$ QG datasets across $4$ QG formats (answer-extraction,\nanswer-abstraction, multi-choice, and boolean QG) to demonstrate the\neffectiveness of our approach. Experimental results demonstrate that our\nUnified-QG can effectively and continually adapt to QG tasks when datasets and\nformats vary. In addition, we verify the ability of a single trained Unified-QG\nmodel in improving $8$ Question Answering (QA) systems' performance through\ngenerating synthetic QA data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Wei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tieke He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiufeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lizhen Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kernelized Concept Erasure. (arXiv:2201.12191v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12191","description":"<p>The representation space of neural models for textual data emerges in an\nunsupervised manner during training. Understanding how those representations\nencode human-interpretable concepts is a fundamental problem. One prominent\napproach for the identification of concepts in neural representations is\nsearching for a linear subspace whose erasure prevents the prediction of the\nconcept from the representations. However, while many linear erasure algorithms\nare tractable and interpretable, neural networks do not necessarily represent\nconcepts in a linear manner. To identify non-linearly encoded concepts, we\npropose a kernelization of a linear minimax game for concept erasure. We\ndemonstrate that it is possible to prevent specific non-linear adversaries from\npredicting the concept. However, the protection does not transfer to different\nnonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded\nconcept remains an open problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francisco Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CL-XABSA: Contrastive Learning for Cross-lingual Aspect-based Sentiment Analysis. (arXiv:2204.00791v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00791","description":"<p>As an extensive research in the field of natural language processing (NLP),\naspect-based sentiment analysis (ABSA) is the task of predicting the sentiment\nexpressed in a text relative to the corresponding aspect. Unfortunately, most\nlanguages lack sufficient annotation resources, thus more and more recent\nresearchers focus on cross-lingual aspect-based sentiment analysis (XABSA).\nHowever, most recent researches only concentrate on cross-lingual data\nalignment instead of model alignment. To this end, we propose a novel\nframework, CL-XABSA: Contrastive Learning for Cross-lingual Aspect-Based\nSentiment Analysis. Based on contrastive learning, we close the distance\nbetween samples with the same label in different semantic spaces, thus\nachieving a convergence of semantic spaces of different languages.\nSpecifically, we design two contrastive strategies, token level contrastive\nlearning of token embeddings (TL-CTE) and sentiment level contrastive learning\nof token embeddings (SL-CTE), to regularize the semantic space of source and\ntarget language to be more uniform. Since our framework can receive datasets in\nmultiple languages during training, our framework can be adapted not only for\nXABSA task but also for multilingual aspect-based sentiment analysis (MABSA).\nTo further improve the performance of our model, we perform knowledge\ndistillation technology leveraging data from unlabeled target language. In the\ndistillation XABSA task, we further explore the comparative effectiveness of\ndifferent data (source dataset, translated dataset, and code-switched dataset).\nThe results demonstrate that the proposed method has a certain improvement in\nthe three tasks of XABSA, distillation XABSA and MABSA. For reproducibility,\nour code for this paper is available at https://github.com/GKLMIP/CL-XABSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaotian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aimin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation. (arXiv:2204.00862v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00862","description":"<p>Existing reference-free metrics have obvious limitations for evaluating\ncontrolled text generation models. Unsupervised metrics can only provide a\ntask-agnostic evaluation result which correlates weakly with human judgments,\nwhereas supervised ones may overfit task-specific data with poor generalization\nability to other datasets. In this paper, we propose an unsupervised\nreference-free metric called CTRLEval, which evaluates controlled text\ngeneration from different aspects by formulating each aspect into multiple text\ninfilling tasks. On top of these tasks, the metric assembles the generation\nprobabilities from a pre-trained language model without any model training.\nExperimental results show that our metric has higher correlations with human\njudgments than other baselines, while obtaining better generalization of\nevaluating generated texts from different models and with different qualities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PreQuEL: Quality Estimation of Machine Translation Outputs in Advance. (arXiv:2205.09178v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09178","description":"<p>We present the task of PreQuEL, Pre-(Quality-Estimation) Learning. A PreQuEL\nsystem predicts how well a given sentence will be translated, without recourse\nto the actual translation, thus eschewing unnecessary resource allocation when\ntranslation quality is bound to be low. PreQuEL can be defined relative to a\ngiven MT system (e.g., some industry service) or generally relative to the\nstate-of-the-art. From a theoretical perspective, PreQuEL places the focus on\nthe source text, tracing properties, possibly linguistic features, that make a\nsentence harder to machine translate.\n</p>\n<p>We develop a baseline model for the task and analyze its performance. We also\ndevelop a data augmentation method (from parallel corpora), that improves\nresults substantially. We show that this augmentation method can improve the\nperformance of the Quality-Estimation task as well. We investigate the\nproperties of the input text that our model is sensitive to, by testing it on\nchallenge sets and different languages. We conclude that it is aware of\nsyntactic and semantic distinctions, and correlates and even over-emphasizes\nthe importance of standard NLP features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Don_Yehiya_S/0/1/0/all/0/1\">Shachar Don-Yehiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Machine Translation with Hyper-Adapters. (arXiv:2205.10835v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10835","description":"<p>Multilingual machine translation suffers from negative interference across\nlanguages. A common solution is to relax parameter sharing with\nlanguage-specific modules like adapters. However, adapters of related languages\nare unable to transfer information, and their total number of parameters\nbecomes prohibitively expensive as the number of languages grows. In this work,\nwe overcome these drawbacks using hyper-adapters -- hyper-networks that\ngenerate adapters from language and layer embeddings. While past work had poor\nresults when scaling hyper-networks, we propose a rescaling fix that\nsignificantly improves convergence and enables training larger hyper-networks.\nWe find that hyper-adapters are more parameter efficient than regular adapters,\nreaching the same performance with up to 12 times less parameters. When using\nthe same number of parameters and FLOPS, our approach consistently outperforms\nregular adapters. Also, hyper-adapters converge faster than alternative\napproaches and scale better than regular dense networks. Our analysis shows\nthat hyper-adapters learn to encode language relatedness, enabling positive\ntransfer across languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baziotis_C/0/1/0/all/0/1\">Christos Baziotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammar Detection for Sentiment Analysis through Improved Viterbi Algorithm. (arXiv:2205.13148v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.13148","description":"<p>Grammar Detection, also referred to as Parts of Speech Tagging of raw text,\nis considered an underlying building block of the various Natural Language\nProcessing pipelines like named entity recognition, question answering, and\nsentiment analysis. In short, forgiven a sentence, Parts of Speech tagging is\nthe task of specifying and tagging each word of a sentence with nouns, verbs,\nadjectives, adverbs, and more. Sentiment Analysis may well be a procedure\naccustomed to determining if a given sentence's emotional tone is neutral,\npositive or negative. To assign polarity scores to the thesis or entities\nwithin phrase, in-text analysis and analytics, machine learning and natural\nlanguage processing, approaches are incorporated. This Sentiment Analysis using\nPOS tagger helps us urge a summary of the broader public over a specific topic.\nFor this, we are using the Viterbi algorithm, Hidden Markov Model, Constraint\nbased Viterbi algorithm for POS tagging. By comparing the accuracies, we select\nthe foremost accurate result of the model for Sentiment Analysis for\ndetermining the character of the sentence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chavali_S/0/1/0/all/0/1\">Surya Teja Chavali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandavalli_C/0/1/0/all/0/1\">Charan Tej Kandavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_S/0/1/0/all/0/1\">Sugash T M</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoNT: Contrastive Neural Text Generation. (arXiv:2205.14690v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14690","description":"<p>Recently, contrastive learning attracts increasing interests in neural text\ngeneration as a new solution to alleviate the exposure bias problem. It\nintroduces a sequence-level training signal which is crucial to generation\ntasks that always rely on auto-regressive decoding. However, previous methods\nusing contrastive learning in neural text generation usually lead to inferior\nperformance. In this paper, we analyse the underlying reasons and propose a new\nContrastive Neural Text generation framework, CoNT. CoNT addresses bottlenecks\nthat prevent contrastive learning from being widely adopted in generation tasks\nfrom three aspects -- the construction of contrastive examples, the choice of\nthe contrastive loss, and the strategy in decoding. We validate CoNT on five\ngeneration tasks with ten benchmarks, including machine translation,\nsummarization, code comment generation, data-to-text generation and commonsense\ngeneration. Experimental results show that CoNT clearly outperforms the\nconventional training framework on all the ten benchmarks with a convincing\nmargin. Especially, CoNT surpasses previous the most competitive contrastive\nlearning method for text generation, by 1.50 BLEU on machine translation and\n1.77 ROUGE-1 on summarization, respectively. It achieves new state-of-the-art\non summarization, code comment generation (without external data) and\ndata-to-text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_C/0/1/0/all/0/1\">Chenxin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1\">Kai Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language Understanding and Generation. (arXiv:2205.14912v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14912","description":"<p>Sequence-to-sequence (seq2seq) learning is a popular fashion for large-scale\npretraining language models. However, the prior seq2seq pretraining models\ngenerally focus on reconstructive objectives on the decoder side and neglect\nthe effect of encoder-side supervision, which we argue may lead to sub-optimal\nperformance. To verify our hypothesis, we first empirically study the\nfunctionalities of the encoder and decoder in seq2seq pretrained language\nmodels, and find that the encoder takes an important but under-exploitation\nrole than the decoder regarding the downstream performance and neuron\nactivation. Therefore, we propose an encoding-enhanced seq2seq pretraining\nstrategy, namely E2S2, which improves the seq2seq models via integrating more\nefficient self-supervised information into the encoders. Specifically, E2S2\nadopts two self-supervised objectives on the encoder side from two aspects: 1)\nlocally denoising the corrupted sentence (denoising objective); and 2) globally\nlearning better sentence representations (contrastive objective). With the help\nof both objectives, the encoder can effectively distinguish the noise tokens\nand capture high-level (i.e. syntactic and semantic) knowledge, thus\nstrengthening the ability of seq2seq model to accurately achieve the\nconditional generation. On a large diversity of downstream natural language\nunderstanding and generation tasks, E2S2 dominantly improves the performance of\nits powerful backbone models, e.g. BART and T5. For example, upon BART\nbackbone, we achieve +1.1% averaged gain on the general language understanding\nevaluation (GLUE) benchmark and +1.75% F_0.5 score improvement on CoNLL2014\ndataset. We also provide in-depth analyses to show the improvement stems from\nbetter linguistic representation. We hope that our work will foster future\nself-supervision research on seq2seq language model pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qihuang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variable-rate hierarchical CPC leads to acoustic unit discovery in speech. (arXiv:2206.02211v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2206.02211","description":"<p>The success of deep learning comes from its ability to capture the\nhierarchical structure of data by learning high-level representations defined\nin terms of low-level ones. In this paper we explore self-supervised learning\nof hierarchical representations of speech by applying multiple levels of\nContrastive Predictive Coding (CPC). We observe that simply stacking two CPC\nmodels does not yield significant improvements over single-level architectures.\nInspired by the fact that speech is often described as a sequence of discrete\nunits unevenly distributed in time, we propose a model in which the output of a\nlow-level CPC module is non-uniformly downsampled to directly minimize the loss\nof a high-level CPC module. The latter is designed to also enforce a prior of\nseparability and discreteness in its representations by enforcing dissimilarity\nof successive high-level representations through focused negative sampling, and\nby quantization of the prediction targets. Accounting for the structure of the\nspeech signal improves upon single-level CPC features and enhances the\ndisentanglement of the learned representations, as measured by downstream\nspeech recognition tasks, while resulting in a meaningful segmentation of the\nsignal that closely resembles phone boundaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cuervo_S/0/1/0/all/0/1\">Santiago Cuervo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lancucki_A/0/1/0/all/0/1\">Adrian &#x141;a&#x144;cucki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marxer_R/0/1/0/all/0/1\">Ricard Marxer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rychlikowski_P/0/1/0/all/0/1\">Pawe&#x142; Rychlikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1\">Jan Chorowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FRAME: Evaluating Rationale-Label Consistency Metrics for Free-Text Rationales. (arXiv:2207.00779v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.00779","description":"<p>Following how humans communicate, free-text rationales aim to use natural\nlanguage to explain neural language model (LM) behavior. However, free-text\nrationales' unconstrained nature makes them prone to hallucination, so it is\nimportant to have metrics for free-text rationale quality. Existing free-text\nrationale metrics measure how consistent the rationale is with the LM's\npredicted label, but there is no protocol for assessing such metrics'\nreliability. Thus, we propose FRAME, a framework for evaluating rationale-label\nconsistency (RLC) metrics for free-text rationales. FRAME is based on three\naxioms: (1) good metrics should yield highest scores for reference rationales,\nwhich maximize RLC by construction; (2) good metrics should be appropriately\nsensitive to semantic perturbation of rationales; and (3) good metrics should\nbe robust to variation in the LM's task performance. Across three text\nclassification datasets, we show that existing RLC metrics cannot satisfy all\nthree FRAME axioms, since they are implemented via model pretraining which\nmuddles the metric's signal. Then, we introduce a non-pretraining RLC metric\nthat greatly outperforms baselines on (1) and (3), while performing\ncompetitively on (2). Finally, we discuss the limitations of using RLC to\nevaluate free-text rationales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Shaoliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Liang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaochang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1\">Maziar Sanjabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Context Pattern Generation for Entity Set Expansion. (arXiv:2207.08087v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.08087","description":"<p>Entity Set Expansion (ESE) is a valuable task that aims to find entities of\nthe target semantic class described by given seed entities. Various Natural\nLanguage Processing (NLP) and Information Retrieval (IR) downstream\napplications have benefited from ESE due to its ability to discover knowledge.\nAlthough existing corpus-based ESE methods have achieved great progress, they\nstill rely on corpora with high-quality entity information annotated, because\nmost of them need to obtain the context patterns through the position of the\nentity in a sentence. Therefore, the quality of the given corpora and their\nentity annotation has become the bottleneck that limits the performance of such\nmethods. To overcome this dilemma and make the ESE models free from the\ndependence on entity annotation, our work aims to explore a new ESE paradigm,\nnamely corpus-independent ESE. Specifically, we devise a context pattern\ngeneration module that utilizes autoregressive language models (e.g., GPT-2) to\nautomatically generate high-quality context patterns for entities. In addition,\nwe propose the GAPA, a novel ESE framework that leverages the aforementioned\nGenerAted PAtterns to expand target entities. Extensive experiments and\ndetailed analyses on three widely used datasets demonstrate the effectiveness\nof our method. All the codes of our experiments are available at\nhttps://github.com/geekjuruo/GAPA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shulin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Best Prompts for Text-to-Image Models and How to Find Them. (arXiv:2209.11711v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2209.11711","description":"<p>Recent progress in generative models, especially in text-guided diffusion\nmodels, has enabled the production of aesthetically-pleasing imagery resembling\nthe works of professional human artists. However, one has to carefully compose\nthe textual description, called the prompt, and augment it with a set of\nclarifying keywords. Since aesthetics are challenging to evaluate\ncomputationally, human feedback is needed to determine the optimal prompt\nformulation and keyword combination. In this paper, we present a\nhuman-in-the-loop approach to learning the most useful combination of prompt\nkeywords using a genetic algorithm. We also show how such an approach can\nimprove the aesthetic appeal of images depicting the same descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pavlichenko_N/0/1/0/all/0/1\">Nikita Pavlichenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhdanov_F/0/1/0/all/0/1\">Fedor Zhdanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ustalov_D/0/1/0/all/0/1\">Dmitry Ustalov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unveiling the Black Box of PLMs with Semantic Anchors: Towards Interpretable Neural Semantic Parsing. (arXiv:2210.01425v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01425","description":"<p>The recent prevalence of pretrained language models (PLMs) has dramatically\nshifted the paradigm of semantic parsing, where the mapping from natural\nlanguage utterances to structured logical forms is now formulated as a Seq2Seq\ntask. Despite the promising performance, previous PLM-based approaches often\nsuffer from hallucination problems due to their negligence of the structural\ninformation contained in the sentence, which essentially constitutes the key\nsemantics of the logical forms. Furthermore, most works treat PLM as a black\nbox in which the generation process of the target logical form is hidden\nbeneath the decoder modules, which greatly hinders the model's intrinsic\ninterpretability. To address these two issues, we propose to incorporate the\ncurrent PLMs with a hierarchical decoder network. By taking the first-principle\nstructures as the semantic anchors, we propose two novel intermediate\nsupervision tasks, namely Semantic Anchor Extraction and Semantic Anchor\nAlignment, for training the hierarchical decoders and probing the model\nintermediate representations in a self-adaptive manner alongside the\nfine-tuning process. We conduct intensive experiments on several semantic\nparsing benchmarks and demonstrate that our approach can consistently\noutperform the baselines. More importantly, by analyzing the intermediate\nrepresentations of the hierarchical decoders, our approach also makes a huge\nstep toward the intrinsic interpretability of PLMs in the domain of semantic\nparsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lunyiu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiuding Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanlin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_J/0/1/0/all/0/1\">Jidong Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Sentiment Analysis By Emotion Lexicon Approach on Vietnamese Texts. (arXiv:2210.02063v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02063","description":"<p>The sentiment analysis task has various applications in practice. In the\nsentiment analysis task, words and phrases that represent positive and negative\nemotions are important. Finding out the words that represent the emotion from\nthe text can improve the performance of the classification models for the\nsentiment analysis task. In this paper, we propose a methodology that combines\nthe emotion lexicon with the classification model to enhance the accuracy of\nthe models. Our experimental results show that the emotion lexicon combined\nwith the classification model improves the performance of models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doan_A/0/1/0/all/0/1\">An Long Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners. (arXiv:2210.02969v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02969","description":"<p>Meta-training, which fine-tunes the language model (LM) on various downstream\ntasks by maximizing the likelihood of the target label given the task\ninstruction and input instance, has improved the zero-shot task generalization\nperformance. However, meta-trained LMs still struggle to generalize to\nchallenging tasks containing novel labels unseen during meta-training. In this\npaper, we propose Flipped Learning, an alternative method of meta-training\nwhich trains the LM to generate the task instruction given the input instance\nand label. During inference, the LM trained with Flipped Learning, referred to\nas Flipped, selects the label option that is most likely to generate the task\ninstruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped\noutperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on\naverage by 8.4% and 9.7% points, respectively. Flipped gives particularly large\nimprovements on tasks with unseen labels, outperforming T0-11B by up to +20%\naverage F1 score. This indicates that the strong task generalization of Flipped\ncomes from improved generalization to novel labels. We release our code at\nhttps://github.com/seonghyeonye/Flipped-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Joongbo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RARR: Researching and Revising What Language Models Say, Using Language Models. (arXiv:2210.08726v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08726","description":"<p>Language models (LMs) now excel at many tasks such as few-shot learning,\nquestion answering, reasoning, and dialog. However, they sometimes generate\nunsupported or misleading content. A user cannot easily determine whether their\noutputs are trustworthy or not, because most LMs do not have any built-in\nmechanism for attribution to external evidence. To enable attribution while\nstill preserving all the powerful advantages of recent generation models, we\npropose RARR (Retrofit Attribution using Research and Revision), a system that\n1) automatically finds attribution for the output of any text generation model\nand 2) post-edits the output to fix unsupported content while preserving the\noriginal output as much as possible. When applied to the output of several\nstate-of-the-art LMs on a diverse set of generation tasks, we find that RARR\nsignificantly improves attribution while otherwise preserving the original\ninput to a much greater degree than previously explored edit models.\nFurthermore, the implementation of RARR requires only a handful of training\nexamples, a large language model, and standard web search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhuyun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupat_P/0/1/0/all/0/1\">Panupong Pasupat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anthony Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaganty_A/0/1/0/all/0/1\">Arun Tejasvi Chaganty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yicheng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Y. Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1\">Ni Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hongrae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1\">Da-Cheng Juan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guu_K/0/1/0/all/0/1\">Kelvin Guu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topical Segmentation of Spoken Narratives: A Test Case on Holocaust Survivor Testimonies. (arXiv:2210.13783v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13783","description":"<p>The task of topical segmentation is well studied, but previous work has\nmostly addressed it in the context of structured, well-defined segments, such\nas segmentation into paragraphs, chapters, or segmenting text that originated\nfrom multiple sources. We tackle the task of segmenting running (spoken)\nnarratives, which poses hitherto unaddressed challenges. As a test case, we\naddress Holocaust survivor testimonies, given in English. Other than the\nimportance of studying these testimonies for Holocaust research, we argue that\nthey provide an interesting test case for topical segmentation, due to their\nunstructured surface level, relative abundance (tens of thousands of such\ntestimonies were collected), and the relatively confined domain that they\ncover. We hypothesize that boundary points between segments correspond to low\nmutual information between the sentences proceeding and following the boundary.\nBased on this hypothesis, we explore a range of algorithmic approaches to the\ntask, building on previous work on segmentation that uses generative Bayesian\nmodeling and state-of-the-art neural machinery. Compared to manually annotated\nreferences, we find that the developed approaches show considerable\nimprovements over previous work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wagner_E/0/1/0/all/0/1\">Eitan Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keydar_R/0/1/0/all/0/1\">Renana Keydar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinchevski_A/0/1/0/all/0/1\">Amit Pinchevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning New Tasks from a Few Examples with Soft-Label Prototypes. (arXiv:2210.17437v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.17437","description":"<p>It has been experimentally demonstrated that humans are able to learn in a\nmanner that allows them to make predictions on categories for which they have\nnot seen any examples (Malaviya et al., 2022). Sucholutsky and Schonlau (2020)\nhave recently presented a machine learning approach that aims to do the same.\nThey utilise synthetically generated data and demonstrate that it is possible\nto achieve sub-linear scaling and develop models that can learn to recognise N\nclasses from M training samples where M is less than N - aka less-than-one shot\nlearning. Their method was, however, defined for univariate or simple\nmultivariate data (Sucholutsky et al., 2021). We extend it to work on large,\nhigh-dimensional and real-world datasets and empirically validate it in this\nnew and challenging setting. We apply this method to learn previously unseen\nNLP tasks from very few examples (4, 8 or 16). We first generate compact,\nsophisticated less-than-one shot representations called soft-label prototypes\nwhich are fitted on training data, capturing the distribution of different\nclasses across the input domain space. We then use a modified k-Nearest\nNeighbours classifier to demonstrate that soft-label prototypes can classify\ndata competitively, even outperforming much more computationally complex\nfew-shot learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Avyav Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1\">Ekaterina Shutova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1\">Helen Yannakoudakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality. (arXiv:2211.00768v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.00768","description":"<p>Recent visuolinguistic pre-trained models show promising progress on various\nend tasks such as image retrieval and video captioning. Yet, they fail\nmiserably on the recently proposed Winoground dataset, which challenges models\nto match paired images and English captions, with items constructed to overlap\nlexically but differ in meaning (e.g., \"there is a mug in some grass\" vs.\n\"there is some grass in a mug\"). By annotating the dataset using new\nfine-grained tags, we show that solving the Winoground task requires not just\ncompositional language understanding, but a host of other abilities like\ncommonsense reasoning or locating small, out-of-focus objects in low-resolution\nimages. In this paper, we identify the dataset's main challenges through a\nsuite of experiments on related tasks (probing task, image retrieval task),\ndata augmentation, and manual inspection of the dataset. Our analysis suggests\nthat a main challenge in visuolinguistic models may lie in fusing visual and\ntextual representations, rather than in compositional language understanding.\nWe release our annotation and code at\nhttps://github.com/ajd12342/why-winoground-hard .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diwan_A/0/1/0/all/0/1\">Anuj Diwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berry_L/0/1/0/all/0/1\">Layne Berry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Automated Speech Recognition Systems for Conversational Speech: A Linguistic Perspective. (arXiv:2211.02812v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.02812","description":"<p>Automatic speech recognition (ASR) meets more informal and free-form input\ndata as voice user interfaces and conversational agents such as the voice\nassistants such as Alexa, Google Home, etc., gain popularity. Conversational\nspeech is both the most difficult and environmentally relevant sort of data for\nspeech recognition. In this paper, we take a linguistic perspective, and take\nthe French language as a case study toward disambiguation of the French\nhomophones. Our contribution aims to provide more insight into human speech\ntranscription accuracy in conditions to reproduce those of state-of-the-art ASR\nsystems, although in a much focused situation. We investigate a case study\ninvolving the most common errors encountered in the automatic transcription of\nFrench language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1\">Hannaneh B. Pasandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1\">Haniyeh B. Pasandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-SAT: Speech and Text Joint Pretraining for Cross-Lingual Multi-Speaker Text-to-Speech. (arXiv:2211.03545v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2211.03545","description":"<p>Speech representation learning has improved both speech understanding and\nspeech synthesis tasks for single language. However, its ability in\ncross-lingual scenarios has not been explored. In this paper, we extend the\npretraining method for cross-lingual multi-speaker speech synthesis tasks,\nincluding cross-lingual multi-speaker voice cloning and cross-lingual\nmulti-speaker speech editing. We propose a speech-text joint pretraining\nframework, where we randomly mask the spectrogram and the phonemes given a\nspeech example and its transcription. By learning to reconstruct the masked\nparts of the input in different languages, our model shows great improvements\nover speaker-embedding-based multi-speaker TTS methods. Moreover, our framework\nis end-to-end for both the training and the inference without any finetuning\neffort. In cross-lingual multi-speaker voice cloning and cross-lingual\nmulti-speaker speech editing tasks, our experiments show that our model\noutperforms speaker-embedding-based multi-speaker TTS methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_X/0/1/0/all/0/1\">Xiaoran Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pang_C/0/1/0/all/0/1\">Chao Pang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_T/0/1/0/all/0/1\">Tian Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_H/0/1/0/all/0/1\">He Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_R/0/1/0/all/0/1\">Renjie Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_P/0/1/0/all/0/1\">Pengfei Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuohuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Junkun Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Liang Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative layer-wise analysis of self-supervised speech models. (arXiv:2211.03929v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.03929","description":"<p>Many self-supervised speech models, varying in their pre-training objective,\ninput modality, and pre-training data, have been proposed in the last few\nyears. Despite impressive empirical successes on downstream tasks, we still\nhave a limited understanding of the properties encoded by the models and the\ndifferences across models. In this work, we examine the intermediate\nrepresentations for a variety of recent models. Specifically, we measure\nacoustic, phonetic, and word-level properties encoded in individual layers,\nusing a lightweight analysis tool based on canonical correlation analysis\n(CCA). We find that these properties evolve across layers differently depending\non the model, and the variations relate to the choice of pre-training\nobjective. We further investigate the utility of our analyses for downstream\ntasks by comparing the property trends with performance on speech recognition\nand spoken language understanding tasks. We discover that CCA trends provide\nreliable guidance to choose layers of interest for downstream tasks and that\nsingle-layer performance often matches or improves upon using all layers,\nsuggesting implications for more efficient use of pre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale. (arXiv:2211.07636v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.07636","description":"<p>We launch EVA, a vision-centric foundation model to explore the limits of\nvisual representation at scale using only publicly accessible data. EVA is a\nvanilla ViT pre-trained to reconstruct the masked out image-text aligned vision\nfeatures conditioned on visible image patches. Via this pretext task, we can\nefficiently scale up EVA to one billion parameters, and sets new records on a\nbroad range of representative vision downstream tasks, such as image\nrecognition, video action recognition, object detection, instance segmentation\nand semantic segmentation without heavy supervised training. Moreover, we\nobserve quantitative changes in scaling EVA result in qualitative changes in\ntransfer learning performance that are not present in other models. For\ninstance, EVA takes a great leap in the challenging large vocabulary instance\nsegmentation task: our model achieves almost the same state-of-the-art\nperformance on LVISv1.0 dataset with over a thousand categories and COCO\ndataset with only eighty categories. Beyond a pure vision encoder, EVA can also\nserve as a vision-centric, multi-modal pivot to connect images and text. We\nfind initializing the vision tower of a giant CLIP from EVA can greatly\nstabilize the training and outperform the training from scratch counterpart\nwith much fewer samples and less compute, providing a new direction for scaling\nup and accelerating the costly training of multi-modal foundation models. To\nfacilitate future research, we release all the code and models at\nhttps://github.com/baaivision/EVA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1\">Binhui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Quan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Ledell Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse and conversation impairments in patients with dementia. (arXiv:2211.07971v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07971","description":"<p>Neurodegeneration characterizes individuals with different dementia subtypes\n(e.g., individuals with Alzheimer's Disease, Primary Progressive Aphasia, and\nParkinson's Disease), leading to progressive decline in cognitive, linguistic,\nand social functioning. Speech and language impairments are early symptoms in\nindividuals with focal forms of neurodegenerative conditions, coupled with\ndeficits in cognitive, social, and behavioral domains. This paper reviews the\nfindings on language and communication deficits and identifies the effects of\ndementia on the production and perception of discourse. It discusses findings\nconcerning (i) language function, cognitive representation, and impairment,\n(ii) communicative competence, emotions, empathy, and theory-of-mind, and (iii)\nspeech-in-interaction. It argues that clinical discourse analysis can provide a\ncomprehensive assessment of language and communication skills in individuals,\nwhich complements the existing neurolinguistic evaluation for (differential)\ndiagnosis, prognosis, and treatment efficacy evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Themistocleous_C/0/1/0/all/0/1\">Charalambos Themistocleous</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Tuning on Layer Normalization for Pre-trained Language Models. (arXiv:2211.08682v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08682","description":"<p>Conventional fine-tuning encounters increasing difficulties given the size of\ncurrent Pre-trained Language Models, which makes parameter-efficient tuning\nbecome the focal point of frontier research. Previous methods in this field add\ntunable adapters into MHA or/and FFN of Transformer blocks to enable PLMs\nachieve transferability. However, as an important part of Transformer\narchitecture, the power of layer normalization for parameter-efficent tuning is\nignored. In this paper, we first propose LN-tuning, by tuning the gain and bias\nterm of Layer Normalization module with only 0.03\\% parameters, which is of\nhigh time-efficency and significantly superior to baselines which are less than\n0.1\\% tunable parameters. Further, we study the unified framework of combining\nLN-tuning with previous ones and we find that: (1) the unified framework of\ncombining prefix-tuning, the adapter-based method working on MHA, and LN-tuning\nachieves SOTA performance. (2) unified framework which tunes MHA and LayerNorm\nsimultaneously can get performance improvement but those which tune FFN and\nLayerNorm simultaneous will cause performance decrease. Ablation study\nvalidates LN-tuning is of no abundant parameters and gives a further\nunderstanding of it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Wang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1\">Yu-Ping Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Y/0/1/0/all/0/1\">Yuan Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Taihao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitask Vision-Language Prompt Tuning. (arXiv:2211.11720v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.11720","description":"<p>Prompt Tuning, conditioning on task-specific learned prompt vectors, has\nemerged as a data-efficient and parameter-efficient method for adapting large\npretrained vision-language models to multiple downstream tasks. However,\nexisting approaches usually consider learning prompt vectors for each task\nindependently from scratch, thereby failing to exploit the rich shareable\nknowledge across different vision-language tasks. In this paper, we propose\nmultitask vision-language prompt tuning (MVLPT), which incorporates cross-task\nknowledge into prompt tuning for vision-language models. Specifically, (i) we\ndemonstrate the effectiveness of learning a single transferable prompt from\nmultiple source tasks to initialize the prompt for each target task; (ii) we\nshow many target tasks can benefit each other from sharing prompt vectors and\nthus can be jointly learned via multitask prompt tuning. We benchmark the\nproposed MVLPT using three representative prompt tuning methods, namely text\nprompt tuning, visual prompt tuning, and the unified vision-language prompt\ntuning. Results in 20 vision tasks demonstrate that the proposed approach\noutperforms all single-task baseline prompt tuning methods, setting the new\nstate-of-the-art on the few-shot ELEVATER benchmarks and cross-task\ngeneralization benchmarks. To understand where the cross-task knowledge is most\neffective, we also conduct a large-scale study on task transferability with 20\nvision tasks in 400 combinations for each prompt tuning method. It shows that\nthe most performant MVLPT for each prompt tuning method prefers different task\ncombinations and many tasks can benefit each other, depending on their visual\nsimilarity and label similarity. Code is available at\nhttps://github.com/sIncerass/MVLPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shijia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_B/0/1/0/all/0/1\">Bohan Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Report on the Euphemisms Detection Shared Task. (arXiv:2211.13327v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.13327","description":"<p>This paper presents The Shared Task on Euphemism Detection for the Third\nWorkshop on Figurative Language Processing (FigLang 2022) held in conjunction\nwith EMNLP 2022. Participants were invited to investigate the euphemism\ndetection task: given input text, identify whether it contains a euphemism. The\ninput data is a corpus of sentences containing potentially euphemistic terms\n(PETs) collected from the GloWbE corpus (Davies and Fuchs, 2015), and are\nhuman-annotated as containing either a euphemistic or literal usage of a PET.\nIn this paper, we present the results and analyze the common themes, methods\nand findings of the participating teams\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Patrick Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_A/0/1/0/all/0/1\">Anna Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jing Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frustratingly Easy Label Projection for Cross-lingual Transfer. (arXiv:2211.15613v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15613","description":"<p>Translating training data into many languages has emerged as a practical\nsolution for improving cross-lingual transfer. For tasks that involve\nspan-level annotations, such as information extraction or question answering,\nan additional label projection step is required to map annotated spans onto the\ntranslated texts. Recently, a few efforts have utilized a simple\nmark-then-translate method to jointly perform translation and projection by\ninserting special markers around the labeled spans in the original sentence.\nHowever, as far as we are aware, no empirical analysis has been conducted on\nhow this approach compares to traditional annotation projection based on word\nalignment. In this paper, we present an extensive empirical study across 42\nlanguages and three tasks (QA, NER, and Event Extraction) to evaluate the\neffectiveness and limitations of both methods, filling an important gap in the\nliterature. Experimental results show that our optimized version of\nmark-then-translate, which we call EasyProject, is easily applied to many\nlanguages and works surprisingly well, outperforming the more complex word\nalignment-based methods. We analyze several key factors that affect end-task\nperformance, and show EasyProject works well because it can accurately preserve\nlabel span boundaries after translation. We will publicly release all our code\nand data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Neural Discourse Deixis Resolution in Dialogue. (arXiv:2211.15980v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15980","description":"<p>We adapt Lee et al.'s (2018) span-based entity coreference model to the task\nof end-to-end discourse deixis resolution in dialogue, specifically by\nproposing extensions to their model that exploit task-specific characteristics.\nThe resulting model, dd-utt, achieves state-of-the-art results on the four\ndatasets in the CODI-CRAC 2021 shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_V/0/1/0/all/0/1\">Vincent Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Contrastive Pre-Training for Efficient Video-Text Retrieval. (arXiv:2212.00986v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2212.00986","description":"<p>We present a simple yet effective end-to-end Video-language Pre-training\n(VidLP) framework, Masked Contrastive Video-language Pretraining (MAC), for\nvideo-text retrieval tasks. Our MAC aims to reduce video representation's\nspatial and temporal redundancy in the VidLP model by a mask sampling mechanism\nto improve pre-training efficiency. Comparing conventional temporal sparse\nsampling, we propose to randomly mask a high ratio of spatial regions and only\nfeed visible regions into the encoder as sparse spatial sampling. Similarly, we\nadopt the mask sampling technique for text inputs for consistency. Instead of\nblindly applying the mask-then-prediction paradigm from MAE, we propose a\nmasked-then-alignment paradigm for efficient video-text alignment. The\nmotivation is that video-text retrieval tasks rely on high-level alignment\nrather than low-level reconstruction, and multimodal alignment with masked\nmodeling encourages the model to learn a robust and general multimodal\nrepresentation from incomplete and unstable inputs. Coupling these designs\nenables efficient end-to-end pre-training: reduce FLOPs (60% off), accelerate\npre-training (by 3x), and improve performance. Our MAC achieves\nstate-of-the-art results on various video-text retrieval datasets, including\nMSR-VTT, DiDeMo, and ActivityNet. Our approach is omnivorous to input\nmodalities. With minimal modifications, we achieve competitive results on\nimage-text retrieval tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1\">Fangxun Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Biaolong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yue Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shuwen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yousong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-12-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}