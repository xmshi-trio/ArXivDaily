{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-25T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Weakly-Supervised Questions for Zero-Shot Relation Extraction. (arXiv:2301.09640v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09640","description":"<p>Zero-Shot Relation Extraction (ZRE) is the task of Relation Extraction where\nthe training and test sets have no shared relation types. This very challenging\ndomain is a good test of a model's ability to generalize. Previous approaches\nto ZRE reframed relation extraction as Question Answering (QA), allowing for\nthe use of pre-trained QA models. However, this method required manually\ncreating gold question templates for each new relation. Here, we do away with\nthese gold templates and instead learn a model that can generate questions for\nunseen relations. Our technique can successfully translate relation\ndescriptions into relevant questions, which are then leveraged to generate the\ncorrect tail entity. On tail entity extraction, we outperform the previous\nstate-of-the-art by more than 16 F1 points without using gold question\ntemplates. On the RE-QA dataset where no previous baseline for relation\nextraction exists, our proposed algorithm comes within 0.7 F1 points of a\nsystem that uses gold question templates. Our model also outperforms the\nstate-of-the-art ZRE baselines on the FewRel and WikiZSL datasets, showing that\nQA models no longer need template questions to match the performance of models\nspecifically tailored to the ZRE task. Our implementation is available at\nhttps://github.com/fyshelab/QA-ZRE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Najafi_S/0/1/0/all/0/1\">Saeed Najafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fyshe_A/0/1/0/all/0/1\">Alona Fyshe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective Explanations: Leveraging Human Input to Align Explainable AI. (arXiv:2301.09656v1 [cs.AI])","link":"http://arxiv.org/abs/2301.09656","description":"<p>While a vast collection of explainable AI (XAI) algorithms have been\ndeveloped in recent years, they are often criticized for significant gaps with\nhow humans produce and consume explanations. As a result, current XAI\ntechniques are often found to be hard to use and lack effectiveness. In this\nwork, we attempt to close these gaps by making AI explanations selective -- a\nfundamental property of human explanations -- by selectively presenting a\nsubset from a large set of model reasons based on what aligns with the\nrecipient's preferences. We propose a general framework for generating\nselective explanations by leveraging human input on a small sample. This\nframework opens up a rich design space that accounts for different selectivity\ngoals, types of input, and more. As a showcase, we use a decision-support task\nto explore selective explanations based on what the decision-maker would\nconsider relevant to the decision task. We conducted two experimental studies\nto examine three out of a broader possible set of paradigms based on our\nproposed framework: in Study 1, we ask the participants to provide their own\ninput to generate selective explanations, with either open-ended or\ncritique-based input. In Study 2, we show participants selective explanations\nbased on input from a panel of similar users (annotators). Our experiments\ndemonstrate the promise of selective explanations in reducing over-reliance on\nAI and improving decision outcomes and subjective perceptions of the AI, but\nalso paint a nuanced picture that attributes some of these positive effects to\nthe opportunity to provide one's own input to augment AI explanations. Overall,\nour work proposes a novel XAI framework inspired by human communication\nbehaviors and demonstrates its potentials to encourage future work to better\nalign AI explanations with human production and consumption of explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1\">Vivian Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chacha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Q. Vera Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noisy Parallel Data Alignment. (arXiv:2301.09685v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09685","description":"<p>An ongoing challenge in current natural language processing is how its major\nadvancements tend to disproportionately favor resource-rich languages, leaving\na significant number of under-resourced languages behind. Due to the lack of\nresources required to train and evaluate models, most modern language\ntechnologies are either nonexistent or unreliable to process endangered, local,\nand non-standardized languages. Optical character recognition (OCR) is often\nused to convert endangered language documents into machine-readable data.\nHowever, such OCR output is typically noisy, and most word alignment models are\nnot built to work under such noisy conditions. In this work, we study the\nexisting word-level alignment models under noisy settings and aim to make them\nmore robust to noisy data. Our noise simulation and structural biasing method,\ntested on multiple language pairs, manages to reduce the alignment error rate\non a state-of-the-art neural-based alignment model up to 59.6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Ruoyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRIMEQA: The Prime Repository for State-of-the-Art MultilingualQuestion Answering Research and Development. (arXiv:2301.09715v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09715","description":"<p>The field of Question Answering (QA) has made remarkable progress in recent\nyears, thanks to the advent of large pre-trained language models, newer\nrealistic benchmark datasets with leaderboards, and novel algorithms for key\ncomponents such as retrievers and readers. In this paper, we introduce PRIMEQA:\na one-stop and open-source QA repository with an aim to democratize QA\nre-search and facilitate easy replication of state-of-the-art (SOTA) QA\nmethods. PRIMEQA supports core QA functionalities like retrieval and reading\ncomprehension as well as auxiliary capabilities such as question generation.It\nhas been designed as an end-to-end toolkit for various use cases: building\nfront-end applications, replicating SOTA methods on pub-lic benchmarks, and\nexpanding pre-existing methods. PRIMEQA is available at :\nhttps://github.com/primeqa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_J/0/1/0/all/0/1\">Jaydeep Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_B/0/1/0/all/0/1\">Bhavani Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franz_M/0/1/0/all/0/1\">Martin Franz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fadnis_K/0/1/0/all/0/1\">Kshitij Fadnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bornea_M/0/1/0/all/0/1\">Mihaela Bornea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenthal_S/0/1/0/all/0/1\">Sara Rosenthal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarley_S/0/1/0/all/0/1\">Scott McCarley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vishwajeet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yulong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Riyaz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roukos_S/0/1/0/all/0/1\">Salim Roukos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Ontologies for Arguments. (arXiv:2301.09759v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09759","description":"<p>Many computational argumentation tasks, like stance classification, are\ntopic-dependent: the effectiveness of approaches to these tasks significantly\ndepends on whether the approaches were trained on arguments from the same\ntopics as those they are tested on. So, which are these topics that researchers\ntrain approaches on? This paper contributes the first comprehensive survey of\ntopic coverage, assessing 45 argument corpora. For the assessment, we take the\nfirst step towards building an argument topic ontology, consulting three\ndiverse authoritative sources: the World Economic Forum, the Wikipedia list of\ncontroversial topics, and Debatepedia. Comparing the topic sets between the\nauthoritative sources and corpora, our analysis shows that the corpora\ntopics-which are mostly those frequently discussed in public online fora - are\ncovered well by the sources. However, other topics from the sources are less\nextensively covered by the corpora of today, revealing interesting future\ndirections for corpus construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ajjour_Y/0/1/0/all/0/1\">Yamen Ajjour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiesel_J/0/1/0/all/0/1\">Johannes Kiesel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v1 [cs.LG])","link":"http://arxiv.org/abs/2301.09767","description":"<p>In this paper, a new perspective is suggested for unsupervised Ontology\nMatching (OM) or Ontology Alignment (OA) by treating it as a translation task.\nOntologies are represented as graphs, and the translation is performed from a\nnode in the source ontology graph to a path in the target ontology graph. The\nproposed framework, Truveta Mapper (TM), leverages a multi-task\nsequence-to-sequence transformer model to perform alignment across multiple\nontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables\nthe model to implicitly learn the relationship between different ontologies via\ntransfer-learning without requiring any explicit cross-ontology manually\nlabeled data. This also enables the formulated framework to outperform existing\nsolutions for both runtime latency and alignment quality. The model is\npre-trained and fine-tuned only on publicly available text corpus and\ninner-ontologies data. The proposed solution outperforms state-of-the-art\napproaches, Edit-Similarity, LogMap, AML, BERTMap, and the recently presented\nnew OM frameworks in Ontology Alignment Evaluation Initiative (OAEI22), offers\nlog-linear complexity in contrast to quadratic in the existing end-to-end\nmethods, and overall makes the OM task efficient and more straightforward\nwithout much post-processing involving mapping extension or mapping repair.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amir_M/0/1/0/all/0/1\">Mariyam Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baruah_M/0/1/0/all/0/1\">Murchana Baruah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslamialishah_M/0/1/0/all/0/1\">Mahsa Eslamialishah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_S/0/1/0/all/0/1\">Sina Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahramali_A/0/1/0/all/0/1\">Alireza Bahramali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naddaf_Sh_S/0/1/0/all/0/1\">Sadra Naddaf-Sh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarandioon_S/0/1/0/all/0/1\">Saman Zarandioon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-Patcher: One Mistake worth One Neuron. (arXiv:2301.09785v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09785","description":"<p>Large Transformer-based Pretrained Language Models (PLMs) dominate almost all\nNatural Language Processing (NLP) tasks. Nevertheless, they still make mistakes\nfrom time to time. For a model deployed in an industrial environment, fixing\nthese mistakes quickly and robustly is vital to improve user experiences.\nPrevious works formalize such problems as Model Editing (ME) and mostly focus\non fixing one mistake. However, the one-mistake-fixing scenario is not an\naccurate abstraction of the real-world challenge. In the deployment of AI\nservices, there are ever-emerging mistakes, and the same mistake may recur if\nnot corrected in time. Thus a preferable solution is to rectify the mistakes as\nsoon as they appear nonstop. Therefore, we extend the existing ME into\nSequential Model Editing (SME) to help develop more practical editing methods.\nOur study shows that most current ME methods could yield unsatisfying results\nin this scenario. We then introduce Transformer-Patcher, a novel model editor\nthat can shift the behavior of transformer-based models by simply adding and\ntraining a few neurons in the last Feed-Forward Network layer. Experimental\nresults on both classification and generation tasks show that\nTransformer-Patcher can successively correct up to thousands of errors\n(Reliability) and generalize to their equivalent inputs (Generality) while\nretaining the model's accuracy on irrelevant inputs (Locality). Our method\noutperforms previous fine-tuning and HyperNetwork-based methods and achieves\nstate-of-the-art performance for Sequential Model Editing (SME). The code is\navailable at https://github.com/ZeroYuHuang/Transformer-Patcher.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zeyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yikang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_W/0/1/0/all/0/1\">Wenge Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhang Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Very Large Pretrained Language Models Learn Storytelling With A Few Examples?. (arXiv:2301.09790v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09790","description":"<p>While pre-trained language models can generate individually fluent sentences\nfor automatic story generation, they struggle to generate stories that are\ncoherent, sensible and interesting. Current state-of-the-art (SOTA) story\ngeneration models explore using higher-level features such as plots or\ncommonsense knowledge to improve the quality of generated stories. Prompt-based\nlearning using very large pre-trained language models (VLPLMs) such as GPT3 has\ndemonstrated impressive performance even across various NLP tasks. In this\npaper, we present an extensive study using automatic and human evaluation to\ncompare the story generation capability of VLPLMs to those SOTA models in three\ndifferent datasets where stories differ in style, register and length. Our\nresults show that VLPLMs generate much higher quality stories than other story\ngeneration models, and to a certain extent rival human authors, although\npreliminary investigation also reveals that they tend to ``plagiarise'' real\nstories in scenarios that involve world knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhuohan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource Compositional Semantic Parsing with Concept Pretraining. (arXiv:2301.09809v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09809","description":"<p>Semantic parsing plays a key role in digital voice assistants such as Alexa,\nSiri, and Google Assistant by mapping natural language to structured meaning\nrepresentations. When we want to improve the capabilities of a voice assistant\nby adding a new domain, the underlying semantic parsing model needs to be\nretrained using thousands of annotated examples from the new domain, which is\ntime-consuming and expensive. In this work, we present an architecture to\nperform such domain adaptation automatically, with only a small amount of\nmetadata about the new domain and without any new training data (zero-shot) or\nwith very few examples (few-shot). We use a base seq2seq (sequence-to-sequence)\narchitecture and augment it with a concept encoder that encodes intent and slot\ntags from the new domain. We also introduce a novel decoder-focused approach to\npretrain seq2seq models to be concept aware using Wikidata and use it to help\nour model learn important concepts and perform well in low-resource settings.\nWe report few-shot and zero-shot results for compositional semantic parsing on\nthe TOPv2 dataset and show that our model outperforms prior approaches in\nfew-shot settings for the TOPv2 and SNIPS datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rongali_S/0/1/0/all/0/1\">Subendhu Rongali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harakere_M/0/1/0/all/0/1\">Mukund Sridhar Harakere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Haidar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arkoudas_K/0/1/0/all/0/1\">Konstantine Arkoudas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamza_W/0/1/0/all/0/1\">Wael Hamza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Stability Analysis of Fine-Tuning a Pre-Trained Model. (arXiv:2301.09820v1 [cs.LG])","link":"http://arxiv.org/abs/2301.09820","description":"<p>Fine-tuning a pre-trained model (such as BERT, ALBERT, RoBERTa, T5, GPT,\netc.) has proven to be one of the most promising paradigms in recent NLP\nresearch. However, numerous recent works indicate that fine-tuning suffers from\nthe instability problem, i.e., tuning the same model under the same setting\nresults in significantly different performance. Many recent works have proposed\ndifferent methods to solve this problem, but there is no theoretical\nunderstanding of why and how these methods work. In this paper, we propose a\nnovel theoretical stability analysis of fine-tuning that focuses on two\ncommonly used settings, namely, full fine-tuning and head tuning. We define the\nstability under each setting and prove the corresponding stability bounds. The\ntheoretical bounds explain why and how several existing methods can stabilize\nthe fine-tuning procedure. In addition to being able to explain most of the\nobserved empirical discoveries, our proposed theoretical analysis framework can\nalso help in the design of effective and provable methods. Based on our theory,\nwe propose three novel strategies to stabilize the fine-tuning procedure,\nnamely, Maximal Margin Regularizer (MMR), Multi-Head Loss (MHLoss), and Self\nUnsupervised Re-Training (SURT). We extensively evaluate our proposed\napproaches on 11 widely used real-world benchmark datasets, as well as hundreds\nof synthetic classification datasets. The experiment results show that our\nproposed methods significantly stabilize the fine-tuning procedure and also\ncorroborate our theoretical analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zihao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+So_A/0/1/0/all/0/1\">Anthony Man-Cho So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Identification of Disaster News For Crisis Management Using Machine Learning. (arXiv:2301.09896v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09896","description":"<p>A lot of news sources picked up on Typhoon Rai (also known locally as Typhoon\nOdette), along with fake news outlets. The study honed in on the issue, to\ncreate a model that can identify between legitimate and illegitimate news\narticles. With this in mind, we chose the following machine learning algorithms\nin our development: Logistic Regression, Random Forest and Multinomial Naive\nBayes. Bag of Words, TF-IDF and Lemmatization were implemented in the Model.\nGathering 160 datasets from legitimate and illegitimate sources, the machine\nlearning was trained and tested. By combining all the machine learning\ntechniques, the Combined BOW model was able to reach an accuracy of 91.07%,\nprecision of 88.33%, recall of 94.64%, and F1 score of 91.38% and Combined\nTF-IDF model was able to reach an accuracy of 91.18%, precision of 86.89%,\nrecall of 94.64%, and F1 score of 90.60%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Regacho_L/0/1/0/all/0/1\">Lord Christian Carl H. Regacho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsushita_A/0/1/0/all/0/1\">Ai Matsushita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceniza_Canillo_A/0/1/0/all/0/1\">Angie M. Ceniza-Canillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual German Biomedical Information Extraction: from Zero-shot to Human-in-the-Loop. (arXiv:2301.09908v1 [cs.HC])","link":"http://arxiv.org/abs/2301.09908","description":"<p>This paper presents our project proposal for extracting biomedical\ninformation from German clinical narratives with limited amounts of\nannotations. We first describe the applied strategies in transfer learning and\nactive learning for solving our problem. After that, we discuss the design of\nthe user interface for both supplying model inspection and obtaining user\nannotations in the interactive environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Siting Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_M/0/1/0/all/0/1\">Mareike Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1\">Daniel Sonntag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conclusion-based Counter-Argument Generation. (arXiv:2301.09911v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09911","description":"<p>In real-world debates, the most common way to counter an argument is to\nreason against its main point, that is, its conclusion. Existing work on the\nautomatic generation of natural language counter-arguments does not address the\nrelation to the conclusion, possibly because many arguments leave their\nconclusion implicit. In this paper, we hypothesize that the key to effective\ncounter-argument generation is to explicitly model the argument's conclusion\nand to ensure that the stance of the generated counter is opposite to that\nconclusion. In particular, we propose a multitask approach that jointly learns\nto generate both the conclusion and the counter of an input argument. The\napproach employs a stance-based ranking component that selects the counter from\na diverse set of generated candidates whose stance best opposes the generated\nconclusion. In both automatic and manual evaluation, we provide evidence that\nour approach generates more relevant and stance-adhering counters than strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alshomary_M/0/1/0/all/0/1\">Milad Alshomary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applications and Challenges of Sentiment Analysis in Real-life Scenarios. (arXiv:2301.09912v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09912","description":"<p>Sentiment analysis has benefited from the availability of lexicons and\nbenchmark datasets created over decades of research. However, its applications\nto the real world are a driving force for research in SA. This chapter\ndescribes some of these applications and related challenges in real-life\nscenarios. In this chapter, we focus on five applications of SA: health, social\npolicy, e-commerce, digital humanities and other areas of NLP. This chapter is\nintended to equip an NLP researcher with the `what', `why' and `how' of\napplications of SA: what is the application about, why it is important and\nchallenging and how current research in SA deals with the application. We note\nthat, while the use of deep learning techniques is a popular paradigm that\nspans these applications, challenges around privacy and selection bias of\ndatasets is a recurring theme across several applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Aditya Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opportunities and Challenges in Neural Dialog Tutoring. (arXiv:2301.09919v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09919","description":"<p>Designing dialog tutors has been challenging as it involves modeling the\ndiverse and complex pedagogical strategies employed by human tutors. Although\nthere have been significant recent advances in neural conversational systems\nusing large language models and growth in available dialog corpora, dialog\ntutoring has largely remained unaffected by these advances. In this paper, we\nrigorously analyze various generative language models on two dialog tutoring\ndatasets for language learning using automatic and human evaluations to\nunderstand the new opportunities brought by these advances as well as the\nchallenges we must overcome to build models that would be usable in real\neducational settings. We find that although current approaches can model\ntutoring in constrained learning scenarios when the number of concepts to be\ntaught and possible teacher strategies are small, they perform poorly in less\nconstrained scenarios. Our human quality evaluation shows that both models and\nground-truth annotations exhibit low performance in terms of equitable\ntutoring, which measures learning opportunities for students and how engaging\nthe dialog is. To understand the behavior of our models in a real tutoring\nsetting, we conduct a user study using expert annotators and find a\nsignificantly large number of model reasoning errors in 45% of conversations.\nFinally, we connect our findings to outline future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Macina_J/0/1/0/all/0/1\">Jakub Macina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daheim_N/0/1/0/all/0/1\">Nico Daheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_T/0/1/0/all/0/1\">Tanmay Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapur_M/0/1/0/all/0/1\">Manu Kapur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitask Instruction-based Prompting for Fallacy Recognition. (arXiv:2301.09992v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09992","description":"<p>Fallacies are used as seemingly valid arguments to support a position and\npersuade the audience about its validity. Recognizing fallacies is an\nintrinsically difficult task both for humans and machines. Moreover, a big\nchallenge for computational models lies in the fact that fallacies are\nformulated differently across the datasets with differences in the input format\n(e.g., question-answer pair, sentence with fallacy fragment), genre (e.g.,\nsocial media, dialogue, news), as well as types and number of fallacies (from 5\nto 18 types per dataset). To move towards solving the fallacy recognition task,\nwe approach these differences across datasets as multiple tasks and show how\ninstruction-based prompting in a multitask setup based on the T5 model improves\nthe results against approaches built for a specific dataset such as T5, BERT or\nGPT-3. We show the ability of this multitask prompting approach to recognize 28\nunique fallacies across domains and genres and study the effect of model size\nand prompt choice by analyzing the per-class (i.e., fallacy type) results.\nFinally, we analyze the effect of annotation quality on model performance, and\nthe feasibility of complementing this approach with external knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alhindi_T/0/1/0/all/0/1\">Tariq Alhindi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musi_E/0/1/0/all/0/1\">Elena Musi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Inclusive Language to Gender-Neutral Machine Translation. (arXiv:2301.10075v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10075","description":"<p>Gender inclusivity in language has become a central topic of debate and\nresearch. Its application in the cross-lingual contexts of human and machine\ntranslation (MT), however, remains largely unexplored. Here, we discuss\nGender-Neutral Translation (GNT) as a form of gender inclusivity in translation\nand advocate for its adoption for MT models, which have been found to\nperpetuate gender bias and discrimination. To this aim, we review a selection\nof relevant institutional guidelines for Gender-Inclusive Language (GIL) to\ncollect and systematize useful strategies of gender neutralization. Then, we\ndiscuss GNT and its scenarios of use, devising a list of desiderata. Finally,\nwe identify the main technical challenges to the implementation of GNT in MT.\nThroughout these contributions we focus on translation from English into\nItalian, as representative of salient linguistic transfer problems, due to the\ndifferent rules for gender marking in their grammar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piergentili_A/0/1/0/all/0/1\">Andrea Piergentili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fucci_D/0/1/0/all/0/1\">Dennis Fucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savoldi_B/0/1/0/all/0/1\">Beatrice Savoldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bentivogli_L/0/1/0/all/0/1\">Luisa Bentivogli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models as Fiduciaries: A Case Study Toward Robustly Communicating With Artificial Intelligence Through Legal Standards. (arXiv:2301.10095v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10095","description":"<p>Artificial Intelligence (AI) is taking on increasingly autonomous roles,\ne.g., browsing the web as a research assistant and managing money. But\nspecifying goals and restrictions for AI behavior is difficult. Similar to how\nparties to a legal contract cannot foresee every potential \"if-then\"\ncontingency of their future relationship, we cannot specify desired AI behavior\nfor all circumstances. Legal standards facilitate the robust communication of\ninherently vague and underspecified goals. Instructions (in the case of\nlanguage models, \"prompts\") that employ legal standards will allow AI agents to\ndevelop shared understandings of the spirit of a directive that can adapt to\nnovel situations, and generalize expectations regarding acceptable actions to\ntake in unspecified states of the world. Standards have built-in context that\nis lacking from other goal specification languages, such as plain language and\nprogramming languages. Through an empirical study on thousands of evaluation\nlabels we constructed from U.S. court opinions, we demonstrate that large\nlanguage models (LLMs) are beginning to exhibit an \"understanding\" of one of\nthe most relevant legal standards for AI agents: fiduciary obligations.\nPerformance comparisons across models suggest that, as LLMs continue to exhibit\nimproved core capabilities, their legal standards understanding will also\ncontinue to improve. OpenAI's latest LLM has 78% accuracy on our data, their\nprevious release has 73% accuracy, and a model from their 2020 GPT-3 paper has\n27% accuracy (worse than random). Our research is an initial step toward a\nframework for evaluating AI understanding of legal standards more broadly, and\nfor conducting reinforcement learning with legal feedback (RLLF).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nay_J/0/1/0/all/0/1\">John J. Nay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Semantic Scholar Open Data Platform. (arXiv:2301.10140v1 [cs.DL])","link":"http://arxiv.org/abs/2301.10140","description":"<p>The volume of scientific output is creating an urgent need for automated\ntools to help scientists keep up with developments in their field. Semantic\nScholar (S2) is an open data platform and website aimed at accelerating science\nby helping scholars discover and understand scientific literature. We combine\npublic and proprietary data sources using state-of-the-art techniques for\nscholarly PDF content extraction and automatic knowledge graph construction to\nbuild the Semantic Scholar Academic Graph, the largest open scientific\nliterature graph to-date, with 200M+ papers, 80M+ authors, 550M+\npaper-authorship edges, and 2.4B+ citation edges. The graph includes advanced\nsemantic features such as structurally parsed text, natural language summaries,\nand vector embeddings. In this paper, we describe the components of the S2 data\nprocessing pipeline and the associated APIs offered by the platform. We will\nupdate this living document to reflect changes as we add new data offerings and\nimprove existing services.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kinney_R/0/1/0/all/0/1\">Rodney Kinney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasiades_C/0/1/0/all/0/1\">Chloe Anastasiades</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Authur_R/0/1/0/all/0/1\">Russell Authur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1\">Jonathan Bragg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buraczynski_A/0/1/0/all/0/1\">Alexandra Buraczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cachola_I/0/1/0/all/0/1\">Isabel Cachola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Candra_S/0/1/0/all/0/1\">Stefan Candra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekhar_Y/0/1/0/all/0/1\">Yoganand Chandrasekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crawford_M/0/1/0/all/0/1\">Miles Crawford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunkelberger_J/0/1/0/all/0/1\">Jason Dunkelberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etzioni_O/0/1/0/all/0/1\">Oren Etzioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_R/0/1/0/all/0/1\">Rob Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1\">Sergey Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorney_J/0/1/0/all/0/1\">Joseph Gorney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_D/0/1/0/all/0/1\">David Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fangzhou Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huff_R/0/1/0/all/0/1\">Regan Huff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_D/0/1/0/all/0/1\">Daniel King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohlmeier_S/0/1/0/all/0/1\">Sebastian Kohlmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langan_M/0/1/0/all/0/1\">Michael Langan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Daniel Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haokun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lochner_J/0/1/0/all/0/1\">Jaron Lochner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacMillan_K/0/1/0/all/0/1\">Kelsey MacMillan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_T/0/1/0/all/0/1\">Tyler Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newell_C/0/1/0/all/0/1\">Chris Newell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1\">Smita Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohatgi_S/0/1/0/all/0/1\">Shaurya Rohatgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayre_P/0/1/0/all/0/1\">Paul Sayre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zejiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1\">Shivashankar Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_A/0/1/0/all/0/1\">Amber Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wade_A/0/1/0/all/0/1\">Alex D. Wade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_L/0/1/0/all/0/1\">Linda Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lucy Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilhelm_C/0/1/0/all/0/1\">Chris Wilhelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Caroline Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiangjiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamarron_A/0/1/0/all/0/1\">Angele Zamarron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuylen_M/0/1/0/all/0/1\">Madeleine Van Zuylen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexi: Self-Supervised Learning of the UI Language. (arXiv:2301.10165v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10165","description":"<p>Humans can learn to operate the user interface (UI) of an application by\nreading an instruction manual or how-to guide. Along with text, these resources\ninclude visual content such as UI screenshots and images of application icons\nreferenced in the text. We explore how to leverage this data to learn generic\nvisio-linguistic representations of UI screens and their components. These\nrepresentations are useful in many real applications, such as accessibility,\nvoice navigation, and task automation. Prior UI representation models rely on\nUI metadata (UI trees and accessibility labels), which is often missing,\nincompletely defined, or not accessible. We avoid such a dependency, and\npropose Lexi, a pre-trained vision and language model designed to handle the\nunique features of UI screens, including their text richness and context\nsensitivity. To train Lexi we curate the UICaption dataset consisting of 114k\nUI images paired with descriptions of their functionality. We evaluate Lexi on\nfour tasks: UI action entailment, instruction-based UI image retrieval,\ngrounding referring expressions, and UI entity recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_S/0/1/0/all/0/1\">Shweti Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_K/0/1/0/all/0/1\">Kushal Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riva_O/0/1/0/all/0/1\">Oriana Riva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Vision-Language Models for Granular Market Change Prediction. (arXiv:2301.10166v1 [q-fin.ST])","link":"http://arxiv.org/abs/2301.10166","description":"<p>Predicting future direction of stock markets using the historical data has\nbeen a fundamental component in financial forecasting. This historical data\ncontains the information of a stock in each specific time span, such as the\nopening, closing, lowest, and highest price. Leveraging this data, the future\ndirection of the market is commonly predicted using various time-series models\nsuch as Long-Short Term Memory networks. This work proposes modeling and\npredicting market movements with a fundamentally new approach, namely by\nutilizing image and byte-based number representation of the stock data\nprocessed with the recently introduced Vision-Language models. We conduct a\nlarge set of experiments on the hourly stock data of the German share index and\nevaluate various architectures on stock price prediction using historical stock\ndata. We conduct a comprehensive evaluation of the results with various metrics\nto accurately depict the actual performance of various approaches. Our\nevaluation results show that our novel approach based on representation of\nstock data as text (bytes) and image significantly outperforms strong deep\nlearning-based baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Wimmer_C/0/1/0/all/0/1\">Christopher Wimmer</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MTTN: Multi-Pair Text to Text Narratives for Prompt Generation. (arXiv:2301.10172v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10172","description":"<p>The explosive popularity of diffusion models[ 1][ 2][ 3 ] has provided a huge\nstage for further development in generative-text modelling. As prompt based\nmodels are very nuanced, such that a carefully generated prompt can produce\ntruely breath taking images, on the contrary producing powerful or even\nmeaningful prompt is a hit or a miss. To lavish on this we have introduced a\nlarge scale derived and synthesized dataset built with on real prompts and\nindexed with popular image-text datasets like MS-COCO[4 ], Flickr[ 5], etc. We\nhave also introduced staging for these sentences that sequentially reduce the\ncontext and increase the complexity, that will further strengthen the output\nbecause of the complex annotations that are being created. MTTN consists of\nover 2.4M sentences that are divided over 5 stages creating a combination\namounting to over 12M pairs, along with a vocab size of consisting more than\n300 thousands unique words that creates an abundance of variations. The\noriginal 2.4M million pairs are broken down in such a manner that it produces a\ntrue scenario of internet lingo that is used globally thereby heightening the\nrobustness of the dataset, and any model trained on it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Archan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Debgandhar Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_M/0/1/0/all/0/1\">Madhurima Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1\">Suchinta Chanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_K/0/1/0/all/0/1\">Kalporup Goswami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Purpose Audio-Visual Corpus for Multi-Modal Persian Speech Recognition: the Arman-AV Dataset. (arXiv:2301.10180v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10180","description":"<p>In recent years, significant progress has been made in automatic lip reading.\nBut these methods require large-scale datasets that do not exist for many\nlow-resource languages. In this paper, we have presented a new multipurpose\naudio-visual dataset for Persian. This dataset consists of almost 220 hours of\nvideos with 1760 corresponding speakers. In addition to lip reading, the\ndataset is suitable for automatic speech recognition, audio-visual speech\nrecognition, and speaker recognition. Also, it is the first large-scale lip\nreading dataset in Persian. A baseline method was provided for each mentioned\ntask. In addition, we have proposed a technique to detect visemes (a visual\nequivalent of a phoneme) in Persian. The visemes obtained by this method\nincrease the accuracy of the lip reading task by 7% relatively compared to the\npreviously proposed visemes, which can be applied to other languages as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peymanfard_J/0/1/0/all/0/1\">Javad Peymanfard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heydarian_S/0/1/0/all/0/1\">Samin Heydarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lashini_A/0/1/0/all/0/1\">Ali Lashini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeinali_H/0/1/0/all/0/1\">Hossein Zeinali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_M/0/1/0/all/0/1\">Mohammad Reza Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozayani_N/0/1/0/all/0/1\">Nasser Mozayani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViHOS: Hate Speech Spans Detection for Vietnamese. (arXiv:2301.10186v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10186","description":"<p>The rise in hateful and offensive language directed at other users is one of\nthe adverse side effects of the increased use of social networking platforms.\nThis could make it difficult for human moderators to review tagged comments\nfiltered by classification systems. To help address this issue, we present the\nViHOS (Vietnamese Hate and Offensive Spans) dataset, the first human-annotated\ncorpus containing 26k spans on 11k comments. We also provide definitions of\nhateful and offensive spans in Vietnamese comments as well as detailed\nannotation guidelines. Besides, we conduct experiments with various\nstate-of-the-art models. Specifically, XLM-R$_{Large}$ achieved the best\nF1-scores in Single span detection and All spans detection, while\nPhoBERT$_{Large}$ obtained the highest in Multiple spans detection. Finally,\nour error analysis demonstrates the difficulties in detecting specific types of\nspans in our data for future research.\n</p>\n<p>Disclaimer: This paper contains real comments that could be considered\nprofane, offensive, or abusive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoang_P/0/1/0/all/0/1\">Phu Gia Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_C/0/1/0/all/0/1\">Canh Duc Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khanh Quoc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Watermark for Large Language Models. (arXiv:2301.10226v1 [cs.LG])","link":"http://arxiv.org/abs/2301.10226","description":"<p>Potential harms of large language models can be mitigated by watermarking\nmodel output, i.e., embedding signals into generated text that are invisible to\nhumans but algorithmically detectable from a short span of tokens. We propose a\nwatermarking framework for proprietary language models. The watermark can be\nembedded with negligible impact on text quality, and can be detected using an\nefficient open-source algorithm without access to the language model API or\nparameters. The watermark works by selecting a randomized set of whitelist\ntokens before a word is generated, and then softly promoting use of whitelist\ntokens during sampling. We propose a statistical test for detecting the\nwatermark with interpretable p-values, and derive an information-theoretic\nframework for analyzing the sensitivity of the watermark. We test the watermark\nusing a multi-billion parameter model from the Open Pretrained Transformer\n(OPT) family, and discuss robustness and security.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirchenbauer_J/0/1/0/all/0/1\">John Kirchenbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_J/0/1/0/all/0/1\">Jonathan Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miers_I/0/1/0/all/0/1\">Ian Miers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Early Frequency Attention for Deep Speaker Representation Learning. (arXiv:2009.01822v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2009.01822","description":"<p>Deep learning techniques have considerably improved speech processing in\nrecent years. Speaker representations extracted by deep learning models are\nbeing used in a wide range of tasks such as speaker recognition and speech\nemotion recognition. Attention mechanisms have started to play an important\nrole in improving deep learning models in the field of speech processing.\nNonetheless, despite the fact that important speaker-related information can be\nembedded in individual frequency-bins of the input spectral representations,\ncurrent attention models are unable to attend to fine-grained information items\nin spectral representations. In this paper we propose Fine-grained Early\nFrequency Attention (FEFA) for speaker representation learning. Our model is a\nsimple and lightweight model that can be integrated into various CNN pipelines\nand is capable of focusing on information items as small as frequency-bins. We\nevaluate the proposed model on three tasks of speaker recognition, speech\nemotion recognition, and spoken digit recognition. We use Three widely used\npublic datasets, namely VoxCeleb, IEMOCAP, and Free Spoken Digit Dataset for\nour experiments. We attach FEFA to several prominent deep learning models and\nevaluate its impact on the final performance. We also compare our work with\nother related works in the area. Our experiments show that by adding FEFA to\ndifferent CNN architectures, performance is consistently improved by\nsubstantial margins, and the models equipped with FEFA outperform all the other\nattentive models. We also test our model against different levels of added\nnoise showing improvements in robustness and less sensitivity compared to the\nbackbone networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hajavi_A/0/1/0/all/0/1\">Amirhossein Hajavi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness through Data Augmentation Loss Consistency. (arXiv:2110.11205v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.11205","description":"<p>While deep learning through empirical risk minimization (ERM) has succeeded\nat achieving human-level performance at a variety of complex tasks, ERM is not\nrobust to distribution shifts or adversarial attacks. Synthetic data\naugmentation followed by empirical risk minimization (DA-ERM) is a simple and\nwidely used solution to improve robustness in ERM. In addition, consistency\nregularization can be applied to further improve the robustness of the model by\nforcing the representation of the original sample and the augmented one to be\nsimilar. However, existing consistency regularization methods are not\napplicable to covariant data augmentation, where the label in the augmented\nsample is dependent on the augmentation function. For example, dialog state\ncovaries with named entity when we augment data with a new named entity. In\nthis paper, we propose data augmented loss invariant regularization (DAIR), a\nsimple form of consistency regularization that is applied directly at the loss\nlevel rather than intermediate features, making it widely applicable to both\ninvariant and covariant data augmentation regardless of network architecture,\nproblem setup, and task. We apply DAIR to real-world learning problems\ninvolving covariant data augmentation: robust neural task-oriented dialog state\ntracking and robust visual question answering. We also apply DAIR to tasks\ninvolving invariant data augmentation: robust regression, robust classification\nagainst adversarial attacks, and robust ImageNet classification under\ndistribution shift. Our experiments show that DAIR consistently outperforms ERM\nand DA-ERM with little marginal computational cost and sets new\nstate-of-the-art results in several benchmarks involving covariant data\naugmentation. Our code of all experiments is available at:\nhttps://github.com/optimization-for-data-driven-science/DAIR.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianjian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halbe_S/0/1/0/all/0/1\">Shaunak Halbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_P/0/1/0/all/0/1\">Pooyan Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1\">Satwik Kottur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1\">Alborz Geramifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razaviyayn_M/0/1/0/all/0/1\">Meisam Razaviyayn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1\">Ahmad Beirami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HTMOT : Hierarchical Topic Modelling Over Time. (arXiv:2112.03104v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2112.03104","description":"<p>Over the years, topic models have provided an efficient way of extracting\ninsights from text. However, while many models have been proposed, none are\nable to model topic temporality and hierarchy jointly. Modelling time provide\nmore precise topics by separating lexically close but temporally distinct\ntopics while modelling hierarchy provides a more detailed view of the content\nof a document corpus. In this study, we therefore propose a novel method,\nHTMOT, to perform Hierarchical Topic Modelling Over Time. We train HTMOT using\na new implementation of Gibbs sampling, which is more efficient. Specifically,\nwe show that only applying time modelling to deep sub-topics provides a way to\nextract specific stories or events while high level topics extract larger\nthemes in the corpus. Our results show that our training procedure is fast and\ncan extract accurate high-level topics and temporally precise sub-topics. We\nmeasured our model's performance using the Word Intrusion task and outlined\nsome limitations of this evaluation method, especially for hierarchical models.\nAs a case study, we focused on the various developments in the space industry\nin 2020.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poumay_J/0/1/0/all/0/1\">Judicael Poumay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ittoo_A/0/1/0/all/0/1\">Ashwin Ittoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can large language models reason about medical questions?. (arXiv:2207.08143v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.08143","description":"<p>Although large language models (LLMs) often produce impressive outputs, it\nremains unclear how they perform in real-world scenarios requiring strong\nreasoning skills and expert domain knowledge. We set out to investigate whether\nGPT-3.5 (Codex and InstructGPT) can be applied to answer and reason about\ndifficult real-world-based questions. We utilize two multiple-choice medical\nexam questions (USMLE and MedMCQA) and a medical reading comprehension dataset\n(PubMedQA). We investigate multiple prompting scenarios: Chain-of-Thought (CoT,\nthink step-by-step), zero- and few-shot (prepending the question with\nquestion-answer exemplars) and retrieval augmentation (injecting Wikipedia\npassages into the prompt). For a subset of the USMLE questions, a medical\nexpert reviewed and annotated the model's CoT. We found that InstructGPT can\noften read, reason and recall expert knowledge. Failure are primarily due to\nlack of knowledge and reasoning errors and trivial guessing heuristics are\nobserved, e.g.\\ too often predicting labels A and D on USMLE. Sampling and\ncombining many completions overcome some of these limitations. Using 100\nsamples, Codex 5-shot CoT not only gives close to well-calibrated predictive\nprobability but also achieves human-level performances on the three datasets.\nUSMLE: 60.2%, MedMCQA: 62.7% and PubMedQA: 78.2%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lievin_V/0/1/0/all/0/1\">Valentin Li&#xe9;vin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hother_C/0/1/0/all/0/1\">Christoffer Egeberg Hother</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning over Different Types of Knowledge Graphs: Static, Temporal and Multi-Modal. (arXiv:2212.05767v5 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.05767","description":"<p>Knowledge graph reasoning (KGR), aiming to deduce new facts from existing\nfacts based on mined logic rules underlying knowledge graphs (KGs), has become\na fast-growing research direction. It has been proven to significantly benefit\nthe usage of KGs in many AI applications, such as question answering and\nrecommendation systems, etc. According to the graph types, the existing KGR\nmodels can be roughly divided into three categories, i.e., static models,\ntemporal models, and multi-modal models. The early works in this domain mainly\nfocus on static KGR and tend to directly apply general knowledge graph\nembedding models to the reasoning task. However, these models are not suitable\nfor more complex but practical tasks, such as inductive static KGR, temporal\nKGR, and multi-modal KGR. To this end, multiple works have been developed\nrecently, but no survey papers and open-source repositories comprehensively\nsummarize and discuss models in this important direction. To fill the gap, we\nconduct a survey for knowledge graph reasoning tracing from static to temporal\nand then to multi-modal KGs. Concretely, the preliminaries, summaries of KGR\nmodels, and typical datasets are introduced and discussed consequently.\nMoreover, we discuss the challenges and potential opportunities. The\ncorresponding open-source repository is shared on GitHub:\nhttps://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Ke Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingyuan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wenxuan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sihang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Foresight -- Generative Pretrained Transformer (GPT) for Modelling of Patient Timelines using EHRs. (arXiv:2212.08072v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08072","description":"<p>Background: Electronic Health Records hold detailed longitudinal information\nabout each patient's health status and general clinical history, a large\nportion of which is stored within the unstructured text. Existing approaches\nfocus mostly on structured data and a subset of single-domain outcomes. We\nexplore how temporal modelling of patients from free text and structured data,\nusing deep generative transformers can be used to forecast a wide range of\nfuture disorders, substances, procedures or findings. Methods: We present\nForesight, a novel transformer-based pipeline that uses named entity\nrecognition and linking tools to convert document text into structured, coded\nconcepts, followed by providing probabilistic forecasts for future medical\nevents such as disorders, substances, procedures and findings. We processed the\nentire free-text portion from three different hospital datasets totalling\n811336 patients covering both physical and mental health. Findings: On tests in\ntwo UK hospitals (King's College Hospital, South London and Maudsley) and the\nUS MIMIC-III dataset precision@10 0.68, 0.76 and 0.88 was achieved for\nforecasting the next disorder in a patient timeline, while precision@10 of\n0.80, 0.81 and 0.91 was achieved for forecasting the next biomedical concept.\nForesight was also validated on 34 synthetic patient timelines by five\nclinicians and achieved relevancy of 97% for the top forecasted candidate\ndisorder. As a generative model, it can forecast follow-on biomedical concepts\nfor as many steps as required. Interpretation: Foresight is a general-purpose\nmodel for biomedical concept modelling that can be used for real-world risk\nforecasting, virtual trials and clinical research to study the progression of\ndisorders, simulate interventions and counterfactuals, and educational\npurposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kraljevic_Z/0/1/0/all/0/1\">Zeljko Kraljevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bean_D/0/1/0/all/0/1\">Dan Bean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shek_A/0/1/0/all/0/1\">Anthony Shek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendayan_R/0/1/0/all/0/1\">Rebecca Bendayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemingway_H/0/1/0/all/0/1\">Harry Hemingway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_J/0/1/0/all/0/1\">Joshua Au Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_A/0/1/0/all/0/1\">Alexander Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baston_A/0/1/0/all/0/1\">Alfie Baston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_J/0/1/0/all/0/1\">Jack Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idowu_E/0/1/0/all/0/1\">Esther Idowu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_J/0/1/0/all/0/1\">James T Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1\">Richard J Dobson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammar construction methods for extended deterministic expressions. (arXiv:2301.01621v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.01621","description":"<p>Extended regular expressions with counting and interleaving are widely used\nin practice. However the related theoretical studies for this kind of\nexpressions currently cannot meet the need of practical work. This paper\ndevelops syntax definitions for extended deterministic expressions and their\nsubclasses, hope to completely solve the long-standing problem that there are\nno syntax definitions for this kind of expressions, which has become an\nimportant reason for restricting the use of extended expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mou_X/0/1/0/all/0/1\">Xiaoying Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Agnostic Data-Driven Inverse Text Normalization. (arXiv:2301.08506v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.08506","description":"<p>With the emergence of automatic speech recognition (ASR) models, converting\nthe spoken form text (from ASR) to the written form is in urgent need. This\ninverse text normalization (ITN) problem attracts the attention of researchers\nfrom various fields. Recently, several works show that data-driven ITN methods\ncan output high-quality written form text. Due to the scarcity of labeled\nspoken-written datasets, the studies on non-English data-driven ITN are quite\nlimited. In this work, we propose a language-agnostic data-driven ITN framework\nto fill this gap. Specifically, we leverage the data augmentation in\nconjunction with neural machine translated data for low resource languages.\nMoreover, we design an evaluation method for language agnostic ITN model when\nonly English data is available. Our empirical evaluation shows this language\nagnostic modeling approach is effective for low resource languages while\npreserving the performance for high resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Szu-Jui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1\">Debjyoti Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yutong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_P/0/1/0/all/0/1\">Peng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuedong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}