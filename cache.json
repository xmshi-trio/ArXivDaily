{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-30T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"CoNAL: Anticipating Outliers with Large Language Models. (arXiv:2211.15718v1 [cs.CL])","link":"http://arxiv.org/abs/2211.15718","description":"<p>In many task settings, text classification models are likely to encounter\nexamples from novel classes on which they cannot predict correctly. Selective\nprediction, in which models abstain on low-confidence examples, provides a\npossible solution, but existing models are often overly confident on OOD\nexamples. To remedy this overconfidence, we introduce Contrastive\nNovelty-Augmented Learning (CoNAL), a two-step method that generates OOD\nexamples representative of novel classes, then trains to decrease confidence on\nthem. First, we generate OOD examples by prompting a large language model\ntwice: we prompt it to enumerate relevant novel labels, then generate examples\nfrom each novel class matching the task format. Second, we train our classifier\nwith a novel contrastive objective that encourages lower confidence on\ngenerated OOD examples than training examples. When trained with CoNAL,\nclassifiers improve in their ability to detect and abstain on OOD examples over\nprior methods by an average of 2.3% AUAC and 5.5% AUROC across 4 NLP datasets,\nwith no cost to in-distribution accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1\">Albert Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlled Language Generation for Language Learning Items. (arXiv:2211.15731v1 [cs.CL])","link":"http://arxiv.org/abs/2211.15731","description":"<p>This work aims to employ natural language generation (NLG) to rapidly\ngenerate items for English language learning applications: this requires both\nlanguage models capable of generating fluent, high-quality English, and to\ncontrol the output of the generation to match the requirements of the relevant\nitems. We experiment with deep pretrained models for this task, developing\nnovel methods for controlling items for factors relevant in language learning:\ndiverse sentences for different proficiency levels and argument structure to\ntest grammar. Human evaluation demonstrates high grammatically scores for all\nmodels (3.4 and above out of 4), and higher length (24%) and complexity (9%)\nover the baseline for the advanced proficiency model. Our results show that we\ncan achieve strong performance while adding additional control to ensure\ndiverse, tailored content for individual users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stowe_K/0/1/0/all/0/1\">Kevin Stowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Debanjan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengxuan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mathematically Modeling the Lexicon Entropy of Emergent Language. (arXiv:2211.15783v1 [cs.CL])","link":"http://arxiv.org/abs/2211.15783","description":"<p>We formulate a stochastic process, FiLex, as a mathematical model of lexicon\nentropy in deep learning-based emergent language systems. Defining a model\nmathematically allows it to generate clear predictions which can be directly\nand decisively tested. We empirically verify across four different environments\nthat FiLex predicts the correct correlation between hyperparameters (training\nsteps, lexicon size, learning rate, rollout buffer size, and Gumbel-Softmax\ntemperature) and the emergent language's entropy in 20 out of 20\nenvironment-hyperparameter combinations. Furthermore, our experiments reveal\nthat different environments show diverse relationships between their\nhyperparameters and entropy which demonstrates the need for a model which can\nmake well-defined predictions at a precise level of granularity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boldt_B/0/1/0/all/0/1\">Brendon Boldt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David Mortensen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Neural Entity Alignment with Compatibility. (arXiv:2211.15833v1 [cs.CL])","link":"http://arxiv.org/abs/2211.15833","description":"<p>Entity Alignment (EA) aims to find equivalent entities between two Knowledge\nGraphs (KGs). While numerous neural EA models have been devised, they are\nmainly learned using labelled data only. In this work, we argue that different\nentities within one KG should have compatible counterparts in the other KG due\nto the potential dependencies among the entities. Making compatible predictions\nthus should be one of the goals of training an EA model along with fitting the\nlabelled data: this aspect however is neglected in current methods. To power\nneural EA models with compatibility, we devise a training framework by\naddressing three problems: (1) how to measure the compatibility of an EA model;\n(2) how to inject the property of being compatible into an EA model; (3) how to\noptimise parameters of the compatibility model. Extensive experiments on\nwidely-used datasets demonstrate the advantages of integrating compatibility\nwithin EA models. In fact, state-of-the-art neural EA models trained within our\nframework using just 5\\% of the labelled data can achieve comparable\neffectiveness with supervised training using 20\\% of the labelled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scells_H/0/1/0/all/0/1\">Harrisen Scells</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Genghong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xia Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifelong Embedding Learning and Transfer for Growing Knowledge Graphs. (arXiv:2211.15845v1 [cs.CL])","link":"http://arxiv.org/abs/2211.15845","description":"<p>Existing knowledge graph (KG) embedding models have primarily focused on\nstatic KGs. However, real-world KGs do not remain static, but rather evolve and\ngrow in tandem with the development of KG applications. Consequently, new facts\nand previously unseen entities and relations continually emerge, necessitating\nan embedding model that can quickly learn and transfer new knowledge through\ngrowth. Motivated by this, we delve into an expanding field of KG embedding in\nthis paper, i.e., lifelong KG embedding. We consider knowledge transfer and\nretention of the learning on growing snapshots of a KG without having to learn\nembeddings from scratch. The proposed model includes a masked KG autoencoder\nfor embedding learning and update, with an embedding transfer strategy to\ninject the learned knowledge into the new entity and relation embeddings, and\nan embedding regularization method to avoid catastrophic forgetting. To\ninvestigate the impacts of different aspects of KG growth, we construct four\ndatasets to evaluate the performance of lifelong KG embedding. Experimental\nresults show that the proposed model outperforms the state-of-the-art inductive\nand lifelong embedding baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yuanning Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zequn Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiqiao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kexin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClueWeb22: 10 Billion Web Documents with Rich Information. (arXiv:2211.15848v1 [cs.IR])","link":"http://arxiv.org/abs/2211.15848","description":"<p>ClueWeb22, the newest iteration of the ClueWeb line of datasets, provides 10\nbillion web pages affiliated with rich information. Its design was influenced\nby the need for a high quality, large scale web corpus to support a range of\nacademic and industry research, for example, in information systems,\nretrieval-augmented AI systems, and model pretraining. Compared with earlier\nClueWeb corpora, the ClueWeb22 corpus is larger, more varied, of\nhigher-quality, and aligned with the document distributions in commercial web\nsearch. Besides raw HTML, ClueWeb22 includes rich information about the web\npages provided by industry-standard document understanding systems, including\nthe visual representation of pages rendered by a web browser, parsed HTML\nstructure information from a neural network parser, and pre-processed cleaned\ndocument text to lower the barrier to entry. Many of these signals have been\nwidely used in industry but are available to the research community for the\nfirst time at this scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Overwijk_A/0/1/0/all/0/1\">Arnold Overwijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VandenBerg_C/0/1/0/all/0/1\">Cameron VandenBerg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1\">Jamie Callan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Opinion Summarization with GPT-3. (arXiv:2211.15914v1 [cs.CL])","link":"http://arxiv.org/abs/2211.15914","description":"<p>Very large language models such as GPT-3 have shown impressive performance\nacross a wide variety of tasks, including text summarization. In this paper, we\nshow that this strong performance extends to opinion summarization. We explore\nseveral pipeline methods for applying GPT-3 to summarize a large collection of\nuser reviews in a zero-shot fashion, notably approaches based on recursive\nsummarization and selecting salient content to summarize through supervised\nclustering or extraction. On two datasets, an aspect-oriented summarization\ndataset of hotel reviews and a generic summarization dataset of Amazon and Yelp\nreviews, we show that the GPT-3 models achieve very strong performance in human\nevaluation. We argue that standard evaluation metrics do not reflect this, and\nevaluate against several new measures targeting faithfulness, factuality, and\ngenericity to contrast these different methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhaskar_A/0/1/0/all/0/1\">Adithya Bhaskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BotSIM: An End-to-End Bot Simulation Toolkit for Commercial Task-Oriented Dialog Systems. (arXiv:2211.15916v1 [cs.CL])","link":"http://arxiv.org/abs/2211.15916","description":"<p>We introduce BotSIM, a modular, open-source Bot SIMulation environment with\ndialog generation, user simulation and conversation analytics capabilities.\nBotSIM aims to serve as a one-stop solution for large-scale data-efficient\nend-to-end evaluation, diagnosis and remediation of commercial task-oriented\ndialog (TOD) systems to significantly accelerate commercial bot development and\nevaluation, reduce cost and time-to-market. BotSIM adopts a layered design\ncomprising the infrastructure layer, the adaptor layer and the application\nlayer. The infrastructure layer hosts key models and components to support\nBotSIM's major functionalities via a streamlined\n\"generation-simulation-remediation\" pipeline. The adaptor layer is used to\nextend BotSIM to accommodate new bot platforms. The application layer provides\na suite of command line tools and a Web App to significantly lower the entry\nbarrier for BotSIM users such as bot admins or practitioners. In this report,\nwe focus on the technical designs of various system components. A detailed case\nstudy using Einstein BotBuilder is also presented to show how to apply BotSIM\npipeline for bot evaluation and remediation. The detailed system descriptions\ncan be found in our system demo paper. The toolkit is available at:\nhttps://github.com/salesforce/BotSIM .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangsen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compressing Cross-Lingual Multi-Task Models at Qualtrics. (arXiv:2211.15927v1 [cs.CL])","link":"http://arxiv.org/abs/2211.15927","description":"<p>Experience management is an emerging business area where organizations focus\non understanding the feedback of customers and employees in order to improve\ntheir end-to-end experiences. This results in a unique set of machine learning\nproblems to help understand how people feel, discover issues they care about,\nand find which actions need to be taken on data that are different in content\nand distribution from traditional NLP domains. In this paper, we present a case\nstudy of building text analysis applications that perform multiple\nclassification tasks efficiently in 12 languages in the nascent business area\nof experience management. In order to scale up modern ML methods on experience\ndata, we leverage cross lingual and multi-task modeling techniques to\nconsolidate our models into a single deployment to avoid overhead. We also make\nuse of model compression and model distillation to reduce overall inference\nlatency and hardware cost to the level acceptable for business needs while\nmaintaining model prediction quality. Our findings show that multi-task\nmodeling improves task performance for a subset of experience management tasks\nin both XLM-R and mBert architectures. Among the compressed architectures we\nexplored, we found that MiniLM achieved the best compression/performance\ntradeoff. Our case study demonstrates a speedup of up to 15.61x with 2.60%\naverage task degradation (or 3.29x speedup with 1.71% degradation) and\nestimated savings of 44% over using the original full-size model. These results\ndemonstrate a successful scaling up of text classification for the challenging\nnew area of ML for experience management.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perry_D/0/1/0/all/0/1\">Daniel Perry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Samir Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gambhir_Y/0/1/0/all/0/1\">Yashmeet Gambhir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhengzheng Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colak_A/0/1/0/all/0/1\">Aaron Colak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending the Subwording Model of Multilingual Pretrained Models for New Languages. (arXiv:2211.15965v1 [cs.CL])","link":"http://arxiv.org/abs/2211.15965","description":"<p>Multilingual pretrained models are effective for machine translation and\ncross-lingual processing because they contain multiple languages in one model.\nHowever, they are pretrained after their tokenizers are fixed; therefore it is\ndifficult to change the vocabulary after pretraining. When we extend the\npretrained models to new languages, we must modify the tokenizers\nsimultaneously. In this paper, we add new subwords to the SentencePiece\ntokenizer to apply a multilingual pretrained model to new languages (Inuktitut\nin this paper). In our experiments, we segmented Inuktitut sentences into\nsubwords without changing the segmentation of already pretrained languages, and\napplied the mBART-50 pretrained model to English-Inuktitut translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imamura_K/0/1/0/all/0/1\">Kenji Imamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Democratizing Machine Learning for Interdisciplinary Scholars: Report on Organizing the NLP+CSS Online Tutorial Series. (arXiv:2211.15971v1 [cs.CL])","link":"http://arxiv.org/abs/2211.15971","description":"<p>Many scientific fields -- including biology, health, education, and the\nsocial sciences -- use machine learning (ML) to help them analyze data at an\nunprecedented scale. However, ML researchers who develop advanced methods\nrarely provide detailed tutorials showing how to apply these methods. Existing\ntutorials are often costly to participants, presume extensive programming\nknowledge, and are not tailored to specific application fields. In an attempt\nto democratize ML methods, we organized a year-long, free, online tutorial\nseries targeted at teaching advanced natural language processing (NLP) methods\nto computational social science (CSS) scholars. Two organizers worked with\nfifteen subject matter experts to develop one-hour presentations with hands-on\nPython code for a range of ML methods and use cases, from data pre-processing\nto analyzing temporal variation of language change. Although live participation\nwas more limited than expected, a comparison of pre- and post-tutorial surveys\nshowed an increase in participants' perceived knowledge of almost one point on\na 7-point Likert scale. Furthermore, participants asked thoughtful questions\nduring tutorials and engaged readily with tutorial content afterwards, as\ndemonstrated by 10K~total views of posted tutorial recordings. In this report,\nwe summarize our organizational efforts and distill five principles for\ndemocratizing ML+X tutorials. We hope future organizers improve upon these\nprinciples and continue to lower barriers to developing ML skills for\nresearchers of all fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stewart_I/0/1/0/all/0/1\">Ian Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keith_K/0/1/0/all/0/1\">Katherine Keith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Neural Discourse Deixis Resolution in Dialogue. (arXiv:2211.15980v1 [cs.CL])","link":"http://arxiv.org/abs/2211.15980","description":"<p>We adapt Lee et al.'s (2018) span-based entity coreference model to the task\nof end-to-end discourse deixis resolution in dialogue, specifically by\nproposing extensions to their model that exploit task-specific characteristics.\nThe resulting model, dd-utt, achieves state-of-the-art results on the four\ndatasets in the CODI-CRAC 2021 shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_V/0/1/0/all/0/1\">Vincent Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Generalized Open Information Extraction. (arXiv:2211.15987v1 [cs.CL])","link":"http://arxiv.org/abs/2211.15987","description":"<p>Open Information Extraction (OpenIE) facilitates the open-domain discovery of\ntextual facts. However, the prevailing solutions evaluate OpenIE models on\nin-domain test sets aside from the training corpus, which certainly violates\nthe initial task principle of domain-independence. In this paper, we propose to\nadvance OpenIE towards a more realistic scenario: generalizing over unseen\ntarget domains with different data distributions from the source training\ndomains, termed Generalized OpenIE. For this purpose, we first introduce GLOBE,\na large-scale human-annotated multi-domain OpenIE benchmark, to examine the\nrobustness of recent OpenIE models to domain shifts, and the relative\nperformance degradation of up to 70% implies the challenges of generalized\nOpenIE. Then, we propose DragonIE, which explores a minimalist graph expression\nof textual fact: directed acyclic graph, to improve the OpenIE generalization.\nExtensive experiments demonstrate that DragonIE beats the previous methods in\nboth in-domain and out-of-domain settings by as much as 6.0% in F1 score\nabsolutely, but there is still ample room for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffG-RL: Leveraging Difference between State and Common Sense. (arXiv:2211.16002v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16002","description":"<p>Taking into account background knowledge as the context has always been an\nimportant part of solving tasks that involve natural language. One\nrepresentative example of such tasks is text-based games, where players need to\nmake decisions based on both description text previously shown in the game, and\ntheir own background knowledge about the language and common sense. In this\nwork, we investigate not simply giving common sense, as can be seen in prior\nresearch, but also its effective usage. We assume that a part of the\nenvironment states different from common sense should constitute one of the\ngrounds for action selection. We propose a novel agent, DiffG-RL, which\nconstructs a Difference Graph that organizes the environment states and common\nsense by means of interactive objects with a dedicated graph encoder. DiffG-RL\nalso contains a framework for extracting the appropriate amount and\nrepresentation of common sense from the source to support the construction of\nthe graph. We validate DiffG-RL in experiments with text-based games that\nrequire common sense and show that it outperforms baselines by 17% of scores.\nThe code is available at https://github.com/ibm/diffg-rl\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tsunehiko Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_D/0/1/0/all/0/1\">Daiki Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tatsubori_M/0/1/0/all/0/1\">Michiaki Tatsubori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Enhanced Contrastive Learning for Solving Math Word Problems. (arXiv:2211.16022v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16022","description":"<p>Solving math word problems is the task that analyses the relation of\nquantities and requires an accurate understanding of contextual natural\nlanguage information. Recent studies show that current models rely on shallow\nheuristics to predict solutions and could be easily misled by small textual\nperturbations. To address this problem, we propose a Textual Enhanced\nContrastive Learning framework, which enforces the models to distinguish\nsemantically similar examples while holding different mathematical logic. We\nadopt a self-supervised manner strategy to enrich examples with subtle textual\nvariance by textual reordering or problem re-construction. We then retrieve the\nhardest to differentiate samples from both equation and textual perspectives\nand guide the model to learn their representations. Experimental results show\nthat our method achieves state-of-the-art on both widely used benchmark\ndatasets and also exquisitely designed challenge datasets in English and\nChinese. \\footnote{Our code and data is available at\n\\url{https://github.com/yiyunya/Textual_CL_MWP}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yibin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhuoyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Multi-Answer Retrieval with Determinantal Point Processes. (arXiv:2211.16029v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16029","description":"<p>Often questions provided to open-domain question answering systems are\nambiguous. Traditional QA systems that provide a single answer are incapable of\nanswering ambiguous questions since the question may be interpreted in several\nways and may have multiple distinct answers. In this paper, we address\nmulti-answer retrieval which entails retrieving passages that can capture\nmajority of the diverse answers to the question. We propose a re-ranking based\napproach using Determinantal point processes utilizing BERT as kernels. Our\nmethod jointly considers query-passage relevance and passage-passage\ncorrelation to retrieve passages that are both query-relevant and diverse.\nResults demonstrate that our re-ranking technique outperforms state-of-the-art\nmethod on the AmbigQA dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nandigam_P/0/1/0/all/0/1\">Poojitha Nandigam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayaprolu_N/0/1/0/all/0/1\">Nikhil Rayaprolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntactic Substitutability as Unsupervised Dependency Syntax. (arXiv:2211.16031v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16031","description":"<p>Syntax is a latent hierarchical structure which underpins the robust and\ncompositional nature of human language. An active line of inquiry is whether\nlarge pretrained language models (LLMs) are able to acquire syntax by training\non text alone; understanding a model's syntactic capabilities is essential to\nunderstanding how it processes and makes use of language. In this paper, we\npropose a new method, SSUD, which allows for the induction of syntactic\nstructures without supervision from gold-standard parses. Instead, we seek to\ndefine formalism-agnostic, model-intrinsic syntactic parses by using a property\nof syntactic relations: syntactic substitutability. We demonstrate both\nquantitative and qualitative gains on dependency parsing tasks using SSUD, and\ninduce syntactic structures which we hope provide clarity into LLMs and\nlinguistic representations, alike.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jian_J/0/1/0/all/0/1\">Jasper Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Extraction Attack against Self-supervised Speech Models. (arXiv:2211.16044v1 [cs.SD])","link":"http://arxiv.org/abs/2211.16044","description":"<p>Self-supervised learning (SSL) speech models generate meaningful\nrepresentations of given clips and achieve incredible performance across\nvarious downstream tasks. Model extraction attack (MEA) often refers to an\nadversary stealing the functionality of the victim model with only query\naccess. In this work, we study the MEA problem against SSL speech model with a\nsmall number of queries. We propose a two-stage framework to extract the model.\nIn the first stage, SSL is conducted on the large-scale unlabeled corpus to\npre-train a small speech model. Secondly, we actively sample a small portion of\nclips from the unlabeled corpus and query the target model with these clips to\nacquire their representations as labels for the small model's second-stage\ntraining. Experiment results show that our sampling methods can effectively\nextract the target model without knowing any information about its model\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_T/0/1/0/all/0/1\">Tsu-Yuan Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen-An Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tung-Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating and reducing the distance between synthetic and real speech distributions. (arXiv:2211.16049v1 [eess.AS])","link":"http://arxiv.org/abs/2211.16049","description":"<p>While modern Text-to-Speech (TTS) systems can produce speech rated highly in\nterms of subjective evaluation, the distance between real and synthetic speech\ndistributions remains understudied, where we use the term \\textit{distribution}\nto mean the sample space of all possible real speech recordings from a given\nset of speakers; or of the synthetic samples that could be generated for the\nsame set of speakers. We evaluate the distance of real and synthetic speech\ndistributions along the dimensions of the acoustic environment, speaker\ncharacteristics and prosody using a range of speech processing measures and the\nrespective Wasserstein distances of their distributions. We reduce these\ndistribution distances along said dimensions by providing utterance-level\ninformation derived from the measures to the model and show they can be\ngenerated at inference time. The improvements to the dimensions translate to\noverall distribution distance reduction approximated using Automatic Speech\nRecognition (ASR) by evaluating the fitness of the synthetic data as training\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Minixhofer_C/0/1/0/all/0/1\">Christoph Minixhofer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klejch_O/0/1/0/all/0/1\">Ond&#x159;ej Klejch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Penalizing Confident Predictions on Largely Perturbed Inputs Does Not Improve Out-of-Distribution Generalization in Question Answering. (arXiv:2211.16093v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16093","description":"<p>Question answering (QA) models are shown to be insensitive to large\nperturbations to inputs; that is, they make correct and confident predictions\neven when given largely perturbed inputs from which humans can not correctly\nderive answers. In addition, QA models fail to generalize to other domains and\nadversarial test sets, while humans maintain high accuracy. Based on these\nobservations, we assume that QA models do not use intended features necessary\nfor human reading but rely on spurious features, causing the lack of\ngeneralization ability. Therefore, we attempt to answer the question: If the\noverconfident predictions of QA models for various types of perturbations are\npenalized, will the out-of-distribution (OOD) generalization be improved? To\nprevent models from making confident predictions on perturbed inputs, we first\nfollow existing studies and maximize the entropy of the output probability for\nperturbed inputs. However, we find that QA models trained to be sensitive to a\ncertain perturbation type are often insensitive to unseen types of\nperturbations. Thus, we simultaneously maximize the entropy for the four\nperturbation types (i.e., word- and sentence-level shuffling and deletion) to\nfurther close the gap between models and humans. Contrary to our expectations,\nalthough models become sensitive to the four types of perturbations, we find\nthat the OOD generalization is not improved. Moreover, the OOD generalization\nis sometimes degraded after entropy maximization. Making unconfident\npredictions on largely perturbed inputs per se may be beneficial to gaining\nhuman trust. However, our negative results suggest that researchers should pay\nattention to the side effect of entropy maximization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1\">Kazutoshi Shinoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1\">Saku Sugawara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dependency-aware Self-training for Entity Alignment. (arXiv:2211.16101v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16101","description":"<p>Entity Alignment (EA), which aims to detect entity mappings (i.e. equivalent\nentity pairs) in different Knowledge Graphs (KGs), is critical for KG fusion.\nNeural EA methods dominate current EA research but still suffer from their\nreliance on labelled mappings. To solve this problem, a few works have explored\nboosting the training of EA models with self-training, which adds confidently\npredicted mappings into the training data iteratively. Though the effectiveness\nof self-training can be glimpsed in some specific settings, we still have very\nlimited knowledge about it. One reason is the existing works concentrate on\ndevising EA models and only treat self-training as an auxiliary tool. To fill\nthis knowledge gap, we change the perspective to self-training to shed light on\nit. In addition, the existing self-training strategies have limited impact\nbecause they introduce either much False Positive noise or a low quantity of\nTrue Positive pseudo mappings. To improve self-training for EA, we propose\nexploiting the dependencies between entities, a particularity of EA, to\nsuppress the noise without hurting the recall of True Positive mappings.\nThrough extensive experiments, we show that the introduction of dependency\nmakes the self-training strategy for EA reach a new level. The value of\nself-training in alleviating the reliance on annotation is actually much higher\nthan what has been realised. Furthermore, we suggest future study on smart data\nannotation to break the ceiling of EA performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tiancheng Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Query-Focused Summarization with Prefix-Merging. (arXiv:2211.16164v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16164","description":"<p>Query-focused summarization has been considered as an important extension for\ntext summarization. It aims to generate a concise highlight for a given query.\nDifferent from text summarization, query-focused summarization has long been\nplagued by the problem of lacking high-quality large-scale datasets. In this\npaper, we investigate the idea that whether we can integrate and transfer the\nknowledge of text summarization and question answering to assist the few-shot\nlearning in query-focused summarization. Here, we propose prefix-merging, a\nprefix-based pretraining strategy for few-shot learning in query-focused\nsummarization. Drawn inspiration from prefix-tuning, we are allowed to\nintegrate the task knowledge from text summarization and question answering\ninto a properly designed prefix and apply the merged prefix to query-focused\nsummarization. With only a small amount of trainable parameters, prefix-merging\noutperforms fine-tuning on query-focused summarization. We further discuss the\ninfluence of different prefix designs and propose a visualized explanation for\nhow prefix-merging works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Ruifeng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziqiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnings from Technological Interventions in a Low Resource Language: Enhancing Information Access in Gondi. (arXiv:2211.16172v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16172","description":"<p>The primary obstacle to developing technologies for low-resource languages is\nthe lack of representative, usable data. In this paper, we report the\ndeployment of technology-driven data collection methods for creating a corpus\nof more than 60,000 translations from Hindi to Gondi, a low-resource vulnerable\nlanguage spoken by around 2.3 million tribal people in south and central India.\nDuring this process, we help expand information access in Gondi across 2\ndifferent dimensions (a) The creation of linguistic resources that can be used\nby the community, such as a dictionary, children's stories, Gondi translations\nfrom multiple sources and an Interactive Voice Response (IVR) based mass\nawareness platform; (b) Enabling its use in the digital domain by developing a\nHindi-Gondi machine translation model, which is compressed by nearly 4 times to\nenable it's edge deployment on low-resource edge devices and in areas of little\nto no internet connectivity. We also present preliminary evaluations of\nutilizing the developed machine translation model to provide assistance to\nvolunteers who are involved in collecting more data for the target language.\nThrough these interventions, we not only created a refined and evaluated corpus\nof 26,240 Hindi-Gondi translations that was used for building the translation\nmodel but also engaged nearly 850 community members who can help take Gondi\nonto the internet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1\">Devansh Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diddee_H/0/1/0/all/0/1\">Harshita Diddee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_A/0/1/0/all/0/1\">Ananya Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1\">Anurag Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santy_S/0/1/0/all/0/1\">Sebastin Santy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mothilal_R/0/1/0/all/0/1\">Ramaravind Kommiya Mothilal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1\">Brij Mohan Lal Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Alok Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_V/0/1/0/all/0/1\">Vishnu Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+U_V/0/1/0/all/0/1\">Venkanna U</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_K/0/1/0/all/0/1\">Kalika Bali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CUNI Submission in WMT22 General Task. (arXiv:2211.16174v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16174","description":"<p>We present the CUNI-Bergamot submission for the WMT22 General translation\ntask. We compete in English$\\rightarrow$Czech direction. Our submission further\nexplores block backtranslation techniques. Compared to the previous work, we\nmeasure performance in terms of COMET score and named entities translation\naccuracy. We evaluate performance of MBR decoding compared to traditional mixed\nbacktranslation training and we show a possible synergy when using both of the\ntechniques simultaneously. The results show that both approaches are effective\nmeans of improving translation quality and they yield even better results when\ncombined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1\">Josef Jon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popel_M/0/1/0/all/0/1\">Martin Popel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SuS-X: Training-Free Name-Only Transfer of Vision-Language Models. (arXiv:2211.16198v1 [cs.CV])","link":"http://arxiv.org/abs/2211.16198","description":"<p>Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet\neffective way to train large-scale vision-language models. CLIP demonstrates\nimpressive zero-shot classification and retrieval on diverse downstream tasks.\nHowever, to leverage its full potential, fine-tuning still appears to be\nnecessary. Fine-tuning the entire CLIP model can be resource-intensive and\nunstable. Moreover, recent methods that aim to circumvent this need for\nfine-tuning still require access to images from the target distribution. In\nthis paper, we pursue a different approach and explore the regime of\ntraining-free \"name-only transfer\" in which the only knowledge we possess about\nthe downstream task comprises the names of downstream target categories. We\npropose a novel method, SuS-X, consisting of two key building blocks -- SuS and\nTIP-X, that requires neither intensive fine-tuning nor costly labelled data.\nSuS-X achieves state-of-the-art zero-shot classification results on 19\nbenchmark datasets. We further show the utility of TIP-X in the training-free\nfew-shot setting, where we again achieve state-of-the-art results over strong\ntraining-free baselines. Code is available at\nhttps://github.com/vishaal27/SuS-X.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Udandarao_V/0/1/0/all/0/1\">Vishaal Udandarao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankush Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoCAD: Automatically Generating Counterfactuals for Mitigating Shortcut Learning. (arXiv:2211.16202v1 [cs.AI])","link":"http://arxiv.org/abs/2211.16202","description":"<p>Recent studies have shown the impressive efficacy of counterfactually\naugmented data (CAD) for reducing NLU models' reliance on spurious features and\nimproving their generalizability. However, current methods still heavily rely\non human efforts or task-specific designs to generate counterfactuals, thereby\nimpeding CAD's applicability to a broad range of NLU tasks. In this paper, we\npresent AutoCAD, a fully automatic and task-agnostic CAD generation framework.\nAutoCAD first leverages a classifier to unsupervisedly identify rationales as\nspans to be intervened, which disentangles spurious and causal features. Then,\nAutoCAD performs controllable generation enhanced by unlikelihood training to\nproduce diverse counterfactuals. Extensive evaluations on multiple\nout-of-domain and challenge benchmarks demonstrate that AutoCAD consistently\nand significantly boosts the out-of-distribution performance of powerful\npre-trained models across different NLU tasks, which is comparable or even\nbetter than previous state-of-the-art human-in-the-loop or task-specific CAD\nmethods. The code is publicly available at https://github.com/thu-coai/AutoCAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jiaxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yeshuang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Which Shortcut Solution Do Question Answering Models Prefer to Learn?. (arXiv:2211.16220v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16220","description":"<p>Question answering (QA) models for reading comprehension tend to learn\nshortcut solutions rather than the solutions intended by QA datasets. QA models\nthat have learned shortcut solutions can achieve human-level performance in\nshortcut examples where shortcuts are valid, but these same behaviors degrade\ngeneralization potential on anti-shortcut examples where shortcuts are invalid.\nVarious methods have been proposed to mitigate this problem, but they do not\nfully take the characteristics of shortcuts themselves into account. We assume\nthat the learnability of shortcuts, i.e., how easy it is to learn a shortcut,\nis useful to mitigate the problem. Thus, we first examine the learnability of\nthe representative shortcuts on extractive and multiple-choice QA datasets.\nBehavioral tests using biased training sets reveal that shortcuts that exploit\nanswer positions and word-label correlations are preferentially learned for\nextractive and multiple-choice QA, respectively. We find that the more\nlearnable a shortcut is, the flatter and deeper the loss landscape is around\nthe shortcut solution in the parameter space. We also find that the\navailability of the preferred shortcuts tends to make the task easier to\nperform from an information-theoretic viewpoint. Lastly, we experimentally show\nthat the learnability of shortcuts can be utilized to construct an effective QA\ntraining set; the more learnable a shortcut is, the smaller the proportion of\nanti-shortcut examples required to achieve comparable performance on shortcut\nand anti-shortcut examples. We claim that the learnability of shortcuts should\nbe considered when designing mitigation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1\">Kazutoshi Shinoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1\">Saku Sugawara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the Measuring Tools: An Automatic Evaluation of Semantic Metrics for Text Corpora. (arXiv:2211.16259v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16259","description":"<p>The ability to compare the semantic similarity between text corpora is\nimportant in a variety of natural language processing applications. However,\nstandard methods for evaluating these metrics have yet to be established. We\npropose a set of automatic and interpretable measures for assessing the\ncharacteristics of corpus-level semantic similarity metrics, allowing sensible\ncomparison of their behavior. We demonstrate the effectiveness of our\nevaluation measures in capturing fundamental characteristics by evaluating them\non a collection of classical and state-of-the-art metrics. Our measures\nrevealed that recently-developed metrics are becoming better in identifying\nsemantic distributional mismatch while classical metrics are more sensitive to\nperturbations in the surface text levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kour_G/0/1/0/all/0/1\">George Kour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ackerman_S/0/1/0/all/0/1\">Samuel Ackerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raz_O/0/1/0/all/0/1\">Orna Raz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farchi_E/0/1/0/all/0/1\">Eitan Farchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmeli_B/0/1/0/all/0/1\">Boaz Carmeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anaby_Tavor_A/0/1/0/all/0/1\">Ateret Anaby-Tavor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Transducer Training: Reduced Memory Consumption with Sample-wise Computation. (arXiv:2211.16270v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16270","description":"<p>The neural transducer is an end-to-end model for automatic speech recognition\n(ASR). While the model is well-suited for streaming ASR, the training process\nremains challenging. During training, the memory requirements may quickly\nexceed the capacity of state-of-the-art GPUs, limiting batch size and sequence\nlengths. In this work, we analyze the time and space complexity of a typical\ntransducer training setup. We propose a memory-efficient training method that\ncomputes the transducer loss and gradients sample by sample. We present\noptimizations to increase the efficiency and parallelism of the sample-wise\nmethod. In a set of thorough benchmarks, we show that our sample-wise method\nsignificantly reduces memory usage, and performs at competitive speed when\ncompared to the default batched computation. As a highlight, we manage to\ncompute the transducer loss and gradients for a batch size of 1024, and audio\nlength of 40 seconds, using only 6 GB of memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Braun_S/0/1/0/all/0/1\">Stefan Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDermott_E/0/1/0/all/0/1\">Erik McDermott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_R/0/1/0/all/0/1\">Roger Hsiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Unsupervised Text Classification: Zero-shot and Similarity-based Approaches. (arXiv:2211.16285v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16285","description":"<p>Text classification of unseen classes is a challenging Natural Language\nProcessing task and is mainly attempted using two different types of\napproaches. Similarity-based approaches attempt to classify instances based on\nsimilarities between text document representations and class description\nrepresentations. Zero-shot text classification approaches aim to generalize\nknowledge gained from a training task by assigning appropriate labels of\nunknown classes to text documents. Although existing studies have already\ninvestigated individual approaches to these categories, the experiments in\nliterature do not provide a consistent comparison. This paper addresses this\ngap by conducting a systematic evaluation of different similarity-based and\nzero-shot approaches for text classification of unseen classes. Different\nstate-of-the-art approaches are benchmarked on four text classification\ndatasets, including a new dataset from the medical domain. Additionally, novel\nSimCSE and SBERT-based baselines are proposed, as other baselines used in\nexisting work yield weak classification results and are easily outperformed.\nFinally, the novel similarity-based Lbl2TransformerVec approach is presented,\nwhich outperforms previous state-of-the-art approaches in unsupervised text\nclassification. Our experiments show that similarity-based approaches\nsignificantly outperform zero-shot approaches in most cases. Additionally,\nusing SimCSE or SBERT embeddings instead of simpler text representations\nincreases similarity-based classification results even further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1\">Tim Schopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_D/0/1/0/all/0/1\">Daniel Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable speech synthesis by learning discrete phoneme-level prosodic representations. (arXiv:2211.16307v1 [cs.SD])","link":"http://arxiv.org/abs/2211.16307","description":"<p>In this paper, we present a novel method for phoneme-level prosody control of\nF0 and duration using intuitive discrete labels. We propose an unsupervised\nprosodic clustering process which is used to discretize phoneme-level F0 and\nduration features from a multispeaker speech dataset. These features are fed as\nan input sequence of prosodic labels to a prosody encoder module which augments\nan autoregressive attention-based text-to-speech model. We utilize various\nmethods in order to improve prosodic control range and coverage, such as\naugmentation, F0 normalization, balanced clustering for duration and\nspeaker-independent clustering. The final model enables fine-grained\nphoneme-level prosody control for all speakers contained in the training set,\nwhile maintaining the speaker identity. Instead of relying on reference\nutterances for inference, we introduce a prior prosody encoder which learns the\nstyle of each speaker and enables speech synthesis without the requirement of\nreference audio. We also fine-tune the multispeaker model to unseen speakers\nwith limited amounts of data, as a realistic application scenario and show that\nthe prosody control capabilities are maintained, verifying that the\nspeaker-independent prosodic clustering is effective. Experimental results show\nthat the model has high output speech quality and that the proposed method\nallows efficient prosody control within each speaker's range despite the\nvariability that a multispeaker setting introduces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ellinas_N/0/1/0/all/0/1\">Nikolaos Ellinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christidou_M/0/1/0/all/0/1\">Myrsini Christidou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vioni_A/0/1/0/all/0/1\">Alexandra Vioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_J/0/1/0/all/0/1\">June Sig Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalamandaris_A/0/1/0/all/0/1\">Aimilios Chalamandaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiakoulis_P/0/1/0/all/0/1\">Pirros Tsiakoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mastorocostas_P/0/1/0/all/0/1\">Paris Mastorocostas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Evaluation Metrics for Code-Switching Automatic Speech Recognition. (arXiv:2211.16319v1 [eess.AS])","link":"http://arxiv.org/abs/2211.16319","description":"<p>Code-switching poses a number of challenges and opportunities for\nmultilingual automatic speech recognition. In this paper, we focus on the\nquestion of robust and fair evaluation metrics. To that end, we develop a\nreference benchmark data set of code-switching speech recognition hypotheses\nwith human judgments. We define clear guidelines for minimal editing of\nautomatic hypotheses. We validate the guidelines using 4-way inter-annotator\nagreement. We evaluate a large number of metrics in terms of correlation with\nhuman judgments. The metrics we consider vary in terms of representation\n(orthographic, phonological, semantic), directness (intrinsic vs extrinsic),\ngranularity (e.g. word, character), and similarity computation method. The\nhighest correlation to human judgment is achieved using transliteration\nfollowed by text normalization. We release the first corpus for human\nacceptance of code-switching speech recognition results in dialectal\nArabic/English conversation speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hamed_I/0/1/0/all/0/1\">Injy Hamed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chellah_O/0/1/0/all/0/1\">Oumnia Chellah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chaining Simultaneous Thoughts for Numerical Reasoning. (arXiv:2211.16482v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16482","description":"<p>Given that rich information is hidden behind ubiquitous numbers in text,\nnumerical reasoning over text should be an essential skill of AI systems. To\nderive precise equations to solve numerical reasoning problems, previous work\nfocused on modeling the structures of equations, and has proposed various\nstructured decoders. Though structure modeling proves to be effective, these\nstructured decoders construct a single equation in a pre-defined autoregressive\norder, potentially placing an unnecessary restriction on how a model should\ngrasp the reasoning process. Intuitively, humans may have numerous pieces of\nthoughts popping up in no pre-defined order; thoughts are not limited to the\nproblem at hand, and can even be concerned with other related problems. By\ncomparing diverse thoughts and chaining relevant pieces, humans are less prone\nto errors. In this paper, we take this inspiration and propose CANTOR, a\nnumerical reasoner that models reasoning steps using a directed acyclic graph\nwhere we produce diverse reasoning steps simultaneously without pre-defined\ndecoding dependencies, and compare and chain relevant ones to reach a solution.\nExtensive experiments demonstrated the effectiveness of CANTOR under both\nfully-supervised and weakly-supervised settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhihong Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coder Reviewer Reranking for Code Generation. (arXiv:2211.16490v1 [cs.LG])","link":"http://arxiv.org/abs/2211.16490","description":"<p>Sampling diverse programs from a code language model and reranking with model\nlikelihood is a popular method for code generation but it is prone to\npreferring degenerate solutions. Inspired by collaborative programming, we\npropose Coder-Reviewer reranking. We augment Coder language models from past\nwork, which generate programs given language instructions, with Reviewer\nmodels, which evaluate the likelihood of the instruction given the generated\nprograms. We perform an extensive study across six datasets with eight models\nfrom three model families. Experimental results show that Coder-Reviewer\nreranking leads to consistent and significant improvement (up to 17% absolute\naccuracy gain) over reranking with the Coder model only. When combined with\nexecutability filtering, Coder-Reviewer reranking can often outperform the\nminimum Bayes risk method. Coder-Reviewer reranking is easy to implement by\nprompting, can generalize to different programming languages, and works well\nwith off-the-shelf hyperparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori B. Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida I. Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abstract Visual Reasoning with Tangram Shapes. (arXiv:2211.16492v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16492","description":"<p>We introduce KiloGram, a resource for studying abstract visual reasoning in\nhumans and machines. Drawing on the history of tangram puzzles as stimuli in\ncognitive science, we build a richly annotated dataset that, with &gt;1k distinct\nstimuli, is orders of magnitude larger and more diverse than prior resources.\nIt is both visually and linguistically richer, moving beyond whole shape\ndescriptions to include segmentation maps and part labels. We use this resource\nto evaluate the abstract visual reasoning capacities of recent multi-modal\nmodels. We observe that pre-trained weights demonstrate limited abstract\nreasoning, which dramatically improves with fine-tuning. We also observe that\nexplicitly describing parts aids abstract reasoning for both humans and models,\nespecially when jointly encoding the linguistic and visual inputs. KiloGram is\navailable at https://lil.nlp.cornell.edu/kilogram .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_A/0/1/0/all/0/1\">Anya Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kojima_N/0/1/0/all/0/1\">Noriyuki Kojima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_N/0/1/0/all/0/1\">Noah Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1\">Alane Suhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vong_W/0/1/0/all/0/1\">Wai Keen Vong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert D. Hawkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TyDiP: A Dataset for Politeness Classification in Nine Typologically Diverse Languages. (arXiv:2211.16496v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16496","description":"<p>We study politeness phenomena in nine typologically diverse languages.\nPoliteness is an important facet of communication and is sometimes argued to be\ncultural-specific, yet existing computational linguistic study is limited to\nEnglish. We create TyDiP, a dataset containing three-way politeness annotations\nfor 500 examples in each language, totaling 4.5K examples. We evaluate how well\nmultilingual models can identify politeness levels -- they show a fairly robust\nzero-shot transfer ability, yet fall short of estimated human accuracy\nsignificantly. We further study mapping the English politeness strategy lexicon\ninto nine languages via automatic translation and lexicon induction, analyzing\nwhether each strategy's impact stays consistent across languages. Lastly, we\nempirically study the complicated relationship between formality and politeness\nthrough transfer experiments. We hope our dataset will support various research\nquestions and applications, from evaluating multilingual models to constructing\npolite multilingual agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1\">Anirudh Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.03842","description":"<p>Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER) than original ASR outputs. Previous works usually use a\nsequence-to-sequence model to correct an ASR output sentence autoregressively,\nwhich causes large latency and cannot be deployed in online ASR services. A\nstraightforward solution to reduce latency, inspired by non-autoregressive\n(NAR) neural machine translation, is to use an NAR sequence generation model\nfor ASR error correction, which, however, comes at the cost of significantly\nincreased ASR error rate. In this paper, observing distinctive error patterns\nand correction operations (i.e., insertion, deletion, and substitution) in ASR,\nwe propose FastCorrect, a novel NAR error correction model based on edit\nalignment. In training, FastCorrect aligns each source token from an ASR output\nsentence to the target tokens from the corresponding ground-truth sentence\nbased on the edit distance between the source and target sentences, and\nextracts the number of target tokens corresponding to each source token during\nedition/correction, which is then used to train a length predictor and to\nadjust the source tokens to match the length of the target sentence for\nparallel generation. In inference, the token number predicted by the length\npredictor is used to adjust the source tokens for target sequence generation.\nExperiments on the public AISHELL-1 dataset and an internal industrial-scale\nASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)\nit speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER\nreduction) compared with the autoregressive correction model; and 2) it\noutperforms the popular NAR models adopted in neural machine translation and\ntext edition by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Ed Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Program Merge Conflict Resolution via Neural Transformers. (arXiv:2109.00084v4 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2109.00084","description":"<p>Collaborative software development is an integral part of the modern software\ndevelopment life cycle, essential to the success of large-scale software\nprojects. When multiple developers make concurrent changes around the same\nlines of code, a merge conflict may occur. Such conflicts stall pull requests\nand continuous integration pipelines for hours to several days, seriously\nhurting developer productivity. To address this problem, we introduce\nMergeBERT, a novel neural program merge framework based on token-level\nthree-way differencing and a transformer encoder model. By exploiting the\nrestricted nature of merge conflict resolutions, we reformulate the task of\ngenerating the resolution sequence as a classification task over a set of\nprimitive merge patterns extracted from real-world merge commit data. Our model\nachieves 63-68% accuracy for merge resolution synthesis, yielding nearly a 3x\nperformance improvement over existing semi-structured, and 2x improvement over\nneural program merge tools. Finally, we demonstrate that MergeBERT is\nsufficiently flexible to work with source code files in Java, JavaScript,\nTypeScript, and C# programming languages. To measure the practical use of\nMergeBERT, we conduct a user study to evaluate MergeBERT suggestions with 25\ndevelopers from large OSS projects on 122 real-world conflicts they\nencountered. Results suggest that in practice, MergeBERT resolutions would be\naccepted at a higher rate than estimated by automatic metrics for precision and\naccuracy. Additionally, we use participant feedback to identify future avenues\nfor improvement of MergeBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1\">Alexey Svyatkovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhoury_S/0/1/0/all/0/1\">Sarah Fakhoury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_N/0/1/0/all/0/1\">Negar Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mytkowicz_T/0/1/0/all/0/1\">Todd Mytkowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinella_E/0/1/0/all/0/1\">Elizabeth Dinella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bird_C/0/1/0/all/0/1\">Christian Bird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Jinu Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_S/0/1/0/all/0/1\">Shuvendu Lahiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition. (arXiv:2109.14420v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14420","description":"<p>Error correction is widely used in automatic speech recognition (ASR) to\npost-process the generated sentence, and can further reduce the word error rate\n(WER). Although multiple candidates are generated by an ASR system through beam\nsearch, current error correction approaches can only correct one sentence at a\ntime, failing to leverage the voting effect from multiple candidates to better\ndetect and correct error tokens. In this work, we propose FastCorrect 2, an\nerror correction model that takes multiple ASR candidates as input for better\ncorrection accuracy. FastCorrect 2 adopts non-autoregressive generation for\nfast inference, which consists of an encoder that processes multiple source\nsentences and a decoder that generates the target sentence in parallel from the\nadjusted source sentence, where the adjustment is based on the predicted\nduration of each source token. However, there are some issues when handling\nmultiple source sentences. First, it is non-trivial to leverage the voting\neffect from multiple source sentences since they usually vary in length. Thus,\nwe propose a novel alignment algorithm to maximize the degree of token\nalignment among multiple sentences in terms of token and pronunciation\nsimilarity. Second, the decoder can only take one adjusted source sentence as\ninput, while there are multiple source sentences. Thus, we develop a candidate\npredictor to detect the most suitable candidate for the decoder. Experiments on\nour inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce\nthe WER over the previous correction model with single candidate by 3.2% and\n2.6%, demonstrating the effectiveness of leveraging multiple candidates in ASR\nerror correction. FastCorrect 2 achieves better performance than the cascaded\nre-scoring and correction pipeline and can serve as a unified post-processing\nmodule for ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Edward Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symbolic Knowledge Distillation: from General Language Models to Commonsense Models. (arXiv:2110.07178v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07178","description":"<p>The common practice for training commonsense models has gone\nfrom-human-to-corpus-to-machine: humans author commonsense knowledge graphs in\norder to train commonsense models. In this work, we investigate an alternative,\nfrom-machine-to-corpus-to-machine: general language models author these\ncommonsense knowledge graphs to train commonsense models. Our study leads to a\nnew framework, Symbolic Knowledge Distillation. As with prior art in Knowledge\nDistillation (Hinton et al., 2015), our approach uses larger models to teach\nsmaller models. A key difference is that we distill knowledge symbolically-as\ntext-in addition to the neural model. We also distill only one aspect-the\ncommonsense of a general language model teacher, allowing the student to be a\ndifferent type, a commonsense model. Altogether, we show that careful prompt\nengineering and a separately trained critic model allow us to selectively\ndistill high-quality causal commonsense from GPT-3, a general language model.\nEmpirical results demonstrate that, for the first time, a human-authored\ncommonsense knowledge graph is surpassed by our automatically distilled variant\nin all three criteria: quantity, quality, and diversity. In addition, it\nresults in a neural commonsense model that surpasses the teacher model's\ncommonsense capabilities despite its 100x smaller size. We apply this to the\nATOMIC resource, and share our new symbolic knowledge graph and commonsense\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Model Compression and Acceleration for Pretrained Language Models. (arXiv:2202.07105v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07105","description":"<p>Despite achieving state-of-the-art performance on many NLP tasks, the high\nenergy cost and long inference delay prevent Transformer-based pretrained\nlanguage models (PLMs) from seeing broader adoption including for edge and\nmobile computing. Efficient NLP research aims to comprehensively consider\ncomputation, time and carbon emission for the entire life-cycle of NLP,\nincluding data preparation, model training and inference. In this survey, we\nfocus on the inference stage and review the current state of model compression\nand acceleration for pretrained language models, including benchmarks, metrics\nand methodology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining On Alzheimer's Diseases Related Knowledge Graph to Identity Potential AD-related Semantic Triples for Drug Repurposing. (arXiv:2202.08712v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.08712","description":"<p>To date, there are no effective treatments for most neurodegenerative\ndiseases. Knowledge graphs can provide comprehensive and semantic\nrepresentation for heterogeneous data, and have been successfully leveraged in\nmany biomedical applications including drug repurposing. Our objective is to\nconstruct a knowledge graph from literature to study relations between\nAlzheimer's disease (AD) and chemicals, drugs and dietary supplements in order\nto identify opportunities to prevent or delay neurodegenerative progression. We\ncollected biomedical annotations and extracted their relations using SemRep via\nSemMedDB. We used both a BERT-based classifier and rule-based methods during\ndata preprocessing to exclude noise while preserving most AD-related semantic\ntriples. The 1,672,110 filtered triples were used to train with knowledge graph\ncompletion algorithms (i.e., TransE, DistMult, and ComplEx) to predict\ncandidates that might be helpful for AD treatment or prevention. Among three\nknowledge graph completion models, TransE outperformed the other two (MR =\n13.45, Hits@1 = 0.306). We leveraged the time-slicing technique to further\nevaluate the prediction results. We found supporting evidence for most highly\nranked candidates predicted by our model which indicates that our approach can\ninform reliable new knowledge. This paper shows that our graph mining model can\npredict reliable new relationships between AD and other entities (i.e., dietary\nsupplements, chemicals, and drugs). The knowledge graph constructed can\nfacilitate data-driven knowledge discoveries and the generation of novel\nhypotheses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nian_Y/0/1/0/all/0/1\">Yi Nian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinyue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jingna Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingcheng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Cui Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models. (arXiv:2205.12247v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12247","description":"<p>Recent work has shown that Pre-trained Language Models (PLMs) store the\nrelational knowledge learned from data and utilize it for performing downstream\ntasks. However, commonsense knowledge across different regions may vary. For\ninstance, the color of bridal dress is white in American weddings whereas it is\nred in Chinese weddings. In this paper, we introduce a benchmark dataset,\nGeo-Diverse Commonsense Multilingual Language Models Analysis (GeoMLAMA), for\nprobing the diversity of the relational knowledge in multilingual PLMs.\nGeoMLAMA contains 3,125 prompts in English, Chinese, Hindi, Persian, and\nSwahili, with a wide coverage of concepts shared by people from American,\nChinese, Indian, Iranian and Kenyan cultures. We benchmark 11 standard\nmultilingual PLMs on GeoMLAMA. Interestingly, we find that 1) larger\nmultilingual PLMs variants do not necessarily store geo-diverse concepts better\nthan its smaller variant; 2) multilingual PLMs are not intrinsically biased\ntowards knowledge from the Western countries (the United States); 3) the native\nlanguage of a country may not be the best language to probe its knowledge and\n4) a language may better probe knowledge about a non-native country than its\nnative country. Code and data are released at\nhttps://github.com/WadeYin9712/GeoMLAMA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Da Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1\">Hritik Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monajatipoor_M/0/1/0/all/0/1\">Masoud Monajatipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Language Models with Memory Augmentation. (arXiv:2205.12674v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12674","description":"<p>Recent work has improved language models (LMs) remarkably by equipping them\nwith a non-parametric memory component. However, most existing approaches only\nintroduce mem-ories at testing time or represent them using a separately\ntrained encoder, resulting in suboptimal training of the language model. In\nthis work, we present TRIME, a novel yet simple training approach designed for\ntraining LMs with memory augmentation. Our approach uses a training objective\nthat directly takes in-batch examples as accessible memory. We also present new\nmethods for memory construction and data batching, which are used for adapting\nto different sets of memories--local, long-term, and external memory--at\ntesting time. We evaluate TRIME on multiple language modeling and machine\ntranslation benchmarks and show that it is able to achieve significant\nimprovements across all the settings. Concretely, TRIME reduces the perplexity\nfrom 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory\nset from the training corpus. Compared to standard LM training, TRIME adds\nnegligible computational overhead and is compatible with different neural\narchitectures, making it a versatile solution for training memory-augmented\nLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset. (arXiv:2207.00220v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.00220","description":"<p>One concern with the rise of large language models lies with their potential\nfor significant harm, particularly from pretraining on biased, obscene,\ncopyrighted, and private information. Emerging ethical approaches have\nattempted to filter pretraining material, but such approaches have been ad hoc\nand failed to take context into account. We offer an approach to filtering\ngrounded in law, which has directly addressed the tradeoffs in filtering\nmaterial. First, we gather and make available the Pile of Law, a 256GB (and\ngrowing) dataset of open-source English-language legal and administrative data,\ncovering court opinions, contracts, administrative rules, and legislative\nrecords. Pretraining on the Pile of Law may help with legal tasks that have the\npromise to improve access to justice. Second, we distill the legal norms that\ngovernments have developed to constrain the inclusion of toxic or private\ncontent into actionable lessons for researchers and discuss how our dataset\nreflects these norms. Third, we show how the Pile of Law offers researchers the\nopportunity to learn such filtering rules directly from the data, providing an\nexciting new research direction in model-based processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1\">Peter Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krass_M/0/1/0/all/0/1\">Mark S. Krass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lucia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_N/0/1/0/all/0/1\">Neel Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel E. Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QUAK: A Synthetic Quality Estimation Dataset for Korean-English Neural Machine Translation. (arXiv:2209.15285v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.15285","description":"<p>With the recent advance in neural machine translation demonstrating its\nimportance, research on quality estimation (QE) has been steadily progressing.\nQE aims to automatically predict the quality of machine translation (MT) output\nwithout reference sentences. Despite its high utility in the real world, there\nremain several limitations concerning manual QE data creation: inevitably\nincurred non-trivial costs due to the need for translation experts, and issues\nwith data scaling and language expansion. To tackle these limitations, we\npresent QUAK, a Korean-English synthetic QE dataset generated in a fully\nautomatic manner. This consists of three sub-QUAK datasets QUAK-M, QUAK-P, and\nQUAK-H, produced through three strategies that are relatively free from\nlanguage constraints. Since each strategy requires no human effort, which\nfacilitates scalability, we scale our data up to 1.58M for QUAK-P, H and 6.58M\nfor QUAK-M. As an experiment, we quantitatively analyze word-level QE results\nin various ways while performing statistical analysis. Moreover, we show that\ndatasets scaled in an efficient way also contribute to performance improvements\nby observing meaningful performance gains in QUAK-M, P when adding data up to\n1.58M.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eo_S/0/1/0/all/0/1\">Sugyeong Eo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1\">Hyeonseok Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jaehyung Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeongmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungseob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Analogical Reasoning over Knowledge Graphs. (arXiv:2210.00312v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.00312","description":"<p>Analogical reasoning is fundamental to human cognition and holds an important\nplace in various fields. However, previous studies mainly focus on single-modal\nanalogical reasoning and ignore taking advantage of structure knowledge.\nNotably, the research in cognitive psychology has demonstrated that information\nfrom multimodal sources always brings more powerful cognitive transfer than\nsingle modality sources. To this end, we introduce the new task of multimodal\nanalogical reasoning over knowledge graphs, which requires multimodal reasoning\nability with the help of background knowledge. Specifically, we construct a\nMultimodal Analogical Reasoning dataSet (MARS) and a multimodal knowledge graph\nMarKG. We evaluate with multimodal knowledge graph embedding and pre-trained\nTransformer baselines, illustrating the potential challenges of the proposed\ntask. We further propose a novel model-agnostic Multimodal analogical reasoning\nframework with Transformer (MarT) motivated by the structure mapping theory,\nwhich can obtain better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Voice Detection and Audio Splicing Detection using SE-Res2Net-Conformer Architecture. (arXiv:2210.03581v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2210.03581","description":"<p>Synthetic voice and splicing audio clips have been generated to spoof\nInternet users and artificial intelligence (AI) technologies such as voice\nauthentication. Existing research work treats spoofing countermeasures as a\nbinary classification problem: bonafide vs. spoof. This paper extends the\nexisting Res2Net by involving the recent Conformer block to further exploit the\nlocal patterns on acoustic features. Experimental results on ASVspoof 2019\ndatabase show that the proposed SE-Res2Net-Conformer architecture is able to\nimprove the spoofing countermeasures performance for the logical access\nscenario.\n</p>\n<p>In addition, this paper also proposes to re-formulate the existing audio\nsplicing detection problem. Instead of identifying the complete splicing\nsegments, it is more useful to detect the boundaries of the spliced segments.\nMoreover, a deep learning approach can be used to solve the problem, which is\ndifferent from the previous signal processing techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeoh_B/0/1/0/all/0/1\">Benedict Yeoh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ng_J/0/1/0/all/0/1\">Jun Wah Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Close Look into the Calibration of Pre-trained Language Models. (arXiv:2211.00151v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.00151","description":"<p>Pre-trained language models (PLMs) achieve remarkable performance on many\ndownstream tasks, but may fail in giving reliable estimates of their predictive\nuncertainty. Given the lack of a comprehensive understanding of PLMs\ncalibration, we take a close look into this new research problem, aiming to\nanswer two questions: (1) Do PLMs learn to become calibrated in the training\nprocess? (2) How effective are existing calibration methods? For the first\nquestion, we conduct fine-grained control experiments to study the dynamic\nchange in PLMs' calibration performance in training. We consider six factors as\ncontrol variables, including dataset difficulty, available training samples,\ntraining steps, the number of tunable parameters, model scale, and pretraining.\nIn experiments, we observe a consistent change in calibration performance\nacross six factors. We find that PLMs don't learn to become calibrated in\ntraining, evidenced by the continual increase in confidence, no matter the\npredictions are correct or not. We highlight that our finding presents some\ncontradiction with two established conclusions: (a) Larger PLMs are more\ncalibrated; (b) Pretraining improves model calibration. Next, we study the\neffectiveness of existing calibration methods in mitigating the overconfidence\nissue, in both in-distribution and various out-of-distribution settings.\nBesides unlearnable calibration methods, we adapt two recently proposed\nlearnable methods that directly collect data to train models to have reasonable\nconfidence estimations. Also, we propose extended learnable methods based on\nexisting ones to further improve or maintain PLMs calibration without\nsacrificing the original task performance. Experimental results show that\nlearnable methods significantly reduce PLMs' confidence in wrong predictions,\nand our methods exhibit superior performance compared with previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lifan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales. (arXiv:2211.01562v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01562","description":"<p>Neural language models (LMs) have achieved impressive results on various\nlanguage-based reasoning tasks by utilizing latent knowledge encoded in their\nown pretrained parameters. To make this reasoning process more explicit, recent\nworks retrieve a rationalizing LM's internal knowledge by training or prompting\nit to generate free-text rationales, which can be used to guide task\npredictions made by either the same LM or a separate reasoning LM. However,\nrationalizing LMs require expensive rationale annotation and/or computation,\nwithout any assurance that their generated rationales improve LM task\nperformance or faithfully reflect LM decision-making. In this paper, we propose\nPINTO, an LM pipeline that rationalizes via prompt-based learning, and learns\nto faithfully reason over rationales via counterfactual regularization. First,\nPINTO maps out a suitable reasoning process for the task input by prompting a\nfrozen rationalizing LM to generate a free-text rationale. Second, PINTO's\nreasoning LM is fine-tuned to solve the task using the generated rationale as\ncontext, while regularized to output less confident predictions when the\nrationale is perturbed. Across four datasets, we show that PINTO significantly\nimproves the generalization ability of the reasoning LM, yielding higher\nperformance on both in-distribution and out-of-distribution test sets. Also, we\nfind that PINTO's rationales are more faithful to its task predictions than\nthose generated by competitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. (arXiv:2211.12588v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.12588","description":"<p>Recently, there has been significant progress in teaching language models to\nperform step-by-step reasoning to solve complex numerical reasoning tasks.\nChain-of-thoughts prompting (CoT) is by far the state-of-art method for these\ntasks. CoT uses language models to perform both reasoning and computation in\nthe multi-step `thought' process. To disentangle computation from reasoning, we\npropose `Program of Thoughts' (PoT), which uses language models (mainly Codex)\nto express the reasoning process as a program. The computation is relegated to\nan external computer, which executes the generated programs to derive the\nanswer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP,\nTabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA)\nfor both few-shot and zero-shot setups. Under both few-shot and zero-shot\nsettings, PoT can show an average performance gain over CoT by around 12\\%\nacross all the evaluated datasets. By combining PoT with self-consistency\ndecoding, we can achieve SoTA performance on all math problem datasets and\nnear-SoTA performance on financial datasets. All of our data and code are\nreleased in\nGithub\\footnote{\\url{https://github.com/wenhuchen/Program-of-Thoughts}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xueguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-label Few-shot ICD Coding as Autoregressive Generation with Prompt. (arXiv:2211.13813v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.13813","description":"<p>Automatic International Classification of Diseases (ICD) coding aims to\nassign multiple ICD codes to a medical note with an average of 3,000+ tokens.\nThis task is challenging due to the high-dimensional space of multi-label\nassignment (155,000+ ICD code candidates) and the long-tail challenge - Many\nICD codes are infrequently assigned yet infrequent ICD codes are important\nclinically. This study addresses the long-tail challenge by transforming this\nmulti-label classification task into an autoregressive generation task.\nSpecifically, we first introduce a novel pretraining objective to generate free\ntext diagnoses and procedure using the SOAP structure, the medical logic\nphysicians use for note documentation. Second, instead of directly predicting\nthe high dimensional space of ICD codes, our model generates the lower\ndimension of text descriptions, which then infer ICD codes. Third, we designed\na novel prompt template for multi-label classification. We evaluate our\nGeneration with Prompt model with the benchmark of all code assignment\n(MIMIC-III-full) and few shot ICD code assignment evaluation benchmark\n(MIMIC-III-few). Experiments on MIMIC-III-few show that our model performs with\na marco F1 30.2, which substantially outperforms the previous MIMIC-III-full\nSOTA model (marco F1 4.3) and the model specifically designed for few/zero shot\nsetting (marco F1 18.7). Finally, we design a novel ensemble learner, a cross\nattention reranker with prompts, to integrate previous SOTA and our best\nfew-shot coding predictions. Experiments on MIMIC-III-full show that our\nensemble learner substantially improves both macro and micro F1, from 10.4 to\n14.6 and from 58.2 to 59.1, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1\">Sunjae Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zonghai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-3-driven pedagogical agents for training children's curious question-asking skills. (arXiv:2211.14228v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.14228","description":"<p>Students' ability to ask curious questions is a crucial skill that improves\ntheir learning processes. To train this skill, previous research has used a\nconversational agent that propose specific cues to prompt children's curiosity\nduring learning. Despite showing pedagogical efficiency, this method is still\nlimited since it relies on generating the said prompts by hand for each\neducational resource, which can be a very long and costly process. In this\ncontext, we leverage the advances in the natural language processing field and\nexplore using a large language model (GPT-3) to automate the generation of this\nagent's curiosity-prompting cues to help children ask more and deeper\nquestions. We then used this study to investigate a different\ncuriosity-prompting behavior for the agent. The study was conducted with 75\nstudents aged between 9 and 10. They either interacted with a hand-crafted\nconversational agent that proposes \"closed\" manually-extracted cues leading to\npredefined questions, a GPT-3-driven one that proposes the same type of cues,\nor a GPT-3-driven one that proposes \"open\" cues that can lead to several\npossible questions. Results showed a similar question-asking performance\nbetween children who had the two \"closed\" agents, but a significantly better\none for participants with the \"open\" agent. Our first results suggest the\nvalidity of using GPT-3 to facilitate the implementation of\ncuriosity-stimulating learning technologies. In a second step, we also show\nthat GPT-3 can be efficient in proposing the relevant open cues that leave\nchildren with more autonomy to express their curiosity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelghani_R/0/1/0/all/0/1\">Rania Abdelghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yen-Hsiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauzeon_H/0/1/0/all/0/1\">H&#xe9;l&#xe8;ne Sauz&#xe9;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SKDBERT: Compressing BERT via Stochastic Knowledge Distillation. (arXiv:2211.14466v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.14466","description":"<p>In this paper, we propose Stochastic Knowledge Distillation (SKD) to obtain\ncompact BERT-style language model dubbed SKDBERT. In each iteration, SKD\nsamples a teacher model from a pre-defined teacher ensemble, which consists of\nmultiple teacher models with multi-level capacities, to transfer knowledge into\nstudent model in an one-to-one manner. Sampling distribution plays an important\nrole in SKD. We heuristically present three types of sampling distributions to\nassign appropriate probabilities for multi-level teacher models. SKD has two\nadvantages: 1) it can preserve the diversities of multi-level teacher models\nvia stochastically sampling single teacher model in each iteration, and 2) it\ncan also improve the efficacy of knowledge distillation via multi-level teacher\nmodels when large capacity gap exists between the teacher model and the student\nmodel. Experimental results on GLUE benchmark show that SKDBERT reduces the\nsize of a BERT$_{\\rm BASE}$ model by 40% while retaining 99.5% performances of\nlanguage understanding and being 100% faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zixiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Guoqing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AWTE-BERT:Attending to Wordpiece Tokenization Explicitly on BERT for Joint Intent Classification and SlotFilling. (arXiv:2211.14829v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.14829","description":"<p>Intent classification and slot filling are two core tasks in natural language\nunderstanding (NLU). The interaction nature of the two tasks makes the joint\nmodels often outperform the single designs. One of the promising solutions,\ncalled BERT (Bidirectional Encoder Representations from Transformers), achieves\nthe joint optimization of the two tasks. BERT adopts the wordpiece to tokenize\neach input token into multiple sub-tokens, which causes a mismatch between the\ntokens and the labels lengths. Previous methods utilize the hidden states\ncorresponding to the first sub-token as input to the classifier, which limits\nperformance improvement since some hidden semantic informations is discarded in\nthe fine-tune process. To address this issue, we propose a novel joint model\nbased on BERT, which explicitly models the multiple sub-tokens features after\nwordpiece tokenization, thereby generating the context features that contribute\nto slot filling. Specifically, we encode the hidden states corresponding to\nmultiple sub-tokens into a context vector via the attention mechanism. Then, we\nfeed each context vector into the slot filling encoder, which preserves the\nintegrity of the sentence. Experimental results demonstrate that our proposed\nmodel achieves significant improvement on intent classification accuracy, slot\nfilling F1, and sentence-level semantic frame accuracy on two public benchmark\ndatasets. The F1 score of the slot filling in particular has been improved from\n96.1 to 98.2 (2.1% absolute) on the ATIS dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhilong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leilei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STAGE: Span Tagging and Greedy Inference Scheme for Aspect Sentiment Triplet Extraction. (arXiv:2211.15003v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15003","description":"<p>Aspect Sentiment Triplet Extraction (ASTE) has become an emerging task in\nsentiment analysis research, aiming to extract triplets of the aspect term, its\ncorresponding opinion term, and its associated sentiment polarity from a given\nsentence. Recently, many neural networks based models with different tagging\nschemes have been proposed, but almost all of them have their limitations:\nheavily relying on 1) prior assumption that each word is only associated with a\nsingle role (e.g., aspect term, or opinion term, etc. ) and 2) word-level\ninteractions and treating each opinion/aspect as a set of independent words.\nHence, they perform poorly on the complex ASTE task, such as a word associated\nwith multiple roles or an aspect/opinion term with multiple words. Hence, we\npropose a novel approach, Span TAgging and Greedy infErence (STAGE), to extract\nsentiment triplets in span-level, where each span may consist of multiple words\nand play different roles simultaneously. To this end, this paper formulates the\nASTE task as a multi-class span classification problem. Specifically, STAGE\ngenerates more accurate aspect sentiment triplet extractions via exploring\nspan-level information and constraints, which consists of two components,\nnamely, span tagging scheme and greedy inference strategy. The former tag all\npossible candidate spans based on a newly-defined tagging set. The latter\nretrieves the aspect/opinion term with the maximum length from the candidate\nsentiment snippet to output sentiment triplets. Furthermore, we propose a\nsimple but effective model based on the STAGE, which outperforms the\nstate-of-the-arts by a large margin on four widely-used datasets. Moreover, our\nSTAGE can be easily generalized to other pair/triplet extraction tasks, which\nalso demonstrates the superiority of the proposed scheme STAGE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shuo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuanyuan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Rui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dangyang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What learning algorithm is in-context learning? Investigations with linear models. (arXiv:2211.15661v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2211.15661","description":"<p>Neural sequence models, especially transformers, exhibit a remarkable\ncapacity for in-context learning. They can construct new predictors from\nsequences of labeled examples $(x, f(x))$ presented in the input without\nfurther parameter updates. We investigate the hypothesis that transformer-based\nin-context learners implement standard learning algorithms implicitly, by\nencoding smaller models in their activations, and updating these implicit\nmodels as new examples appear in the context. Using linear regression as a\nprototypical problem, we offer three sources of evidence for this hypothesis.\nFirst, we prove by construction that transformers can implement learning\nalgorithms for linear models based on gradient descent and closed-form ridge\nregression. Second, we show that trained in-context learners closely match the\npredictors computed by gradient descent, ridge regression, and exact\nleast-squares regression, transitioning between different predictors as\ntransformer depth and dataset noise vary, and converging to Bayesian estimators\nfor large widths and depths. Third, we present preliminary evidence that\nin-context learners share algorithmic features with these predictors: learners'\nlate layers non-linearly encode weight vectors and moment matrices. These\nresults suggest that in-context learning is understandable in algorithmic\nterms, and that (at least in the linear case) learners may rediscover standard\nestimation algorithms. Code and reference implementations are released at\nhttps://github.com/ekinakyurek/google-research/blob/master/incontext.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1\">Ekin Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}