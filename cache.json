{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Social Biases through the Text-to-Image Generation Lens. (arXiv:2304.06034v1 [cs.CY])","link":"http://arxiv.org/abs/2304.06034","description":"<p>Text-to-Image (T2I) generation is enabling new applications that support\ncreators, designers, and general end users of productivity software by\ngenerating illustrative content with high photorealism starting from a given\ndescriptive text as a prompt. Such models are however trained on massive\namounts of web data, which surfaces the peril of potential harmful biases that\nmay leak in the generation process itself. In this paper, we take a\nmulti-dimensional approach to studying and quantifying common social biases as\nreflected in the generated images, by focusing on how occupations, personality\ntraits, and everyday situations are depicted across representations of\n(perceived) gender, age, race, and geographical location. Through an extensive\nset of both automated and human evaluation experiments we present findings for\ntwo popular T2I models: DALLE-v2 and Stable Diffusion. Our results reveal that\nthere exist severe occupational biases of neutral prompts majorly excluding\ngroups of people from results for both models. Such biases can get mitigated by\nincreasing the amount of specification in the prompt itself, although the\nprompting mitigation will not address discrepancies in image quality or other\nusages of the model or its representations in other scenarios. Further, we\nobserve personality traits being associated with only a limited set of people\nat the intersection of race, gender, and age. Finally, an analysis of\ngeographical location representations on everyday situations (e.g., park, food,\nweddings) shows that for most situations, images generated through default\nlocation-neutral prompts are closer and more similar to images generated for\nlocations of United States and Germany.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naik_R/0/1/0/all/0/1\">Ranjita Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nushi_B/0/1/0/all/0/1\">Besmira Nushi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Fake Generated Scientific Abstracts. (arXiv:2304.06148v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06148","description":"<p>The widespread adoption of Large Language Models and publicly available\nChatGPT has marked a significant turning point in the integration of Artificial\nIntelligence into people's everyday lives. The academic community has taken\nnotice of these technological advancements and has expressed concerns regarding\nthe difficulty of discriminating between what is real and what is artificially\ngenerated. Thus, researchers have been working on developing effective systems\nto identify machine-generated text. In this study, we utilize the GPT-3 model\nto generate scientific paper abstracts through Artificial Intelligence and\nexplore various text representation methods when combined with Machine Learning\nmodels with the aim of identifying machine-written text. We analyze the models'\nperformance and address several research questions that rise during the\nanalysis of the results. By conducting this research, we shed light on the\ncapabilities and limitations of Artificial Intelligence generated text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theocharopoulos_P/0/1/0/all/0/1\">Panagiotis C. Theocharopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anagnostou_P/0/1/0/all/0/1\">Panagiotis Anagnostou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoukala_A/0/1/0/all/0/1\">Anastasia Tsoukala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgakopoulos_S/0/1/0/all/0/1\">Spiros V. Georgakopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasoulis_S/0/1/0/all/0/1\">Sotiris K. Tasoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plagianakos_V/0/1/0/all/0/1\">Vassilis P. Plagianakos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LINGO : Visually Debiasing Natural Language Instructions to Support Task Diversity. (arXiv:2304.06184v1 [cs.HC])","link":"http://arxiv.org/abs/2304.06184","description":"<p>Cross-task generalization is a significant outcome that defines mastery in\nnatural language understanding. Humans show a remarkable aptitude for this, and\ncan solve many different types of tasks, given definitions in the form of\ntextual instructions and a small set of examples. Recent work with pre-trained\nlanguage models mimics this learning style: users can define and exemplify a\ntask for the model to attempt as a series of natural language prompts or\ninstructions. While prompting approaches have led to higher cross-task\ngeneralization compared to traditional supervised learning, analyzing 'bias' in\nthe task instructions given to the model is a difficult problem, and has thus\nbeen relatively unexplored. For instance, are we truly modeling a task, or are\nwe modeling a user's instructions? To help investigate this, we develop LINGO,\na novel visual analytics interface that supports an effective, task-driven\nworkflow to (1) help identify bias in natural language task instructions, (2)\nalter (or create) task instructions to reduce bias, and (3) evaluate\npre-trained model performance on debiased task instructions. To robustly\nevaluate LINGO, we conduct a user study with both novice and expert instruction\ncreators, over a dataset of 1,616 linguistic tasks and their natural language\ninstructions, spanning 55 different languages. For both user groups, LINGO\npromotes the creation of more difficult tasks for pre-trained models, that\ncontain higher linguistic diversity and lower instruction bias. We additionally\ndiscuss how the insights learned in developing and evaluating LINGO can aid in\nthe design of future dashboards that aim to minimize the effort involved in\nprompt creation across multiple domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arunkumar_A/0/1/0/all/0/1\">Anjana Arunkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shubham Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_R/0/1/0/all/0/1\">Rakhi Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_S/0/1/0/all/0/1\">Sriram Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryan_C/0/1/0/all/0/1\">Chris Bryan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using large language models for (de-)formalization and natural argumentation exercises for beginner's students. (arXiv:2304.06186v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06186","description":"<p>We describe two systems that use text-davinci-003, a large language model,\nfor the automatized correction of (i) exercises in translating back and forth\nbetween natural language and the languages of propositional logic and\nfirst-order predicate logic and (ii) exercises in writing simple arguments in\nnatural language in non-mathematical scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carl_M/0/1/0/all/0/1\">Merlin Carl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LeafAI: query generator for clinical cohort discovery rivaling a human programmer. (arXiv:2304.06203v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06203","description":"<p>Objective: Identifying study-eligible patients within clinical databases is a\ncritical step in clinical research. However, accurate query design typically\nrequires extensive technical and biomedical expertise. We sought to create a\nsystem capable of generating data model-agnostic queries while also providing\nnovel logical reasoning capabilities for complex clinical trial eligibility\ncriteria.\n</p>\n<p>Materials and Methods: The task of query creation from eligibility criteria\nrequires solving several text-processing problems, including named entity\nrecognition and relation extraction, sequence-to-sequence transformation,\nnormalization, and reasoning. We incorporated hybrid deep learning and\nrule-based modules for these, as well as a knowledge base of the Unified\nMedical Language System (UMLS) and linked ontologies. To enable data-model\nagnostic query creation, we introduce a novel method for tagging database\nschema elements using UMLS concepts. To evaluate our system, called LeafAI, we\ncompared the capability of LeafAI to a human database programmer to identify\npatients who had been enrolled in 8 clinical trials conducted at our\ninstitution. We measured performance by the number of actual enrolled patients\nmatched by generated queries.\n</p>\n<p>Results: LeafAI matched a mean 43% of enrolled patients with 27,225 eligible\nacross 8 clinical trials, compared to 27% matched and 14,587 eligible in\nqueries by a human database programmer. The human programmer spent 26 total\nhours crafting queries compared to several minutes by LeafAI.\n</p>\n<p>Conclusions: Our work contributes a state-of-the-art data model-agnostic\nquery generation system capable of conditional reasoning using a knowledge\nbase. We demonstrate that LeafAI can rival a human programmer in finding\npatients eligible for clinical trials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dobbins_N/0/1/0/all/0/1\">Nicholas J Dobbins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_K/0/1/0/all/0/1\">Kristine Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">H. Nina Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrington_R/0/1/0/all/0/1\">Robert Harrington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzuner_O/0/1/0/all/0/1\">Ozlem Uzuner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LasUIE: Unifying Information Extraction with Latent Adaptive Structure-aware Generative Language Model. (arXiv:2304.06248v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06248","description":"<p>Universally modeling all typical information extraction tasks (UIE) with one\ngenerative language model (GLM) has revealed great potential by the latest\nstudy, where various IE predictions are unified into a linearized hierarchical\nexpression under a GLM. Syntactic structure information, a type of effective\nfeature which has been extensively utilized in IE community, should also be\nbeneficial to UIE. In this work, we propose a novel structure-aware GLM, fully\nunleashing the power of syntactic knowledge for UIE. A heterogeneous structure\ninductor is explored to unsupervisedly induce rich heterogeneous structural\nrepresentations by post-training an existing GLM. In particular, a structural\nbroadcaster is devised to compact various latent trees into explicit high-order\nforests, helping to guide a better generation during decoding. We finally\nintroduce a task-oriented structure fine-tuning mechanism, further adjusting\nthe learned structures to most coincide with the end-task's need. Over 12 IE\nbenchmarks across 7 tasks our system shows significant improvements over the\nbaseline UIE system. Further in-depth analyses show that our GLM learns rich\ntask-adaptive structural bias that greatly resolves the UIE crux, the\nlong-range dependence issue and boundary identifying. Source codes are open at\nhttps://github.com/ChocoWu/LasUIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengqiong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rule-based detection of access to education and training in Germany. (arXiv:2304.06307v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06307","description":"<p>As a result of transformation processes, the German labor market is highly\ndependent on vocational training, retraining and continuing education. To match\ntraining seekers and offers, we present a novel approach towards the automated\ndetection of access to education and training in German training offers and\nadvertisements. We will in particular focus on (a) general school and education\ndegrees and schoolleaving certificates, (b) professional experience, (c) a\nprevious apprenticeship and (d) a list of skills provided by the German Federal\nEmployment Agency. This novel approach combines several methods: First, we\nprovide a mapping of synonyms in education combining different qualifications\nand adding deprecated terms. Second, we provide a rule-based matching to\nidentify the need for professional experience or apprenticeship. However, not\nall access requirements can be matched due to incompatible data schemata or\nnon-standardizes requirements, e.g initial tests or interviews. While we can\nidentify several shortcomings, the presented approach offers promising results\nfor two data sets: training and re-training advertisements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dorpinghaus_J/0/1/0/all/0/1\">Jens D&#xf6;rpinghaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samray_D/0/1/0/all/0/1\">David Samray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helmrich_R/0/1/0/all/0/1\">Robert Helmrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computational modeling of semantic change. (arXiv:2304.06337v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06337","description":"<p>In this chapter we provide an overview of computational modeling for semantic\nchange using large and semi-large textual corpora. We aim to provide a key for\nthe interpretation of relevant methods and evaluation techniques, and also\nprovide insights into important aspects of the computational study of semantic\nchange. We discuss the pros and cons of different classes of models with\nrespect to the properties of the data from which one wishes to model semantic\nchange, and which avenues are available to evaluate the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tahmasebi_N/0/1/0/all/0/1\">Nina Tahmasebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubossarsky_H/0/1/0/all/0/1\">Haim Dubossarsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. (arXiv:2304.06364v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06364","description":"<p>Evaluating the general abilities of foundation models to tackle human-level\ntasks is a vital aspect of their development and application in the pursuit of\nArtificial General Intelligence (AGI). Traditional benchmarks, which rely on\nartificial datasets, may not accurately represent human-level capabilities. In\nthis paper, we introduce AGIEval, a novel benchmark specifically designed to\nassess foundation model in the context of human-centric standardized exams,\nsuch as college entrance exams, law school admission tests, math competitions,\nand lawyer qualification tests. We evaluate several state-of-the-art foundation\nmodels, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark.\nImpressively, GPT-4 surpasses average human performance on SAT, LSAT, and math\ncompetitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5%\naccuracy on the English test of the Chinese national college entrance exam.\nThis demonstrates the extraordinary performance of contemporary foundation\nmodels. In contrast, we also find that GPT-4 is less proficient in tasks that\nrequire complex reasoning or specific domain knowledge. Our comprehensive\nanalyses of model capabilities (understanding, knowledge, reasoning, and\ncalculation) reveal these models' strengths and limitations, providing valuable\ninsights into future directions for enhancing their general capabilities. By\nconcentrating on tasks pertinent to human cognition and decision-making, our\nbenchmark delivers a more meaningful and robust evaluation of foundation\nmodels' performance in real-world scenarios. The data, code, and all model\noutputs are released in https://github.com/microsoft/AGIEval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Ruixiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiduo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shuai Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanlin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saied_A/0/1/0/all/0/1\">Amin Saied</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sign Language Translation from Instructional Videos. (arXiv:2304.06371v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06371","description":"<p>The advances in automatic sign language translation (SLT) to spoken languages\nhave been mostly benchmarked with datasets of limited size and restricted\ndomains. Our work advances the state of the art by providing the first baseline\nresults on How2Sign, a large and broad dataset.\n</p>\n<p>We train a Transformer over I3D video features, using the reduced BLEU as a\nreference metric for validation, instead of the widely used BLEU score. We\nreport a result of 8.03 on the BLEU score, and publish the first open-source\nimplementation of its kind to promote further advances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tarres_L/0/1/0/all/0/1\">Laia Tarr&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duarte_A/0/1/0/all/0/1\">Amanda Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torres_J/0/1/0/all/0/1\">Jordi Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1\">Xavier Gir&#xf3;-i-Nieto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards hypergraph cognitive networks as feature-rich models of knowledge. (arXiv:2304.06375v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06375","description":"<p>Semantic networks provide a useful tool to understand how related concepts\nare retrieved from memory. However, most current network approaches use\npairwise links to represent memory recall patterns. Pairwise connections\nneglect higher-order associations, i.e. relationships between more than two\nconcepts at a time. These higher-order interactions might covariate with (and\nthus contain information about) how similar concepts are along psycholinguistic\ndimensions like arousal, valence, familiarity, gender and others. We overcome\nthese limits by introducing feature-rich cognitive hypergraphs as quantitative\nmodels of human memory where: (i) concepts recalled together can all engage in\nhyperlinks involving also more than two concepts at once (cognitive hypergraph\naspect), and (ii) each concept is endowed with a vector of psycholinguistic\nfeatures (feature-rich aspect). We build hypergraphs from word association data\nand use evaluation methods from machine learning features to predict concept\nconcreteness. Since concepts with similar concreteness tend to cluster together\nin human memory, we expect to be able to leverage this structure. Using word\nassociation data from the Small World of Words dataset, we compared a pairwise\nnetwork and a hypergraph with N=3586 concepts/nodes. Interpretable artificial\nintelligence models trained on (1) psycholinguistic features only, (2)\npairwise-based feature aggregations, and on (3) hypergraph-based aggregations\nshow significant differences between pairwise and hypergraph links.\nSpecifically, our results show that higher-order and feature-rich hypergraph\nmodels contain richer information than pairwise networks leading to improved\nprediction of word concreteness. The relation with previous studies about\nconceptual clustering and compartmentalisation in associative knowledge and\nhuman memory are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Citraro_S/0/1/0/all/0/1\">Salvatore Citraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deyne_S/0/1/0/all/0/1\">Simon De Deyne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stella_M/0/1/0/all/0/1\">Massimo Stella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossetti_G/0/1/0/all/0/1\">Giulio Rossetti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v1 [cs.AI])","link":"http://arxiv.org/abs/2304.06377","description":"<p>Being able to create meaningful symbols and proficiently use them for higher\ncognitive functions such as communication, reasoning, planning, etc., is\nessential and unique for human intelligence. Current deep neural networks are\nstill far behind human's ability to create symbols for such higher cognitive\nfunctions. Here we propose a solution, named SEA-net, to endow neural networks\nwith ability of symbol creation, semantic understanding and communication.\nSEA-net generates symbols that dynamically configure the network to perform\nspecific tasks. These symbols capture compositional semantic information that\nenables the system to acquire new functions purely by symbolic manipulation or\ncommunication. In addition, we found that these self-generated symbols exhibit\nan intrinsic structure resembling that of natural language, suggesting a common\nframework underlying the generation and understanding of symbols in both human\nbrains and artificial neural networks. We hope that it will be instrumental in\nproducing more capable systems in the future that can synergize the strengths\nof connectionist and symbolic approaches for AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Liangxuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpectFormer: Frequency and Attention is what you need in a Vision Transformer. (arXiv:2304.06446v1 [cs.CV])","link":"http://arxiv.org/abs/2304.06446","description":"<p>Vision transformers have been applied successfully for image recognition\ntasks. There have been either multi-headed self-attention based (ViT\n\\cite{dosovitskiy2020image}, DeIT, \\cite{touvron2021training}) similar to the\noriginal work in textual models or more recently based on spectral layers\n(Fnet\\cite{lee2021fnet}, GFNet\\cite{rao2021global},\nAFNO\\cite{guibas2021efficient}). We hypothesize that both spectral and\nmulti-headed attention plays a major role. We investigate this hypothesis\nthrough this work and observe that indeed combining spectral and multi-headed\nattention layers provides a better transformer architecture. We thus propose\nthe novel Spectformer architecture for transformers that combines spectral and\nmulti-headed attention layers. We believe that the resulting representation\nallows the transformer to capture the feature representation appropriately and\nit yields improved performance over other transformer representations. For\ninstance, it improves the top-1 accuracy by 2\\% on ImageNet compared to both\nGFNet-H and LiT. SpectFormer-S reaches 84.25\\% top-1 accuracy on ImageNet-1K\n(state of the art for small version). Further, Spectformer-L achieves 85.7\\%\nthat is the state of the art for the comparable base version of the\ntransformers. We further ensure that we obtain reasonable results in other\nscenarios such as transfer learning on standard datasets such as CIFAR-10,\nCIFAR-100, Oxford-IIIT-flower, and Standford Car datasets. We then investigate\nits use in downstream tasks such of object detection and instance segmentation\non the MS-COCO dataset and observe that Spectformer shows consistent\nperformance that is comparable to the best backbones and can be further\noptimized and improved. Hence, we believe that combined spectral and attention\nlayers are what are needed for vision transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patro_B/0/1/0/all/0/1\">Badri N. Patro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agneeswaran_V/0/1/0/all/0/1\">Vijay Srinivas Agneeswaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PDF-VQA: A New Dataset for Real-World VQA on PDF Documents. (arXiv:2304.06447v1 [cs.CV])","link":"http://arxiv.org/abs/2304.06447","description":"<p>Document-based Visual Question Answering examines the document understanding\nof document images in conditions of natural language questions. We proposed a\nnew document-based VQA dataset, PDF-VQA, to comprehensively examine the\ndocument understanding from various aspects, including document element\nrecognition, document layout structural understanding as well as contextual\nunderstanding and key information extraction. Our PDF-VQA dataset extends the\ncurrent scale of document understanding that limits on the single document page\nto the new scale that asks questions over the full document of multiple pages.\nWe also propose a new graph-based VQA model that explicitly integrates the\nspatial and hierarchically structural relationships between different document\nelements to boost the document structural understanding. The performances are\ncompared with several baselines over different question types and\ntasks\\footnote{The full dataset will be released after paper acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yihao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Siwen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyunsuk Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masakhane-Afrisenti at SemEval-2023 Task 12: Sentiment Analysis using Afro-centric Language Models and Adapters for Low-resource African Languages. (arXiv:2304.06459v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06459","description":"<p>AfriSenti-SemEval Shared Task 12 of SemEval-2023. The task aims to perform\nmonolingual sentiment classification (sub-task A) for 12 African languages,\nmultilingual sentiment classification (sub-task B), and zero-shot sentiment\nclassification (task C). For sub-task A, we conducted experiments using\nclassical machine learning classifiers, Afro-centric language models, and\nlanguage-specific models. For task B, we fine-tuned multilingual pre-trained\nlanguage models that support many of the languages in the task. For task C, we\nused we make use of a parameter-efficient Adapter approach that leverages\nmonolingual texts in the target language for effective zero-shot transfer. Our\nfindings suggest that using pre-trained Afro-centric language models improves\nperformance for low-resource African languages. We also ran experiments using\nadapters for zero-shot tasks, and the results suggest that we can obtain\npromising results by using adapters with a limited amount of resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azime_I/0/1/0/all/0/1\">Israel Abebe Azime</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Azzawi_S/0/1/0/all/0/1\">Sana Sabah Al-Azzawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonja_A/0/1/0/all/0/1\">Atnafu Lambebo Tonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shode_I/0/1/0/all/0/1\">Iyanuoluwa Shode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awokoya_A/0/1/0/all/0/1\">Ayodele Awokoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oduwole_M/0/1/0/all/0/1\">Mardiyyah Oduwole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fanijo_S/0/1/0/all/0/1\">Samuel Fanijo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awosan_O/0/1/0/all/0/1\">Oyinkansola Awosan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousuf_O/0/1/0/all/0/1\">Oreen Yousuf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era. (arXiv:2304.06488v1 [cs.CY])","link":"http://arxiv.org/abs/2304.06488","description":"<p>OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is\ndemonstrated to be one small step for generative AI (GAI), but one giant leap\nfor artificial general intelligence (AGI). Since its official release in\nNovember 2022, ChatGPT has quickly attracted numerous users with extensive\nmedia coverage. Such unprecedented attention has also motivated numerous\nresearchers to investigate ChatGPT from various aspects. According to Google\nscholar, there are more than 500 articles with ChatGPT in their titles or\nmentioning it in their abstracts. Considering this, a review is urgently\nneeded, and our work fills this gap. Overall, this work is the first to survey\nChatGPT with a comprehensive review of its underlying technology, applications,\nand challenges. Moreover, we present an outlook on how ChatGPT might evolve to\nrealize general-purpose AIGC (a.k.a. AI-generated content), which will be a\nsignificant milestone for the development of AGI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenshuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Sheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dam_S/0/1/0/all/0/1\">Sumit Kumar Dam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengchun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jung Uk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seong Tae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinwoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1\">Gyeong-Moon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Sung-Ho Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lik-Hang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_P/0/1/0/all/0/1\">Pan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Choong Seon Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are LLMs All You Need for Task-Oriented Dialogue?. (arXiv:2304.06556v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06556","description":"<p>Instructions-tuned Large Language Models (LLMs) gained recently huge\npopularity thanks to their ability to interact with users through conversation.\nIn this work we aim to evaluate their ability to complete multi-turn tasks and\ninteract with external databases in the context of established task-oriented\ndialogue benchmarks. We show that for explicit belief state tracking, LLMs\nunderperform compared to specialized task-specific models. Nevertheless, they\nshow ability to guide the dialogue to successful ending if given correct slot\nvalues. Furthermore this ability improves with access to true belief state\ndistribution or in-domain examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hudecek_V/0/1/0/all/0/1\">Vojt&#x11b;ch Hude&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning. (arXiv:2304.06588v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06588","description":"<p>This paper assesses the accuracy, reliability and bias of the Large Language\nModel (LLM) ChatGPT-4 on the text analysis task of classifying the political\naffiliation of a Twitter poster based on the content of a tweet. The LLM is\ncompared to manual annotation by both expert classifiers and crowd workers,\ngenerally considered the gold standard for such tasks. We use Twitter messages\nfrom United States politicians during the 2020 election, providing a ground\ntruth against which to measure accuracy. The paper finds that ChatGPT-4 has\nachieves higher accuracy, higher reliability, and equal or lower bias than the\nhuman classifiers. The LLM is able to correctly annotate messages that require\nreasoning on the basis of contextual knowledge, and inferences around the\nauthor's intentions - traditionally seen as uniquely human abilities. These\nfindings suggest that LLM will have substantial impact on the use of textual\ndata in the social sciences, by enabling interpretive research at a scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tornberg_P/0/1/0/all/0/1\">Petter T&#xf6;rnberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the State of the Art in Legal QA Systems. (arXiv:2304.06623v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06623","description":"<p>Answering questions related to the legal domain is a complex task, primarily\ndue to the intricate nature and diverse range of legal document systems.\nProviding an accurate answer to a legal query typically necessitates\nspecialized knowledge in the relevant domain, which makes this task all the\nmore challenging, even for human experts. QA (Question answering systems) are\ndesigned to generate answers to questions asked in human languages. They use\nnatural language processing to understand questions and search through\ninformation to find relevant answers. QA has various practical applications,\nincluding customer service, education, research, and cross-lingual\ncommunication. However, they face challenges such as improving natural language\nunderstanding and handling complex and ambiguous questions. Answering questions\nrelated to the legal domain is a complex task, primarily due to the intricate\nnature and diverse range of legal document systems. Providing an accurate\nanswer to a legal query typically necessitates specialized knowledge in the\nrelevant domain, which makes this task all the more challenging, even for human\nexperts. At this time, there is a lack of surveys that discuss legal question\nanswering. To address this problem, we provide a comprehensive survey that\nreviews 14 benchmark datasets for question-answering in the legal field as well\nas presents a comprehensive review of the state-of-the-art Legal Question\nAnswering deep learning models. We cover the different architectures and\ntechniques used in these studies and the performance and limitations of these\nmodels. Moreover, we have established a public GitHub repository where we\nregularly upload the most recent articles, open data, and source code. The\nrepository is available at:\n\\url{https://github.com/abdoelsayed2016/Legal-Question-Answering-Review}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_A/0/1/0/all/0/1\">Abdelrahman Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piryani_B/0/1/0/all/0/1\">Bhawna Piryani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PGTask: Introducing the Task of Profile Generation from Dialogues. (arXiv:2304.06634v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06634","description":"<p>Recent approaches have attempted to personalize dialogue systems by\nleveraging profile information into models. However, this knowledge is scarce\nand difficult to obtain, which makes the extraction/generation of profile\ninformation from dialogues a fundamental asset. To surpass this limitation, we\nintroduce the Profile Generation Task (PGTask). We contribute with a new\ndataset for this problem, comprising profile sentences aligned with related\nutterances, extracted from a corpus of dialogues. Furthermore, using\nstate-of-the-art methods, we provide a benchmark for profile generation on this\nnovel dataset. Our experiments disclose the challenges of profile generation,\nand we hope that this introduces a new research direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_R/0/1/0/all/0/1\">Rui Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1\">Joao P. Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coheur_L/0/1/0/all/0/1\">Lu&#xed;sa Coheur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Useful are Educational Questions Generated by Large Language Models?. (arXiv:2304.06638v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06638","description":"<p>Controllable text generation (CTG) by large language models has a huge\npotential to transform education for teachers and students alike. Specifically,\nhigh quality and diverse question generation can dramatically reduce the load\non teachers and improve the quality of their educational content. Recent work\nin this domain has made progress with generation, but fails to show that real\nteachers judge the generated questions as sufficiently useful for the classroom\nsetting; or if instead the questions have errors and/or pedagogically unhelpful\ncontent. We conduct a human evaluation with teachers to assess the quality and\nusefulness of outputs from combining CTG and question taxonomies (Bloom's and a\ndifficulty taxonomy). The results demonstrate that the questions generated are\nhigh quality and sufficiently useful, showing their promise for widespread use\nin the classroom setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elkins_S/0/1/0/all/0/1\">Sabina Elkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochmar_E/0/1/0/all/0/1\">Ekaterina Kochmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie C.K. Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serban_I/0/1/0/all/0/1\">Iulian Serban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection. (arXiv:2304.06653v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06653","description":"<p>It has been reported that clustering-based topic models, which cluster\nhigh-quality sentence embeddings with an appropriate word selection method, can\ngenerate better topics than generative probabilistic topic models. However,\nthese approaches suffer from the inability to select appropriate parameters and\nincomplete models that overlook the quantitative relation between words with\ntopics and topics with text. To solve these issues, we propose graph to topic\n(G2T), a simple but effective framework for topic modelling. The framework is\ncomposed of four modules. First, document representation is acquired using\npretrained language models. Second, a semantic graph is constructed according\nto the similarity between document representations. Third, communities in\ndocument semantic graphs are identified, and the relationship between topics\nand documents is quantified accordingly. Fourth, the word--topic distribution\nis computed based on a variant of TFIDF. Automatic evaluation suggests that G2T\nachieved state-of-the-art performance on both English and Chinese documents\nwith different lengths. Human judgements demonstrate that G2T can produce\ntopics with better interpretability and coverage than baselines. In addition,\nG2T can not only determine the topic number automatically but also give the\nprobabilistic distribution of words in topics and topics in documents. Finally,\nG2T is publicly available, and the distillation experiments provide instruction\non how it works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Leihang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiapeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiang Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v1 [cs.CV])","link":"http://arxiv.org/abs/2304.06671","description":"<p>Spatial control is a core capability in controllable image generation.\nAdvancements in layout-guided image generation have shown promising results on\nin-distribution (ID) datasets with similar spatial configurations. However, it\nis unclear how these models perform when facing out-of-distribution (OOD)\nsamples with arbitrary, unseen layouts. In this paper, we propose LayoutBench,\na diagnostic benchmark for layout-guided image generation that examines four\ncategories of spatial control skills: number, position, size, and shape. We\nbenchmark two recent representative layout-guided image generation methods and\nobserve that the good ID layout control may not generalize well to arbitrary\nlayouts in the wild (e.g., objects at the boundary). Next, we propose\nIterInpaint, a new baseline that generates foreground and background regions in\na step-by-step manner via inpainting, demonstrating stronger generalizability\nthan existing models on OOD layouts in LayoutBench. We perform quantitative and\nqualitative evaluation and fine-grained analysis on the four LayoutBench skills\nto pinpoint the weaknesses of existing models. Lastly, we show comprehensive\nablation studies on IterInpaint, including training task ratio, crop&amp;paste vs.\nrepaint, and generation order. Project website: https://layoutbench.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Verbs in Action: Improving verb understanding in video-language models. (arXiv:2304.06708v1 [cs.CV])","link":"http://arxiv.org/abs/2304.06708","description":"<p>Understanding verbs is crucial to modelling how people and objects interact\nwith each other and the environment through space and time. Recently,\nstate-of-the-art video-language models based on CLIP have been shown to have\nlimited verb understanding and to rely extensively on nouns, restricting their\nperformance in real-world video applications that require action and temporal\nunderstanding. In this work, we improve verb understanding for CLIP-based\nvideo-language models by proposing a new Verb-Focused Contrastive (VFC)\nframework. This consists of two main components: (1) leveraging pretrained\nlarge language models (LLMs) to create hard negatives for cross-modal\ncontrastive learning, together with a calibration strategy to balance the\noccurrence of concepts in positive and negative pairs; and (2) enforcing a\nfine-grained, verb phrase alignment loss. Our method achieves state-of-the-art\nresults for zero-shot performance on three downstream tasks that focus on verb\nunderstanding: video-text matching, video question-answering and video\nclassification. To the best of our knowledge, this is the first work which\nproposes a method to alleviate the verb understanding problem, and does not\nsimply highlight it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Momeni_L/0/1/0/all/0/1\">Liliane Momeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1\">Mathilde Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge. (arXiv:2202.07138v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.07138","description":"<p>Natural language processing (NLP) aims at investigating the interactions\nbetween agents and humans, processing and analyzing large amounts of natural\nlanguage data. Large-scale language models play an important role in current\nnatural language processing. However, the challenges of explainability and\ncomplexity come along with the developments of language models. One way is to\nintroduce logical relations and rules into natural language processing models,\nsuch as making use of Automated Planning. Automated planning (AI planning)\nfocuses on building symbolic domain models and synthesizing plans to transit\ninitial states to goals based on domain models. Recently, there have been\nplenty of works related to these two fields, which have the abilities to\ngenerate explicit knowledge, e.g., preconditions and effects of action models,\nand learn from tacit knowledge, e.g., neural models, respectively. Integrating\nAI planning and natural language processing effectively improves the\ncommunication between human and intelligent agents. This paper outlines the\ncommons and relations between AI planning and natural language processing,\nargues that each of them can effectively impact on the other one by five areas:\n(1) planning-based text understanding, (2) planning-based natural language\nprocessing, (3) planning-based explainability, (4) text-based human-robot\ninteraction, and (5) applications. We also explore some potential future issues\nbetween AI planning and natural language processing. To the best of our\nknowledge, this survey is the first work that addresses the deep connections\nbetween AI planning and Natural language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1\">Kebing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_H/0/1/0/all/0/1\">Hankz Hankui Zhuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PePe: Personalized Post-editing Model utilizing User-generated Post-edits. (arXiv:2209.10139v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.10139","description":"<p>Incorporating personal preference is crucial in advanced machine translation\ntasks. Despite the recent advancement of machine translation, it remains a\ndemanding task to properly reflect personal style. In this paper, we introduce\na personalized automatic post-editing framework to address this challenge,\nwhich effectively generates sentences considering distinct personal behaviors.\nTo build this framework, we first collect post-editing data that connotes the\nuser preference from a live machine translation system. Specifically,\nreal-world users enter source sentences for translation and edit the\nmachine-translated outputs according to the user's preferred style. We then\npropose a model that combines a discriminator module and user-specific\nparameters on the APE framework. Experimental results show that the proposed\nmethod outperforms other baseline models on four different metrics (i.e., BLEU,\nTER, YiSi-1, and human evaluation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tae_Y/0/1/0/all/0/1\">Yunwon Tae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Cheonbok Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models. (arXiv:2210.05619v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05619","description":"<p>While multilingual language models can improve NLP performance on\nlow-resource languages by leveraging higher-resource languages, they also\nreduce average performance on all languages (the 'curse of multilinguality').\nHere we show another problem with multilingual models: grammatical structures\nin higher-resource languages bleed into lower-resource languages, a phenomenon\nwe call grammatical structure bias. We show this bias via a novel method for\ncomparing the fluency of multilingual models to the fluency of monolingual\nSpanish and Greek models: testing their preference for two carefully-chosen\nvariable grammatical structures (optional pronoun-drop in Spanish and optional\nSubject-Verb ordering in Greek). We find that multilingual BERT is biased\ntoward the English-like setting (explicit pronouns and Subject-Verb-Object\nordering) as compared to our monolingual control language model. With our case\nstudies, we hope to bring to light the fine-grained ways in which multilingual\nmodels can be biased,and encourage more linguistically-aware fluency\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_I/0/1/0/all/0/1\">Isabel Papadimitriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_K/0/1/0/all/0/1\">Kezia Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SQA3D: Situated Question Answering in 3D Scenes. (arXiv:2210.07474v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.07474","description":"<p>We propose a new task to benchmark scene understanding of embodied agents:\nSituated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g.,\n3D scan), SQA3D requires the tested agent to first understand its situation\n(position, orientation, etc.) in the 3D scene as described by text, then reason\nabout its surrounding environment and answer a question under that situation.\nBased upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k\nunique situations, along with 20.4k descriptions and 33.4k diverse reasoning\nquestions for these situations. These questions examine a wide spectrum of\nreasoning capabilities for an intelligent agent, ranging from spatial relation\ncomprehension to commonsense understanding, navigation, and multi-hop\nreasoning. SQA3D imposes a significant challenge to current multi-modal\nespecially 3D reasoning models. We evaluate various state-of-the-art approaches\nand find that the best one only achieves an overall score of 47.20%, while\namateur human participants can reach 90.06%. We believe SQA3D could facilitate\nfuture embodied AI research with stronger situation understanding and reasoning\ncapability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_S/0/1/0/all/0/1\">Silong Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yitao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning for Language Understanding. (arXiv:2212.05961v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05961","description":"<p>Data augmentation is a widely used technique in machine learning to improve\nmodel performance. However, existing data augmentation techniques in natural\nlanguage understanding (NLU) may not fully capture the complexity of natural\nlanguage variations, and they can be challenging to apply to large datasets.\nThis paper proposes the Random Position Noise (RPN) algorithm, a novel data\naugmentation technique that operates at the word vector level. RPN modifies the\nword embeddings of the original text by introducing noise based on the existing\nvalues of selected word vectors, allowing for more fine-grained modifications\nand better capturing natural language variations. Unlike traditional data\naugmentation methods, RPN does not require gradients in the computational graph\nduring virtual sample updates, making it simpler to apply to large datasets.\nExperimental results demonstrate that RPN consistently outperforms existing\ndata augmentation techniques across various NLU tasks, including sentiment\nanalysis, natural language inference, and paraphrase detection. Moreover, RPN\nperforms well in low-resource settings and is applicable to any model featuring\na word embeddings layer. The proposed RPN algorithm is a promising approach for\nenhancing NLU performance and addressing the challenges associated with\ntraditional data augmentation techniques in large-scale NLU tasks. Our\nexperimental results demonstrated that the RPN algorithm achieved\nstate-of-the-art performance in all seven NLU tasks, thereby highlighting its\neffectiveness and potential for real-world NLU applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhengqing Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhuanzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaolong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xuecong Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Huiwen Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Multilingual Semantic Parser. (arXiv:2301.12920v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12920","description":"<p>Current multilingual semantic parsing (MSP) datasets are almost all collected\nby translating the utterances in the existing datasets from the resource-rich\nlanguage to the target language. However, manual translation is costly. To\nreduce the translation effort, this paper proposes the first active learning\nprocedure for MSP (AL-MSP). AL-MSP selects only a subset from the existing\ndatasets to be translated. We also propose a novel selection method that\nprioritizes the examples diversifying the logical form structures with more\nlexical choices, and a novel hyperparameter tuning method that needs no extra\nannotation cost. Our experiments show that AL-MSP significantly reduces\ntranslation costs with ideal selection methods. Our selection method with\nproper hyperparameters yields better parsing performance than the other\nbaselines on two multilingual datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EvoText: Enhancing Natural Language Generation Models via Self-Escalation Learning for Up-to-Date Knowledge and Improved Performance. (arXiv:2302.03896v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03896","description":"<p>In recent years, pretrained models have been widely used in various fields,\nincluding natural language understanding, computer vision, and natural language\ngeneration. However, the performance of these language generation models is\nhighly dependent on the model size and the dataset size. While larger models\nexcel in some aspects, they cannot learn up-to-date knowledge and are\nrelatively difficult to relearn. In this paper, we introduce EvoText, a novel\ntraining method that enhances the performance of any natural language\ngeneration model without requiring additional datasets during the entire\ntraining process (although a prior dataset is necessary for pretraining).\nEvoText employs two models: $G$, a text generation model, and $D$, a model that\ncan determine whether the data generated by $G$ is legitimate. Initially, the\nfine-tuned $D$ model serves as the knowledge base. The text generated by $G$ is\nthen input to $D$ to determine whether it is legitimate. Finally, $G$ is\nfine-tuned based on $D$'s output. EvoText enables the model to learn up-to-date\nknowledge through a self-escalation process that builds on a priori knowledge.\nWhen EvoText needs to learn something new, it simply fine-tunes the $D$ model.\nOur approach applies to autoregressive language modeling for all Transformer\nclasses. With EvoText, eight models achieved stable improvements in seven\nnatural language processing tasks without any changes to the model structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhengqing Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Huiwen Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.04054","description":"<p>Reliability of machine learning evaluation -- the consistency of observed\nevaluation scores across replicated model training runs -- is affected by\nseveral sources of nondeterminism which can be regarded as measurement noise.\nCurrent tendencies to remove noise in order to enforce reproducibility of\nresearch results neglect inherent nondeterminism at the implementation level\nand disregard crucial interaction effects between algorithmic noise factors and\ndata properties. This limits the scope of conclusions that can be drawn from\nsuch experiments. Instead of removing noise, we propose to incorporate several\nsources of variance, including their interaction with data properties, into an\nanalysis of significance and reliability of machine learning evaluation, with\nthe aim to draw inferences beyond particular instances of trained models. We\nshow how to use linear mixed effects models (LMEMs) to analyze performance\nevaluation scores, and to conduct statistical inference with a generalized\nlikelihood ratio test (GLRT). This allows us to incorporate arbitrary sources\nof noise like meta-parameter variations into statistical significance testing,\nand to assess performance differences conditional on data properties.\nFurthermore, a variance component analysis (VCA) enables the analysis of the\ncontribution of noise sources to overall variance and the computation of a\nreliability coefficient by the ratio of substantial to total variance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagmann_M/0/1/0/all/0/1\">Michael Hagmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_P/0/1/0/all/0/1\">Philipp Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hulk: Graph Neural Networks for Optimizing Regionally Distributed Computing Systems. (arXiv:2302.13741v2 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2302.13741","description":"<p>Large deep learning models have shown great potential for delivering\nexceptional results in various applications. However, the training process can\nbe incredibly challenging due to the models' vast parameter sizes, often\nconsisting of hundreds of billions of parameters. Common distributed training\nmethods, such as data parallelism, tensor parallelism, and pipeline\nparallelism, demand significant data communication throughout the process,\nleading to prolonged wait times for some machines in physically distant\ndistributed systems. To address this issue, we propose a novel solution called\nHulk, which utilizes a modified graph neural network to optimize distributed\ncomputing systems. Hulk not only optimizes data communication efficiency\nbetween different countries or even different regions within the same city, but\nalso provides optimal distributed deployment of models in parallel. For\nexample, it can place certain layers on a machine in a specific region or pass\nspecific parameters of a model to a machine in a particular location. By using\nHulk in experiments, we were able to improve the time efficiency of training\nlarge deep learning models on distributed systems by more than 20\\%. Our open\nsource collection of unlabeled data:https://github.com/DLYuanGod/Hulk.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhengqing Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Huiwen Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT as a Factual Inconsistency Evaluator for Text Summarization. (arXiv:2303.15621v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.15621","description":"<p>The performance of text summarization has been greatly boosted by pre-trained\nlanguage models. A main concern of existing methods is that most generated\nsummaries are not factually inconsistent with their source documents. To\nalleviate the problem, many efforts have focused on developing effective\nfactuality evaluation metrics based on natural language inference, question\nanswering, and syntactic dependency et al. However, these approaches are\nlimited by either their high computational complexity or the uncertainty\nintroduced by multi-component pipelines, resulting in only partial agreement\nwith human judgement. Most recently, large language models(LLMs) have shown\nexcellent performance in not only text generation but also language\ncomprehension. In this paper, we particularly explore ChatGPT's ability to\nevaluate factual inconsistency under a zero-shot setting by examining it on\nboth coarse-grained and fine-grained evaluation tasks including binary\nentailment inference, summary ranking, and consistency rating. Experimental\nresults indicate that ChatGPT generally outperforms previous evaluation metrics\nacross the three tasks, indicating its great potential for factual\ninconsistency evaluation. However, a closer inspection of ChatGPT's output\nreveals certain limitations including its preference for more lexically similar\ncandidates, false reasoning, and inadequate understanding of instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zheheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02017","description":"<p>Large language models have revolutionized the field of artificial\nintelligence and have been used in various applications. Among these models,\nChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,\nit stands out as a powerful tool that has been widely adopted. ChatGPT has been\nsuccessfully applied in numerous areas, including chatbots, content generation,\nlanguage translation, personalized recommendations, and even medical diagnosis\nand treatment. Its success in these applications can be attributed to its\nability to generate human-like responses, understand natural language, and\nadapt to different contexts. Its versatility and accuracy make it a powerful\ntool for natural language processing (NLP). However, there are also limitations\nto ChatGPT, such as its tendency to produce biased responses and its potential\nto perpetuate harmful language patterns. This article provides a comprehensive\noverview of ChatGPT, its applications, advantages, and limitations.\nAdditionally, the paper emphasizes the importance of ethical considerations\nwhen using this robust tool in real-world scenarios. Finally, This paper\ncontributes to ongoing discussions surrounding artificial intelligence and its\nimpact on vision and NLP domains by providing insights into prompt engineering\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hariri_W/0/1/0/all/0/1\">Walid Hariri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.04370","description":"<p>Human intelligence has the remarkable ability to assemble basic skills into\ncomplex ones so as to solve complex tasks. This ability is equally important\nfor Artificial Intelligence (AI), and thus, we assert that in addition to the\ndevelopment of large, comprehensive intelligent models, it is equally crucial\nto equip such models with the capability to harness various domain-specific\nexpert models for complex task-solving in the pursuit of Artificial General\nIntelligence (AGI). Recent developments in Large Language Models (LLMs) have\ndemonstrated remarkable learning and reasoning abilities, making them promising\nas a controller to select, synthesize, and execute external models to solve\ncomplex tasks. In this project, we develop OpenAGI, an open-source AGI research\nplatform, specifically designed to offer complex, multi-step tasks and\naccompanied by task-specific datasets, evaluation metrics, and a diverse range\nof extensible models. OpenAGI formulates complex tasks as natural language\nqueries, serving as input to the LLM. The LLM subsequently selects,\nsynthesizes, and executes models provided by OpenAGI to address the task.\nFurthermore, we propose a Reinforcement Learning from Task Feedback (RLTF)\nmechanism, which uses the task-solving result as feedback to improve the LLM's\ntask-solving ability. Thus, the LLM is responsible for synthesizing various\nexternal models for solving complex tasks, while RLTF provides feedback to\nimprove its task-solving ability, enabling a feedback loop for self-improving\nAI. We believe that the paradigm of LLMs operating various expert models for\ncomplex task-solving is a promising approach towards AGI. To facilitate the\ncommunity's long-term improvement and evaluation of AGI's ability, we\nopen-source the code, benchmark, and evaluation methods of the OpenAGI project\nat https://github.com/agiresearch/OpenAGI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jianchao Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Juntao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding. (arXiv:2304.05368v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.05368","description":"<p>Large language models (LLMs) have made significant progress in various\ndomains, including healthcare. However, the specialized nature of clinical\nlanguage understanding tasks presents unique challenges and limitations that\nwarrant further investigation. In this study, we conduct a comprehensive\nevaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within\nthe realm of clinical language understanding tasks. These tasks span a diverse\nrange, including named entity recognition, relation extraction, natural\nlanguage inference, semantic textual similarity, document classification, and\nquestion-answering. We also introduce a novel prompting strategy,\nself-questioning prompting (SQP), tailored to enhance LLMs' performance by\neliciting informative questions and answers pertinent to the clinical scenarios\nat hand. Our evaluation underscores the significance of task-specific learning\nstrategies and prompting techniques for improving LLMs' effectiveness in\nhealthcare-related tasks. Additionally, our in-depth error analysis on the\nchallenging relation extraction task offers valuable insights into error\ndistribution and potential avenues for improvement using SQP. Our study sheds\nlight on the practical implications of employing LLMs in the specialized domain\nof healthcare, serving as a foundation for future research and the development\nof potential applications in healthcare settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petzold_L/0/1/0/all/0/1\">Linda Petzold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distinguishing ChatGPT(-3.5, -4)-generated and human-written papers through Japanese stylometric analysis. (arXiv:2304.05534v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.05534","description":"<p>Text-generative artificial intelligence (AI), including ChatGPT, equipped\nwith GPT-3.5 and GPT-4, from OpenAI, has attracted considerable attention\nworldwide. In this study, first, we compared Japanese stylometric features\ngenerated by GPT (-3.5 and -4) and those written by humans. In this work, we\nperformed multi-dimensional scaling (MDS) to confirm the distributions of 216\ntexts of three classes (72 academic papers written by 36 single authors, 72\ntexts generated by GPT-3.5, and 72 texts generated by GPT-4 on the basis of the\ntitles of the aforementioned papers) focusing on the following stylometric\nfeatures: (1) bigrams of parts-of-speech, (2) bigram of postpositional particle\nwords, (3) positioning of commas, and (4) rate of function words. MDS revealed\ndistinct distributions at each stylometric feature of GPT (-3.5 and -4) and\nhuman. Although GPT-4 is more powerful than GPT-3.5 because it has more\nparameters, both GPT (-3.5 and -4) distributions are likely to overlap. These\nresults indicate that although the number of parameters may increase in the\nfuture, AI-generated texts may not be close to that written by humans in terms\nof stylometric features. Second, we verified the classification performance of\nrandom forest (RF) for two classes (GPT and human) focusing on Japanese\nstylometric features. This study revealed the high performance of RF in each\nstylometric feature. Furthermore, the RF classifier focusing on the rate of\nfunction words achieved 98.1% accuracy. The RF classifier focusing on all\nstylometric features reached 100% in terms of all performance indexes\n(accuracy, recall, precision, and F1 score). This study concluded that at this\nstage we human discriminate ChatGPT from human limited to Japanese language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaitsu_W/0/1/0/all/0/1\">Wataru Zaitsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Mingzhe Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Gender Bias in West Slavic Language Models. (arXiv:2304.05783v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.05783","description":"<p>Pre-trained language models have been known to perpetuate biases from the\nunderlying datasets to downstream tasks. However, these findings are\npredominantly based on monolingual language models for English, whereas there\nare few investigative studies of biases encoded in language models for\nlanguages beyond English. In this paper, we fill this gap by analysing gender\nbias in West Slavic language models. We introduce the first template-based\ndataset in Czech, Polish, and Slovak for measuring gender bias towards male,\nfemale and non-binary subjects. We complete the sentences using both mono- and\nmultilingual language models and assess their suitability for the masked\nlanguage modelling objective. Next, we measure gender bias encoded in West\nSlavic language models by quantifying the toxicity and genderness of the\ngenerated words. We find that these language models produce hurtful completions\nthat depend on the subject's gender. Perhaps surprisingly, Czech, Slovak, and\nPolish language models produce more hurtful completions with men as subjects,\nwhich, upon inspection, we find is due to completions being related to\nviolence, death, and sickness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinkova_S/0/1/0/all/0/1\">Sandra Martinkov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanczak_K/0/1/0/all/0/1\">Karolina Sta&#x144;czak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Homographic Disambiguation Representation for Neural Machine Translation. (arXiv:2304.05860v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.05860","description":"<p>Homographs, words with the same spelling but different meanings, remain\nchallenging in Neural Machine Translation (NMT). While recent works leverage\nvarious word embedding approaches to differentiate word sense in NMT, they do\nnot focus on the pivotal components in resolving ambiguities of homographs in\nNMT: the hidden states of an encoder. In this paper, we propose a novel\napproach to tackle homographic issues of NMT in the latent space. We first\ntrain an encoder (aka \"HDR-encoder\") to learn universal sentence\nrepresentations in a natural language inference (NLI) task. We further\nfine-tune the encoder using homograph-based synset sentences from WordNet,\nenabling it to learn word-level homographic disambiguation representations\n(HDR). The pre-trained HDR-encoder is subsequently integrated with a\ntransformer-based NMT in various schemes to improve translation accuracy.\nExperiments on four translation directions demonstrate the effectiveness of the\nproposed method in enhancing the performance of NMT systems in the BLEU scores\n(up to +2.3 compared to a solid baseline). The effects can be verified by other\nmetrics (F1, precision, and recall) of translation accuracy in an additional\ndisambiguation task. Visualization methods like heatmaps, T-SNE and translation\nexamples are also utilized to demonstrate the effects of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}