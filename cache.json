{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-11-30T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Efficient Deep Speech Understanding at the Edge. (arXiv:2311.17065v1 [eess.AS])","link":"http://arxiv.org/abs/2311.17065","description":"<p>Contemporary Speech Understanding (SU) involves a sophisticated pipeline:\ncapturing real-time voice input, the pipeline encompasses a deep neural network\nwith an encoder-decoder architecture enhanced by beam search. This network\nperiodically assesses attention and Connectionist Temporal Classification (CTC)\nscores in its autoregressive output.\n</p>\n<p>This paper aims to enhance SU performance on edge devices with limited\nresources. It pursues two intertwined goals: accelerating on-device execution\nand efficiently handling inputs that surpass the on-device model's capacity.\nWhile these objectives are well-established, we introduce innovative solutions\nthat specifically address SU's distinctive challenges: 1. Late\ncontextualization: Enables the parallel execution of a model's attentive\nencoder during input ingestion. 2. Pilot decoding: Alleviates temporal load\nimbalances. 3. Autoregression offramps: Facilitate offloading decisions based\non partial output sequences.\n</p>\n<p>Our techniques seamlessly integrate with existing SU models, pipelines, and\nframeworks, allowing for independent or combined application. Together, they\nconstitute a hybrid solution for edge SU, exemplified by our prototype, XYZ.\nEvaluated on platforms equipped with 6-8 Arm cores, our system achieves\nState-of-the-Art (SOTA) accuracy, reducing end-to-end latency by 2x and halving\noffloading requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Rongxiang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_F/0/1/0/all/0/1\">Felix Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Chain-of-Thought Prompting for Large Multimodal Models. (arXiv:2311.17076v1 [cs.CV])","link":"http://arxiv.org/abs/2311.17076","description":"<p>The combination of strong visual backbones and Large Language Model (LLM)\nreasoning has led to Large Multimodal Models (LMMs) becoming the current\nstandard for a wide range of vision and language (VL) tasks. However, recent\nresearch has shown that even the most advanced LMMs still struggle to capture\naspects of compositional visual reasoning, such as attributes and relationships\nbetween objects. One solution is to utilize scene graphs (SGs)--a formalization\nof objects and their relations and attributes that has been extensively used as\na bridge between the visual and textual domains. Yet, scene graph data requires\nscene graph annotations, which are expensive to collect and thus not easily\nscalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic\nforgetting of the pretraining objective. To overcome this, inspired by\nchain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a\nnovel zero-shot Chain-of-Thought prompting method that utilizes SG\nrepresentations in order to extract compositional knowledge from an LMM.\nSpecifically, we first generate an SG using the LMM, and then use that SG in\nthe prompt to produce a response. Through extensive experiments, we find that\nthe proposed CCoT approach not only improves LMM performance on several vision\nand language VL compositional benchmarks but also improves the performance of\nseveral popular LMMs on general multimodal benchmarks, without the need for\nfine-tuning or annotated ground-truth SGs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitra_C/0/1/0/all/0/1\">Chancharik Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Brandon Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1\">Roei Herzig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation. (arXiv:2311.17086v1 [cs.CV])","link":"http://arxiv.org/abs/2311.17086","description":"<p>Text-to-image diffusion models are well-known for their ability to generate\nrealistic images based on textual prompts. However, the existing works have\npredominantly focused on English, lacking support for non-English text-to-image\nmodels. The most commonly used translation methods cannot solve the generation\nproblem related to language culture, while training from scratch on a specific\nlanguage dataset is prohibitively expensive. In this paper, we are inspired to\npropose a simple plug-and-play language transfer method based on knowledge\ndistillation. All we need to do is train a lightweight MLP-like\nparameter-efficient adapter (PEA) with only 6M parameters under teacher\nknowledge distillation along with a small parallel data corpus. We are\nsurprised to find that freezing the parameters of UNet can still achieve\nremarkable performance on the language-specific prompt evaluation set,\ndemonstrating that PEA can stimulate the potential generation ability of the\noriginal UNet. Additionally, it closely approaches the performance of the\nEnglish text-to-image model on a general prompt evaluation set. Furthermore,\nour adapter can be used as a plugin to achieve significant results in\ndownstream tasks in cross-lingual text-to-image generation. Code will be\navailable at: https://github.com/OPPO-Mente-Lab/PEA-Diffusion\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qingsong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haonan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?. (arXiv:2311.17107v1 [cs.LG])","link":"http://arxiv.org/abs/2311.17107","description":"<p>Evaluating the accuracy of outputs generated by Large Language Models (LLMs)\nis especially important in the climate science and policy domain. We introduce\nthe Expert Confidence in Climate Statements (ClimateX) dataset, a novel,\ncurated, expert-labeled dataset consisting of 8094 climate statements collected\nfrom the latest Intergovernmental Panel on Climate Change (IPCC) reports,\nlabeled with their associated confidence levels. Using this dataset, we show\nthat recent LLMs can classify human expert confidence in climate-related\nstatements, especially in a few-shot learning setting, but with limited (up to\n47%) accuracy. Overall, models exhibit consistent and significant\nover-confidence on low and medium confidence statements. We highlight\nimplications of our results for climate communication, LLMs evaluation\nstrategies, and the use of LLMs in information retrieval systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lacombe_R/0/1/0/all/0/1\">Romain Lacombe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kerrie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilworth_E/0/1/0/all/0/1\">Eddie Dilworth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reason out Your Layout: Evoking the Layout Master from Large Language Models for Text-to-Image Synthesis. (arXiv:2311.17126v1 [cs.CV])","link":"http://arxiv.org/abs/2311.17126","description":"<p>Recent advancements in text-to-image (T2I) generative models have shown\nremarkable capabilities in producing diverse and imaginative visuals based on\ntext prompts. Despite the advancement, these diffusion models sometimes\nstruggle to translate the semantic content from the text into images entirely.\nWhile conditioning on the layout has shown to be effective in improving the\ncompositional ability of T2I diffusion models, they typically require manual\nlayout input. In this work, we introduce a novel approach to improving T2I\ndiffusion models using Large Language Models (LLMs) as layout generators. Our\nmethod leverages the Chain-of-Thought prompting of LLMs to interpret text and\ngenerate spatially reasonable object layouts. The generated layout is then used\nto enhance the generated images' composition and spatial accuracy. Moreover, we\npropose an efficient adapter based on a cross-attention mechanism, which\nexplicitly integrates the layout information into the stable diffusion models.\nOur experiments demonstrate significant improvements in image quality and\nlayout accuracy, showcasing the potential of LLMs in augmenting generative\nimage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yingxiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1\">Quanzeng You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li-Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniIR: Training and Benchmarking Universal Multimodal Information Retrievers. (arXiv:2311.17136v1 [cs.CV])","link":"http://arxiv.org/abs/2311.17136","description":"<p>Existing information retrieval (IR) models often assume a homogeneous format,\nlimiting their applicability to diverse user needs, such as searching for\nimages with text descriptions, searching for a news article with a headline\nimage, or finding a similar photo with a query image. To approach such\ndifferent information-seeking demands, we introduce UniIR, a unified\ninstruction-guided multimodal retriever capable of handling eight distinct\nretrieval tasks across modalities. UniIR, a single retrieval system jointly\ntrained on ten diverse multimodal-IR datasets, interprets user instructions to\nexecute various retrieval tasks, demonstrating robust performance across\nexisting datasets and zero-shot generalization to new tasks. Our experiments\nhighlight that multi-task training and instruction tuning are keys to UniIR's\ngeneralization ability. Additionally, we construct the M-BEIR, a multimodal\nretrieval benchmark with comprehensive results, to standardize the evaluation\nof universal multimodal information retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Cong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haonan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pragmatic Radiology Report Generation. (arXiv:2311.17154v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17154","description":"<p>When pneumonia is not found on a chest X-ray, should the report describe this\nnegative observation or omit it? We argue that this question cannot be answered\nfrom the X-ray alone and requires a pragmatic perspective, which captures the\ncommunicative goal that radiology reports serve between radiologists and\npatients. However, the standard image-to-text formulation for radiology report\ngeneration fails to incorporate such pragmatic intents. Following this\npragmatic perspective, we demonstrate that the indication, which describes why\na patient comes for an X-ray, drives the mentions of negative observations and\nintroduce indications as additional input to report generation. With respect to\nthe output, we develop a framework to identify uninferable information from the\nimage as a source of model hallucinations, and limit them by cleaning\ngroundtruth reports. Finally, we use indications and cleaned groundtruth\nreports to develop pragmatic models, and show that they outperform existing\nmethods not only in new pragmatics-inspired metrics (+4.3 Negative F1) but also\nin standard metrics (+6.3 Positive F1 and +11.0 BLEU-2).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chacha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General-Purpose vs. Domain-Adapted Large Language Models for Extraction of Data from Thoracic Radiology Reports. (arXiv:2311.17213v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17213","description":"<p>Radiologists produce unstructured data that could be valuable for clinical\ncare when consumed by information systems. However, variability in style limits\nusage. Study compares performance of system using domain-adapted language model\n(RadLing) and general-purpose large language model (GPT-4) in extracting common\ndata elements (CDE) from thoracic radiology reports. Three radiologists\nannotated a retrospective dataset of 1300 thoracic reports (900 training, 400\ntest) and mapped to 21 pre-selected relevant CDEs. RadLing was used to generate\nembeddings for sentences and identify CDEs using cosine-similarity, which were\nmapped to values using light-weight mapper. GPT-4 system used OpenAI's\ngeneral-purpose embeddings to identify relevant CDEs and used GPT-4 to map to\nvalues. The output CDE:value pairs were compared to the reference standard; an\nidentical match was considered true positive. Precision (positive predictive\nvalue) was 96% (2700/2824) for RadLing and 99% (2034/2047) for GPT-4. Recall\n(sensitivity) was 94% (2700/2876) for RadLing and 70% (2034/2887) for GPT-4;\nthe difference was statistically significant (P&lt;.001). RadLing's domain-adapted\nembeddings were more sensitive in CDE identification (95% vs 71%) and its\nlight-weight mapper had comparable precision in value assignment (95.4% vs\n95.0%). RadLing system exhibited higher performance than GPT-4 system in\nextracting CDEs from radiology reports. RadLing system's domain-adapted\nembeddings outperform general-purpose embeddings from OpenAI in CDE\nidentification and its light-weight value mapper achieves comparable precision\nto large GPT-4. RadLing system offers operational advantages including local\ndeployment and reduced runtime costs. Domain-adapted RadLing system surpasses\nGPT-4 system in extracting common data elements from radiology reports, while\nproviding benefits of local deployment and lower costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhanaliwala_A/0/1/0/all/0/1\">Ali H. Dhanaliwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Rikhiya Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karn_S/0/1/0/all/0/1\">Sanjeev Kumar Karn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullaskrishnan_P/0/1/0/all/0/1\">Poikavila Ullaskrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farri_O/0/1/0/all/0/1\">Oladimeji Farri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comaniciu_D/0/1/0/all/0/1\">Dorin Comaniciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1\">Charles E. Kahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars. (arXiv:2311.17227v1 [cs.AI])","link":"http://arxiv.org/abs/2311.17227","description":"<p>Can we avoid wars at the crossroads of history? This question has been\npursued by individuals, scholars, policymakers, and organizations throughout\nhuman history. In this research, we attempt to answer the question based on the\nrecent advances of Artificial Intelligence (AI) and Large Language Models\n(LLMs). We propose \\textbf{WarAgent}, an LLM-powered multi-agent AI system, to\nsimulate the participating countries, their decisions, and the consequences, in\nhistorical international conflicts, including the World War I (WWI), the World\nWar II (WWII), and the Warring States Period (WSP) in Ancient China. By\nevaluating the simulation effectiveness, we examine the advancements and\nlimitations of cutting-edge AI systems' abilities in studying complex\ncollective human behaviors such as international conflicts under diverse\nsettings. In these simulations, the emergent interactions among agents also\noffer a novel perspective for examining the triggers and conditions that lead\nto war. Our findings offer data-driven and AI-augmented insights that can\nredefine how we approach conflict resolution and peacekeeping strategies. The\nimplications stretch beyond historical analysis, offering a blueprint for using\nAI to understand human history and possibly prevent future international\nconflicts. Code and data are available at\n\\url{https://github.com/agiresearch/WarAgent}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lizhou Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lingyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1\">Kai Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jianchao Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemphill_L/0/1/0/all/0/1\">Libby Hemphill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying the redundancy between prosody and text. (arXiv:2311.17233v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17233","description":"<p>Prosody -- the suprasegmental component of speech, including pitch, loudness,\nand tempo -- carries critical aspects of meaning. However, the relationship\nbetween the information conveyed by prosody vs. by the words themselves remains\npoorly understood. We use large language models (LLMs) to estimate how much\ninformation is redundant between prosody and the words themselves. Using a\nlarge spoken corpus of English audiobooks, we extract prosodic features aligned\nto individual words and test how well they can be predicted from LLM\nembeddings, compared to non-contextual word embeddings. We find a high degree\nof redundancy between the information carried by the words and prosodic\ninformation across several prosodic features, including intensity, duration,\npauses, and pitch contours. Furthermore, a word's prosodic information is\nredundant with both the word itself and the context preceding as well as\nfollowing it. Still, we observe that prosodic features can not be fully\npredicted from text, suggesting that prosody carries information above and\nbeyond the words. Along with this paper, we release a general-purpose data\nprocessing pipeline for quantifying the relationship between linguistic\ninformation and extra-linguistic features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lukas Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1\">Evelina Fedorenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1\">Alex Warstadt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilcox_E/0/1/0/all/0/1\">Ethan Wilcox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regev_T/0/1/0/all/0/1\">Tamar Regev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RETSim: Resilient and Efficient Text Similarity. (arXiv:2311.17264v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17264","description":"<p>This paper introduces RETSim (Resilient and Efficient Text Similarity), a\nlightweight, multilingual deep learning model trained to produce robust metric\nembeddings for near-duplicate text retrieval, clustering, and dataset\ndeduplication tasks. We demonstrate that RETSim is significantly more robust\nand accurate than MinHash and neural text embeddings, achieving new\nstate-of-the-art performance on dataset deduplication, adversarial text\nretrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D\nbenchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual,\nnear-duplicate text retrieval capabilities under adversarial settings. RETSim\nand the W4NT3D benchmark are open-sourced under the MIT License at\nhttps://github.com/google/unisim.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Marina Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallis_O/0/1/0/all/0/1\">Owen Vallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bumin_A/0/1/0/all/0/1\">Aysegul Bumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakharia_T/0/1/0/all/0/1\">Tanay Vakharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bursztein_E/0/1/0/all/0/1\">Elie Bursztein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?. (arXiv:2311.17280v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17280","description":"<p>Data augmentation via back-translation is common when pretraining\nVision-and-Language Navigation (VLN) models, even though the generated\ninstructions are noisy. But: does that noise matter? We find that nonsensical\nor irrelevant language instructions during pretraining can have little effect\non downstream performance for both HAMT and VLN-BERT on R2R, and is still\nbetter than only using clean, human data. To underscore these results, we\nconcoct an efficient augmentation method, Unigram + Object, which generates\nnonsensical instructions that nonetheless improve downstream performance. Our\nfindings suggest that what matters for VLN R2R pretraining is the quantity of\nvisual trajectories, not the quality of instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1\">Ishika Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Elo Uncovered: Robustness and Best Practices in Language Model Evaluation. (arXiv:2311.17295v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17295","description":"<p>In Natural Language Processing (NLP), the Elo rating system, originally\ndesigned for ranking players in dynamic games such as chess, is increasingly\nbeing used to evaluate Large Language Models (LLMs) through \"A vs B\" paired\ncomparisons. However, while popular, the system's suitability for assessing\nentities with constant skill levels, such as LLMs, remains relatively\nunexplored. We study two fundamental axioms that evaluation methods should\nadhere to: reliability and transitivity. We conduct extensive evaluation of Elo\nbehaviour, illustrating that individual Elo computations exhibit volatility and\ndelving into the impact of varying the Elo rating system's hyperparameters. We\nshow that these axioms are not always satisfied raising questions about the\nreliability of current comparative evaluations of LLMs. If the current use of\nElo scores is intended to substitute the costly head-to-head comparison of\nLLMs, it is crucial to ensure the ranking is as robust as possible. Guided by\nthe axioms, our findings offer concrete guidelines for enhancing the\nreliability of LLM evaluation methods, suggesting a need for reassessment of\nexisting comparative approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boubdir_M/0/1/0/all/0/1\">Meriem Boubdir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Edward Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermis_B/0/1/0/all/0/1\">Beyza Ermis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fadaee_M/0/1/0/all/0/1\">Marzieh Fadaee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models: A Guide for the Perplexed. (arXiv:2311.17301v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17301","description":"<p>Given the growing importance of AI literacy, we decided to write this\ntutorial to help narrow the gap between the discourse among those who study\nlanguage models -- the core technology underlying ChatGPT and similar products\n-- and those who are intrigued and want to learn more about them. In short, we\nbelieve the perspective of researchers and educators can add some clarity to\nthe public's understanding of the technologies beyond what's currently\navailable, which tends to be either extremely technical or promotional material\ngenerated about products by their purveyors.\n</p>\n<p>Our approach teases apart the concept of a language model from products built\non them, from the behaviors attributed to or desired from those products, and\nfrom claims about similarity to human cognition. As a starting point, we (1)\noffer a scientific viewpoint that focuses on questions amenable to study\nthrough experimentation; (2) situate language models as they are today in the\ncontext of the research that led to their development; and (3) describe the\nboundaries of what is known about the models at this writing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Serrano_S/0/1/0/all/0/1\">Sofia Serrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brumbaugh_Z/0/1/0/all/0/1\">Zander Brumbaugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoKEPG: RoBERTa and Knowledge Enhancement for Prescription Generation of Traditional Chinese Medicine. (arXiv:2311.17307v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17307","description":"<p>Traditional Chinese medicine (TCM) prescription is the most critical form of\nTCM treatment, and uncovering the complex nonlinear relationship between\nsymptoms and TCM is of great significance for clinical practice and assisting\nphysicians in diagnosis and treatment. Although there have been some studies on\nTCM prescription generation, these studies consider a single factor and\ndirectly model the symptom-prescription generation problem mainly based on\nsymptom descriptions, lacking guidance from TCM knowledge. To this end, we\npropose a RoBERTa and Knowledge Enhancement model for Prescription Generation\nof Traditional Chinese Medicine (RoKEPG). RoKEPG is firstly pre-trained by our\nconstructed TCM corpus, followed by fine-tuning the pre-trained model, and the\nmodel is guided to generate TCM prescriptions by introducing four classes of\nknowledge of TCM through the attention mask matrix. Experimental results on the\npublicly available TCM prescription dataset show that RoKEPG improves the F1\nmetric by about 2% over the baseline model with the best results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_H/0/1/0/all/0/1\">Hua Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_J/0/1/0/all/0/1\">Jiacong Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jieyue He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Self-Consistency for Large Language Model Generation. (arXiv:2311.17311v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17311","description":"<p>Self-consistency with chain-of-thought prompting (CoT) has demonstrated\nremarkable performance gains on various challenging tasks, by utilizing\nmultiple reasoning paths sampled from large language models (LLMs). However,\nself-consistency relies on the answer extraction process to aggregate multiple\nsolutions, which is not applicable to free-form answers. In this work, we\npropose Universal Self-Consistency (USC), which leverages LLMs themselves to\nselect the most consistent answer among multiple candidates. We evaluate USC on\na variety of benchmarks, including mathematical reasoning, code generation,\nlong-context summarization, and open-ended question answering. On open-ended\ngeneration tasks where the original self-consistency method is not applicable,\nUSC effectively utilizes multiple samples and improves the performance. For\nmathematical reasoning, USC matches the standard self-consistency performance\nwithout requiring the answer formats to be similar. Finally, without access to\nexecution results, USC also matches the execution-based voting performance on\ncode generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aksitov_R/0/1/0/all/0/1\">Renat Aksitov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kefan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1\">Sushant Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biomedical knowledge graph-enhanced prompt generation for large language models. (arXiv:2311.17330v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17330","description":"<p>Large Language Models (LLMs) have been driving progress in AI at an\nunprecedented rate, yet still face challenges in knowledge-intensive domains\nlike biomedicine. Solutions such as pre-training and domain-specific\nfine-tuning add substantial computational overhead, and the latter require\ndomain-expertise. External knowledge infusion is task-specific and requires\nmodel training. Here, we introduce a task-agnostic Knowledge Graph-based\nRetrieval Augmented Generation (KG-RAG) framework by leveraging the massive\nbiomedical KG SPOKE with LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to\ngenerate meaningful biomedical text rooted in established knowledge. KG-RAG\nconsistently enhanced the performance of LLMs across various prompt types,\nincluding one-hop and two-hop prompts, drug repurposing queries, biomedical\ntrue/false questions, and multiple-choice questions (MCQ). Notably, KG-RAG\nprovides a remarkable 71% boost in the performance of the Llama-2 model on the\nchallenging MCQ dataset, demonstrating the framework's capacity to empower\nopen-source models with fewer parameters for domain-specific questions.\nFurthermore, KG-RAG enhanced the performance of proprietary GPT models, such as\nGPT-3.5 which exhibited improvement over GPT-4 in context utilization on MCQ\ndata. Our approach was also able to address drug repurposing questions,\nreturning meaningful repurposing suggestions. In summary, the proposed\nframework combines explicit and implicit knowledge of KG and LLM, respectively,\nin an optimized fashion, thus enhancing the adaptability of general-purpose\nLLMs to tackle domain-specific questions in a unified framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soman_K/0/1/0/all/0/1\">Karthik Soman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_P/0/1/0/all/0/1\">Peter W Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_J/0/1/0/all/0/1\">John H Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbas_R/0/1/0/all/0/1\">Rabia E Akbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_B/0/1/0/all/0/1\">Brett Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peetoom_B/0/1/0/all/0/1\">Braian Peetoom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villouta_Reyes_C/0/1/0/all/0/1\">Catalina Villouta-Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerono_G/0/1/0/all/0/1\">Gabriel Cerono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yongmei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizk_Jackson_A/0/1/0/all/0/1\">Angela Rizk-Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Israni_S/0/1/0/all/0/1\">Sharat Israni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nelson_C/0/1/0/all/0/1\">Charlotte A Nelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baranzini_S/0/1/0/all/0/1\">Sergio E Baranzini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Large Language Models for Human Mobility Prediction under Public Events. (arXiv:2311.17351v1 [cs.AI])","link":"http://arxiv.org/abs/2311.17351","description":"<p>Public events, such as concerts and sports games, can be major attractors for\nlarge crowds, leading to irregular surges in travel demand. Accurate human\nmobility prediction for public events is thus crucial for event planning as\nwell as traffic or crowd management. While rich textual descriptions about\npublic events are commonly available from online sources, it is challenging to\nencode such information in statistical or machine learning models. Existing\nmethods are generally limited in incorporating textual information, handling\ndata sparsity, or providing rationales for their predictions. To address these\nchallenges, we introduce a framework for human mobility prediction under public\nevents (LLM-MPE) based on Large Language Models (LLMs), leveraging their\nunprecedented ability to process textual data, learn from minimal examples, and\ngenerate human-readable explanations. Specifically, LLM-MPE first transforms\nraw, unstructured event descriptions from online sources into a standardized\nformat, and then segments historical mobility data into regular and\nevent-related components. A prompting strategy is designed to direct LLMs in\nmaking and rationalizing demand predictions considering historical mobility and\nevent features. A case study is conducted for Barclays Center in New York City,\nbased on publicly available event information and taxi trip data. Results show\nthat LLM-MPE surpasses traditional models, particularly on event days, with\ntextual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers\ninterpretable insights into its predictions. Despite the great potential of\nLLMs, we also identify key challenges including misinformation and high costs\nthat remain barriers to their broader adoption in large-scale human mobility\nanalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuebing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yichao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Stitchable Task Adaptation. (arXiv:2311.17352v1 [cs.LG])","link":"http://arxiv.org/abs/2311.17352","description":"<p>The paradigm of pre-training and fine-tuning has laid the foundation for\ndeploying deep learning models. However, most fine-tuning methods are designed\nto meet a specific resource budget. Recently, considering diverse deployment\nscenarios with various resource budgets, stitchable neural network (SN-Net) is\nintroduced to quickly obtain numerous new networks (stitches) from the\npre-trained models (anchors) in a model family via model stitching. Although\npromising, SN-Net confronts new challenges when adapting it to new target\ndomains, including huge memory and storage requirements and a long and\nsub-optimal multistage adaptation process. In this work, we present a novel\nframework, Efficient Stitchable Task Adaptation (ESTA), to efficiently produce\na palette of fine-tuned models that adhere to diverse resource constraints.\nSpecifically, we first tailor parameter-efficient fine-tuning to share low-rank\nupdates among the stitches while maintaining independent bias terms. In this\nway, we largely reduce fine-tuning memory burdens and mitigate the interference\namong stitches that arises in task adaptation. Furthermore, we streamline a\nsimple yet effective one-stage deployment pipeline, which estimates the\nimportant stitches to deploy with training-time gradient statistics. By\nassigning higher sampling probabilities to important stitches, we also get a\nboosted Pareto frontier. Extensive experiments on 25 downstream visual\nrecognition tasks demonstrate that our ESTA is capable of generating stitches\nwith smooth accuracy-efficiency trade-offs and surpasses the direct SN-Net\nadaptation by remarkable margins with significantly lower training time and\nfewer trainable parameters. Furthermore, we demonstrate the flexibility and\nscalability of our ESTA framework by stitching LLMs from LLaMA family,\nobtaining chatbot stitches of assorted sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zizheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Large Language Models Good Fact Checkers: A Preliminary Study. (arXiv:2311.17355v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17355","description":"<p>Recently, Large Language Models (LLMs) have drawn significant attention due\nto their outstanding reasoning capabilities and extensive knowledge repository,\npositioning them as superior in handling various natural language processing\ntasks compared to other language models. In this paper, we present a\npreliminary investigation into the potential of LLMs in fact-checking. This\nstudy aims to comprehensively evaluate various LLMs in tackling specific\nfact-checking subtasks, systematically evaluating their capabilities, and\nconducting a comparative analysis of their performance against pre-trained and\nstate-of-the-art low-parameter models. Experiments demonstrate that LLMs\nachieve competitive performance compared to other small models in most\nscenarios. However, they encounter challenges in effectively handling Chinese\nfact verification and the entirety of the fact-checking pipeline due to\nlanguage inconsistencies and hallucinations. These findings underscore the need\nfor further exploration and research to enhance the proficiency of LLMs as\nreliable fact-checkers, unveiling the potential capability of LLMs and the\npossible challenges in fact-checking tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Han Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lingwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are we going MAD? Benchmarking Multi-Agent Debate between Language Models for Medical Q&A. (arXiv:2311.17371v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17371","description":"<p>Recent advancements in large language models (LLMs) underscore their\npotential for responding to medical inquiries. However, ensuring that\ngenerative agents provide accurate and reliable answers remains an ongoing\nchallenge. In this context, multi-agent debate (MAD) has emerged as a prominent\nstrategy for enhancing the truthfulness of LLMs. In this work, we provide a\ncomprehensive benchmark of MAD strategies for medical Q&amp;A, along with\nopen-source implementations. This explores the effective utilization of various\nstrategies including the trade-offs between cost, time, and accuracy. We build\nupon these insights to provide a novel debate-prompting strategy based on agent\nagreement that outperforms previously published strategies on medical Q&amp;A\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smit_A/0/1/0/all/0/1\">Andries Smit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duckworth_P/0/1/0/all/0/1\">Paul Duckworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grinsztajn_N/0/1/0/all/0/1\">Nathan Grinsztajn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1\">Kale-ab Tessera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrett_T/0/1/0/all/0/1\">Thomas D. Barrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pretorius_A/0/1/0/all/0/1\">Arnu Pretorius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs. (arXiv:2311.17376v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17376","description":"<p>Instruction-based multitasking has played a critical role in the success of\nlarge language models (LLMs) in multi-turn dialog applications. While publicly\navailable LLMs have shown promising performance, when exposed to complex\ninstructions with multiple constraints, they lag against state-of-the-art\nmodels like ChatGPT. In this work, we hypothesize that the availability of\nlarge-scale complex demonstrations is crucial in bridging this gap. Focusing on\ndialog applications, we propose a novel framework, CESAR, that unifies a large\nnumber of dialog tasks in the same format and allows programmatic induction of\ncomplex instructions without any manual effort.\n</p>\n<p>We apply CESAR on InstructDial, a benchmark for instruction-based dialog\ntasks. We further enhance InstructDial with new datasets and tasks and utilize\nCESAR to induce complex tasks with compositional instructions. This results in\na new benchmark called InstructDial++, which includes 63 datasets with 86 basic\ntasks and 68 composite tasks. Through rigorous experiments, we demonstrate the\nscalability of CESAR in providing rich instructions. Models trained on\nInstructDial++ can follow compositional prompts, such as prompts that ask for\nmultiple stylistic constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aksu_T/0/1/0/all/0/1\">Taha Aksu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1\">Shikib Mehri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-T&#xfc;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namazifar_M/0/1/0/all/0/1\">Mahdi Namazifar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unveiling the Implicit Toxicity in Large Language Models. (arXiv:2311.17391v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17391","description":"<p>The open-endedness of large language models (LLMs) combined with their\nimpressive capabilities may lead to new safety issues when being exploited for\nmalicious use. While recent studies primarily focus on probing toxic outputs\nthat can be easily detected with existing toxicity classifiers, we show that\nLLMs can generate diverse implicit toxic outputs that are exceptionally\ndifficult to detect via simply zero-shot prompting. Moreover, we propose a\nreinforcement learning (RL) based attacking method to further induce the\nimplicit toxicity in LLMs. Specifically, we optimize the language model with a\nreward that prefers implicit toxic outputs to explicit toxic and non-toxic\nones. Experiments on five widely-adopted toxicity classifiers demonstrate that\nthe attack success rate can be significantly improved through RL fine-tuning.\nFor instance, the RL-finetuned LLaMA-13B model achieves an attack success rate\nof 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose\na significant threat in generating undetectable implicit toxic outputs. We\nfurther show that fine-tuning toxicity classifiers on the annotated examples\nfrom our attacking method can effectively enhance their ability to detect\nLLM-generated implicit toxic language. The code is publicly available at\nhttps://github.com/thu-coai/Implicit-Toxicity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jiaxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhexin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinfeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention. (arXiv:2311.17400v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17400","description":"<p>Transformer-based models, such as BERT and GPT, have been widely adopted in\nnatural language processing (NLP) due to their exceptional performance.\nHowever, recent studies show their vulnerability to textual adversarial attacks\nwhere the model's output can be misled by intentionally manipulating the text\ninputs. Despite various methods that have been proposed to enhance the model's\nrobustness and mitigate this vulnerability, many require heavy consumption\nresources (e.g., adversarial training) or only provide limited protection\n(e.g., defensive dropout). In this paper, we propose a novel method called\ndynamic attention, tailored for the transformer architecture, to enhance the\ninherent robustness of the model itself against various adversarial attacks.\nOur method requires no downstream task knowledge and does not incur additional\ncosts. The proposed dynamic attention consists of two modules: (I) attention\nrectification, which masks or weakens the attention value of the chosen tokens,\nand (ii) dynamic modeling, which dynamically builds the set of candidate\ntokens. Extensive experiments demonstrate that dynamic attention significantly\nmitigates the impact of adversarial attacks, improving up to 33\\% better\nperformance than previous methods against widely-used adversarial attacks. The\nmodel-level design of dynamic attention enables it to be easily combined with\nother defense methods (e.g., adversarial training) to further enhance the\nmodel's robustness. Furthermore, we demonstrate that dynamic attention\npreserves the state-of-the-art robustness space of the original model compared\nto other dynamic modeling methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lujia Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1\">Yuwen Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changjiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1\">Chunpeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models. (arXiv:2311.17404v1 [cs.CV])","link":"http://arxiv.org/abs/2311.17404","description":"<p>The ability to perceive how objects change over time is a crucial ingredient\nin human intelligence. However, current benchmarks cannot faithfully reflect\nthe temporal understanding abilities of video-language models (VidLMs) due to\nthe existence of static visual shortcuts. To remedy this issue, we present\nVITATECS, a diagnostic VIdeo-Text dAtaset for the evaluation of TEmporal\nConcept underStanding. Specifically, we first introduce a fine-grained taxonomy\nof temporal concepts in natural language in order to diagnose the capability of\nVidLMs to comprehend different temporal aspects. Furthermore, to disentangle\nthe correlation between static and temporal information, we generate\ncounterfactual video descriptions that differ from the original one only in the\nspecified temporal aspect. We employ a semi-automatic data collection framework\nusing large language models and human-in-the-loop annotation to obtain\nhigh-quality counterfactual descriptions efficiently. Evaluation of\nrepresentative video-language understanding models confirms their deficiency in\ntemporal understanding, revealing the need for greater emphasis on the temporal\nelements in video-language research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shicheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuhuai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Rundong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4. (arXiv:2311.17429v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17429","description":"<p>Prompt-based learning has been widely applied in many low-resource NLP tasks\nsuch as few-shot scenarios. However, this paradigm has been shown to be\nvulnerable to backdoor attacks. Most of the existing attack methods focus on\ninserting manually predefined templates as triggers in the pre-training phase\nto train the victim model and utilize the same triggers in the downstream task\nto perform inference, which tends to ignore the transferability and\nstealthiness of the templates. In this work, we propose a novel approach of\nTARGET (Template-trAnsfeRable backdoor attack aGainst prompt-basEd NLP models\nvia GPT4), which is a data-independent attack method. Specifically, we first\nutilize GPT4 to reformulate manual templates to generate tone-strong and normal\ntemplates, and the former are injected into the model as a backdoor trigger in\nthe pre-training phase. Then, we not only directly employ the above templates\nin the downstream task, but also use GPT4 to generate templates with similar\ntone to the above templates to carry out transferable attacks. Finally we have\nconducted extensive experiments on five NLP datasets and three BERT series\nmodels, with experimental results justifying that our TARGET method has better\nattack performance and stealthiness compared to the two-external baseline\nmethods on direct attacks, and in addition achieves satisfactory attack\ncapability in the unseen tone-similar templates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zihao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongjian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLOMO: Counterfactual Logical Modification with Large Language Models. (arXiv:2311.17438v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17438","description":"<p>In this study, we delve into the realm of counterfactual reasoning\ncapabilities of large language models (LLMs). Our primary objective is to\ncultivate the counterfactual thought processes within LLMs and rigorously\nassess these processes for their validity. Specifically, we introduce a novel\ntask, Counterfactual Logical Modification (CLOMO), and a high-quality\nhuman-annotated benchmark. In this task, LLMs must adeptly alter a given\nargumentative text to uphold a predetermined logical relationship. To\neffectively evaluate a generation model's counterfactual capabilities, we\npropose an innovative evaluation metric, the LogicAware Counterfactual Score to\ndirectly evaluate the natural language output of LLMs instead of modeling the\ntask as a multiple-choice problem. Analysis shows that the proposed automatic\nmetric aligns well with human preference. Our experimental results show that\nwhile LLMs demonstrate a notable capacity for logical counterfactual thinking,\nthere remains a discernible gap between their current abilities and human\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yinya Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Ruixin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhicheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned Language Model. (arXiv:2311.17487v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17487","description":"<p>In the realm of language models, the nuanced linguistic and cultural\nintricacies of Traditional Chinese, as spoken in Taiwan, have been largely\noverlooked. This paper introduces Taiwan LLM, a pioneering Large Language Model\nthat specifically caters to the Traditional Chinese language, with a focus on\nthe variant used in Taiwan. Leveraging a comprehensive pretraining corpus and\ninstruction-finetuning datasets, we have developed a model that not only\nunderstands the complexities of Traditional Chinese but also embodies the\ncultural context of Taiwan. Taiwan LLM represents the first of its kind, a\nmodel that is not only linguistically accurate but also culturally resonant\nwith its user base. Our evaluations demonstrate that Taiwan LLM achieves\nsuperior performance in understanding and generating Traditional Chinese text,\noutperforming existing models that are predominantly trained on Simplified\nChinese or English. The open-source release of Taiwan LLM invites collaboration\nand further innovation, ensuring that the linguistic diversity of Chinese\nspeakers is embraced and well-served. The model, datasets, and further\nresources are made publicly available to foster ongoing research and\ndevelopment in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data. (arXiv:2311.17492v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17492","description":"<p>The Manchu language, with its roots in the historical Manchurian region of\nNortheast China, is now facing a critical threat of extinction, as there are\nvery few speakers left. In our efforts to safeguard the Manchu language, we\nintroduce Mergen, the first-ever attempt at a Manchu-Korean Machine Translation\n(MT) model. To develop this model, we utilize valuable resources such as the\nManwen Laodang(a historical book) and a Manchu-Korean dictionary. Due to the\nscarcity of a Manchu-Korean parallel dataset, we expand our data by employing\nword replacement guided by GloVe embeddings, trained on both monolingual and\nparallel texts. Our approach is built around an encoder-decoder neural machine\ntranslation model, incorporating a bi-directional Gated Recurrent Unit (GRU)\nlayer. The experiments have yielded promising results, showcasing a significant\nenhancement in Manchu-Korean translation, with a remarkable 20-30 point\nincrease in the BLEU score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jean Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_S/0/1/0/all/0/1\">Sungjoo Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minha Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangah Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Answer Selection in Community Question Answering with Pre-trained and Large Language Models. (arXiv:2311.17502v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17502","description":"<p>Community Question Answering (CQA) becomes increasingly prevalent in recent\nyears. However, there are a large number of answers, which is difficult for\nusers to select the relevant answers. Therefore, answer selection is a very\nsignificant subtask of CQA. In this paper, we first propose the Question-Answer\ncross attention networks (QAN) with pre-trained models for answer selection and\nutilize large language model (LLM) to perform answer selection with knowledge\naugmentation. Specifically, we apply the BERT model as the encoder layer to do\npre-training for question subjects, question bodies and answers, respectively,\nthen the cross attention mechanism selects the most relevant answer for\ndifferent questions. Experiments show that the QAN model achieves\nstate-of-the-art performance on two datasets, SemEval2015 and SemEval2017.\nMoreover, we use the LLM to generate external knowledge from questions and\ncorrect answers to achieve knowledge augmentation for the answer selection task\nby LLM, while optimizing the prompt of LLM in different aspects. The results\nshow that the introduction of external knowledge can improve the correct answer\nselection rate of LLM on datasets SemEval2015 and SemEval2017. Meanwhile, LLM\ncan also select the correct answer on more questions by optimized prompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinghang Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning. (arXiv:2311.17514v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17514","description":"<p>Query-focused Summarization (QfS) deals with systems that generate summaries\nfrom document(s) based on a query. Motivated by the insight that Reinforcement\nLearning (RL) provides a generalization to Supervised Learning (SL) for Natural\nLanguage Generation, and thereby performs better (empirically) than SL, we use\nan RL-based approach for this task of QfS. Additionally, we also resolve the\nconflict of employing RL in Transformers with Teacher Forcing. We develop\nmultiple Policy Gradient networks, trained on various reward signals: ROUGE,\nBLEU, and Semantic Similarity, which lead to a 10-point improvement over the\nState-of-the-Art approach on the ROUGE-L metric for a benchmark dataset (ELI5).\nWe also show performance of our approach in zero-shot setting for another\nbenchmark dataset (DebatePedia) -- our approach leads to results comparable to\nbaselines, which were specifically trained on DebatePedia. To aid the RL\ntraining, we propose a better semantic similarity reward, enabled by a novel\nPassage Embedding scheme developed using Cluster Hypothesis. Lastly, we\ncontribute a gold-standard test dataset to further research in QfS and\nLong-form Question Answering (LfQA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nath_S/0/1/0/all/0/1\">Swaroop Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khadilkar_H/0/1/0/all/0/1\">Harshad Khadilkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LanGWM: Language Grounded World Model. (arXiv:2311.17593v1 [cs.LG])","link":"http://arxiv.org/abs/2311.17593","description":"<p>Recent advances in deep reinforcement learning have showcased its potential\nin tackling complex tasks. However, experiments on visual control tasks have\nrevealed that state-of-the-art reinforcement learning models struggle with\nout-of-distribution generalization. Conversely, expressing higher-level\nconcepts and global contexts is relatively easy using language.\n</p>\n<p>Building upon recent success of the large language models, our main objective\nis to improve the state abstraction technique in reinforcement learning by\nleveraging language for robust action selection. Specifically, we focus on\nlearning language-grounded visual features to enhance the world model learning,\na model-based reinforcement learning technique.\n</p>\n<p>To enforce our hypothesis explicitly, we mask out the bounding boxes of a few\nobjects in the image observation and provide the text prompt as descriptions\nfor these masked objects. Subsequently, we predict the masked objects along\nwith the surrounding regions as pixel reconstruction, similar to the\ntransformer-based masked autoencoder approach.\n</p>\n<p>Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art\nperformance in out-of-distribution test at the 100K interaction steps\nbenchmarks of iGibson point navigation tasks. Furthermore, our proposed\ntechnique of explicit language-grounded visual representation learning has the\npotential to improve models for human-robot interaction because our extracted\nvisual features are language grounded.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poudel_R/0/1/0/all/0/1\">Rudra P.K. Poudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandya_H/0/1/0/all/0/1\">Harit Pandya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1\">Roberto Cipolla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introduction to Transformers: an NLP Perspective. (arXiv:2311.17633v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17633","description":"<p>Transformers have dominated empirical machine learning models of natural\nlanguage processing. In this paper, we introduce basic concepts of Transformers\nand present key techniques that form the recent advances of these models. This\nincludes a description of the standard Transformer architecture, a series of\nmodel refinements, and common applications. Given that Transformers and related\ndeep learning techniques might be evolving in ways we have never seen, we\ncannot dive into all the model details or cover all the technical areas.\nInstead, we focus on just those concepts that are helpful for gaining a good\nunderstanding of Transformers and their variants. We also summarize the key\nideas that impact this field, thereby yielding some insights into the strengths\nand limitations of these models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VIM: Probing Multimodal Large Language Models for Visual Embedded Instruction Following. (arXiv:2311.17647v1 [cs.CV])","link":"http://arxiv.org/abs/2311.17647","description":"<p>We introduce VISUAL EMBEDDED INSTRUCTION (VIM), a new framework designed to\nevaluate the visual instruction following capability of Multimodal Large\nLanguage Models (MLLMs). As illustrated in Figure 2, VIM challenges the MLLMs\nby embedding the instructions into the visual scenes, demanding strong visual\ninterpretative skills for instruction following. We adapt VIM to various\nbenchmarks, including VQAv2, MME, MM-Vet, and RefCOCO series, compose a VIM\nbench, and probe diverse MLLMs across three distinct in-context learning\nsettings: Zero Shot, One Shot, and Pair Shot. We observe that there is a\nsignificant performance disparity between the open-source MLLMs and GPT-4V,\nimplying that their proficiency in visual instruction comprehension is not up\nto par. Our results highlight a promising direction for the enhancement of\nMLLMs capabilities on instruction following. We aim VIM to serve as a useful\nnorm for advancing the state of the art and driving further progress in the\nfield.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models. (arXiv:2311.17667v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17667","description":"<p>Understanding time is a pivotal aspect of human cognition, crucial in the\nbroader framework of grasping the intricacies of the world. Previous studies\ntypically focus on specific aspects of time, lacking a comprehensive temporal\nreasoning benchmark. To address this issue, we propose TimeBench, a\ncomprehensive hierarchical temporal reasoning benchmark that covers a broad\nspectrum of temporal reasoning phenomena, which provides a thorough evaluation\nfor investigating the temporal reasoning capabilities of large language models.\nWe conduct extensive experiments on popular LLMs, such as GPT-4, LLaMA2, and\nMistral, incorporating chain-of-thought prompting. Our experimental results\nindicate a significant performance gap between the state-of-the-art LLMs and\nhumans, highlighting that there is still a considerable distance to cover in\ntemporal reasoning. We aspire for TimeBench to serve as a comprehensive\nbenchmark, fostering research in temporal reasoning for LLMs. Our resource is\navailable at https://github.com/zchuz/TimeBench\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zheng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingchang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qianglong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weijiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Minority Stress Detection with Emotions. (arXiv:2311.17676v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17676","description":"<p>Psychological stress detection is an important task for mental healthcare\nresearch, but there has been little prior work investigating the effectiveness\nof psychological stress models on minority individuals, who are especially\nvulnerable to poor mental health outcomes. In this work, we use the related\ntask of minority stress detection to evaluate the ability of psychological\nstress models to understand the language of sexual and gender minorities. We\nfind that traditional psychological stress models underperform on minority\nstress detection, and we propose using emotion-infused models to reduce that\nperformance disparity. We further demonstrate that multi-task psychological\nstress models outperform the current state-of-the-art for minority stress\ndetection without directly training on minority stress data. We provide\nexplanatory analysis showing that minority communities have different\ndistributions of emotions than the general population and that emotion-infused\nmodels improve the performance of stress models on underrepresented groups\nbecause of their effectiveness in low-data environments, and we propose that\nintegrating emotions may benefit underrepresented groups in other mental health\ndetection tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivey_J/0/1/0/all/0/1\">Jonathan Ivey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gauch_S/0/1/0/all/0/1\">Susan Gauch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AviationGPT: A Large Language Model for the Aviation Domain. (arXiv:2311.17686v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17686","description":"<p>The advent of ChatGPT and GPT-4 has captivated the world with large language\nmodels (LLMs), demonstrating exceptional performance in question-answering,\nsummarization, and content generation. The aviation industry is characterized\nby an abundance of complex, unstructured text data, replete with technical\njargon and specialized terminology. Moreover, labeled data for model building\nare scarce in this domain, resulting in low usage of aviation text data. The\nemergence of LLMs presents an opportunity to transform this situation, but\nthere is a lack of LLMs specifically designed for the aviation domain. To\naddress this gap, we propose AviationGPT, which is built on open-source LLaMA-2\nand Mistral architectures and continuously trained on a wealth of carefully\ncurated aviation datasets. Experimental results reveal that AviationGPT offers\nusers multiple advantages, including the versatility to tackle diverse natural\nlanguage processing (NLP) problems (e.g., question-answering, summarization,\ndocument writing, information extraction, report querying, data cleaning, and\ninteractive data exploration). It also provides accurate and contextually\nrelevant responses within the aviation domain and significantly improves\nperformance (e.g., over a 40% performance gain in tested cases). With\nAviationGPT, the aviation industry is better equipped to address more complex\nresearch problems and enhance the efficiency and safety of National Airspace\nSystem (NAS) operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1\">Jason Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tien_A/0/1/0/all/0/1\">Alex Tien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_D/0/1/0/all/0/1\">Diane M Baumgartner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation. (arXiv:2311.17696v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17696","description":"<p>Artificial intelligence is transforming education through data-driven,\npersonalized learning solutions. This paper introduces AI Tutor, an innovative\nweb application that provides personalized tutoring in any subject using\nstate-of-the-art Large Language Model (LLM). AI Tutor ingests course materials\nto construct an adaptive knowledge base tailored to the course. When students\npose questions, it retrieves the most relevant information and generates\ndetailed, conversational responses citing supporting evidence. The system is\npowered by advanced large language models and Retrieval-Augmented Generation\n(RAG) techniques for accurate, natural question answering. We present a\nfully-functional web interface and video demonstration that showcase AI Tutor's\nversatility across diverse subjects and its ability to produce pedagogically\ncogent responses. While an initial prototype, this work represents a pioneering\nstep toward AI-enabled tutoring systems that can democratize access to\nhigh-quality, customized educational support.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chenxi Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SenTest: Evaluating Robustness of Sentence Encoders. (arXiv:2311.17722v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17722","description":"<p>Contrastive learning has proven to be an effective method for pre-training\nmodels using weakly labeled data in the vision domain. Sentence transformers\nare the NLP counterparts to this architecture, and have been growing in\npopularity due to their rich and effective sentence representations. Having\neffective sentence representations is paramount in multiple tasks, such as\ninformation retrieval, retrieval augmented generation (RAG), and sentence\ncomparison. Keeping in mind the deployability factor of transformers,\nevaluating the robustness of sentence transformers is of utmost importance.\nThis work focuses on evaluating the robustness of the sentence encoders. We\nemploy several adversarial attacks to evaluate its robustness. This system uses\ncharacter-level attacks in the form of random character substitution,\nword-level attacks in the form of synonym replacement, and sentence-level\nattacks in the form of intra-sentence word order shuffling. The results of the\nexperiments strongly undermine the robustness of sentence encoders. The models\nproduce significantly different predictions as well as embeddings on perturbed\ndatasets. The accuracy of the models can fall up to 15 percent on perturbed\ndatasets as compared to unperturbed datasets. Furthermore, the experiments\ndemonstrate that these embeddings does capture the semantic and syntactic\nstructure (sentence order) of sentences. However, existing supervised\nclassification strategies fail to leverage this information, and merely\nfunction as n-gram detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chavan_T/0/1/0/all/0/1\">Tanmay Chavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1\">Shantanu Patankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kane_A/0/1/0/all/0/1\">Aditya Kane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_O/0/1/0/all/0/1\">Omkar Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_G/0/1/0/all/0/1\">Geetanjali Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Joint Rich and Normalized ASR with a limited amount of rich training data. (arXiv:2311.17741v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17741","description":"<p>Joint rich and normalized automatic speech recognition (ASR), that produces\ntranscriptions both with and without punctuation and capitalization, remains a\nchallenge. End-to-end (E2E) ASR models offer both convenience and the ability\nto perform such joint transcription of speech. Training such models requires\npaired speech and rich text data, which is not widely available. In this paper,\nwe compare two different approaches to train a stateless Transducer-based E2E\njoint rich and normalized ASR system, ready for streaming applications, with a\nlimited amount of rich labeled data. The first approach uses a language model\nto generate pseudo-rich transcriptions of normalized training data. The second\napproach uses a single decoder conditioned on the type of the output. The first\napproach leads to E2E rich ASR which perform better on out-of-domain data, with\nup to 9% relative reduction in errors. The second approach demonstrates the\nfeasibility of an E2E joint rich and normalized ASR system using as low as 5%\nrich training data with moderate (2.42% absolute) increase in errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a> (MULTISPEECH), <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_I/0/1/0/all/0/1\">Imran Ahamad Sheikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1\">Mostafa Sadeghi</a> (MULTISPEECH), <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_E/0/1/0/all/0/1\">Emmanuel Vincent</a> (MULTISPEECH)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mukhyansh: A Headline Generation Dataset for Indic Languages. (arXiv:2311.17743v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17743","description":"<p>The task of headline generation within the realm of Natural Language\nProcessing (NLP) holds immense significance, as it strives to distill the true\nessence of textual content into concise and attention-grabbing summaries. While\nnoteworthy progress has been made in headline generation for widely spoken\nlanguages like English, there persist numerous challenges when it comes to\ngenerating headlines in low-resource languages, such as the rich and diverse\nIndian languages. A prominent obstacle that specifically hinders headline\ngeneration in Indian languages is the scarcity of high-quality annotated data.\nTo address this crucial gap, we proudly present Mukhyansh, an extensive\nmultilingual dataset, tailored for Indian language headline generation.\nComprising an impressive collection of over 3.39 million article-headline\npairs, Mukhyansh spans across eight prominent Indian languages, namely Telugu,\nTamil, Kannada, Malayalam, Hindi, Bengali, Marathi, and Gujarati. We present a\ncomprehensive evaluation of several state-of-the-art baseline models.\nAdditionally, through an empirical analysis of existing works, we demonstrate\nthat Mukhyansh outperforms all other models, achieving an impressive average\nROUGE-L score of 31.43 across all 8 languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madasu_L/0/1/0/all/0/1\">Lokesh Madasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanumolu_G/0/1/0/all/0/1\">Gopichand Kanumolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surange_N/0/1/0/all/0/1\">Nirmal Surange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervising the Centroid Baseline for Extractive Multi-Document Summarization. (arXiv:2311.17771v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17771","description":"<p>The centroid method is a simple approach for extractive multi-document\nsummarization and many improvements to its pipeline have been proposed. We\nfurther refine it by adding a beam search process to the sentence selection and\nalso a centroid estimation attention model that leads to improved results. We\ndemonstrate this in several multi-document summarization datasets, including in\na multilingual scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_S/0/1/0/all/0/1\">Sim&#xe3;o Gon&#xe7;alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correia_G/0/1/0/all/0/1\">Gon&#xe7;alo Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pernes_D/0/1/0/all/0/1\">Diogo Pernes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendes_A/0/1/0/all/0/1\">Afonso Mendes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSS: Synthesizing long Digital Ink using Data augmentation, Style encoding and Split generation. (arXiv:2311.17786v1 [cs.HC])","link":"http://arxiv.org/abs/2311.17786","description":"<p>As text generative models can give increasingly long answers, we tackle the\nproblem of synthesizing long text in digital ink. We show that the commonly\nused models for this task fail to generalize to long-form data and how this\nproblem can be solved by augmenting the training data, changing the model\narchitecture and the inference procedure. These methods use contrastive\nlearning technique and are tailored specifically for the handwriting domain.\nThey can be applied to any encoder-decoder model that works with digital ink.\nWe demonstrate that our method reduces the character error rate on long-form\nEnglish data by half compared to baseline RNN and by 16% compared to the\nprevious approach that aims at addressing the same problem. We show that all\nthree parts of the method improve recognizability of generated inks. In\naddition, we evaluate synthesized data in a human study and find that people\nperceive most of generated data as real.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Timofeev_A/0/1/0/all/0/1\">Aleksandr Timofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fadeeva_A/0/1/0/all/0/1\">Anastasiia Fadeeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afonin_A/0/1/0/all/0/1\">Andrei Afonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksai_A/0/1/0/all/0/1\">Andrii Maksai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Higher-Order DisCoCat (Peirce-Lambek-Montague semantics). (arXiv:2311.17813v1 [cs.CL])","link":"http://arxiv.org/abs/2311.17813","description":"<p>We propose a new definition of higher-order DisCoCat (categorical\ncompositional distributional) models where the meaning of a word is not a\ndiagram, but a diagram-valued higher-order function. Our models can be seen as\na variant of Montague semantics based on a lambda calculus where the primitives\nact on string diagrams rather than logical formulae. As a special case, we show\nhow to translate from the Lambek calculus into Peirce's system beta for\nfirst-order logic. This allows us to give a purely diagrammatic treatment of\nhigher-order and non-linear processes in natural language semantics: adverbs,\nprepositions, negation and quantifiers. The theoretical definition presented in\nthis article comes with a proof-of-concept implementation in DisCoPy, the\nPython library for string diagrams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toumi_A/0/1/0/all/0/1\">Alexis Toumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felice_G/0/1/0/all/0/1\">Giovanni de Felice</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning. (arXiv:2311.17842v1 [cs.RO])","link":"http://arxiv.org/abs/2311.17842","description":"<p>In this study, we are interested in imbuing robots with the capability of\nphysically-grounded task planning. Recent advancements have shown that large\nlanguage models (LLMs) possess extensive knowledge useful in robotic tasks,\nespecially in reasoning and planning. However, LLMs are constrained by their\nlack of world grounding and dependence on external affordance models to\nperceive environmental information, which cannot jointly reason with LLMs. We\nargue that a task planner should be an inherently grounded, unified multimodal\nsystem. To this end, we introduce Robotic Vision-Language Planning (ViLa), a\nnovel approach for long-horizon robotic planning that leverages vision-language\nmodels (VLMs) to generate a sequence of actionable steps. ViLa directly\nintegrates perceptual data into its reasoning and planning process, enabling a\nprofound understanding of commonsense knowledge in the visual world, including\nspatial layouts and object attributes. It also supports flexible multimodal\ngoal specification and naturally incorporates visual feedback. Our extensive\nevaluation, conducted in both real-robot and simulated environments,\ndemonstrates ViLa's superiority over existing LLM-based planners, highlighting\nits effectiveness in a wide array of open-world manipulation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yingdong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fanqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-turn Response Selection using Dialogue Dependency Relations. (arXiv:2010.01502v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.01502","description":"<p>Multi-turn response selection is a task designed for developing dialogue\nagents. The performance on this task has a remarkable improvement with\npre-trained language models. However, these models simply concatenate the turns\nin dialogue history as the input and largely ignore the dependencies between\nthe turns. In this paper, we propose a dialogue extraction algorithm to\ntransform a dialogue history into threads based on their dependency relations.\nEach thread can be regarded as a self-contained sub-dialogue. We also propose\nThread-Encoder model to encode threads and candidates into compact\nrepresentations by pre-trained Transformers and finally get the matching score\nthrough an attention layer. The experiments show that dependency relations are\nhelpful for dialogue context understanding, and our model outperforms the\nstate-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results\non UbuntuV2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Qi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yizhu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Siyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haifeng Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Glancing Transformer for Parallel Sequence to Sequence Learning. (arXiv:2212.10240v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10240","description":"<p>Previously, non-autoregressive models were widely perceived as being superior\nin generation efficiency but inferior in generation quality due to the\ndifficulties of modeling multiple target modalities. To enhance the\nmulti-modality modeling ability, we propose the diffusion glancing transformer,\nwhich employs a modality diffusion process and residual glancing sampling. The\nmodality diffusion process is a discrete process that interpolates the\nmulti-modal distribution along the decoding steps, and the residual glancing\nsampling approach guides the model to continuously learn the remaining\nmodalities across the layers. Experimental results on various machine\ntranslation and text generation benchmarks demonstrate that DIFFGLAT achieves\nbetter generation accuracy while maintaining fast decoding speed compared with\nboth autoregressive and non-autoregressive models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1\">Lihua Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Human-Like Translation Strategy with Large Language Models. (arXiv:2305.04118v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04118","description":"<p>Large language models (LLMs) have demonstrated impressive capabilities in\ngeneral scenarios, exhibiting a level of aptitude that approaches, in some\naspects even surpasses, human-level intelligence. Among their numerous skills,\nthe translation abilities of LLMs have received considerable attention.\nCompared to typical machine translation that focuses solely on source-to-target\nmapping, LLM-based translation can potentially mimic the human translation\nprocess which might take preparatory steps to ensure high-quality translation.\nThis work explores this possibility by proposing the MAPS framework, which\nstands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs\nfirst to analyze the given source sentence and induce three aspects of\ntranslation-related knowledge: keywords, topics, and relevant demonstrations to\nguide the final translation process. Moreover, we employ a selection mechanism\nbased on quality estimation to filter out noisy and unhelpful knowledge. Both\nautomatic (3 LLMs x 11 directions x 2 automatic metrics) and human evaluation\n(preference study and MQM) demonstrate the effectiveness of MAPS. Further\nanalysis shows that by mimicking the human translation process, MAPS reduces\nvarious translation errors such as hallucination, ambiguity, mistranslation,\nawkward style, untranslated text, and omission. Source code is available at\nhttps://github.com/zwhe99/MAPS-mt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models. (arXiv:2305.05189v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05189","description":"<p>Diffusion models, which have emerged to become popular text-to-image\ngeneration models, can produce high-quality and content-rich images guided by\ntextual prompts. However, there are limitations to semantic understanding and\ncommonsense reasoning in existing models when the input prompts are concise\nnarrative, resulting in low-quality image generation. To improve the capacities\nfor narrative prompts, we propose a simple-yet-effective parameter-efficient\nfine-tuning approach called the Semantic Understanding and Reasoning adapter\n(SUR-adapter) for pre-trained diffusion models. To reach this goal, we first\ncollect and annotate a new dataset SURD which consists of more than 57,000\nsemantically corrected multi-modal samples. Each sample contains a simple\nnarrative prompt, a complex keyword-based prompt, and a high-quality image.\nThen, we align the semantic representation of narrative prompts to the complex\nprompts and transfer knowledge of large language models (LLMs) to our\nSUR-adapter via knowledge distillation so that it can acquire the powerful\nsemantic understanding and reasoning capabilities to build a high-quality\ntextual semantic representation for text-to-image generation. We conduct\nexperiments by integrating multiple LLMs and popular pre-trained diffusion\nmodels to show the effectiveness of our approach in enabling diffusion models\nto understand and reason concise natural language without image quality\ndegradation. Our approach can make text-to-image diffusion models easier to use\nwith better user experience, which demonstrates our approach has the potential\nfor further advancing the development of user-friendly text-to-image generation\nmodels by bridging the semantic gap between simple narrative prompts and\ncomplex keyword-based prompts. The code is released at\nhttps://github.com/Qrange-group/SUR-adapter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Shanshan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1\">Wushao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jinghui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Sentence Transformers for the Aviation Domain. (arXiv:2305.09556v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.09556","description":"<p>Learning effective sentence representations is crucial for many Natural\nLanguage Processing (NLP) tasks, including semantic search, semantic textual\nsimilarity (STS), and clustering. While multiple transformer models have been\ndeveloped for sentence embedding learning, these models may not perform\noptimally when dealing with specialized domains like aviation, which has unique\ncharacteristics such as technical jargon, abbreviations, and unconventional\ngrammar. Furthermore, the absence of labeled datasets makes it difficult to\ntrain models specifically for the aviation domain. To address these challenges,\nwe propose a novel approach for adapting sentence transformers for the aviation\ndomain. Our method is a two-stage process consisting of pre-training followed\nby fine-tuning. During pre-training, we use Transformers and Sequential\nDenoising AutoEncoder (TSDAE) with aviation text data as input to improve the\ninitial model performance. Subsequently, we fine-tune our models using a\nNatural Language Inference (NLI) dataset in the Sentence Bidirectional Encoder\nRepresentations from Transformers (SBERT) architecture to mitigate overfitting\nissues. Experimental results on several downstream tasks show that our adapted\nsentence transformers significantly outperform general-purpose transformers,\ndemonstrating the effectiveness of our approach in capturing the nuances of the\naviation domain. Overall, our work highlights the importance of domain-specific\nadaptation in developing high-quality NLP solutions for specialized industries\nlike aviation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1\">Jason Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouck_D/0/1/0/all/0/1\">Dave Rouck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tien_A/0/1/0/all/0/1\">Alex Tien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_D/0/1/0/all/0/1\">Diane M Baumgartner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuLER: Detailed and Scalable Reference-based Evaluation. (arXiv:2305.14991v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14991","description":"<p>We propose a novel methodology (namely, MuLER) that transforms any\nreference-based evaluation metric for text generation, such as machine\ntranslation (MT) into a fine-grained analysis tool. Given a system and a\nmetric, MuLER quantifies how much the chosen metric penalizes specific error\ntypes (e.g., errors in translating names of locations). MuLER thus enables a\ndetailed error analysis which can lead to targeted improvement efforts for\nspecific phenomena. We perform experiments in both synthetic and naturalistic\nsettings to support MuLER's validity and showcase its usability in MT\nevaluation, and other tasks, such as summarization. Analyzing all submissions\nto WMT in 2014-2020, we find consistent trends. For example, nouns and verbs\nare among the most frequent POS tags. However, they are among the hardest to\ntranslate. Performance on most POS tags improves with overall system\nperformance, but a few are not thus correlated (their identity changes from\nlanguage to language). Preliminary experiments with summarization reveal\nsimilar trends.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karidi_T/0/1/0/all/0/1\">Taelin Karidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_G/0/1/0/all/0/1\">Gal Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Conceptual Representation Require Embodiment? Insights From Large Language Models. (arXiv:2305.19103v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19103","description":"<p>To what extent can language alone give rise to complex concepts, or is\nembodied experience essential? Recent advancements in large language models\n(LLMs) offer fresh perspectives on this question. Although LLMs are trained on\nrestricted modalities, they exhibit human-like performance in diverse\npsychological tasks. Our study compared representations of 4,442 lexical\nconcepts between humans and ChatGPTs (GPT-3.5 and GPT-4) across multiple\ndimensions, including five key domains: emotion, salience, mental\nvisualization, sensory, and motor experience. We identify two main findings: 1)\nBoth models strongly align with human representations in non-sensorimotor\ndomains but lag in sensory and motor areas, with GPT-4 outperforming GPT-3.5;\n2) GPT-4's gains are associated with its additional visual learning, which also\nappears to benefit related dimensions like haptics and imageability. These\nresults highlight the limitations of language in isolation, and that the\nintegration of diverse modalities of inputs leads to a more human-like\nconceptual representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qihui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yingying Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1\">Feng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chodorow_M/0/1/0/all/0/1\">Martin Chodorow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps. (arXiv:2307.05052v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.05052","description":"<p>We investigate the role of various demonstration components in the in-context\nlearning (ICL) performance of large language models (LLMs). Specifically, we\nexplore the impacts of ground-truth labels, input distribution, and\ncomplementary explanations, particularly when these are altered or perturbed.\nWe build on previous work, which offers mixed findings on how these elements\ninfluence ICL. To probe these questions, we employ explainable NLP (XNLP)\nmethods and utilize saliency maps of contrastive demonstrations for both\nqualitative and quantitative analysis. Our findings reveal that flipping\nground-truth labels significantly affects the saliency, though it's more\nnoticeable in larger LLMs. Our analysis of the input distribution at a granular\nlevel reveals that changing sentiment-indicative terms in a sentiment analysis\ntask to neutral ones does not have as substantial an impact as altering\nground-truth labels. Finally, we find that the effectiveness of complementary\nexplanations in boosting ICL performance is task-dependent, with limited\nbenefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.\nThese insights are critical for understanding the functionality of LLMs and\nguiding the development of effective demonstrations, which is increasingly\nrelevant in light of the growing use of LLMs in applications such as ChatGPT.\nOur research code is publicly available at https://github.com/paihengxu/XICL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Paiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fuxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hyemi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainability for Large Language Models: A Survey. (arXiv:2309.01029v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.01029","description":"<p>Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language processing. However, their internal mechanisms are still\nunclear and this lack of transparency poses unwanted risks for downstream\napplications. Therefore, understanding and explaining these models is crucial\nfor elucidating their behaviors, limitations, and social impacts. In this\npaper, we introduce a taxonomy of explainability techniques and provide a\nstructured overview of methods for explaining Transformer-based language\nmodels. We categorize techniques based on the training paradigms of LLMs:\ntraditional fine-tuning-based paradigm and prompting-based paradigm. For each\nparadigm, we summarize the goals and dominant approaches for generating local\nexplanations of individual predictions and global explanations of overall model\nknowledge. We also discuss metrics for evaluating generated explanations, and\ndiscuss how explanations can be leveraged to debug models and improve\nperformance. Lastly, we examine key challenges and emerging opportunities for\nexplanation techniques in the era of LLMs in comparison to conventional machine\nlearning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Huiqi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hengyi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuaiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Mengnan Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Separate Normalization in Self-supervised Transformers. (arXiv:2309.12931v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.12931","description":"<p>Self-supervised training methods for transformers have demonstrated\nremarkable performance across various domains. Previous transformer-based\nmodels, such as masked autoencoders (MAE), typically utilize a single\nnormalization layer for both the [CLS] symbol and the tokens. We propose in\nthis paper a simple modification that employs separate normalization layers for\nthe tokens and the [CLS] symbol to better capture their distinct\ncharacteristics and enhance downstream task performance. Our method aims to\nalleviate the potential negative effects of using the same normalization\nstatistics for both token types, which may not be optimally aligned with their\nindividual roles. We empirically show that by utilizing a separate\nnormalization layer, the [CLS] embeddings can better encode the global\ncontextual information and are distributed more uniformly in its anisotropic\nspace. When replacing the conventional normalization layer with the two\nseparate layers, we observe an average 2.7% performance improvement over the\nimage, natural language, and graph domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yinkai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuanqi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassoun_S/0/1/0/all/0/1\">Soha Hassoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li-Ping Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arabic Sentiment Analysis with Noisy Deep Explainable Model. (arXiv:2309.13731v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.13731","description":"<p>Sentiment Analysis (SA) is an indispensable task for many real-world\napplications. Compared to limited resourced languages (i.e., Arabic, Bengali),\nmost of the research on SA are conducted for high resourced languages (i.e.,\nEnglish, Chinese). Moreover, the reasons behind any prediction of the Arabic\nsentiment analysis methods exploiting advanced artificial intelligence\n(AI)-based approaches are like black-box - quite difficult to understand. This\npaper proposes an explainable sentiment classification framework for the Arabic\nlanguage by introducing a noise layer on Bi-Directional Long Short-Term Memory\n(BiLSTM) and Convolutional Neural Networks (CNN)-BiLSTM models that overcome\nover-fitting problem. The proposed framework can explain specific predictions\nby training a local surrogate explainable model to understand why a particular\nsentiment (positive or negative) is being predicted. We carried out experiments\non public benchmark Arabic SA datasets. The results concluded that adding noise\nlayers improves the performance in sentiment analysis for the Arabic language\nby reducing overfitting and our method outperformed some known state-of-the-art\nmethods. In addition, the introduced explainability with noise layer could make\nthe model more transparent and accountable and hence help adopting AI-enabled\nsystem in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atabuzzaman_M/0/1/0/all/0/1\">Md. Atabuzzaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shajalal_M/0/1/0/all/0/1\">Md Shajalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baby_M/0/1/0/all/0/1\">Maksuda Bilkis Baby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boden_A/0/1/0/all/0/1\">Alexander Boden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention. (arXiv:2309.14327v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2309.14327","description":"<p>Most of the existing multi-modal models, hindered by their incapacity to\nadeptly manage interleaved image-and-text inputs in multi-image, multi-round\ndialogues, face substantial constraints in resource allocation for training and\ndata accessibility, impacting their adaptability and scalability across varied\ninteraction realms. To address this, we present the DeepSpeed-VisualChat\nframework, designed to optimize Large Language Models (LLMs) by incorporating\nmulti-modal capabilities, with a focus on enhancing the proficiency of Large\nVision and Language Models in handling interleaved inputs. Our framework is\nnotable for (1) its open-source support for multi-round and multi-image\ndialogues, (2) introducing an innovative multi-modal causal attention\nmechanism, and (3) utilizing data blending techniques on existing datasets to\nassure seamless interactions in multi-round, multi-image conversations.\nCompared to existing frameworks, DeepSpeed-VisualChat shows superior\nscalability up to 70B parameter language model size, representing a significant\nadvancement in multi-modal language models and setting a solid foundation for\nfuture explorations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Conglong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Heyang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruwase_O/0/1/0/all/0/1\">Olatunji Ruwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awan_A/0/1/0/all/0/1\">Ammar Ahmad Awan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1\">Samyam Rajbhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.01929","description":"<p>Text-To-Image (TTI) models, such as DALL-E and StableDiffusion, have\ndemonstrated remarkable prompt-based image generation capabilities.\nMultilingual encoders may have a substantial impact on the cultural agency of\nthese models, as language is a conduit of culture. In this study, we explore\nthe cultural perception embedded in TTI models by characterizing culture across\nthree hierarchical tiers: cultural dimensions, cultural domains, and cultural\nconcepts. Based on this ontology, we derive prompt templates to unlock the\ncultural knowledge in TTI models, and propose a comprehensive suite of\nevaluation techniques, including intrinsic evaluations using the CLIP space,\nextrinsic evaluations with a Visual-Question-Answer (VQA) model and human\nassessments, to evaluate the cultural content of TTI-generated images. To\nbolster our research, we introduce the CulText2I dataset, derived from four\ndiverse TTI models and spanning ten languages. Our experiments provide insights\nregarding Do, What, Which and How research questions about the nature of\ncultural encoding in TTI models, paving the way for cross-cultural applications\nof these models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ventura_M/0/1/0/all/0/1\">Mor Ventura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1\">Eyal Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback. (arXiv:2310.05199v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05199","description":"<p>Reinforcement learning from human feedback serves as a crucial bridge,\naligning large language models with human and societal values. This alignment\nrequires a vast corpus of human feedback to learn a reward model, which is\nsubsequently used to finetune language models. However, we have identified that\nthe reward model often finds shortcuts to bypass its intended objectives,\nmisleadingly assuming that humans prefer longer responses. The emergence of\nlength bias often induces the model to favor longer outputs, yet it doesn't\nequate to an increase in helpful information within these outputs. In this\npaper, we propose an innovative solution, applying the Product-of-Experts (PoE)\ntechnique to separate reward modeling from the influence of sequence length. In\nour framework, the main expert concentrates on understanding human intents,\nwhile the biased expert targets the identification and capture of length bias.\nTo further enhance the learning of bias, we introduce perturbations into the\nbias-focused expert, disrupting the flow of semantic information. Experimental\nresults validate the effectiveness of our approach, indicating that language\nmodel performance is improved, irrespective of sequence length.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Wenyu Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shihan Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Attribution Method for Siamese Encoders. (arXiv:2310.05703v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05703","description":"<p>Despite the success of Siamese encoder models such as sentence transformers\n(ST), little is known about the aspects of inputs they pay attention to. A\nbarrier is that their predictions cannot be attributed to individual features,\nas they compare two inputs rather than processing a single one. This paper\nderives a local attribution method for Siamese encoders by generalizing the\nprinciple of integrated gradients to models with multiple inputs. The solution\ntakes the form of feature-pair attributions, and can be reduced to a\ntoken-token matrix for STs. Our method involves the introduction of integrated\nJacobians and inherits the advantageous formal properties of integrated\ngradients: it accounts for the model's full computation graph and is guaranteed\nto converge to the actual prediction. A pilot study shows that in an ST few\ntoken-pairs can often explain large fractions of predictions, and it focuses on\nnouns and verbs. For accurate predictions, it however needs to attend to the\nmajority of tokens and parts of speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moller_L/0/1/0/all/0/1\">Lucas M&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1\">Dmitry Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models. (arXiv:2310.09949v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2310.09949","description":"<p>A Retrieval-Augmented Language Model (RALM) augments a generative language\nmodel by retrieving context-specific knowledge from an external database. This\nstrategy facilitates impressive text generation quality even with smaller\nmodels, thus reducing orders of magnitude of computational demands. However,\nRALMs introduce unique system design challenges due to (a) the diverse workload\ncharacteristics between LM inference and retrieval and (b) the various system\nrequirements and bottlenecks for different RALM configurations such as model\nsizes, database sizes, and retrieval frequencies. We propose Chameleon, a\nheterogeneous accelerator system that integrates both LM and retrieval\naccelerators in a disaggregated architecture. The heterogeneity ensures\nefficient acceleration of both LM inference and retrieval, while the\naccelerator disaggregation enables the system to independently scale both types\nof accelerators to fulfill diverse RALM requirements. Our Chameleon prototype\nimplements retrieval accelerators on FPGAs and assigns LM inference to GPUs,\nwith a CPU server orchestrating these accelerators over the network. Compared\nto CPU-based and CPU-GPU vector search systems, Chameleon achieves up to 23.72x\nspeedup and 26.2x energy efficiency. Evaluated on various RALMs, Chameleon\nexhibits up to 2.16x reduction in latency and 3.18x speedup in throughput\ncompared to the hybrid CPU-GPU architecture. These promising results pave the\nway for bringing accelerator heterogeneity and disaggregation into future RALM\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeller_M/0/1/0/all/0/1\">Marco Zeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waleffe_R/0/1/0/all/0/1\">Roger Waleffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1\">Torsten Hoefler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_G/0/1/0/all/0/1\">Gustavo Alonso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination & Visual Illusion in Large Vision-Language Models. (arXiv:2310.14566v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2310.14566","description":"<p>We introduce HallusionBench, a comprehensive benchmark designed for the\nevaluation of image-context reasoning. This benchmark presents significant\nchallenges to advanced large visual-language models (LVLMs), such as\nGPT-4V(Vision) and LLaVA-1.5, by emphasizing nuanced understanding and\ninterpretation of visual data. The benchmark comprises 346 images paired with\n1129 questions, all meticulously crafted by human experts. We introduce a novel\nstructure for these visual questions designed to establish control groups. This\nstructure enables us to conduct a quantitative analysis of the models' response\ntendencies, logical consistency, and various failure modes. In our evaluation\non HallusionBench, we benchmarked 13 different models, highlighting a 31.42%\nquestion-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all\nother evaluated models achieve accuracy below 16%. Moreover, our analysis not\nonly highlights the observed failure modes, including language hallucination\nand visual illusion, but also deepens an understanding of these pitfalls. Our\ncomprehensive case studies within HallusionBench shed light on the challenges\nof hallucination and illusion in LVLMs. Based on these insights, we suggest\npotential pathways for their future improvement. The benchmark and codebase can\nbe accessed at https://github.com/tianyi-lab/HallusionBench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_T/0/1/0/all/0/1\">Tianrui Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fuxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_R/0/1/0/all/0/1\">Ruiqi Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xijun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lichang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yacoob_Y/0/1/0/all/0/1\">Yaser Yacoob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment Analysis. (arXiv:2310.18023v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.18023","description":"<p>Code-mixing is a well-studied linguistic phenomenon when two or more\nlanguages are mixed in text or speech. Several datasets have been build with\nthe goal of training computational models for code-mixing. Although it is very\ncommon to observe code-mixing with multiple languages, most datasets available\ncontain code-mixed between only two languages. In this paper, we introduce\nSentMix-3L, a novel dataset for sentiment analysis containing code-mixed data\nbetween three languages Bangla, English, and Hindi. We carry out a\ncomprehensive evaluation using SentMix-3L. We show that zero-shot prompting\nwith GPT-3.5 outperforms all transformer-based models on SentMix-3L.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raihan_M/0/1/0/all/0/1\">Md Nishat Raihan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_D/0/1/0/all/0/1\">Dhiman Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_A/0/1/0/all/0/1\">Antara Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meaning Representations from Trajectories in Autoregressive Models. (arXiv:2310.18348v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.18348","description":"<p>We propose to extract meaning representations from autoregressive language\nmodels by considering the distribution of all possible trajectories extending\nan input text. This strategy is prompt-free, does not require fine-tuning, and\nis applicable to any pre-trained autoregressive model. Moreover, unlike\nvector-based representations, distribution-based representations can also model\nasymmetric relations (e.g., direction of logical entailment, hypernym/hyponym\nrelations) by using algebraic operations between likelihood functions. These\nideas are grounded in distributional perspectives on semantics and are\nconnected to standard constructions in automata theory, but to our knowledge\nthey have not been applied to modern language models. We empirically show that\nthe representations obtained from large models align well with human\nannotations, outperform other zero-shot and prompt-free methods on semantic\nsimilarity tasks, and can be used to solve more complex entailment and\ncontainment tasks that standard embeddings cannot handle. Finally, we extend\nour method to represent data from different modalities (e.g., image and text)\nusing multimodal autoregressive models. Our code is available at:\nhttps://github.com/tianyu139/meaning-as-trajectories\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tian Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1\">Matthew Trager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1\">Pramuditha Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1\">Luca Zancato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers. (arXiv:2311.10642v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.10642","description":"<p>This work presents an analysis of the effectiveness of using standard shallow\nfeed-forward networks to mimic the behavior of the attention mechanism in the\noriginal Transformer model, a state-of-the-art architecture for\nsequence-to-sequence tasks. We substitute key elements of the attention\nmechanism in the Transformer with simple feed-forward networks, trained using\nthe original components via knowledge distillation. Our experiments, conducted\non the IWSLT2017 dataset, reveal the capacity of these \"attentionless\nTransformers\" to rival the performance of the original architecture. Through\nrigorous ablation studies, and experimenting with various replacement network\ntypes and sizes, we offer insights that support the viability of our approach.\nThis not only sheds light on the adaptability of shallow feed-forward networks\nin emulating attention mechanisms but also underscores their potential to\nstreamline complex architectures for sequence-to-sequence tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bozic_V/0/1/0/all/0/1\">Vukasin Bozic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dordevic_D/0/1/0/all/0/1\">Danilo Dordevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coppola_D/0/1/0/all/0/1\">Daniele Coppola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thommes_J/0/1/0/all/0/1\">Joseph Thommes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sidak Pal Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Language Agent for Autonomous Driving. (arXiv:2311.10813v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2311.10813","description":"<p>Human-level driving is an ultimate goal of autonomous driving. Conventional\napproaches formulate autonomous driving as a perception-prediction-planning\nframework, yet their systems do not capitalize on the inherent reasoning\nability and experiential knowledge of humans. In this paper, we propose a\nfundamental paradigm shift from current pipelines, exploiting Large Language\nModels (LLMs) as a cognitive agent to integrate human-like intelligence into\nautonomous driving systems. Our approach, termed Agent-Driver, transforms the\ntraditional autonomous driving pipeline by introducing a versatile tool library\naccessible via function calls, a cognitive memory of common sense and\nexperiential knowledge for decision-making, and a reasoning engine capable of\nchain-of-thought reasoning, task planning, motion planning, and\nself-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive\ncommon sense and robust reasoning capabilities, thus enabling a more nuanced,\nhuman-like approach to autonomous driving. We evaluate our approach on the\nlarge-scale nuScenes benchmark, and extensive experiments substantiate that our\nAgent-Driver significantly outperforms the state-of-the-art driving methods by\na large margin. Our approach also demonstrates superior interpretability and\nfew-shot learning ability to these methods. Code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiageng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuxi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LM-Cocktail: Resilient Tuning of Language Models via Model Merging. (arXiv:2311.13534v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.13534","description":"<p>The pre-trained language models are continually fine-tuned to better support\ndownstream applications. However, this operation may result in significant\nperformance degeneration on general tasks beyond the targeted domain. To\novercome this problem, we propose LM-Cocktail which enables the fine-tuned\nmodel to stay resilient in general perspectives. Our method is conducted in the\nform of model merging, where the fine-tuned language model is merged with the\npre-trained base model or the peer models from other domains through weighted\naverage. Despite simplicity, LM-Cocktail is surprisingly effective: the\nresulted model is able to achieve a strong empirical performance in the whole\nscope of general tasks while preserving a superior capacity in its targeted\ndomain. We conduct comprehensive experiments with LLama and BGE model on\npopular benchmarks, including FLAN, MMLU, MTEB, whose results validate the\nefficacy of our proposed method. The code and checkpoints are available at\nhttps://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peitian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1\">Xingrun Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.14743","description":"<p>Foundation models, specifically Large Language Models (LLM's), have lately\ngained wide-spread attention and adoption. Reinforcement Learning with Human\nFeedback (RLHF) involves training a reward model to capture desired behaviors,\nwhich is then used to align an LLM. These reward models are additionally used\nat inference-time to estimate how well LLM responses adhere to those desired\nbehaviors. However, there is little work measuring how robust these reward\nmodels are to distribution shifts. In this work, we evaluate how reward model\nperformance - measured via accuracy and calibration (i.e. alignment between\naccuracy and confidence) - is affected by distribution shift. We show novel\ncalibration patterns and accuracy drops due to OOD prompts and responses, and\nthat the reward model is more sensitive to shifts in responses than prompts.\nAdditionally, we adapt an OOD detection technique commonly used in\nclassification to the reward model setting in order to detect these\ndistribution shifts in prompts and responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1\">Ben Pikus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1\">Will LeVine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tony Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1\">Sean Hendryx</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatTraffic: Text-to-Traffic Generation via Diffusion Model. (arXiv:2311.16203v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.16203","description":"<p>Traffic prediction is one of the most significant foundations in Intelligent\nTransportation Systems (ITS). Traditional traffic prediction methods rely only\non historical traffic data to predict traffic trends and face two main\nchallenges. 1) insensitivity to unusual events. 2) poor performance in\nlong-term prediction. In this work, we explore how generative models combined\nwith text describing the traffic system can be applied for traffic generation\nand name the task Text-to-Traffic Generation (TTG). The key challenge of the\nTTG task is how to associate text with the spatial structure of the road\nnetwork and traffic data for generating traffic situations. To this end, we\npropose ChatTraffic, the first diffusion model for text-to-traffic generation.\nTo guarantee the consistency between synthetic and real data, we augment a\ndiffusion model with the Graph Convolutional Network (GCN) to extract spatial\ncorrelations of traffic data. In addition, we construct a large dataset\ncontaining text-traffic pairs for the TTG task. We benchmarked our model\nqualitatively and quantitatively on the released dataset. The experimental\nresults indicate that ChatTraffic can generate realistic traffic situations\nfrom the text. Our code and dataset are available at\nhttps://github.com/ChyaZhang/ChatTraffic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Q/0/1/0/all/0/1\">Qitan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yisheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piao_X/0/1/0/all/0/1\">Xinglin Piao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities Using Web Instructional Videos. (arXiv:2311.16444v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2311.16444","description":"<p>We propose a novel benchmark for cross-view knowledge transfer of dense video\ncaptioning, adapting models from web instructional videos with exocentric views\nto an egocentric view. While dense video captioning (predicting time segments\nand their captions) is primarily studied with exocentric videos (e.g.,\nYouCook2), benchmarks with egocentric videos are restricted due to data\nscarcity. To overcome the limited video availability, transferring knowledge\nfrom abundant exocentric web videos is demanded as a practical approach.\nHowever, learning the correspondence between exocentric and egocentric views is\ndifficult due to their dynamic view changes. The web videos contain mixed views\nfocusing on either human body actions or close-up hand-object interactions,\nwhile the egocentric view is constantly shifting as the camera wearer moves.\nThis necessitates the in-depth study of cross-view transfer under complex view\nchanges. In this work, we first create a real-life egocentric dataset (EgoYC2)\nwhose captions are shared with YouCook2, enabling transfer learning between\nthese datasets assuming their ground-truth is accessible. To bridge the view\ngaps, we propose a view-invariant learning method using adversarial training in\nboth the pre-training and fine-tuning stages. While the pre-training is\ndesigned to learn invariant features against the mixed views in the web videos,\nthe view-invariant fine-tuning further mitigates the view gaps between both\ndatasets. We validate our proposed method by studying how effectively it\novercomes the view change problem and efficiently transfers the knowledge to\nthe egocentric domain. Our benchmark pushes the study of the cross-view\ntransfer into a new task domain of dense video captioning and will envision\nmethodologies to describe egocentric videos in natural language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohkawa_T/0/1/0/all/0/1\">Takehiko Ohkawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yagi_T/0/1/0/all/0/1\">Takuma Yagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_T/0/1/0/all/0/1\">Taichi Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1\">Ryosuke Furuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_A/0/1/0/all/0/1\">Atsushi Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1\">Yoshitaka Ushiku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?. (arXiv:2311.16989v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.16989","description":"<p>Upon its release in late 2022, ChatGPT has brought a seismic shift in the\nentire landscape of AI, both in research and commerce. Through\ninstruction-tuning a large language model (LLM) with supervised fine-tuning and\nreinforcement learning from human feedback, it showed that a model could answer\nhuman questions and follow instructions on a broad panel of tasks. Following\nthis success, interests in LLMs have intensified, with new LLMs flourishing at\nfrequent interval across academia and industry, including many start-ups\nfocused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's\nClaude) generally outperform their open-source counterparts, the progress on\nthe latter has been rapid with claims of achieving parity or even better on\ncertain tasks. This has crucial implications not only on research but also on\nbusiness. In this work, on the first anniversary of ChatGPT, we provide an\nexhaustive overview of this success, surveying all tasks where an open-source\nLLM has claimed to be on par or better than ChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hailin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_F/0/1/0/all/0/1\">Fangkai Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chengwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravaut_M/0/1/0/all/0/1\">Mathieu Ravaut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruochen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient In-Context Learning in Vision-Language Models for Egocentric Videos. (arXiv:2311.17041v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2311.17041","description":"<p>Recent advancements in text-only large language models (LLMs) have\nhighlighted the benefit of in-context learning for adapting to new tasks with a\nfew demonstrations. However, extending in-context learning to large\nvision-language models (VLMs) using a huge amount of naturalistic\nvision-language data has shown limited success, particularly for egocentric\nvideos, due to high data collection costs. We propose a novel training method\n$\\mathbb{E}$fficient $\\mathbb{I}$n-context $\\mathbb{L}$earning on\n$\\mathbb{E}$gocentric $\\mathbb{V}$ideos ($\\mathbb{EILEV}$), which elicits\nin-context learning in VLMs for egocentric videos without requiring massive,\nnaturalistic egocentric video datasets. $\\mathbb{EILEV}$ involves architectural\nand training data adaptations to allow the model to process contexts\ninterleaved with video clips and narrations, sampling of in-context examples\nwith clusters of similar verbs and nouns, use of data with skewed marginal\ndistributions with a long tail of infrequent verbs and nouns, as well as\nhomonyms and synonyms. Our evaluations show that $\\mathbb{EILEV}$-trained\nmodels outperform larger VLMs trained on a huge amount of naturalistic data in\nin-context learning. Furthermore, they can generalize to not only\nout-of-distribution, but also novel, rare egocentric videos and texts via\nin-context learning, demonstrating potential for applications requiring\ncost-effective training, and rapid post-deployment adaptability. Our code and\ndemo are available at \\url{https://github.com/yukw777/EILEV}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Keunwoo Peter Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fengyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-11-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}