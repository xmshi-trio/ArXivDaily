{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-07-25T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11760","description":"<p>Large language models (LLMs) have achieved significant performance in many\nfields such as reasoning, language understanding, and math problem-solving, and\nare regarded as a crucial step to artificial general intelligence (AGI).\nHowever, the sensitivity of LLMs to prompts remains a major bottleneck for\ntheir daily adoption. In this paper, we take inspiration from psychology and\npropose EmotionPrompt to explore emotional intelligence to enhance the\nperformance of LLMs. EmotionPrompt operates on a remarkably straightforward\nprinciple: the incorporation of emotional stimulus into prompts. Experimental\nresults demonstrate that our \\method, using the same single prompt templates,\nsignificantly outperforms original zero-shot prompt and Zero-shot-CoT on 8\ntasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further,\nEmotionPrompt was observed to improve both truthfulness and informativeness. We\nbelieve that EmotionPrompt heralds a novel avenue for exploring\ninterdisciplinary knowledge for humans-LLMs interaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kaijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1\">Jianxun Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness of ChatGPT and the Role Of Explainable-Guided Prompts. (arXiv:2307.11761v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11761","description":"<p>Our research investigates the potential of Large-scale Language Models\n(LLMs), specifically OpenAI's GPT, in credit risk assessment-a binary\nclassification task. Our findings suggest that LLMs, when directed by\njudiciously designed prompts and supplemented with domain-specific knowledge,\ncan parallel the performance of traditional Machine Learning (ML) models.\nIntriguingly, they achieve this with significantly less data-40 times less,\nutilizing merely 20 data points compared to the ML's 800. LLMs particularly\nexcel in minimizing false positives and enhancing fairness, both being vital\naspects of risk analysis. While our results did not surpass those of classical\nML models, they underscore the potential of LLMs in analogous tasks, laying a\ngroundwork for future explorations into harnessing the capabilities of LLMs in\ndiverse ML tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deldjoo_Y/0/1/0/all/0/1\">Yashar Deldjoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity-based Memory Enhanced Joint Entity and Relation Extraction. (arXiv:2307.11762v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11762","description":"<p>Document-level joint entity and relation extraction is a challenging\ninformation extraction problem that requires a unified approach where a single\nneural network performs four sub-tasks: mention detection, coreference\nresolution, entity classification, and relation extraction. Existing methods\noften utilize a sequential multi-task learning approach, in which the arbitral\ndecomposition causes the current task to depend only on the previous one,\nmissing the possible existence of the more complex relationships between them.\nIn this paper, we present a multi-task learning framework with bidirectional\nmemory-like dependency between tasks to address those drawbacks and perform the\njoint problem more accurately. Our empirical studies show that the proposed\napproach outperforms the existing methods and achieves state-of-the-art results\non the BioCreative V CDR corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kosciukiewicz_W/0/1/0/all/0/1\">Witold Kosciukiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wojcik_M/0/1/0/all/0/1\">Mateusz Wojcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajdanowicz_T/0/1/0/all/0/1\">Tomasz Kajdanowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonczarek_A/0/1/0/all/0/1\">Adam Gonczarek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensi-BERT: Towards Sensitivity Driven Fine-Tuning for Parameter-Efficient BERT. (arXiv:2307.11764v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11764","description":"<p>Large pre-trained language models have recently gained significant traction\ndue to their improved performance on various down-stream tasks like text\nclassification and question answering, requiring only few epochs of\nfine-tuning. However, their large model sizes often prohibit their applications\non resource-constrained edge devices. Existing solutions of yielding\nparameter-efficient BERT models largely rely on compute-exhaustive training and\nfine-tuning. Moreover, they often rely on additional compute heavy models to\nmitigate the performance gap. In this paper, we present Sensi-BERT, a\nsensitivity driven efficient fine-tuning of BERT models that can take an\noff-the-shelf pre-trained BERT model and yield highly parameter-efficient\nmodels for downstream tasks. In particular, we perform sensitivity analysis to\nrank each individual parameter tensor, that then is used to trim them\naccordingly during fine-tuning for a given parameter or FLOPs budget. Our\nexperiments show the efficacy of Sensi-BERT across different downstream tasks\nincluding MNLI, QQP, QNLI, and SST-2, demonstrating better performance at\nsimilar or smaller parameter budget compared to various existing alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Souvik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nittur_S/0/1/0/all/0/1\">Sharath Sridhar Nittur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szankin_M/0/1/0/all/0/1\">Maciej Szankin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_S/0/1/0/all/0/1\">Sairam Sundaresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Three-way Decisions with Evaluative Linguistic Expressions. (arXiv:2307.11766v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11766","description":"<p>We propose a linguistic interpretation of three-way decisions, where the\nregions of acceptance, rejection, and non-commitment are constructed by using\nthe so-called evaluative linguistic expressions, which are expressions of\nnatural language such as small, medium, very short, quite roughly strong,\nextremely good, etc. Our results highlight new connections between two\ndifferent research areas: three-way decisions and the theory of evaluative\nlinguistic expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boffa_S/0/1/0/all/0/1\">Stefania Boffa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciucci_D/0/1/0/all/0/1\">Davide Ciucci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognition of Mental Adjectives in An Efficient and Automatic Style. (arXiv:2307.11767v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11767","description":"<p>In recent years, commonsense reasoning has received more and more attention\nfrom academic community. We propose a new lexical inference task, Mental and\nPhysical Classification (MPC), to handle commonsense reasoning in a reasoning\ngraph. Mental words relate to mental activities, which fall into six\ncategories: Emotion, Need, Perceiving, Reasoning, Planning and Personality.\nPhysical words describe physical attributes of an object, like color, hardness,\nspeed and malleability. A BERT model is fine-tuned for this task and active\nlearning algorithm is adopted in the training framework to reduce the required\nannotation resources. The model using ENTROPY strategy achieves satisfactory\naccuracy and requires only about 300 labeled words. We also compare our result\nwith SentiWordNet to check the difference between MPC and subjectivity\nclassification task in sentiment analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question Decomposition Improves the Faithfulness of Model-Generated Reasoning. (arXiv:2307.11768v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11768","description":"<p>As large language models (LLMs) perform more difficult tasks, it becomes\nharder to verify the correctness and safety of their behavior. One approach to\nhelp with this issue is to prompt LLMs to externalize their reasoning, e.g., by\nhaving them generate step-by-step reasoning as they answer a question\n(Chain-of-Thought; CoT). The reasoning may enable us to check the process that\nmodels use to perform tasks. However, this approach relies on the stated\nreasoning faithfully reflecting the model's actual reasoning, which is not\nalways the case. To improve over the faithfulness of CoT reasoning, we have\nmodels generate reasoning by decomposing questions into subquestions.\nDecomposition-based methods achieve strong performance on question-answering\ntasks, sometimes approaching that of CoT while improving the faithfulness of\nthe model's stated reasoning on several recently-proposed metrics. By forcing\nthe model to answer simpler subquestions in separate contexts, we greatly\nincrease the faithfulness of model-generated reasoning over CoT, while still\nachieving some of the performance gains of CoT. Our results show it is possible\nto improve the faithfulness of model-generated reasoning; continued\nimprovements may lead to reasoning that enables us to verify the correctness\nand safety of LLM behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1\">Ansh Radhakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Karina Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anna Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Carol Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denison_C/0/1/0/all/0/1\">Carson Denison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1\">Danny Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1\">Evan Hubinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kernion_J/0/1/0/all/0/1\">Jackson Kernion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukosiute_K/0/1/0/all/0/1\">Kamil&#x117; Luko&#x161;i&#x16b;t&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Newton Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1\">Nicholas Schiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rausch_O/0/1/0/all/0/1\">Oliver Rausch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1\">Sam McCandlish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Showk_S/0/1/0/all/0/1\">Sheer El Showk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanham_T/0/1/0/all/0/1\">Tamera Lanham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maxwell_T/0/1/0/all/0/1\">Tim Maxwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1\">Venkatesa Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatfield_Dodds_Z/0/1/0/all/0/1\">Zac Hatfield-Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1\">Jan Brauner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain. (arXiv:2307.11769v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11769","description":"<p>Engineering knowledge-based (or expert) systems require extensive manual\neffort and domain knowledge. As Large Language Models (LLMs) are trained using\nan enormous amount of cross-domain knowledge, it becomes possible to automate\nsuch engineering processes. This paper presents an empirical automation and\nsemi-automation framework for domain knowledge distillation using prompt\nengineering and the LLM ChatGPT. We assess the framework empirically in the\nautonomous driving domain and present our key observations. In our\nimplementation, we construct the domain knowledge ontology by \"chatting\" with\nChatGPT. The key finding is that while fully automated domain ontology\nconstruction is possible, human supervision and early intervention typically\nimprove efficiency and output quality as they lessen the effects of response\nrandomness and the butterfly effect. We, therefore, also develop a web-based\ndistillation assistant enabling supervision and flexible intervention at\nruntime. We hope our findings and tools could inspire future research toward\nrevolutionizing the engineering of knowledge-based systems across application\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1\">Antonio A. Bruto da Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jason Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patrick_I/0/1/0/all/0/1\">Irvine Patrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khastgir_S/0/1/0/all/0/1\">Siddartha Khastgir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jennings_P/0/1/0/all/0/1\">Paul Jennings</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Evaluation of Topic Models and Dimensionality Reduction Methods for 2D Text Spatialization. (arXiv:2307.11770v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11770","description":"<p>Topic models are a class of unsupervised learning algorithms for detecting\nthe semantic structure within a text corpus. Together with a subsequent\ndimensionality reduction algorithm, topic models can be used for deriving\nspatializations for text corpora as two-dimensional scatter plots, reflecting\nsemantic similarity between the documents and supporting corpus analysis.\nAlthough the choice of the topic model, the dimensionality reduction, and their\nunderlying hyperparameters significantly impact the resulting layout, it is\nunknown which particular combinations result in high-quality layouts with\nrespect to accuracy and perception metrics. To investigate the effectiveness of\ntopic models and dimensionality reduction methods for the spatialization of\ncorpora as two-dimensional scatter plots (or basis for landscape-type\nvisualizations), we present a large-scale, benchmark-based computational\nevaluation. Our evaluation consists of (1) a set of corpora, (2) a set of\nlayout algorithms that are combinations of topic models and dimensionality\nreductions, and (3) quality metrics for quantifying the resulting layout. The\ncorpora are given as document-term matrices, and each document is assigned to a\nthematic class. The chosen metrics quantify the preservation of local and\nglobal properties and the perceptual effectiveness of the two-dimensional\nscatter plots. By evaluating the benchmark on a computing cluster, we derived a\nmultivariate dataset with over 45 000 individual layouts and corresponding\nquality metrics. Based on the results, we propose guidelines for the effective\ndesign of text spatializations that are based on topic models and\ndimensionality reductions. As a main result, we show that interpretable topic\nmodels are beneficial for capturing the structure of text corpora. We\nfurthermore recommend the use of t-SNE as a subsequent dimensionality\nreduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atzberger_D/0/1/0/all/0/1\">Daniel Atzberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cech_T/0/1/0/all/0/1\">Tim Cech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheibel_W/0/1/0/all/0/1\">Willy Scheibel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1\">Matthias Trapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_R/0/1/0/all/0/1\">Rico Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dollner_J/0/1/0/all/0/1\">J&#xfc;rgen D&#xf6;llner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schreck_T/0/1/0/all/0/1\">Tobias Schreck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"an integrated npl approach to sentiment analysis in satisfaction surveys. (arXiv:2307.11771v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11771","description":"<p>The research project aims to apply an integrated approach to natural language\nprocessing NLP to satisfaction surveys. It will focus on understanding and\nextracting relevant information from survey responses, analyzing feelings, and\nidentifying recurring word patterns. NLP techniques will be used to determine\nemotional polarity, classify responses into positive, negative, or neutral\ncategories, and use opinion mining to highlight participants opinions. This\napproach will help identify the most relevant aspects for participants and\nunderstand their opinions in relation to those specific aspects. A key\ncomponent of the research project will be the analysis of word patterns in\nsatisfaction survey responses using NPL. This analysis will provide a deeper\nunderstanding of feelings, opinions, and themes and trends present in\nrespondents responses. The results obtained from this approach can be used to\nidentify areas for improvement, understand respondents preferences, and make\nstrategic decisions based on analysis to improve respondent satisfaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pinto_Luque_E/0/1/0/all/0/1\">Edson B. Pinto-Luque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v1 [cs.IR])","link":"http://arxiv.org/abs/2307.11772","description":"<p>The task of entity alignment between knowledge graphs (KGs) aims to identify\nevery pair of entities from two different KGs that represent the same entity.\nMany machine learning-based methods have been proposed for this task. However,\nto our best knowledge, existing methods all require manually crafted seed\nalignments, which are expensive to obtain. In this paper, we propose the first\nfully automatic alignment method named AutoAlign, which does not require any\nmanually crafted seed alignments. Specifically, for predicate embeddings,\nAutoAlign constructs a predicate-proximity-graph with the help of large\nlanguage models to automatically capture the similarity between predicates\nacross two KGs. For entity embeddings, AutoAlign first computes the entity\nembeddings of each KG independently using TransE, and then shifts the two KGs'\nentity embeddings into the same vector space by computing the similarity\nbetween entities based on their attributes. Thus, both predicate alignment and\nentity alignment can be done without manually crafted seed alignments.\nAutoAlign is not only fully automatic, but also highly effective. Experiments\nusing real-world KGs show that AutoAlign improves the performance of entity\nalignment significantly compared to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trisedya_B/0/1/0/all/0/1\">Bayu Distiawan Trisedya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jianzhong Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Topical Approach to Capturing Customer Insight In Social Media. (arXiv:2307.11775v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11775","description":"<p>The age of social media has opened new opportunities for businesses. This\nflourishing wealth of information is outside traditional channels and\nframeworks of classical marketing research, including that of Marketing Mix\nModeling (MMM). Textual data, in particular, poses many challenges that data\nanalysis practitioners must tackle. Social media constitute massive,\nheterogeneous, and noisy document sources. Industrial data acquisition\nprocesses include some amount of ETL. However, the variability of noise in the\ndata and the heterogeneity induced by different sources create the need for\nad-hoc tools. Put otherwise, customer insight extraction in fully unsupervised,\nnoisy contexts is an arduous task. This research addresses the challenge of\nfully unsupervised topic extraction in noisy, Big Data contexts. We present\nthree approaches we built on the Variational Autoencoder framework: the\nEmbedded Dirichlet Process, the Embedded Hierarchical Dirichlet Process, and\nthe time-aware Dynamic Embedded Dirichlet Process. These nonparametric\napproaches concerning topics present the particularity of determining word\nembeddings and topic embeddings. These embeddings do not require transfer\nlearning, but knowledge transfer remains possible. We test these approaches on\nbenchmark and automotive industry-related datasets from a real-world use case.\nWe show that our models achieve equal to better performance than\nstate-of-the-art methods and that the field of topic modeling would benefit\nfrom improved evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palencia_Olivar_M/0/1/0/all/0/1\">Miguel Palencia-Olivar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transsion TSUP's speech recognition system for ASRU 2023 MADASR Challenge. (arXiv:2307.11778v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11778","description":"<p>This paper presents a speech recognition system developed by the Transsion\nSpeech Understanding Processing Team (TSUP) for the ASRU 2023 MADASR Challenge.\nThe system focuses on adapting ASR models for low-resource Indian languages and\ncovers all four tracks of the challenge. For tracks 1 and 2, the acoustic model\nutilized a squeezeformer encoder and bidirectional transformer decoder with\njoint CTC-Attention training loss. Additionally, an external KenLM language\nmodel was used during TLG beam search decoding. For tracks 3 and 4, pretrained\nIndicWhisper models were employed and finetuned on both the challenge dataset\nand publicly available datasets. The whisper beam search decoding was also\nmodified to support an external KenLM language model, which enabled better\nutilization of the additional text provided by the challenge. The proposed\nmethod achieved word error rates (WER) of 24.17%, 24.43%, 15.97%, and 15.97%\nfor Bengali language in the four tracks, and WER of 19.61%, 19.54%, 15.48%, and\n15.48% for Bhojpuri language in the four tracks. These results demonstrate the\neffectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gaosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1\">An Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiyong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shuming Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianchao Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Extractive-Abstractive Axis: Measuring Content \"Borrowing\" in Generative Language Models. (arXiv:2307.11779v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11779","description":"<p>Generative language models produce highly abstractive outputs by design, in\ncontrast to extractive responses in search engines. Given this characteristic\nof LLMs and the resulting implications for content Licensing &amp; Attribution, we\npropose the the so-called Extractive-Abstractive axis for benchmarking\ngenerative models and highlight the need for developing corresponding metrics,\ndatasets and annotation guidelines. We limit our discussion to the text\nmodality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teneva_N/0/1/0/all/0/1\">Nedelina Teneva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Conversational Shaping for Intelligent Agents. (arXiv:2307.11785v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11785","description":"<p>The recent emergence of deep learning methods has enabled the research\ncommunity to achieve state-of-the art results in several domains including\nnatural language processing. However, the current robocall system remains\nunstable and inaccurate: text generator and chat-bots can be tedious and\nmisunderstand human-like dialogue. In this work, we study the performance of\ntwo models able to enhance an intelligent conversational agent through\nadversarial conversational shaping: a generative adversarial network with\npolicy gradient (GANPG) and a generative adversarial network with reward for\nevery generation step (REGS) based on the REGS model presented in Li et al.\n[18] . This model is able to assign rewards to both partially and fully\ngenerated text sequences. We discuss performance with different training\ndetails : seq2seq [ 36] and transformers [37 ] in a reinforcement learning\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tarasiewicz_P/0/1/0/all/0/1\">Piotr Tarasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenjeyev_S/0/1/0/all/0/1\">Sultan Kenjeyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebag_I/0/1/0/all/0/1\">Ilana Sebag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshehabi_S/0/1/0/all/0/1\">Shehab Alshehabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM Cognitive Judgements Differ From Human. (arXiv:2307.11787v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11787","description":"<p>Large Language Models (LLMs) have lately been on the spotlight of\nresearchers, businesses, and consumers alike. While the linguistic capabilities\nof such models have been studied extensively, there is growing interest in\ninvestigating them as cognitive subjects. In the present work I examine GPT-3\nand ChatGPT capabilities on an limited-data inductive reasoning task from the\ncognitive science literature. The results suggest that these models' cognitive\njudgements are not human-like.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lamprinidis_S/0/1/0/all/0/1\">Sotiris Lamprinidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying QNLP to sentiment analysis in finance. (arXiv:2307.11788v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11788","description":"<p>As an application domain where the slightest qualitative improvements can\nyield immense value, finance is a promising candidate for early quantum\nadvantage. Focusing on the rapidly advancing field of Quantum Natural Language\nProcessing (QNLP), we explore the practical applicability of the two central\napproaches DisCoCat and Quantum-Enhanced Long Short-Term Memory (QLSTM) to the\nproblem of sentiment analysis in finance. Utilizing a novel ChatGPT-based data\ngeneration approach, we conduct a case study with more than 1000 realistic\nsentences and find that QLSTMs can be trained substantially faster than\nDisCoCat while also achieving close to classical results for their available\nsoftware implementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stein_J/0/1/0/all/0/1\">Jonas Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christ_I/0/1/0/all/0/1\">Ivo Christ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_N/0/1/0/all/0/1\">Nicolas Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansky_M/0/1/0/all/0/1\">Maximilian Balthasar Mansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_R/0/1/0/all/0/1\">Robert M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linnhof_Popien_C/0/1/0/all/0/1\">Claudia Linnhof-Popien</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting Large Language Models with Speech Recognition Abilities. (arXiv:2307.11795v1 [eess.AS])","link":"http://arxiv.org/abs/2307.11795","description":"<p>Large language models have proven themselves highly flexible, able to solve a\nwide range of generative tasks, such as abstractive summarization and\nopen-ended question answering. In this paper we extend the capabilities of LLMs\nby directly attaching a small audio encoder allowing it to perform speech\nrecognition. By directly prepending a sequence of audial embeddings to the text\ntoken embeddings, the LLM can be converted to an automatic speech recognition\n(ASR) system, and be used in the exact same manner as its textual counterpart.\nExperiments on Multilingual LibriSpeech (MLS) show that incorporating a\nconformer encoder into the open sourced LLaMA-7B allows it to outperform\nmonolingual baselines by 18% and perform multilingual speech recognition\ndespite LLaMA being trained overwhelmingly on English text. Furthermore, we\nperform ablation studies to investigate whether the LLM can be completely\nfrozen during training to maintain its original capabilities, scaling up the\naudio encoder, and increasing the audio encoder striding to generate fewer\nembeddings. The results from these studies show that multilingual ASR is\npossible even when the LLM is frozen or when strides of almost 1 second are\nused in the audio encoder opening up the possibility for LLMs to operate on\nlong-form audio.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fathullah_Y/0/1/0/all/0/1\">Yassir Fathullah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chunyang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lakomkin_E/0/1/0/all/0/1\">Egor Lakomkin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_J/0/1/0/all/0/1\">Junteng Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shangguan_Y/0/1/0/all/0/1\">Yuan Shangguan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1\">Jinxi Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiong_W/0/1/0/all/0/1\">Wenhan Xiong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahadeokar_J/0/1/0/all/0/1\">Jay Mahadeokar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fuegen_C/0/1/0/all/0/1\">Christian Fuegen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seltzer_M/0/1/0/all/0/1\">Mike Seltzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Document Analytics for Banking Process Automation. (arXiv:2307.11845v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11845","description":"<p>In response to growing FinTech competition and the need for improved\noperational efficiency, this research focuses on understanding the potential of\nadvanced document analytics, particularly using multimodal models, in banking\nprocesses. We perform a comprehensive analysis of the diverse banking document\nlandscape, highlighting the opportunities for efficiency gains through\nautomation and advanced analytics techniques in the customer business. Building\non the rapidly evolving field of natural language processing (NLP), we\nillustrate the potential of models such as LayoutXLM, a cross-lingual,\nmultimodal, pre-trained model, for analyzing diverse documents in the banking\nsector. This model performs a text token classification on German company\nregister extracts with an overall F1 score performance of around 80\\%. Our\nempirical evidence confirms the critical role of layout information in\nimproving model performance and further underscores the benefits of integrating\nimage information. Interestingly, our study shows that over 75% F1 score can be\nachieved with only 30% of the training data, demonstrating the efficiency of\nLayoutXLM. Through addressing state-of-the-art document analysis frameworks,\nour study aims to enhance process efficiency and demonstrate the real-world\napplicability and benefits of multimodal models within banking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gerling_C/0/1/0/all/0/1\">Christopher Gerling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lessmann_S/0/1/0/all/0/1\">Stefan Lessmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MythQA: Query-Based Large-Scale Check-Worthy Claim Detection through Multi-Answer Open-Domain Question Answering. (arXiv:2307.11848v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11848","description":"<p>Check-worthy claim detection aims at providing plausible misinformation to\ndownstream fact-checking systems or human experts to check. This is a crucial\nstep toward accelerating the fact-checking process. Many efforts have been put\ninto how to identify check-worthy claims from a small scale of pre-collected\nclaims, but how to efficiently detect check-worthy claims directly from a\nlarge-scale information source, such as Twitter, remains underexplored. To fill\nthis gap, we introduce MythQA, a new multi-answer open-domain question\nanswering(QA) task that involves contradictory stance mining for query-based\nlarge-scale check-worthy claim detection. The idea behind this is that\ncontradictory claims are a strong indicator of misinformation that merits\nscrutiny by the appropriate authorities. To study this task, we construct\nTweetMythQA, an evaluation dataset containing 522 factoid multi-answer\nquestions based on controversial topics. Each question is annotated with\nmultiple answers. Moreover, we collect relevant tweets for each distinct\nanswer, then classify them into three categories: \"Supporting\", \"Refuting\", and\n\"Neutral\". In total, we annotated 5.3K tweets. Contradictory evidence is\ncollected for all answers in the dataset. Finally, we present a baseline system\nfor MythQA and evaluate existing NLP models for each system component using the\nTweetMythQA dataset. We provide initial benchmarks and identify key challenges\nfor future models to improve upon. Code and data are available at:\nhttps://github.com/TonyBY/Myth-QA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colas_A/0/1/0/all/0/1\">Anthony Colas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daisy Zhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention. (arXiv:2307.11864v1 [cs.SI])","link":"http://arxiv.org/abs/2307.11864","description":"<p>In this paper, we present a novel method for detecting fake and Large\nLanguage Model (LLM)-generated profiles in the LinkedIn Online Social Network\nimmediately upon registration and before establishing connections. Early fake\nprofile identification is crucial to maintaining the platform's integrity since\nit prevents imposters from acquiring the private and sensitive information of\nlegitimate users and from gaining an opportunity to increase their credibility\nfor future phishing and scamming activities. This work uses textual information\nprovided in LinkedIn profiles and introduces the Section and Subsection Tag\nEmbedding (SSTE) method to enhance the discriminative characteristics of these\ndata for distinguishing between legitimate profiles and those created by\nimposters manually or by using an LLM. Additionally, the dearth of a large\npublicly available LinkedIn dataset motivated us to collect 3600 LinkedIn\nprofiles for our research. We will release our dataset publicly for research\npurposes. This is, to the best of our knowledge, the first large publicly\navailable LinkedIn dataset for fake LinkedIn account detection. Within our\nparadigm, we assess static and contextualized word embeddings, including GloVe,\nFlair, BERT, and RoBERTa. We show that the suggested method can distinguish\nbetween legitimate and fake profiles with an accuracy of about 95% across all\nword embeddings. In addition, we show that SSTE has a promising accuracy for\nidentifying LLM-generated profiles, despite the fact that no LLM-generated\nprofiles were employed during the training phase, and can achieve an accuracy\nof approximately 90% when only 20 LLM-generated profiles are added to the\ntraining set. It is a significant finding since the proliferation of several\nLLMs in the near future makes it extremely challenging to design a single\nsystem that can identify profiles created with various LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayoobi_N/0/1/0/all/0/1\">Navid Ayoobi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sadat Shahriar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Arjun Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots. (arXiv:2307.11865v1 [cs.RO])","link":"http://arxiv.org/abs/2307.11865","description":"<p>This work explores the capacity of large language models (LLMs) to address\nproblems at the intersection of spatial planning and natural language\ninterfaces for navigation.Our focus is on following relatively complex\ninstructions that are more akin to natural conversation than traditional\nexplicit procedural directives seen in robotics. Unlike most prior work, where\nnavigation directives are provided as imperative commands (e.g., go to the\nfridge), we examine implicit directives within conversational interactions. We\nleverage the 3D simulator AI2Thor to create complex and repeatable scenarios at\nscale, and augment it by adding complex language queries for 40 object types.\nWe demonstrate that a robot can better parse descriptive language queries than\nexisting methods by using an LLM to interpret the user interaction in the\ncontext of a list of the objects in the scene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kakodkar_N/0/1/0/all/0/1\">Nikhil Kakodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivkin_D/0/1/0/all/0/1\">Dmitriy Rivkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baghi_B/0/1/0/all/0/1\">Bobak H. Baghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogan_F/0/1/0/all/0/1\">Francois Hogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1\">Gregory Dudek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors. (arXiv:2307.11922v1 [cs.LG])","link":"http://arxiv.org/abs/2307.11922","description":"<p>Large language models (LLMs) are being applied as actors for sequential\ndecision making tasks in domains such as robotics and games, utilizing their\ngeneral world knowledge and planning abilities. However, previous work does\nlittle to explore what environment state information is provided to LLM actors\nvia language. Exhaustively describing high-dimensional states can impair\nperformance and raise inference costs for LLM actors. Previous LLM actors avoid\nthe issue by relying on hand-engineered, task-specific protocols to determine\nwhich features to communicate about a state and which to leave out. In this\nwork, we propose Brief Language INputs for DEcision-making Responses (BLINDER),\na method for automatically selecting concise state descriptions by learning a\nvalue function for task-conditioned state descriptions. We evaluate BLINDER on\nthe challenging video game NetHack and a robotic manipulation task. Our method\nimproves task success rate, reduces input size and compute costs, and\ngeneralizes between LLM actors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nottingham_K/0/1/0/all/0/1\">Kolby Nottingham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razeghi_Y/0/1/0/all/0/1\">Yasaman Razeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyungmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanier_J/0/1/0/all/0/1\">JB Lanier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldi_P/0/1/0/all/0/1\">Pierre Baldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_R/0/1/0/all/0/1\">Roy Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Vision-and-Language Navigation from YouTube Videos. (arXiv:2307.11984v1 [cs.CV])","link":"http://arxiv.org/abs/2307.11984","description":"<p>Vision-and-language navigation (VLN) requires an embodied agent to navigate\nin realistic 3D environments using natural language instructions. Existing VLN\nmethods suffer from training on small-scale environments or unreasonable\npath-instruction datasets, limiting the generalization to unseen environments.\nThere are massive house tour videos on YouTube, providing abundant real\nnavigation experiences and layout information. However, these videos have not\nbeen explored for VLN before. In this paper, we propose to learn an agent from\nthese videos by creating a large-scale dataset which comprises reasonable\npath-instruction pairs from house tour videos and pre-training the agent on it.\nTo achieve this, we have to tackle the challenges of automatically constructing\npath-instruction pairs and exploiting real layout knowledge from raw and\nunlabeled videos. To address these, we first leverage an entropy-based method\nto construct the nodes of a path trajectory. Then, we propose an action-aware\ngenerator for generating instructions from unlabeled trajectories. Last, we\ndevise a trajectory judgment pretext task to encourage the agent to mine the\nlayout knowledge. Experimental results show that our method achieves\nstate-of-the-art performance on two popular benchmarks (R2R and REVERIE). Code\nis available at https://github.com/JeremyLinky/YouTube-VLN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kunyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Diwei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Thomas H. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models. (arXiv:2307.11991v1 [cs.CL])","link":"http://arxiv.org/abs/2307.11991","description":"<p>The demand for psychological counseling has grown significantly in recent\nyears, particularly with the global outbreak of COVID-19, which has heightened\nthe need for timely and professional mental health support. Online\npsychological counseling has emerged as the predominant mode of providing\nservices in response to this demand. In this study, we propose the Psy-LLM\nframework, an AI-based system leveraging Large Language Models (LLMs) for\nquestion-answering in online psychological consultation. Our framework combines\npre-trained LLMs with real-world professional Q&amp;A from psychologists and\nextensively crawled psychological articles. The Psy-LLM framework serves as a\nfront-end tool for healthcare professionals, allowing them to provide immediate\nresponses and mindfulness activities to alleviate patient stress. Additionally,\nit functions as a screening tool to identify urgent cases requiring further\nassistance. We evaluated the framework using intrinsic metrics, such as\nperplexity, and extrinsic evaluation metrics, with human participant\nassessments of response helpfulness, fluency, relevance, and logic. The results\ndemonstrate the effectiveness of the Psy-LLM framework in generating coherent\nand relevant answers to psychological questions. This article concludes by\ndiscussing the potential of large language models to enhance mental health\nsupport through AI technologies in online psychological consultation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yukun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zicong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Ken Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yichao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery. (arXiv:2307.12045v1 [cs.CV])","link":"http://arxiv.org/abs/2307.12045","description":"<p>The visual-question localized-answering (VQLA) system can serve as a\nknowledgeable assistant in surgical education. Except for providing text-based\nanswers, the VQLA system can highlight the interested region for better\nsurgical scene understanding. However, deep neural networks (DNNs) suffer from\ncatastrophic forgetting when learning new knowledge. Specifically, when DNNs\nlearn on incremental classes or tasks, their performance on old tasks drops\ndramatically. Furthermore, due to medical data privacy and licensing issues, it\nis often difficult to access old data when updating continual learning (CL)\nmodels. Therefore, we develop a non-exemplar continual surgical VQLA framework,\nto explore and balance the rigidity-plasticity trade-off of DNNs in a\nsequential learning paradigm. We revisit the distillation loss in CL tasks, and\npropose rigidity-plasticity-aware distillation (RP-Dist) and self-calibrated\nheterogeneous distillation (SH-Dist) to preserve the old knowledge. The weight\naligning (WA) technique is also integrated to adjust the weight bias between\nold and new tasks. We further establish a CL framework on three public surgical\ndatasets in the context of surgical settings that consist of overlapping\nclasses between old and new surgical VQLA tasks. With extensive experiments, we\ndemonstrate that our proposed method excellently reconciles learning and\nforgetting on the continual surgical VQLA over conventional CL methods. Our\ncode is publicly accessible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Long Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongliang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback. (arXiv:2307.12057v1 [cs.CL])","link":"http://arxiv.org/abs/2307.12057","description":"<p>Memory is identified as a crucial human faculty that allows for the retention\nof visual and linguistic information within the hippocampus and neurons in the\nbrain, which can subsequently be retrieved to address real-world challenges\nthat arise through a lifetime of learning. The resolution of complex AI tasks\nthrough the application of acquired knowledge represents a stride toward the\nrealization of artificial general intelligence. However, despite the prevalence\nof Large Language Models (LLMs) like GPT-3.5 and GPT-4 , which have displayed\nremarkable capabilities in language comprehension, generation, interaction, and\nreasoning, they are inhibited by constraints on context length that preclude\nthe processing of extensive, continually evolving knowledge bases. This paper\nproposes that LLMs could be augmented through the selective integration of\nknowledge from external repositories, and in doing so, introduces a novel\nmethodology for External Reasoning, exemplified by ChatPDF. Central to this\napproach is the establishment of a tiered policy for \\textbf{External Reasoning\nbased on Multiple LLM Interchange Assistance}, where the level of support\nrendered is modulated across entry, intermediate, and advanced tiers based on\nthe complexity of the query, with adjustments made in response to human\nfeedback. A comprehensive evaluation of this methodology is conducted using\nmultiple LLMs and the results indicate state-of-the-art performance, surpassing\nexisting solutions including ChatPDF.com. Moreover, the paper emphasizes that\nthis approach is more efficient compared to the direct processing of full text\nby LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Akide Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks. (arXiv:2307.12114v1 [cs.CL])","link":"http://arxiv.org/abs/2307.12114","description":"<p>We evaluate four state-of-the-art instruction-tuned large language models\n(LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13\nreal-world clinical and biomedical natural language processing (NLP) tasks in\nEnglish, such as named-entity recognition (NER), question-answering (QA),\nrelation extraction (RE), etc. Our overall results demonstrate that the\nevaluated LLMs begin to approach performance of state-of-the-art models in\nzero- and few-shot scenarios for most tasks, and particularly well for the QA\ntask, even though they have never seen examples from these tasks before.\nHowever, we observed that the classification and RE tasks perform below what\ncan be achieved with a specifically trained model for the medical field, such\nas PubMedBERT. Finally, we noted that no LLM outperforms all the others on all\nthe studied tasks, with some models being better suited for certain tasks than\nothers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Labrak_Y/0/1/0/all/0/1\">Yanis Labrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouvier_M/0/1/0/all/0/1\">Mickael Rouvier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufour_R/0/1/0/all/0/1\">Richard Dufour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Topic-Enhanced Argument Mining from Heterogeneous Sources. (arXiv:2307.12131v1 [cs.CL])","link":"http://arxiv.org/abs/2307.12131","description":"<p>Given a controversial target such as ``nuclear energy'', argument mining aims\nto identify the argumentative text from heterogeneous sources. Current\napproaches focus on exploring better ways of integrating the target-associated\nsemantic information with the argumentative text. Despite their empirical\nsuccesses, two issues remain unsolved: (i) a target is represented by a word or\na phrase, which is insufficient to cover a diverse set of target-related\nsubtopics; (ii) the sentence-level topic information within an argument, which\nwe believe is crucial for argument mining, is ignored. To tackle the above\nissues, we propose a novel explainable topic-enhanced argument mining approach.\nSpecifically, with the use of the neural topic model and the language model,\nthe target information is augmented by explainable topic representations.\nMoreover, the sentence-level topic information within the argument is captured\nby minimizing the distance between its latent topic distribution and its\nsemantic representation through mutual learning. Experiments have been\nconducted on the benchmark dataset in both the in-target setting and the\ncross-target setting. Results demonstrate the superiority of the proposed model\nagainst the state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_J/0/1/0/all/0/1\">Jiasheng Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yingjie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xingyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Deyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding. (arXiv:2307.12134v1 [cs.CL])","link":"http://arxiv.org/abs/2307.12134","description":"<p>End-to-end (E2E) spoken language understanding (SLU) systems that generate a\nsemantic parse from speech have become more promising recently. This approach\nuses a single model that utilizes audio and text representations from\npre-trained speech recognition models (ASR), and outperforms traditional\npipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems\nstill show weakness when text representation quality is low due to ASR\ntranscription errors. To overcome this issue, we propose a novel E2E SLU system\nthat enhances robustness to ASR errors by fusing audio and text representations\nbased on the estimated modality confidence of ASR hypotheses. We introduce two\nnovel techniques: 1) an effective method to encode the quality of ASR\nhypotheses and 2) an effective approach to integrate them into E2E SLU models.\nWe show accuracy improvements on STOP dataset and share the analysis to\ndemonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Suyoun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Ju Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1\">Michael L. Seltzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Misinformation on YouTube through Transcript Contextual Analysis with Transformer Models. (arXiv:2307.12155v1 [cs.CL])","link":"http://arxiv.org/abs/2307.12155","description":"<p>Misinformation on YouTube is a significant concern, necessitating robust\ndetection strategies. In this paper, we introduce a novel methodology for video\nclassification, focusing on the veracity of the content. We convert the\nconventional video classification task into a text classification task by\nleveraging the textual content derived from the video transcripts. We employ\nadvanced machine learning techniques like transfer learning to solve the\nclassification challenge. Our approach incorporates two forms of transfer\nlearning: (a) fine-tuning base transformer models such as BERT, RoBERTa, and\nELECTRA, and (b) few-shot learning using sentence-transformers MPNet and\nRoBERTa-large. We apply the trained models to three datasets: (a) YouTube\nVaccine-misinformation related videos, (b) YouTube Pseudoscience videos, and\n(c) Fake-News dataset (a collection of articles). Including the Fake-News\ndataset extended the evaluation of our approach beyond YouTube videos. Using\nthese datasets, we evaluated the models distinguishing valid information from\nmisinformation. The fine-tuned models yielded Matthews Correlation\nCoefficient&gt;0.81, accuracy&gt;0.90, and F1 score&gt;0.90 in two of three datasets.\nInterestingly, the few-shot models outperformed the fine-tuned ones by 20% in\nboth Accuracy and F1 score for the YouTube Pseudoscience dataset, highlighting\nthe potential utility of this approach -- especially in the context of limited\ntraining data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christodoulou_C/0/1/0/all/0/1\">Christos Christodoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salamanos_N/0/1/0/all/0/1\">Nikos Salamanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonidou_P/0/1/0/all/0/1\">Pantelitsa Leonidou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_M/0/1/0/all/0/1\">Michail Papadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirivianos_M/0/1/0/all/0/1\">Michael Sirivianos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Imitation Game: Detecting Human and AI-Generated Texts in the Era of Large Language Models. (arXiv:2307.12166v1 [cs.CL])","link":"http://arxiv.org/abs/2307.12166","description":"<p>The potential of artificial intelligence (AI)-based large language models\n(LLMs) holds considerable promise in revolutionizing education, research, and\npractice. However, distinguishing between human-written and AI-generated text\nhas become a significant task. This paper presents a comparative study,\nintroducing a novel dataset of human-written and LLM-generated texts in\ndifferent genres: essays, stories, poetry, and Python code. We employ several\nmachine learning models to classify the texts. Results demonstrate the efficacy\nof these models in discerning between human and AI-generated text, despite the\ndataset's limited sample size. However, the task becomes more challenging when\nclassifying GPT-generated text, particularly in story writing. The results\nindicate that the models exhibit superior performance in binary classification\ntasks, such as distinguishing human-generated text from a specific LLM,\ncompared to the more complex multiclass tasks that involve discerning among\nhuman-generated and multiple LLMs. Our findings provide insightful implications\nfor AI text detection while our dataset paves the way for future research in\nthis evolving area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hayawi_K/0/1/0/all/0/1\">Kadhim Hayawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_S/0/1/0/all/0/1\">Sujith Samuel Mathew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FATRER: Full-Attention Topic Regularizer for Accurate and Robust Conversational Emotion Recognition. (arXiv:2307.12221v1 [cs.CL])","link":"http://arxiv.org/abs/2307.12221","description":"<p>This paper concentrates on the understanding of interlocutors' emotions\nevoked in conversational utterances. Previous studies in this literature mainly\nfocus on more accurate emotional predictions, while ignoring model robustness\nwhen the local context is corrupted by adversarial attacks. To maintain\nrobustness while ensuring accuracy, we propose an emotion recognizer augmented\nby a full-attention topic regularizer, which enables an emotion-related global\nview when modeling the local context in a conversation. A joint topic modeling\nstrategy is introduced to implement regularization from both representation and\nloss perspectives. To avoid over-regularization, we drop the constraints on\nprior distributions that exist in traditional topic modeling and perform\nprobabilistic approximations based entirely on attention alignment. Experiments\nshow that our models obtain more favorable results than state-of-the-art\nmodels, and gain convincing robustness under three types of adversarial\nattacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuzhao Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Di Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Is All You Need. (arXiv:1706.03762v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1706.03762","description":"<p>The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1\">Ashish Vaswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_N/0/1/0/all/0/1\">Niki Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1\">Jakob Uszkoreit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_L/0/1/0/all/0/1\">Llion Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1\">Aidan N. Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_L/0/1/0/all/0/1\">Lukasz Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polosukhin_I/0/1/0/all/0/1\">Illia Polosukhin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Coreference Resolution by Leveraging Entity-Centric Features with Graph Neural Networks and Second-order Inference. (arXiv:2009.04639v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.04639","description":"<p>One of the major challenges in coreference resolution is how to make use of\nentity-level features defined over clusters of mentions rather than mention\npairs. However, coreferent mentions usually spread far apart in an entire text,\nwhich makes it extremely difficult to incorporate entity-level features. We\npropose a graph neural network-based coreference resolution method that can\ncapture the entity-centric information by encouraging the sharing of features\nacross all mentions that probably refer to the same real-world entity. Mentions\nare linked to each other via the edges modeling how likely two linked mentions\npoint to the same entity. Modeling by such graphs, the features between\nmentions can be shared by message passing operations in an entity-centric\nmanner. A global inference algorithm up to second-order features is also\npresented to optimally cluster mentions into consistent groups. Experimental\nresults show our graph neural network-based method combing with the\nsecond-order decoding algorithm (named GNNCR) achieved close to\nstate-of-the-art performance on the English CoNLL-2012 Shared Task dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhenqiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Summarization by Jointly Extracting Sentences and Keywords. (arXiv:2009.07481v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.07481","description":"<p>We present RepRank, an unsupervised graph-based ranking model for extractive\nmulti-document summarization in which the similarity between words, sentences,\nand word-to-sentence can be estimated by the distances between their vector\nrepresentations in a unified vector space. In order to obtain desirable\nrepresentations, we propose a self-attention based learning method that\nrepresent a sentence by the weighted sum of its word embeddings, and the\nweights are concentrated to those words hopefully better reflecting the content\nof a document. We show that salient sentences and keywords can be extracted in\na joint and mutual reinforcement process using our learned representations, and\nprove that this process always converges to a unique solution leading to\nimprovement in performance. A variant of absorbing random walk and the\ncorresponding sampling-based algorithm are also described to avoid redundancy\nand increase diversity in the summaries. Experiment results with multiple\nbenchmark datasets show that RepRank achieved the best or comparable\nperformance in ROUGE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XTQA: Span-Level Explanations of the Textbook Question Answering. (arXiv:2011.12662v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.12662","description":"<p>Textbook Question Answering (TQA) is a task that one should answer a\ndiagram/non-diagram question given a large multi-modal context consisting of\nabundant essays and diagrams. We argue that the explainability of this task\nshould place students as a key aspect to be considered. To address this issue,\nwe devise a novel architecture towards span-level eXplanations of the TQA\n(XTQA) based on our proposed coarse-to-fine grained algorithm, which can\nprovide not only the answers but also the span-level evidences to choose them\nfor students. This algorithm first coarsely chooses top $M$ paragraphs relevant\nto questions using the TF-IDF method, and then chooses top $K$ evidence spans\nfinely from all candidate spans within these paragraphs by computing the\ninformation gain of each span to questions. Experimental results shows that\nXTQA significantly improves the state-of-the-art performance compared with\nbaselines. The source code is available at\nhttps://github.com/keep-smile-001/opentqa\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jie Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Q/0/1/0/all/0/1\">Qi Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qingyu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pinghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SparseGAN: Sparse Generative Adversarial Network for Text Generation. (arXiv:2103.11578v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11578","description":"<p>It is still a challenging task to learn a neural text generation model under\nthe framework of generative adversarial networks (GANs) since the entire\ntraining process is not differentiable. The existing training strategies either\nsuffer from unreliable gradient estimations or imprecise sentence\nrepresentations. Inspired by the principle of sparse coding, we propose a\nSparseGAN that generates semantic-interpretable, but sparse sentence\nrepresentations as inputs to the discriminator. The key idea is that we treat\nan embedding matrix as an over-complete dictionary, and use a linear\ncombination of very few selected word embeddings to approximate the output\nfeature representation of the generator at each time step. With such\nsemantic-rich representations, we not only reduce unnecessary noises for\nefficient adversarial training, but also make the entire training process fully\ndifferentiable. Experiments on multiple text generation datasets yield\nperformance improvements, especially in sequence-level metrics, such as BLEU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liping Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiehang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAnoBERT: System Log Anomaly Detection based on BERT Masked Language Model. (arXiv:2111.09564v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.09564","description":"<p>The system log generated in a computer system refers to large-scale data that\nare collected simultaneously and used as the basic data for determining errors,\nintrusion and abnormal behaviors. The aim of system log anomaly detection is to\npromptly identify anomalies while minimizing human intervention, which is a\ncritical problem in the industry. Previous studies performed anomaly detection\nthrough algorithms after converting various forms of log data into a\nstandardized template using a parser. Particularly, a template corresponding to\na specific event should be defined in advance for all the log data using which\nthe information within the log key may get lost. In this study, we propose\nLAnoBERT, a parser free system log anomaly detection method that uses the BERT\nmodel, exhibiting excellent natural language processing performance. The\nproposed method, LAnoBERT, learns the model through masked language modeling,\nwhich is a BERT-based pre-training method, and proceeds with unsupervised\nlearning-based anomaly detection using the masked language modeling loss\nfunction per log key during the test process. In addition, we also propose an\nefficient inference process to establish a practically applicable pipeline to\nthe actual system. Experiments on three well-known log datasets, i.e., HDFS,\nBGL, and Thunderbird, show that not only did LAnoBERT yield a higher anomaly\ndetection performance compared to unsupervised learning-based benchmark models,\nbut also it resulted in a comparable performance with supervised learning-based\nbenchmark models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yukyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jina Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1\">Pilsung Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSTEA: Improving Dialogue State Tracking via Entity Adaptive Pre-training. (arXiv:2207.03858v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.03858","description":"<p>Dialogue State Tracking (DST) is critical for comprehensively interpreting\nuser and system utterances, thereby forming the cornerstone of efficient\ndialogue systems. Despite past research efforts focused on enhancing DST\nperformance through alterations to the model structure or integrating\nadditional features like graph relations, they often require additional\npre-training with external dialogue corpora. In this study, we propose DSTEA,\nimproving Dialogue State Tracking via Entity Adaptive pre-training, which can\nenhance the encoder through by intensively training key entities in dialogue\nutterances. DSTEA identifies these pivotal entities from input dialogues\nutilizing four different methods: ontology information, named-entity\nrecognition, the spaCy, and the flair library. Subsequently, it employs\nselective knowledge masking to train the model effectively. Remarkably, DSTEA\nonly requires pre-training without the direct infusion of extra knowledge into\nthe DST model. This approach resulted in substantial performance improvements\nof four robust DST models on MultiWOZ 2.0, 2.1, and 2.2, with joint goal\naccuracy witnessing an increase of up to 2.69% (from 52.41% to 55.10%). Further\nvalidation of DSTEA's efficacy was provided through comparative experiments\nconsidering various entity types and different entity adaptive pre-training\nconfigurations such as masking strategy and masking rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yukyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Takyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1\">Hoonsang Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1\">Pilsung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_J/0/1/0/all/0/1\">Junseong Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Misuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning \"O\" Helps for Learning More: Handling the Concealed Entity Problem for Class-incremental NER. (arXiv:2210.04676v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04676","description":"<p>As the categories of named entities rapidly increase, the deployed NER models\nare required to keep updating toward recognizing more entity types, creating a\ndemand for class-incremental learning for NER. Considering the privacy concerns\nand storage constraints, the standard paradigm for class-incremental NER\nupdates the models with training data only annotated with the new classes, yet\nthe entities from other entity classes are unlabeled, regarded as \"Non-entity\"\n(or \"O\"). In this work, we conduct an empirical study on the \"Unlabeled Entity\nProblem\" and find that it leads to severe confusion between \"O\" and entities,\ndecreasing class discrimination of old classes and declining the model's\nability to learn new classes. To solve the Unlabeled Entity Problem, we propose\na novel representation learning method to learn discriminative representations\nfor the entity classes and \"O\". Specifically, we propose an entity-aware\ncontrastive learning method that adaptively detects entity clusters in \"O\".\nFurthermore, we propose two effective distance-based relabeling strategies for\nbetter learning the old classes. We introduce a more realistic and challenging\nbenchmark for class-incremental NER, and the proposed method achieves up to\n10.62\\% improvement over the baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junzhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunwen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Log-linear Guardedness and its Implications. (arXiv:2210.10012v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.10012","description":"<p>Methods for erasing human-interpretable concepts from neural representations\nthat assume linearity have been found to be tractable and useful. However, the\nimpact of this removal on the behavior of downstream classifiers trained on the\nmodified representations is not fully understood. In this work, we formally\ndefine the notion of log-linear guardedness as the inability of an adversary to\npredict the concept directly from the representation, and study its\nimplications. We show that, in the binary case, under certain assumptions, a\ndownstream log-linear model cannot recover the erased concept. However, we\ndemonstrate that a multiclass log-linear model \\emph{can} be constructed that\nindirectly recovers the concept in some cases, pointing to the inherent\nlimitations of log-linear guardedness as a downstream bias mitigation\ntechnique. These findings shed light on the theoretical limitations of linear\nerasure methods and highlight the need for further research on the connections\nbetween intrinsic and extrinsic bias in neural models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward expanding the scope of radiology report summarization to multiple anatomies and modalities. (arXiv:2211.08584v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08584","description":"<p>Radiology report summarization (RRS) is a growing area of research. Given the\nFindings section of a radiology report, the goal is to generate a summary\n(called an Impression section) that highlights the key observations and\nconclusions of the radiology study. However, RRS currently faces essential\nlimitations.First, many prior studies conduct experiments on private datasets,\npreventing reproduction of results and fair comparisons across different\nsystems and solutions. Second, most prior approaches are evaluated solely on\nchest X-rays. To address these limitations, we propose a dataset (MIMIC-RRS)\ninvolving three new modalities and seven new anatomies based on the MIMIC-III\nand MIMIC-CXR datasets. We then conduct extensive experiments to evaluate the\nperformance of models both within and across modality-anatomy pairs in\nMIMIC-RRS. In addition, we evaluate their clinical efficacy via RadGraph, a\nfactual correctness metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Maya Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delbrouck_J/0/1/0/all/0/1\">Jean-Benoit Delbrouck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style Classification of Rabbinic Literature for Detection of Lost Midrash Tanhuma Material. (arXiv:2211.09710v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.09710","description":"<p>Midrash collections are complex rabbinic works that consist of text in\nmultiple languages, which evolved through long processes of unstable oral and\nwritten transmission. Determining the origin of a given passage in such a\ncompilation is not always straightforward and is often a matter of dispute\namong scholars, yet it is essential for scholars' understanding of the passage\nand its relationship to other texts in the rabbinic corpus. To help solve this\nproblem, we propose a system for classification of rabbinic literature based on\nits style, leveraging recent advances in natural language processing for Hebrew\ntexts. Additionally, we demonstrate how this method can be applied to uncover\nlost material from a specific midrash genre, Tan\\d{h}uma-Yelammedenu, that has\nbeen preserved in later anthologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tannor_S/0/1/0/all/0/1\">Shlomo Tannor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dershowitz_N/0/1/0/all/0/1\">Nachum Dershowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavee_M/0/1/0/all/0/1\">Moshe Lavee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Knowledge Graph Reasoning on Graph Types: Static, Dynamic, and Multimodal. (arXiv:2212.05767v7 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.05767","description":"<p>Knowledge graph reasoning (KGR), aiming to deduce new facts from existing\nfacts based on mined logic rules underlying knowledge graphs (KGs), has become\na fast-growing research direction. It has been proven to significantly benefit\nthe usage of KGs in many AI applications, such as question answering,\nrecommendation systems, and etc. According to the graph types, existing KGR\nmodels can be roughly divided into three categories, i.e., static models,\ntemporal models, and multi-modal models. Early works in this domain mainly\nfocus on static KGR, and recent works try to leverage the temporal and\nmulti-modal information, which are more practical and closer to real-world.\nHowever, no survey papers and open-source repositories comprehensively\nsummarize and discuss models in this important direction. To fill the gap, we\nconduct a first survey for knowledge graph reasoning tracing from static to\ntemporal and then to multi-modal KGs. Concretely, the models are reviewed based\non bi-level taxonomy, i.e., top-level (graph types) and base-level (techniques\nand scenarios). Besides, the performances, as well as datasets, are summarized\nand presented. Moreover, we point out the challenges and potential\nopportunities to enlighten the readers. The corresponding open-source\nrepository is shared on GitHub\nhttps://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Ke Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingyuan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wenxuan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sihang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Next Chapter: A Study of Large Language Models in Storytelling. (arXiv:2301.09790v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.09790","description":"<p>To enhance the quality of generated stories, recent story generation models\nhave been investigating the utilization of higher-level attributes like plots\nor commonsense knowledge. The application of prompt-based learning with large\nlanguage models (LLMs), exemplified by GPT-3, has exhibited remarkable\nperformance in diverse natural language processing (NLP) tasks. This paper\nconducts a comprehensive investigation, utilizing both automatic and human\nevaluation, to compare the story generation capacity of LLMs with recent models\nacross three datasets with variations in style, register, and length of\nstories. The results demonstrate that LLMs generate stories of significantly\nhigher quality compared to other story generation models. Moreover, they\nexhibit a level of performance that competes with human authors, albeit with\nthe preliminary observation that they tend to replicate real stories in\nsituations involving world knowledge, resembling a form of plagiarism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhuohan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. (arXiv:2301.11305v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11305","description":"<p>The increasing fluency and widespread usage of large language models (LLMs)\nhighlight the desirability of corresponding tools aiding detection of\nLLM-generated text. In this paper, we identify a property of the structure of\nan LLM's probability function that is useful for such detection. Specifically,\nwe demonstrate that text sampled from an LLM tends to occupy negative curvature\nregions of the model's log probability function. Leveraging this observation,\nwe then define a new curvature-based criterion for judging if a passage is\ngenerated from a given LLM. This approach, which we call DetectGPT, does not\nrequire training a separate classifier, collecting a dataset of real or\ngenerated passages, or explicitly watermarking generated text. It uses only log\nprobabilities computed by the model of interest and random perturbations of the\npassage from another generic pre-trained language model (e.g., T5). We find\nDetectGPT is more discriminative than existing zero-shot methods for model\nsample detection, notably improving detection of fake news articles generated\nby 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline\nto 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code,\ndata, and other project information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_E/0/1/0/all/0/1\">Eric Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yoonho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khazatsky_A/0/1/0/all/0/1\">Alexander Khazatsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Harmful Agendas in News Articles. (arXiv:2302.00102v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00102","description":"<p>Manipulated news online is a growing problem which necessitates the use of\nautomated systems to curtail its spread. We argue that while misinformation and\ndisinformation detection have been studied, there has been a lack of investment\nin the important open challenge of detecting harmful agendas in news articles;\nidentifying harmful agendas is critical to flag news campaigns with the\ngreatest potential for real world harm. Moreover, due to real concerns around\ncensorship, harmful agenda detectors must be interpretable to be effective. In\nthis work, we propose this new task and release a dataset, NewsAgendas, of\nannotated news articles for agenda identification. We show how interpretable\nsystems can be effective on this task and demonstrate that they can perform\ncomparably to black-box models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subbiah_M/0/1/0/all/0/1\">Melanie Subbiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Amrita Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yilun Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1\">Tharindu Kumarage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding. (arXiv:2303.04245v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.04245","description":"<p>While the successes of transformers across many domains are indisputable,\naccurate understanding of the learning mechanics is still largely lacking.\nTheir capabilities have been probed on benchmarks which include a variety of\nstructured and reasoning tasks -- but mathematical understanding is lagging\nsubstantially behind. Recent lines of work have begun studying representational\naspects of this question: that is, the size/depth/complexity of attention-based\nnetworks to perform certain tasks. However, there is no guarantee the learning\ndynamics will converge to the constructions proposed. In our paper, we provide\nfine-grained mechanistic understanding of how transformers learn \"semantic\nstructure\", understood as capturing co-occurrence structure of words.\nPrecisely, we show, through a combination of mathematical analysis and\nexperiments on Wikipedia data and synthetic data modeled by Latent Dirichlet\nAllocation (LDA), that the embedding layer and the self-attention layer encode\nthe topical structure. In the former case, this manifests as higher average\ninner product of embeddings between same-topic words. In the latter, it\nmanifests as higher average pairwise attention between same-topic words. The\nmathematical results involve several assumptions to make the analysis\ntractable, which we verify on data, and might be of independent interest as\nwell.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1\">Andrej Risteski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MenuCraft: Interactive Menu System Design with Large Language Models. (arXiv:2303.04496v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.04496","description":"<p>Menu system design is a challenging task involving many design options and\nvarious human factors. For example, one crucial factor that designers need to\nconsider is the semantic and systematic relation of menu commands. However,\ncapturing these relations can be challenging due to limited available\nresources. With the advancement of neural language models, large language\nmodels can utilize their vast pre-existing knowledge in designing and refining\nmenu systems. In this paper, we propose MenuCraft, an AI-assisted designer for\nmenu design that enables collaboration between the designer and a dialogue\nsystem to design menus. MenuCraft offers an interactive language-based menu\ndesign tool that simplifies the menu design process and enables easy\ncustomization of design options. MenuCraft supports a variety of interactions\nthrough dialog that allows performing zero/few-shot learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kargaran_A/0/1/0/all/0/1\">Amir Hossein Kargaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikeghbal_N/0/1/0/all/0/1\">Nafiseh Nikeghbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heydarnoori_A/0/1/0/all/0/1\">Abbas Heydarnoori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review. (arXiv:2303.13379v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13379","description":"<p>Educational technology innovations leveraging large language models (LLMs)\nhave shown the potential to automate the laborious process of generating and\nanalysing textual content. While various innovations have been developed to\nautomate a range of educational tasks (e.g., question generation, feedback\nprovision, and essay grading), there are concerns regarding the practicality\nand ethicality of these innovations. Such concerns may hinder future research\nand the adoption of LLMs-based innovations in authentic educational contexts.\nTo address this, we conducted a systematic scoping review of 118 peer-reviewed\npapers published since 2017 to pinpoint the current state of research on using\nLLMs to automate and support educational tasks. The findings revealed 53 use\ncases for LLMs in automating education tasks, categorised into nine main\ncategories: profiling/labelling, detection, grading, teaching support,\nprediction, knowledge representation, feedback, content generation, and\nrecommendation. Additionally, we also identified several practical and ethical\nchallenges, including low technological readiness, lack of replicability and\ntransparency, and insufficient privacy and beneficence considerations. The\nfindings were summarised into three recommendations for future studies,\nincluding updating existing innovations with state-of-the-art models (e.g.,\nGPT-3/4), embracing the initiative of open-sourcing models/systems, and\nadopting a human-centred approach throughout the developmental process. As the\nintersection of AI and education is continuously evolving, the findings of this\nstudy can serve as an essential reference point for researchers, allowing them\nto leverage the strengths, learn from the limitations, and uncover potential\nresearch opportunities enabled by ChatGPT and other generative AI models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Lixiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1\">Lele Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Linxuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Maldonado_R/0/1/0/all/0/1\">Roberto Martinez-Maldonado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueqiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasevic_D/0/1/0/all/0/1\">Dragan Ga&#x161;evi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELVIS: Empowering Locality of Vision Language Pre-training with Intra-modal Similarity. (arXiv:2304.05303v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.05303","description":"<p>Deep learning has shown great potential in assisting radiologists in reading\nchest X-ray (CXR) images, but its need for expensive annotations for improving\nperformance prevents widespread clinical application. Visual language\npre-training (VLP) can alleviate the burden and cost of annotation by\nleveraging routinely generated reports for radiographs, which exist in large\nquantities as well as in paired form (image-text pairs). Additionally,\nextensions to localization-aware VLPs are being proposed to address the needs\nfor accurate localization of abnormalities for computer-aided diagnosis (CAD)\nin CXR. However, we find that the formulation proposed by locality-aware VLP\nliterature actually leads to a loss in spatial relationships required for\ndownstream localization tasks. Therefore, we propose Empowering Locality of VLP\nwith Intra-modal Similarity, ELVIS, a VLP aware of intra-modal locality, to\nbetter preserve the locality within radiographs or reports, which enhances the\nability to comprehend location references in text reports. Our locality-aware\nVLP method significantly outperforms state-of-the art baselines in multiple\nsegmentation tasks and the MS-CXR phrase grounding task. Qualitatively, we show\nthat ELVIS focuses well on regions of interest described in the report text\ncompared to prior approaches, allowing for enhanced interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Sumin Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">JaeWoong Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tae Soo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooi_T/0/1/0/all/0/1\">Thijs Kooi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.08649","description":"<p>Models based on bidirectional encoder representations from transformers\n(BERT) produce state of the art (SOTA) results on many natural language\nprocessing (NLP) tasks such as named entity recognition (NER), part-of-speech\n(POS) tagging etc. An interesting phenomenon occurs when classifying long\ndocuments such as those from the US supreme court where BERT-based models can\nbe considered difficult to use on a first-pass or out-of-the-box basis. In this\npaper, we experiment with several BERT-based classification techniques for US\nsupreme court decisions or supreme court database (SCDB) and compare them with\nthe previous SOTA results. We then compare our results specifically with SOTA\nmodels for long documents. We compare our results for two classification tasks:\n(1) a broad classification task with 15 categories and (2) a fine-grained\nclassification task with 279 categories. Our best result produces an accuracy\nof 80\\% on the 15 broad categories and 60\\% on the fine-grained 279 categories\nwhich marks an improvement of 8\\% and 28\\% respectively from previously\nreported SOTA results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vatsal_S/0/1/0/all/0/1\">Shubham Vatsal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyers_A/0/1/0/all/0/1\">Adam Meyers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1\">John E. Ortega</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards autonomous system: flexible modular production system enhanced with large language model agents. (arXiv:2304.14721v4 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2304.14721","description":"<p>In this paper, we present a novel framework that combines large language\nmodels (LLMs), digital twins and industrial automation system to enable\nintelligent planning and control of production processes. We retrofit the\nautomation system for a modular production facility and create executable\ncontrol interfaces of fine-granular functionalities and coarse-granular skills.\nLow-level functionalities are executed by automation components, and high-level\nskills are performed by automation modules. Subsequently, a digital twin system\nis developed, registering these interfaces and containing additional\ndescriptive information about the production system. Based on the retrofitted\nautomation system and the created digital twins, LLM-agents are designed to\ninterpret descriptive information in the digital twins and control the physical\nsystem through service interfaces. These LLM-agents serve as intelligent agents\non different levels within an automation system, enabling autonomous planning\nand control of flexible production. Given a task instruction as input, the\nLLM-agents orchestrate a sequence of atomic functionalities and skills to\naccomplish the task. We demonstrate how our implemented prototype can handle\nun-predefined tasks, plan a production process, and execute the operations.\nThis research highlights the potential of integrating LLMs into industrial\nautomation systems in the context of smart factory for more agile, flexible,\nand adaptive production processes, while it also underscores the critical\ninsights and limitations for future work. Demos at:\nhttps://github.com/YuchenXia/GPT4IndustrialAutomation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuchen Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_M/0/1/0/all/0/1\">Manthan Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jazdi_N/0/1/0/all/0/1\">Nasser Jazdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyrich_M/0/1/0/all/0/1\">Michael Weyrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information. (arXiv:2305.01788v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01788","description":"<p>Visual Word Sense Disambiguation (VWSD) is a task to find the image that most\naccurately depicts the correct sense of the target word for the given context.\nPreviously, image-text matching models often suffered from recognizing\npolysemous words. This paper introduces an unsupervised VWSD approach that uses\ngloss information of an external lexical knowledge-base, especially the sense\ndefinitions. Specifically, we suggest employing Bayesian inference to\nincorporate the sense definitions when sense information of the answer is not\nprovided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we\npropose a context-aware definition generation with GPT-3. Experimental results\nshow that the VWSD performance significantly increased with our Bayesian\ninference-based approach. In addition, our context-aware definition generation\nachieved prominent performance improvement in OOD examples exhibiting better\nperformance than the existing definition generation method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1\">Sunjae Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garodia_R/0/1/0/all/0/1\">Rishabh Garodia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minhwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MGR: Multi-generator Based Rationalization. (arXiv:2305.04492v8 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.04492","description":"<p>Rationalization is to employ a generator and a predictor to construct a\nself-explaining NLP model in which the generator selects a subset of\nhuman-intelligible pieces of the input text to the following predictor.\nHowever, rationalization suffers from two key challenges, i.e., spurious\ncorrelation and degeneration, where the predictor overfits the spurious or\nmeaningless pieces solely selected by the not-yet well-trained generator and in\nturn deteriorates the generator. Although many studies have been proposed to\naddress the two challenges, they are usually designed separately and do not\ntake both of them into account. In this paper, we propose a simple yet\neffective method named MGR to simultaneously solve the two problems. The key\nidea of MGR is to employ multiple generators such that the occurrence stability\nof real pieces is improved and more meaningful pieces are delivered to the\npredictor. Empirically, we show that MGR improves the F1 score by up to 20.9%\nas compared to state-of-the-art methods. Codes are available at\nhttps://github.com/jugechengzi/Rationalization-MGR .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haozhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuankai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model. (arXiv:2305.11435v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2305.11435","description":"<p>In this paper, we show that representations capturing syllabic units emerge\nwhen training a self-supervised speech model with a visually-grounded training\nobjective. We demonstrate that a nearly identical model architecture (HuBERT)\ntrained with a masked language modeling loss does not exhibit this same\nability, suggesting that the visual grounding objective is responsible for the\nemergence of this phenomenon. We propose the use of a minimum cut algorithm to\nautomatically predict syllable boundaries in speech, followed by a 2-stage\nclustering method to group identical syllables together. We show that our model\nnot only outperforms a state-of-the-art syllabic segmentation method on the\nlanguage it was trained on (English), but also generalizes in a zero-shot\nfashion to Estonian. Finally, we show that the same model is capable of\nzero-shot generalization for a word segmentation task on 4 other languages from\nthe Zerospeech Challenge, in some cases beating the previous state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents. (arXiv:2305.13040v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13040","description":"<p>Task-oriented dialogue (TOD) models have made significant progress in recent\nyears. However, previous studies primarily focus on datasets written by\nannotators, which has resulted in a gap between academic research and\nreal-world spoken conversation scenarios. While several small-scale spoken TOD\ndatasets are proposed to address robustness issues such as ASR errors, they\nignore the unique challenges in spoken conversation. To tackle the limitations,\nwe introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,\ncontaining 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from\nhuman-to-human spoken conversations. SpokenWOZ further incorporates common\nspoken characteristics such as word-by-word processing and reasoning in spoken\nlanguage. Based on these characteristics, we present cross-turn slot and\nreasoning slot detection as new challenges. We conduct experiments on various\nbaselines, including text-modal models, newly proposed dual-modal models, and\nLLMs, e.g., ChatGPT. The results show that the current models still have\nsubstantial room for improvement in spoken conversation, where the most\nadvanced dialogue state tracker only achieves 25.65% in joint goal accuracy and\nthe SOTA end-to-end model only correctly completes the user request in 52.1% of\ndialogues. The dataset, code, and leaderboard are available:\nhttps://spokenwoz.github.io/SpokenWOZ-github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shuzheng Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wentao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Haoyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yinpei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hangyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art. (arXiv:2305.16259v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16259","description":"<p>The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural\nLanguage Processing (NLP) during the past decade. However, the demands of long\ndocument analysis are quite different from those of shorter texts, while the\never increasing size of documents uploaded on-line renders automated\nunderstanding of lengthy texts a critical issue. Relevant applications include\nautomated Web mining, legal document review, medical records analysis,\nfinancial reports analysis, contract management, environmental impact\nassessment, news aggregation, etc. Despite the relatively recent development of\nefficient algorithms for analyzing long documents, practical tools in this\nfield are currently flourishing. This article serves as an entry point into\nthis dynamic domain and aims to achieve two objectives. Firstly, it provides an\noverview of the relevant neural building blocks, serving as a concise tutorial\nfor the field. Secondly, it offers a brief examination of the current\nstate-of-the-art in long document NLP, with a primary focus on two key tasks:\ndocument classification and document summarization. Sentiment analysis for long\ntexts is also covered, since it is typically treated as a particular case of\ndocument classification. Consequently, this article presents an introductory\nexploration of document-level analysis, addressing the primary challenges,\nconcerns, and existing solutions. Finally, the article presents publicly\navailable annotated datasets that can facilitate further research in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsirmpas_D/0/1/0/all/0/1\">Dimitrios Tsirmpas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkionis_I/0/1/0/all/0/1\">Ioannis Gkionis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mademlis_I/0/1/0/all/0/1\">Ioannis Mademlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_G/0/1/0/all/0/1\">Georgios Papadopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Emotion Experiencer Recognition. (arXiv:2305.16731v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16731","description":"<p>The most prominent subtask in emotion analysis is emotion classification; to\nassign a category to a textual unit, for instance a social media post. Many\nresearch questions from the social sciences do, however, not only require the\ndetection of the emotion of an author of a post but to understand who is\nascribed an emotion in text. This task is tackled by emotion role labeling\nwhich aims at extracting who is described in text to experience an emotion,\nwhy, and towards whom. This could, however, be considered overly sophisticated\nif the main question to answer is who feels which emotion. A targeted approach\nfor such setup is to classify emotion experiencer mentions (aka \"emoters\")\nregarding the emotion they presumably perceive. This task is similar to named\nentity recognition of person names with the difference that not every mentioned\nentity name is an emoter. While, very recently, data with emoter annotations\nhas been made available, no experiments have yet been performed to detect such\nmentions. With this paper, we provide baseline experiments to understand how\nchallenging the task is. We further evaluate the impact on experiencer-specific\nemotion categorization and appraisal detection in a pipeline, when gold\nmentions are not available. We show that experiencer detection in text is a\nchallenging task, with a precision of .82 and a recall of .56 (F1 =.66). These\nresults motivate future work of jointly modeling emoter spans and\nemotion/appraisal predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wegge_M/0/1/0/all/0/1\">Maximilian Wegge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.03341","description":"<p>We introduce Inference-Time Intervention (ITI), a technique designed to\nenhance the truthfulness of large language models (LLMs). ITI operates by\nshifting model activations during inference, following a set of directions\nacross a limited number of attention heads. This intervention significantly\nimproves the performance of LLaMA models on the TruthfulQA benchmark. On an\ninstruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from\n32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and\ndemonstrate how to balance it by tuning the intervention strength. ITI is\nminimally invasive and computationally inexpensive. Moreover, the technique is\ndata efficient: while approaches like RLHF require extensive annotations, ITI\nlocates truthful directions using only few hundred examples. Our findings\nsuggest that LLMs may have an internal representation of the likelihood of\nsomething being true, even as they produce falsehoods on the surface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kenneth Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_O/0/1/0/all/0/1\">Oam Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viegas_F/0/1/0/all/0/1\">Fernanda Vi&#xe9;gas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1\">Martin Wattenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.14096","description":"<p>Entity-level fine-grained sentiment analysis in the financial domain is a\ncrucial subtask of sentiment analysis and currently faces numerous challenges.\nThe primary challenge stems from the lack of high-quality and large-scale\nannotated corpora specifically designed for financial text sentiment analysis,\nwhich in turn limits the availability of data necessary for developing\neffective text processing techniques. Recent advancements in large language\nmodels (LLMs) have yielded remarkable performance in natural language\nprocessing tasks, primarily centered around language pattern matching. In this\npaper, we propose a novel and extensive Chinese fine-grained financial\nsentiment analysis dataset, FinChina SA, for enterprise early warning. We\nthoroughly evaluate and experiment with well-known existing open-source LLMs\nusing our dataset. We firmly believe that our dataset will serve as a valuable\nresource to advance the exploration of real-world financial sentiment analysis\ntasks, which should be the focus of future research. The FinChina SA dataset is\npublicly available at https://github.com/YerayL/FinChina-SA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yinyu Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanru Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Weiqiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youhao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a Biomedical Expert? -- Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks. (arXiv:2306.16108v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.16108","description":"<p>We assessed the performance of commercial Large Language Models (LLMs)\nGPT-3.5-Turbo and GPT-4 on tasks from the 2023 BioASQ challenge. In Task 11b\nPhase B, which is focused on answer generation, both models demonstrated\ncompetitive abilities with leading systems. Remarkably, they achieved this with\nsimple zero-shot learning, grounded with relevant snippets. Even without\nrelevant snippets, their performance was decent, though not on par with the\nbest systems. Interestingly, the older and cheaper GPT-3.5-Turbo system was\nable to compete with GPT-4 in the grounded Q&amp;A setting on factoid and list\nanswers. In Task 11b Phase A, focusing on retrieval, query expansion through\nzero-shot learning improved performance, but the models fell short compared to\nother systems. The code needed to rerun these experiments is available through\nGitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ateia_S/0/1/0/all/0/1\">Samy Ateia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruschwitz_U/0/1/0/all/0/1\">Udo Kruschwitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved NL2SQL based on Multi-layer Expert Network. (arXiv:2306.17727v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.17727","description":"<p>The Natural Language to SQL (NL2SQL) technique is used to convert natural\nlanguage queries into executable SQL statements. Typically, slot-filling is\nemployed as a classification method for multi-task cases to achieve this goal.\nHowever, slot-filling can result in inaccurate SQL statement generation due to\nnegative migration issues arising from different classification tasks. To\novercome this limitation, this study introduces a new approach called\nMulti-Layer Expert Generate SQL (MLEG-SQL), which utilizes a dedicated\nmulti-task hierarchical network. The lower layer of the network extracts\nsemantic features of natural language statements, while the upper layer builds\na specialized expert system for handling specific classification tasks. This\nhierarchical approach mitigates performance degradation resulting from\ndifferent task conflicts. The proposed method was evaluated on the WiKSQL\ndataset and was found to be effective in generating accurate SQL statements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1\">Chenduo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection. (arXiv:2307.02591v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.02591","description":"<p>Opioid related aberrant behaviors (ORAB) present novel risk factors for\nopioid overdose. Previously, ORAB have been mainly assessed by survey results\nand by monitoring drug administrations. Such methods however, cannot scale up\nand do not cover the entire spectrum of aberrant behaviors. On the other hand,\nORAB are widely documented in electronic health record notes. This paper\nintroduces a novel biomedical natural language processing benchmark dataset\nnamed ODD, for ORAB Detection Dataset. ODD is an expert-annotated dataset\ncomprising of more than 750 publicly available EHR notes. ODD has been designed\nto identify ORAB from patients' EHR notes and classify them into nine\ncategories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior, 3)\nOpioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiapines, 7)\nMedication Changes, 8) Central Nervous System-related, and 9) Social\nDeterminants of Health. We explored two state-of-the-art natural language\nprocessing (NLP) models (finetuning pretrained language models and\nprompt-tuning approaches) to identify ORAB. Experimental results show that the\nprompt-tuning models outperformed the finetuning models in most cateogories and\nthe gains were especially higher among uncommon categories (Suggested aberrant\nbehavior, Diagnosed opioid dependency and Medication change). Although the best\nmodel achieved the highest 83.92% on area under precision recall curve,\nuncommon classes (Suggested Aberrant Behavior, Diagnosed Opioid Dependence, and\nMedication Change) still have a large room for performance improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1\">Sunjae Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weisong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Druhl_E/0/1/0/all/0/1\">Emily Druhl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Minhee L. Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reisman_J/0/1/0/all/0/1\">Joel I. Reisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerns_R/0/1/0/all/0/1\">Robert D. Kerns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_W/0/1/0/all/0/1\">William Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.03104","description":"<p>Sentence embeddings enable us to capture the semantic similarity of short\ntexts. Most sentence embedding models are trained for general semantic textual\nsimilarity (STS) tasks. Therefore, to use sentence embeddings in a particular\ndomain, the model must be adapted to it in order to achieve good results.\nUsually, this is done by fine-tuning the entire sentence embedding model for\nthe domain of interest. While this approach yields state-of-the-art results,\nall of the model's weights are updated during fine-tuning, making this method\nresource-intensive. Therefore, instead of fine-tuning entire sentence embedding\nmodels for each target domain individually, we propose to train lightweight\nadapters. These domain-specific adapters do not require fine-tuning all\nunderlying sentence embedding model parameters. Instead, we only train a small\nnumber of additional parameters while keeping the weights of the underlying\nsentence embedding model fixed. Training domain-specific adapters allows always\nusing the same base model and only exchanging the domain-specific adapters to\nadapt sentence embeddings to a specific domain. We show that using adapters for\nparameter-efficient domain adaptation of sentence embeddings yields competitive\nperformance within 1% of a domain-adapted, entirely fine-tuned sentence\nembedding model while only training approximately 3.6% of the parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1\">Tim Schopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_D/0/1/0/all/0/1\">Dennis Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Apples to Apples: Generating Aspect-Aware Comparative Sentences from User Reviews. (arXiv:2307.03691v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.03691","description":"<p>It is time-consuming to find the best product among many similar\nalternatives. Comparative sentences can help to contrast one item from others\nin a way that highlights important features of an item that stand out. Given\nreviews of one or multiple items and relevant item features, we generate\ncomparative review sentences to aid users to find the best fit. Specifically,\nour model consists of three successive components in a transformer: (i) an item\nencoding module to encode an item for comparison, (ii) a comparison generation\nmodule that generates comparative sentences in an autoregressive manner, (iii)\na novel decoding method for user personalization. We show that our pipeline\ngenerates fluent and diverse comparative sentences. We run experiments on the\nrelevance and fidelity of our generated sentences in a human evaluation study\nand find that our algorithm creates comparative review sentences that are\nrelevant and truthful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Echterhoff_J/0/1/0/all/0/1\">Jessica Echterhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1\">An Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad. (arXiv:2307.04827v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2307.04827","description":"<p>Launchpad is a musical instrument that allows users to create and perform\nmusic by pressing illuminated buttons. To assist and inspire the design of the\nLaunchpad light effect, and provide a more accessible approach for beginners to\ncreate music visualization with this instrument, we proposed the LaunchpadGPT\nmodel to generate music visualization designs on Launchpad automatically. Based\non the language model with excellent generation ability, our proposed\nLaunchpadGPT takes an audio piece of music as input and outputs the lighting\neffects of Launchpad-playing in the form of a video (Launchpad-playing video).\nWe collect Launchpad-playing videos and process them to obtain music and\ncorresponding video frame of Launchpad-playing as prompt-completion pairs, to\ntrain the language model. The experiment result shows the proposed method can\ncreate better music visualization than random generation methods and hold the\npotential for a broader range of music visualization applications. Our code is\navailable at https://github.com/yunlong10/LaunchpadGPT/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Siting Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yunlong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge. (arXiv:2307.07851v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.07851","description":"<p>Generic sentence embeddings provide a coarse-grained approximation of\nsemantic textual similarity but ignore specific aspects that make texts\nsimilar. Conversely, aspect-based sentence embeddings provide similarities\nbetween texts based on certain predefined aspects. Thus, similarity predictions\nof texts are more targeted to specific requirements and more easily\nexplainable. In this paper, we present AspectCSE, an approach for aspect-based\ncontrastive learning of sentence embeddings. Results indicate that AspectCSE\nachieves an average improvement of 3.97% on information retrieval tasks across\nmultiple aspects compared to the previous best results. We also propose using\nWikidata knowledge graph properties to train models of multi-aspect sentence\nembeddings in which multiple specific aspects are simultaneously considered\nduring similarity predictions. We demonstrate that multi-aspect embeddings\noutperform single-aspect embeddings on aspect-specific information retrieval\ntasks. Finally, we examine the aspect-based sentence embedding space and\ndemonstrate that embeddings of semantically similar aspect labels are often\nclose, even without explicit similarity training between different aspect\nlabels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1\">Tim Schopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerber_E/0/1/0/all/0/1\">Emanuel Gerber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorff_M/0/1/0/all/0/1\">Malte Ostendorff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disco-Bench: A Discourse-Aware Evaluation Benchmark for Language Modelling. (arXiv:2307.08074v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.08074","description":"<p>Modeling discourse -- the linguistic phenomena that go beyond individual\nsentences, is a fundamental yet challenging aspect of natural language\nprocessing (NLP). However, existing evaluation benchmarks primarily focus on\nthe evaluation of inter-sentence properties and overlook critical discourse\nphenomena that cross sentences. To bridge the gap, we propose Disco-Bench, a\nbenchmark that can evaluate intra-sentence discourse properties across a\ndiverse set of NLP tasks, covering understanding, translation, and generation.\nDisco-Bench consists of 9 document-level testsets in the literature domain,\nwhich contain rich discourse phenomena (e.g. cohesion and coherence) in Chinese\nand/or English. For linguistic analysis, we also design a diagnostic test suite\nthat can examine whether the target models learn discourse knowledge. We\ntotally evaluate 20 general-, in-domain and commercial models based on\nTransformer, advanced pretraining architectures and large language models\n(LLMs). Our results show (1) the challenge and necessity of our evaluation\nbenchmark; (2) fine-grained pretraining based on literary document-level\ntraining data consistently improves the modeling of discourse information. We\nwill release the datasets, pretrained models, and leaderboard, which we hope\ncan significantly facilitate research in this field:\nhttps://github.com/longyuewangdcu/Disco-Bench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zefeng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Donghuai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Mathematical Derivations with Large Language Models. (arXiv:2307.09998v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.09998","description":"<p>The derivation of mathematical results in specialised fields using Large\nLanguage Models (LLMs) is an emerging research direction that can help identify\nmodels' limitations, and potentially support mathematical discovery. In this\npaper, we leverage a symbolic engine to generate derivations of equations at\nscale, and investigate the capabilities of LLMs when deriving goal equations\nfrom premises. Specifically, we employ in-context learning for GPT and\nfine-tune a range of T5 models to compare the robustness and generalisation of\npre-training strategies to specialised models. Empirical results show that\nfine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and\nout-of-distribution test sets in terms of absolute performance. However, an\nin-depth analysis reveals that the fine-tuned models are more sensitive to\nperturbations involving unseen symbols and (to a lesser extent) changes to\nequation structure. In addition, we analyse 1.7K equations and over 200\nderivations to highlight common reasoning errors such as the inclusion of\nincorrect, irrelevant, and redundant equations, along with the tendency to skip\nderivation steps. Finally, we explore the suitability of existing metrics for\nevaluating mathematical derivations finding evidence that, while they capture\ngeneral properties such as sensitivity to perturbations, they fail to highlight\nfine-grained reasoning errors and essential differences between models.\nOverall, this work demonstrates that training models on synthetic data can\nimprove their mathematical capabilities beyond larger architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meadows_J/0/1/0/all/0/1\">Jordan Meadows</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on Fertility Proposals Using Multi-Grained Topic Analysis Methods. (arXiv:2307.10025v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2307.10025","description":"<p>Fertility issues are closely related to population security, in 60 years\nChina's population for the first time in a negative growth trend, the change of\nfertility policy is of great concern to the community. 2023 \"two sessions\"\nproposal \"suggests that the country in the form of legislation, the birth of\nthe registration of the cancellation of the marriage restriction\" This topic\nwas once a hot topic on the Internet, and \"unbundling\" the relationship between\nbirth registration and marriage has become the focus of social debate. In this\npaper, we adopt co-occurrence semantic analysis, topic analysis and sentiment\nanalysis to conduct multi-granularity semantic analysis of microblog comments.\nIt is found that the discussion on the proposal of \"removing marriage\nrestrictions from birth registration\" involves the individual, society and the\nstate at three dimensions, and is detailed into social issues such as personal\nbehaviour, social ethics and law, and national policy, with people's sentiment\ninclined to be negative in most of the topics. Based on this, eight proposals\nwere made to provide a reference for governmental decision making and to form a\nreference method for researching public opinion on political issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yulin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning. (arXiv:2307.10234v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.10234","description":"<p>This study presents a thorough examination of various Generative Pretrained\nTransformer (GPT) methodologies in sentiment analysis, specifically in the\ncontext of Task 4 on the SemEval 2017 dataset. Three primary strategies are\nemployed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2)\nfine-tuning GPT models, and 3) an inventive approach to embedding\nclassification. The research yields detailed comparative insights among these\nstrategies and individual GPT models, revealing their unique strengths and\npotential limitations. Additionally, the study compares these GPT-based\nmethodologies with other current, high-performing models previously used with\nthe same dataset. The results illustrate the significant superiority of the GPT\napproaches in terms of predictive performance, more than 22\\% in F1-score\ncompared to the state-of-the-art. Further, the paper sheds light on common\nchallenges in sentiment analysis tasks, such as understanding context and\ndetecting sarcasm. It underscores the enhanced capabilities of the GPT models\nto effectively handle these complexities. Taken together, these findings\nhighlight the promising potential of GPT models in sentiment analysis, setting\nthe stage for future research in this field. The code can be found at\nhttps://github.com/DSAatUSU/SentimentGPT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kheiri_K/0/1/0/all/0/1\">Kiana Kheiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimi_H/0/1/0/all/0/1\">Hamid Karimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model. (arXiv:2307.10443v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.10443","description":"<p>Despite the significant progress made by transformer models in machine\nreading comprehension tasks, they still fall short in handling complex\nreasoning tasks due to the absence of explicit knowledge in the input sequence.\nTo address this limitation, many recent works have proposed injecting external\nknowledge into the model. However, selecting relevant external knowledge,\nensuring its availability, and requiring additional processing steps remain\nchallenging. In this paper, we introduce a novel attention pattern that\nintegrates reasoning knowledge derived from a heterogeneous graph into the\ntransformer architecture without relying on external knowledge. The proposed\nattention pattern comprises three key elements: global-local attention for word\ntokens, graph attention for entity tokens that exhibit strong attention towards\ntokens connected in the graph as opposed to those unconnected, and the\nconsideration of the type of relationship between each entity token and word\ntoken. This results in optimized attention between the two if a relationship\nexists. The pattern is coupled with special relative position labels, allowing\nit to integrate with LUKE's entity-aware self-attention mechanism. The\nexperimental findings corroborate that our model outperforms both the\ncutting-edge LUKE-Graph and the baseline LUKE model on the ReCoRD dataset that\nfocuses on commonsense reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foolad_S/0/1/0/all/0/1\">Shima Foolad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiani_K/0/1/0/all/0/1\">Kourosh Kiani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v3 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2307.10490","description":"<p>We demonstrate how images and sounds can be used for indirect prompt and\ninstruction injection in multi-modal LLMs. An attacker generates an adversarial\nperturbation corresponding to the prompt and blends it into an image or audio\nrecording. When the user asks the (unmodified, benign) model about the\nperturbed image or audio, the perturbation steers the model to output the\nattacker-chosen text and/or make the subsequent dialog follow the attacker's\ninstruction. We illustrate this attack with several proof-of-concept examples\ntargeting LLaVa and PandaGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1\">Eugene Bagdasaryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_T/0/1/0/all/0/1\">Tsung-Yin Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassi_B/0/1/0/all/0/1\">Ben Nassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1\">Vitaly Shmatikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation. (arXiv:2307.11019v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.11019","description":"<p>Knowledge-intensive tasks (e.g., open-domain question answering (QA)) require\na substantial amount of factual knowledge and often rely on external\ninformation for assistance. Recently, large language models (LLMs) (e.g.,\nChatGPT), have demonstrated impressive prowess in solving a wide range of tasks\nwith world knowledge, including knowledge-intensive tasks. However, it remains\nunclear how well LLMs are able to perceive their factual knowledge boundaries,\nparticularly how they behave when incorporating retrieval augmentation. In this\nstudy, we present an initial analysis of the factual knowledge boundaries of\nLLMs and how retrieval augmentation affects LLMs on open-domain QA. Specially,\nwe focus on three primary research questions and analyze them by examining QA\nperformance, priori judgement and posteriori judgement of LLMs. We show\nevidence that LLMs possess unwavering confidence in their capabilities to\nrespond to questions and the accuracy of their responses. Furthermore,\nretrieval augmentation proves to be an effective approach in enhancing LLMs'\nawareness of knowledge boundaries, thereby improving their judgemental\nabilities. Additionally, we also find that LLMs have a propensity to rely on\nthe provided retrieval results when formulating answers, while the quality of\nthese results significantly impacts their reliance. The code to reproduce this\nwork is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yingqi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CausE: Towards Causal Knowledge Graph Embedding. (arXiv:2307.11610v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.11610","description":"<p>Knowledge graph embedding (KGE) focuses on representing the entities and\nrelations of a knowledge graph (KG) into the continuous vector spaces, which\ncan be employed to predict the missing triples to achieve knowledge graph\ncompletion (KGC). However, KGE models often only briefly learn structural\ncorrelations of triple data and embeddings would be misled by the trivial\npatterns and noisy links in real-world KGs. To address this issue, we build the\nnew paradigm of KGE in the context of causality and embedding disentanglement.\nWe further propose a Causality-enhanced knowledge graph Embedding (CausE)\nframework. CausE employs causal intervention to estimate the causal effect of\nthe confounder embeddings and design new training objectives to make stable\npredictions. Experimental results demonstrate that CausE could outperform the\nbaseline models and achieve state-of-the-art KGC performance. We release our\ncode in https://github.com/zjukg/CausE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/2307.08303","description":"<p>Dense retrieval (DR) converts queries and documents into dense embeddings and\nmeasures the similarity between queries and documents in vector space. One of\nthe challenges in DR is the lack of domain-specific training data. While DR\nmodels can learn from large-scale public datasets like MS MARCO through\ntransfer learning, evidence shows that not all DR models and domains can\nbenefit from transfer learning equally. Recently, some researchers have\nresorted to large language models (LLMs) to improve the zero-shot and few-shot\nDR models. However, the hard prompts or human-written prompts utilized in these\nworks cannot guarantee the good quality of generated weak queries. To tackle\nthis, we propose soft prompt tuning for augmenting DR (SPTAR): For each task,\nwe leverage soft prompt-tuning to optimize a task-specific soft prompt on\nlimited ground truth data and then prompt the LLMs to tag unlabeled documents\nwith weak queries, yielding enough weak document-query pairs to train\ntask-specific dense retrievers. We design a filter to select high-quality\nexample document-query pairs in the prompt to further improve the quality of\nweak tagged queries. To the best of our knowledge, there is no prior work\nutilizing soft prompt tuning to augment DR models. The experiments demonstrate\nthat SPTAR outperforms the unsupervised baselines BM25 and the recently\nproposed LLMs-based augmentation method for DR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhiyuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?. (arXiv:2307.10719v1 [cs.AI] CROSS LISTED)","link":"http://arxiv.org/abs/2307.10719","description":"<p>Large language models (LLMs) have exhibited impressive capabilities in\ncomprehending complex instructions. However, their blind adherence to provided\ninstructions has led to concerns regarding risks of malicious use. Existing\ndefence mechanisms, such as model fine-tuning or output censorship using LLMs,\nhave proven to be fallible, as LLMs can still generate problematic responses.\nCommonly employed censorship approaches treat the issue as a machine learning\nproblem and rely on another LM to detect undesirable content in LLM outputs. In\nthis paper, we present the theoretical limitations of such semantic censorship\napproaches. Specifically, we demonstrate that semantic censorship can be\nperceived as an undecidable problem, highlighting the inherent challenges in\ncensorship that arise due to LLMs' programmatic and instruction-following\ncapabilities. Furthermore, we argue that the challenges extend beyond semantic\ncensorship, as knowledgeable attackers can reconstruct impermissible outputs\nfrom a collection of permissible ones. As a result, we propose that the problem\nof censorship needs to be reevaluated; it should be treated as a security\nproblem which warrants the adaptation of security-based approaches to mitigate\npotential risks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glukhov_D/0/1/0/all/0/1\">David Glukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1\">Ilia Shumailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1\">Nicolas Papernot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papyan_V/0/1/0/all/0/1\">Vardan Papyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-07-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}