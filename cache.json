{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-05T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Using Language Models on Low-end Hardware. (arXiv:2305.02350v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02350","description":"<p>This paper evaluates the viability of using fixed language models for\ntraining text classification networks on low-end hardware. We combine language\nmodels with a CNN architecture and put together a comprehensive benchmark with\n8 datasets covering single-label and multi-label classification of topic,\nsentiment, and genre. Our observations are distilled into a list of trade-offs,\nconcluding that there are scenarios, where not fine-tuning a language model\nyields competitive effectiveness at faster training, requiring only a quarter\nof the memory compared to fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziegner_F/0/1/0/all/0/1\">Fabian Ziegner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borst_J/0/1/0/all/0/1\">Janos Borst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity Tracking in Language Models. (arXiv:2305.02363v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02363","description":"<p>Keeping track of how states and relations of entities change as a text or\ndialog unfolds is a key prerequisite to discourse understanding. Despite this\nfact, there have been few systematic investigations into the ability of large\nlanguage models (LLMs) to track discourse entities. In this work, we present a\ntask to probe to what extent a language model can infer the final state of an\nentity given an English description of the initial state and a series of\nstate-changing operations. We use this task to first investigate whether\nFlan-T5, GPT-3 and GPT-3.5 can track the state of entities, and find that only\nGPT-3.5 models, which have been pretrained on large amounts of code, exhibit\nthis ability. We then investigate whether smaller models pretrained primarily\non text can learn to track entities, through finetuning T5 on several\ntraining/evaluation splits. While performance degrades for more complex splits,\nwe find that even for splits with almost no lexical overlap between training\nand evaluation, a finetuned model can often perform non-trivial entity\ntracking. Taken together, these results suggest that language models can learn\nto track entities but pretraining on large text corpora alone does not make\nthis capacity surface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Najoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_S/0/1/0/all/0/1\">Sebastian Schuster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives. (arXiv:2305.02364v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02364","description":"<p>Sustaining coherent and engaging narratives requires dialogue or storytelling\nagents to understand how the personas of speakers or listeners ground the\nnarrative. Specifically, these agents must infer personas of their listeners to\nproduce statements that cater to their interests. They must also learn to\nmaintain consistent speaker personas for themselves throughout the narrative,\nso that their counterparts feel involved in a realistic conversation or story.\n</p>\n<p>However, personas are diverse and complex: they entail large quantities of\nrich interconnected world knowledge that is challenging to robustly represent\nin general narrative systems (e.g., a singer is good at singing, and may have\nattended conservatoire). In this work, we construct a new large-scale persona\ncommonsense knowledge graph, PeaCoK, containing ~100K human-validated persona\nfacts. Our knowledge graph schematizes five dimensions of persona knowledge\nidentified in previous studies of human interactive behaviours, and distils\nfacts in this schema from both existing commonsense knowledge graphs and\nlarge-scale pretrained language models. Our analysis indicates that PeaCoK\ncontains rich and precise world persona inferences that help downstream systems\ngenerate more consistent and engaging narratives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Silin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borges_B/0/1/0/all/0/1\">Beatriz Borges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Soyoung Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayazit_D/0/1/0/all/0/1\">Deniz Bayazit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanno_S/0/1/0/all/0/1\">Saya Kanno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wakaki_H/0/1/0/all/0/1\">Hiromi Wakaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitsufuji_Y/0/1/0/all/0/1\">Yuki Mitsufuji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Plagiarism Detection Approach Combining BERT-based Word Embedding, Attention-based LSTMs and an Improved Differential Evolution Algorithm. (arXiv:2305.02374v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02374","description":"<p>Detecting plagiarism involves finding similar items in two different sources.\nIn this article, we propose a novel method for detecting plagiarism that is\nbased on attention mechanism-based long short-term memory (LSTM) and\nbidirectional encoder representations from transformers (BERT) word embedding,\nenhanced with optimized differential evolution (DE) method for pre-training and\na focal loss function for training. BERT could be included in a downstream task\nand fine-tuned as a task-specific BERT can be included in a downstream task and\nfine-tuned as a task-specific structure, while the trained BERT model is\ncapable of detecting various linguistic characteristics. Unbalanced\nclassification is one of the primary issues with plagiarism detection. We\nsuggest a focal loss-based training technique that carefully learns minority\nclass instances to solve this. Another issue that we tackle is the training\nphase itself, which typically employs gradient-based methods like\nback-propagation for the learning process and thus suffers from some drawbacks,\nincluding sensitivity to initialization. To initiate the BP process, we suggest\na novel DE algorithm that makes use of a clustering-based mutation operator.\nHere, a winning cluster is identified for the current DE population, and a\nfresh updating method is used to produce potential answers. We evaluate our\nproposed approach on three benchmark datasets ( MSRP, SNLI, and SemEval2014)\nand demonstrate that it performs well when compared to both conventional and\npopulation-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moravvej_S/0/1/0/all/0/1\">Seyed Vahid Moravvej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousavirad_S/0/1/0/all/0/1\">Seyed Jalaleddin Mousavirad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_D/0/1/0/all/0/1\">Diego Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_F/0/1/0/all/0/1\">Fardin Mohammadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approximating CKY with Transformers. (arXiv:2305.02386v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02386","description":"<p>We investigate the ability of transformer models to approximate the CKY\nalgorithm, using them to directly predict a parse and thus avoid the CKY\nalgorithm's cubic dependence on sentence length. We find that on standard\nconstituency parsing benchmarks this approach achieves competitive or better\nperformance than comparable parsers that make use of CKY, while being faster.\nWe also evaluate the viability of this approach for parsing under random PCFGs.\nHere we find that performance declines as the grammar becomes more ambiguous,\nsuggesting that the transformer is not fully capturing the CKY computation.\nHowever, we also find that incorporating additional inductive bias is helpful,\nand we propose a novel approach that makes use of gradients with respect to\nchart representations in predicting the parse, in analogy with the CKY\nalgorithm being the subgradient of a partition function variant with respect to\nthe chart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalighinejad_G/0/1/0/all/0/1\">Ghazal Khalighinejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_O/0/1/0/all/0/1\">Ollie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1\">Sam Wiseman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defending against Insertion-based Textual Backdoor Attacks via Attribution. (arXiv:2305.02394v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02394","description":"<p>Textual backdoor attack, as a novel attack model, has been shown to be\neffective in adding a backdoor to the model during training. Defending against\nsuch backdoor attacks has become urgent and important. In this paper, we\npropose AttDef, an efficient attribution-based pipeline to defend against two\ninsertion-based poisoning attacks, BadNL and InSent. Specifically, we regard\nthe tokens with larger attribution scores as potential triggers since larger\nattribution words contribute more to the false prediction results and therefore\nare more likely to be poison triggers. Additionally, we further utilize an\nexternal pre-trained language model to distinguish whether input is poisoned or\nnot. We show that our proposed method can generalize sufficiently well in two\ncommon attack scenarios (poisoning training data and testing data), which\nconsistently improves previous methods. For instance, AttDef can successfully\nmitigate both attacks with an average accuracy of 79.97% (56.59% up) and 48.34%\n(3.99% up) under pre-training and post-training attack defense respectively,\nachieving the new state-of-the-art performance on prediction recovery over four\nbenchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiazhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhuofeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vydiswaran_V/0/1/0/all/0/1\">V.G. Vinod Vydiswaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents. (arXiv:2305.02412v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02412","description":"<p>Pre-trained large language models (LLMs) capture procedural knowledge about\nthe world. Recent work has leveraged LLM's ability to generate abstract plans\nto simplify challenging control tasks, either by action scoring, or action\nmodeling (fine-tuning). However, the transformer architecture inherits several\nconstraints that make it difficult for the LLM to directly serve as the agent:\ne.g. limited input lengths, fine-tuning inefficiency, bias from pre-training,\nand incompatibility with non-text environments. To maintain compatibility with\na low-level trainable actor, we propose to instead use the knowledge in LLMs to\nsimplify the control problem, rather than solving it. We propose the Plan,\nEliminate, and Track (PET) framework. The Plan module translates a task\ndescription into a list of high-level sub-tasks. The Eliminate module masks out\nirrelevant objects and receptacles from the observation for the current\nsub-task. Finally, the Track module determines whether the agent has\naccomplished each sub-task. On the AlfWorld instruction following benchmark,\nthe PET framework leads to a significant 15% improvement over SOTA for\ngeneralization to human goal specifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">So Yeon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azaria_A/0/1/0/all/0/1\">Amos Azaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1\">Tom Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1\">Shrimai Prabhumoye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer. (arXiv:2305.02423v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02423","description":"<p>Recent studies show that prompt tuning can better leverage the power of large\nlanguage models than fine-tuning on downstream natural language understanding\ntasks. However, the existing prompt tuning methods have training instability\nissues, as the variance of scores under different random seeds is quite large.\nTo address this critical problem, we first investigate and find that the loss\nlandscape of vanilla prompt tuning is precipitous when it is visualized, where\na slight change of input data can cause a big fluctuation in the loss\nlandscape. This is an essential factor that leads to the instability of prompt\ntuning. Based on this observation, we introduce perturbation-based\nregularizers, which can smooth the loss landscape, into prompt tuning. We\npropose a new algorithm, called Prompt Tuning with Perturbation-based\nregularizer~(PTP), which can not only alleviate training instability\ndramatically but also boost the performance of prompt tuning. We design two\nkinds of perturbation-based regularizers, including random-noise-based and\nadversarial-based. In particular, our proposed perturbations are flexible on\nboth text space and embedding space. Extensive experiments show the\neffectiveness of our proposed methods in stabilizing the training. Our new\nalgorithms improve the state-of-the-art prompt tuning methods by 1.94\\% and\n2.34\\% on SuperGLUE and FewGLUE benchmarks, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lichang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Minhao Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Learning on Sequence to Sequence Models. (arXiv:2305.02424v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02424","description":"<p>Backdoor learning has become an emerging research area towards building a\ntrustworthy machine learning system. While a lot of works have studied the\nhidden danger of backdoor attacks in image or text classification, there is a\nlimited understanding of the model's robustness on backdoor attacks when the\noutput space is infinite and discrete. In this paper, we study a much more\nchallenging problem of testing whether sequence-to-sequence (seq2seq) models\nare vulnerable to backdoor attacks. Specifically, we find by only injecting\n0.2\\% samples of the dataset, we can cause the seq2seq model to generate the\ndesignated keyword and even the whole sentence. Furthermore, we utilize Byte\nPair Encoding (BPE) to create multiple new triggers, which brings new\nchallenges to backdoor detection since these backdoors are not static.\nExtensive experiments on machine translation and text summarization have been\nconducted to show our proposed methods could achieve over 90\\% attack success\nrate on multiple datasets and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lichang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Minhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"evaluating bert and parsbert for analyzing persian advertisement data. (arXiv:2305.02426v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02426","description":"<p>This paper discusses the impact of the Internet on modern trading and the\nimportance of data generated from these transactions for organizations to\nimprove their marketing efforts. The paper uses the example of Divar, an online\nmarketplace for buying and selling products and services in Iran, and presents\na competition to predict the percentage of a car sales ad that would be\npublished on the Divar website. Since the dataset provides a rich source of\nPersian text data, the authors use the Hazm library, a Python library designed\nfor processing Persian text, and two state-of-the-art language models, mBERT\nand ParsBERT, to analyze it. The paper's primary objective is to compare the\nperformance of mBERT and ParsBERT on the Divar dataset. The authors provide\nsome background on data mining, Persian language, and the two language models,\nexamine the dataset's composition and statistical features, and provide details\non their fine-tuning and training configurations for both approaches. They\npresent the results of their analysis and highlight the strengths and\nweaknesses of the two language models when applied to Persian text data. The\npaper offers valuable insights into the challenges and opportunities of working\nwith low-resource languages such as Persian and the potential of advanced\nlanguage models like BERT for analyzing such data. The paper also explains the\ndata mining process, including steps such as data cleaning and normalization\ntechniques. Finally, the paper discusses the types of machine learning\nproblems, such as supervised, unsupervised, and reinforcement learning, and the\npattern evaluation techniques, such as confusion matrix. Overall, the paper\nprovides an informative overview of the use of language models and data mining\ntechniques for analyzing text data in low-resource languages, using the example\nof the Divar dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehrban_A/0/1/0/all/0/1\">Ali Mehrban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahadian_P/0/1/0/all/0/1\">Pegah Ahadian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory. (arXiv:2305.02437v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02437","description":"<p>With direct access to human-written reference as memory, retrieval-augmented\ngeneration has achieved much progress in a wide range of text generation tasks.\nSince better memory would typically prompt better generation~(we define this as\nprimal problem), previous works mainly focus on how to retrieve better memory.\nHowever, one fundamental limitation exists for current literature: the memory\nis retrieved from a fixed corpus and is bounded by the quality of the corpus.\nDue to the finite retrieval space, bounded memory would greatly limit the\npotential of the memory-augmented generation model. In this paper, by exploring\nthe duality of the primal problem: better generation also prompts better\nmemory, we propose a framework called Selfmem, which iteratively adopts a\nretrieval-augmented generator itself to generate an unbounded memory pool and\nuses a memory selector to pick one generated memory for the next generation\nround. By combining the primal and dual problem, a retrieval-augmented\ngeneration model could lift itself up with its own output in the infinite\ngeneration space. To verify our framework, we conduct extensive experiments\nacross various text generation scenarios including neural machine translation,\nabstractive summarization and dialogue generation over seven datasets and\nachieve state-of-the-art results in JRC-Acquis(four directions), XSum(50.3\nROUGE-1) and BigPatent(62.9 ROUGE-1).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Di Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying the Dissimilarity of Texts. (arXiv:2305.02457v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02457","description":"<p>Quantifying the dissimilarity of two texts is an important aspect of a number\nof natural language processing tasks, including semantic information retrieval,\ntopic classification, and document clustering. In this paper, we compared the\nproperties and performance of different dissimilarity measures $D$ using three\ndifferent representations of texts -- vocabularies, word frequency\ndistributions, and vector embeddings -- and three simple tasks -- clustering\ntexts by author, subject, and time period. Using the Project Gutenberg\ndatabase, we found that the generalised Jensen--Shannon divergence applied to\nword frequencies performed strongly across all tasks, that $D$'s based on\nvector embedding representations led to stronger performance for smaller texts,\nand that the optimal choice of approach was ultimately task-dependent. We also\ninvestigated, both analytically and numerically, the behaviour of the different\n$D$'s when the two texts varied in length by a factor $h$. We demonstrated that\nthe (natural) estimator of the Jaccard distance between vocabularies was\ninconsistent and computed explicitly the $h$-dependency of the bias of the\nestimator of the generalised Jensen--Shannon divergence applied to word\nfrequencies. We also found numerically that the Jensen--Shannon divergence and\nembedding-based approaches were robust to changes in $h$, while the Jaccard\ndistance was not.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shade_B/0/1/0/all/0/1\">Benjamin Shade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altmann_E/0/1/0/all/0/1\">Eduardo G. Altmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge. (arXiv:2305.02459v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02459","description":"<p>While transformer-based systems have enabled greater accuracies with fewer\ntraining examples, data acquisition obstacles still persist for rare-class\ntasks -- when the class label is very infrequent (e.g. &lt; 5% of samples). Active\nlearning has in general been proposed to alleviate such challenges, but choice\nof selection strategy, the criteria by which rare-class examples are chosen,\nhas not been systematically evaluated. Further, transformers enable iterative\ntransfer-learning approaches. We propose and investigate transfer- and active\nlearning solutions to the rare class problem of dissonance detection through\nutilizing models trained on closely related tasks and the evaluation of\nacquisition strategies, including a proposed probability-of-rare-class (PRC)\napproach. We perform these experiments for a specific rare class problem:\ncollecting language samples of cognitive dissonance from social media. We find\nthat PRC is a simple and effective strategy to guide annotations and ultimately\nimprove model accuracy while transfer-learning in a specific order can improve\nthe cold-start performance of the learner but does not benefit iterations of\nactive learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varadarajan_V/0/1/0/all/0/1\">Vasudha Varadarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juhng_S/0/1/0/all/0/1\">Swanie Juhng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahwish_S/0/1/0/all/0/1\">Syeda Mahwish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luby_J/0/1/0/all/0/1\">Jonah Luby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luhmann_C/0/1/0/all/0/1\">Christian Luhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H. Andrew Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction. (arXiv:2305.02466v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02466","description":"<p>A proven therapeutic technique to overcome negative thoughts is to replace\nthem with a more hopeful \"reframed thought.\" Although therapy can help people\npractice and learn this Cognitive Reframing of Negative Thoughts, clinician\nshortages and mental health stigma commonly limit people's access to therapy.\nIn this paper, we conduct a human-centered study of how language models may\nassist people in reframing negative thoughts. Based on psychology literature,\nwe define a framework of seven linguistic attributes that can be used to\nreframe a thought. We develop automated metrics to measure these attributes and\nvalidate them with expert judgements from mental health practitioners. We\ncollect a dataset of 600 situations, thoughts and reframes from practitioners\nand use it to train a retrieval-enhanced in-context learning model that\neffectively generates reframed thoughts and controls their linguistic\nattributes. To investigate what constitutes a \"high-quality\" reframe, we\nconduct an IRB-approved randomized field study on a large mental health website\nwith over 2,000 participants. Amongst other findings, we show that people\nprefer highly empathic or specific reframes, as opposed to reframes that are\noverly positive. Our findings provide key implications for the use of LMs to\nassist people in overcoming negative thoughts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ashish Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rushton_K/0/1/0/all/0/1\">Kevin Rushton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_I/0/1/0/all/0/1\">Inna Wanyin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1\">David Wadden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_K/0/1/0/all/0/1\">Khendra G. Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miner_A/0/1/0/all/0/1\">Adam S. Miner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Theresa Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althoff_T/0/1/0/all/0/1\">Tim Althoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System. (arXiv:2305.02468v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02468","description":"<p>Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks\nby tracking dialogue states and generating appropriate responses to help users\nachieve defined goals. Recently, end-to-end dialogue models pre-trained based\non large datasets have shown promising performance in the conversational\nsystem. However, they share the same parameters to train tasks of the dialogue\nsystem (NLU, DST, NLG), so debugging each task is challenging. Also, they\nrequire a lot of effort to fine-tune large parameters to create a task-oriented\nchatbot, making it difficult for non-experts to handle. Therefore, we intend to\ntrain relatively lightweight and fast models compared to PLM. In this paper, we\npropose an End-to-end TOD system with Task-Optimized Adapters which learn\nindependently per task, adding only small number of parameters after fixed\nlayers of pre-trained network. We also enhance the performance of the DST and\nNLG modules through reinforcement learning, overcoming the learning curve that\nhas lacked at the adapter learning and enabling the natural and consistent\nresponse generation that is appropriate for the goal. Our method is a\nmodel-agnostic approach and does not require prompt-tuning as only input data\nwithout a prompt. As results of the experiment, our method shows competitive\nperformance on the MultiWOZ benchmark compared to the existing end-to-end\nmodels. In particular, we attain state-of-the-art performance on the DST task\nof 2.2 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bang_N/0/1/0/all/0/1\">Namo Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jeehyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_M/0/1/0/all/0/1\">Myoung-Wan Koo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward the Automated Construction of Probabilistic Knowledge Graphs for the Maritime Domain. (arXiv:2305.02471v1 [cs.AI])","link":"http://arxiv.org/abs/2305.02471","description":"<p>International maritime crime is becoming increasingly sophisticated, often\nassociated with wider criminal networks. Detecting maritime threats by means of\nfusing data purely related to physical movement (i.e., those generated by\nphysical sensors, or hard data) is not sufficient. This has led to research and\ndevelopment efforts aimed at combining hard data with other types of data\n(especially human-generated or soft data). Existing work often assumes that\ninput soft data is available in a structured format, or is focused on\nextracting certain relevant entities or concepts to accompany or annotate hard\ndata. Much less attention has been given to extracting the rich knowledge about\nthe situations of interest implicitly embedded in the large amount of soft data\nexisting in unstructured formats (such as intelligence reports and news\narticles). In order to exploit the potentially useful and rich information from\nsuch sources, it is necessary to extract not only the relevant entities and\nconcepts but also their semantic relations, together with the uncertainty\nassociated with the extracted knowledge (i.e., in the form of probabilistic\nknowledge graphs). This will increase the accuracy of and confidence in, the\nextracted knowledge and facilitate subsequent reasoning and learning. To this\nend, we propose Maritime DeepDive, an initial prototype for the automated\nconstruction of probabilistic knowledge graphs from natural language data for\nthe maritime domain. In this paper, we report on the current implementation of\nMaritime DeepDive, together with preliminary results on extracting\nprobabilistic events from maritime piracy incidents. This pipeline was\nevaluated on a manually crafted gold standard, yielding promising results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shiri_F/0/1/0/all/0/1\">Fatemeh Shiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Teresa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_R/0/1/0/all/0/1\">Reza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shuang Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT-steered Editing Instructor for Customization of Abstractive Summarization. (arXiv:2305.02483v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02483","description":"<p>Tailoring outputs of large language models, such as ChatGPT, to specific user\nneeds remains a challenge despite their impressive generation quality. In this\npaper, we propose a tri-agent generation pipeline consisting of a generator, an\ninstructor, and an editor to enhance the customization of generated outputs.\nThe generator produces an initial output, the user-specific instructor\ngenerates editing instructions, and the editor generates a revised output\naligned with user preferences. The inference-only large language model\n(ChatGPT) serves as both the generator and the editor, while a smaller model\nacts as the user-specific instructor to guide the generation process toward\nuser needs. The instructor is trained using editor-steered reinforcement\nlearning, leveraging feedback from the large-scale editor model to optimize\ninstruction generation. Experimental results on two abstractive summarization\ndatasets demonstrate the effectiveness of our approach in generating outputs\nthat better fulfill user expectations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoML-GPT: Automatic Machine Learning with GPT. (arXiv:2305.02499v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02499","description":"<p>AI tasks encompass a wide range of domains and fields. While numerous AI\nmodels have been designed for specific tasks and applications, they often\nrequire considerable human efforts in finding the right model architecture,\noptimization algorithm, and hyperparameters. Recent advances in large language\nmodels (LLMs) like ChatGPT show remarkable capabilities in various aspects of\nreasoning, comprehension, and interaction. Consequently, we propose developing\ntask-oriented prompts and automatically utilizing LLMs to automate the training\npipeline. To implement this concept, we present the AutoML-GPT, which employs\nGPT as the bridge to diverse AI models and dynamically trains models with\noptimized hyperparameters. AutoML-GPT dynamically takes user requests from the\nmodel and data cards and composes the corresponding prompt paragraph.\nUltimately, with this prompt paragraph, AutoML-GPT will automatically conduct\nthe experiments from data processing to model architecture, hyperparameter\ntuning, and predicted training log. By leveraging {\\ours}'s robust language\ncapabilities and the available AI models, AutoML-GPT can tackle numerous\nintricate AI tasks across various tasks and datasets. This approach achieves\nremarkable results in computer vision, natural language processing, and other\nchallenging areas. Extensive experiments and ablation studies demonstrate that\nour method can be general, effective, and beneficial for many AI tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shujian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chengyue Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lemeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingchao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"USTC-NELSLIP at SemEval-2023 Task 2: Statistical Construction and Dual Adaptation of Gazetteer for Multilingual Complex NER. (arXiv:2305.02517v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02517","description":"<p>This paper describes the system developed by the USTC-NELSLIP team for\nSemEval-2023 Task 2 Multilingual Complex Named Entity Recognition (MultiCoNER\nII). A method named Statistical Construction and Dual Adaptation of Gazetteer\n(SCDAG) is proposed for Multilingual Complex NER. The method first utilizes a\nstatistics-based approach to construct a gazetteer. Secondly, the\nrepresentations of gazetteer networks and language models are adapted by\nminimizing the KL divergence between them at both the sentence-level and\nentity-level. Finally, these two networks are then integrated for supervised\nnamed entity recognition (NER) training. The proposed method is applied to\nXLM-R with a gazetteer built from Wikidata, and shows great generalization\nability across different tracks. Experimental results and detailed analysis\nverify the effectiveness of the proposed method. The official results show that\nour system ranked 1st on one track (Hindi) in this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun-Yu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jia-Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiajun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoyi Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning over Untrimmed Videos. (arXiv:2305.02519v1 [cs.CV])","link":"http://arxiv.org/abs/2305.02519","description":"<p>Building benchmarks to systemically analyze different capabilities of video\nquestion answering (VideoQA) models is challenging yet crucial. Existing\nbenchmarks often use non-compositional simple questions and suffer from\nlanguage biases, making it difficult to diagnose model weaknesses incisively. A\nrecent benchmark AGQA poses a promising paradigm to generate QA pairs\nautomatically from pre-annotated scene graphs, enabling it to measure diverse\nreasoning abilities with granular control. However, its questions have\nlimitations in reasoning about the fine-grained semantics in videos as such\ninformation is absent in its scene graphs. To this end, we present ANetQA, a\nlarge-scale benchmark that supports fine-grained compositional reasoning over\nthe challenging untrimmed videos from ActivityNet. Similar to AGQA, the QA\npairs in ANetQA are automatically generated from annotated video scene graphs.\nThe fine-grained properties of ANetQA are reflected in the following: (i)\nuntrimmed videos with fine-grained semantics; (ii) spatio-temporal scene graphs\nwith fine-grained taxonomies; and (iii) diverse questions generated from\nfine-grained templates. ANetQA attains 1.4 billion unbalanced and 13.4 million\nbalanced QA pairs, which is an order of magnitude larger than AGQA with a\nsimilar number of videos. Comprehensive experiments are performed for\nstate-of-the-art methods. The best model achieves 44.5% accuracy while human\nperformance tops out at 84.5%, leaving sufficient room for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lixiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jianping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Kui Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])","link":"http://arxiv.org/abs/2305.02531","description":"<p>Language has a strong influence on our perceptions of time and rewards. This\nraises the question of whether large language models, when asked in different\nlanguages, show different preferences for rewards over time and if their\nchoices are similar to those of humans. In this study, we analyze the responses\nof GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages,\nexploring preferences between smaller, sooner rewards and larger, later\nrewards. Our results show that GPT displays greater patience when prompted in\nlanguages with weak future tense references (FTR), such as German and Mandarin,\ncompared to languages with strong FTR, like English and French. These findings\nare consistent with existing literature and suggest a correlation between GPT's\nchoices and the preferences of speakers of these languages. However, further\nanalysis reveals that the preference for earlier or later rewards does not\nsystematically change with reward gaps, indicating a lexicographic preference\nfor earlier payments. While GPT may capture intriguing variations across\nlanguages, our findings indicate that the choices made by these models do not\ncorrespond to those of human decision-makers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/econ/1/au:+Goli_A/0/1/0/all/0/1\">Ali Goli</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Singh_A/0/1/0/all/0/1\">Amandeep Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. (arXiv:2305.02547v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02547","description":"<p>Despite the many use cases for large language models (LLMs) in the design of\nchatbots in various industries and the research showing the importance of\npersonalizing chatbots to cater to different personality traits, little work\nhas been done to evaluate whether the behaviors of personalized LLMs can\nreflect certain personality traits accurately and consistently. We consider\nstudying the behavior of LLM-based simulated agents which refer to as LLM\npersonas and present a case study with GPT-3.5 (text-davinci-003) to\ninvestigate whether LLMs can generate content with consistent, personalized\ntraits when assigned Big Five personality types and gender roles. We created\n320 LLM personas (5 females and 5 males for each of the 32 Big Five personality\ntypes) and prompted them to complete the classic 44-item Big Five Inventory\n(BFI) and then write an 800-word story about their childhood. Results showed\nthat LLM personas' self-reported BFI scores are consistent with their assigned\npersonality types, with large effect sizes found on all five traits. Moreover,\nsignificant correlations were found between assigned personality types and some\nLinguistic Inquiry and Word Count (LIWC) psycholinguistic features of their\nwritings. For instance, extroversion is associated with pro-social and active\nwords, and neuroticism is associated with words related to negative emotions\nand mental health. Besides, we only found significant differences in using\ntechnological and cultural words in writing between LLM-generated female and\nmale personas. This work provides a first step for further research on\npersonalized LLMs and their applications in Human-AI conversation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiajie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xubo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabbara_J/0/1/0/all/0/1\">Jad Kabbara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1\">Deb Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction. (arXiv:2305.02549v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02549","description":"<p>The recent advent of self-supervised pre-training techniques has led to a\nsurge in the use of multimodal learning in form document understanding.\nHowever, existing approaches that extend the mask language modeling to other\nmodalities require careful multi-task tuning, complex reconstruction target\ndesigns, or additional pre-training data. In FormNetV2, we introduce a\ncentralized multimodal graph contrastive learning strategy to unify\nself-supervised pre-training for all modalities in one loss. The graph\ncontrastive objective maximizes the agreement of multimodal representations,\nproviding a natural interplay for all modalities without special customization.\nIn addition, we extract image features within the bounding box that joins a\npair of tokens connected by a graph edge, capturing more targeted visual cues\nwithout loading a sophisticated and separately pre-trained image embedder.\nFormNetV2 establishes new state-of-the-art performance on FUNSD, CORD, SROIE\nand Payment benchmarks with a more compact model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozat_T/0/1/0/all/0/1\">Timothy Dozat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perot_V/0/1/0/all/0/1\">Vincent Perot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guolong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kihyuk Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glushnev_N/0/1/0/all/0/1\">Nikolai Glushnev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renshen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Shangbang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1\">Siyang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_N/0/1/0/all/0/1\">Nan Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithful Question Answering with Monte-Carlo Planning. (arXiv:2305.02556v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02556","description":"<p>Although large language models demonstrate remarkable question-answering\nperformances, revealing the intermediate reasoning steps that the models\nfaithfully follow remains challenging. In this paper, we propose FAME (FAithful\nquestion answering with MontE-carlo planning) to answer questions based on\nfaithful reasoning steps. The reasoning steps are organized as a structured\nentailment tree, which shows how premises are used to produce intermediate\nconclusions that can prove the correctness of the answer. We formulate the task\nas a discrete decision-making problem and solve it through the interaction of a\nreasoning environment and a controller. The environment is modular and contains\nseveral basic task-oriented modules, while the controller proposes actions to\nassemble the modules. Since the search space could be large, we introduce a\nMonte-Carlo planning algorithm to do a look-ahead search and select actions\nthat will eventually lead to high-quality steps. FAME achieves state-of-the-art\nperformance on the standard benchmark. It can produce valid and faithful\nreasoning steps compared with large language models with a much smaller model\nsize.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Ruixin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Hong Kong's Legal Judgments from a Computational Linguistics point-of-view. (arXiv:2305.02558v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02558","description":"<p>Analysis and extraction of useful information from legal judgments using\ncomputational linguistics was one of the earliest problems posed in the domain\nof information retrieval. Presently, several commercial vendors exist who\nautomate such tasks. However, a crucial bottleneck arises in the form of\nexorbitant pricing and lack of resources available in analysis of judgements\nmete out by Hong Kong's Legal System. This paper attempts to bridge this gap by\nproviding several statistical, machine learning, deep learning and zero-shot\nlearning based methods to effectively analyze legal judgments from Hong Kong's\nCourt System. The methods proposed consists of: (1) Citation Network Graph\nGeneration, (2) PageRank Algorithm, (3) Keyword Analysis and Summarization, (4)\nSentiment Polarity, and (5) Paragrah Classification, in order to be able to\nextract key insights from individual as well a group of judgments together.\nThis would make the overall analysis of judgments in Hong Kong less tedious and\nmore automated in order to extract insights quickly using fast inferencing. We\nalso provide an analysis of our results by benchmarking our results using Large\nLanguage Models making robust use of the HuggingFace ecosystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1\">Sankalok Sen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models. (arXiv:2305.02564v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02564","description":"<p>To better support information retrieval tasks such as web search and\nopen-domain question answering, growing effort is made to develop\nretrieval-oriented language models, e.g., RetroMAE and many others. Most of the\nexisting works focus on improving the semantic representation capability for\nthe contextualized embedding of the [CLS] token. However, recent study shows\nthat the ordinary tokens besides [CLS] may provide extra information, which\nhelp to produce a better representation effect. As such, it's necessary to\nextend the current methods where all contextualized embeddings can be jointly\npre-trained for the retrieval tasks. In this work, we propose a novel\npre-training method called Duplex Masked Auto-Encoder, a.k.a. DupMAE. It is\ndesigned to improve the quality of semantic representation where all\ncontextualized embeddings of the pre-trained model can be leveraged. It takes\nadvantage of two complementary auto-encoding tasks: one reconstructs the input\nsentence on top of the [CLS] embedding; the other one predicts the bag-of-words\nfeature of the input sentence based on the ordinary tokens' embeddings. The two\ntasks are jointly conducted to train a unified encoder, where the whole\ncontextualized embeddings are aggregated in a compact way to produce the final\nsemantic representation. DupMAE is simple but empirically competitive: it\nsubstantially improves the pre-trained model's representation capability and\ntransferability, where superior retrieval performances can be achieved on\npopular benchmarks, like MS MARCO and BEIR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Statistical Methods to Deep Learning, Automatic Keyphrase Prediction: A Survey. (arXiv:2305.02579v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02579","description":"<p>Keyphrase prediction aims to generate phrases (keyphrases) that highly\nsummarizes a given document. Recently, researchers have conducted in-depth\nstudies on this task from various perspectives. In this paper, we\ncomprehensively summarize representative studies from the perspectives of\ndominant models, datasets and evaluation metrics. Our work analyzes up to 167\nprevious works, achieving greater coverage of this task than previous surveys.\nParticularly, we focus highly on deep learning-based keyphrase prediction,\nwhich attracts increasing attention of this task in recent years. Afterwards,\nwe conduct several groups of experiments to carefully compare representative\nmodels. To the best of our knowledge, our work is the first attempt to compare\nthese models using the identical commonly-used datasets and evaluation metric,\nfacilitating in-depth analyses of their disadvantages and advantages. Finally,\nwe discuss the possible research directions of this task in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1\">Binbin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Liangying Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Suhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiangpeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Huan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re$^3$Dial: Retrieve, Reorganize and Rescale Dialogue Corpus for Long-Turn Open-Domain Dialogue Pre-training. (arXiv:2305.02606v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02606","description":"<p>Large-scale open-domain dialogue data crawled from public social media has\ngreatly improved the performance of dialogue models. However, long-turn\ndialogues are still highly scarce. Specifically, most dialogue sessions in\nexisting corpora have less than three turns. To alleviate this issue, we\npropose the Retrieve, Reorganize and Rescale framework (Re$^3$Dial), which can\nautomatically construct a billion-scale long-turn dialogue corpus from existing\nshort-turn dialogue data. Re$^3$Dial first trains an Unsupervised Dense Session\nRetriever (UDSR) to capture semantic and discourse relationships within\nmulti-turn dialogues for retrieving relevant and coherent sessions. It then\nreorganizes the short-turn dialogues into long-turn sessions via recursively\nretrieving and selecting the consecutive sessions with our proposed diversity\nsampling strategy. Extensive evaluations on multiple multi-turn dialogue\nbenchmarks demonstrate that Re$^3$Dial consistently and significantly improves\nthe dialogue model's ability to utilize long-term context for modeling\nmulti-turn dialogues across different pre-training settings. Finally, we build\na toolkit for efficiently rescaling dialogue corpus with Re$^3$Dial, which\nenables us to construct a corpus containing 1B Chinese dialogue sessions with\n11.3 turns on average (5X longer than the original EVA corpus). We will release\nour UDSR model, toolkit, and data for public use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jiaxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DN at SemEval-2023 Task 12: Low-Resource Language Text Classification via Multilingual Pretrained Language Model Fine-tuning. (arXiv:2305.02607v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02607","description":"<p>In recent years, sentiment analysis has gained significant importance in\nnatural language processing. However, most existing models and datasets for\nsentiment analysis are developed for high-resource languages, such as English\nand Chinese, leaving low-resource languages, particularly African languages,\nlargely unexplored. The AfriSenti-SemEval 2023 Shared Task 12 aims to fill this\ngap by evaluating sentiment analysis models on low-resource African languages.\nIn this paper, we present our solution to the shared task, where we employed\ndifferent multilingual XLM-R models with classification head trained on various\ndata, including those retrained in African dialects and fine-tuned on target\nlanguages. Our team achieved the third-best results in Subtask B, Track 16:\nMultilingual, demonstrating the effectiveness of our approach. While our model\nshowed relatively good results on multilingual data, it performed poorly in\nsome languages. Our findings highlight the importance of developing more\ncomprehensive datasets and models for low-resource African languages to advance\nsentiment analysis research. We also provided the solution on the github\nrepository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Homskiy_D/0/1/0/all/0/1\">Daniil Homskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maloyan_N/0/1/0/all/0/1\">Narek Maloyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affective Reasoning at Utterance Level in Conversations: A Causal Discovery Approach. (arXiv:2305.02615v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02615","description":"<p>The affective reasoning task is a set of emerging affect-based tasks in\nconversation, including Emotion Recognition in Conversation (ERC),Emotion-Cause\nPair Extraction (ECPE), and Emotion-Cause Span Recognition (ECSR). Existing\nmethods make various assumptions on the apparent relationship while neglecting\nthe essential causal model due to the nonuniqueness of skeletons and\nunobservability of implicit causes. This paper settled down the above two\nproblems and further proposed Conversational Affective Causal Discovery (CACD).\nIt is a novel causal discovery method showing how to discover causal\nrelationships in a conversation via designing a common skeleton and generating\na substitute for implicit causes. CACD contains two steps: (i) building a\ncommon centering one graph node causal skeleton for all utterances in\nvariable-length conversations; (ii) Causal Auto-Encoder (CAE) correcting the\nskeleton to yield causal representation through generated implicit causes and\nknown explicit causes. Comprehensive experiments demonstrate that our novel\nmethod significantly outperforms the SOTA baselines in six affect-related\ndatasets on the three tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jing Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenjing Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A framework for the emergence and analysis of language in social learning agents. (arXiv:2305.02632v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02632","description":"<p>Artificial neural networks (ANNs) are increasingly used as research models,\nbut questions remain about their generalizability and representational\ninvariance. Biological neural networks under social constraints evolved to\nenable communicable representations, demonstrating generalization capabilities.\nThis study proposes a communication protocol between cooperative agents to\nanalyze the formation of individual and shared abstractions and their impact on\ntask performance. This communication protocol aims to mimic language features\nby encoding high-dimensional information through low-dimensional\nrepresentation. Using grid-world mazes and reinforcement learning, teacher ANNs\npass a compressed message to a student ANN for better task completion. Through\nthis, the student achieves a higher goal-finding rate and generalizes the goal\nlocation across task worlds. Further optimizing message content to maximize\nstudent reward improves information encoding, suggesting that an accurate\nrepresentation in the space of messages requires bi-directional input. This\nhighlights the role of language as a common representation between agents and\nits implications on generalization capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wieczorek_T/0/1/0/all/0/1\">Tobias J. Wieczorek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchumatchenko_T/0/1/0/all/0/1\">Tatjana Tchumatchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvajal_C/0/1/0/all/0/1\">Carlos Wert Carvajal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eggl_M/0/1/0/all/0/1\">Maximilian F. Eggl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conformal Nucleus Sampling. (arXiv:2305.02633v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02633","description":"<p>Language models generate text based on successively sampling the next word. A\ndecoding procedure based on nucleus (top-$p$) sampling chooses from the\nsmallest possible set of words whose cumulative probability exceeds the\nprobability $p$. In this work, we assess whether a top-$p$ set is indeed\naligned with its probabilistic meaning in various linguistic contexts. We\nemploy conformal prediction, a calibration procedure that focuses on the\nconstruction of minimal prediction sets according to a desired confidence\nlevel, to calibrate the parameter $p$ as a function of the entropy of the next\nword distribution. We find that OPT models are overconfident, and that\ncalibration shows a moderate inverse scaling with model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberger_J/0/1/0/all/0/1\">Jacob Goldberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Weakly-Supervised Hate Speech Classification Across Datasets. (arXiv:2305.02637v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02637","description":"<p>As pointed out by several scholars, current research on hate speech (HS)\nrecognition is characterized by unsystematic data creation strategies and\ndiverging annotation schemata. Subsequently, supervised-learning models tend to\ngeneralize poorly to datasets they were not trained on, and the performance of\nthe models trained on datasets labeled using different HS taxonomies cannot be\ncompared. To ease this problem, we propose applying extremely weak supervision\nthat only relies on the class name rather than on class samples from the\nannotated data. We demonstrate the effectiveness of a state-of-the-art\nweakly-supervised text classification model in various in-dataset and\ncross-dataset settings. Furthermore, we conduct an in-depth quantitative and\nqualitative analysis of the source of poor generalizability of HS\nclassification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yiping Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanner_L/0/1/0/all/0/1\">Leo Wanner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadam_V/0/1/0/all/0/1\">Vishakha Laxman Kadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shvets_A/0/1/0/all/0/1\">Alexander Shvets</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Language-Specific Layers for Multilingual Machine Translation. (arXiv:2305.02665v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02665","description":"<p>Multilingual Machine Translation promises to improve translation quality\nbetween non-English languages. This is advantageous for several reasons, namely\nlower latency (no need to translate twice), and reduced error cascades (e.g.,\navoiding losing gender and formality information when translating through\nEnglish). On the downside, adding more languages reduces model capacity per\nlanguage, which is usually countered by increasing the overall model size,\nmaking training harder and inference slower. In this work, we introduce\nLanguage-Specific Transformer Layers (LSLs), which allow us to increase model\ncapacity, while keeping the amount of computation and the number of parameters\nused in the forward pass constant. The key idea is to have some layers of the\nencoder be source or target language-specific, while keeping the remaining\nlayers shared. We study the best way to place these layers using a neural\narchitecture search inspired approach, and achieve an improvement of 1.3 chrF\n(1.5 spBLEU) points over not using LSLs on a separate decoder architecture, and\n1.9 chrF (2.2 spBLEU) on a shared decoder one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pires_T/0/1/0/all/0/1\">Telmo Pessoa Pires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_R/0/1/0/all/0/1\">Robin M. Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yi-Hsiu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peitz_S/0/1/0/all/0/1\">Stephan Peitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neighboring Words Affect Human Interpretation of Saliency Explanations. (arXiv:2305.02679v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02679","description":"<p>Word-level saliency explanations (\"heat maps over words\") are often used to\ncommunicate feature-attribution in text-based models. Recent studies found that\nsuperficial factors such as word length can distort human interpretation of the\ncommunicated saliency scores. We conduct a user study to investigate how the\nmarking of a word's neighboring words affect the explainee's perception of the\nword's importance in the context of a saliency explanation. We find that\nneighboring words have significant effects on the word's importance rating.\nConcretely, we identify that the influence changes based on neighboring\ndirection (left vs. right) and a-priori linguistic and computational measures\nof phrases and collocations (vs. unrelated neighboring words). Our results\nquestion whether text-based saliency explanations should be continued to be\ncommunicated at word level, and inform future research on alternative saliency\nexplanation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1\">Alon Jacovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Big Data and Large Numbers. Interpreting Zipf's Law. (arXiv:2305.02687v1 [physics.soc-ph])","link":"http://arxiv.org/abs/2305.02687","description":"<p>It turns out that some empirical facts in Big Data are the effects of\nproperties of large numbers. Zipf's law noise is an example of such an\nartefact. We expose several properties of the power law distributions and of\nsimilar distribution that occur when the population is finite and the rank and\ncounts of elements in the population are natural numbers. Consequences in the\ninterpretation of Zipf's law are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Teodorescu_H/0/1/0/all/0/1\">Horia-Nicolai L. Teodorescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Asynchronous Updating Reinforcement Learning Framework for Task-oriented Dialog System. (arXiv:2305.02718v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02718","description":"<p>Reinforcement learning has been applied to train the dialog systems in many\nworks. Previous approaches divide the dialog system into multiple modules\nincluding DST (dialog state tracking) and DP (dialog policy), and train these\nmodules simultaneously. However, different modules influence each other during\ntraining. The errors from DST might misguide the dialog policy, and the system\naction brings extra difficulties for the DST module. To alleviate this problem,\nwe propose Asynchronous Updating Reinforcement Learning framework (AURL) that\nupdates the DST module and the DP module asynchronously under a cooperative\nsetting. Furthermore, curriculum learning is implemented to address the problem\nof unbalanced data distribution during reinforcement learning sampling, and\nmultiple user models are introduced to increase the dialog diversity. Results\non the public SSD-PHONE dataset show that our method achieves a compelling\nresult with a 31.37% improvement on the dialog success rate. The code is\npublicly available via https://github.com/shunjiu/AURL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuwei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Caixia Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Dialogue Topic Segmentation with Topic-aware Utterance Representation. (arXiv:2305.02747v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02747","description":"<p>Dialogue Topic Segmentation (DTS) plays an essential role in a variety of\ndialogue modeling tasks. Previous DTS methods either focus on semantic\nsimilarity or dialogue coherence to assess topic similarity for unsupervised\ndialogue segmentation. However, the topic similarity cannot be fully identified\nvia semantic similarity or dialogue coherence. In addition, the unlabeled\ndialogue data, which contains useful clues of utterance relationships, remains\nunderexploited. In this paper, we propose a novel unsupervised DTS framework,\nwhich learns topic-aware utterance representations from unlabeled dialogue data\nthrough neighboring utterance matching and pseudo-segmentation. Extensive\nexperiments on two benchmark datasets (i.e., DialSeg711 and Doc2Dial)\ndemonstrate that our method significantly outperforms the strong baseline\nmethods. For reproducibility, we provide our code and data\nat:https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial-start.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Haoyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects. (arXiv:2305.02750v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02750","description":"<p>Proactive dialogue systems, related to a wide range of real-world\nconversational applications, equip the conversational agent with the capability\nof leading the conversation direction towards achieving pre-defined targets or\nfulfilling certain goals from the system side. It is empowered by advanced\ntechniques to progress to more complicated tasks that require strategical and\nmotivational interactions. In this survey, we provide a comprehensive overview\nof the prominent problems and advanced designs for conversational agent's\nproactivity in different types of dialogues. Furthermore, we discuss challenges\nthat meet the real-world application needs but require a greater research focus\nin the future. We hope that this first survey of proactive dialogue systems can\nprovide the community with a quick access and an overall picture to this\npractical problem, and stimulate more progresses on conversational AI to the\nnext level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VendorLink: An NLP approach for Identifying & Linking Vendor Migrants & Potential Aliases on Darknet Markets. (arXiv:2305.02763v1 [cs.CY])","link":"http://arxiv.org/abs/2305.02763","description":"<p>The anonymity on the Darknet allows vendors to stay undetected by using\nmultiple vendor aliases or frequently migrating between markets. Consequently,\nillegal markets and their connections are challenging to uncover on the\nDarknet. To identify relationships between illegal markets and their vendors,\nwe propose VendorLink, an NLP-based approach that examines writing patterns to\nverify, identify, and link unique vendor accounts across text advertisements\n(ads) on seven public Darknet markets. In contrast to existing literature,\nVendorLink utilizes the strength of supervised pre-training to perform\nclosed-set vendor verification, open-set vendor identification, and\nlow-resource market adaption tasks. Through VendorLink, we uncover (i) 15\nmigrants and 71 potential aliases in the Alphabay-Dreams-Silk dataset, (ii) 17\nmigrants and 3 potential aliases in the Valhalla-Berlusconi dataset, and (iii)\n75 migrants and 10 potential aliases in the Traderoute-Agora dataset.\nAltogether, our approach can help Law Enforcement Agencies (LEA) make more\ninformed decisions by verifying and identifying migrating vendors and their\npotential aliases on existing and Low-Resource (LR) emerging Darknet markets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxena_V/0/1/0/all/0/1\">Vageesh Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rethmeier_N/0/1/0/all/0/1\">Nils Rethmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijck_G/0/1/0/all/0/1\">Gijs Van Dijck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spanakis_G/0/1/0/all/0/1\">Gerasimos Spanakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Politics of Language Choice: How the Russian-Ukrainian War Influences Ukrainians' Language Use on Twitter. (arXiv:2305.02770v1 [cs.CY])","link":"http://arxiv.org/abs/2305.02770","description":"<p>The use of language is innately political and often a vehicle of cultural\nidentity as well as the basis for nation building. Here, we examine language\nchoice and tweeting activity of Ukrainian citizens based on more than 4 million\ngeo-tagged tweets from over 62,000 users before and during the\nRussian-Ukrainian War, from January 2020 to October 2022. Using statistical\nmodels, we disentangle sample effects, arising from the in- and outflux of\nusers on Twitter, from behavioural effects, arising from behavioural changes of\nthe users. We observe a steady shift from the Russian language towards the\nUkrainian language already before the war, which drastically speeds up with its\noutbreak. We attribute these shifts in large part to users' behavioural\nchanges. Notably, we find that many Russian-tweeting users perform a\nhard-switch to Ukrainian as a result of the war.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Racek_D/0/1/0/all/0/1\">Daniel Racek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_B/0/1/0/all/0/1\">Brittany I. Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thurner_P/0/1/0/all/0/1\">Paul W. Thurner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kauermann_G/0/1/0/all/0/1\">G&#xf6;ran Kauermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Model Learning for Various Neural Machine Translation. (arXiv:2305.02777v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02777","description":"<p>Existing neural machine translation (NMT) studies mainly focus on developing\ndataset-specific models based on data from different tasks (e.g., document\ntranslation and chat translation). Although the dataset-specific models have\nachieved impressive performance, it is cumbersome as each dataset demands a\nmodel to be designed, trained, and stored. In this work, we aim to unify these\ntranslation tasks into a more general setting. Specifically, we propose a\n``versatile'' model, i.e., the Unified Model Learning for NMT (UMLNMT) that\nworks with data from different tasks, and can translate well in multiple\nsettings simultaneously, and theoretically it can be as many as possible.\nThrough unified learning, UMLNMT is able to jointly train across multiple\ntasks, implementing intelligent on-demand translation. On seven widely-used\ntranslation tasks, including sentence translation, document translation, and\nchat translation, our UMLNMT results in substantial improvements over\ndataset-specific models with significantly reduced model deployment costs.\nFurthermore, UMLNMT can achieve competitive or better performance than\nstate-of-the-art dataset-specific methods. Human evaluation and in-depth\nanalysis also demonstrate the superiority of our approach on generating diverse\nand high-quality translations. Additionally, we provide a new genre translation\ndataset about famous aphorisms with 186k Chinese-&gt;English sentence pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v1 [cs.SE])","link":"http://arxiv.org/abs/2305.02783","description":"<p>The recent improvement in code generation capabilities due to the use of\nlarge language models has mainly benefited general purpose programming\nlanguages. Domain specific languages, such as the ones used for IT Automation,\nhave received far less attention, despite involving many active developers and\nbeing an essential component of modern cloud platforms. This work focuses on\nthe generation of Ansible-YAML, a widely used markup language for IT\nAutomation. We present Ansible Wisdom, a natural-language to Ansible-YAML code\ngeneration tool, aimed at improving IT automation productivity. Ansible Wisdom\nis a transformer-based model, extended by training with a new dataset\ncontaining Ansible-YAML. We also develop two novel performance metrics for YAML\nand Ansible to capture the specific characteristics of this domain. Results\nshow that Ansible Wisdom can accurately generate Ansible script from natural\nlanguage prompts with performance comparable or better than existing state of\nthe art code generation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pujar_S/0/1/0/all/0/1\">Saurabh Pujar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buratti_L/0/1/0/all/0/1\">Luca Buratti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaojie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupuis_N/0/1/0/all/0/1\">Nicolas Dupuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_B/0/1/0/all/0/1\">Burn Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suneja_S/0/1/0/all/0/1\">Sahil Suneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sood_A/0/1/0/all/0/1\">Atin Sood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalawade_G/0/1/0/all/0/1\">Ganesh Nalawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1\">Matt Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morari_A/0/1/0/all/0/1\">Alessandro Morari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Ruchir Puri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BranchNorm: Robustly Scaling Extremely Deep Transformers. (arXiv:2305.02790v1 [cs.LG])","link":"http://arxiv.org/abs/2305.02790","description":"<p>Recently, DeepNorm scales Transformers into extremely deep (i.e., 1000\nlayers) and reveals the promising potential of deep scaling. To stabilize the\ntraining of deep models, DeepNorm (Wang et al., 2022) attempts to constrain the\nmodel update to a constant value. Although applying such a constraint can\nbenefit the early stage of model training, it may lead to undertrained models\nduring the whole training procedure. In this paper, we propose BranchNorm,\nwhich dynamically rescales the non-residual branch of Transformer in accordance\nwith the training period. BranchNorm not only theoretically stabilizes the\ntraining with smooth gradient norms at the early stage, but also encourages\nbetter convergence in the subsequent training stage. Experiment results on\nmultiple translation tasks demonstrate that BranchNorm achieves a better\ntrade-off between training stability and converge performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xianfeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research. (arXiv:2305.02797v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02797","description":"<p>Recent advances in deep learning methods for natural language processing\n(NLP) have created new business opportunities and made NLP research critical\nfor industry development. As one of the big players in the field of NLP,\ntogether with governments and universities, it is important to track the\ninfluence of industry on research. In this study, we seek to quantify and\ncharacterize industry presence in the NLP community over time. Using a corpus\nwith comprehensive metadata of 78,187 NLP publications and 701 resumes of NLP\npublication authors, we explore the industry presence in the field since the\nearly 90s. We find that industry presence among NLP authors has been steady\nbefore a steep increase over the past five years (180% growth from 2017 to\n2022). A few companies account for most of the publications and provide funding\nto academic researchers through grants and internships. Our study shows that\nthe presence and impact of the industry on natural language processing research\nare significant and fast-growing. This work calls for increased transparency of\nindustry influence in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdalla_M/0/1/0/all/0/1\">Mohamed Abdalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neveol_A/0/1/0/all/0/1\">Aur&#xe9;lie N&#xe9;v&#xe9;ol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ducel_F/0/1/0/all/0/1\">Fanny Ducel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_K/0/1/0/all/0/1\">Kar&#xeb;n Fort</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Sentence Representation with Variational Autoencoders and Attention. (arXiv:2305.02810v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02810","description":"<p>In this thesis, we develop methods to enhance the interpretability of recent\nrepresentation learning techniques in natural language processing (NLP) while\naccounting for the unavailability of annotated data. We choose to leverage\nVariational Autoencoders (VAEs) due to their efficiency in relating\nobservations to latent generative factors and their effectiveness in\ndata-efficient learning and interpretable representation learning. As a first\ncontribution, we identify and remove unnecessary components in the functioning\nscheme of semi-supervised VAEs making them faster, smaller and easier to\ndesign. Our second and main contribution is to use VAEs and Transformers to\nbuild two models with inductive bias to separate information in latent\nrepresentations into understandable concepts without annotated data. The first\nmodel, Attention-Driven VAE (ADVAE), is able to separately represent and\ncontrol information about syntactic roles in sentences. The second model,\nQKVAE, uses separate latent variables to form keys and values for its\nTransformer decoder and is able to separate syntactic and semantic information\nin its neural representations. In transfer experiments, QKVAE has competitive\nperformance compared to supervised models and equivalent performance to a\nsupervised model using 50K annotated samples. Additionally, QKVAE displays\nimproved syntactic role disentanglement capabilities compared to ADVAE.\nOverall, we demonstrate that it is possible to enhance the interpretability of\nstate-of-the-art deep learning architectures for language modeling with\nunannotated data in situations where text data is abundant but annotations are\nscarce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Felhi_G/0/1/0/all/0/1\">Ghazi Felhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Space Grounded Weighted Decoding for Multi-Attribute Controllable Dialogue Generation. (arXiv:2305.02820v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02820","description":"<p>Controlling chatbot utterance generation with multiple attributes such as\npersonalities, emotions and dialogue acts is a practically useful but\nunder-studied problem. We propose a novel controllable generation framework\ncalled DASC that possesses strong controllability with weighted decoding\nparadigm, while improving generation quality with the grounding in an attribute\nsemantics space. Generation with multiple attributes is then intuitively\nimplemented with an interpolation of multiple attribute embeddings. Experiments\nshow that DASC can achieve state-of-the-art control accuracy in 3-aspect\ncontrollable generation tasks while also producing interesting and reasonably\nsensible responses, even if in an out-of-distribution robustness test.\nVisualization of the meaningful representations learned in the attribute\nsemantic space also supports its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiling Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReMask: A Robust Information-Masking Approach for Domain Counterfactual Generation. (arXiv:2305.02858v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02858","description":"<p>Domain shift is a big challenge in NLP, thus, many approaches resort to\nlearning domain-invariant features to mitigate the inference phase domain\nshift. Such methods, however, fail to leverage the domain-specific nuances\nrelevant to the task at hand. To avoid such drawbacks, domain counterfactual\ngeneration aims to transform a text from the source domain to a given target\ndomain. However, due to the limited availability of data, such frequency-based\nmethods often miss and lead to some valid and spurious domain-token\nassociations. Hence, we employ a three-step domain obfuscation approach that\ninvolves frequency and attention norm-based masking, to mask domain-specific\ncues, and unmasking to regain the domain generic context. Our experiments\nempirically show that the counterfactual samples sourced from our masked text\nlead to improved domain transfer on 10 out of 12 domain sentiment\nclassification settings, with an average of 2% accuracy improvement over the\nstate-of-the-art for unsupervised domain adaptation (UDA). Further, our model\noutperforms the state-of-the-art by achieving 1.4% average accuracy improvement\nin the adversarial domain adaptation (ADA) setting. Moreover, our model also\nshows its domain adaptation efficacy on a large multi-domain intent\nclassification dataset where it attains state-of-the-art results. We release\nthe codes publicly at \\url{https://github.com/declare-lab/remask}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1\">Rishabh Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_N/0/1/0/all/0/1\">Navonil Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aditya_S/0/1/0/all/0/1\">Somak Aditya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CausalAPM: Generalizable Literal Disentanglement for NLU Debiasing. (arXiv:2305.02865v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02865","description":"<p>Dataset bias, i.e., the over-reliance on dataset-specific literal heuristics,\nis getting increasing attention for its detrimental effect on the\ngeneralization ability of NLU models. Existing works focus on eliminating\ndataset bias by down-weighting problematic data in the training process, which\ninduce the omission of valid feature information while mitigating bias. In this\nwork, We analyze the causes of dataset bias from the perspective of causal\ninference and propose CausalAPM, a generalizable literal disentangling\nframework to ameliorate the bias problem from feature granularity. The proposed\napproach projects literal and semantic information into independent feature\nsubspaces, and constrains the involvement of literal information in subsequent\npredictions. Extensive experiments on three NLP benchmarks (MNLI, FEVER, and\nQQP) demonstrate that our proposed framework significantly improves the OOD\ngeneralization performance while maintaining ID performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Songyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shihan Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_J/0/1/0/all/0/1\">Junjie Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2x Faster Language Model Pre-training via Masked Structural Growth. (arXiv:2305.02869v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02869","description":"<p>Acceleration of large language model pre-training is a critical issue in\npresent NLP research. In this paper, we focus on speeding up pre-training by\nprogressively growing from a small Transformer structure to a large one. There\nare two main research problems related to progressive growth: growth schedule\nand growth operator. For growth schedule, existing work has explored\nmulti-stage expansion of depth and feedforward layers. However, the impact of\neach dimension on the schedule's efficiency is still an open question. For\ngrowth operator, existing work relies on the initialization of new weights to\ninherit knowledge, and achieve only non-strict function preservation, limiting\nfurther optimization of training dynamics. To address these issues, we propose\nMasked Structural Growth (MSG), including growth schedules involving all\npossible dimensions and strictly function-preserving growth operators that is\nindependent of the initialization of new weights. Experiments show that MSG is\nsignificantly faster than related work: we achieve a speed-up of 80% for\nBert-base and 120% for Bert-large pre-training. Moreover, MSG is able to\nimprove fine-tuning performances at the same time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yiqun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yequan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An automatically discovered chain-of-thought prompt generalizes to novel models and datasets. (arXiv:2305.02897v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02897","description":"<p>Emergent chain-of-thought (CoT) reasoning capabilities promise to improve\nperformance and explainability of large language models (LLMs). However,\nuncertainties remain about how prompting strategies formulated for previous\nmodel generations generalize to new model generations and different datasets.\nIn this small-scale study we compare the performance of a range of zero-shot\nprompts for inducing CoT reasoning across six recently released LLMs\n(davinci-002, davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere\ncommand-xlarge) on a mixture of six question-answering datasets, including\ndatasets from scientific and medical domains. We find that a CoT prompt that\nwas previously discovered through automated prompt discovery shows robust\nperformance across experimental conditions and produces best results when\napplied to the state-of-the-art model GPT-4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hebenstreit_K/0/1/0/all/0/1\">Konstantin Hebenstreit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Praas_R/0/1/0/all/0/1\">Robert Praas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiesewetter_L/0/1/0/all/0/1\">Louis P Kiesewetter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end spoken language understanding using joint CTC loss and self-supervised, pretrained acoustic encoders. (arXiv:2305.02937v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02937","description":"<p>It is challenging to extract semantic meanings directly from audio signals in\nspoken language understanding (SLU), due to the lack of textual information.\nPopular end-to-end (E2E) SLU models utilize sequence-to-sequence automatic\nspeech recognition (ASR) models to extract textual embeddings as input to infer\nsemantics, which, however, require computationally expensive auto-regressive\ndecoding. In this work, we leverage self-supervised acoustic encoders\nfine-tuned with Connectionist Temporal Classification (CTC) to extract textual\nembeddings and use joint CTC and SLU losses for utterance-level SLU tasks.\nExperiments show that our model achieves 4% absolute improvement over the the\nstate-of-the-art (SOTA) dialogue act classification model on the DSTC2 dataset\nand 1.3% absolute improvement over the SOTA SLU model on the SLURP dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radfar_M/0/1/0/all/0/1\">Martin Radfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_C/0/1/0/all/0/1\">Clement Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data. (arXiv:2305.02993v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02993","description":"<p>This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence\nNatural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2\ntasks, a Natural Language Inference (NLI) task, and an evidence selection task\non clinical trial data. The proposed challenges require multi-hop biomedical\nand numerical reasoning, which are of significant importance to the development\nof systems capable of large-scale interpretation and retrieval of medical\nevidence, to provide personalized evidence-based care.\n</p>\n<p>Task 1, the entailment task, received 643 submissions from 40 participants,\nand Task 2, the evidence selection task, received 364 submissions from 23\nparticipants. The tasks are challenging, with the majority of submitted systems\nfailing to significantly outperform the majority class baseline on the\nentailment task, and we observe significantly better performance on the\nevidence selection task than on the entailment task. Increasing the number of\nmodel parameters leads to a direct increase in performance, far more\nsignificant than the effect of biomedical pre-training. Future works could\nexplore the limitations of large models for generalization and numerical\ninference, and investigate methods to augment clinical datasets to allow for\nmore rigorous testing and to facilitate fine-tuning.\n</p>\n<p>We envisage that the dataset, models, and results of this task will be useful\nto the biomedical NLI and evidence retrieval communities. The dataset,\ncompetition leaderboard, and website are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jullien_M/0/1/0/all/0/1\">Ma&#xeb;l Jullien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frost_H/0/1/0/all/0/1\">Hannah Frost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ORegan_P/0/1/0/all/0/1\">Paul O&#x27;Regan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1\">Donal Landers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the nonlinear correlation of ML performance between data subpopulations. (arXiv:2305.02995v1 [cs.LG])","link":"http://arxiv.org/abs/2305.02995","description":"<p>Understanding the performance of machine learning (ML) models across diverse\ndata distributions is critically important for reliable applications. Despite\nrecent empirical studies positing a near-perfect linear correlation between\nin-distribution (ID) and out-of-distribution (OOD) accuracies, we empirically\ndemonstrate that this correlation is more nuanced under subpopulation shifts.\nThrough rigorous experimentation and analysis across a variety of datasets,\nmodels, and training epochs, we demonstrate that OOD performance often has a\nnonlinear correlation with ID performance in subpopulation shifts. Our\nfindings, which contrast previous studies that have posited a linear\ncorrelation in model performance during distribution shifts, reveal a \"moon\nshape\" correlation (parabolic uptrend curve) between the test performance on\nthe majority subpopulation and the minority subpopulation. This non-trivial\nnonlinear correlation holds across model architectures, hyperparameters,\ntraining durations, and the imbalance between subpopulations. Furthermore, we\nfound that the nonlinearity of this \"moon shape\" is causally influenced by the\ndegree of spurious correlations in the training data. Our controlled\nexperiments show that stronger spurious correlation in the training data\ncreates more nonlinear performance correlation. We provide complementary\nexperimental and theoretical analyses for this phenomenon, and discuss its\nimplications for ML reliability and fairness. Our work highlights the\nimportance of understanding the nonlinear effects of model improvement on\nperformance in different subpopulations, and has the potential to inform the\ndevelopment of more equitable and responsible machine learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Weixin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yining Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yongchan Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Selection of Anchor Items for CUR-based k-NN search with Cross-Encoders. (arXiv:2305.02996v1 [cs.IR])","link":"http://arxiv.org/abs/2305.02996","description":"<p>Cross-encoder models, which jointly encode and score a query-item pair, are\ntypically prohibitively expensive for k-nearest neighbor search. Consequently,\nk-NN search is performed not with a cross-encoder, but with a heuristic\nretrieve (e.g., using BM25 or dual-encoder) and re-rank approach. Recent work\nproposes ANNCUR (Yadav et al., 2022) which uses CUR matrix factorization to\nproduce an embedding space for efficient vector-based search that directly\napproximates the cross-encoder without the need for dual-encoders. ANNCUR\ndefines this shared query-item embedding space by scoring the test query\nagainst anchor items which are sampled uniformly at random. While this\nminimizes average approximation error over all items, unsuitably high\napproximation error on top-k items remains and leads to poor recall of top-k\n(and especially top-1) items. Increasing the number of anchor items is a\nstraightforward way of improving the approximation error and hence k-NN recall\nof ANNCUR but at the cost of increased inference latency. In this paper, we\npropose a new method for adaptively choosing anchor items that minimizes the\napproximation error for the practically important top-k neighbors for a query\nwith minimal computational overhead. Our proposed method incrementally selects\na suitable set of anchor items for a given test query over several rounds,\nusing anchors chosen in previous rounds to inform selection of more anchor\nitems. Empirically, our method consistently improves k-NN recall as compared to\nboth ANNCUR and the widely-used dual-encoder-based retrieve-and-rerank\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1\">Nishant Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monath_N/0/1/0/all/0/1\">Nicholas Monath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NatCS: Eliciting Natural Customer Support Dialogues. (arXiv:2305.03007v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03007","description":"<p>Despite growing interest in applications based on natural customer support\nconversations, there exist remarkably few publicly available datasets that\nreflect the expected characteristics of conversations in these settings.\nExisting task-oriented dialogue datasets, which were collected to benchmark\ndialogue systems mainly in written human-to-bot settings, are not\nrepresentative of real customer support conversations and do not provide\nrealistic benchmarks for systems that are applied to natural data. To address\nthis gap, we introduce NatCS, a multi-domain collection of spoken customer\nservice conversations. We describe our process for collecting synthetic\nconversations between customers and agents based on natural language phenomena\nobserved in real conversations. Compared to previous dialogue datasets, the\nconversations collected with our approach are more representative of real\nhuman-to-human conversations along multiple metrics. Finally, we demonstrate\npotential uses of NatCS, including dialogue act classification and intent\ninduction from conversations as potential applications, showing that dialogue\nact annotations in NatCS provide more effective training data for modeling real\nconversations compared to existing synthetic written datasets. We publicly\nrelease NatCS to facilitate research in natural dialog systems\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gung_J/0/1/0/all/0/1\">James Gung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeng_E/0/1/0/all/0/1\">Emily Moeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_W/0/1/0/all/0/1\">Wesley Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Arshit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence. (arXiv:2305.03010v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03010","description":"<p>Sentence-level representations are beneficial for various natural language\nprocessing tasks. It is commonly believed that vector representations can\ncapture rich linguistic properties. Currently, large language models (LMs)\nachieve state-of-the-art performance on sentence embedding. However, some\nrecent works suggest that vector representations from LMs can cause information\nleakage. In this work, we further investigate the information leakage issue and\npropose a generative embedding inversion attack (GEIA) that aims to reconstruct\ninput sequences based only on their sentence embeddings. Given the black-box\naccess to a language model, we treat sentence embeddings as initial tokens'\nrepresentations and train or fine-tune a powerful decoder model to decode the\nwhole sequences directly. We conduct extensive experiments to demonstrate that\nour generative inversion attack outperforms previous embedding inversion\nattacks in classification metrics and generates coherent and contextually\nsimilar sentences as the original inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingshi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models. (arXiv:2305.03025v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03025","description":"<p>This project focuses on enhancing open-source large language models through\ninstruction-tuning and providing comprehensive evaluations of their\nperformance. We explore how various training data factors, such as quantity,\nquality, and linguistic distribution, influence the performance of\ninstruction-tuned models trained on publicly accessible high-quality\ninstruction datasets for both English and Chinese languages. Our goal is to\nsupplement evaluation with quantitative analyses, providing valuable insights\nfor the continued advancement of open-source chat models. Our model, data, and\ncode are publicly available for others to use and build upon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_F/0/1/0/all/0/1\">Fangkai Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bosheng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tianze Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_Z/0/1/0/all/0/1\">Zhanfeng Mo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What changes when you randomly choose BPE merge operations? Not much. (arXiv:2305.03029v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03029","description":"<p>We introduce three simple randomized variants of byte pair encoding (BPE) and\nexplore whether randomizing the selection of merge operations substantially\naffects a downstream machine translation task. We focus on translation into\nmorphologically rich languages, hypothesizing that this task may show\nsensitivity to the method of choosing subwords. Analysis using a Bayesian\nlinear model indicates that two of the variants perform nearly\nindistinguishably compared to standard BPE while the other degrades performance\nless than we anticipated. We conclude that although standard BPE is widely\nused, there exists an interesting universe of potential variations on it worth\ninvestigating. Our code is available at: https://github.com/bltlab/random-bpe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saleva_J/0/1/0/all/0/1\">Jonne S&#xe4;lev&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. (arXiv:2305.03047v1 [cs.LG])","link":"http://arxiv.org/abs/2305.03047","description":"<p>Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised\nfine-tuning (SFT) with human annotations and reinforcement learning from human\nfeedback (RLHF) to align the output of large language models (LLMs) with human\nintentions, ensuring they are helpful, ethical, and reliable. However, this\ndependence can significantly constrain the true potential of AI-assistant\nagents due to the high cost of obtaining human supervision and the related\nissues on quality, reliability, diversity, self-consistency, and undesirable\nbiases. To address these challenges, we propose a novel approach called\nSELF-ALIGN, which combines principle-driven reasoning and the generative power\nof LLMs for the self-alignment of AI agents with minimal human supervision. Our\napproach encompasses four stages: first, we use an LLM to generate synthetic\nprompts, and a topic-guided method to augment the prompt diversity; second, we\nuse a small set of human-written principles for AI models to follow, and guide\nthe LLM through in-context learning from demonstrations (of principles\napplication) to produce helpful, ethical, and reliable responses to user's\nqueries; third, we fine-tune the original LLM with the high-quality\nself-aligned responses so that the resulting model can generate desirable\nresponses for each query directly without the principle set and the\ndemonstrations anymore; and finally, we offer a refinement step to address the\nissues of overly-brief or indirect responses. Applying SELF-ALIGN to the\nLLaMA-65b base language model, we develop an AI assistant named Dromedary. With\nfewer than 300 lines of human annotations (including &lt; 200 seed prompts, 16\ngeneric principles, and 5 exemplars for in-context learning). Dromedary\nsignificantly surpasses the performance of several state-of-the-art AI systems,\nincluding Text-Davinci-003 and Alpaca, on benchmark datasets with various\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhiqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yikang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qinhong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenfang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1\">David Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalize Segment Anything Model with One Shot. (arXiv:2305.03048v1 [cs.CV])","link":"http://arxiv.org/abs/2305.03048","description":"<p>Driven by large-data pre-training, Segment Anything Model (SAM) has been\ndemonstrated as a powerful and promptable framework, revolutionizing the\nsegmentation models. Despite the generality, customizing SAM for specific\nvisual concepts without man-powered prompting is under explored, e.g.,\nautomatically segmenting your pet dog in different images. In this paper, we\npropose a training-free Personalization approach for SAM, termed as PerSAM.\nGiven only a single image with a reference mask, PerSAM first localizes the\ntarget concept by a location prior, and segments it within other images or\nvideos via three techniques: target-guided attention, target-semantic\nprompting, and cascaded post-refinement. In this way, we effectively adapt SAM\nfor private use without any training. To further alleviate the mask ambiguity,\nwe present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the\nentire SAM, we introduce two learnable weights for multi-scale masks, only\ntraining 2 parameters within 10 seconds for improved performance. To\ndemonstrate our efficacy, we construct a new segmentation dataset, PerSeg, for\npersonalized evaluation, and test our methods on video object segmentation with\ncompetitive performance. Besides, our approach can also enhance DreamBooth to\npersonalize Stable Diffusion for text-to-image generation, which discards the\nbackground disturbance for better target appearance learning. Code is released\nat https://github.com/ZrrSkywalker/Personalize-SAM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengkai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shilin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junting Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QNLP in Practice: Running Compositional Models of Meaning on a Quantum Computer. (arXiv:2102.12846v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.12846","description":"<p>Quantum Natural Language Processing (QNLP) deals with the design and\nimplementation of NLP models intended to be run on quantum hardware. In this\npaper, we present results on the first NLP experiments conducted on Noisy\nIntermediate-Scale Quantum (NISQ) computers for datasets of size greater than\n100 sentences. Exploiting the formal similarity of the compositional model of\nmeaning by Coecke, Sadrzadeh and Clark (2010) with quantum theory, we create\nrepresentations for sentences that have a natural mapping to quantum circuits.\nWe use these representations to implement and successfully train NLP models\nthat solve simple sentence classification tasks on quantum hardware. We conduct\nquantum simulations that compare the syntax-sensitive model of Coecke et al.\nwith two baselines that use less or no syntax; specifically, we implement the\nquantum analogues of a \"bag-of-words\" model, where syntax is not taken into\naccount at all, and of a word-sequence model, where only word order is\nrespected. We demonstrate that all models converge smoothly both in simulations\nand when run on quantum hardware, and that the results are the expected ones\nbased on the nature of the tasks and the datasets used. Another important goal\nof this paper is to describe in a way accessible to AI and NLP researchers the\nmain principles, process and challenges of experiments on quantum hardware. Our\naim in doing this is to take the first small steps in this unexplored research\nterritory and pave the way for practical Quantum Natural Language Processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorenz_R/0/1/0/all/0/1\">Robin Lorenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pearson_A/0/1/0/all/0/1\">Anna Pearson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meichanetzidis_K/0/1/0/all/0/1\">Konstantinos Meichanetzidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kartsaklis_D/0/1/0/all/0/1\">Dimitri Kartsaklis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECOLA: Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations. (arXiv:2203.09590v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09590","description":"<p>Since conventional knowledge embedding models cannot take full advantage of\nthe abundant textual information, there have been extensive research efforts in\nenhancing knowledge embedding using texts. However, existing enhancement\napproaches cannot apply to temporal knowledge graphs (tKGs), which contain\ntime-dependent event knowledge with complex temporal dynamics. Specifically,\nexisting enhancement approaches often assume knowledge embedding is\ntime-independent. In contrast, the entity embedding in tKG models usually\nevolves, which poses the challenge of aligning temporally relevant texts with\nentities. To this end, we propose to study enhancing temporal knowledge\nembedding with textual data in this paper. As an approach to this task, we\npropose Enhanced Temporal Knowledge Embeddings with Contextualized Language\nRepresentations (ECOLA), which takes the temporal aspect into account and\ninjects textual information into temporal knowledge embedding. To evaluate\nECOLA, we introduce three new datasets for training and evaluating ECOLA.\nExtensive experiments show that ECOLA significantly enhances temporal KG\nembedding models with up to 287% relative improvements regarding Hits@1 on the\nlink prediction task. The code and models are publicly available on\nhttps://anonymous.4open.science/r/ECOLA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Ruotong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jindong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zifeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yujia Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppl_H/0/1/0/all/0/1\">Heinz K&#xf6;ppl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MiniDisc: Minimal Distillation Schedule for Language Model Compression. (arXiv:2205.14570v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14570","description":"<p>Recent studies have uncovered that language model distillation is less\neffective when facing a large capacity gap between the teacher and the student,\nand introduced teacher assistant-based distillation to bridge the gap. As a\nconnection, the scale and the performance of the teacher assistant is of vital\nimportance to bring the knowledge from the teacher to the student. However,\nexisting teacher assistant-based methods require maximally many trials before\nscheduling an optimal teacher assistant. To this end, we propose a minimal\ndistillation schedule (MiniDisc) for scheduling the optimal teacher assistant\nin minimally one trial. In particular, motivated by the finding that the\nperformance of the student is positively correlated to the scale-performance\ntradeoff of the teacher assistant, MiniDisc is designed with a\n$\\lambda$-tradeoff to measure the optimality of the teacher assistant without\ntrial distillation to the student. MiniDisc then can schedule the optimal\nteacher assistant with the best $\\lambda$-tradeoff in a sandwich framework.\nMiniDisc is evaluated with an extensive set of experiments on GLUE.\nExperimental results demonstrate the improved efficiency our MiniDisc compared\nto several state-of-the-art baselines. We further apply MiniDisc to a language\nmodel with billions of parameters and show its scalability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiahao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yunsen Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks. (arXiv:2205.15171v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.15171","description":"<p>Societal biases are reflected in large pre-trained language models and their\nfine-tuned versions on downstream tasks. Common in-processing bias mitigation\napproaches, such as adversarial training and mutual information removal,\nintroduce additional optimization criteria, and update the model to reach a new\ndebiased state. However, in practice, end-users and practitioners might prefer\nto switch back to the original model, or apply debiasing only on a specific\nsubset of protected attributes. To enable this, we propose a novel modular bias\nmitigation approach, consisting of stand-alone highly sparse debiasing\nsubnetworks, where each debiasing module can be integrated into the core model\non-demand at inference time. Our approach draws from the concept of \\emph{diff}\npruning, and proposes a novel training regime adaptable to various\nrepresentation disentanglement optimizations. We conduct experiments on three\nclassification tasks with gender, race, and age as protected attributes. The\nresults show that our modular approach, while maintaining task performance,\nimproves (or at least remains on-par with) the effectiveness of bias mitigation\nin comparison with baseline finetuning. Particularly on a two-attribute\ndataset, our approach with separately learned debiasing subnetworks shows\neffective utilization of either or both the subnetworks for selective bias\nmitigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hauzenberger_L/0/1/0/all/0/1\">Lukas Hauzenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masoudian_S/0/1/0/all/0/1\">Shahed Masoudian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Deepak Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1\">Markus Schedl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Incremental Event Detection. (arXiv:2209.01979v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.01979","description":"<p>Event detection tasks can enable the quick detection of events from texts and\nprovide powerful support for downstream natural language processing tasks. Most\nsuch methods can only detect a fixed set of predefined event classes. To extend\nthem to detect a new class without losing the ability to detect old classes\nrequires costly retraining of the model from scratch. Incremental learning can\neffectively solve this problem, but it requires abundant data of new classes.\nIn practice, however, the lack of high-quality labeled data of new event\nclasses makes it difficult to obtain enough data for model training. To address\nthe above mentioned issues, we define a new task, few-shot incremental event\ndetection, which focuses on learning to detect a new event class with limited\ndata, while retaining the ability to detect old classes to the extent possible.\nWe created a benchmark dataset IFSED for the few-shot incremental event\ndetection task based on FewEvent and propose two benchmarks, IFSED-K and\nIFSED-KP. Experimental results show that our approach has a higher F1-score\nthan baseline methods and is more stable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hanwen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jianyong Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is It Worth the (Environmental) Cost? Limited Evidence for Temporal Adaptation via Continuous Training. (arXiv:2210.07365v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07365","description":"<p>Language is constantly changing and evolving, leaving language models to\nbecome quickly outdated. Consequently, we should continuously update our models\nwith new data to expose them to new events and facts. However, that requires\nadditional computing, which means new carbon emissions. Do any measurable\nbenefits justify this cost? This paper looks for empirical evidence to support\ncontinuous training. We reproduce existing benchmarks and extend them to\ninclude additional time periods, models, and tasks. Our results show that the\ndownstream task performance of temporally adapted English models for social\nmedia data do not improve over time. Pretrained models without temporal\nadaptation are actually significantly more effective and efficient. However, we\nalso note a lack of suitable temporal benchmarks. Our findings invite a\ncritical reflection on when and how to temporally adapt language models,\naccounting for sustainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1\">Giuseppe Attanasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nozza_D/0/1/0/all/0/1\">Debora Nozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Math Word Problems via Cooperative Reasoning induced Language Models. (arXiv:2210.16257v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16257","description":"<p>Large-scale pre-trained language models (PLMs) bring new opportunities to\nchallenging problems, especially those that need high-level intelligence, such\nas the math word problem (MWPs). However, directly applying existing PLMs to\nMWPs can fail as the generation process lacks sufficient supervision and thus\nlacks fast adaptivity as humans. We notice that human reasoning has a dual\nreasoning framework that consists of an immediate reaction system (system 1)\nand a delicate reasoning system (system 2), where the entire reasoning is\ndetermined by their interaction. This inspires us to develop a cooperative\nreasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe),\nresulting in a human-like reasoning architecture with system 1 as the generator\nand system 2 as the verifier. In our approach, the generator is responsible for\ngenerating reasoning paths, and the verifiers are used to supervise the\nevaluation in order to obtain reliable feedback for the generator. We evaluate\nour CoRe framework on several mathematical reasoning datasets and achieve\ndecent improvement over state-of-the-art methods, up to 9.6% increase over best\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_R/0/1/0/all/0/1\">Ruyi Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization. (arXiv:2212.07672v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2212.07672","description":"<p>Multimodal abstractive summarization (MAS) aims to produce a concise summary\ngiven the multimodal data (text and vision). Existing studies mainly focus on\nhow to effectively use the visual features from the perspective of an article,\nhaving achieved impressive success on the high-resource English dataset.\nHowever, less attention has been paid to the visual features from the\nperspective of the summary, which may limit the model performance, especially\nin the low- and zero-resource scenarios. In this paper, we propose to improve\nthe summary quality through summary-oriented visual features. To this end, we\ndevise two auxiliary tasks including vision to summary task and masked image\nmodeling task. Together with the main summarization task, we optimize the MAS\nmodel via the training objectives of all these tasks. By these means, the MAS\nmodel can be enhanced by capturing the summary-oriented visual features,\nthereby yielding more accurate summaries. Experiments on 44 languages, covering\nmid-high-, low-, and zero-resource scenarios, verify the effectiveness and\nsuperiority of the proposed approach, which achieves state-of-the-art\nperformance under all scenarios. Additionally, we will contribute a large-scale\nmultilingual multimodal abstractive summarization (MM-Sum) dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation. (arXiv:2212.09246v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09246","description":"<p>Pre-trained language models, despite their rapid advancements powered by\nscale, still fall short of robust commonsense capabilities. And yet, scale\nappears to be the winning recipe; after all, the largest models seem to have\nacquired the largest amount of commonsense capabilities. Or is it?\n</p>\n<p>In this paper, we investigate the possibility of a seemingly impossible\nmatch: can smaller language models with dismal commonsense capabilities (i.e.,\nGPT-2), ever win over models that are orders of magnitude larger and better\n(i.e., GPT-3), if the smaller models are powered with novel commonsense\ndistillation algorithms? The key intellectual question we ask here is whether\nit is possible, if at all, to design a learning algorithm that does not benefit\nfrom scale, yet leads to a competitive level of commonsense acquisition. In\nthis work, we study the generative models of commonsense knowledge, focusing on\nthe task of generating generics, statements of commonsense facts about everyday\nconcepts, e.g., birds can fly.\n</p>\n<p>We introduce a novel commonsense distillation framework, I2D2, that loosely\nfollows the Symbolic Knowledge Distillation of West et al. but breaks the\ndependence on the extreme-scale models as the teacher model by two innovations:\n(1) the novel adaptation of NeuroLogic Decoding to enhance the generation\nquality of the weak, off-the-shelf language models, and (2) self-imitation\nlearning to iteratively learn from the model's own enhanced commonsense\nacquisition capabilities. Empirical results suggest that scale is not the only\nway, as novel algorithms can be a promising alternative. Moreover, our study\nleads to a new corpus of generics, Gen-A-Tomic, that is of the largest and\nhighest quality available to date.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Lianhui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09597","description":"<p>Reasoning, as an essential ability for complex problem-solving, can provide\nback-end support for various real-world applications, such as medical\ndiagnosis, negotiation, etc. This paper provides a comprehensive survey of\ncutting-edge research on reasoning with language model prompting. We introduce\nresearch works with comparisons and summaries and provide systematic resources\nto help beginners. We also discuss the potential reasons for emerging such\nreasoning abilities and highlight future research directions. Resources are\navailable at https://github.com/zjunlp/Prompt4ReasoningPapers (updated\nperiodically).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Shuofei Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yixin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers. (arXiv:2212.10325v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10325","description":"<p>Diffusion model, a new generative modelling paradigm, has achieved great\nsuccess in image, audio, and video generation. However, considering the\ndiscrete categorical nature of text, it is not trivial to extend continuous\ndiffusion models to natural language, and text diffusion models are less\nstudied. Sequence-to-sequence text generation is one of the essential natural\nlanguage processing topics. In this work, we apply diffusion models to approach\nsequence-to-sequence text generation, and explore whether the superiority\ngeneration performance of diffusion model can transfer to natural language\ndomain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence\ngeneration. SeqDiffuSeq uses an encoder-decoder Transformers architecture to\nmodel denoising function. In order to improve generation quality, SeqDiffuSeq\ncombines the self-conditioning technique and a newly proposed adaptive noise\nschedule technique. The adaptive noise schedule has the difficulty of denoising\nevenly distributed across time steps, and considers exclusive noise schedules\nfor tokens at different positional order. Experiment results illustrate the\ngood performance on sequence-to-sequence generation in terms of text quality\nand inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11916","description":"<p>In recent years, pre-trained large language models have demonstrated\nremarkable efficiency in achieving an inference-time few-shot learning\ncapability known as in-context learning. However, existing literature has\nhighlighted the sensitivity of this capability to the selection of few-shot\ndemonstrations. The underlying mechanisms by which this capability arises from\nregular language model pretraining objectives remain poorly understood. In this\nstudy, we aim to examine the in-context learning phenomenon through a Bayesian\nlens, viewing large language models as topic models that implicitly infer\ntask-related information from demonstrations. On this premise, we propose an\nalgorithm for selecting optimal demonstrations from a set of annotated data and\ndemonstrate a significant 12.5% improvement relative to the random selection\nbaseline, averaged over eight GPT2 and GPT3 models on eight different\nreal-world text classification datasets. Our empirical findings support our\nhypothesis that large language models implicitly infer a latent concept\nvariable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steyvers_M/0/1/0/all/0/1\">Mark Steyvers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data. (arXiv:2302.00674v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.00674","description":"<p>Few-shot learning is valuable in many real-world applications, but learning a\ngeneralizable model without overfitting to the few labeled datapoints is\nchallenging. In this work, we focus on Few-shot Learning with Auxiliary Data\n(FLAD), a training paradigm that assumes access to auxiliary data during\nfew-shot learning in hopes of improving generalization. Previous works have\nproposed automated methods for mixing auxiliary and target data, but these\nmethods typically scale linearly (or worse) with the number of auxiliary\ndatasets, limiting their practicality. In this work we relate FLAD to the\nexplore-exploit dilemma that is central to the multi-armed bandit setting and\nderive algorithms whose computational complexity is independent of the number\nof auxiliary datasets, allowing us to scale to 100x more auxiliary datasets\nthan prior methods. We propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and\ncompare them with prior FLAD methods that either explore or exploit, finding\nthat the combination of exploration and exploitation is crucial. Through\nextensive experimentation we find that our methods outperform all pre-existing\nFLAD methods by 4% and lead to the first 3 billion parameter language models\nthat outperform the 175 billion parameter GPT-3. Overall, our work suggests\nthat the discovery of better, more efficient mixing strategies for FLAD may\nprovide a viable path towards substantially improving generalization in\nfew-shot learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Large Language Models. (arXiv:2303.18223v9 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.18223","description":"<p>Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yupeng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Yingqian Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zican Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yushuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xinyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2304.04099","description":"<p>Unsupervised discovery of stories with correlated news articles in real-time\nhelps people digest massive news streams without expensive human annotations. A\ncommon approach of the existing studies for unsupervised online story discovery\nis to represent news articles with symbolic- or graph-based embedding and\nincrementally cluster them into stories. Recent large language models are\nexpected to improve the embedding further, but a straightforward adoption of\nthe models by indiscriminately encoding all information in articles is\nineffective to deal with text-rich and evolving news streams. In this work, we\npropose a novel thematic embedding with an off-the-shelf pretrained sentence\nencoder to dynamically represent articles and stories by considering their\nshared temporal themes. To realize the idea for unsupervised online story\ndiscovery, a scalable framework USTORY is introduced with two main techniques,\ntheme- and time-aware dynamic embedding and novelty-aware adaptive clustering,\nfueled by lightweight story summaries. A thorough evaluation with real news\ndata sets demonstrates that USTORY achieves higher story discovery performances\nthan baselines while being robust and scalable to various streaming settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Susik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongha Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The language of sounds unheard: Exploring musical timbre semantics of large language models. (arXiv:2304.07830v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.07830","description":"<p>Semantic dimensions of sound have been playing a central role in\nunderstanding the nature of auditory sensory experience as well as the broader\nrelation between perception, language, and meaning. Accordingly, and given the\nrecent proliferation of large language models (LLMs), here we asked whether\nsuch models exhibit an organisation of perceptual semantics similar to those\nobserved in humans. Specifically, we prompted ChatGPT, a chatbot based on a\nstate-of-the-art LLM, to rate musical instrument sounds on a set of 20 semantic\nscales. We elicited multiple responses in separate chats, analogous to having\nmultiple human raters. ChatGPT generated semantic profiles that only partially\ncorrelated with human ratings, yet showed robust agreement along well-known\npsychophysical dimensions of musical sounds such as brightness (bright-dark)\nand pitch height (deep-high). Exploratory factor analysis suggested the same\ndimensionality but different spatial configuration of a latent factor space\nbetween the chatbot and human ratings. Unexpectedly, the chatbot showed degrees\nof internal variability that were comparable in magnitude to that of human\nratings. Our work highlights the potential of LLMs to capture salient\ndimensions of human sensory experience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siedenburg_K/0/1/0/all/0/1\">Kai Siedenburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1\">Charalampos Saitis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers. (arXiv:2304.09116v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2304.09116","description":"<p>Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild\ndatasets is important to capture the diversity in human speech such as speaker\nidentities, prosodies, and styles (e.g., singing). Current large TTS systems\nusually quantize speech into discrete tokens and use language models to\ngenerate these tokens one by one, which suffer from unstable prosody, word\nskipping/repeating issue, and poor voice quality. In this paper, we develop\nNaturalSpeech 2, a TTS system that leverages a neural audio codec with residual\nvector quantizers to get the quantized latent vectors and uses a diffusion\nmodel to generate these latent vectors conditioned on text input. To enhance\nthe zero-shot capability that is important to achieve diverse speech synthesis,\nwe design a speech prompting mechanism to facilitate in-context learning in the\ndiffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to\nlarge-scale datasets with 44K hours of speech and singing data and evaluate its\nvoice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS\nsystems by a large margin in terms of prosody/timbre similarity, robustness,\nand voice quality in a zero-shot setting, and performs novel zero-shot singing\nsynthesis with only a speech prompt. Audio samples are available at\nhttps://speechresearch.github.io/naturalspeech2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shen_K/0/1/0/all/0/1\">Kai Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ju_Z/0/1/0/all/0/1\">Zeqian Ju</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yanqing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.14108","description":"<p>Large multimodal datasets have been instrumental in recent breakthroughs such\nas CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive\nthe same research attention as model architectures or training algorithms. To\naddress this shortcoming in the machine learning ecosystem, we introduce\nDataComp, a benchmark where the training code is fixed and researchers innovate\nby proposing new training sets. We provide a testbed for dataset experiments\ncentered around a new candidate pool of 12.8B image-text pairs from Common\nCrawl. Participants in our benchmark design new filtering techniques or curate\nnew data sources and then evaluate their new dataset by running our\nstandardized CLIP training code and testing on 38 downstream test sets. Our\nbenchmark consists of multiple scales, with four candidate pool sizes and\nassociated compute budgets ranging from 12.8M to 12.8B samples seen during\ntraining. This multi-scale design facilitates the study of scaling trends and\nmakes the benchmark accessible to researchers with varying resources.\n</p>\n<p>Our baseline experiments show that the DataComp workflow is a promising way\nof improving multimodal datasets. We introduce DataComp-1B, a dataset created\nby applying a simple filtering algorithm to the 12.8B candidate pool. The\nresulting 1.4B subset enables training a CLIP ViT-L/14 from scratch to 79.2%\nzero-shot accuracy on ImageNet. Our new ViT-L/14 model outperforms a larger\nViT-g/14 trained on LAION-2B by 0.7 percentage points while requiring 9x less\ntraining compute. We also outperform OpenAI's CLIP ViT-L/14 by 3.7 percentage\npoints, which is trained with the same compute budget as our model. These gains\nhighlight the potential for improving model performance by carefully curating\ntraining sets. We view DataComp-1B as only the first step and hope that\nDataComp paves the way toward the next generation of multimodal datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1\">Samir Yitzhak Gadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_A/0/1/0/all/0/1\">Alex Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayase_J/0/1/0/all/0/1\">Jonathan Hayase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smyrnis_G/0/1/0/all/0/1\">Georgios Smyrnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thao Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marten_R/0/1/0/all/0/1\">Ryan Marten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Dhruba Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jieyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orgad_E/0/1/0/all/0/1\">Eyal Orgad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Entezari_R/0/1/0/all/0/1\">Rahim Entezari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daras_G/0/1/0/all/0/1\">Giannis Daras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratt_S/0/1/0/all/0/1\">Sarah Pratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1\">Vivek Ramanujan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marathe_K/0/1/0/all/0/1\">Kalyani Marathe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mussmann_S/0/1/0/all/0/1\">Stephen Mussmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vencu_R/0/1/0/all/0/1\">Richard Vencu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherti_M/0/1/0/all/0/1\">Mehdi Cherti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1\">Pang Wei Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saukh_O/0/1/0/all/0/1\">Olga Saukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1\">Alexander Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaumont_R/0/1/0/all/0/1\">Romain Beaumont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Sewoong Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alex Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1\">Jenia Jitsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1\">Vaishaal Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting Vision and Language Generative Models with Semantic Visual Priors. (arXiv:2304.14986v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.14986","description":"<p>When applied to Image-to-text models, interpretability methods often provide\ntoken-by-token explanations namely, they compute a visual explanation for each\ntoken of the generated sequence. Those explanations are expensive to compute\nand unable to comprehensively explain the model's output. Therefore, these\nmodels often require some sort of approximation that eventually leads to\nmisleading explanations. We develop a framework based on SHAP, that allows for\ngenerating comprehensive, meaningful explanations leveraging the meaning\nrepresentation of the output sequence as a whole. Moreover, by exploiting\nsemantic priors in the visual backbone, we extract an arbitrary number of\nfeatures that allows the efficient computation of Shapley values on large-scale\nmodels, generating at the same time highly meaningful visual explanations. We\ndemonstrate that our method generates semantically more expressive explanations\nthan traditional methods at a lower compute cost and that it can be generalized\nover other explainability methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cafagna_M/0/1/0/all/0/1\">Michele Cafagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1\">Lina M. Rojas-Barahona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot In-context Learning for Knowledge Base Question Answering. (arXiv:2305.01750v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01750","description":"<p>Question answering over knowledge bases is considered a difficult problem due\nto the challenge of generalizing to a wide variety of possible natural language\nquestions. Additionally, the heterogeneity of knowledge base schema items\nbetween different knowledge bases often necessitates specialized training for\ndifferent knowledge base question-answering (KBQA) datasets. To handle\nquestions over diverse KBQA datasets with a unified training-free framework, we\npropose KB-BINDER, which for the first time enables few-shot in-context\nlearning over KBQA tasks. Firstly, KB-BINDER leverages large language models\nlike Codex to generate logical forms as the draft for a specific question by\nimitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge\nbase to bind the generated draft to an executable one with BM25 score matching.\nThe experimental results on four public heterogeneous KBQA datasets show that\nKB-BINDER can achieve a strong performance with only a few in-context\ndemonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even\noutperform the state-of-the-art trained models. On GrailQA and WebQSP, our\nmodel is also on par with other fully-trained models. We believe KB-BINDER can\nserve as an important baseline for future research. Our code is available at\nhttps://github.com/ltl3A87/KB-BINDER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianle Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xueguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_A/0/1/0/all/0/1\">Alex Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01876","description":"<p>Concepts benefit natural language understanding but are far from complete in\nexisting knowledge graphs (KGs). Recently, pre-trained language models (PLMs)\nhave been widely used in text-based concept extraction (CE). However, PLMs tend\nto mine the co-occurrence associations from massive corpus as pre-trained\nknowledge rather than the real causal effect between tokens. As a result, the\npre-trained knowledge confounds PLMs to extract biased concepts based on\nspurious co-occurrence correlations, inevitably resulting in low precision. In\nthis paper, through the lens of a Structural Causal Model (SCM), we propose\nequipping the PLM-based extractor with a knowledge-guided prompt as an\nintervention to alleviate concept bias. The prompt adopts the topic of the\ngiven entity from the existing knowledge in KGs to mitigate the spurious\nco-occurrence correlations between entities and biased concepts. Our extensive\nexperiments on representative multilingual KG datasets justify that our\nproposed prompt can effectively alleviate concept bias and improve the\nperformance of PLM-based CE models.The code has been released on\nhttps://github.com/siyuyuan/KPCE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Siyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Deqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shuyu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Rui Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Contrastive Learning of Sentence Embeddings from AI Feedback. (arXiv:2305.01918v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01918","description":"<p>Contrastive learning has become a popular approach in natural language\nprocessing, particularly for the learning of sentence embeddings. However, the\ndiscrete nature of natural language makes it difficult to ensure the quality of\npositive and negative sample pairs generated through data augmentation methods.\nAlthough supervised contrastive learning can produce more accurate sample pairs\nwith human feedback labels, it still lacks fine-grained training signals. In\nthis paper, we propose to improve \\textbf{C}ontrastive \\textbf{L}earning of\nsentence embeddings from \\textbf{AI} \\textbf{F}eedback \\textbf{(CLAIF)}. Our\nmethod utilizes AI feedback from large pre-trained language models (LLMs) to\nconstruct sample pairs with fine-grained sample similarity scores to improve\ncontrastive learning. Besides, we combine human feedback and AI feedback to\nprovide better supervision signals for supervised contrastive learning of\nsentence embeddings. Experimental results show that our method achieves\nstate-of-the-art performance on several semantic textual similarity (STS) and\ntransfer learning tasks compared to other unsupervised and supervised\ncontrastive learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qinyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaogui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs. (arXiv:2305.01938v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01938","description":"<p>Discrete reasoning over table-text documents (e.g., financial reports) gains\nincreasing attention in recent two years. Existing works mostly simplify this\nchallenge by manually selecting and transforming document pages to structured\ntables and paragraphs, hindering their practical application. In this work, we\nexplore a more realistic problem setting in the form of TAT-DQA, i.e. to answer\nthe question over a visually-rich table-text document. Specifically, we propose\na novel Doc2SoarGraph framework with enhanced discrete reasoning capability by\nharnessing the differences and correlations among different elements (e.g.,\nquantities, dates) of the given question and document with Semantic-oriented\nhierarchical Graph structures. We conduct extensive experiments on TAT-DQA\ndataset, and the results show that our proposed framework outperforms the best\nbaseline model by 17.73% and 16.91% in terms of Exact Match (EM) and F1 score\nrespectively on the test set, achieving the new state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengbin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zifeng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Moxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Statistical Exploration of Text Partition Into Constituents: The Case of the Priestly Source in the Books of Genesis and Exodus. (arXiv:2305.02170v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02170","description":"<p>We present a pipeline for a statistical textual exploration, offering a\nstylometry-based explanation and statistical validation of a hypothesized\npartition of a text. Given a parameterization of the text, our pipeline: (1)\ndetects literary features yielding the optimal overlap between the hypothesized\nand unsupervised partitions, (2) performs a hypothesis-testing analysis to\nquantify the statistical significance of the optimal overlap, while conserving\nimplicit correlations between units of text that are more likely to be grouped,\nand (3) extracts and quantifies the importance of features most responsible for\nthe classification, estimates their statistical stability and cluster-wise\nabundance.\n</p>\n<p>We apply our pipeline to the first two books in the Bible, where one\nstylistic component stands out in the eyes of biblical scholars, namely, the\nPriestly component. We identify and explore statistically significant stylistic\ndifferences between the Priestly and non-Priestly components.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoffe_G/0/1/0/all/0/1\">Gideon Yoffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhler_A/0/1/0/all/0/1\">Axel B&#xfc;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dershowitz_N/0/1/0/all/0/1\">Nachum Dershowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finkelstein_I/0/1/0/all/0/1\">Israel Finkelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piasetzky_E/0/1/0/all/0/1\">Eli Piasetzky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romer_T/0/1/0/all/0/1\">Thomas R&#xf6;mer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sober_B/0/1/0/all/0/1\">Barak Sober</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simplified TinyBERT: Knowledge Distillation for Document Retrieval. (arXiv:2009.07531v2 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/2009.07531","description":"<p>Despite the effectiveness of utilizing the BERT model for document ranking,\nthe high computational cost of such approaches limits their uses. To this end,\nthis paper first empirically investigates the effectiveness of two knowledge\ndistillation models on the document ranking task. In addition, on top of the\nrecently proposed TinyBERT model, two simplifications are proposed. Evaluations\non two different and widely-used benchmarks demonstrate that Simplified\nTinyBERT with the proposed simplifications not only boosts TinyBERT, but also\nsignificantly outperforms BERT-Base when providing 15$\\times$ speedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuanang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Ben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Kai Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yingfei Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v2 [q-bio.QM] CROSS LISTED)","link":"http://arxiv.org/abs/2212.00735","description":"<p>In the field of antibody engineering, an essential task is to design a novel\nantibody whose paratopes bind to a specific antigen with correct epitopes.\nUnderstanding antibody structure and its paratope can facilitate a mechanistic\nunderstanding of its function. Therefore, antibody structure prediction from\nits sequence alone has always been a highly valuable problem for de novo\nantibody design. AlphaFold2, a breakthrough in the field of structural biology,\nprovides a solution to predict protein structure based on protein sequences and\ncomputationally expensive coevolutionary multiple sequence alignments (MSAs).\nHowever, the computational efficiency and undesirable prediction accuracy of\nantibodies, especially on the complementarity-determining regions (CDRs) of\nantibodies limit their applications in the industrially high-throughput drug\ndesign. To learn an informative representation of antibodies, we employed a\ndeep antibody language model (ALM) on curated sequences from the observed\nantibody space database via a transformer model. We also developed a novel\nmodel named xTrimoABFold to predict antibody structure from antibody sequence\nbased on the pretrained ALM as well as efficient evoformers and structural\nmodules. The model was trained end-to-end on the antibody structures in PDB by\nminimizing the ensemble loss of domain-specific focal loss on CDR and the\nframe-aligned point loss. xTrimoABFold outperforms AlphaFold2 and other protein\nlanguage model based SOTAs, e.g., OmegaFold, HelixFold-Single, and IgFold with\na large significant margin (30+\\% improvement on RMSD) while performing 151\ntimes faster than AlphaFold2. To the best of our knowledge, xTrimoABFold\nachieved state-of-the-art antibody structure prediction. Its improvement in\nboth accuracy and efficiency makes it a valuable tool for de novo antibody\ndesign and could make further improvements in immuno-theory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_Y/0/1/0/all/0/1\">Yining Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gong_X/0/1/0/all/0/1\">Xumeng Gong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1\">Shaochuan Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_B/0/1/0/all/0/1\">Bing Yang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sun_Y/0/1/0/all/0/1\">YiWu Sun</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shi_C/0/1/0/all/0/1\">Chuan Shi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_Y/0/1/0/all/0/1\">Yangang Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Song_L/0/1/0/all/0/1\">Le Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Imperceptible Document Manipulations against Neural Ranking Models. (arXiv:2305.01860v1 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/2305.01860","description":"<p>Adversarial attacks have gained traction in order to identify potential\nvulnerabilities in neural ranking models (NRMs), but current attack methods\noften introduce grammatical errors, nonsensical expressions, or incoherent text\nfragments, which can be easily detected. Additionally, current methods rely\nheavily on the use of a well-imitated surrogate NRM to guarantee the attack\neffect, which makes them difficult to use in practice. To address these issues,\nwe propose a framework called Imperceptible DocumEnt Manipulation (IDEM) to\nproduce adversarial documents that are less noticeable to both algorithms and\nhumans. IDEM instructs a well-established generative language model, such as\nBART, to generate connection sentences without introducing easy-to-detect\nerrors, and employs a separate position-wise merging strategy to balance\nrelevance and coherence of the perturbed text. Experimental results on the\npopular MS MARCO benchmark demonstrate that IDEM can outperform strong\nbaselines while preserving fluency and correctness of the target documents as\nevidenced by automatic and human evaluations. Furthermore, the separation of\nadversarial text generation from the surrogate NRM makes IDEM more robust and\nless affected by the quality of the surrogate NRM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuanang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Ben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yingfei Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}