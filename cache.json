{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-29T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Bidirectional Representations for Low Resource Spoken Language Understanding. (arXiv:2211.14320v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14320","description":"<p>Most spoken language understanding systems use a pipeline approach composed\nof an automatic speech recognition interface and a natural language\nunderstanding module. This approach forces hard decisions when converting\ncontinuous inputs into discrete language symbols. Instead, we propose a\nrepresentation model to encode speech in rich bidirectional encodings that can\nbe used for downstream tasks such as intent prediction. The approach uses a\nmasked language modelling objective to learn the representations, and thus\nbenefits from both the left and right contexts. We show that the performance of\nthe resulting encodings before fine-tuning is better than comparable models on\nmultiple datasets, and that fine-tuning the top layers of the representation\nmodel improves the current state of the art on the Fluent Speech Command\ndataset, also in a low-data regime, when a limited amount of labelled data is\nused for training. Furthermore, we propose class attention as a spoken language\nunderstanding module, efficient both in terms of speed and number of\nparameters. Class attention can be used to visually explain the predictions of\nour model, which goes a long way in understanding how the model makes\npredictions. We perform experiments in English and in Dutch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meeus_Q/0/1/0/all/0/1\">Quentin Meeus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+hamme_H/0/1/0/all/0/1\">Hugo Van hamme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Machine Learning, Natural Language Processing Analysis of Youth Perspectives: Key Trends and Focus Areas for Sustainable Youth Development Policies. (arXiv:2211.14321v1 [cs.CY])","link":"http://arxiv.org/abs/2211.14321","description":"<p>Investing in children and youth is a critical step towards inclusive,\nequitable, and sustainable development for current and future generations.\nSeveral international agendas for accomplishing common global goals emphasize\nthe need for active youth participation and engagement for sustainable\ndevelopment. The 2030 Agenda for Sustainable Development emphasizes the need\nfor youth engagement and the inclusion of youth perspectives as an important\nstep toward addressing each of the 17 Sustainable Development Goals. The aim of\nthis study is to analyze youth perspectives, values, and sentiments towards\nissues addressed by the 17 Sustainable Development Goals through social network\nanalysis using machine learning. Social network data collected during 7 major\nsustainability conferences aimed at engaging children and youth is analyzed\nusing natural language processing techniques for sentiment analysis. This data\ncategorized using a natural language processing text classifier trained on a\nsample dataset of social network data during the 7 youth sustainability\nconferences for deeper understanding of youth perspectives in relation to the\nSDGs. Machine learning identified demographic and location attributes and\nfeatures are utilized in order to identify bias and demographic differences\nbetween ages, gender, and race among youth. Using natural language processing,\nthe qualitative data collected from over 7 different countries in 3 languages\nare systematically translated, categorized, and analyzed, revealing key trends\nand focus areas for sustainable youth development policies. The obtained\nresults reveal the general youth's depth of knowledge on sustainable\ndevelopment and their attitudes towards each of the 17 SDGs. The findings of\nthis study serve as a guide toward better understanding the interests, roles,\nand perspectives of children and youth in achieving the goals of Agenda 2030.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kushaagra Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Moral- and Event- Centric Inspection of Gender Bias in Fairy Tales at A Large Scale. (arXiv:2211.14358v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14358","description":"<p>Fairy tales are a common resource for young children to learn a language or\nunderstand how a society works. However, gender bias, e.g., stereotypical\ngender roles, in this literature may cause harm and skew children's world view.\nInstead of decades of qualitative and manual analysis of gender bias in fairy\ntales, we computationally analyze gender bias in a fairy tale dataset\ncontaining 624 fairy tales from 7 different cultures. We specifically examine\ngender difference in terms of moral foundations, which are measures of human\nmorality, and events, which reveal human activities associated with each\ncharacter. We find that the number of male characters is two times that of\nfemale characters, showing a disproportionate gender representation. Our\nanalysis further reveal stereotypical portrayals of both male and female\ncharacters in terms of moral foundations and events. Female characters turn out\nmore associated with care-, loyalty- and sanctity- related moral words, while\nmale characters are more associated with fairness- and authority- related moral\nwords. Female characters' events are often about emotion (e.g., weep),\nappearance (e.g., comb), household (e.g., bake), etc.; while male characters'\nevents are more about profession (e.g., hunt), violence (e.g., destroy),\njustice (e.g., judge), etc. Gender bias in terms of moral foundations shows an\nobvious difference across cultures. For example, female characters are more\nassociated with care and sanctity in high uncertainty-avoidance cultures which\nare less open to changes and unpredictability. Based on the results, we propose\nimplications for children's literature and early literacy research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhixuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiaxin Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuning BERT on Partially Annotated NER Corpora. (arXiv:2211.14360v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14360","description":"<p>Most Named Entity Recognition (NER) models operate under the assumption that\ntraining datasets are fully labelled. While it is valid for established\ndatasets like CoNLL 2003 and OntoNotes, sometimes it is not feasible to obtain\nthe complete dataset annotation. These situations may occur, for instance,\nafter selective annotation of entities for cost reduction. This work presents\nan approach to finetuning BERT on such partially labelled datasets using\nself-supervision and label preprocessing. Our approach outperforms the previous\nLSTM-based label preprocessing baseline, significantly improving the\nperformance on poorly labelled datasets. We demonstrate that following our\napproach while finetuning RoBERTa on CoNLL 2003 dataset with only 10% of total\nentities labelled is enough to reach the performance of the baseline trained on\nthe same dataset with 50% of the entities labelled.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scherbakov_V/0/1/0/all/0/1\">Viktor Scherbakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayorov_V/0/1/0/all/0/1\">Vladimir Mayorov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Naughtyformer: A Transformer Understands Offensive Humor. (arXiv:2211.14369v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14369","description":"<p>Jokes are intentionally written to be funny, but not all jokes are created\nthe same. Some jokes may be fit for a classroom of kindergarteners, but others\nare best reserved for a more mature audience. While recent work has shown\nimpressive results on humor detection in text, here we instead investigate the\nmore nuanced task of detecting humor subtypes, especially of the less innocent\nvariety. To that end, we introduce a novel jokes dataset filtered from Reddit\nand solve the subtype classification task using a finetuned Transformer dubbed\nthe Naughtyformer. Moreover, we show that our model is significantly better at\ndetecting offensiveness in jokes compared to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Leonard Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_A/0/1/0/all/0/1\">Alexander Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Steve Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jason Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretability Analysis of Deep Models for COVID-19 Detection. (arXiv:2211.14372v1 [eess.AS])","link":"http://arxiv.org/abs/2211.14372","description":"<p>During the outbreak of COVID-19 pandemic, several research areas joined\nefforts to mitigate the damages caused by SARS-CoV-2. In this paper we present\nan interpretability analysis of a convolutional neural network based model for\nCOVID-19 detection in audios. We investigate which features are important for\nmodel decision process, investigating spectrograms, F0, F0 standard deviation,\nsex and age. Following, we analyse model decisions by generating heat maps for\nthe trained models to capture their attention during the decision process.\nFocusing on a explainable Inteligence Artificial approach, we show that studied\nmodels can taken unbiased decisions even in the presence of spurious data in\nthe training set, given the adequate preprocessing steps. Our best model has\n94.44% of accuracy in detection, with results indicating that models favors\nspectrograms for the decision process, particularly, high energy areas in the\nspectrogram related to prosodic domains, while F0 also leads to efficient\nCOVID-19 detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Silva_D/0/1/0/all/0/1\">Daniel Peixoto Pinto da Silva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gris_L/0/1/0/all/0/1\">Lucas Rafael Stefanel Gris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Finger_M/0/1/0/all/0/1\">Marcelo Finger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Svartman_F/0/1/0/all/0/1\">Flaviane Svartman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raposo_B/0/1/0/all/0/1\">Beatriz Raposo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martins_M/0/1/0/all/0/1\">Marcus Vin&#xed;cius Moreira Martins</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aluisio_S/0/1/0/all/0/1\">Sandra Maria Alu&#xed;sio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Berti_L/0/1/0/all/0/1\">Larissa Cristina Berti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teixeira_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Teixeira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis of Social Biases Present in BERT Variants Across Multiple Languages. (arXiv:2211.14402v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14402","description":"<p>Although large pre-trained language models have achieved great success in\nmany NLP tasks, it has been shown that they reflect human biases from their\npre-training corpora. This bias may lead to undesirable outcomes when these\nmodels are applied in real-world settings. In this paper, we investigate the\nbias present in monolingual BERT models across a diverse set of languages\n(English, Greek, and Persian). While recent research has mostly focused on\ngender-related biases, we analyze religious and ethnic biases as well and\npropose a template-based method to measure any kind of bias, based on sentence\npseudo-likelihood, that can handle morphologically complex languages with\ngender-based adjective declensions. We analyze each monolingual model via this\nmethod and visualize cultural similarities and differences across different\ndimensions of bias. Ultimately, we conclude that current methods of probing for\nbias are highly language-dependent, necessitating cultural insights regarding\nthe unique ways bias is expressed in each language and culture (e.g. through\ncoded language, synecdoche, and other similar linguistic concepts). We also\nhypothesize that higher measured social biases in the non-English BERT models\ncorrelate with user-generated content in their training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Milios_A/0/1/0/all/0/1\">Aristides Milios</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+BehnamGhader_P/0/1/0/all/0/1\">Parishad BehnamGhader</a> (1 and 2) ((1) McGill University, (2) Mila)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLAMI-1M: A Multilingual Image-Text Fashion Dataset. (arXiv:2211.14451v1 [cs.CV])","link":"http://arxiv.org/abs/2211.14451","description":"<p>We introduce GLAMI-1M: the largest multilingual image-text classification\ndataset and benchmark. The dataset contains images of fashion products with\nitem descriptions, each in 1 of 13 languages. Categorization into 191 classes\nhas high-quality annotations: all 100k images in the test set and 75% of the 1M\ntraining set were human-labeled. The paper presents baselines for image-text\nclassification showing that the dataset presents a challenging fine-grained\nclassification problem: The best scoring EmbraceNet model using both visual and\ntextual features achieves 69.7% accuracy. Experiments with a modified Imagen\nmodel show the dataset is also suitable for image generation conditioned on\ntext. The dataset, source code and model checkpoints are published at\nhttps://github.com/glami/glami-1m\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kosar_V/0/1/0/all/0/1\">Vaclav Kosar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoskovec_A/0/1/0/all/0/1\">Anton&#xed;n Hoskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulc_M/0/1/0/all/0/1\">Milan &#x160;ulc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartyzal_R/0/1/0/all/0/1\">Radek Bartyzal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Model for Word Level Language Identification in Code-mixed Kannada-English Texts. (arXiv:2211.14459v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14459","description":"<p>Using code-mixed data in natural language processing (NLP) research currently\ngets a lot of attention. Language identification of social media code-mixed\ntext has been an interesting problem of study in recent years due to the\nadvancement and influences of social media in communication. This paper\npresents the Instituto Polit\\'ecnico Nacional, Centro de Investigaci\\'on en\nComputaci\\'on (CIC) team's system description paper for the CoLI-Kanglish\nshared task at ICON2022. In this paper, we propose the use of a Transformer\nbased model for word-level language identification in code-mixed Kannada\nEnglish texts. The proposed model on the CoLI-Kenglish dataset achieves a\nweighted F1-score of 0.84 and a macro F1-score of 0.61.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tonja_A/0/1/0/all/0/1\">Atnafu Lambebo Tonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yigezu_M/0/1/0/all/0/1\">Mesay Gemeda Yigezu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikova_O/0/1/0/all/0/1\">Olga Kolesnikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tash_M/0/1/0/all/0/1\">Moein Shahiki Tash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidorov_G/0/1/0/all/0/1\">Grigori Sidorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbuk_A/0/1/0/all/0/1\">Alexander Gelbuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SKDBERT: Compressing BERT via Stochastic Knowledge Distillation. (arXiv:2211.14466v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14466","description":"<p>In this paper, we propose Stochastic Knowledge Distillation (SKD) to obtain\ncompact BERT-style language model dubbed SKDBERT. In each iteration, SKD\nsamples a teacher model from a pre-defined teacher ensemble, which consists of\nmultiple teacher models with multi-level capacities, to transfer knowledge into\nstudent model in an one-to-one manner. Sampling distribution plays an important\nrole in SKD. We heuristically present three types of sampling distributions to\nassign appropriate probabilities for multi-level teacher models. SKD has two\nadvantages: 1) it can preserve the diversities of multi-level teacher models\nvia stochastically sampling single teacher model in each iteration, and 2) it\ncan also improve the efficacy of knowledge distillation via multi-level teacher\nmodels when large capacity gap exists between the teacher model and the student\nmodel. Experimental results on GLUE benchmark show that SKDBERT reduces the\nsize of a BERT$_{\\rm BASE}$ model by 40% while retaining 99.5% performances of\nlanguage understanding and being 100% faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zixiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Guoqing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Better Document-level Relation Extraction via Iterative Inference. (arXiv:2211.14470v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14470","description":"<p>Document-level relation extraction (RE) aims to extract the relations between\nentities from the input document that usually containing many\ndifficultly-predicted entity pairs whose relations can only be predicted\nthrough relational inference. Existing methods usually directly predict the\nrelations of all entity pairs of input document in a one-pass manner, ignoring\nthe fact that predictions of some entity pairs heavily depend on the predicted\nresults of other pairs. To deal with this issue, in this paper, we propose a\nnovel document-level RE model with iterative inference. Our model is mainly\ncomposed of two modules: 1) a base module expected to provide preliminary\nrelation predictions on entity pairs; 2) an inference module introduced to\nrefine these preliminary predictions by iteratively dealing with\ndifficultly-predicted entity pairs depending on other pairs in an easy-to-hard\nmanner. Unlike previous methods which only consider feature information of\nentity pairs, our inference module is equipped with two Extended Cross\nAttention units, allowing it to exploit both feature information and previous\npredictions of entity pairs during relational inference. Furthermore, we adopt\na two-stage strategy to train our model. At the first stage, we only train our\nbase module. During the second stage, we train the whole model, where\ncontrastive learning is introduced to enhance the training of inference module.\nExperimental results on three commonly-used datasets show that our model\nconsistently outperforms other competitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yidong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1\">Zhongjian Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_Z/0/1/0/all/0/1\">Zijun Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qingguo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaodong Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PCRED: Zero-shot Relation Triplet Extraction with Potential Candidate Relation Selection and Entity Boundary Detection. (arXiv:2211.14477v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14477","description":"<p>Zero-shot relation triplet extraction (ZeroRTE) aims to extract relation\ntriplets from unstructured texts, while the relation sets at the training and\ntesting stages are disjoint. Previous state-of-the-art method handles this\nchallenging task by leveraging pretrained language models to generate data as\nadditional training samples, which increases the training cost and severely\nconstrains the model performance. We tackle this task from a new perspective\nand propose a novel method named PCRED for ZeroRTE with Potential Candidate\nRelation selection and Entity boundary Detection. The model adopts a\nrelation-first paradigm, which firstly recognizes unseen relations through\ncandidate relation selection. By this approach, the semantics of relations are\nnaturally infused in the context. Entities are extracted based on the context\nand the semantics of relations subsequently. We evaluate our model on two\nZeroRTE datasets. The experiment result shows that our method consistently\noutperforms previous works. Besides, our model does not rely on any additional\ndata, which boasts the advantages of simplicity and effectiveness. Our code is\navailable at https://anonymous.4open.science/r/PCRED.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yuquan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predictive linguistic cues for fake news: a societal artificial intelligence problem. (arXiv:2211.14505v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14505","description":"<p>Media news are making a large part of public opinion and, therefore, must not\nbe fake. News on web sites, blogs, and social media must be analyzed before\nbeing published. In this paper, we present linguistic characteristics of media\nnews items to differentiate between fake news and real news using machine\nlearning algorithms. Neural fake news generation, headlines created by\nmachines, semantic incongruities in text and image captions generated by\nmachine are other types of fake news problems. These problems use neural\nnetworks which mainly control distributional features rather than evidence. We\npropose applying correlation between features set and class, and correlation\namong the features to compute correlation attribute evaluation metric and\ncovariance metric to compute variance of attributes over the news items.\nFeatures unique, negative, positive, and cardinal numbers with high values on\nthe metrics are observed to provide a high area under the curve (AUC) and\nF1-score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aneja_S/0/1/0/all/0/1\">Sandhya Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aneja_N/0/1/0/all/0/1\">Nagender Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexicon-injected Semantic Parsing for Task-Oriented Dialog. (arXiv:2211.14508v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14508","description":"<p>Recently, semantic parsing using hierarchical representations for dialog\nsystems has captured substantial attention. Task-Oriented Parse (TOP), a tree\nrepresentation with intents and slots as labels of nested tree nodes, has been\nproposed for parsing user utterances. Previous TOP parsing methods are limited\non tackling unseen dynamic slot values (e.g., new songs and locations added),\nwhich is an urgent matter for real dialog systems. To mitigate this issue, we\nfirst propose a novel span-splitting representation for span-based parser that\noutperforms existing methods. Then we present a novel lexicon-injected semantic\nparser, which collects slot labels of tree representation as a lexicon, and\ninjects lexical features to the span representation of parser. An additional\nslot disambiguation technique is involved to remove inappropriate span match\noccurrences from the lexicon. Our best parser produces a new state-of-the-art\nresult (87.62%) on the TOP dataset, and demonstrates its adaptability to\nfrequently updated slot lexicon entries in real task-oriented dialog, with no\nneed of retraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaojun Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenlin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baojun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Automatic SOAP Classification System Using Weakly Supervision And Transfer Learning. (arXiv:2211.14539v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14539","description":"<p>In this paper, we introduce a comprehensive framework for developing a\nmachine learning-based SOAP (Subjective, Objective, Assessment, and Plan)\nclassification system without manually SOAP annotated training data or with\nless manually SOAP annotated training data. The system is composed of the\nfollowing two parts: 1) Data construction, 2) A neural network-based SOAP\nclassifier, and 3) Transfer learning framework. In data construction, since a\nmanual construction of a large size training dataset is expensive, we propose a\nrule-based weak labeling method utilizing the structured information of an EHR\nnote. Then, we present a SOAP classifier composed of a pre-trained language\nmodel and bi-directional long-short term memory with conditional random field\n(Bi-LSTM-CRF). Finally, we propose a transfer learning framework that re-uses\nthe trained parameters of the SOAP classifier trained with the weakly labeled\ndataset for datasets collected from another hospital. The proposed weakly\nlabel-based learning model successfully performed SOAP classification (89.99\nF1-score) on the notes collected from the target hospital. Otherwise, in the\nnotes collected from other hospitals and departments, the performance\ndramatically decreased. Meanwhile, we verified that the transfer learning\nframework is advantageous for inter-hospital adaptation of the model increasing\nthe models' performance in every cases. In particular, the transfer learning\napproach was more efficient when the manually annotated data size was smaller.\nWe showed that SOAP classification models trained with our weakly labeling\nalgorithm can perform SOAP classification without manually annotated data on\nthe EHR notes from the same hospital. The transfer learning framework helps\nSOAP classification model's inter-hospital migration with a minimal size of the\nmanually annotated dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1\">Sunjae Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexical Complexity Controlled Sentence Generation. (arXiv:2211.14540v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14540","description":"<p>Text generation rarely considers the control of lexical complexity, which\nlimits its more comprehensive practical application. We introduce a novel task\nof lexical complexity controlled sentence generation, which aims at keywords to\nsentence generation with desired complexity levels. It has enormous potential\nin domains such as grade reading, language teaching and acquisition. The\nchallenge of this task is to generate fluent sentences only using the words of\ngiven complexity levels. We propose a simple but effective approach for this\ntask based on complexity embedding. Compared with potential solutions, our\napproach fuses the representations of the word complexity levels into the model\nto get better control of lexical complexity. And we demonstrate the feasibility\nof the approach for both training models from scratch and fine-tuning the\npre-trained models. To facilitate the research, we develop two datasets in\nEnglish and Chinese respectively, on which extensive experiments are conducted.\nResults show that our approach better controls lexical complexity and generates\nhigher quality sentences than baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jinran Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liner Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Cunliang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Junhui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Erhong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Expressive Text-to-Speech. (arXiv:2211.14548v1 [eess.AS])","link":"http://arxiv.org/abs/2211.14548","description":"<p>The goal of expressive Text-to-speech (TTS) is to synthesize natural speech\nwith desired content, prosody, emotion, or timbre, in high expressiveness. Most\nof previous studies attempt to generate speech from given labels of styles and\nemotions, which over-simplifies the problem by classifying styles and emotions\ninto a fixed number of pre-defined categories. In this paper, we introduce a\nnew task setting, Contextual TTS (CTTS). The main idea of CTTS is that how a\nperson speaks depends on the particular context she is in, where the context\ncan typically be represented as text. Thus, in the CTTS task, we propose to\nutilize such context to guide the speech synthesis process instead of relying\non explicit labels of styles and emotions. To achieve this task, we construct a\nsynthetic dataset and develop an effective framework. Experiments show that our\nframework can generate high-quality expressive speech based on the given\ncontext both in synthetic datasets and real-world scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tu_J/0/1/0/all/0/1\">Jianhong Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1\">Zeyu Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaohuan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_K/0/1/0/all/0/1\">Kai Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_J/0/1/0/all/0/1\">Ju Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Who are you referring to? Weakly supervised coreference resolution with multimodal grounding. (arXiv:2211.14563v1 [cs.CV])","link":"http://arxiv.org/abs/2211.14563","description":"<p>Coreference resolution aims at identifying words and phrases which refer to\nsame entity in a text, a core tool in natural language processing. In this\npaper, we propose a novel task, resolving coreferences in multimodal data,\nlong-form textual descriptions of visual scenes. Most existing image-text\ndatasets only contain short sentences without coreferent expressions, or\ncoreferences are not annotated. To this end, we first introduce a new dataset,\nFlickr30k-Coref in which coreference chains and bounding box localization of\nthese chains are annotated. We propose a new technique that learns to identify\ncoreference chains through weakly supervised grounding from image-text pairs\nand a regularization using prior linguistic knowledge. Our model yields large\nperformance gains over prior work in coreference resolution and weakly\nsupervised grounding of long-form text descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Arushi Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1\">Basura Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Text Representation Methods and Their Genealogy. (arXiv:2211.14591v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14591","description":"<p>In recent years, with the advent of highly scalable\nartificial-neural-network-based text representation methods the field of\nnatural language processing has seen unprecedented growth and sophistication.\nIt has become possible to distill complex linguistic information of text into\nmultidimensional dense numeric vectors with the use of the distributional\nhypothesis. As a consequence, text representation methods have been evolving at\nsuch a quick pace that the research community is struggling to retain knowledge\nof the methods and their interrelations. We contribute threefold to this lack\nof compilation, composition, and systematization by providing a survey of\ncurrent approaches, by arranging them in a genealogy, and by conceptualizing a\ntaxonomy of text representation methods to examine and explain the\nstate-of-the-art. Our research is a valuable guide and reference for artificial\nintelligence researchers and practitioners interested in natural language\nprocessing applications such as recommender systems, chatbots, and sentiment\nanalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siebers_P/0/1/0/all/0/1\">Philipp Siebers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janiesch_C/0/1/0/all/0/1\">Christian Janiesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zschech_P/0/1/0/all/0/1\">Patrick Zschech</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The distribution of syntactic dependency distances. (arXiv:2211.14620v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14620","description":"<p>The syntactic structure of a sentence can be represented as a graph where\nvertices are words and edges indicate syntactic dependencies between them. In\nthis setting, the distance between two syntactically linked words can be\ndefined as the difference between their positions. Here we want to contribute\nto the characterization of the actual distribution of syntactic dependency\ndistances, and unveil its relationship with short-term memory limitations. We\npropose a new double-exponential model in which decay in probability is allowed\nto change after a break-point. This transition could mirror the transition from\nthe processing of words chunks to higher-level structures. We find that a\ntwo-regime model -- where the first regime follows either an exponential or a\npower-law decay -- is the most likely one in all 20 languages we considered,\nindependently of sentence length and annotation style. Moreover, the\nbreak-point is fairly stable across languages and averages values of 4-5 words,\nsuggesting that the amount of words that can be simultaneously processed\nabstracts from the specific language to a high degree. Finally, we give an\naccount of the relation between the best estimated model and the closeness of\nsyntactic dependencies, as measured by a recently introduced optimality score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrini_S/0/1/0/all/0/1\">Sonia Petrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for Discriminative Words in Multidimensional Continuous Feature Space. (arXiv:2211.14631v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14631","description":"<p>Word feature vectors have been proven to improve many NLP tasks. With recent\nadvances in unsupervised learning of these feature vectors, it became possible\nto train it with much more data, which also resulted in better quality of\nlearned features. Since it learns joint probability of latent features of\nwords, it has the advantage that we can train it without any prior knowledge\nabout the goal task we want to solve. We aim to evaluate the universal\napplicability property of feature vectors, which has been already proven to\nhold for many standard NLP tasks like part-of-speech tagging or syntactic\nparsing. In our case, we want to understand the topical focus of text documents\nand design an efficient representation suitable for discriminating different\ntopics. The discriminativeness can be evaluated adequately on text\ncategorisation task. We propose a novel method to extract discriminative\nkeywords from documents. We utilise word feature vectors to understand the\nrelations between words better and also understand the latent topics which are\ndiscussed in the text and not mentioned directly but inferred logically. We\nalso present a simple way to calculate document feature vectors out of\nextracted discriminative words. We evaluate our method on the four most popular\ndatasets for text categorisation. We show how different discriminative metrics\ninfluence the overall results. We demonstrate the effectiveness of our approach\nby achieving state-of-the-art results on text categorisation task using just a\nsmall number of extracted keywords. We prove that word feature vectors can\nsubstantially improve the topical inference of documents' meaning. We conclude\nthat distributed representation of words can be used to build higher levels of\nabstraction as we demonstrate and build feature vectors of documents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sajgalik_M/0/1/0/all/0/1\">Marius Sajgalik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barla_M/0/1/0/all/0/1\">Michal Barla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielikova_M/0/1/0/all/0/1\">Maria Bielikova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender Biases Unexpectedly Fluctuate in the Pre-training Stage of Masked Language Models. (arXiv:2211.14639v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14639","description":"<p>Masked language models pick up gender biases during pre-training. Such biases\nare usually attributed to a certain model architecture and its pre-training\ncorpora, with the implicit assumption that other variations in the pre-training\nprocess, such as the choices of the random seed or the stopping point, have no\neffect on the biases measured. However, we show that severe fluctuations exist\nat the fundamental level of individual templates, invalidating the assumption.\nFurther against the intuition of how humans acquire biases, these fluctuations\nare not correlated with the certainty of the predicted pronouns or the\nprofession frequencies in pre-training corpora. We release our code and data to\nbenefit future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Kenan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hanchun Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel multimodal dynamic fusion network for disfluency detection in spoken utterances. (arXiv:2211.14700v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14700","description":"<p>Disfluency, though originating from human spoken utterances, is primarily\nstudied as a uni-modal text-based Natural Language Processing (NLP) task. Based\non early-fusion and self-attention-based multimodal interaction between text\nand acoustic modalities, in this paper, we propose a novel multimodal\narchitecture for disfluency detection from individual utterances. Our\narchitecture leverages a multimodal dynamic fusion network that adds minimal\nparameters over an existing text encoder commonly used in prior art to leverage\nthe prosodic and acoustic cues hidden in speech. Through experiments, we show\nthat our proposed model achieves state-of-the-art results on the widely used\nEnglish Switchboard for disfluency detection and outperforms prior unimodal and\nmultimodal systems in literature by a significant margin. In addition, we make\na thorough qualitative analysis and show that, unlike text-only systems, which\nsuffer from spurious correlations in the data, our system overcomes this\nproblem through additional cues from speech signals. We make all our codes\npublicly available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_U/0/1/0/all/0/1\">Utkarsh Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sonal Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suri_M/0/1/0/all/0/1\">Manan Suri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BadPrompt: Backdoor Attacks on Continuous Prompts. (arXiv:2211.14719v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14719","description":"<p>The prompt-based learning paradigm has gained much research attention\nrecently. It has achieved state-of-the-art performance on several NLP tasks,\nespecially in the few-shot scenarios. While steering the downstream tasks, few\nworks have been reported to investigate the security problems of the\nprompt-based models. In this paper, we conduct the first study on the\nvulnerability of the continuous prompt learning algorithm to backdoor attacks.\nWe observe that the few-shot scenarios have posed a great challenge to backdoor\nattacks on the prompt-based models, limiting the usability of existing NLP\nbackdoor methods. To address this challenge, we propose BadPrompt, a\nlightweight and task-adaptive algorithm, to backdoor attack continuous prompts.\nSpecially, BadPrompt first generates candidate triggers which are indicative\nfor predicting the targeted label and dissimilar to the samples of the\nnon-targeted labels. Then, it automatically selects the most effective and\ninvisible trigger for each sample with an adaptive trigger optimization\nalgorithm. We evaluate the performance of BadPrompt on five datasets and two\ncontinuous prompt models. The results exhibit the abilities of BadPrompt to\neffectively attack continuous prompts while maintaining high performance on the\nclean test sets, outperforming the baseline models by a large margin. The\nsource code of BadPrompt is publicly available at\nhttps://github.com/papersPapers/BadPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xiangrui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haidong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Sihan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojie Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-PuDu at SemEval-2022 Task 7: A Replaced Token Detection Task Pre-trained Model with Pattern-aware Ensembling for Identifying Plausible Clarifications. (arXiv:2211.14734v1 [cs.CL])","link":"http://arxiv.org/abs/2211.14734","description":"<p>This paper describes our winning system on SemEval 2022 Task 7: Identifying\nPlausible Clarifications of Implicit and Underspecified Phrases in\nInstructional Texts. A replaced token detection pre-trained model is utilized\nwith minorly different task-specific heads for SubTask-A: Multi-class\nClassification and SubTask-B: Ranking. Incorporating a pattern-aware ensemble\nmethod, our system achieves a 68.90% accuracy score and 0.8070 spearman's rank\ncorrelation score surpassing the 2nd place with a large margin by 2.7 and 2.2\npercent points for SubTask-A and SubTask-B, respectively. Our approach is\nsimple and easy to implement, and we conducted ablation studies and qualitative\nand quantitative analyses for the working strategies used in our system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Junyuan Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yanjun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Li Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guixiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MNER-QG: An End-to-End MRC framework for Multimodal Named Entity Recognition with Query Grounding. (arXiv:2211.14739v1 [cs.CV])","link":"http://arxiv.org/abs/2211.14739","description":"<p>Multimodal named entity recognition (MNER) is a critical step in information\nextraction, which aims to detect entity spans and classify them to\ncorresponding entity types given a sentence-image pair. Existing methods either\n(1) obtain named entities with coarse-grained visual clues from attention\nmechanisms, or (2) first detect fine-grained visual regions with toolkits and\nthen recognize named entities. However, they suffer from improper alignment\nbetween entity types and visual regions or error propagation in the two-stage\nmanner, which finally imports irrelevant visual information into texts. In this\npaper, we propose a novel end-to-end framework named MNER-QG that can\nsimultaneously perform MRC-based multimodal named entity recognition and query\ngrounding. Specifically, with the assistance of queries, MNER-QG can provide\nprior knowledge of entity types and visual regions, and further enhance\nrepresentations of both texts and images. To conduct the query grounding task,\nwe provide manual annotations and weak supervisions that are obtained via\ntraining a highly flexible visual grounding model with transfer learning. We\nconduct extensive experiments on two public MNER datasets, Twitter2015 and\nTwitter2017. Experimental results show that MNER-QG outperforms the current\nstate-of-the-art models on the MNER task, and also improves the query grounding\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Meihuizi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lejian Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhendong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaqi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigation as the Attacker Wishes? Towards Building Byzantine-Robust Embodied Agents under Federated Learning. (arXiv:2211.14769v1 [cs.AI])","link":"http://arxiv.org/abs/2211.14769","description":"<p>Federated embodied agent learning protects the data privacy of individual\nvisual environments by keeping data locally at each client (the individual\nenvironment) during training. However, since the local data is inaccessible to\nthe server under federated learning, attackers may easily poison the training\ndata of the local client to build a backdoor in the agent without notice.\nDeploying such an agent raises the risk of potential harm to humans, as the\nattackers may easily navigate and control the agent as they wish via the\nbackdoor. Towards Byzantine-robust federated embodied agent learning, in this\npaper, we study the attack and defense for the task of vision-and-language\nnavigation (VLN), where the agent is required to follow natural language\ninstructions to navigate indoor environments. First, we introduce a simple but\neffective attack strategy, Navigation as Wish (NAW), in which the malicious\nclient manipulates local trajectory data to implant a backdoor into the global\nmodel. Results on two VLN datasets (R2R and RxR) show that NAW can easily\nnavigate the deployed VLN agent regardless of the language instruction, without\naffecting its performance on normal test sets. Then, we propose a new\nPrompt-Based Aggregation (PBA) to defend against the NAW attack in federated\nVLN, which provides the server with a ''prompt'' of the vision-and-language\nalignment variance between the benign and malicious clients so that they can be\ndistinguished during training. We validate the effectiveness of the PBA method\non protecting the global model from the NAW attack, which outperforms other\nstate-of-the-art defense methods by a large margin in the defense metrics on\nR2R and RxR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_Z/0/1/0/all/0/1\">Zonglin Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiwen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alignment-Enriched Tuning for Patch-Level Pre-trained Document Image Models. (arXiv:2211.14777v1 [cs.CV])","link":"http://arxiv.org/abs/2211.14777","description":"<p>Alignment between image and text has shown promising improvements on\npatch-level pre-trained document image models. However, investigating more\neffective or finer-grained alignment techniques during pre-training requires a\nlarge amount of computation cost and time. Thus, a question naturally arises:\nCould we fine-tune the pre-trained models adaptive to downstream tasks with\nalignment objectives and achieve comparable or better performance? In this\npaper, we propose a new model architecture with alignment-enriched tuning\n(dubbed AETNet) upon pre-trained document image models, to adapt downstream\ntasks with the joint task-specific supervised and alignment-aware contrastive\nobjective. Specifically, we introduce an extra visual transformer as the\nalignment-ware image encoder and an extra text transformer as the\nalignment-ware text encoder before multimodal fusion. We consider alignment in\nthe following three aspects: 1) document-level alignment by leveraging the\ncross-modal and intra-modal contrastive loss; 2) global-local alignment for\nmodeling localized and structural information in document images; and 3)\nlocal-level alignment for more accurate patch-level information. Experiments on\nvarious downstream tasks show that AETNet can achieve state-of-the-art\nperformance on various downstream tasks. Notably, AETNet consistently\noutperforms state-of-the-art pre-trained models, such as LayoutLMv3 with\nfine-tuning techniques, on three different downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiabang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Financial Event Extraction Using Wikipedia-Based Weak Supervision. (arXiv:1911.10783v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1911.10783","description":"<p>Extraction of financial and economic events from text has previously been\ndone mostly using rule-based methods, with more recent works employing machine\nlearning techniques. This work is in line with this latter approach, leveraging\nrelevant Wikipedia sections to extract weak labels for sentences describing\neconomic events. Whereas previous weakly supervised approaches required a\nknowledge-base of such events, or corresponding financial figures, our approach\nrequires no such additional data, and can be employed to extract economic\nevents related to companies which are not even mentioned in the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ein_Dor_L/0/1/0/all/0/1\">Liat Ein-Dor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gera_A/0/1/0/all/0/1\">Ariel Gera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toledo_Ronen_O/0/1/0/all/0/1\">Orith Toledo-Ronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halfon_A/0/1/0/all/0/1\">Alon Halfon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznajder_B/0/1/0/all/0/1\">Benjamin Sznajder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dankin_L/0/1/0/all/0/1\">Lena Dankin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilu_Y/0/1/0/all/0/1\">Yonatan Bilu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation. (arXiv:2004.08694v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.08694","description":"<p>Question Generation (QG) is fundamentally a simple syntactic transformation;\nhowever, many aspects of semantics influence what questions are good to form.\nWe implement this observation by developing SynQG, a set of transparent\nsyntactic rules leveraging universal dependencies, shallow semantic parsing,\nlexical resources, and custom rules which transform declarative sentences into\nquestion-answer pairs. We utilize PropBank argument descriptions and VerbNet\nstate predicates to incorporate shallow semantic content, which helps generate\nquestions of a descriptive nature and produce inferential and semantically\nricher questions than existing systems. In order to improve syntactic fluency\nand eliminate grammatically incorrect questions, we employ back-translation\nover the output of these syntactic rules. A set of crowd-sourced evaluations\nshows that our system can generate a larger number of highly grammatical and\nrelevant questions than previous QG systems and that back-translation\ndrastically improves grammaticality at a slight cost of generating irrelevant\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1\">Kaustubh D. Dhole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forecasting financial markets with semantic network analysis in the COVID-19 crisis. (arXiv:2009.04975v3 [q-fin.GN] UPDATED)","link":"http://arxiv.org/abs/2009.04975","description":"<p>This paper uses a new textual data index for predicting stock market data.\nThe index is applied to a large set of news to evaluate the importance of one\nor more general economic-related keywords appearing in the text. The index\nassesses the importance of the economic-related keywords, based on their\nfrequency of use and semantic network position. We apply it to the Italian\npress and construct indices to predict Italian stock and bond market returns\nand volatilities in a recent sample period, including the COVID-19 crisis. The\nevidence shows that the index captures the different phases of financial time\nseries well. Moreover, results indicate strong evidence of predictability for\nbond market data, both returns and volatilities, short and long maturities, and\nstock market volatility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Grassi_S/0/1/0/all/0/1\">S. Grassi</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Ravazzolo_F/0/1/0/all/0/1\">F. Ravazzolo</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Violante_F/0/1/0/all/0/1\">F. Violante</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierachical Delta-Attention Method for Multimodal Fusion. (arXiv:2011.10916v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.10916","description":"<p>In vision and linguistics; the main input modalities are facial expressions,\nspeech patterns, and the words uttered. The issue with analysis of any one mode\nof expression (Visual, Verbal or Vocal) is that lot of contextual information\ncan get lost. This asks researchers to inspect multiple modalities to get a\nthorough understanding of the cross-modal dependencies and temporal context of\nthe situation to analyze the expression. This work attempts at preserving the\nlong-range dependencies within and across different modalities, which would be\nbottle-necked by the use of recurrent networks and adds the concept of\ndelta-attention to focus on local differences per modality to capture the\nidiosyncrasy of different people. We explore a cross-attention fusion technique\nto get the global view of the emotion expressed through these\ndelta-self-attended modalities, in order to fuse all the local nuances and\nglobal context together. The addition of attention is new to the multi-modal\nfusion field and currently being scrutinized for on what stage the attention\nmechanism should be used, this work achieves competitive accuracy for overall\nand per-class classification which is close to the current state-of-the-art\nwith almost half number of parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panchal_K/0/1/0/all/0/1\">Kunjal Panchal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CL-XABSA: Contrastive Learning for Cross-lingual Aspect-based Sentiment Analysis. (arXiv:2204.00791v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00791","description":"<p>As an extensive research in the field of natural language processing (NLP),\naspect-based sentiment analysis (ABSA) is the task of predicting the sentiment\nexpressed in a text relative to the corresponding aspect. Unfortunately, most\nlanguages lack sufficient annotation resources, thus more and more recent\nresearchers focus on cross-lingual aspect-based sentiment analysis (XABSA).\nHowever, most recent researches only concentrate on cross-lingual data\nalignment instead of model alignment. To this end, we propose a novel\nframework, CL-XABSA: Contrastive Learning for Cross-lingual Aspect-Based\nSentiment Analysis. Based on contrastive learning, we close the distance\nbetween samples with the same label in different semantic spaces, thus\nachieving a convergence of semantic spaces of different languages.\nSpecifically, we design two contrastive strategies, token level contrastive\nlearning of token embeddings (TL-CTE) and sentiment level contrastive learning\nof token embeddings (SL-CTE), to regularize the semantic space of source and\ntarget language to be more uniform. Since our framework can receive datasets in\nmultiple languages during training, our framework can be adapted not only for\nXABSA task but also for multilingual aspect-based sentiment analysis (MABSA).\nTo further improve the performance of our model, we perform knowledge\ndistillation technology leveraging data from unlabeled target language. In the\ndistillation XABSA task, we further explore the comparative effectiveness of\ndifferent data (source dataset, translated dataset, and code-switched dataset).\nThe results demonstrate that the proposed method has a certain improvement in\nthe three tasks of XABSA, distillation XABSA and MABSA. For reproducibility,\nour code for this paper is available at https://github.com/GKLMIP/CL-XABSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaotian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aimin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Passage Retrieval with Zero-Shot Question Generation. (arXiv:2204.07496v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07496","description":"<p>We propose a simple and effective re-ranking method for improving passage\nretrieval in open question answering. The re-ranker re-scores retrieved\npassages with a zero-shot question generation model, which uses a pre-trained\nlanguage model to compute the probability of the input question conditioned on\na retrieved passage. This approach can be applied on top of any retrieval\nmethod (e.g. neural or keyword-based), does not require any domain- or\ntask-specific training (and therefore is expected to generalize better to data\ndistribution shifts), and provides rich cross-attention between query and\npassage (i.e. it must explain every token in the question). When evaluated on a\nnumber of open-domain retrieval datasets, our re-ranker improves strong\nunsupervised retrieval models by 6%-18% absolute and strong supervised models\nby up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new\nstate-of-the-art results on full open-domain question answering by simply\nadding the new re-ranker to existing models with no further changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachan_D/0/1/0/all/0/1\">Devendra Singh Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unbiased and Efficient Sampling of Dependency Trees. (arXiv:2205.12621v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12621","description":"<p>Most computational models of dependency syntax consist of distributions over\nspanning trees. However, the majority of dependency treebanks require that\nevery valid dependency tree has a single edge coming out of the ROOT node, a\nconstraint that is not part of the definition of spanning trees. For this\nreason all standard inference algorithms for spanning trees are suboptimal for\ninference over dependency trees.\n</p>\n<p>Zmigrod et al. (2021b) proposed algorithms for sampling with and without\nreplacement from the dependency tree distribution that incorporate the\nsingle-root constraint. In this paper we show that their fastest algorithm for\nsampling with replacement, Wilson-RC, is in fact producing biased samples and\nwe provide two alternatives that are unbiased. Additionally, we propose two\nalgorithms (one incremental, one parallel) that reduce the asymptotic runtime\nof algorithm for sampling k trees without replacement to O(kn3). These\nalgorithms are both asymptotically and practically more efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stanojevic_M/0/1/0/all/0/1\">Milo&#x161; Stanojevi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIRECTOR: Generator-Classifiers For Supervised Language Modeling. (arXiv:2206.07694v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.07694","description":"<p>Current language models achieve low perplexity but their resulting\ngenerations still suffer from toxic responses, repetitiveness and\ncontradictions. The standard language modeling setup fails to address these\nissues. In this paper, we introduce a new architecture, {\\sc Director}, that\nconsists of a unified generator-classifier with both a language modeling and a\nclassification head for each output token. Training is conducted jointly using\nboth standard language modeling data, and data labeled with desirable and\nundesirable sequences. Experiments in several settings show that the model has\ncompetitive training and decoding speed compared to standard language models\nwhile yielding superior results, alleviating known issues while maintaining\ngeneration quality. It also outperforms existing model guiding approaches in\nterms of both accuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_K/0/1/0/all/0/1\">Kushal Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1\">Sainbayar Sukhbaatar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality. (arXiv:2207.07036v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.07036","description":"<p>While audio-visual speech models can yield superior performance and\nrobustness compared to audio-only models, their development and adoption are\nhindered by the lack of labeled and unlabeled audio-visual data and the cost to\ndeploy one model per modality. In this paper, we present u-HuBERT, a\nself-supervised pre-training framework that can leverage both multimodal and\nunimodal speech with a unified masked cluster prediction objective. By\nutilizing modality dropout during pre-training, we demonstrate that a single\nfine-tuned model can achieve performance on par or better than the\nstate-of-the-art modality-specific models. Moreover, our model fine-tuned only\non audio can perform well with audio-visual and visual speech input, achieving\nzero-shot modality generalization for multiple speech processing tasks. In\nparticular, our single model yields 1.2%/1.4%/27.2% speech recognition word\nerror rate on LRS3 with audio-visual/audio/visual input. Codes and models are\navailable at https://github.com/facebookresearch/av_hubert\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GSRFormer: Grounded Situation Recognition Transformer with Alternate Semantic Attention Refinement. (arXiv:2208.08965v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2208.08965","description":"<p>Grounded Situation Recognition (GSR) aims to generate structured semantic\nsummaries of images for \"human-like\" event understanding. Specifically, GSR\ntask not only detects the salient activity verb (e.g. buying), but also\npredicts all corresponding semantic roles (e.g. agent and goods). Inspired by\nobject detection and image captioning tasks, existing methods typically employ\na two-stage framework: 1) detect the activity verb, and then 2) predict\nsemantic roles based on the detected verb. Obviously, this illogical framework\nconstitutes a huge obstacle to semantic understanding. First, pre-detecting\nverbs solely without semantic roles inevitably fails to distinguish many\nsimilar daily activities (e.g., offering and giving, buying and selling).\nSecond, predicting semantic roles in a closed auto-regressive manner can hardly\nexploit the semantic relations among the verb and roles. To this end, in this\npaper we propose a novel two-stage framework that focuses on utilizing such\nbidirectional relations within verbs and roles. In the first stage, instead of\npre-detecting the verb, we postpone the detection step and assume a pseudo\nlabel, where an intermediate representation for each corresponding semantic\nrole is learned from images. In the second stage, we exploit transformer layers\nto unearth the potential semantic relations within both verbs and semantic\nroles. With the help of a set of support images, an alternate learning scheme\nis designed to simultaneously optimize the results: update the verb using nouns\ncorresponding to the image, and update nouns using verbs from support images.\nExtensive experimental results on challenging SWiG benchmarks show that our\nrenovated framework outperforms other state-of-the-art methods under various\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhi-Qi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Alexander G. Hauptmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unit Testing for Concepts in Neural Networks. (arXiv:2208.10244v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.10244","description":"<p>Many complex problems are naturally understood in terms of symbolic concepts.\nFor example, our concept of \"cat\" is related to our concepts of \"ears\" and\n\"whiskers\" in a non-arbitrary way. Fodor (1998) proposes one theory of\nconcepts, which emphasizes symbolic representations related via constituency\nstructures. Whether neural networks are consistent with such a theory is open\nfor debate. We propose unit tests for evaluating whether a system's behavior is\nconsistent with several key aspects of Fodor's criteria. Using a simple visual\nconcept learning task, we evaluate several modern neural architectures against\nthis specification. We find that models succeed on tests of groundedness,\nmodularlity, and reusability of concepts, but that important questions about\ncausality remain open. Resolving these will require new methods for analyzing\nmodels' internal states.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovering_C/0/1/0/all/0/1\">Charles Lovering</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models. (arXiv:2208.11445v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.11445","description":"<p>The ability to extrapolate, i.e., to make predictions on sequences that are\nlonger than those presented as training examples, is a challenging problem for\ncurrent deep learning models. Recent work shows that this limitation persists\nin state-of-the-art Transformer-based models. Most solutions to this problem\nuse specific architectures or training methods that do not generalize to other\ntasks. We demonstrate that large language models can succeed in extrapolation\nwithout modifying their architecture or training procedure. Our experimental\nresults show that generating step-by-step rationales and introducing marker\ntokens are both required for effective extrapolation. First, we induce a\nlanguage model to produce step-by-step rationales before outputting the answer\nto effectively communicate the task to the model. However, as sequences become\nlonger, we find that current models struggle to keep track of token positions.\nTo address this issue, we interleave output tokens with markup tokens that act\nas explicit positional and counting symbols. Our findings show how these two\ncomplementary approaches enable remarkable sequence extrapolation and highlight\na limitation of current architectures to effectively generalize without\nexplicit surface form guidance. Code available at\nhttps://github.com/MirelleB/induced-rationales-markup-tokens\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bueno_M/0/1/0/all/0/1\">Mirelle Bueno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemmell_C/0/1/0/all/0/1\">Carlos Gemmell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalton_J/0/1/0/all/0/1\">Jeffrey Dalton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extend and Explain: Interpreting Very Long Language Models. (arXiv:2209.01174v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.01174","description":"<p>While Transformer language models (LMs) are state-of-the-art for information\nextraction, long text introduces computational challenges requiring suboptimal\npreprocessing steps or alternative model architectures. Sparse attention LMs\ncan represent longer sequences, overcoming performance hurdles. However, it\nremains unclear how to explain predictions from these models, as not all tokens\nattend to each other in the self-attention layers, and long sequences pose\ncomputational challenges for explainability algorithms when runtime depends on\ndocument length. These challenges are severe in the medical context where\ndocuments can be very long, and machine learning (ML) models must be auditable\nand trustworthy. We introduce a novel Masked Sampling Procedure (MSP) to\nidentify the text blocks that contribute to a prediction, apply MSP in the\ncontext of predicting diagnoses from medical text, and validate our approach\nwith a blind review by two clinicians. Our method identifies about 1.7x more\nclinically informative text blocks than the previous state-of-the-art, runs up\nto 100x faster, and is tractable for generating important phrase pairs. MSP is\nparticularly well-suited to long LMs but can be applied to any text classifier.\nWe provide a general implementation of MSP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stremmel_J/0/1/0/all/0/1\">Joel Stremmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_B/0/1/0/all/0/1\">Brian L. Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertzberg_J/0/1/0/all/0/1\">Jeffrey Hertzberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_J/0/1/0/all/0/1\">Jaime Murillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allotey_L/0/1/0/all/0/1\">Llewelyn Allotey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halperin_E/0/1/0/all/0/1\">Eran Halperin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvNeXt Based Neural Network for Audio Anti-Spoofing. (arXiv:2209.06434v4 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2209.06434","description":"<p>Automatic speaker verification (ASV) has been widely used in the real life\nfor identity authentication. However, with the rapid development of speech\nconversion and speech synthesis algorithms, ASV systems are vulnerable to\nspoofing attacks. In recent years, there have many works about synthetic speech\ndetection, researchers had proposed a number of anti-spoofing methods based on\nhand-crafted features to improve the detection accuracy and robustness of ASV\nsystems. However, using hand-crafted features rather than raw waveform would\nlose certain information for anti-spoofing, which will reduce the detection\nperformance of the system. Inspired by the promising performance of ConvNeXt in\nimage classification tasks, we revise the ConvNeXt network architecture\naccordingly for spoof attacks detection task and propose a lightweight\nend-to-end anti-spoofing model. By integrating the revised architecture with\nthe channel attention block and using the focal loss function, the proposed\nmodel can focus on the most informative sub-bands of speech representations and\nthe difficult samples that are hard for models to classify. Experiments show\nthat our proposed best single system could achieve an equal error rate of 0.75%\nand min-tDCF of 0.0212 for the ASVSpoof 2019 LA evaluation dataset, which\noutperforms the state-of-the-art systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qiaowei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jinghui Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yitao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Ying Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_W/0/1/0/all/0/1\">Wing W.Y. Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Attention Matrix for Every Decision: Faithfulness-based Arbitration Among Multiple Attention-Based Interpretations of Transformers in Text Classification. (arXiv:2209.10876v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.10876","description":"<p>Transformers are widely used in natural language processing, where they\nconsistently achieve state-of-the-art performance. This is mainly due to their\nattention-based architecture, which allows them to model rich linguistic\nrelations between (sub)words. However, transformers are difficult to interpret.\nBeing able to provide reasoning for its decisions is an important property for\na model in domains where human lives are affected. With transformers finding\nwide use in such fields, the need for interpretability techniques tailored to\nthem arises. We propose a new technique that selects the most faithful\nattention-based interpretation among the several ones that can be obtained by\ncombining different head, layer and matrix operations. In addition, two\nvariations are introduced towards (i) reducing the computational complexity,\nthus being faster and friendlier to the environment, and (ii) enhancing the\nperformance in multi-label data. We further propose a new faithfulness metric\nthat is more suitable for transformer models and exhibits high correlation with\nthe area under the precision-recall curve based on ground truth rationales. We\nvalidate the utility of our contributions with a series of quantitative and\nqualitative experiments on seven datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mylonas_N/0/1/0/all/0/1\">Nikolaos Mylonas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mollas_I/0/1/0/all/0/1\">Ioannis Mollas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory in humans and deep language models: Linking hypotheses for model augmentation. (arXiv:2210.01869v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01869","description":"<p>The computational complexity of the self-attention mechanism in Transformer\nmodels significantly limits their ability to generalize over long temporal\ndurations. Memory-augmentation, or the explicit storing of past information in\nexternal memory for subsequent predictions, has become a constructive avenue\nfor mitigating this limitation. We argue that memory-augmented Transformers can\nbenefit substantially from considering insights from the memory literature in\nhumans. We detail an approach for integrating evidence from the human memory\nsystem through the specification of cross-domain linking hypotheses. We then\nprovide an empirical demonstration to evaluate the use of surprisal as a\nlinking hypothesis, and further identify the limitations of this approach to\ninform future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raccah_O/0/1/0/all/0/1\">Omri Raccah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Phoebe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willke_T/0/1/0/all/0/1\">Ted L. Willke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poeppel_D/0/1/0/all/0/1\">David Poeppel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1\">Vy A. Vo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReAct: Synergizing Reasoning and Acting in Language Models. (arXiv:2210.03629v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03629","description":"<p>While large language models (LLMs) have demonstrated impressive capabilities\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\naction plan generation) have primarily been studied as separate topics. In this\npaper, we explore the use of LLMs to generate both reasoning traces and\ntask-specific actions in an interleaved manner, allowing for greater synergy\nbetween the two: reasoning traces help the model induce, track, and update\naction plans as well as handle exceptions, while actions allow it to interface\nwith external sources, such as knowledge bases or environments, to gather\nadditional information. We apply our approach, named ReAct, to a diverse set of\nlanguage and decision making tasks and demonstrate its effectiveness over\nstate-of-the-art baselines, as well as improved human interpretability and\ntrustworthiness over methods without reasoning or acting components.\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\nReAct overcomes issues of hallucination and error propagation prevalent in\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\ngenerates human-like task-solving trajectories that are more interpretable than\nbaselines without reasoning traces. On two interactive decision making\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\nreinforcement learning methods by an absolute success rate of 34% and 10%\nrespectively, while being prompted with only one or two in-context examples.\nProject site with code: https://react-lm.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jeffrey Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafran_I/0/1/0/all/0/1\">Izhak Shafran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation. (arXiv:2210.15461v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15461","description":"<p>Multimodal Machine Translation (MMT) focuses on enhancing text-only\ntranslation with visual features, which has attracted considerable attention\nfrom both natural language processing and computer vision communities. Recent\nadvances still struggle to train a separate model for each language pair, which\nis costly and unaffordable when the number of languages increases in the real\nworld. In other words, the multilingual multimodal machine translation\n(Multilingual MMT) task has not been investigated, which aims to handle the\naforementioned issues by providing a shared semantic space for multiple\nlanguages. Besides, the image modality has no language boundaries, which is\nsuperior to bridging the semantic gap between languages. To this end, we first\npropose the Multilingual MMT task by establishing two new Multilingual MMT\nbenchmark datasets covering seven languages. Then, an effective baseline LVP-M3\nusing visual prompts is proposed to support translations between different\nlanguages, which includes three stages (token encoding, language-aware visual\nprompt generation, and language translation). Extensive experimental results on\nour constructed benchmark datasets demonstrate the effectiveness of LVP-M3\nmethod for Multilingual MMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zheng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intriguing Properties of Compression on Multilingual Models. (arXiv:2211.02738v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.02738","description":"<p>Multilingual models are often particularly dependent on scaling to generalize\nto a growing number of languages. Compression techniques are widely relied upon\nto reconcile the growth in model size with real world resource constraints, but\ncompression can have a disparate effect on model performance for low-resource\nlanguages. It is thus crucial to understand the trade-offs between scale,\nmultilingualism, and compression. In this work, we propose an experimental\nframework to characterize the impact of sparsifying multilingual pre-trained\nlanguage models during fine-tuning. Applying this framework to mBERT named\nentity recognition models across 40 languages, we find that compression confers\nseveral intriguing and previously unknown generalization properties. In\ncontrast to prior findings, we find that compression may improve model\nrobustness over dense models. We additionally observe that under certain\nsparsification regimes compression may aid, rather than disproportionately\nimpact the performance of low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ogueji_K/0/1/0/all/0/1\">Kelechi Ogueji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahia_O/0/1/0/all/0/1\">Orevaoghene Ahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onilude_G/0/1/0/all/0/1\">Gbemileke Onilude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech separation with large-scale self-supervised learning. (arXiv:2211.05172v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2211.05172","description":"<p>Self-supervised learning (SSL) methods such as WavLM have shown promising\nspeech separation (SS) results in small-scale simulation-based experiments. In\nthis work, we extend the exploration of the SSL-based SS by massively scaling\nup both the pre-training data (more than 300K hours) and fine-tuning data (10K\nhours). We also investigate various techniques to efficiently integrate the\npre-trained model with the SS network under a limited computation budget,\nincluding a low frame rate SSL model training setup and a fine-tuning scheme\nusing only the part of the pre-trained model. Compared with a supervised\nbaseline and the WavLM-based SS model using feature embeddings obtained with\nthe previously released 94K hours trained WavLM, our proposed model obtains\n15.9% and 11.2% of relative word error rate (WER) reductions, respectively, for\na simulated far-field speech mixture test set. For conversation transcription\non real meeting recordings using continuous speech separation, the proposed\nmodel achieves 6.8% and 10.6% of relative WER reductions over the purely\nsupervised baseline on AMI and ICSI evaluation sets, respectively, while\nreducing the computational cost by 38%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sivasankaran_S/0/1/0/all/0/1\">Sunit Sivasankaran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eskimez_S/0/1/0/all/0/1\">Sefik Emre Eskimez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT on a Data Diet: Finding Important Examples by Gradient-Based Pruning. (arXiv:2211.05610v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05610","description":"<p>Current pre-trained language models rely on large datasets for achieving\nstate-of-the-art performance. However, past research has shown that not all\nexamples in a dataset are equally important during training. In fact, it is\nsometimes possible to prune a considerable fraction of the training set while\nmaintaining the test performance. Established on standard vision benchmarks,\ntwo gradient-based scoring metrics for finding important examples are GraNd and\nits estimated version, EL2N. In this work, we employ these two metrics for the\nfirst time in NLP. We demonstrate that these metrics need to be computed after\nat least one epoch of fine-tuning and they are not reliable in early steps.\nFurthermore, we show that by pruning a small portion of the examples with the\nhighest GraNd/EL2N scores, we can not only preserve the test accuracy, but also\nsurpass it. This paper details adjustments and implementation choices which\nenable GraNd and EL2N to be applied to NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1\">Mohsen Fayyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghazadeh_E/0/1/0/all/0/1\">Ehsan Aghazadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modarressi_A/0/1/0/all/0/1\">Ali Modarressi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1\">Yadollah Yaghoobzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1\">Samira Ebrahimi Kahou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. (arXiv:2211.10438v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.10438","description":"<p>Large language models (LLMs) show excellent performance but are compute- and\nmemory-intensive. Quantization can reduce memory and accelerate inference.\nHowever, for LLMs beyond 100 billion parameters, existing methods cannot\nmaintain accuracy or do not run efficiently on hardware. We propose\nSmoothQuant, a training-free, accuracy-preserving, and general-purpose\npost-training quantization (PTQ) solution to enable 8-bit weight, 8-bit\nactivation (W8A8) quantization for LLMs that can be implemented efficiently. We\nobserve that systematic outliers appear at fixed activation channels. Based on\nthe fact that weights are easy to quantize while activations are not,\nSmoothQuant smooths the activation outliers by offline migrating the\nquantization difficulty from activations to weights with a mathematically\nequivalent transformation. SmoothQuant enables an INT8 quantization of both\nweights and activations for all the GEMMs in LLMs, including OPT-175B,\nBLOOM-176B, and GLM-130B. SmoothQuant has better hardware efficiency than\nexisting techniques using mixed-precision activation quantization or\nweight-only quantization. We demonstrate up to 1.56x speedup and 2x memory\nreduction for LLMs with negligible loss in accuracy. Thanks to the\nhardware-friendly design, we integrate SmoothQuant into FasterTransformer, a\nstate-of-the-art LLM serving framework, and achieve faster inference speed with\nhalf the number of GPUs compared to FP16. Our work offers a turn-key solution\nthat reduces hardware costs and democratizes LLMs. Code is available at:\nhttps://github.com/mit-han-lab/smoothquant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Guangxuan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Ji Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seznec_M/0/1/0/all/0/1\">Mickael Seznec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demouth_J/0/1/0/all/0/1\">Julien Demouth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L3Cube-HindBERT and DevBERT: Pre-Trained BERT Transformer models for Devanagari based Hindi and Marathi Languages. (arXiv:2211.11418v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11418","description":"<p>The monolingual Hindi BERT models currently available on the model hub do not\nperform better than the multi-lingual models on downstream tasks. We present\nL3Cube-HindBERT, a Hindi BERT model pre-trained on Hindi monolingual corpus.\n</p>\n<p>Further, since Indic languages, Hindi and Marathi share the Devanagari\nscript, we train a single model for both languages. We release DevBERT, a\nDevanagari BERT model trained on both Marathi and Hindi monolingual datasets.\nWe evaluate these models on downstream Hindi and Marathi text classification\nand named entity recognition tasks. The HindBERT and DevBERT-based models show\nsuperior performance compared to their multi-lingual counterparts. These models\nare shared at https://huggingface.co/l3cube-pune .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Acceptability Judgements via Examining the Topology of Attention Maps. (arXiv:2205.09630v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2205.09630","description":"<p>The role of the attention mechanism in encoding linguistic knowledge has\nreceived special interest in NLP. However, the ability of the attention heads\nto judge the grammatical acceptability of a sentence has been underexplored.\nThis paper approaches the paradigm of acceptability judgments with topological\ndata analysis (TDA), showing that the geometric properties of the attention\ngraph can be efficiently exploited for two standard practices in linguistics:\nbinary judgments and linguistic minimal pairs. Topological features enhance the\nBERT-based acceptability classifier scores by $8$%-$24$% on CoLA in three\nlanguages (English, Italian, and Swedish). By revealing the topological\ndiscrepancy between attention maps of minimal pairs, we achieve the human-level\nperformance on the BLiMP benchmark, outperforming nine statistical and\nTransformer LM baselines. At the same time, TDA provides the foundation for\nanalyzing the linguistic functions of attention heads and interpreting the\ncorrespondence between the graph features and grammatical phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cherniavskii_D/0/1/0/all/0/1\">Daniil Cherniavskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulchinskii_E/0/1/0/all/0/1\">Eduard Tulchinskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proskurina_I/0/1/0/all/0/1\">Irina Proskurina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushnareva_L/0/1/0/all/0/1\">Laida Kushnareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovski_D/0/1/0/all/0/1\">Dmitri Piontkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hey ASR System! Why Aren't You More Inclusive? Automatic Speech Recognition Systems' Bias and Proposed Bias Mitigation Techniques. A Literature Review. (arXiv:2211.09511v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2211.09511","description":"<p>Speech is the fundamental means of communication between humans. The advent\nof AI and sophisticated speech technologies have led to the rapid proliferation\nof human-to-computer-based interactions, fueled primarily by Automatic Speech\nRecognition (ASR) systems. ASR systems normally take human speech in the form\nof audio and convert it into words, but for some users, it cannot decode the\nspeech, and any output text is filled with errors that are incomprehensible to\nthe human reader. These systems do not work equally for everyone and actually\nhinder the productivity of some users. In this paper, we present research that\naddresses ASR biases against gender, race, and the sick and disabled, while\nexploring studies that propose ASR debiasing techniques for mitigating these\ndiscriminations. We also discuss techniques for designing a more accessible and\ninclusive ASR technology. For each approach surveyed, we also provide a summary\nof the investigation and methods applied, the ASR systems and corpora used, and\nthe research findings, and highlight their strengths and/or weaknesses.\nFinally, we propose future opportunities for Natural Language Processing\nresearchers to explore in the next level creation of ASR technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngueajio_M/0/1/0/all/0/1\">Mikel K. Ngueajio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_G/0/1/0/all/0/1\">Gloria Washington</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}