{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-10-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Visualize Before You Write: Imagination-Guided Open-Ended Text Generation. (arXiv:2210.03765v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03765","description":"<p>Recent advances in text-to-image synthesis make it possible to visualize\nmachine imaginations for a given context. On the other hand, when generating\ntext, human writers are gifted at creative visualization, which enhances their\nwritings by forming imaginations as blueprints before putting down the stories\nin words. Inspired by such a cognitive process, we ask the natural question of\nwhether we can endow machines with the same ability to utilize visual\ninformation and construct a general picture of the context to guide text\ngeneration. In this work, we propose iNLG that uses machine-generated images to\nguide language models (LM) in open-ended text generation. The experiments and\nanalyses demonstrate the effectiveness of iNLG on open-ended text generation\ntasks, including text completion, story generation, and concept-to-text\ngeneration in few-shot scenarios. Both automatic metrics and human evaluations\nverify that the text snippets generated by our iNLG are coherent and\ninformative while displaying minor degeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1\">An Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedPC: Federated Learning for Language Generation with Personal and Context Preference Embeddings. (arXiv:2210.03766v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03766","description":"<p>Federated learning is a training paradigm that learns from multiple\ndistributed users without aggregating data on a centralized server. Such a\nparadigm promises the ability to deploy machine-learning at-scale to a diverse\npopulation of end-users without first collecting a large, labeled dataset for\nall possible tasks. As federated learning typically averages learning updates\nacross a decentralized population, there is a growing need for personalization\nof federated learning systems (i.e conversational agents must be able to\npersonalize to a specific user's preferences). In this work, we propose a new\ndirection for personalization research within federated learning, leveraging\nboth personal embeddings and shared context embeddings. We also present an\napproach to predict these ``preference'' embeddings, enabling personalization\nwithout backpropagation. Compared to state-of-the-art personalization\nbaselines, our approach achieves a 50\\% improvement in test-time perplexity\nusing 0.001\\% of the memory required by baseline approaches, and achieving\ngreater sample- and compute-efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Andrew Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tambwekar_P/0/1/0/all/0/1\">Pradyumna Tambwekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gombolay_M/0/1/0/all/0/1\">Matthew Gombolay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"xDBTagger: Explainable Natural Language Interface to Databases Using Keyword Mappings and Schema Graph. (arXiv:2210.03768v1 [cs.DB])","link":"http://arxiv.org/abs/2210.03768","description":"<p>Translating natural language queries (NLQ) into structured query language\n(SQL) in interfaces to relational databases is a challenging task that has been\nwidely studied by researchers from both the database and natural language\nprocessing communities. Numerous works have been proposed to attack the natural\nlanguage interfaces to databases (NLIDB) problem either as a conventional\npipeline-based or an end-to-end deep-learning-based solution. Nevertheless,\nregardless of the approach preferred, such solutions exhibit black-box nature,\nwhich makes it difficult for potential users targeted by these systems to\ncomprehend the decisions made to produce the translated SQL. To this end, we\npropose xDBTagger, an explainable hybrid translation pipeline that explains the\ndecisions made along the way to the user both textually and visually. We also\nevaluate xDBTagger quantitatively in three real-world relational databases. The\nevaluation results indicate that in addition to being fully interpretable,\nxDBTagger is effective in terms of accuracy and translates the queries more\nefficiently compared to other state-of-the-art pipeline-based systems up to\n10000 times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Usta_A/0/1/0/all/0/1\">Arif Usta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karakayali_A/0/1/0/all/0/1\">Akifhan Karakayali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulusoy_O/0/1/0/all/0/1\">&#xd6;zg&#xfc;r Ulusoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts. (arXiv:2210.03797v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03797","description":"<p>Recent progress in language model pre-training has led to important\nimprovements in Named Entity Recognition (NER). Nonetheless, this progress has\nbeen mainly tested in well-formatted documents such as news, Wikipedia, or\nscientific articles. In social media the landscape is different, in which it\nadds another layer of complexity due to its noisy and dynamic nature. In this\npaper, we focus on NER in Twitter, one of the largest social media platforms,\nand construct a new NER dataset, TweetNER7, which contains seven entity types\nannotated over 11,382 tweets from September 2019 to August 2021. The dataset\nwas constructed by carefully distributing the tweets over time and taking\nrepresentative trends as a basis. Along with the dataset, we provide a set of\nlanguage model baselines and perform an analysis on the language model\nperformance on the task, especially analyzing the impact of different time\nperiods. In particular, we focus on three important temporal aspects in our\nanalysis: short-term degradation of NER models over time, strategies to\nfine-tune a language model over different periods, and self-labeling as an\nalternative to lack of recently-labeled data. TweetNER7 is released publicly\n(https://huggingface.co/datasets/tner/tweetner7) along with the models\nfine-tuned on it (NER models have been integrated into TweetNLP and can be\nfound athttps://github.com/asahi417/tner/tree/master/examples/tweetner7_paper).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ushio_A/0/1/0/all/0/1\">Asahi Ushio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_L/0/1/0/all/0/1\">Leonardo Neves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1\">Vitor Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbieri_F/0/1/0/all/0/1\">Francesco Barbieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval Augmented Visual Question Answering with Outside Knowledge. (arXiv:2210.03809v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03809","description":"<p>Outside-Knowledge Visual Question Answering (OK-VQA) is a challenging VQA\ntask that requires retrieval of external knowledge to answer questions about\nimages. Recent OK-VQA systems use Dense Passage Retrieval (DPR) to retrieve\ndocuments from external knowledge bases, such as Wikipedia, but with DPR\ntrained separately from answer generation, introducing a potential limit on the\noverall system performance. Instead, we propose a joint training scheme which\nincludes differentiable DPR integrated with answer generation so that the\nsystem can be trained in an end-to-end fashion. Our experiments show that our\nscheme outperforms recent OK-VQA systems with strong DPR for retrieval. We also\nintroduce new diagnostic metrics to analyze how retrieval and generation\ninteract. The strong retrieval ability of our model significantly reduces the\nnumber of retrieved documents needed in training, yielding significant benefits\nin answer quality and computation required for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weizhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1\">Bill Byrne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis of the Effects of Decoding Algorithms on Fairness in Open-Ended Language Generation. (arXiv:2210.03826v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03826","description":"<p>Several prior works have shown that language models (LMs) can generate text\ncontaining harmful social biases and stereotypes. While decoding algorithms\nplay a central role in determining properties of LM generated text, their\nimpact on the fairness of the generations has not been studied. We present a\nsystematic analysis of the impact of decoding algorithms on LM fairness, and\nanalyze the trade-off between fairness, diversity and quality. Our experiments\nwith top-$p$, top-$k$ and temperature decoding algorithms, in open-ended\nlanguage generation, show that fairness across demographic groups changes\nsignificantly with change in decoding algorithm's hyper-parameters. Notably,\ndecoding algorithms that output more diverse text also output more texts with\nnegative sentiment and regard. We present several findings and provide\nrecommendations on standardized reporting of decoding details in fairness\nevaluations and optimization of decoding algorithms for fairness alongside\nquality and diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1\">Jwala Dhamala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking BERT: Evaluating and Optimizing Sparsified Attention. (arXiv:2210.03841v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03841","description":"<p>Transformers allow attention between all pairs of tokens, but there is reason\nto believe that most of these connections - and their quadratic time and memory\n- may not be necessary. But which ones? We evaluate the impact of\nsparsification patterns with a series of ablation experiments. First, we\ncompare masks based on syntax, lexical similarity, and token position to random\nconnections, and measure which patterns reduce performance the least. We find\nthat on three common finetuning tasks even using attention that is at least 78%\nsparse can have little effect on performance if applied at later transformer\nlayers, but that applying sparsity throughout the network reduces performance\nsignificantly. Second, we vary the degree of sparsity for three patterns\nsupported by previous work, and find that connections to neighbouring tokens\nare the most significant. Finally, we treat sparsity as an optimizable\nparameter, and present an algorithm to learn degrees of neighboring connections\nthat gives a fine-grained control over the accuracy-sparsity trade-off while\napproaching the performance of existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1\">Siddhartha Brahma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zablotskaia_P/0/1/0/all/0/1\">Polina Zablotskaia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mimno_D/0/1/0/all/0/1\">David Mimno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering. (arXiv:2210.03849v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03849","description":"<p>With the recent advance in large pre-trained language models, researchers\nhave achieved record performances in NLP tasks that mostly focus on language\npattern matching. The community is experiencing the shift of the challenge from\nhow to model language to the imitation of complex reasoning abilities like\nhuman beings. In this work, we investigate the application domain of finance\nthat involves real-world, complex numerical reasoning. We propose a new\nlarge-scale dataset, ConvFinQA, aiming to study the chain of numerical\nreasoning in conversational question answering. Our dataset poses great\nchallenge in modeling long-range, complex numerical reasoning paths in\nreal-world conversations. We conduct comprehensive experiments and analyses\nwith both the neural symbolic methods and the prompting-based methods, to\nprovide insights into the reasoning mechanisms of these two divisions. We\nbelieve our new dataset should serve as a valuable resource to push forward the\nexploration of real-world, complex reasoning tasks as the next research focus.\nOur dataset and code is publicly available at\nhttps://github.com/czyssrs/ConvFinQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smiley_C/0/1/0/all/0/1\">Charese Smiley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhiqiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sameena Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models. (arXiv:2210.03858v1 [cs.LG])","link":"http://arxiv.org/abs/2210.03858","description":"<p>There are growing interests in adapting large-scale language models using\nparameter-efficient fine-tuning methods. However, accelerating the model itself\nand achieving better inference efficiency through model compression has not\nbeen thoroughly explored yet. Model compression could provide the benefits of\nreducing memory footprints, enabling low-precision computations, and ultimately\nachieving cost-effective inference. To combine parameter-efficient adaptation\nand model compression, we propose AlphaTuning consisting of post-training\nquantization of the pre-trained language model and fine-tuning only some parts\nof quantized parameters for a target task. Specifically, AlphaTuning works by\nemploying binary-coding quantization, which factorizes the full-precision\nparameters into binary parameters and a separate set of scaling factors. During\nthe adaptation phase, the binary values are frozen for all tasks, while the\nscaling factors are fine-tuned for the downstream task. We demonstrate that\nAlphaTuning, when applied to GPT-2 and OPT, performs competitively with full\nfine-tuning on a variety of downstream tasks while achieving &gt;10x compression\nratio under 4-bit quantization and &gt;1,000x reduction in the number of trainable\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1\">Se Jung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeonghoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1\">Jeongin Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin-Hwa Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1\">Baeseong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byeongwook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_N/0/1/0/all/0/1\">Nako Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongsoo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Efficiency with a Single GPU: An Exploration of Transfer Methods for Small Language Models. (arXiv:2210.03871v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03871","description":"<p>Multi-task learning (MTL), instruction tuning, and prompting have recently\nbeen shown to improve the generalizability of large language models to new\ntasks. However, the benefits of such methods are less well-documented in\nsmaller language models, with some studies finding contradictory results. In\nthis work, we explore and isolate the effects of (i) model size, (ii) general\npurpose MTL, (iii) in-domain MTL, (iv) instruction tuning, and (v) few-shot\nfine-tuning for models with fewer than 500 million parameters. Our experiments\nin the zero-shot setting demonstrate that models gain 31% relative improvement,\non average, from general purpose MTL, with an additional 37.6% relative gain\nfrom in-domain MTL. Contradictory to prior works on large models, we find that\ninstruction tuning provides a modest 2% performance improvement for small\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Adithya Sagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_M/0/1/0/all/0/1\">Mike Ross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Lose Yourself! Empathetic Response Generation via Explicit Self-Other Awareness. (arXiv:2210.03884v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03884","description":"<p>As a critical step to achieve human-like chatbots, empathetic response\ngeneration has attained increasing interests. Previous attempts are incomplete\nand not sufficient enough to elicit empathy because they only focus on the\ninitial aspect of empathy to automatically mimic the feelings and thoughts of\nthe user via other-awareness. However, they ignore to maintain and take the own\nviews of the system into account, which is a crucial process to achieve the\nempathy called self-other awareness. To this end, we propose to generate\nEmpathetic response with explicit Self-Other Awareness (EmpSOA). Specifically,\nthree stages, self-other differentiation, self-other modulation and self-other\ngeneration, are devised to clearly maintain, regulate and inject the self-other\naware information into the process of empathetic response generation. Both\nautomatic and human evaluations on the benchmark dataset demonstrate the\nsuperiority of EmpSOA to generate more empathetic responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weixiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving End-to-End Text Image Translation From the Auxiliary Text Translation Task. (arXiv:2210.03887v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03887","description":"<p>End-to-end text image translation (TIT), which aims at translating the source\nlanguage embedded in images to the target language, has attracted intensive\nattention in recent research. However, data sparsity limits the performance of\nend-to-end text image translation. Multi-task learning is a non-trivial way to\nalleviate this problem via exploring knowledge from complementary related\ntasks. In this paper, we propose a novel text translation enhanced text image\ntranslation, which trains the end-to-end model with text translation as an\nauxiliary task. By sharing model parameters and multi-task training, our model\nis able to take full advantage of easily-available large-scale text parallel\ncorpus. Extensive experimental results show our proposed method outperforms\nexisting end-to-end methods, and the joint multi-task learning with both text\ntranslation and recognition tasks achieves better results, proving translation\nand recognition auxiliary tasks are complementary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Cong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_M/0/1/0/all/0/1\">Mei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Linghui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Short Text Pre-training with Extended Token Classification for E-commerce Query Understanding. (arXiv:2210.03915v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03915","description":"<p>E-commerce query understanding is the process of inferring the shopping\nintent of customers by extracting semantic meaning from their search queries.\nThe recent progress of pre-trained masked language models (MLM) in natural\nlanguage processing is extremely attractive for developing effective query\nunderstanding models. Specifically, MLM learns contextual text embedding via\nrecovering the masked tokens in the sentences. Such a pre-training process\nrelies on the sufficient contextual information. It is, however, less effective\nfor search queries, which are usually short text. When applying masking to\nshort search queries, most contextual information is lost and the intent of the\nsearch queries may be changed. To mitigate the above issues for MLM\npre-training on search queries, we propose a novel pre-training task\nspecifically designed for short text, called Extended Token Classification\n(ETC). Instead of masking the input text, our approach extends the input by\ninserting tokens via a generator network, and trains a discriminator to\nidentify which tokens are inserted in the extended input. We conduct\nexperiments in an E-commerce store to demonstrate the effectiveness of ETC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xianfeng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qingyu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goutam_R/0/1/0/all/0/1\">Rahul Goutam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Label Errors in Token Classification Data. (arXiv:2210.03920v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03920","description":"<p>Mislabeled examples are a common issue in real-world data, particularly for\ntasks like token classification where many labels must be chosen on a\nfine-grained basis. Here we consider the task of finding sentences that contain\nlabel errors in token classification datasets. We study 11 different\nstraightforward methods that score tokens/sentences based on the predicted\nclass probabilities output by a (any) token classification model (trained via\nany procedure). In precision-recall evaluations based on real-world label\nerrors in entity recognition data from CoNLL-2003, we identify a simple and\neffective method that consistently detects those sentences containing label\nerrors when applied with different token classification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei-Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Jonas Mueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Teachers Can Be Dense with Knowledge. (arXiv:2210.03923v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03923","description":"<p>Recent advances in distilling pretrained language models have discovered\nthat, besides the expressiveness of knowledge, the student-friendliness should\nbe taken into consideration to realize a truly knowledgable teacher. Based on a\npilot study, we find that over-parameterized teachers can produce expressive\nyet student-unfriendly knowledge, and are thus limited in overall\nknowledgableness. To remove the parameters that result in\nstudent-unfriendliness, we propose a sparse teacher trick under the guidance of\nan overall knowledgable score for each teacher parameter. The knowledgable\nscore is essentially an interpolation of the expressiveness and\nstudent-friendliness scores. The aim is to ensure that the expressive\nparameters are retained while the student-unfriendly ones are removed.\nExtensive experiments on the GLUE benchmark show that the proposed sparse\nteachers can be dense with knowledge and lead to students with compelling\nperformance in comparison with a series of competitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EgoTaskQA: Understanding Human Tasks in Egocentric Videos. (arXiv:2210.03929v1 [cs.CV])","link":"http://arxiv.org/abs/2210.03929","description":"<p>Understanding human tasks through video observations is an essential\ncapability of intelligent agents. The challenges of such capability lie in the\ndifficulty of generating a detailed understanding of situated actions, their\neffects on object states (i.e., state changes), and their causal dependencies.\nThese challenges are further aggravated by the natural parallelism from\nmulti-tasking and partial observations in multi-agent collaboration. Most prior\nworks leverage action localization or future prediction as an indirect metric\nfor evaluating such task understanding from videos. To make a direct\nevaluation, we introduce the EgoTaskQA benchmark that provides a single home\nfor the crucial dimensions of task understanding through question-answering on\nreal-world egocentric videos. We meticulously design questions that target the\nunderstanding of (1) action dependencies and effects, (2) intents and goals,\nand (3) agents' beliefs about others. These questions are divided into four\ntypes, including descriptive (what status?), predictive (what will?),\nexplanatory (what caused?), and counterfactual (what if?) to provide diagnostic\nanalyses on spatial, temporal, and causal understandings of goal-oriented\ntasks. We evaluate state-of-the-art video reasoning models on our benchmark and\nshow their significant gaps between humans in understanding complex\ngoal-oriented egocentric videos. We hope this effort will drive the vision\ncommunity to move onward with goal-oriented video understanding and reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_B/0/1/0/all/0/1\">Baoxiong Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Ting Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Fine-Grained Visual Understanding for Video Question Answering via Decoupling Spatial-Temporal Modeling. (arXiv:2210.03941v1 [cs.CV])","link":"http://arxiv.org/abs/2210.03941","description":"<p>While recent large-scale video-language pre-training made great progress in\nvideo question answering, the design of spatial modeling of video-language\nmodels is less fine-grained than that of image-language models; existing\npractices of temporal modeling also suffer from weak and noisy alignment\nbetween modalities. To learn fine-grained visual understanding, we decouple\nspatial-temporal modeling and propose a hybrid pipeline, Decoupled\nSpatial-Temporal Encoders, integrating an image- and a video-language encoder.\nThe former encodes spatial semantics from larger but sparsely sampled frames\nindependently of time, while the latter models temporal dynamics at lower\nspatial but higher temporal resolution. To help the video-language model learn\ntemporal relations for video QA, we propose a novel pre-training objective,\nTemporal Referring Modeling, which requires the model to identify temporal\npositions of events in video sequences. Extensive experiments demonstrate that\nour model outperforms previous work pre-trained on orders of magnitude larger\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_B/0/1/0/all/0/1\">Bing-Chen Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tsung-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_J/0/1/0/all/0/1\">Jia-Fong Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConstGCN: Constrained Transmission-based Graph Convolutional Networks for Document-level Relation Extraction. (arXiv:2210.03949v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03949","description":"<p>Document-level relation extraction with graph neural networks faces a\nfundamental graph construction gap between training and inference - the golden\ngraph structure only available during training, which causes that most methods\nadopt heuristic or syntactic rules to construct a prior graph as a pseudo\nproxy. In this paper, we propose $\\textbf{ConstGCN}$, a novel graph\nconvolutional network which performs knowledge-based information propagation\nbetween entities along with all specific relation spaces without any prior\ngraph construction. Specifically, it updates the entity representation by\naggregating information from all other entities along with each relation space,\nthus modeling the relation-aware spatial information. To control the\ninformation flow passing through the indeterminate relation spaces, we propose\nto constrain the propagation using transmitting scores learned from the Noise\nContrastive Estimation between fact triples. Experimental results show that our\nmethod outperforms the previous state-of-the-art (SOTA) approaches on the DocRE\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Ji Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1\">Kaisheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine Translation. (arXiv:2210.03953v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03953","description":"<p>Non-autoregressive translation (NAT) models are typically trained with the\ncross-entropy loss, which forces the model outputs to be aligned verbatim with\nthe target sentence and will highly penalize small shifts in word positions.\nLatent alignment models relax the explicit alignment by marginalizing out all\nmonotonic latent alignments with the CTC loss. However, they cannot handle\nnon-monotonic alignments, which is non-negligible as there is typically global\nword reordering in machine translation. In this work, we explore non-monotonic\nlatent alignments for NAT. We extend the alignment space to non-monotonic\nalignments to allow for the global word reordering and further consider all\nalignments that overlap with the target sentence. We non-monotonically match\nthe alignments to the target sentence and train the latent alignment model to\nmaximize the F1 score of non-monotonic matching. Extensive experiments on major\nWMT benchmarks show that our method substantially improves the translation\nperformance of CTC-based models. Our best model achieves 30.06 BLEU on WMT14\nEn-De with only one-iteration decoding, closing the gap between\nnon-autoregressive and autoregressive models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1\">Chenze Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDA: Simple Discrete Augmentation for Contrastive Sentence Representation Learning. (arXiv:2210.03963v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03963","description":"<p>Contrastive learning methods achieve state-of-the-art results in unsupervised\nsentence representation learning. Although playing essential roles in\ncontrastive learning, data augmentation methods applied on sentences have not\nbeen fully explored. Current SOTA method SimCSE utilizes a simple dropout\nmechanism as continuous augmentation which outperforms discrete augmentations\nsuch as cropping, word deletion and synonym replacement. To understand the\nunderlying rationales, we revisit existing approaches and attempt to\nhypothesize the desiderata of reasonable data augmentation methods: balance of\nsemantic consistency and expression diversity. Based on the hypothesis, we\npropose three simple yet effective discrete sentence augmentation methods,\ni.e., punctuation insertion, affirmative auxiliary and double negation. The\npunctuation marks, auxiliaries and negative words act as minimal noises in\nlexical level to produce diverse sentence expressions. Unlike traditional\naugmentation methods which randomly modify the sentence, our augmentation rules\nare well designed for generating semantically consistent and grammatically\ncorrect sentences. We conduct extensive experiments on both English and Chinese\nsemantic textual similarity datasets. The results show the robustness and\neffectiveness of the proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhenyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dongsheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jinghui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fei Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KG-MTT-BERT: Knowledge Graph Enhanced BERT for Multi-Type Medical Text Classification. (arXiv:2210.03970v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03970","description":"<p>Medical text learning has recently emerged as a promising area to improve\nhealthcare due to the wide adoption of electronic health record (EHR) systems.\nThe complexity of the medical text such as diverse length, mixed text types,\nand full of medical jargon, poses a great challenge for developing effective\ndeep learning models. BERT has presented state-of-the-art results in many NLP\ntasks, such as text classification and question answering. However, the\nstandalone BERT model cannot deal with the complexity of the medical text,\nespecially the lengthy clinical notes. Herein, we develop a new model called\nKG-MTT-BERT (Knowledge Graph Enhanced Multi-Type Text BERT) by extending the\nBERT model for long and multi-type text with the integration of the medical\nknowledge graph. Our model can outperform all baselines and other\nstate-of-the-art models in diagnosis-related group (DRG) classification, which\nrequires comprehensive medical text for accurate classification. We also\ndemonstrated that our model can effectively handle multi-type text and the\nintegration of medical knowledge graph can significantly improve the\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaorong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhenyu Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition. (arXiv:2210.03980v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03980","description":"<p>Continual Learning for Named Entity Recognition (CL-NER) aims to learn a\ngrowing number of entity types over time from a stream of data. However, simply\nlearning Other-Class in the same way as new entity types amplifies the\ncatastrophic forgetting and leads to a substantial performance drop. The main\ncause behind this is that Other-Class samples usually contain old entity types,\nand the old knowledge in these Other-Class samples is not preserved properly.\nThanks to the causal inference, we identify that the forgetting is caused by\nthe missing causal effect from the old data. To this end, we propose a unified\ncausal framework to retrieve the causality from both new entity types and\nOther-Class. Furthermore, we apply curriculum learning to mitigate the impact\nof label noise and introduce a self-adaptive weight for balancing the causal\neffects between new entity types and Other-Class. Experimental results on three\nbenchmark datasets show that our method outperforms the state-of-the-art method\nby a large margin. Moreover, our method can be combined with the existing\nstate-of-the-art methods to improve the performance in CL-NER\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Junhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhanxian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haibin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianli Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bird-Eye Transformers for Text Generation Models. (arXiv:2210.03985v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03985","description":"<p>Transformers have become an indispensable module for text generation models\nsince their great success in machine translation. Previous works attribute\nthe~success of transformers to the query-key-value dot-product attention, which\nprovides a robust inductive bias by the fully connected token graphs. However,\nwe found that self-attention has a severe limitation. When predicting the\n(i+1)-th token, self-attention only takes the i-th token as an information\ncollector, and it tends to give a high attention weight to those tokens similar\nto itself. Therefore, most of the historical information that occurred before\nthe i-th token is not taken into consideration. Based on this observation, in\nthis paper, we propose a new architecture, called bird-eye transformer(BET),\nwhich goes one step further to improve the performance of transformers by\nreweighting self-attention to encourage it to focus more on important\nhistorical information. We have conducted experiments on multiple text\ngeneration tasks, including machine translation (2 datasets) and language\nmodels (3 datasets). These experimental~results show that our proposed model\nachieves a better performance than the baseline transformer architectures\non~all~datasets. The code is released at:\n\\url{https://sites.google.com/view/bet-transformer/home}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1\">Lei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuhang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yordanov_Y/0/1/0/all/0/1\">Yordan Yordanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvatori_T/0/1/0/all/0/1\">Tommaso Salvatori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Language Models for Paragraph-Level Question Generation. (arXiv:2210.03992v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03992","description":"<p>Powerful generative models have led to recent progress in question generation\n(QG). However, it is difficult to measure advances in QG research since there\nare no standardized resources that allow a uniform comparison among approaches.\nIn this paper, we introduce QG-Bench, a multilingual and multidomain benchmark\nfor QG that unifies existing question answering datasets by converting them to\na standard QG setting. It includes general-purpose datasets such as SQuAD for\nEnglish, datasets from ten domains and two styles, as well as datasets in eight\ndifferent languages. Using QG-Bench as a reference, we perform an extensive\nanalysis of the capabilities of language models for the task. First, we propose\nrobust QG baselines based on fine-tuning generative language models. Then, we\ncomplement automatic evaluation based on standard metrics with an extensive\nmanual evaluation, which in turn sheds light on the difficulty of evaluating QG\nmodels. Finally, we analyse both the domain adaptability of these models as\nwell as the effectiveness of multilingual models in languages other than\nEnglish. QG-Bench is released along with the fine-tuned models presented in the\npaper https://github.com/asahi417/lm-question-generation, which are also\navailable as a demo https://autoqg.net/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ushio_A/0/1/0/all/0/1\">Asahi Ushio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alva_Manchego_F/0/1/0/all/0/1\">Fernando Alva-Manchego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ngram-OAXE: Phrase-Based Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation. (arXiv:2210.03999v1 [cs.CL])","link":"http://arxiv.org/abs/2210.03999","description":"<p>Recently, a new training oaxe loss has proven effective to ameliorate the\neffect of multimodality for non-autoregressive translation (NAT), which removes\nthe penalty of word order errors in the standard cross-entropy loss. Starting\nfrom the intuition that reordering generally occurs between phrases, we extend\noaxe by only allowing reordering between ngram phrases and still requiring a\nstrict match of word order within the phrases. Extensive experiments on NAT\nbenchmarks across language pairs and data scales demonstrate the effectiveness\nand universality of our approach. %Further analyses show that the proposed\nngram-oaxe alleviates the multimodality problem with a better modeling of\nphrase translation. Further analyses show that ngram-oaxe indeed improves the\ntranslation of ngram phrases, and produces more fluent translation with a\nbetter modeling of sentence structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1\">Cunxiao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EDU-level Extractive Summarization with Varying Summary Lengths. (arXiv:2210.04029v1 [cs.CL])","link":"http://arxiv.org/abs/2210.04029","description":"<p>Extractive models usually formulate text summarization as extracting top-k\nimportant sentences from document as summary. Few work exploited extracting\nfiner-grained Elementary Discourse Unit (EDU) and there is little analysis and\njustification for the extractive unit selection. To fill such a gap, this paper\nfirstly conducts oracle analysis to compare the upper bound of performance for\nmodels based on EDUs and sentences. The analysis provides evidences from both\ntheoretical and experimental perspectives to justify that EDUs make more\nconcise and precise summary than sentences without losing salient information.\nThen, considering this merit of EDUs, this paper further proposes EDU-level\nextractive model with Varying summary Lengths (EDU-VL) and develops the\ncorresponding learning algorithm. EDU-VL learns to encode and predict\nprobabilities of EDUs in document, and encode EDU-level candidate summaries\nwith different lengths based on various $k$ values and select the best\ncandidate summary in an end-to-end training manner. Finally, the proposed and\ndeveloped approach is experimented on single and multi-document benchmark\ndatasets and shows the improved performances in comparison with the\nstate-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuping Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_C/0/1/0/all/0/1\">Ching-Hsun Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jiayu Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shengzhong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiao-Jun Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Task-Adaptive Pretraining for Dialogue Response Selection. (arXiv:2210.04073v1 [cs.CL])","link":"http://arxiv.org/abs/2210.04073","description":"<p>Recent advancements in dialogue response selection (DRS) are based on the\n\\textit{task-adaptive pre-training (TAP)} approach, by first initializing their\nmodel with BERT~\\cite{devlin-etal-2019-bert}, and adapt to dialogue data with\ndialogue-specific or fine-grained pre-training tasks. However, it is uncertain\nwhether BERT is the best initialization choice, or whether the proposed\ndialogue-specific fine-grained learning tasks are actually better than MLM+NSP.\nThis paper aims to verify assumptions made in previous works and understand the\nsource of improvements for DRS. We show that initializing with RoBERTa achieve\nsimilar performance as BERT, and MLM+NSP can outperform all previously proposed\nTAP tasks, during which we also contribute a new state-of-the-art on the Ubuntu\ncorpus. Additional analyses shows that the main source of improvements comes\nfrom the TAP step, and that the NSP task is crucial to DRS, different from\ncommon NLU tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tzu-Hsiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_T/0/1/0/all/0/1\">Ta-Chung Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are All Steps Equally Important? Benchmarking Essentiality Detection of Events. (arXiv:2210.04074v1 [cs.CL])","link":"http://arxiv.org/abs/2210.04074","description":"<p>Natural language often describes events in different granularities, such that\nmore coarse-grained (goal) events can often be decomposed into fine-grained\nsequences of (step) events. A critical but overlooked challenge in\nunderstanding an event process lies in the fact that the step events are not\nequally important to the central goal. In this paper, we seek to fill this gap\nby studying how well current models can understand the essentiality of\ndifferent step events towards a goal event. As discussed by cognitive studies,\nsuch an ability enables the machine to mimic human's commonsense reasoning\nabout preconditions and necessary efforts of daily-life tasks. Our work\ncontributes with a high-quality corpus of (goal, step) pairs from a community\nguideline website WikiHow, where the steps are manually annotated with their\nessentiality w.r.t. the goal. The high IAA indicates that humans have a\nconsistent understanding of the events. Despite evaluating various statistical\nand massive pre-trained NLU models, we observe that existing SOTA models all\nperform drastically behind humans, indicating the need for future investigation\nof this crucial yet challenging task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yueguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuqian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding. (arXiv:2210.04105v1 [cs.CL])","link":"http://arxiv.org/abs/2210.04105","description":"<p>With the advent of pre-trained language models (LMs), increasing research\nefforts have been focusing on infusing commonsense and domain-specific\nknowledge to prepare LMs for downstream tasks. These works attempt to leverage\nknowledge graphs, the de facto standard of symbolic knowledge representation,\nalong with pre-trained LMs. While existing approaches leverage external\nknowledge, it remains an open question how to jointly incorporate knowledge\ngraphs representing varying contexts, from local (e.g., sentence), to\ndocument-level, to global knowledge, to enable knowledge-rich and interpretable\nexchange across these contexts. Such rich contextualization can be especially\nbeneficial for long document understanding tasks since standard pre-trained LMs\nare typically bounded by the input sequence length. In light of these\nchallenges, we propose KALM, a Knowledge-Aware Language Model that jointly\nleverages knowledge in local, document-level, and global contexts for long\ndocument understanding. KALM first encodes long documents and knowledge graphs\ninto the three knowledge-aware context representations. It then processes each\ncontext with context-specific layers, followed by a context fusion layer that\nfacilitates interpretable knowledge exchange to derive an overarching document\nrepresentation. Extensive experiments demonstrate that KALM achieves\nstate-of-the-art performance on three long document understanding tasks across\n6 datasets/settings. Further analyses reveal that the three knowledge-aware\ncontexts are complementary and they all contribute to model performance, while\nthe importance and information exchange patterns of different contexts vary\nwith respect to different tasks and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhaoxuan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhenyu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic modeling of rational communication with conditionals. (arXiv:2105.05502v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.05502","description":"<p>While a large body of work has scrutinized the meaning of conditional\nsentences, considerably less attention has been paid to formal models of their\npragmatic use and interpretation. Here, we take a probabilistic approach to\npragmatic reasoning about indicative conditionals which flexibly integrates\ngradient beliefs about richly structured world states. We model listeners'\nupdate of their prior beliefs about the causal structure of the world and the\njoint probabilities of the consequent and antecedent based on assumptions about\nthe speaker's utterance production protocol. We show that, when supplied with\nnatural contextual assumptions, our model uniformly explains a number of\ninferences attested in the literature, including epistemic inferences,\nconditional perfection and the dependency between antecedent and consequent of\na conditional. We argue that this approach also helps explain three puzzles\nintroduced by Douven (2012) about updating with conditionals: depending on the\nutterance context, the listener's belief in the antecedent may increase,\ndecrease or remain unchanged.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grusdt_B/0/1/0/all/0/1\">Britta Grusdt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassiter_D/0/1/0/all/0/1\">Daniel Lassiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franke_M/0/1/0/all/0/1\">Michael Franke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Digging Errors in NMT: Evaluating and Understanding Model Errors from Partial Hypothesis Space. (arXiv:2106.15217v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.15217","description":"<p>Solid evaluation of neural machine translation (NMT) is key to its\nunderstanding and improvement. Current evaluation of an NMT system is usually\nbuilt upon a heuristic decoding algorithm (e.g., beam search) and an evaluation\nmetric assessing similarity between the translation and golden reference.\nHowever, this system-level evaluation framework is limited by evaluating only\none best hypothesis and search errors brought by heuristic decoding algorithms.\nTo better understand NMT models, we propose a novel evaluation protocol, which\ndefines model errors with model's ranking capability over hypothesis space. To\ntackle the problem of exponentially large space, we propose two approximation\nmethods, top region evaluation along with an exact top-$k$ decoding algorithm,\nwhich finds top-ranked hypotheses in the whole hypothesis space, and Monte\nCarlo sampling evaluation, which simulates hypothesis space from a broader\nperspective. To quantify errors, we define our NMT model errors by measuring\ndistance between the hypothesis array ranked by the model and the ideally\nranked hypothesis array. After confirming the strong correlation with human\njudgment, we apply our evaluation to various NMT benchmarks and model\narchitectures. We show that the state-of-the-art Transformer models face\nserious ranking issues and only perform at the random chance level in the top\nregion. We further analyze model errors on architectures with different depths\nand widths, as well as different data-augmentation techniques, showing how\nthese factors affect model errors. Finally, we connect model errors with the\nsearch algorithms and provide interesting findings of beam search inductive\nbias and correlation with Minimum Bayes Risk (MBR) decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling chronic pain experiences from online reports using the Reddit Reports of Chronic Pain dataset. (arXiv:2108.10218v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10218","description":"<p>Purpose: Reveal and quantify qualities of reported experiences of chronic\npain on social media, from multiple pathological backgrounds, by means of the\nnovel Reddit Reports of Chronic Pain (RRCP) dataset, using Natural Language\nProcessing techniques. Methods: Define and validate the RRCP dataset for a set\nof subreddits related to chronic pain. Identify the main concerns discussed in\neach subreddit. Model each subreddit according to their main concerns. Compare\nsubreddit models. Results: The RRCP dataset comprises 86,537 Reddit submissions\nfrom 12 subreddits related to chronic pain (each related to one pathological\nbackground). Each RRCP subreddit has various main concerns. Some of these\nconcerns are shared between multiple subreddits (e.g., the subreddit Sciatica\nsemantically entails the subreddit backpain in their various concerns, but not\nthe other way around), whilst some concerns are exclusive to specific\nsubreddits (e.g., Interstitialcystitis and CrohnsDisease). These results\nsuggest that the reported experience of chronic pain, from multiple pathologies\n(i.e., subreddits), has concerns relevant to all, and concerns exclusive to\ncertain pathologies. Our analysis details each of these concerns and their\nrelations. Conclusion: Although limited by intrinsic qualities of the Reddit\nplatform, to the best of our knowledge, this is the first research work\nattempting to model the linguistic expression of various chronic pain-inducing\npathologies and comparing these models to identify and quantify the\nsimilarities and differences between the corresponding emergent chronic pain\nexperiences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nunes_D/0/1/0/all/0/1\">Diogo A.P. Nunes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_J/0/1/0/all/0/1\">Joana Ferreira Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_F/0/1/0/all/0/1\">Fani Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matos_D/0/1/0/all/0/1\">David Martins de Matos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Rule Generation for Time Expression Normalization. (arXiv:2108.13658v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13658","description":"<p>The understanding of time expressions includes two sub-tasks: recognition and\nnormalization. In recent years, significant progress has been made in the\nrecognition of time expressions while research on normalization has lagged\nbehind. Existing SOTA normalization methods highly rely on rules or grammars\ndesigned by experts, which limits their performance on emerging corpora, such\nas social media texts. In this paper, we model time expression normalization as\na sequence of operations to construct the normalized temporal value, and we\npresent a novel method called ARTime, which can automatically generate\nnormalization rules from training data without expert interventions.\nSpecifically, ARTime automatically captures possible operation sequences from\nannotated data and generates normalization rules on time expressions with\ncommon surface forms. The experimental results show that ARTime can\nsignificantly surpass SOTA methods on the Tweets benchmark, and achieves\ncompetitive results with existing expert-engineered rule methods on the\nTempEval-3 benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wentao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinmao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TVStoryGen: A Dataset for Generating Stories with Character Descriptions. (arXiv:2109.08833v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08833","description":"<p>We introduce TVStoryGen, a story generation dataset that requires generating\ndetailed TV show episode recaps from a brief summary and a set of documents\ndescribing the characters involved. Unlike other story generation datasets,\nTVStoryGen contains stories that are authored by professional screen-writers\nand that feature complex interactions among multiple characters. Generating\nstories in TVStoryGen requires drawing relevant information from the lengthy\nprovided documents about characters based on the brief summary. In addition, we\npropose to train reverse models on our dataset for evaluating the faithfulness\nof generated stories. We create TVStoryGen from fan-contributed websites, which\nallows us to collect 26k episode recaps with 1868.7 tokens on average.\nEmpirically, we take a hierarchical story generation approach and find that the\nneural model that uses oracle content selectors for character descriptions\ndemonstrates the best performance on automatic metrics, showing the potential\nof our dataset to inspire future research on story generation with constraints.\nQualitative analysis shows that the best-performing model sometimes generates\ncontent that is unfaithful to the short summaries, suggesting promising\ndirections for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Reasoning via Template Filling. (arXiv:2111.00539v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00539","description":"<p>Large-scale sequence-to-sequence models have shown to be adept at both\nmultiple-choice and open-domain commonsense reasoning tasks. However, the\ncurrent systems do not provide the ability to control the various attributes of\nthe reasoning chain. To enable better controllability, we propose to study the\ncommonsense reasoning as a template filling task (TemplateCSR) -- where the\nlanguage models fills reasoning templates with the given constraints as control\nfactors. As an approach to TemplateCSR, we (i) propose a dataset of commonsense\nreasoning template-expansion pairs and (ii) introduce POTTER, a pretrained\nsequence-to-sequence model using prompts to perform commonsense reasoning\nacross concepts. Our experiments show that our approach outperforms baselines\nboth in generation metrics and factuality metrics. We also present a detailed\nerror analysis on our approach's ability to reliably perform commonsense\nreasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_D/0/1/0/all/0/1\">Dheeraj Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1\">Vivek Khetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacaleanu_B/0/1/0/all/0/1\">Bogdan Sacaleanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gershman_A/0/1/0/all/0/1\">Anatole Gershman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fano_A/0/1/0/all/0/1\">Andrew Fano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections. (arXiv:2111.00701v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00701","description":"<p>While there has been substantial progress in text comprehension through\nsimple factoid question answering, more holistic comprehension of a discourse\nstill presents a major challenge. Someone critically reflecting on a text as\nthey read it will pose curiosity-driven, often open-ended questions, which\nreflect deep understanding of the content and require complex reasoning to\nanswer. A key challenge in building and evaluating models for this type of\ndiscourse comprehension is the lack of annotated data, especially since finding\nanswers to such questions (which may not be answered at all) requires high\ncognitive load for annotators over long documents. This paper presents a novel\nparadigm that enables scalable data collection targeting the comprehension of\nnews documents, viewing these questions through the lens of discourse. The\nresulting corpus, DCQA (Discourse Comprehension by Question Answering),\nconsists of 22,430 question-answer pairs across 607 English documents. DCQA\ncaptures both discourse and semantic links between sentences in the form of\nfree-form, open-ended questions. On an evaluation set that we annotated on\nquestions from the INQUISITIVE dataset, we show that DCQA provides valuable\nsupervision for answering open-ended questions. We additionally design\npre-training methods utilizing existing question-answering resources, and use\nsynthetic data to accommodate unanswerable questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_W/0/1/0/all/0/1\">Wei-Jen Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalton_C/0/1/0/all/0/1\">Cutter Dalton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simmons_M/0/1/0/all/0/1\">Mark Simmons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_E/0/1/0/all/0/1\">Eliza Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Predictive Uncertainty by Looking Back at Model Explanations. (arXiv:2201.03742v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03742","description":"<p>Predictive uncertainty estimation of pre-trained language models is an\nimportant measure of how likely people can trust their predictions. However,\nlittle is known about what makes a model prediction uncertain. Explaining\npredictive uncertainty is an important complement to explaining prediction\nlabels in helping users understand model decision making and gaining their\ntrust on model predictions, while has been largely ignored in prior works. In\nthis work, we propose to explain the predictive uncertainty of pre-trained\nlanguage models by extracting uncertain words from existing model explanations.\nWe find the uncertain words are those identified as making negative\ncontributions to prediction labels, while actually explaining the predictive\nuncertainty. Experiments show that uncertainty explanations are indispensable\nto explaining models and helping humans understand model prediction behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Contrastive Learning for Self-supervised Entity Alignment. (arXiv:2201.06225v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06225","description":"<p>Self-supervised entity alignment (EA) aims to link equivalent entities across\ndifferent knowledge graphs (KGs) without seed alignments. The current SOTA\nself-supervised EA method draws inspiration from contrastive learning,\noriginally designed in computer vision based on instance discrimination and\ncontrastive loss, and suffers from two shortcomings. Firstly, it puts\nunidirectional emphasis on pushing sampled negative entities far away rather\nthan pulling positively aligned pairs close, as is done in the well-established\nsupervised EA. Secondly, KGs contain rich side information (e.g., entity\ndescription), and how to effectively leverage those information has not been\nadequately investigated in self-supervised EA. In this paper, we propose an\ninteractive contrastive learning model for self-supervised EA. The model\nencodes not only structures and semantics of entities (including entity name,\nentity description, and entity neighborhood), but also conducts cross-KG\ncontrastive learning by building pseudo-aligned entity pairs. Experimental\nresults show that our approach outperforms previous best self-supervised\nresults by a large margin (over 9% average improvement) and performs on par\nwith previous SOTA supervised counterparts, demonstrating the effectiveness of\nthe interactive contrastive learning for self-supervised EA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1\">Kaisheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhenhao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Minghao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Ling Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data. (arXiv:2202.02842v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02842","description":"<p>The search for effective and robust metrics has been the focus of recent\ntheoretical and empirical work on generalization of deep neural networks (NNs).\nIn this paper, we discuss the performance of natural language processing (NLP)\nmodels, and we evaluate various existing and novel generalization metrics.\nCompared to prior studies, we (i) focus on NLP instead of computer vision (CV),\n(ii) focus on generalization metrics that predict test error instead of the\ngeneralization gap, (iii) focus on generalization metrics that do not need the\naccess to data, and (iv) focus on the heavy-tail (HT) phenomenon that has\nreceived comparatively less attention in the study of NNs. We extend recent\nHT-based work which focuses on power law (PL) distributions, and we study\nexponential and exponentially truncated power law (E-TPL) fitting to the\nempirical spectral densities (ESDs) of weight matrices. Our empirical studies\nare carried on (i) hundreds of Transformers trained in different settings, in\nwhich we systematically vary different hyperparameters, (ii) a total of 51\npretrained Transformers from eight families of Huggingface NLP models,\nincluding BERT, GPT2, etc., and (iii) a total of 28 existing and novel\ngeneralization metrics. From our empirical analyses, we show that shape\nmetrics, or the metrics obtained from fitting the shape of the ESDs, perform\nuniformly better at predicting generalization performance than scale metrics\ncommonly studied in the literature, as measured by the rank correlations with\nthe generalization performance. We also show that among the three HT\ndistributions considered in our paper, the E-TPL fitting of ESDs performs the\nmost robustly when the models are trained in experimental settings, while the\nPL fitting achieves the best performance on well-trained Huggingface models,\nand that both E-TPL and PL metrics (which are both shape metrics) outperform\nscale metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaoqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theisen_R/0/1/0/all/0/1\">Ryan Theisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodgkinson_L/0/1/0/all/0/1\">Liam Hodgkinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_C/0/1/0/all/0/1\">Charles H. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring and Reducing Model Update Regression in Structured Prediction for NLP. (arXiv:2202.02976v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02976","description":"<p>Recent advance in deep learning has led to the rapid adoption of machine\nlearning-based NLP models in a wide range of applications. Despite the\ncontinuous gain in accuracy, backward compatibility is also an important aspect\nfor industrial applications, yet it received little research attention.\nBackward compatibility requires that the new model does not regress on cases\nthat were correctly handled by its predecessor. This work studies model update\nregression in structured prediction tasks. We choose syntactic dependency\nparsing and conversational semantic parsing as representative examples of\nstructured prediction tasks in NLP. First, we measure and analyze model update\nregression in different model update settings. Next, we explore and benchmark\nexisting techniques for reducing model update regression including model\nensemble and knowledge distillation. We further propose a simple and effective\nmethod, Backward-Congruent Re-ranking (BCR), by taking into account the\ncharacteristics of structured prediction. Experiments show that BCR can better\nmitigate model update regression than model ensemble and knowledge distillation\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1\">Elman Mansimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yi-An Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Evaluation Metrics for Paraphrase Generation. (arXiv:2202.08479v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08479","description":"<p>In this paper we revisit automatic metrics for paraphrase evaluation and\nobtain two findings that disobey conventional wisdom: (1) Reference-free\nmetrics achieve better performance than their reference-based counterparts. (2)\nMost commonly used metrics do not align well with human annotation. Underlying\nreasons behind the above findings are explored through additional experiments\nand in-depth analyses. Based on the experiments and analyses, we propose\nParaScore, a new evaluation metric for paraphrase generation. It possesses the\nmerits of reference-based and reference-free metrics and explicitly models\nlexical divergence. Experimental results demonstrate that ParaScore\nsignificantly outperforms existing metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lingfeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT-LID: Leveraging BERT to Improve Spoken Language Identification. (arXiv:2203.00328v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00328","description":"<p>Language identification is the task of automatically determining the identity\nof a language conveyed by a spoken segment. It has a profound impact on the\nmultilingual interoperability of an intelligent speech system. Despite language\nidentification attaining high accuracy on medium or long utterances(&gt;3s), the\nperformance on short utterances (&lt;=1s) is still far from satisfactory. We\npropose a BERT-based language identification system (BERT-LID) to improve\nlanguage identification performance, especially on short-duration speech\nsegments. We extend the original BERT model by taking the phonetic\nposteriorgrams (PPG) derived from the front-end phone recognizer as input. Then\nwe deployed the optimal deep classifier followed by it for language\nidentification. Our BERT-LID model can improve the baseline accuracy by about\n6.5% on long-segment identification and 19.9% on short-segment identification,\ndemonstrating our BERT-LID's effectiveness to language identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yuting Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junhong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinfeng Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models. (arXiv:2203.01104v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.01104","description":"<p>Recently, Mixture-of-Experts (short as MoE) architecture has achieved\nremarkable success in increasing the model capacity of large-scale language\nmodels. However, MoE requires incorporating significantly more parameters than\nthe base model being extended. In this paper, we propose building a\nparameter-efficient MoE architecture by sharing information among experts. We\nadopt the matrix product operator (MPO, a tensor decomposition from quantum\nmany-body physics) to reconstruct the parameter matrix in the expert layer and\nincrease model capacity for pre-trained language models by sharing parameters\nof the central tensor (containing the core information) among different experts\nwhile enabling the specificity through the auxiliary tensors (complementing the\ncentral tensor) of different experts. To address the unbalanced optimization\nissue, we further design the gradient mask strategy for the MPO-based MoE\narchitecture. Extensive experiments based on T5 and GPT-2 show improved\nperformance and efficiency of the pre-trained language model (27.2x reduction\nin total parameters for the superior model performance, compared with the\nSwitch Transformers). Our code is publicly available at\n\\url{https://github.com/RUCAIBox/MPOE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Ze-Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhong-Yi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Providing Insights for Open-Response Surveys via End-to-End Context-Aware Clustering. (arXiv:2203.01294v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.01294","description":"<p>Teachers often conduct surveys in order to collect data from a predefined\ngroup of students to gain insights into topics of interest. When analyzing\nsurveys with open-ended textual responses, it is extremely time-consuming,\nlabor-intensive, and difficult to manually process all the responses into an\ninsightful and comprehensive report. In the analysis step, traditionally, the\nteacher has to read each of the responses and decide on how to group them in\norder to extract insightful information. Even though it is possible to group\nthe responses only using certain keywords, such an approach would be limited\nsince it not only fails to account for embedded contexts but also cannot detect\npolysemous words or phrases and semantics that are not expressible in single\nwords. In this work, we present a novel end-to-end context-aware framework that\nextracts, aggregates, and abbreviates embedded semantic patterns in\nopen-response survey data. Our framework relies on a pre-trained natural\nlanguage model in order to encode the textual data into semantic vectors. The\nencoded vectors then get clustered either into an optimally tuned number of\ngroups or into a set of groups with pre-specified titles. In the former case,\nthe clusters are then further analyzed to extract a representative set of\nkeywords or summary sentences that serve as the labels of the clusters. In our\nframework, for the designated clusters, we finally provide context-aware\nwordclouds that demonstrate the semantically prominent keywords within each\ngroup. Honoring user privacy, we have successfully built the on-device\nimplementation of our framework suitable for real-time analysis on mobile\ndevices and have tested it on a synthetic dataset. Our framework reduces the\ncosts at-scale by automating the process of extracting the most insightful\ninformation pieces from survey data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esmaeilzadeh_S/0/1/0/all/0/1\">Soheil Esmaeilzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Brian Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsi_D/0/1/0/all/0/1\">Davood Shamsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vikingstad_O/0/1/0/all/0/1\">Onar Vikingstad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MarkBERT: Marking Word Boundaries Improves Chinese BERT. (arXiv:2203.06378v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06378","description":"<p>We present a Chinese BERT model dubbed MarkBERT that uses word information in\nthis work. Existing word-based BERT models regard words as basic units,\nhowever, due to the vocabulary limit of BERT, they only cover high-frequency\nwords and fall back to character level when encountering out-of-vocabulary\n(OOV) words. Different from existing works, MarkBERT keeps the vocabulary being\nChinese characters and inserts boundary markers between contiguous words. Such\ndesign enables the model to handle any words in the same way, no matter they\nare OOV words or not. Besides, our model has two additional benefits: first, it\nis convenient to add word-level learning objectives over markers, which is\ncomplementary to traditional character and sentence-level pretraining tasks;\nsecond, it can easily incorporate richer semantics such as POS tags of words by\nreplacing generic markers with POS tag-specific markers. With the simple\nmarkers insertion, MarkBERT can improve the performances of various downstream\ntasks including language understanding and sequence labeling. \\footnote{All the\ncodes and models will be made publicly available at\n\\url{https://github.com/daiyongya/markbert}}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Universal Sentence Embeddings with Prompt-based Contrastive Learning and Energy-based Learning. (arXiv:2203.06875v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06875","description":"<p>Contrastive learning has been demonstrated to be effective in enhancing\npre-trained language models (PLMs) to derive superior universal sentence\nembeddings. However, existing contrastive methods still have two limitations.\nFirstly, previous works may acquire poor performance under domain shift\nsettings, thus hindering the application of sentence representations in\npractice. We attribute this low performance to the over-parameterization of\nPLMs with millions of parameters. To alleviate it, we propose PromCSE\n(Prompt-based Contrastive Learning for Sentence Embeddings), which only trains\nsmall-scale \\emph{Soft Prompt} (i.e., a set of trainable vectors) while keeping\nPLMs fixed. Secondly, the commonly used NT-Xent loss function of contrastive\nlearning does not fully exploit hard negatives in supervised learning settings.\nTo this end, we propose to integrate an Energy-based Hinge loss to enhance the\npairwise discriminative power, inspired by the connection between the NT-Xent\nloss and the Energy-based Learning paradigm. Empirical results on seven\nstandard semantic textual similarity (STS) tasks and a domain-shifted STS task\nboth show the effectiveness of our method compared with the current\nstate-of-the-art sentence embedding models. Our code is publicly avaliable at\nhttps://github.com/YJiangcm/PromCSE\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General and Domain Adaptive Chinese Spelling Check with Error Consistent Pretraining. (arXiv:2203.10929v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10929","description":"<p>The lack of label data is one of the significant bottlenecks for Chinese\nSpelling Check (CSC). Existing researches use the method of automatic\ngeneration by exploiting unlabeled data to expand the supervised corpus.\nHowever, there is a big gap between the real input scenario and automatic\ngenerated corpus. Thus, we develop a competitive general speller ECSpell which\nadopts the Error Consistent masking strategy to create data for pretraining.\nThis error consistency masking strategy is used to specify the error types of\nautomatically generated sentences which is consistent with real scene. The\nexperimental result indicates our model outperforms previous state-of-the-art\nmodels on the general benchmark. Moreover, spellers often work within a\nparticular domain in real life. Due to lots of uncommon domain terms,\nexperiments on our built domain specific datasets show that general models\nperform terribly. Inspired by the common practice of input methods, we propose\nto add an alterable user dictionary to handle the zero-shot domain adaption\nproblem. Specifically, we attach a User Dictionary guided inference module (UD)\nto a general token classification based speller. Our experiments demonstrate\nthat ECSpell$^{UD}$, namely ECSpell combined with UD, surpasses all the other\nbaselines largely, even approaching the performance on the general benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1\">Qi Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziqiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_L/0/1/0/all/0/1\">Lei Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_C/0/1/0/all/0/1\">Chunhui Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guohong Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Clinical Coding: What, Why, and Where We Are?. (arXiv:2203.11092v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.11092","description":"<p>Clinical coding is the task of transforming medical information in a\npatient's health records into structured codes so that they can be used for\nstatistical analysis. This is a cognitive and time-consuming task that follows\na standard process in order to achieve a high level of consistency. Clinical\ncoding could potentially be supported by an automated system to improve the\nefficiency and accuracy of the process. We introduce the idea of automated\nclinical coding and summarise its challenges from the perspective of Artificial\nIntelligence (AI) and Natural Language Processing (NLP), based on the\nliterature, our project experience over the past two and half years (late 2019\n- early 2022), and discussions with clinical coding experts in Scotland and the\nUK. Our research reveals the gaps between the current deep learning-based\napproach applied to clinical coding and the need for explainability and\nconsistency in real-world practice. Knowledge-based methods that represent and\nreason the standard, explainable process of a task may need to be incorporated\ninto deep learning-based methods for clinical coding. Automated clinical coding\nis a promising task for AI, despite the technical and organisational\nchallenges. Coders are needed to be involved in the development process. There\nis much to achieve to develop and deploy an AI-based automated system to\nsupport coding in the next five years and beyond.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falis_M/0/1/0/all/0/1\">Mat&#xfa;&#x161; Falis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteley_W/0/1/0/all/0/1\">William Whiteley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alex_B/0/1/0/all/0/1\">Beatrice Alex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matterson_J/0/1/0/all/0/1\">Joshua Matterson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Single-Channel Speech for Multi-Channel End-to-End Speech Recognition: A Comparative Study. (arXiv:2203.16757v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.16757","description":"<p>Recently, the end-to-end training approach for multi-channel ASR has shown\nits effectiveness, which usually consists of a beamforming front-end and a\nrecognition back-end. However, the end-to-end training becomes more difficult\ndue to the integration of multiple modules, particularly considering that\nmulti-channel speech data recorded in real environments are limited in size.\nThis raises the demand to exploit the single-channel data for multi-channel\nend-to-end ASR. In this paper, we systematically compare the performance of\nthree schemes to exploit external single-channel data for multi-channel\nend-to-end ASR, namely back-end pre-training, data scheduling, and data\nsimulation, under different settings such as the sizes of the single-channel\ndata and the choices of the front-end. Extensive experiments on CHiME-4 and\nAISHELL-4 datasets demonstrate that while all three methods improve the\nmulti-channel end-to-end speech recognition performance, data simulation\noutperforms the other two, at the cost of longer training time. Data scheduling\noutperforms back-end pre-training marginally but nearly consistently,\npresumably because that in the pre-training stage, the back-end tends to\noverfit on the single-channel data, especially when the single-channel data\nsize is small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+An_K/0/1/0/all/0/1\">Keyu An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Ji Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can language models learn from explanations in context?. (arXiv:2204.02329v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02329","description":"<p>Language Models (LMs) can perform new tasks by adapting to a few in-context\nexamples. For humans, explanations that connect examples to task principles can\nimprove learning. We therefore investigate whether explanations of few-shot\nexamples can help LMs. We annotate questions from 40 challenging tasks with\nanswer explanations, and various matched control explanations. We evaluate how\ndifferent types of explanations, instructions, and controls affect zero- and\nfew-shot performance. We analyze these results using statistical multilevel\nmodeling techniques that account for the nested dependencies among conditions,\ntasks, prompts, and models. We find that explanations can improve performance\n-- even without tuning. Furthermore, explanations hand-tuned for performance on\na small validation set offer substantially larger benefits, and building a\nprompt by selecting examples and explanations together substantially improves\nperformance over selecting examples alone. Finally, even untuned explanations\noutperform carefully matched controls, suggesting that the benefits are due to\nthe link between an example and its explanation, rather than lower-level\nfeatures. However, only large models benefit. In summary, explanations can\nsupport the in-context learning of large LMs on challenging tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew K. Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stephanie C. Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthewson_K/0/1/0/all/0/1\">Kory Matthewson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1\">Michael Henry Tessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creswell_A/0/1/0/all/0/1\">Antonia Creswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">James L. McClelland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jane X. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Dialogue Policy for Continual Reinforcement Learning. (arXiv:2204.05928v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05928","description":"<p>Continual learning is one of the key components of human learning and a\nnecessary requirement of artificial intelligence. As dialogue can potentially\nspan infinitely many topics and tasks, a task-oriented dialogue system must\nhave the capability to continually learn, dynamically adapting to new\nchallenges while preserving the knowledge it already acquired. Despite the\nimportance, continual reinforcement learning of the dialogue policy has\nremained largely unaddressed. The lack of a framework with training protocols,\nbaseline models and suitable metrics, has so far hindered research in this\ndirection. In this work we fill precisely this gap, enabling research in\ndialogue policy optimisation to go from static to dynamic learning. We provide\na continual learning algorithm, baseline architectures and metrics for\nassessing continual learning models. Moreover, we propose the dynamic dialogue\npolicy transformer (DDPT), a novel dynamic architecture that can integrate new\nknowledge seamlessly, is capable of handling large state spaces and obtains\nsignificant zero-shot performance when being exposed to unseen domains, without\nany growth in network parameter size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1\">Christian Geishauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1\">Carel van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1\">Michael Heck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsien-Chin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Contamination Helps Explain the Cross-lingual Capabilities of English Pretrained Models. (arXiv:2204.08110v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08110","description":"<p>English pretrained language models, which make up the backbone of many modern\nNLP systems, require huge amounts of unlabeled training data. These models are\ngenerally presented as being trained only on English text but have been found\nto transfer surprisingly well to other languages. We investigate this\nphenomenon and find that common English pretraining corpora actually contain\nsignificant amounts of non-English text: even when less than 1% of data is not\nEnglish (well within the error rate of strong language classifiers), this leads\nto hundreds of millions of foreign language tokens in large-scale datasets. We\nthen demonstrate that even these small percentages of non-English data\nfacilitate cross-lingual transfer for models trained on them, with target\nlanguage performance strongly correlated to the amount of in-language data seen\nduring pretraining. In light of these findings, we argue that no model is truly\nmonolingual when pretrained at scale, which should be considered when\nevaluating cross-lingual transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blevins_T/0/1/0/all/0/1\">Terra Blevins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification. (arXiv:2204.13413v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13413","description":"<p>Hierarchical text classification (HTC) is a challenging subtask of\nmulti-label classification due to its complex label hierarchy. Recently, the\npretrained language models (PLM)have been widely adopted in HTC through a\nfine-tuning paradigm. However, in this paradigm, there exists a huge gap\nbetween the classification tasks with sophisticated label hierarchy and the\nmasked language model (MLM) pretraining tasks of PLMs and thus the potentials\nof PLMs can not be fully tapped. To bridge the gap, in this paper, we propose\nHPT, a Hierarchy-aware Prompt Tuning method to handle HTC from a multi-label\nMLM perspective. Specifically, we construct a dynamic virtual template and\nlabel words that take the form of soft prompts to fuse the label hierarchy\nknowledge and introduce a zero-bounded multi-label cross entropy loss to\nharmonize the objectives of HTC and MLM. Extensive experiments show HPT\nachieves state-of-the-art performances on 3 popular HTC datasets and is adept\nat handling the imbalance and low resource situations. Our code is available at\nhttps://github.com/wzh9969/HPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Binghuai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks. (arXiv:2205.00305v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00305","description":"<p>Transformer-based pre-trained models with millions of parameters require\nlarge storage. Recent approaches tackle this shortcoming by training adapters,\nbut these approaches still require a relatively large number of parameters. In\nthis study, AdapterBias, a surprisingly simple yet effective adapter\narchitecture, is proposed. AdapterBias adds a token-dependent shift to the\nhidden output of transformer layers to adapt to downstream tasks with only a\nvector and a linear layer. Extensive experiments are conducted to demonstrate\nthe effectiveness of AdapterBias. The experiments show that our proposed method\ncan dramatically reduce the trainable parameters compared to the previous works\nwith a minimal decrease in task performances compared with fine-tuned\npre-trained models. We further find that AdapterBias automatically learns to\nassign more significant representation shifts to the tokens related to the task\nin consideration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chin-Lun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zih-Ching Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yun-Ru Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UL2: Unifying Language Learning Paradigms. (arXiv:2205.05131v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05131","description":"<p>Existing pre-trained models are generally geared towards a particular class\nof problems. To date, there seems to be still no consensus on what the right\narchitecture and pre-training setup should be. This paper presents a unified\nframework for pre-training models that are universally effective across\ndatasets and setups. We begin by disentangling architectural archetypes with\npre-training objectives -- two concepts that are commonly conflated. Next, we\npresent a generalized and unified perspective for self-supervision in NLP and\nshow how different pre-training objectives can be cast as one another and how\ninterpolating between different objectives can be effective. We then propose\nMixture-of-Denoisers (MoD), a pre-training objective that combines diverse\npre-training paradigms together. We furthermore introduce a notion of mode\nswitching, wherein downstream fine-tuning is associated with specific\npre-training schemes. We conduct extensive ablative experiments to compare\nmultiple pre-training objectives and find that our method pushes the\nPareto-frontier by outperforming T5 and/or GPT-like models across multiple\ndiverse setups. Finally, by scaling our model up to 20B parameters, we achieve\nSOTA performance on 50 well-established supervised NLP tasks ranging from\nlanguage generation (with automated and human evaluation), language\nunderstanding, text classification, question answering, commonsense reasoning,\nlong text reasoning, structured knowledge grounding and information retrieval.\nOur model also achieve strong results at in-context learning, outperforming\n175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on\none-shot summarization. Finally, we show that UL2 20B works well with\nchain-of-thought prompting and reasoning. We release Flax-based T5X model\ncheckpoints for the 20B model at\n\\url{https://github.com/google-research/google-research/tree/master/ul2}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huaixiu Steven Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Pretrained Language Models Good Long-tailed Learners. (arXiv:2205.05461v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05461","description":"<p>Prompt-tuning has shown appealing performance in few-shot classification by\nvirtue of its capability in effectively exploiting pre-trained knowledge. This\nmotivates us to check the hypothesis that prompt-tuning is also a promising\nchoice for long-tailed classification, since the tail classes are intuitively\nfew-shot ones. To achieve this aim, we conduct empirical studies to examine the\nhypothesis. The results demonstrate that prompt-tuning makes pretrained\nlanguage models at least good long-tailed learners. For intuitions on why\nprompt-tuning can achieve good performance in long-tailed classification, we\ncarry out in-depth analyses by progressively bridging the gap between\nprompt-tuning and commonly used finetuning. The summary is that the classifier\nstructure and parameterization form the key to making good long-tailed\nlearners, in comparison with the less important input structure. Finally, we\nverify the applicability of our finding to few-shot classification. Good\nlong-tailed learners can be abbreviated as Glee.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Lei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL. (arXiv:2205.06983v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.06983","description":"<p>Relational structures such as schema linking and schema encoding have been\nvalidated as a key component to qualitatively translating natural language into\nSQL queries. However, introducing these structural relations comes with prices:\nthey often result in a specialized model structure, which largely prohibits\nusing large pretrained models in text-to-SQL. To address this problem, we\npropose RASAT: a Transformer seq2seq architecture augmented with relation-aware\nself-attention that could leverage a variety of relational structures while\ninheriting the pretrained parameters from the T5 model effectively. Our model\ncan incorporate almost all types of existing relations in the literature, and\nin addition, we propose introducing co-reference relations for the multi-turn\nscenario. Experimental results on three widely used text-to-SQL datasets,\ncovering both single-turn and multi-turn scenarios, have shown that RASAT could\nachieve state-of-the-art results across all three benchmarks (75.5% EX on\nSpider, 52.6% IEX on SParC, and 37.4% IEX on CoSQL).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiexing Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jingyao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Ziwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiangpeng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenghu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Tokenization Learning. (arXiv:2205.11443v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11443","description":"<p>In the presented study, we discover that so called \"transition freedom\"\nmetric appears superior for unsupervised tokenization purposes, compared to\nstatistical metrics such as mutual information and conditional probability,\nproviding F-measure scores in range from 0.71 to 1.0 across explored corpora.\nWe find that different languages require different derivatives of that metric\n(such as variance and \"peak values\") for successful tokenization. Larger\ntraining corpora does not necessarily effect in better tokenization quality,\nwhile compacting the models eliminating statistically weak evidence tends to\nimprove performance. Proposed unsupervised tokenization technique provides\nquality better or comparable to lexicon-based one, depending on the language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolonin_A/0/1/0/all/0/1\">Anton Kolonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1\">Vignav Ramesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLUTE: Figurative Language Understanding through Textual Explanations. (arXiv:2205.12404v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12404","description":"<p>Figurative language understanding has been recently framed as a recognizing\ntextual entailment (RTE) task (a.k.a. natural language inference, or NLI).\nHowever, similar to classical RTE/NLI datasets, the current benchmarks suffer\nfrom spurious correlations and annotation artifacts. To tackle this problem,\nwork on NLI has built explanation-based datasets such as e-SNLI, allowing us to\nprobe whether language models are right for the right reasons.Yet no such data\nexists for figurative language, making it harder to assess genuine\nunderstanding of such expressions. To address this issue, we release FLUTE, a\ndataset of 9,000 figurative NLI instances with explanations, spanning four\ncategories: Sarcasm, Simile, Metaphor, and Idioms. We collect the data through\na model-in-the-loop framework based on GPT-3, crowd workers, and expert\nannotators. We show how utilizing GPT-3 in conjunction with human annotators\n(novices and experts) can aid in scaling up the creation of datasets even for\nsuch complex linguistic phenomena as figurative language. The baseline\nperformance of the T5 model fine-tuned on FLUTE shows that our dataset can\nbring us a step closer to develop-ing models that understand figurative\nlanguage through textual explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saakyan_A/0/1/0/all/0/1\">Arkadiy Saakyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Debanjan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset. (arXiv:2205.12522v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12522","description":"<p>Research in massively multilingual image captioning has been severely\nhampered by a lack of high-quality evaluation datasets. In this paper we\npresent the Crossmodal-3600 dataset (XM3600 in short), a geographically diverse\nset of 3600 images annotated with human-generated reference captions in 36\nlanguages. The images were selected from across the world, covering regions\nwhere the 36 languages are spoken, and annotated with captions that achieve\nconsistency in terms of style across all languages, while avoiding annotation\nartifacts due to direct translation. We apply this benchmark to model selection\nfor massively multilingual image captioning models, and show superior\ncorrelation results with human evaluations when using XM3600 as golden\nreferences for automatic metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thapliyal_A/0/1/0/all/0/1\">Ashish V. Thapliyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pont_Tuset_J/0/1/0/all/0/1\">Jordi Pont-Tuset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DFM: Dialogue Foundation Model for Universal Large-Scale Dialogue-Oriented Task Learning. (arXiv:2205.12662v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12662","description":"<p>Building a universal conversational agent has been a long-standing goal of\nthe dialogue research community. Most previous works only focus on a small set\nof dialogue tasks. In this work, we aim to build a unified dialogue foundation\nmodel (DFM) which can be used to solve massive diverse dialogue tasks. To\nachieve this goal, a large-scale well-annotated dialogue dataset with rich task\ndiversity (DialogZoo) is collected. We introduce a framework to unify all\ndialogue tasks and propose novel auxiliary self-supervised tasks to achieve\nstable training of DFM on the highly diverse large scale DialogZoo corpus.\nExperiments show that, compared with models of the same size, DFM can achieve\nstate-of-the-art or competitive performance on very rich cross-domain\ndownstream dialogue tasks. This demonstrates that DFM largely extends the\nability of unified dialogue pre-trained model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jijia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuncong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Da Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Su Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_F/0/1/0/all/0/1\">Fujiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Q/0/1/0/all/0/1\">Qingliang Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering. (arXiv:2206.01201v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01201","description":"<p>This paper revisits visual representation in knowledge-based visual question\nanswering (VQA) and demonstrates that using regional information in a better\nway can significantly improve the performance. While visual representation is\nextensively studied in traditional VQA, it is under-explored in knowledge-based\nVQA even though these two tasks share the common spirit, i.e., rely on visual\ninput to answer the question. Specifically, we observe that in most\nstate-of-the-art knowledge-based VQA methods: 1) visual features are extracted\neither from the whole image or in a sliding window manner for retrieving\nknowledge, and the important relationship within/among object regions is\nneglected; 2) visual features are not well utilized in the final answering\nmodel, which is counter-intuitive to some extent. Based on these observations,\nwe propose a new knowledge-based VQA method REVIVE, which tries to utilize the\nexplicit information of object regions not only in the knowledge retrieval\nstage but also in the answering model. The key motivation is that object\nregions and inherent relationship are important for knowledge-based VQA. We\nperform extensive experiments on the standard OK-VQA dataset and achieve new\nstate-of-the-art performance, i.e., 58.0% accuracy, surpassing previous\nstate-of-the-art method by a large margin (+3.6%). We also conduct detailed\nanalysis and show the necessity of regional information in different framework\ncomponents for knowledge-based VQA. Code is publicly available at\nhttps://github.com/yzleroy/REVIVE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech. (arXiv:2206.02147v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2206.02147","description":"<p>Polyphone disambiguation aims to capture accurate pronunciation knowledge\nfrom natural text sequences for reliable Text-to-speech (TTS) systems. However,\nprevious approaches require substantial annotated training data and additional\nefforts from language experts, making it difficult to extend high-quality\nneural TTS systems to out-of-domain daily conversations and countless languages\nworldwide. This paper tackles the polyphone disambiguation problem from a\nconcise and novel perspective: we propose Dict-TTS, a semantic-aware generative\ntext-to-speech model with an online website dictionary (the existing prior\ninformation in the natural language). Specifically, we design a\nsemantics-to-pronunciation attention (S2PA) module to match the semantic\npatterns between the input text sequence and the prior semantics in the\ndictionary and obtain the corresponding pronunciations; The S2PA module can be\neasily trained with the end-to-end TTS model without any annotated phoneme\nlabels. Experimental results in three languages show that our model outperforms\nseveral strong baseline models in terms of pronunciation accuracy and improves\nthe prosody modeling of TTS systems. Further extensive analyses demonstrate\nthat each design in Dict-TTS is effective. The code is available at\n\\url{https://github.com/Zain-Jiang/Dict-TTS}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1\">Ziyue Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhe_S/0/1/0/all/0/1\">Su Zhe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Z/0/1/0/all/0/1\">Zhenhui Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation. (arXiv:2206.02369v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.02369","description":"<p>While large-scale neural language models, such as GPT2 and BART, have\nachieved impressive results on various text generation tasks, they tend to get\nstuck in undesirable sentence-level loops with maximization-based decoding\nalgorithms (\\textit{e.g.}, greedy search). This phenomenon is counter-intuitive\nsince there are few consecutive sentence-level repetitions in human corpora\n(e.g., 0.02\\% in Wikitext-103). To investigate the underlying reasons for\ngenerating consecutive sentence-level repetitions, we study the relationship\nbetween the probabilities of the repetitive tokens and their previous\nrepetitions in the context. Through our quantitative experiments, we find that\n1) Language models have a preference to repeat the previous sentence; 2) The\nsentence-level repetitions have a \\textit{self-reinforcement effect}: the more\ntimes a sentence is repeated in the context, the higher the probability of\ncontinuing to generate that sentence; 3) The sentences with higher initial\nprobabilities usually have a stronger self-reinforcement effect. Motivated by\nour findings, we propose a simple and effective training method \\textbf{DITTO}\n(Pseu\\underline{D}o-Repet\\underline{IT}ion\nPenaliza\\underline{T}i\\underline{O}n), where the model learns to penalize\nprobabilities of sentence-level repetitions from pseudo repetitive data.\nAlthough our method is motivated by mitigating repetitions, experiments show\nthat DITTO not only mitigates the repetition issue without sacrificing\nperplexity, but also achieves better generation quality. Extensive experiments\non open-ended text generation (Wikitext-103) and text summarization\n(CNN/DailyMail) demonstrate the generality and effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaojiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HYCEDIS: HYbrid Confidence Engine for Deep Document Intelligence System. (arXiv:2206.02628v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2206.02628","description":"<p>Measuring the confidence of AI models is critical for safely deploying AI in\nreal-world industrial systems. One important application of confidence\nmeasurement is information extraction from scanned documents. However, there\nexists no solution to provide reliable confidence score for current\nstate-of-the-art deep-learning-based information extractors. In this paper, we\npropose a complete and novel architecture to measure confidence of current deep\nlearning models in document information extraction task. Our architecture\nconsists of a Multi-modal Conformal Predictor and a Variational\nCluster-oriented Anomaly Detector, trained to faithfully estimate its\nconfidence on its outputs without the need of host models modification. We\nevaluate our architecture on real-wold datasets, not only outperforming\ncompeting confidence estimators by a huge margin but also demonstrating\ngeneralization ability to out-of-distribution data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Bao-Sinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quang-Bach Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Tuan-Anh Nguyen Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Video Question Answering via Frozen Bidirectional Language Models. (arXiv:2206.08155v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08155","description":"<p>Video question answering (VideoQA) is a complex task that requires diverse\nmulti-modal data for training. Manual annotation of question and answers for\nvideos, however, is tedious and prohibits scalability. To tackle this problem,\nrecent methods consider zero-shot settings with no manual annotation of visual\nquestion-answer. In particular, a promising approach adapts frozen\nautoregressive language models pretrained on Web-scale text-only data to\nmulti-modal inputs. In contrast, we here build on frozen bidirectional language\nmodels (BiLM) and show that such an approach provides a stronger and cheaper\nalternative for zero-shot VideoQA. In particular, (i) we combine visual inputs\nwith the frozen BiLM using light trainable modules, (ii) we train such modules\nusing Web-scraped multi-modal data, and finally (iii) we perform zero-shot\nVideoQA inference through masked language modeling, where the masked text is\nthe answer to a given question. Our proposed approach, FrozenBiLM, outperforms\nthe state of the art in zero-shot VideoQA by a significant margin on a variety\nof datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA,\nTGIF-FrameQA, How2QA and TVQA. It also demonstrates competitive performance in\nthe few-shot and fully-supervised setting. Our code and models are publicly\navailable at https://github.com/antoyang/FrozenBiLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GERNERMED++: Transfer Learning in German Medical NLP. (arXiv:2206.14504v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14504","description":"<p>We present a statistical model for German medical natural language processing\ntrained for named entity recognition (NER) as an open, publicly available\nmodel. The work serves as a refined successor to our first GERNERMED model\nwhich is substantially outperformed by our work. We demonstrate the\neffectiveness of combining multiple techniques in order to achieve strong\nresults in entity recognition performance by the means of transfer-learning on\npretrained deep language models (LM), word-alignment and neural machine\ntranslation. Due to the sparse situation on open, public medical entity\nrecognition models for German texts, this work offers benefits to the German\nresearch community on medical NLP as a baseline model. Since our model is based\non public English data, its weights are provided without legal restrictions on\nusage and distribution. The sample code and the statistical model is available\nat: https://github.com/frankkramer-lab/GERNERMED-pp\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frei_J/0/1/0/all/0/1\">Johann Frei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frei_Stuber_L/0/1/0/all/0/1\">Ludwig Frei-Stuber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kramer_F/0/1/0/all/0/1\">Frank Kramer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision. (arXiv:2206.14719v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14719","description":"<p>Clinical trials are essential for drug development but are extremely\nexpensive and time-consuming to conduct. It is beneficial to study similar\nhistorical trials when designing a clinical trial. However, lengthy trial\ndocuments and lack of labeled data make trial similarity search difficult. We\npropose a zero-shot clinical trial retrieval method, Trial2Vec, which learns\nthrough self-supervision without annotating similar clinical trials.\nSpecifically, the meta-structure of trial documents (e.g., title, eligibility\ncriteria, target disease) along with clinical knowledge (e.g., UMLS knowledge\nbase https://www.nlm.nih.gov/research/umls/index.html) are leveraged to\nautomatically generate contrastive samples. Besides, Trial2Vec encodes trial\ndocuments considering meta-structure thus producing compact embeddings\naggregating multi-aspect information from the whole document. We show that our\nmethod yields medically interpretable embeddings by visualization and it gets a\n15% average improvement over the best baselines on precision/recall for trial\nretrieval, which is evaluated on our labeled 1600 trial pairs. In addition, we\nprove the pre-trained embeddings benefit the downstream trial outcome\nprediction task over 240k trials. Software ias available at\nhttps://github.com/RyanWangZf/Trial2Vec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forecasting Future World Events with Neural Networks. (arXiv:2206.15474v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.15474","description":"<p>Forecasting future world events is a challenging but valuable task. Forecasts\nof climate, geopolitical conflict, pandemics and economic indicators help shape\npolicy and decision making. In these domains, the judgment of expert humans\ncontributes to the best forecasts. Given advances in language modeling, can\nthese forecasts be automated? To this end, we introduce Autocast, a dataset\ncontaining thousands of forecasting questions and an accompanying news corpus.\nQuestions are taken from forecasting tournaments, ensuring high quality,\nreal-world importance, and diversity. The news corpus is organized by date,\nallowing us to precisely simulate the conditions under which humans made past\nforecasts (avoiding leakage from the future). Motivated by the difficulty of\nforecasting numbers across orders of magnitude (e.g. global cases of COVID-19\nin 2022), we also curate IntervalQA, a dataset of numerical questions and\nmetrics for calibration. We test language models on our forecasting task and\nfind that performance is far below a human expert baseline. However,\nperformance improves with increased model size and incorporation of relevant\ninformation from the news corpus. In sum, Autocast poses a novel challenge for\nlarge language models and improved performance could bring large practical\nbenefits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Andy Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tristan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ryan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Joe Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1\">Mantas Mazeika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Richard Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1\">Owain Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Correspondences between word learning in children and captioning models. (arXiv:2207.09847v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.09847","description":"<p>For human children as well as machine learning systems, a key challenge in\nlearning a word is linking the word to the visual phenomena it describes. By\norganizing model output into word categories used to analyze child language\nlearning data, we show a correspondence between word learning in children and\nthe performance of image captioning models. Although captioning models are\ntrained only on standard machine learning data, we find that their performance\nin producing words from a variety of word categories correlates with the age at\nwhich children acquire words from each of those categories. To explain why this\ncorrespondence exists, we show that the performance of captioning models is\ncorrelated with human judgments of the concreteness of words, suggesting that\nthese models are capturing the complex real-world association between words and\nvisual phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rane_S/0/1/0/all/0/1\">Sunayana Rane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nencheva_M/0/1/0/all/0/1\">Mira L. Nencheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lew_Williams_C/0/1/0/all/0/1\">Casey Lew-Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Perception as a Phenomenon of Quantization. (arXiv:2208.03726v2 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2208.03726","description":"<p>For two decades, the formalism of quantum mechanics has been successfully\nused to describe human decision processes, situations of heuristic reasoning,\nand the contextuality of concepts and their combinations. The phenomenon of\n'categorical perception' has put us on track to find a possible deeper cause of\nthe presence of this quantum structure in human cognition. Thus, we show that\nin an archetype of human perception consisting of the reconciliation of a\nbottom up stimulus with a top down cognitive expectation pattern, there arises\nthe typical warping of categorical perception, where groups of stimuli clump\ntogether to form quanta, which move away from each other and lead to a\ndiscretization of a dimension. The individual concepts, which are these quanta,\ncan be modeled by a quantum prototype theory with the square of the absolute\nvalue of a corresponding Schr\\\"odinger wave function as the fuzzy prototype\nstructure, and the superposition of two such wave functions accounts for the\ninterference pattern that occurs when these concepts are combined. Using a\nsimple quantum measurement model, we analyze this archetype of human\nperception, provide an overview of the experimental evidence base for\ncategorical perception with the phenomenon of warping leading to quantization,\nand illustrate our analyses with two examples worked out in detail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Aerts_D/0/1/0/all/0/1\">Diederik Aerts</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Arguelles_J/0/1/0/all/0/1\">Jonito Aerts Argu&#xeb;lles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training Transformers on Indian Legal Text. (arXiv:2209.06049v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.06049","description":"<p>Natural Language Processing in the legal domain been benefited hugely by the\nemergence of Transformer-based Pre-trained Language Models (PLMs) pre-trained\non legal text. There exist PLMs trained over European and US legal text, most\nnotably LegalBERT. However, with the rapidly increasing volume of NLP\napplications on Indian legal documents, and the distinguishing characteristics\nof Indian legal text, it has become necessary to pre-train LMs over Indian\nlegal text as well. In this work, we introduce transformer-based PLMs\npre-trained over a large corpus of Indian legal documents. We also apply these\nPLMs over several benchmark legal NLP tasks over both Indian legal text, as\nwell as over legal text belonging to other domains (countries). The NLP tasks\nwith which we experiment include Legal Statute Identification from facts,\nSemantic segmentation of court judgements, and Court Judgement Prediction. Our\nexperiments demonstrate the utility of the India-specific PLMs developed in\nthis work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Shounak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_A/0/1/0/all/0/1\">Arpan Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Fake News Detection of Influential Domain via Domain- and Instance-Level Transfer. (arXiv:2209.08902v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.08902","description":"<p>Both real and fake news in various domains, such as politics, health, and\nentertainment are spread via online social media every day, necessitating fake\nnews detection for multiple domains. Among them, fake news in specific domains\nlike politics and health has more serious potential negative impacts on the\nreal world (e.g., the infodemic led by COVID-19 misinformation). Previous\nstudies focus on multi-domain fake news detection, by equally mining and\nmodeling the correlation between domains. However, these multi-domain methods\nsuffer from a seesaw problem: the performance of some domains is often improved\nat the cost of hurting the performance of other domains, which could lead to an\nunsatisfying performance in specific domains. To address this issue, we propose\na Domain- and Instance-level Transfer Framework for Fake News Detection\n(DITFEND), which could improve the performance of specific target domains. To\ntransfer coarse-grained domain-level knowledge, we train a general model with\ndata of all domains from the meta-learning perspective. To transfer\nfine-grained instance-level knowledge and adapt the general model to a target\ndomain, we train a language model on the target domain to evaluate the\ntransferability of each data instance in source domains and re-weigh each\ninstance's contribution. Offline experiments on two datasets demonstrate the\neffectiveness of DITFEND. Online experiments show that DITFEND brings\nadditional improvements over the base models in a real-world scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nan_Q/0/1/0/all/0/1\">Qiong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danding Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Qiang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuhui Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jintao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Whodunit? Learning to Contrast for Authorship Attribution. (arXiv:2209.11887v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.11887","description":"<p>Authorship attribution is the task of identifying the author of a given text.\nThe key is finding representations that can differentiate between authors.\nExisting approaches typically use manually designed features that capture a\ndataset's content and style, but these approaches are dataset-dependent and\nyield inconsistent performance across corpora. In this work, we propose\n\\textit{learning} author-specific representations by fine-tuning pre-trained\ngeneric language representations with a contrastive objective (Contra-X). We\nshow that Contra-X learns representations that form highly separable clusters\nfor different authors. It advances the state-of-the-art on multiple human and\nmachine authorship attribution benchmarks, enabling improvements of up to 6.8%\nover cross-entropy fine-tuning. However, we find that Contra-X improves overall\naccuracy at the cost of sacrificing performance for some authors. Resolving\nthis tension will be an important direction for future work. To the best of our\nknowledge, we are the first to integrate contrastive learning with pre-trained\nlanguage model fine-tuning for authorship attribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ai_B/0/1/0/all/0/1\">Bo Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yugin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to tackle an emerging topic? Combining strong and weak labels for Covid news NER. (arXiv:2209.15108v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.15108","description":"<p>Being able to train Named Entity Recognition (NER) models for emerging topics\nis crucial for many real-world applications especially in the medical domain\nwhere new topics are continuously evolving out of the scope of existing models\nand datasets. For a realistic evaluation setup, we introduce a novel COVID-19\nnews NER dataset (COVIDNEWS-NER) and release 3000 entries of hand annotated\nstrongly labelled sentences and 13000 auto-generated weakly labelled sentences.\nBesides the dataset, we propose CONTROSTER, a recipe to strategically combine\nweak and strong labels in improving NER in an emerging topic through transfer\nlearning. We show the effectiveness of CONTROSTER on COVIDNEWS-NER while\nproviding analysis on combining weak and strong labels for training. Our key\nfindings are: (1) Using weak data to formulate an initial backbone before\ntuning on strong data outperforms methods trained on only strong or weak data.\n(2) A combination of out-of-domain and in-domain weak label training is crucial\nand can overcome saturation when being training on weak labels from a single\nsource.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ficek_A/0/1/0/all/0/1\">Aleksander Ficek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALT: A software for readability analysis of Portuguese-language texts. (arXiv:2210.00553v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.00553","description":"<p>In the initial stage of human life, communication, seen as a process of\nsocial interaction, was always the best way to reach consensus between the\nparties. Understanding and credibility in this process are essential for the\nmutual agreement to be validated. But, how to do it so that this communication\nreaches the great mass? This is the main challenge when what is sought is the\ndissemination of information and its approval. In this context, this study\npresents the ALT software, developed from original readability metrics adapted\nto the Portuguese language, available on the web, to reduce communication\ndifficulties. The development of the software was motivated by the theory of\ncommunicative action of Habermas, which uses a multidisciplinary style to\nmeasure the credibility of the discourse in the communication channels used to\nbuild and maintain a safe and healthy relationship with the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreno_G/0/1/0/all/0/1\">Gleice Carvalho de Lima Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souza_M/0/1/0/all/0/1\">Marco P. M. de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_N/0/1/0/all/0/1\">Nelson Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_A/0/1/0/all/0/1\">Adriana Kroenke Hein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Language with Visual Affordances over Unstructured Data. (arXiv:2210.01911v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2210.01911","description":"<p>Recent works have shown that Large Language Models (LLMs) can be applied to\nground natural language to a wide variety of robot skills. However, in\npractice, learning multi-task, language-conditioned robotic skills typically\nrequires large-scale data collection and frequent human intervention to reset\nthe environment or help correcting the current policies. In this work, we\npropose a novel approach to efficiently learn general-purpose\nlanguage-conditioned robot skills from unstructured, offline and reset-free\ndata in the real world by exploiting a self-supervised visuo-lingual affordance\nmodel, which requires annotating as little as 1% of the total data with\nlanguage. We evaluate our method in extensive experiments both in simulated and\nreal-world robotic tasks, achieving state-of-the-art performance on the\nchallenging CALVIN benchmark and learning over 25 distinct visuomotor\nmanipulation tasks with a single policy in the real world. We find that when\npaired with LLMs to break down abstract natural language instructions into\nsubgoals via few-shot prompting, our method is capable of completing\nlong-horizon, multi-tier tasks in the real world, while requiring an order of\nmagnitude less data than previous approaches. Code and videos are available at\n<a href=\"http://hulc2.cs.uni-freiburg.de\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mees_O/0/1/0/all/0/1\">Oier Mees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borja_Diaz_J/0/1/0/all/0/1\">Jessica Borja-Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grape: Knowledge Graph Enhanced Passage Reader for Open-domain Question Answering. (arXiv:2210.02933v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02933","description":"<p>A common thread of open-domain question answering (QA) models employs a\nretriever-reader pipeline that first retrieves a handful of relevant passages\nfrom Wikipedia and then peruses the passages to produce an answer. However,\neven state-of-the-art readers fail to capture the complex relationships between\nentities appearing in questions and retrieved passages, leading to answers that\ncontradict the facts. In light of this, we propose a novel knowledge Graph\nenhanced passage reader, namely Grape, to improve the reader performance for\nopen-domain QA. Specifically, for each pair of question and retrieved passage,\nwe first construct a localized bipartite graph, attributed to entity embeddings\nextracted from the intermediate layer of the reader model. Then, a graph neural\nnetwork learns relational knowledge while fusing graph and contextual\nrepresentations into the hidden states of the reader model. Experiments on\nthree open-domain QA benchmarks show Grape can improve the state-of-the-art\nperformance by up to 2.2 exact match score with a negligible overhead increase,\nwith the same retriever and retrieved passages. Our code is publicly available\nat https://github.com/jumxglhf/GRAPE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_M/0/1/0/all/0/1\">Mingxuan Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuxu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yanfang Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"State-of-the-art generalisation research in NLP: a taxonomy and review. (arXiv:2210.03050v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03050","description":"<p>The ability to generalise well is one of the primary desiderata of natural\nlanguage processing (NLP). Yet, what `good generalisation' entails and how it\nshould be evaluated is not well understood, nor are there any common standards\nto evaluate it. In this paper, we aim to lay the ground-work to improve both of\nthese issues. We present a taxonomy for characterising and understanding\ngeneralisation research in NLP, we use that taxonomy to present a comprehensive\nmap of published generalisation studies, and we make recommendations for which\nareas might deserve attention in the future. Our taxonomy is based on an\nextensive literature review of generalisation research, and contains five axes\nalong which studies can differ: their main motivation, the type of\ngeneralisation they aim to solve, the type of data shift they consider, the\nsource by which this data shift is obtained, and the locus of the shift within\nthe modelling pipeline. We use our taxonomy to classify over 400 previous\npapers that test generalisation, for a total of more than 600 individual\nexperiments. Considering the results of this review, we present an in-depth\nanalysis of the current state of generalisation research in NLP, and make\nrecommendations for the future. Along with this paper, we release a webpage\nwhere the results of our review can be dynamically explored, and which we\nintend to up-date as new NLP generalisation studies are published. With this\nwork, we aim to make steps towards making state-of-the-art generalisation\ntesting the new status quo in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giulianelli_M/0/1/0/all/0/1\">Mario Giulianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dankers_V/0/1/0/all/0/1\">Verna Dankers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christodoulopoulos_C/0/1/0/all/0/1\">Christos Christodoulopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasri_K/0/1/0/all/0/1\">Karim Lasri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinclair_A/0/1/0/all/0/1\">Arabella Sinclair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulmer_D/0/1/0/all/0/1\">Dennis Ulmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schottmann_F/0/1/0/all/0/1\">Florian Schottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batsuren_K/0/1/0/all/0/1\">Khuyagbaatar Batsuren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kaiser Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1\">Koustuv Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalatbari_L/0/1/0/all/0/1\">Leila Khalatbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryskina_M/0/1/0/all/0/1\">Maria Ryskina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Extraction: A Survey. (arXiv:2210.03419v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03419","description":"<p>Extracting the reported events from text is one of the key research themes in\nnatural language processing. This process includes several tasks such as event\ndetection, argument extraction, role labeling. As one of the most important\ntopics in natural language processing and natural language understanding, the\napplications of event extraction spans across a wide range of domains such as\nnewswire, biomedical domain, history and humanity, and cyber security. This\nreport presents a comprehensive survey for event detection from textual\ndocuments. In this report, we provide the task definition, the evaluation\nmethod, as well as the benchmark datasets and a taxonomy of methodologies for\nevent extraction. We also present our vision of future research direction in\nevent detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1\">Viet Dac Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}