{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"CAB: Empathetic Dialogue Generation with Cognition, Affection and Behavior. (arXiv:2302.01935v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01935","description":"<p>Empathy is an important characteristic to be considered when building a more\nintelligent and humanized dialogue agent. However, existing methods did not\nfully comprehend empathy as a complex process involving three aspects:\ncognition, affection and behavior. In this paper, we propose CAB, a novel\nframework that takes a comprehensive perspective of cognition, affection and\nbehavior to generate empathetic responses. For cognition, we build paths\nbetween critical keywords in the dialogue by leveraging external knowledge.\nThis is because keywords in a dialogue are the core of sentences. Building the\nlogic relationship between keywords, which is overlooked by the majority of\nexisting works, can improve the understanding of keywords and contextual logic,\nthus enhance the cognitive ability. For affection, we capture the emotional\ndependencies with dual latent variables that contain both interlocutors'\nemotions. The reason is that considering both interlocutors' emotions\nsimultaneously helps to learn the emotional dependencies. For behavior, we use\nappropriate dialogue acts to guide the dialogue generation to enhance the\nempathy expression. Extensive experiments demonstrate that our\nmulti-perspective model outperforms the state-of-the-art models in both\nautomatic and manual evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Donghong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Rui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zikun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring The Impact Of Programming Language Distribution. (arXiv:2302.01973v1 [cs.LG])","link":"http://arxiv.org/abs/2302.01973","description":"<p>Current benchmarks for evaluating neural code models focus on only a small\nsubset of programming languages, excluding many popular languages such as Go or\nRust. To ameliorate this issue, we present the BabelCode framework for\nexecution-based evaluation of any benchmark in any language. BabelCode enables\nnew investigations into the qualitative performance of models' memory, runtime,\nand individual test case results. Additionally, we present a new code\ntranslation dataset called Translating Python Programming Puzzles (TP3) from\nthe Python Programming Puzzles (Schuster et al. 2021) benchmark that involves\ntranslating expert-level python functions to any language. With both BabelCode\nand the TP3 benchmark, we investigate if balancing the distributions of 14\nlanguages in a training dataset improves a large language model's performance\non low-resource languages. Training a model on a balanced corpus results in, on\naverage, 12.34% higher $pass@k$ across all tasks and languages compared to the\nbaseline. We find that this strategy achieves 66.48% better $pass@k$ on\nlow-resource languages at the cost of only a 12.94% decrease to high-resource\nlanguages. In our three translation tasks, this strategy yields, on average,\n30.77% better low-resource $pass@k$ while having 19.58% worse high-resource\n$pass@k$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orlanski_G/0/1/0/all/0/1\">Gabriel Orlanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kefan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_J/0/1/0/all/0/1\">Jeffrey Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howland_J/0/1/0/all/0/1\">Joshua Howland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malmaud_J/0/1/0/all/0/1\">Jonathan Malmaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rishah Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catasta_M/0/1/0/all/0/1\">Michele Catasta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSST! Prosodic Speech Segmentation with Transformers. (arXiv:2302.01984v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01984","description":"<p>Self-attention mechanisms have enabled transformers to achieve\nsuperhuman-level performance on many speech-to-text (STT) tasks, yet the\nchallenge of automatic prosodic segmentation has remained unsolved. In this\npaper we finetune Whisper, a pretrained STT model, to annotate intonation unit\n(IU) boundaries by repurposing low-frequency tokens. Our approach achieves an\naccuracy of 95.8%, outperforming previous methods without the need for\nlarge-scale labeled data or enterprise grade compute resources. We also\ndiminish input signals by applying a series of filters, finding that low pass\nfilters at a 3.2 kHz level improve segmentation performance in out of sample\nand out of distribution contexts. We release our model as both a transcription\ntool and a baseline for further improvements in prosodic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roll_N/0/1/0/all/0/1\">Nathan Roll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_C/0/1/0/all/0/1\">Calbert Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todd_S/0/1/0/all/0/1\">Simon Todd</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Witscript: A System for Generating Improvised Jokes in a Conversation. (arXiv:2302.02008v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02008","description":"<p>A chatbot is perceived as more humanlike and likeable if it includes some\njokes in its output. But most existing joke generators were not designed to be\nintegrated into chatbots. This paper presents Witscript, a novel joke\ngeneration system that can improvise original, contextually relevant jokes,\nsuch as humorous responses during a conversation. The system is based on joke\nwriting algorithms created by an expert comedy writer. Witscript employs\nwell-known tools of natural language processing to extract keywords from a\ntopic sentence and, using wordplay, to link those keywords and related words to\ncreate a punch line. Then a pretrained neural network language model that has\nbeen fine-tuned on a dataset of TV show monologue jokes is used to complete the\njoke response by filling the gap between the topic sentence and the punch line.\nA method of internal scoring filters out jokes that don't meet a preset\nstandard of quality. Human evaluators judged Witscript's responses to input\nsentences to be jokes more than 40% of the time. This is evidence that\nWitscript represents an important next step toward giving a chatbot a humanlike\nsense of humor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toplyn_J/0/1/0/all/0/1\">Joe Toplyn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Interpretability via Explicit Word Interaction Graph Layer. (arXiv:2302.02016v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02016","description":"<p>Recent NLP literature has seen growing interest in improving model\ninterpretability. Along this direction, we propose a trainable neural network\nlayer that learns a global interaction graph between words and then selects\nmore informative words using the learned word interactions. Our layer, we call\nWIGRAPH, can plug into any neural network-based NLP text classifiers right\nafter its word embedding layer. Across multiple SOTA NLP models and various NLP\ndatasets, we demonstrate that adding the WIGRAPH layer substantially improves\nNLP models' interpretability and enhances models' prediction performance at the\nsame time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1\">Arshdeep Sekhon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Aman Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextShield: Beyond Successfully Detecting Adversarial Sentences in Text Classification. (arXiv:2302.02023v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02023","description":"<p>Adversarial attack serves as a major challenge for neural network models in\nNLP, which precludes the model's deployment in safety-critical applications. A\nrecent line of work, detection-based defense, aims to distinguish adversarial\nsentences from benign ones. However, {the core limitation of previous detection\nmethods is being incapable of giving correct predictions on adversarial\nsentences unlike defense methods from other paradigms.} To solve this issue,\nthis paper proposes TextShield: (1) we discover a link between text attack and\nsaliency information, and then we propose a saliency-based detector, which can\neffectively detect whether an input sentence is adversarial or not. (2) We\ndesign a saliency-based corrector, which converts the detected adversary\nsentences to benign ones. By combining the saliency-based detector and\ncorrector, TextShield extends the detection-only paradigm to a\ndetection-correction paradigm, thus filling the gap in the existing\ndetection-based defense. Comprehensive experiments show that (a) TextShield\nconsistently achieves higher or comparable performance than state-of-the-art\ndefense methods across various attacks on different benchmarks. (b) our\nsaliency-based detector outperforms existing detectors for detecting\nadversarial sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lingfeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ze Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Few-Shot Identification of Morality Frames using In-Context Learning. (arXiv:2302.02029v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02029","description":"<p>Data scarcity is a common problem in NLP, especially when the annotation\npertains to nuanced socio-linguistic concepts that require specialized\nknowledge. As a result, few-shot identification of these concepts is desirable.\nFew-shot in-context learning using pre-trained Large Language Models (LLMs) has\nbeen recently applied successfully in many NLP tasks. In this paper, we study\nfew-shot identification of a psycho-linguistic concept, Morality Frames (Roy et\nal., 2021), using LLMs. Morality frames are a representation framework that\nprovides a holistic view of the moral sentiment expressed in text, identifying\nthe relevant moral foundation (Haidt and Graham, 2007) and at a finer level of\ngranularity, the moral sentiment expressed towards the entities mentioned in\nthe text. Previous studies relied on human annotation to identify morality\nframes in text which is expensive. In this paper, we propose prompting-based\napproaches using pretrained Large Language Models for identification of\nmorality frames, relying only on few-shot exemplars. We compare our models'\nperformance with few-shot RoBERTa and found promising results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shamik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakshatri_N/0/1/0/all/0/1\">Nishanth Sridhar Nakshatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Deficiency in Masked Language Modeling. (arXiv:2302.02060v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02060","description":"<p>Masked Language Modeling (MLM) has been one of the most prominent approaches\nfor pretraining bidirectional text encoders due to its simplicity and\neffectiveness. One notable concern about MLM is that the special\n$\\texttt{[MASK]}$ symbol causes a discrepancy between pretraining data and\ndownstream data as it is present only in pretraining but not in fine-tuning. In\nthis work, we offer a new perspective on the consequence of such a discrepancy:\nWe demonstrate empirically and theoretically that MLM pretraining allocates\nsome model dimensions exclusively for representing $\\texttt{[MASK]}$ tokens,\nresulting in a representation deficiency for real tokens and limiting the\npretrained model's expressiveness when it is adapted to downstream data without\n$\\texttt{[MASK]}$ tokens. Motivated by the identified issue, we propose MAE-LM,\nwhich pretrains the Masked Autoencoder architecture with MLM where\n$\\texttt{[MASK]}$ tokens are excluded from the encoder. Empirically, we show\nthat MAE-LM improves the utilization of model dimensions for real token\nrepresentations, and MAE-LM consistently outperforms MLM-pretrained models\nacross different pretraining settings and model sizes when fine-tuned on the\nGLUE and SQuAD benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_J/0/1/0/all/0/1\">Jitin Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sinong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1\">Marjan Ghazvininejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lived Experience Matters: Automatic Detection of Stigma toward People Who Use Substances on Social Media. (arXiv:2302.02064v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02064","description":"<p>Stigma toward people who use substances (PWUS) is a leading barrier to\nseeking treatment. Further, those in treatment are more likely to drop out if\nthey experience higher levels of stigmatization. While related concepts of hate\nspeech and toxicity, including those targeted toward vulnerable populations,\nhave been the focus of automatic content moderation research, stigma and, in\nparticular, people who use substances have not. This paper explores stigma\ntoward PWUS using a data set of roughly 5,000 public Reddit posts. We performed\na crowd-sourced annotation task where workers are asked to annotate each post\nfor the presence of stigma toward PWUS and answer a series of questions related\nto their experiences with substance use. Results show that workers who use\nsubstances or know someone with a substance use disorder are more likely to\nrate a post as stigmatizing. Building on this, we use a supervised machine\nlearning framework that centers workers with lived substance use experience to\nlabel each Reddit post as stigmatizing. Modeling person-level demographics in\naddition to comment-level language results in a classification accuracy (as\nmeasured by AUC) of 0.69 -- a 17% increase over modeling language alone.\nFinally, we explore the linguist cues which distinguish stigmatizing content:\nPWUS substances and those who don't agree that language around othering\n(\"people\", \"they\") and terms like \"addict\" are stigmatizing, while PWUS (as\nopposed to those who do not) find discussions around specific substances more\nstigmatizing. Our findings offer insights into the nature of perceived stigma\nin substance use. Additionally, these results further establish the subjective\nnature of such machine learning tasks, highlighting the need for understanding\ntheir social contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_S/0/1/0/all/0/1\">Salvatore Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellew_D/0/1/0/all/0/1\">Douglas Bellew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habib_D/0/1/0/all/0/1\">Daniel Roy Sadek Habib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Joao Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smitterberg_C/0/1/0/all/0/1\">Chase Smitterberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devoto_A/0/1/0/all/0/1\">Amanda Devoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Himelein_Wachowiak_M/0/1/0/all/0/1\">McKenzie Himelein-Wachowiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curtis_B/0/1/0/all/0/1\">Brenda Curtis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heterogeneous Federated Knowledge Graph Embedding Learning and Unlearning. (arXiv:2302.02069v1 [cs.LG])","link":"http://arxiv.org/abs/2302.02069","description":"<p>Federated Learning (FL) recently emerges as a paradigm to train a global\nmachine learning model across distributed clients without sharing raw data.\nKnowledge Graph (KG) embedding represents KGs in a continuous vector space,\nserving as the backbone of many knowledge-driven applications. As a promising\ncombination, federated KG embedding can fully take advantage of knowledge\nlearned from different clients while preserving the privacy of local data.\nHowever, realistic problems such as data heterogeneity and knowledge forgetting\nstill remain to be concerned. In this paper, we propose FedLU, a novel FL\nframework for heterogeneous KG embedding learning and unlearning. To cope with\nthe drift between local optimization and global convergence caused by data\nheterogeneity, we propose mutual knowledge distillation to transfer local\nknowledge to global, and absorb global knowledge back. Moreover, we present an\nunlearning method based on cognitive neuroscience, which combines retroactive\ninterference and passive decay to erase specific knowledge from local clients\nand propagate to the global model by reusing knowledge distillation. We\nconstruct new datasets for assessing realistic performance of the\nstate-of-the-arts. Extensive experiments show that FedLU achieves superior\nresults in both link prediction and knowledge forgetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiangrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guangyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FGSI: Distant Supervision for Relation Extraction method based on Fine-Grained Semantic Information. (arXiv:2302.02078v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02078","description":"<p>The main purpose of relation extraction is to extract the semantic\nrelationships between tagged pairs of entities in a sentence, which plays an\nimportant role in the semantic understanding of sentences and the construction\nof knowledge graphs. In this paper, we propose that the key semantic\ninformation within a sentence plays a key role in the relationship extraction\nof entities. We propose the hypothesis that the key semantic information inside\nthe sentence plays a key role in entity relationship extraction. And based on\nthis hypothesis, we split the sentence into three segments according to the\nlocation of the entity from the inside of the sentence, and find the\nfine-grained semantic features inside the sentence through the intra-sentence\nattention mechanism to reduce the interference of irrelevant noise information.\nThe proposed relational extraction model can make full use of the available\npositive semantic information. The experimental results show that the proposed\nrelation extraction model improves the accuracy-recall curves and P@N values\ncompared with existing methods, which proves the effectiveness of this model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenghong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Weidong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guohui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zengxiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yuqi Yue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Prediction Backward-Compatiblility in NLP Model Upgrade with Gated Fusion. (arXiv:2302.02080v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02080","description":"<p>When upgrading neural models to a newer version, new errors that were not\nencountered in the legacy version can be introduced, known as regression\nerrors. This inconsistent behavior during model upgrade often outweighs the\nbenefits of accuracy gain and hinders the adoption of new models. To mitigate\nregression errors from model upgrade, distillation and ensemble have proven to\nbe viable solutions without significant compromise in performance. Despite the\nprogress, these approaches attained an incremental reduction in regression\nwhich is still far from achieving backward-compatible model upgrade. In this\nwork, we propose a novel method, Gated Fusion, that promotes backward\ncompatibility via learning to mix predictions between old and new models.\nEmpirical results on two distinct model upgrade scenarios show that our method\nreduces the number of regression errors by 62% on average, outperforming the\nstrongest baseline by an average of 25%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yi-An Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1\">Elman Mansimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Theory of Mind May Have Spontaneously Emerged in Large Language Models. (arXiv:2302.02083v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02083","description":"<p>Theory of mind (ToM), or the ability to impute unobservable mental states to\nothers, is central to human social interactions, communication, empathy,\nself-consciousness, and morality. We administer classic false-belief tasks,\nwidely used to test ToM in humans, to several language models, without any\nexamples or pre-training. Our results show that models published before 2022\nshow virtually no ability to solve ToM tasks. Yet, the January 2022 version of\nGPT-3 (davinci-002) solved 70% of ToM tasks, a performance comparable with that\nof seven-year-old children. Moreover, its November 2022 version (davinci-003),\nsolved 93% of ToM tasks, a performance comparable with that of nine-year-old\nchildren. These findings suggest that ToM-like ability (thus far considered to\nbe uniquely human) may have spontaneously emerged as a byproduct of language\nmodels' improving language skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kosinski_M/0/1/0/all/0/1\">Michal Kosinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Completion Method Combined With Adaptive Enhanced Semantic Information. (arXiv:2302.02116v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02116","description":"<p>Translation models tend to ignore the rich semantic information in triads in\nthe process of knowledge graph complementation. To remedy this shortcoming,\nthis paper constructs a knowledge graph complementation method that\nincorporates adaptively enhanced semantic information. The hidden semantic\ninformation inherent in the triad is obtained by fine-tuning the BERT model,\nand the attention feature embedding method is used to calculate the semantic\nattention scores between relations and entities in positive and negative triads\nand incorporate them into the structural information to form a soft constraint\nrule for semantic information. The rule is added to the original translation\nmodel to realize the adaptive enhancement of semantic information. In addition,\nthe method takes into account the effect of high-dimensional vectors on the\neffect, and uses the BERT-whitening method to reduce the dimensionality and\ngenerate a more efficient semantic vector representation. After experimental\ncomparison, the proposed method performs better on both FB15K and WIN18\ndatasets, with a numerical improvement of about 2.6% compared with the original\ntranslation model, which verifies the reasonableness and effectiveness of the\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Weidong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zengxiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guohui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yuqi Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenghong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New cross-domain strategy based XAI models for fake news detection. (arXiv:2302.02122v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02122","description":"<p>In this study, we presented a four-level cross-domain strategy for fake news\ndetection on pre-trained models. Cross-domain text classification is a task of\na model adopting a target domain by using the knowledge of the source domain.\nExplainability is crucial in understanding the behaviour of these complex\nmodels. A fine-tune BERT model is used to. perform cross-domain classification\nwith several experiments using datasets from different domains. Explanatory\nmodels like Anchor, ELI5, LIME and SHAP are used to design a novel explainable\napproach to cross-domain levels. The experimental analysis has given an ideal\npair of XAI models on different levels of cross-domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanneganti_D/0/1/0/all/0/1\">Deepak Kanneganti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weight, Is Attention All We Need? AEIUOrder: Greedy Ordering of Layer Weight Matrices in Transformer Improves Translation. (arXiv:2302.02123v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02123","description":"<p>Prior work has attempted to understand the internal structures and\nfunctionalities of Transformer-based encoder-decoder architectures on the level\nof multi-head attention and feed-forward sublayers. Interpretations have\nfocused on the encoder and decoder, along with the combinatorial possibilities\nof the self-attention, cross-attention, and feed-forward sublayers. Could we\nimprove the quality of translation by diving into the Transformer sublayer\nabstractions and permuting its layer weight matrices? We propose AEIUOrder to\ngreedily reorder layer weight matrices in the encoder by their\nwell-trainedness, as measured by Random Matrix Theory (RMT) metrics, and\nreverse the ordering scheme for the encoder. The objective is to maximize Total\nwell-trainedness in the encoder while the decoder structure serves to represent\nthe reverse process of encoding. On the standard Transformer (6 layers, model\ndimension 512), AEIUOrder achieves a BLEU score of 34.62 (baseline 34.31) on\nthe IWSLT 2016 German-to-English translation task, and 27.95 BLEU on the WMT\n2014 English-to-German translation task (baseline 27.91). AEIUOrder is also\nrealized on Transformers with various depths and embedding dimensions, showing\nsignificant improvements on deeper, wider models than on their shallower,\nslimmer counterparts. For instance, the 8-layer, 768-dimension and the 4-layer,\n1024-dimension Transformers achieve respective 29.1 and 29.31 BLEU scores on\nthe IWSLT 2016 English-to-German translation task (28.53 and 28.97 on\nrespective baselines). Our results suggest that the RMT-motivated approach to\nmaximize \\textit{Total well-trainedness}, by greedily reordering its layer\nweight matrices, facilitates the model to learn representations and generate\ntranslations more effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_E/0/1/0/all/0/1\">Elicia Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interaction Order Prediction for Temporal Graphs. (arXiv:2302.02128v1 [cs.SI])","link":"http://arxiv.org/abs/2302.02128","description":"<p>Link prediction in graphs is a task that has been widely investigated. It has\nbeen applied in various domains such as knowledge graph completion,\ncontent/item recommendation, social network recommendations and so on. The\ninitial focus of most research was on link prediction in static graphs.\nHowever, there has recently been abundant work on modeling temporal graphs, and\nconsequently one of the tasks that has been researched is link prediction in\ntemporal graphs. However, most of the existing work does not focus on the order\nof link formation, and only predicts the existence of links. In this study, we\naim to predict the order of node interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bannur_N/0/1/0/all/0/1\">Nayana Bannur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1\">Mashrin Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardhan_H/0/1/0/all/0/1\">Harsha Vardhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LipFormer: Learning to Lipread Unseen Speakers based on Visual-Landmark Transformers. (arXiv:2302.02141v1 [cs.CV])","link":"http://arxiv.org/abs/2302.02141","description":"<p>Lipreading refers to understanding and further translating the speech of a\nspeaker in the video into natural language. State-of-the-art lipreading methods\nexcel in interpreting overlap speakers, i.e., speakers appear in both training\nand inference sets. However, generalizing these methods to unseen speakers\nincurs catastrophic performance degradation due to the limited number of\nspeakers in training bank and the evident visual variations caused by the\nshape/color of lips for different speakers. Therefore, merely depending on the\nvisible changes of lips tends to cause model overfitting. To address this\nproblem, we propose to use multi-modal features across visual and landmarks,\nwhich can describe the lip motion irrespective to the speaker identities. Then,\nwe develop a sentence-level lipreading framework based on visual-landmark\ntransformers, namely LipFormer. Specifically, LipFormer consists of a lip\nmotion stream, a facial landmark stream, and a cross-modal fusion. The\nembeddings from the two streams are produced by self-attention, which are fed\nto the cross-attention module to achieve the alignment between visuals and\nlandmarks. Finally, the resulting fused features can be decoded to output texts\nby a cascade seq2seq model. Experiments demonstrate that our method can\neffectively enhance the model generalization to unseen speakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Feng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Deyin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yincen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Richang Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invariants for neural automata. (arXiv:2302.02149v1 [cs.NE])","link":"http://arxiv.org/abs/2302.02149","description":"<p>Computational modeling of neurodynamical systems often deploys neural\nnetworks and symbolic dynamics. A particular way for combining these approaches\nwithin a framework called vector symbolic architectures leads to neural\nautomata. An interesting research direction we have pursued under this\nframework has been to consider mapping symbolic dynamics onto neurodynamics,\nrepresented as neural automata. This representation theory, enables us to ask\nquestions, such as, how does the brain implement Turing computations.\nSpecifically, in this representation theory, neural automata result from the\nassignment of symbols and symbol strings to numbers, known as G\\\"odel encoding.\nUnder this assignment symbolic computation becomes represented by trajectories\nof state vectors in a real phase space, that allows for statistical correlation\nanalyses with real-world measurements and experimental data. However, these\nassignments are usually completely arbitrary. Hence, it makes sense to address\nthe problem question of, which aspects of the dynamics observed under such a\nrepresentation is intrinsic to the dynamics and which are not. In this study,\nwe develop a formally rigorous mathematical framework for the investigation of\nsymmetries and invariants of neural automata under different encodings. As a\ncentral concept we define patterns of equality for such systems. We consider\ndifferent macroscopic observables, such as the mean activation level of the\nneural network, and ask for their invariance properties. Our main result shows\nthat only step functions that are defined over those patterns of equality are\ninvariant under recodings, while the mean activation is not. Our work could be\nof substantial importance for related regression studies of real-world\nmeasurements with neurosymbolic processors for avoiding confounding results\nthat are dependant on a particular encoding and not intrinsic to the dynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uria_Albizuri_J/0/1/0/all/0/1\">Jone Uria-Albizuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmantini_G/0/1/0/all/0/1\">Giovanni Sirio Carmantini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graben_P/0/1/0/all/0/1\">Peter beim Graben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_S/0/1/0/all/0/1\">Serafim Rodrigues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Many and Which Training Points Would Need to be Removed to Flip this Prediction?. (arXiv:2302.02169v1 [cs.LG])","link":"http://arxiv.org/abs/2302.02169","description":"<p>We consider the problem of identifying a minimal subset of training data\n$\\mathcal{S}_t$ such that if the instances comprising $\\mathcal{S}_t$ had been\nremoved prior to training, the categorization of a given test point $x_t$ would\nhave been different. Identifying such a set may be of interest for a few\nreasons. First, the cardinality of $\\mathcal{S}_t$ provides a measure of\nrobustness (if $|\\mathcal{S}_t|$ is small for $x_t$, we might be less confident\nin the corresponding prediction), which we show is correlated with but\ncomplementary to predicted probabilities. Second, interrogation of\n$\\mathcal{S}_t$ may provide a novel mechanism for contesting a particular model\nprediction: If one can make the case that the points in $\\mathcal{S}_t$ are\nwrongly labeled or irrelevant, this may argue for overturning the associated\nprediction. Identifying $\\mathcal{S}_t$ via brute-force is intractable. We\npropose comparatively fast approximation methods to find $\\mathcal{S}_t$ based\non influence functions, and find that -- for simple convex text classification\nmodels -- these approaches can often successfully identify relatively small\nsets of training examples which, if removed, would flip the prediction. To our\nknowledge, this is the first work in to investigate the problem of identifying\na minimal training set necessary to flip a given prediction in the context of\nmachine learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinghan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sarthak Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction Grammar Provides Unique Insight into Neural Language Models. (arXiv:2302.02178v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02178","description":"<p>Construction Grammar (CxG) has recently been used as the basis for probing\nstudies that have investigated the performance of large pretrained language\nmodels (PLMs) with respect to the structure and meaning of constructions. In\nthis position paper, we make suggestions for the continuation and augmentation\nof this line of research. We look at probing methodology that was not designed\nwith CxG in mind, as well as probing methodology that was designed for specific\nconstructions. We analyse selected previous work in detail, and provide our\nview of the most important challenges and research questions that this\npromising new field faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weissweiler_L/0/1/0/all/0/1\">Leonie Weissweiler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Taiqi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otani_N/0/1/0/all/0/1\">Naoki Otani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David R. Mortensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levin_L/0/1/0/all/0/1\">Lori Levin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Benchmark and Scoring Algorithm for Enriching Arabic Synonyms. (arXiv:2302.02232v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02232","description":"<p>This paper addresses the task of extending a given synset with additional\nsynonyms taking into account synonymy strength as a fuzzy value. Given a\nmono/multilingual synset and a threshold (a fuzzy value [0-1]), our goal is to\nextract new synonyms above this threshold from existing lexicons. We present\ntwofold contributions: an algorithm and a benchmark dataset. The dataset\nconsists of 3K candidate synonyms for 500 synsets. Each candidate synonym is\nannotated with a fuzzy value by four linguists. The dataset is important for\n(i) understanding how much linguists (dis/)agree on synonymy, in addition to\n(ii) using the dataset as a baseline to evaluate our algorithm. Our proposed\nalgorithm extracts synonyms from existing lexicons and computes a fuzzy value\nfor each candidate. Our evaluations show that the algorithm behaves like a\nlinguist and its fuzzy values are close to those proposed by linguists (using\nRMSE and MAE). The dataset and a demo page are publicly available at\nhttps://portal.sina.birzeit.edu/synonyms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_S/0/1/0/all/0/1\">Sana Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarrar_M/0/1/0/all/0/1\">Mustafa Jarrar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarrar_R/0/1/0/all/0/1\">Radi Jarrar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bounhas_I/0/1/0/all/0/1\">Ibrahim Bounhas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing. (arXiv:2302.02275v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02275","description":"<p>Sequence-to-Sequence (S2S) models have achieved remarkable success on various\ntext generation tasks. However, learning complex structures with S2S models\nremains challenging as external neural modules and additional lexicons are\noften supplemented to predict non-textual outputs. We present a systematic\nstudy of S2S modeling using contained decoding on four core tasks:\npart-of-speech tagging, named entity recognition, constituency and dependency\nparsing, to develop efficient exploitation methods costing zero extra\nparameters. In particular, 3 lexically diverse linearization schemas and\ncorresponding constrained decoding methods are designed and evaluated.\nExperiments show that although more lexicalized schemas yield longer output\nsequences that require heavier training, their sequences being closer to\nnatural language makes them easier to learn. Moreover, S2S models using our\nconstrained decoding outperform other S2S approaches using external resources.\nOur best models perform better than or comparably to the state-of-the-art for\nall 4 tasks, lighting a promise for S2S models to generate non-sequential\nstructures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Han He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Semantic Approach to Negation Detection and Word Disambiguation with Natural Language Processing. (arXiv:2302.02291v1 [cs.CL])","link":"http://arxiv.org/abs/2302.02291","description":"<p>This study aims to demonstrate the methods for detecting negations in a\nsentence by uniquely evaluating the lexical structure of the text via word\nsense disambiguation. Additionally, the proposed method examined all the unique\nfeatures of the related expressions within a text to resolve the contextual\nusage of the sentence and the effect of negation on sentiment analysis. The\napplication of popular expression detectors skips this important step, thereby\nneglecting the root words caught in the web of negation, and making text\nclassification difficult for machine learning and sentiment analysis. This\nstudy adopts the Natural Language Processing (NLP) approach to discover and\nantonimize words that were negated for better accuracy in text classification.\nThis method acts as a lens that reads through a given word sequence using a\nknowledge base provided by an NLP library called WordHoard in order to detect\nnegation signals. Early results show that our initial analysis improved\ntraditional sentiment analysis that sometimes neglects word negations or\nassigns an inverse polarity score. The SentiWordNet analyzer was improved by\n35%, the Vader analyzer by 20% and the TextBlob analyzer by 6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Okpala_I/0/1/0/all/0/1\">Izunna Okpala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_G/0/1/0/all/0/1\">Guillermo Romera Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapia_A/0/1/0/all/0/1\">Andrea Tapia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halse_S/0/1/0/all/0/1\">Shane Halse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kropczynski_J/0/1/0/all/0/1\">Jess Kropczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v16 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1906.02358","description":"<p>Sinhala is the native language of the Sinhalese people who make up the\nlargest ethnic group of Sri Lanka. The language belongs to the globe-spanning\nlanguage tree, Indo-European. However, due to poverty in both linguistic and\neconomic capital, Sinhala, in the perspective of Natural Language Processing\ntools and research, remains a resource-poor language which has neither the\neconomic drive its cousin English has nor the sheer push of the law of numbers\na language such as Chinese has. A number of research groups from Sri Lanka have\nnoticed this dearth and the resultant dire need for proper tools and research\nfor Sinhala natural language processing. However, due to various reasons, these\nattempts seem to lack coordination and awareness of each other. The objective\nof this paper is to fill that gap of a comprehensive literature survey of the\npublicly available Sinhala natural language tools and research so that the\nresearchers working in this field can better utilize contributions of their\npeers. As such, we shall be uploading this paper to arXiv and perpetually\nupdate it periodically to reflect the advances made in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation. (arXiv:2106.05970v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.05970","description":"<p>Automatic evaluations for natural language generation (NLG) conventionally\nrely on token-level or embedding-level comparisons with text references. This\ndiffers from human language processing, for which visual imagination often\nimproves comprehension. In this work, we propose ImaginE, an imagination-based\nautomatic evaluation metric for natural language generation. With the help of\nStableDiffusion, a state-of-the-art text-to-image generator, we automatically\ngenerate an image as the embodied imagination for the text snippet and compute\nthe imagination similarity using contextual embeddings. Experiments spanning\nseveral text generation tasks demonstrate that adding machine-generated images\nwith our ImaginE displays great potential in introducing multi-modal\ninformation into NLG evaluation, and improves existing automatic metrics'\ncorrelations with human similarity judgments in both reference-based and\nreference-free evaluation scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1\">An Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepPSL: End-to-end perception and reasoning. (arXiv:2109.13662v4 [eess.SY] UPDATED)","link":"http://arxiv.org/abs/2109.13662","description":"<p>We introduce DeepPSL a variant of probabilistic soft logic (PSL) to produce\nan end-to-end trainable system that integrates reasoning and perception. PSL\nrepresents first-order logic in terms of a convex graphical model -- hinge-loss\nMarkov random fields (HL-MRFs). PSL stands out among probabilistic logic\nframeworks due to its tractability having been applied to systems of more than\n1 billion ground rules. The key to our approach is to represent predicates in\nfirst-order logic using deep neural networks and then to approximately\nback-propagate through the HL-MRF and thus train every aspect of the\nfirst-order system being represented. We believe that this approach represents\nan interesting direction for the integration of deep learning and reasoning\ntechniques with applications to knowledge base learning, multi-task learning,\nand explainability. Evaluation on three different tasks demonstrates that\nDeepPSL significantly outperforms state-of-the-art neuro-symbolic methods on\nscalability while achieving comparable or better accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dasaratha_S/0/1/0/all/0/1\">Sridhar Dasaratha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puranam_S/0/1/0/all/0/1\">Sai Akhil Puranam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phogat_K/0/1/0/all/0/1\">Karmvir Singh Phogat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tiyyagura_S/0/1/0/all/0/1\">Sunil Reddy Tiyyagura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duffy_N/0/1/0/all/0/1\">Nigel P. Duffy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bag-of-Vectors Autoencoders for Unsupervised Conditional Text Generation. (arXiv:2110.07002v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07002","description":"<p>Text autoencoders are often used for unsupervised conditional text generation\nby applying mappings in the latent space to change attributes to the desired\nvalues. Recently, Mai et al. (2020) proposed Emb2Emb, a method to learn these\nmappings in the embedding space of an autoencoder. However, their method is\nrestricted to autoencoders with a single-vector embedding, which limits how\nmuch information can be retained. We address this issue by extending their\nmethod to Bag-of-Vectors Autoencoders (BoV-AEs), which encode the text into a\nvariable-size bag of vectors that grows with the size of the text, as in\nattention-based models. This allows to encode and reconstruct much longer texts\nthan standard autoencoders. Analogous to conventional autoencoders, we propose\nregularization techniques that facilitate learning meaningful operations in the\nlatent space. Finally, we adapt Emb2Emb for a training scheme that learns to\nmap an input bag to an output bag, including a novel loss function and neural\narchitecture. Our empirical evaluations on unsupervised sentiment transfer show\nthat our method performs substantially better than a standard autoencoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mai_F/0/1/0/all/0/1\">Florian Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings. (arXiv:2201.05575v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05575","description":"<p>Previous knowledge graph embedding approaches usually map entities to\nrepresentations and utilize score functions to predict the target entities, yet\nthey typically struggle to reason rare or emerging unseen entities. In this\npaper, we propose kNN-KGE, a new knowledge graph embedding approach with\npre-trained language models, by linearly interpolating its entity distribution\nwith k-nearest neighbors. We compute the nearest neighbors based on the\ndistance in the entity embedding space from the knowledge store. Our approach\ncan allow rare or emerging entities to be memorized explicitly rather than\nimplicitly in model parameters. Experimental results demonstrate that our\napproach can improve inductive and transductive link prediction results and\nyield better performance for low-resource settings with only a few triples,\nwhich might be easier to reason via explicit memory. Code is available at\nhttps://github.com/zjunlp/KNN-KG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence. (arXiv:2201.11176v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11176","description":"<p>Recently, there has been a growing interest in designing text generation\nsystems from a discourse coherence perspective, e.g., modeling the\ninterdependence between sentences. Still, recent BERT-based evaluation metrics\nare weak in recognizing coherence, and thus are not reliable in a way to spot\nthe discourse-level improvements of those text generation systems. In this\nwork, we introduce DiscoScore, a parametrized discourse metric, which uses BERT\nto model discourse coherence from different perspectives, driven by Centering\ntheory. Our experiments encompass 16 non-discourse and discourse metrics,\nincluding DiscoScore and popular coherence models, evaluated on summarization\nand document-level machine translation (MT). We find that (i) the majority of\nBERT-based metrics correlate much worse with human rated coherence than early\ndiscourse metrics, invented a decade ago; (ii) the recent state-of-the-art\nBARTScore is weak when operated at system level -- which is particularly\nproblematic as systems are typically compared in this manner. DiscoScore, in\ncontrast, achieves strong system-level correlation with human ratings, not only\nin coherence but also in factual consistency and other aspects, and surpasses\nBARTScore by over 10 correlation points on average. Further, aiming to\nunderstand DiscoScore, we provide justifications to the importance of discourse\ncoherence for evaluation metrics, and explain the superiority of one variant\nover another. Our code is available at\n\\url{https://github.com/AIPHES/DiscoScore}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strube_M/0/1/0/all/0/1\">Michael Strube</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locally Typical Sampling. (arXiv:2202.00666v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00666","description":"<p>Today's probabilistic language generators fall short when it comes to\nproducing coherent and fluent text despite the fact that the underlying models\nperform well under standard metrics, e.g., perplexity. This discrepancy has\npuzzled the language generation community for the last few years. In this work,\nwe posit that the abstraction of natural language generation as a discrete\nstochastic process--which allows for an information-theoretic analysis--can\nprovide new insights into the behavior of probabilistic language generators,\ne.g., why high-probability texts can be dull or repetitive. Humans use language\nas a means of communicating information, aiming to do so in a simultaneously\nefficient and error-minimizing manner; in fact, psycholinguistics research\nsuggests humans choose each word in a string with this subconscious goal in\nmind. We formally define the set of strings that meet this criterion: those for\nwhich each word has an information content close to the expected information\ncontent, i.e., the conditional entropy of our model. We then propose a simple\nand efficient procedure for enforcing this criterion when generating from\nprobabilistic models, which we call locally typical sampling. Automatic and\nhuman evaluations show that, in comparison to nucleus and top-k sampling,\nlocally typical sampling offers competitive performance (in both abstractive\nsummarization and story generation) in terms of quality while consistently\nreducing degenerate repetitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiher_G/0/1/0/all/0/1\">Gian Wiher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction of English Resume Corpus and Test with Pre-trained Language Models. (arXiv:2208.03219v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.03219","description":"<p>Information extraction(IE) has always been one of the essential tasks of NLP.\nMoreover, one of the most critical application scenarios of information\nextraction is the information extraction of resumes. Constructed text is\nobtained by classifying each part of the resume. It is convenient to store\nthese texts for later search and analysis. Furthermore, the constructed resume\ndata can also be used in the AI resume screening system. Significantly reduce\nthe labor cost of HR. This study aims to transform the information extraction\ntask of resumes into a simple sentence classification task. Based on the\nEnglish resume dataset produced by the prior study. The classification rules\nare improved to create a larger and more fine-grained classification dataset of\nresumes. This corpus is also used to test some current mainstream Pre-training\nlanguage models (PLMs) performance.Furthermore, in order to explore the\nrelationship between the number of training samples and the correctness rate of\nthe resume dataset, we also performed comparison experiments with training sets\nof different train set sizes.The final multiple experimental results show that\nthe resume dataset with improved annotation rules and increased sample size of\nthe dataset improves the accuracy of the original resume dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chengguang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_T/0/1/0/all/0/1\">Tatsunori Mori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wolfies at SemEval-2022 Task 8: Feature extraction pipeline with transformers for Multi-lingual news article similarity. (arXiv:2208.09715v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.09715","description":"<p>This work is about finding the similarity between a pair of news articles.\nThere are seven different objective similarity metrics provided in the dataset\nfor each pair and the news articles are in multiple different languages. On top\nof the pre-trained embedding model, we calculated cosine similarity for\nbaseline results and feed-forward neural network was then trained on top of it\nto improve the results. We also built separate pipelines for each similarity\nmetric for feature extraction. We could see significant improvement from\nbaseline results using feature extraction and feed-forward neural network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_N/0/1/0/all/0/1\">Nikhil Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Ranjith Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Perturbation-Based Gradient Estimation for Discrete Latent Variable Models. (arXiv:2209.04862v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2209.04862","description":"<p>The integration of discrete algorithmic components in deep learning\narchitectures has numerous applications. Recently, Implicit Maximum Likelihood\nEstimation (IMLE, Niepert, Minervini, and Franceschi 2021), a class of gradient\nestimators for discrete exponential family distributions, was proposed by\ncombining implicit differentiation through perturbation with the path-wise\ngradient estimator. However, due to the finite difference approximation of the\ngradients, it is especially sensitive to the choice of the finite difference\nstep size, which needs to be specified by the user. In this work, we present\nAdaptive IMLE (AIMLE), the first adaptive gradient estimator for complex\ndiscrete distributions: it adaptively identifies the target distribution for\nIMLE by trading off the density of gradient information with the degree of bias\nin the gradient estimates. We empirically evaluate our estimator on synthetic\nexamples, as well as on Learning to Explain, Discrete Variational\nAuto-Encoders, and Neural Relational Inference tasks. In our experiments, we\nshow that our adaptive gradient estimator can produce faithful estimates while\nrequiring orders of magnitude fewer samples than other gradient estimators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franceschi_L/0/1/0/all/0/1\">Luca Franceschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1\">Mathias Niepert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bidirectional Language Models Are Also Few-shot Learners. (arXiv:2209.14500v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2209.14500","description":"<p>Large language models such as GPT-3 (Brown et al., 2020) can perform\narbitrary tasks without undergoing fine-tuning after being prompted with only a\nfew labeled examples. An arbitrary task can be reformulated as a natural\nlanguage prompt, and a language model can be asked to generate the completion,\nindirectly performing the task in a paradigm known as prompt-based learning. To\ndate, emergent prompt-based learning capabilities have mainly been demonstrated\nfor unidirectional language models. However, bidirectional language models\npre-trained on denoising objectives such as masked language modeling produce\nstronger learned representations for transfer learning. This motivates the\npossibility of prompting bidirectional models, but their pre-training\nobjectives have made them largely incompatible with the existing prompting\nparadigm. We present SAP (Sequential Autoregressive Prompting), a technique\nthat enables the prompting of bidirectional models. Utilizing the machine\ntranslation task as a case study, we prompt the bidirectional mT5 model (Xue et\nal., 2021) with SAP and demonstrate its few-shot and zero-shot translations\noutperform the few-shot translations of unidirectional models like GPT-3 and\nXGLM (Lin et al., 2021), despite mT5's approximately 50% fewer parameters. We\nfurther show SAP is effective on question answering and summarization. For the\nfirst time, our results demonstrate prompt-based learning is an emergent\nproperty of a broader class of language models, rather than only unidirectional\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Ajay Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bryan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasooli_M/0/1/0/all/0/1\">Mohammad Sadegh Rasooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Non-monotonic Self-terminating Language Model. (arXiv:2210.00660v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.00660","description":"<p>Recent large-scale neural autoregressive sequence models have shown\nimpressive performances on a variety of natural language generation tasks.\nHowever, their generated sequences often exhibit degenerate properties such as\nnon-termination, undesirable repetition, and premature termination, when\ngenerated with decoding algorithms such as greedy search, beam search, top-$k$\nsampling, and nucleus sampling. In this paper, we focus on the problem of\nnon-terminating sequences resulting from an incomplete decoding algorithm. We\nfirst define an incomplete probable decoding algorithm which includes greedy\nsearch, top-$k$ sampling, and nucleus sampling, beyond the incomplete decoding\nalgorithm originally put forward by Welleck et al. (2020). We then propose a\nnon-monotonic self-terminating language model, which significantly relaxes the\nconstraint of monotonically increasing termination probability in the\noriginally proposed self-terminating language model by Welleck et al. (2020),\nto address the issue of non-terminating sequences when using incomplete\nprobable decoding algorithms. We prove that our proposed model prevents\nnon-terminating sequences when using not only incomplete probable decoding\nalgorithms but also beam search. We empirically validate our model on sequence\ncompletion tasks with various architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eugene Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Cheolhyoung Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing. (arXiv:2210.04878v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04878","description":"<p>Prior work in semantic parsing has shown that conventional seq2seq models\nfail at compositional generalization tasks. This limitation led to a resurgence\nof methods that model alignments between sentences and their corresponding\nmeaning representations, either implicitly through latent variables or\nexplicitly by taking advantage of alignment annotations. We take the second\ndirection and propose TPOL, a two-step approach that first translates input\nsentences monotonically and then reorders them to obtain the correct output.\nThis is achieved with a modular framework comprising a Translator and a\nReorderer component. We test our approach on two popular semantic parsing\ndatasets. Our experiments show that by means of the monotonic translations,\nTPOL can learn reliable lexico-logical patterns from aligned data,\nsignificantly improving compositional generalization both over conventional\nseq2seq models, as well as over other approaches that exploit gold alignments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cazzaro_F/0/1/0/all/0/1\">Francesco Cazzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatelli_D/0/1/0/all/0/1\">Davide Locatelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quattoni_A/0/1/0/all/0/1\">Ariadna Quattoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreras_X/0/1/0/all/0/1\">Xavier Carreras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RedHOT: A Corpus of Annotated Medical Questions, Experiences, and Claims on Social Media. (arXiv:2210.06331v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06331","description":"<p>We present Reddit Health Online Talk (RedHOT), a corpus of 22,000 richly\nannotated social media posts from Reddit spanning 24 health conditions.\nAnnotations include demarcations of spans corresponding to medical claims,\npersonal experiences, and questions. We collect additional granular annotations\non identified claims. Specifically, we mark snippets that describe patient\nPopulations, Interventions, and Outcomes (PIO elements) within these. Using\nthis corpus, we introduce the task of retrieving trustworthy evidence relevant\nto a given claim made on social media. We propose a new method to automatically\nderive (noisy) supervision for this task which we use to train a dense\nretrieval model; this outperforms baseline models. Manual evaluation of\nretrieval results performed by medical doctors indicate that while our system\nperformance is promising, there is considerable room for improvement. Collected\nannotations (and scripts to assemble the dataset), are available at\nhttps://github.com/sominw/redhot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadhwa_S/0/1/0/all/0/1\">Somin Wadhwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1\">Vivek Khetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amir_S/0/1/0/all/0/1\">Silvio Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models. (arXiv:2210.06475v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.06475","description":"<p>We introduce equi-tuning, a novel fine-tuning method that transforms\n(potentially non-equivariant) pretrained models into group equivariant models\nwhile incurring minimum $L_2$ loss between the feature representations of the\npretrained and the equivariant models. Large pretrained models can be\nequi-tuned for different groups to satisfy the needs of various downstream\ntasks. Equi-tuned models benefit from both group equivariance as an inductive\nbias and semantic priors from pretrained models. We provide applications of\nequi-tuning on three different tasks: image classification, compositional\ngeneralization in language, and fairness in natural language generation (NLG).\nWe also provide a novel group-theoretic definition for fairness in NLG. The\neffectiveness of this definition is shown by testing it against a standard\nempirical method of fairness in NLG. We provide experimental results for\nequi-tuning using a variety of pretrained models: Alexnet, Resnet, VGG, and\nDensenet for image classification; RNNs, GRUs, and LSTMs for compositional\ngeneralization; and GPT2 for fairness in NLG. We test these models on benchmark\ndatasets across all considered tasks to show the generality and effectiveness\nof the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Sourya Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattigeri_P/0/1/0/all/0/1\">Prasanna Sattigeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1\">Karthikeyan Natesan Ramamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenthamarakshan_V/0/1/0/all/0/1\">Vijil Chenthamarakshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1\">Kush R. Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_L/0/1/0/all/0/1\">Lav R. Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Reasoning on Hybrid-knowledge sources for Task-Oriented Dialog. (arXiv:2210.07295v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07295","description":"<p>Traditional systems designed for task oriented dialog utilize knowledge\npresent only in structured knowledge sources to generate responses. However,\nrelevant information required to generate responses may also reside in\nunstructured sources, such as documents. Recent state of the art models such as\nHyKnow and SeKnow aimed at overcoming these challenges make limiting\nassumptions about the knowledge sources. For instance, these systems assume\nthat certain types of information, such as a phone number, is always present in\na structured knowledge base (KB) while information about aspects such as\nentrance ticket prices, would always be available in documents.\n</p>\n<p>In this paper, we create a modified version of the MutliWOZ-based dataset\nprepared by SeKnow to demonstrate how current methods have significant\ndegradation in performance when strict assumptions about the source of\ninformation are removed. Then, in line with recent work exploiting pre-trained\nlanguage models, we fine-tune a BART based model using prompts for the tasks of\nquerying knowledge sources, as well as, for response generation, without making\nassumptions about the information present in each knowledge source. Through a\nseries of experiments, we demonstrate that our model is robust to perturbations\nto knowledge modality (source of information), and that it can fuse information\nfrom structured as well as unstructured knowledge to generate responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_M/0/1/0/all/0/1\">Mayank Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contractor_D/0/1/0/all/0/1\">Danish Contractor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghu_D/0/1/0/all/0/1\">Dinesh Raghu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MTEB: Massive Text Embedding Benchmark. (arXiv:2210.07316v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07316","description":"<p>Text embeddings are commonly evaluated on a small set of datasets from a\nsingle task not covering their possible applications to other tasks. It is\nunclear whether state-of-the-art embeddings on semantic textual similarity\n(STS) can be equally well applied to other tasks like clustering or reranking.\nThis makes progress in the field difficult to track, as various models are\nconstantly being proposed without proper evaluation. To solve this problem, we\nintroduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding\ntasks covering a total of 58 datasets and 112 languages. Through the\nbenchmarking of 33 models on MTEB, we establish the most comprehensive\nbenchmark of text embeddings to date. We find that no particular text embedding\nmethod dominates across all tasks. This suggests that the field has yet to\nconverge on a universal text embedding method and scale it up sufficiently to\nprovide state-of-the-art results on all embedding tasks. MTEB comes with\nopen-source code and a public leaderboard at\nhttps://github.com/embeddings-benchmark/mteb.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tazi_N/0/1/0/all/0/1\">Nouamane Tazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magne_L/0/1/0/all/0/1\">Lo&#xef;c Magne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Automated Essay Scoring using Transformer Models. (arXiv:2210.12809v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12809","description":"<p>Automated essay scoring is one of the most important problem in Natural\nLanguage Processing. It has been explored for a number of years, and it remains\npartially solved. In addition to its economic and educational usefulness, it\npresents research problems. Transfer learning has proved to be beneficial in\nNLP. Data augmentation techniques have also helped build state-of-the-art\nmodels for automated essay scoring. Many works in the past have attempted to\nsolve this problem by using RNNs, LSTMs, etc. This work examines the\ntransformer models like BERT, RoBERTa, etc. We empirically demonstrate the\neffectiveness of transformer models and data augmentation for automated essay\ngrading across many topics using a single model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kshitij Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverse scaling can become U-shaped. (arXiv:2211.02011v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.02011","description":"<p>Scaling up language models has been empirically shown to improve performance\nand unlock emergent abilities. Conversely, observing worse performance as a\nfunction of scale (\"inverse scaling\") would indicate that scaling encourages\nbehaviors that are misaligned with human preferences. The Inverse Scaling Prize\nidentified eleven such inverse scaling tasks, evaluated on models of up to 280B\nparameters and up to 500 zettaFLOPs of training compute.\n</p>\n<p>This paper takes a closer look at these inverse scaling tasks. We evaluate\nmodels of up to 540B parameters, trained on five times more compute than those\nevaluated in the Inverse Scaling Prize. With this increased range of model\nsizes and training compute, ten out of the eleven tasks exhibit what we call\n\"U-shaped scaling\" -- performance decreases up to a certain model size, and\nthen increases again up to the largest model evaluated. U-shaped scaling can be\nseen as emergent ability unlocked by scaling and implies that inverse scaling\nmay not hold for larger models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiFSMNv2: Pushing Binary Neural Networks for Keyword Spotting to Real-Network Performance. (arXiv:2211.06987v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.06987","description":"<p>Deep neural networks, such as the Deep-FSMN, have been widely studied for\nkeyword spotting (KWS) applications while suffering expensive computation and\nstorage. Therefore, network compression technologies like binarization are\nstudied to deploy KWS models on edge. In this paper, we present a strong yet\nefficient binary neural network for KWS, namely BiFSMNv2, pushing it to the\nreal-network accuracy performance. First, we present a Dual-scale Thinnable\n1-bit-Architecture to recover the representation capability of the binarized\ncomputation units by dual-scale activation binarization and liberate the\nspeedup potential from an overall architecture perspective. Second, we also\nconstruct a Frequency Independent Distillation scheme for KWS\nbinarization-aware training, which distills the high and low-frequency\ncomponents independently to mitigate the information mismatch between\nfull-precision and binarized representations. Moreover, we propose the Learning\nPropagation Binarizer, a general and efficient binarizer that enables the\nforward and backward propagation of binary KWS networks to be continuously\nimproved through learning. We implement and deploy the BiFSMNv2 on ARMv8\nreal-world hardware with a novel Fast Bitwise Computation Kernel, which is\nproposed to fully utilize registers and increase instruction throughput.\nComprehensive experiments show our BiFSMNv2 outperforms existing binary\nnetworks for KWS by convincing margins across different datasets and achieves\ncomparable accuracy with the full-precision networks (only a tiny 1.51% drop on\nSpeech Commands V1-12). We highlight that benefiting from the compact\narchitecture and optimized hardware kernel, BiFSMNv2 can achieve an impressive\n25.1x speedup and 20.2x storage-saving on edge hardware.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Haotong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xudong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiakai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Automata-Based Task Knowledge Representation from Large-Scale Generative Language Models. (arXiv:2212.01944v2 [cs.FL] UPDATED)","link":"http://arxiv.org/abs/2212.01944","description":"<p>Automata-based representations play an important role in control and planning\nin sequential decision-making, but obtaining high-level task knowledge for\nbuilding automata is often difficult. Although large-scale generative language\nmodels (GLMs) can help automatically distill task knowledge, the textual\noutputs from GLMs are not amenable for formal verification or use in sequential\ndecision-making. We propose a novel algorithm named GLM2FSA, which obtains\nhigh-level task knowledge represented in a finite state automaton (FSA) from a\ngiven brief description of the task goal. GLM2FSA sends queries to a GLM for\ntask knowledge in textual form and then builds an FSA to represent the textual\nknowledge. It fills the gap between text and automata-based representations,\nand the constructed FSA can be directly utilized in formal verification. We\nprovide an algorithm for iteratively refining the queries to the GLM based on\nthe outcomes, e.g., counter-examples, from verification. We demonstrate the\nalgorithm on examples that range from everyday tasks, e.g., crossing a road and\nmaking coffee, to security applications to laboratory safety protocols.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaglione_J/0/1/0/all/0/1\">Jean-Rapha&#xeb;l Gaglione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Concept Knowledge Graph for User Next Intent Prediction at Alipay. (arXiv:2301.00503v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.00503","description":"<p>This paper illustrates the technologies of user next intent prediction with a\nconcept knowledge graph. The system has been deployed on the Web at Alipay,\nserving more than 100 million daily active users. To explicitly characterize\nuser intent, we propose \\textbf{AlipayKG}, which is an offline concept\nknowledge graph in the Life-Service domain modeling the historical behaviors of\nusers, the rich content interacted by users and the relations between them. We\nfurther introduce a Transformer-based model which integrates expert rules from\nthe knowledge graph to infer the online user's next intent. Experimental\nresults demonstrate that the proposed system can effectively enhance the\nperformance of the downstream tasks while retaining explainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yacheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Qianghuai Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yixin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analogical Inference Enhanced Knowledge Graph Embedding. (arXiv:2301.00982v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2301.00982","description":"<p>Knowledge graph embedding (KGE), which maps entities and relations in a\nknowledge graph into continuous vector spaces, has achieved great success in\npredicting missing links in knowledge graphs. However, knowledge graphs often\ncontain incomplete triples that are difficult to inductively infer by KGEs. To\naddress this challenge, we resort to analogical inference and propose a novel\nand general self-supervised framework AnKGE to enhance KGE models with\nanalogical inference capability. We propose an analogical object retriever that\nretrieves appropriate analogical objects from entity-level, relation-level, and\ntriple-level. And in AnKGE, we train an analogy function for each level of\nanalogical inference with the original element embedding from a well-trained\nKGE model as input, which outputs the analogical object embedding. In order to\ncombine inductive inference capability from the original KGE model and\nanalogical inference capability enhanced by AnKGE, we interpolate the analogy\nscore with the base model score and introduce the adaptive weights in the score\nfunction for prediction. Through extensive experiments on FB15k-237 and WN18RR\ndatasets, we show that AnKGE achieves competitive results on link prediction\ntask and well performs analogical inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case Study using Latent Dirichlet Allocation Method. (arXiv:2301.03029v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.03029","description":"<p>Topic Modelling (TM) is from the research branches of natural language\nunderstanding (NLU) and natural language processing (NLP) that is to facilitate\ninsightful analysis from large documents and datasets, such as a summarisation\nof main topics and the topic changes. This kind of discovery is getting more\npopular in real-life applications due to its impact on big data analytics. In\nthis study, from the social-media and healthcare domain, we apply popular\nLatent Dirichlet Allocation (LDA) methods to model the topic changes in Swedish\nnewspaper articles about Coronavirus. We describe the corpus we created\nincluding 6515 articles, methods applied, and statistics on topic changes over\napproximately 1 year and two months period of time from 17th January 2020 to\n13th March 2021. We hope this work can be an asset for grounding applications\nof topic modelling and can be inspiring for similar case studies in an era with\npandemics, to support socio-economic impact research as well as clinical and\nhealthcare analytics. Our data and source code are openly available at\nhttps://github. com/poethan/Swed_Covid_TM Keywords: Latent Dirichlet Allocation\n(LDA); Topic Modelling; Coronavirus; Pandemics; Natural Language Understanding\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Griciute_B/0/1/0/all/0/1\">Bernadeta Grici&#x16b;t&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logically at Factify 2: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture. (arXiv:2301.03127v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.03127","description":"<p>In this paper, we present the Logically submissions to De-Factify 2 challenge\n(DE-FACTIFY 2023) on the task 1 of Multi-Modal Fact Checking. We describes our\nsubmissions to this challenge including explored evidence retrieval and\nselection techniques, pre-trained cross-modal and unimodal models, and a\ncross-modal veracity model based on the well established Transformer Encoder\n(TE) architecture which is heavily relies on the concept of self-attention.\nExploratory analysis is also conducted on this Factify 2 data set that uncovers\nthe salient multi-modal patterns and hypothesis motivating the architecture\nproposed in this work. A series of preliminary experiments were done to\ninvestigate and benchmarking different pre-trained embedding models, evidence\nretrieval settings and thresholds. The final system, a standard two-stage\nevidence based veracity detection system, yields weighted avg. 0.79 on both val\nset and final blind test set on the task 1, which achieves 3rd place with a\nsmall margin to the top performing system on the leaderboard among 9\nparticipants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verschuuren_P/0/1/0/all/0/1\">Pim Jordi Verschuuren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eeden_A/0/1/0/all/0/1\">Adelize van Eeden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikonomou_S/0/1/0/all/0/1\">Stylianos Oikonomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandhakavi_A/0/1/0/all/0/1\">Anil Bandhakavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's Just a Matter of Time: Detecting Depression with Time-Enriched Multimodal Transformers. (arXiv:2301.05453v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.05453","description":"<p>Depression detection from user-generated content on the internet has been a\nlong-lasting topic of interest in the research community, providing valuable\nscreening tools for psychologists. The ubiquitous use of social media platforms\nlays out the perfect avenue for exploring mental health manifestations in posts\nand interactions with other users. Current methods for depression detection\nfrom social media mainly focus on text processing, and only a few also utilize\nimages posted by users. In this work, we propose a flexible time-enriched\nmultimodal transformer architecture for detecting depression from social media\nposts, using pretrained models for extracting image and text embeddings. Our\nmodel operates directly at the user-level, and we enrich it with the relative\ntime between posts by using time2vec positional embeddings. Moreover, we\npropose another model variant, which can operate on randomly sampled and\nunordered sets of posts to be more robust to dataset noise. We show that our\nmethod, using EmoBERTa and CLIP embeddings, surpasses other methods on two\nmultimodal datasets, obtaining state-of-the-art results of 0.931 F1 score on a\npopular multimodal Twitter dataset, and 0.902 F1 score on the only multimodal\nReddit dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers as Algorithms: Generalization and Stability in In-context Learning. (arXiv:2301.07067v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2301.07067","description":"<p>In-context learning (ICL) is a type of prompting where a transformer model\noperates on a sequence of (input, output) examples and performs inference\non-the-fly. In this work, we formalize in-context learning as an algorithm\nlearning problem where a transformer model implicitly constructs a hypothesis\nfunction at inference-time. We first explore the statistical aspects of this\nabstraction through the lens of multitask learning: We obtain generalization\nbounds for ICL when the input prompt is (1) a sequence of i.i.d. (input, label)\npairs or (2) a trajectory arising from a dynamical system. The crux of our\nanalysis is relating the excess risk to the stability of the algorithm\nimplemented by the transformer. We characterize when transformer/attention\narchitecture provably obeys the stability condition and also provide empirical\nverification. For generalization on unseen tasks, we identify an inductive bias\nphenomenon in which the transfer learning risk is governed by the task\ncomplexity and the number of MTL tasks in a highly predictable manner. Finally,\nwe provide numerical evaluations that (1) demonstrate transformers can indeed\nimplement near-optimal algorithms on classical regression problems with i.i.d.\nand dynamic data, (2) provide insights on stability, and (3) verify our\ntheoretical predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingcong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ildiz_M/0/1/0/all/0/1\">M. Emrullah Ildiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1\">Dimitris Papailiopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1\">Samet Oymak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.07695","description":"<p>We present a new text-to-SQL dataset for electronic health records (EHRs).\nThe utterances were collected from 222 hospital staff, including physicians,\nnurses, insurance review and health records teams, and more. To construct the\nQA dataset on structured EHR data, we conducted a poll at a university hospital\nand templatized the responses to create seed questions. Then, we manually\nlinked them to two open-source EHR databases, MIMIC-III and eICU, and included\nthem with various time expressions and held-out unanswerable questions in the\ndataset, which were all collected from the poll. Our dataset poses a unique set\nof challenges: the model needs to 1) generate SQL queries that reflect a wide\nrange of needs in the hospital, including simple retrieval and complex\noperations such as calculating survival rate, 2) understand various time\nexpressions to answer time-sensitive questions in healthcare, and 3)\ndistinguish whether a given question is answerable or unanswerable based on the\nprediction confidence. We believe our dataset, EHRSQL, could serve as a\npractical benchmark to develop and assess QA models on structured EHR data and\ntake one step further towards bridging the gap between text-to-SQL research and\nits real-life deployment in healthcare. EHRSQL is available at\nhttps://github.com/glee4810/EHRSQL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gyubok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1\">Hyeonji Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Seongsu Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yeonsu Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Woncheol Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seongjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong-Yeup Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?. (arXiv:2301.09017v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.09017","description":"<p>Recent advancements in Large Language Models (LLMs) have drawn increasing\nattention since the learned embeddings pretrained on large-scale datasets have\nshown powerful ability in various downstream applications. However, whether the\nlearned knowledge by LLMs can be transferred to clinical cardiology remains\nunknown. In this work, we aim to bridge this gap by transferring the knowledge\nof LLMs to clinical Electrocardiography (ECG). We propose an approach for\ncardiovascular disease diagnosis and automatic ECG diagnosis report generation.\nWe also introduce an additional loss function by Optimal Transport (OT) to\nalign the distribution between ECG and language embedding. The learned\nembeddings are evaluated on two downstream tasks: (1) automatic ECG diagnosis\nreport generation, and (2) zero-shot cardiovascular disease detection. Our\napproach is able to generate high-quality cardiac diagnosis reports and also\nachieves competitive zero-shot classification performance even compared with\nsupervised baselines, which proves the feasibility of transferring knowledge\nfrom LLMs to the cardiac domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jielin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">William Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiacheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengdi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_M/0/1/0/all/0/1\">Michael Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1\">Emerson Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_D/0/1/0/all/0/1\">Douglas Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StockEmotions: Discover Investor Emotions for Financial Sentiment Analysis and Multivariate Time Series. (arXiv:2301.09279v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.09279","description":"<p>There has been growing interest in applying NLP techniques in the financial\ndomain, however, resources are extremely limited. This paper introduces\nStockEmotions, a new dataset for detecting emotions in the stock market that\nconsists of 10,000 English comments collected from StockTwits, a financial\nsocial media platform. Inspired by behavioral finance, it proposes 12\nfine-grained emotion classes that span the roller coaster of investor emotion.\nUnlike existing financial sentiment datasets, StockEmotions presents granular\nfeatures such as investor sentiment classes, fine-grained emotions, emojis, and\ntime series data. To demonstrate the usability of the dataset, we perform a\ndataset analysis and conduct experimental downstream tasks. For financial\nsentiment/emotion classification tasks, DistilBERT outperforms other baselines,\nand for multivariate time series forecasting, a Temporal Attention LSTM model\ncombining price index, text, and emotion features achieves the best performance\nthan using a single feature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jean Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youn_H/0/1/0/all/0/1\">Hoyoul Luis Youn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10405","description":"<p>Recently decades have witnessed the empirical success of framing Knowledge\nGraph (KG) embeddings via language models. However, language model-based KG\nembeddings are usually deployed as static artifacts, which are challenging to\nmodify without re-training after deployment. To address this issue, we propose\na new task of editing language model-based KG embeddings in this paper. The\nproposed task aims to enable data-efficient and fast updates to KG embeddings\nwithout damaging the performance of the rest. We build four new datasets:\nE-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge\nediting baselines demonstrating the limited ability of previous models to\nhandle the proposed challenging task. We further propose a simple yet strong\nbaseline dubbed KGEditor, which utilizes additional parametric layers of the\nhyper network to edit/add facts. Comprehensive experimental results demonstrate\nthat KGEditor can perform better when updating specific facts while not\naffecting the rest with low training resources. Code and datasets will be\navailable in https://github.com/zjunlp/PromptKG/tree/main/deltaKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1\">Bozhong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zelin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FewShotTextGCN: K-hop neighborhood regularization for few-shot learning on graphs. (arXiv:2301.10481v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10481","description":"<p>We present FewShotTextGCN, a novel method designed to effectively utilize the\nproperties of word-document graphs for improved learning in low-resource\nsettings. We introduce K-hop Neighbourhood Regularization, a regularizer for\nheterogeneous graphs, and show that it stabilizes and improves learning when\nonly a few training samples are available. We furthermore propose a\nsimplification in the graph-construction method, which results in a graph that\nis $\\sim$7 times less dense and yields better performance in little-resource\nsettings while performing on par with the state of the art in high-resource\nsettings. Finally, we introduce a new variant of Adaptive Pseudo-Labeling\ntailored for word-document graphs. When using as little as 20 samples for\ntraining, we outperform a strong TextGCN baseline with 17% in absolute accuracy\non average over eight languages. We demonstrate that our method can be applied\nto document classification without any language model pretraining on a wide\nrange of typologically diverse languages while performing on par with large\npretrained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heijden_N/0/1/0/all/0/1\">Niels van der Heijden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1\">Ekaterina Shutova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1\">Helen Yannakoudakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Discerning Several Thousand Judgments: GPT-3 Rates the Article + Adjective + Numeral + Noun Construction. (arXiv:2301.12564v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12564","description":"<p>Knowledge of syntax includes knowledge of rare, idiosyncratic constructions.\nLLMs must overcome frequency biases in order to master such constructions. In\nthis study, I prompt GPT-3 to give acceptability judgments on the\nEnglish-language Article + Adjective + Numeral + Noun construction (e.g., \"a\nlovely five days\"). I validate the prompt using the CoLA corpus of\nacceptability judgments and then zero in on the AANN construction. I compare\nGPT- 3's judgments to crowdsourced human judgments on a subset of sentences.\nGPT-3's judgments are broadly similar to human judgments and generally align\nwith proposed constraints in the literature but, in some cases, GPT-3's\njudgments and human judgments diverge from the literature and from each other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining. (arXiv:2301.12596v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2301.12596","description":"<p>While neural text-to-speech (TTS) has achieved human-like natural synthetic\nspeech, multilingual TTS systems are limited to resource-rich languages due to\nthe need for paired text and studio-quality audio data. This paper proposes a\nmethod for zero-shot multilingual TTS using text-only data for the target\nlanguage. The use of text-only data allows the development of TTS systems for\nlow-resource languages for which only textual resources are available, making\nTTS accessible to thousands of languages. Inspired by the strong cross-lingual\ntransferability of multilingual language models, our framework first performs\nmasked language model pretraining with multilingual text-only data. Then we\ntrain this model with a paired data in a supervised manner, while freezing a\nlanguage-aware embedding layer. This allows inference even for languages not\nincluded in the paired data but present in the text-only data. Evaluation\nresults demonstrate highly intelligible zero-shot TTS with a character error\nrate of less than 12% for an unseen language. All experiments were conducted\nusing public datasets and the implementation will be made available for\nreproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saeki_T/0/1/0/all/0/1\">Takaaki Saeki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maiti_S/0/1/0/all/0/1\">Soumi Maiti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xinjian Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takamichi_S/0/1/0/all/0/1\">Shinnosuke Takamichi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saruwatari_H/0/1/0/all/0/1\">Hiroshi Saruwatari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation $\\approx$ Label Smoothing: Fact or Fallacy?. (arXiv:2301.12609v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2301.12609","description":"<p>Contrary to its original interpretation as a facilitator of knowledge\ntransfer from one model to another, some recent studies have suggested that\nknowledge distillation (KD) is instead a form of regularization. Perhaps the\nstrongest support of all for this claim is found in its apparent similarities\nwith label smoothing (LS). This paper investigates the stated equivalence of\nthese two methods by examining the predictive uncertainties of the models they\ntrain. Experiments on four text classification tasks involving teachers and\nstudents of different capacities show that: (a) In most settings, KD and LS\ndrive model uncertainty (entropy) in completely opposite directions, and (b) In\nKD, the student's predictive uncertainty is a direct function of that of its\nteacher, reinforcing the knowledge transfer view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex. (arXiv:2301.12868v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12868","description":"<p>Semantic parsing is a technique aimed at constructing a structured\nrepresentation of the meaning of a natural-language question. Recent\nadvancements in few-shot language models trained on code have demonstrated\nsuperior performance in generating these representations compared to\ntraditional unimodal language models, which are trained on downstream tasks.\nDespite these advancements, existing fine-tuned neural semantic parsers are\nsusceptible to adversarial attacks on natural-language inputs. While it has\nbeen established that the robustness of smaller semantic parsers can be\nenhanced through adversarial training, this approach is not feasible for large\nlanguage models in real-world scenarios, as it requires both substantial\ncomputational resources and expensive human annotation on in-domain semantic\nparsing data. This paper presents the first empirical study on the adversarial\nrobustness of a large prompt-based language model of code, \\codex. Our results\ndemonstrate that the state-of-the-art (SOTA) code-language models are\nvulnerable to carefully crafted adversarial examples. To address this\nchallenge, we propose methods for improving robustness without the need for\nsignificant amounts of labeled data or heavy computational resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yujin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiri_F/0/1/0/all/0/1\">Fatemeh Shiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Multilingual Semantic Parser. (arXiv:2301.12920v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12920","description":"<p>Current multilingual semantic parsing (MSP) datasets are almost all collected\nby translating the utterances in the existing datasets from the resource-rich\nlanguage to the target language. However, manual translation is costly. To\nreduce the translation effort, this paper proposes the first active learning\nprocedure for MSP (AL-MSP). AL-MSP selects only a subset from the existing\ndatasets to be translated. We also propose a novel selection method that\nprioritizes the examples diversifying the logical form structures with more\nlexical choices, and a novel hyperparameter tuning method that needs no extra\nannotation cost. Our experiments show that AL-MSP significantly reduces\ntranslation costs with ideal selection methods. Our selection method with\nproper hyperparameters yields better parsing performance than the other\nbaselines on two multilingual datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal-Discovery Performance of ChatGPT in the context of Neuropathic Pain Diagnosis. (arXiv:2301.13819v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.13819","description":"<p>ChatGPT has demonstrated exceptional proficiency in natural language\nconversation, e.g., it can answer a wide range of questions while no previous\nlarge language models can. Thus, we would like to push its limit and explore\nits ability to answer causal discovery questions by using a medical benchmark\n(Tu et al. 2019) in causal discovery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1\">Ruibo Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data. (arXiv:2302.00674v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.00674","description":"<p>Few-shot learning involves learning an effective model from only a few\nlabeled datapoints. The use of a small training set makes it difficult to avoid\noverfitting but also makes few-shot learning applicable to many important\nreal-world settings. In this work, we focus on Few-shot Learning with Auxiliary\nData (FLAD), a training paradigm that assumes access to auxiliary data during\nfew-shot learning in hopes of improving generalization. Introducing auxiliary\ndata during few-shot learning leads to essential design choices where\nhand-designed heuristics can lead to sub-optimal performance. In this work, we\nfocus on automated sampling strategies for FLAD and relate them to the\nexplore-exploit dilemma that is central in multi-armed bandit settings. Based\non this connection we propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and\ncompare them with methods that either explore or exploit, finding that the\ncombination of exploration and exploitation is crucial. Using our proposed\nalgorithms to train T5 yields a 9% absolute improvement over the explicitly\nmulti-task pre-trained T0 model across 11 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Show me your NFT and I tell you how it will perform: Multimodal representation learning for NFT selling price prediction. (arXiv:2302.01676v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.01676","description":"<p>Non-Fungible Tokens (NFTs) represent deeds of ownership, based on blockchain\ntechnologies and smart contracts, of unique crypto assets on digital art forms\n(e.g., artworks or collectibles). In the spotlight after skyrocketing in 2021,\nNFTs have attracted the attention of crypto enthusiasts and investors intent on\nplacing promising investments in this profitable market. However, the NFT\nfinancial performance prediction has not been widely explored to date.\n</p>\n<p>In this work, we address the above problem based on the hypothesis that NFT\nimages and their textual descriptions are essential proxies to predict the NFT\nselling prices. To this purpose, we propose MERLIN, a novel multimodal deep\nlearning framework designed to train Transformer-based language and visual\nmodels, along with graph neural network models, on collections of NFTs' images\nand texts. A key aspect in MERLIN is its independence on financial features, as\nit exploits only the primary data a user interested in NFT trading would like\nto deal with, i.e., NFT images and textual descriptions. By learning dense\nrepresentations of such data, a price-category classification task is performed\nby MERLIN models, which can also be tuned according to user preferences in the\ninference phase to mimic different risk-return investment profiles.\nExperimental evaluation on a publicly available dataset has shown that MERLIN\nmodels achieve significant performances according to several financial\nassessment criteria, fostering profitable investments, and also beating\nbaseline machine-learning classifiers based on financial features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Costa_D/0/1/0/all/0/1\">Davide Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cava_L/0/1/0/all/0/1\">Lucio La Cava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagarelli_A/0/1/0/all/0/1\">Andrea Tagarelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LIQUID: A Framework for List Question Answering Dataset Generation. (arXiv:2302.01691v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.01691","description":"<p>Question answering (QA) models often rely on large-scale training datasets,\nwhich necessitates the development of a data generation framework to reduce the\ncost of manual annotations. Although several recent studies have aimed to\ngenerate synthetic questions with single-span answers, no study has been\nconducted on the creation of list questions with multiple, non-contiguous spans\nas answers. To address this gap, we propose LIQUID, an automated framework for\ngenerating list QA datasets from unlabeled corpora. We first convert a passage\nfrom Wikipedia or PubMed into a summary and extract named entities from the\nsummarized text as candidate answers. This allows us to select answers that are\nsemantically correlated in context and is, therefore, suitable for constructing\nlist questions. We then create questions using an off-the-shelf question\ngenerator with the extracted entities and original passage. Finally, iterative\nfiltering and answer expansion are performed to ensure the accuracy and\ncompleteness of the answers. Using our synthetic data, we significantly improve\nthe performance of the previous best list QA models by exact-match F1 scores of\n5.0 on MultiSpanQA, 1.9 on Quoref, and 2.8 averaged across three BioASQ\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seongyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}