{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-24T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Lang2LTL: Translating Natural Language Commands to Temporal Robot Task Specification. (arXiv:2302.11649v1 [cs.RO])","link":"http://arxiv.org/abs/2302.11649","description":"<p>Natural language provides a powerful modality to program robots to perform\ntemporal tasks. Linear temporal logic (LTL) provides unambiguous semantics for\nformal descriptions of temporal tasks. However, existing approaches cannot\naccurately and robustly translate English sentences to their equivalent LTL\nformulas in unseen environments. To address this problem, we propose Lang2LTL,\na novel modular system that leverages pretrained large language models to first\nextract referring expressions from a natural language command, then ground the\nexpressions to real-world landmarks and objects, and finally translate the\ncommand into an LTL task specification for the robot. It enables any robotic\nsystem to interpret natural language navigation commands without additional\ntraining, provided that it tracks its position and has a semantic map with\nlandmarks labeled with free-form text. We demonstrate the state-of-the-art\nability to generalize to multi-scale navigation domains such as OpenStreetMap\n(OSM) and CleanUp World (a simulated household environment). Lang2LTL achieves\nan average accuracy of 88.4% in translating challenging LTL formulas in 22\nunseen OSM environments as evaluated on a new corpus of over 10,000 commands,\n22 times better than the previous SoTA. Without modification, the best\nperforming Lang2LTL model on the OSM dataset can translate commands in CleanUp\nWorld with 82.8% accuracy. As a part of our proposed comprehensive evaluation\nprocedures, we collected a new labeled dataset of English commands representing\n2,125 unique LTL formulas, the largest ever dataset of natural language\ncommands to LTL specifications for robotic tasks with the most diverse LTL\nformulas, 40 times more than previous largest dataset. Finally, we integrated\nLang2LTL with a planner to command a quadruped mobile robot to perform\nmulti-step navigational tasks in an analog real-world environment created in\nthe lab.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jason Xinyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idrees_I/0/1/0/all/0/1\">Ifrah Idrees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sam Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schornstein_B/0/1/0/all/0/1\">Benjamin Schornstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1\">Stefanie Tellex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ankit Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v1 [cs.CV])","link":"http://arxiv.org/abs/2302.11713","description":"<p>Large language models have demonstrated an emergent capability in answering\nknowledge intensive questions. With recent progress on web-scale visual and\nlanguage pre-training, do these models also understand how to answer visual\ninformation seeking questions? To answer this question, we present InfoSeek, a\nVisual Question Answering dataset that focuses on asking information-seeking\nquestions, where the information can not be answered by common sense knowledge.\nWe perform a multi-stage human annotation to collect a natural distribution of\nhigh-quality visual information seeking question-answer pairs. We also\nconstruct a large-scale, automatically collected dataset by combining existing\nvisual entity recognition datasets and Wikidata, which provides over one\nmillion examples for model fine-tuning and validation. Based on InfoSeek, we\nanalyzed various pre-trained Visual QA systems to gain insights into the\ncharacteristics of different pre-trained models. Our analysis shows that it is\nchallenging for the state-of-the-art multi-modal pre-trained models to answer\nvisual information seeking questions, but this capability is improved through\nfine-tuning on the automated InfoSeek dataset. We hope our analysis paves the\nway to understand and develop the next generation of multi-modal pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1\">Yi Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haitian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLSP 2022 -- EVJVQA Challenge: Multilingual Visual Question Answering. (arXiv:2302.11752v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11752","description":"<p>Visual Question Answering (VQA) is a challenging task of natural language\nprocessing (NLP) and computer vision (CV), attracting significant attention\nfrom researchers. English is a resource-rich language that has witnessed\nvarious developments in datasets and models for visual question answering.\nVisual question answering in other languages also would be developed for\nresources and models. In addition, there is no multilingual dataset targeting\nthe visual content of a particular country with its own objects and cultural\ncharacteristics. To address the weakness, we provide the research community\nwith a benchmark dataset named EVJVQA, including 33,000+ pairs of\nquestion-answer over three languages: Vietnamese, English, and Japanese, on\napproximately 5,000 images taken from Vietnam for evaluating multilingual VQA\nsystems or models. EVJVQA is used as a benchmark dataset for the challenge of\nmultilingual visual question answering at the 9th Workshop on Vietnamese\nLanguage and Speech Processing (VLSP 2022). This task attracted 62 participant\nteams from various universities and organizations. In this article, we present\ndetails of the organization of the challenge, an overview of the methods\nemployed by shared-task participants, and the results. The highest performances\nare 0.4392 in F1-score and 0.4009 in BLUE on the private test set. The\nmultilingual QA systems proposed by the top 2 teams use ViT for the pre-trained\nvision model and mT5 for the pre-trained language model, a powerful pre-trained\nlanguage model based on the transformer architecture. EVJVQA is a challenging\ndataset that motivates NLP and CV researchers to further explore the\nmultilingual models or systems for visual question answering systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nghia Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Duong T.D Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khanh Quoc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUTANT: A Multi-sentential Code-mixed Hinglish Dataset. (arXiv:2302.11766v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11766","description":"<p>The multi-sentential long sequence textual data unfolds several interesting\nresearch directions pertaining to natural language processing and generation.\nThough we observe several high-quality long-sequence datasets for English and\nother monolingual languages, there is no significant effort in building such\nresources for code-mixed languages such as Hinglish (code-mixing of\nHindi-English). In this paper, we propose a novel task of identifying\nmulti-sentential code-mixed text (MCT) from multilingual articles. As a use\ncase, we leverage multilingual articles from two different data sources and\nbuild a first-of-its-kind multi-sentential code-mixed Hinglish dataset i.e.,\nMUTANT. We propose a token-level language-aware pipeline and extend the\nexisting metrics measuring the degree of code-mixing to a multi-sentential\nframework and automatically identify MCT in the multilingual articles. The\nMUTANT dataset comprises 67k articles with 85k identified Hinglish MCTs. To\nfacilitate future research, we make the publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1\">Vivek Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empathetic Response Generation via Emotion Cause Transition Graph. (arXiv:2302.11787v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11787","description":"<p>Empathetic dialogue is a human-like behavior that requires the perception of\nboth affective factors (e.g., emotion status) and cognitive factors (e.g.,\ncause of the emotion). Besides concerning emotion status in early work, the\nlatest approaches study emotion causes in empathetic dialogue. These approaches\nfocus on understanding and duplicating emotion causes in the context to show\nempathy for the speaker. However, instead of only repeating the contextual\ncauses, the real empathic response often demonstrate a logical and\nemotion-centered transition from the causes in the context to those in the\nresponses. In this work, we propose an emotion cause transition graph to\nexplicitly model the natural transition of emotion causes between two adjacent\nturns in empathetic dialogue. With this graph, the concept words of the emotion\ncauses in the next turn can be predicted and used by a specifically designed\nconcept-aware decoder to generate the empathic response. Automatic and human\nexperimental results on the benchmark dataset demonstrate that our method\nproduces more empathetic, coherent, informative, and specific responses than\nexisting models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yushan Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ying Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yuexian Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FiTs: Fine-grained Two-stage Training for Knowledge-aware Question Answering. (arXiv:2302.11799v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11799","description":"<p>Knowledge-aware question answering (KAQA) requires the model to answer\nquestions over a knowledge base, which is essential for both open-domain QA and\ndomain-specific QA, especially when language models alone cannot provide all\nthe knowledge needed. Despite the promising result of recent KAQA systems which\ntend to integrate linguistic knowledge from pre-trained language models (PLM)\nand factual knowledge from knowledge graphs (KG) to answer complex questions, a\nbottleneck exists in effectively fusing the representations from PLMs and KGs\nbecause of (i) the semantic and distributional gaps between them, and (ii) the\ndifficulties in joint reasoning over the provided knowledge from both\nmodalities. To address the above two problems, we propose a Fine-grained\nTwo-stage training framework (FiTs) to boost the KAQA system performance: The\nfirst stage aims at aligning representations from the PLM and the KG, thus\nbridging the modality gaps between them, named knowledge adaptive\npost-training. The second stage, called knowledge-aware fine-tuning, aims to\nimprove the model's joint reasoning ability based on the aligned\nrepresentations. In detail, we fine-tune the post-trained model via two\nauxiliary self-supervised tasks in addition to the QA supervision. Extensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\non three benchmarks in the commonsense reasoning (i.e., CommonsenseQA,\nOpenbookQA) and medical question answering (i.e., MedQA-USMILE) domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qichen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Bowen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teacher Intervention: Improving Convergence of Quantization Aware Training for Ultra-Low Precision Transformers. (arXiv:2302.11812v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11812","description":"<p>Pre-trained Transformer models such as BERT have shown great success in a\nwide range of applications, but at the cost of substantial increases in model\ncomplexity. Quantization-aware training (QAT) is a promising method to lower\nthe implementation cost and energy consumption. However, aggressive\nquantization below 2-bit causes considerable accuracy degradation due to\nunstable convergence, especially when the downstream dataset is not abundant.\nThis work proposes a proactive knowledge distillation method called Teacher\nIntervention (TI) for fast converging QAT of ultra-low precision pre-trained\nTransformers. TI intervenes layer-wise signal propagation with the intact\nsignal from the teacher to remove the interference of propagated quantization\nerrors, smoothing loss surface of QAT and expediting the convergence.\nFurthermore, we propose a gradual intervention mechanism to stabilize the\nrecovery of subsections of Transformer layers from quantization. The proposed\nschemes enable fast convergence of QAT and improve the model accuracy\nregardless of the diverse characteristics of downstream fine-tuning tasks. We\ndemonstrate that TI consistently achieves superior accuracy with significantly\nlower fine-tuning iterations on well-known Transformers of natural language\nprocessing as well as computer vision compared to the state-of-the-art QAT\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_K/0/1/0/all/0/1\">Kyuhong Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seongmin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1\">Wonyong Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jungwook Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Knowledge Selection for Document Grounded Dialogs. (arXiv:2302.11849v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11849","description":"<p>Multi-document grounded dialogue systems (DGDS) belong to a class of\nconversational agents that answer users' requests by finding supporting\nknowledge from a collection of documents. Most previous studies aim to improve\nthe knowledge retrieval model or propose more effective ways to incorporate\nexternal knowledge into a parametric generation model. These methods, however,\nfocus on retrieving knowledge from mono-granularity language units (e.g.\npassages, sentences, or spans in documents), which is not enough to effectively\nand efficiently capture precise knowledge in long documents. This paper\nproposes Re3G, which aims to optimize both coarse-grained knowledge retrieval\nand fine-grained knowledge extraction in a unified framework. Specifically, the\nformer efficiently finds relevant passages in a retrieval-and-reranking\nprocess, whereas the latter effectively extracts finer-grain spans within those\npassages to incorporate into a parametric answer generation model (BART, T5).\nExperiments on DialDoc Shared Task demonstrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yeqin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Haomin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Cheng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cam-Tu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Training of Mixture-of-Experts Language GANs. (arXiv:2302.11875v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11875","description":"<p>Despite the dramatic success in image generation, Generative Adversarial\nNetworks (GANs) still face great challenges in synthesizing sequences of\ndiscrete elements, in particular human language. The difficulty in generator\ntraining arises from the limited representation capacity and uninformative\nlearning signals obtained from the discriminator. In this work, we (1) first\nempirically show that the mixture-of-experts approach is able to enhance the\nrepresentation capacity of the generator for language GANs and (2) harness the\nFeature Statistics Alignment (FSA) paradigm to render fine-grained learning\nsignals to advance the generator training. Specifically, FSA forces the mean\nstatistics of the distribution of fake data to approach that of real samples as\nclose as possible in the finite-dimensional feature space. Empirical study on\nsynthetic and real benchmarks shows the superior performance in quantitative\nevaluation and demonstrates the effectiveness of our approach to adversarial\ntext generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yekun Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qiyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junge Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Simplification via Large Language Models. (arXiv:2302.11957v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11957","description":"<p>Sentence Simplification aims to rephrase complex sentences into simpler\nsentences while retaining original meaning. Large Language models (LLMs) have\ndemonstrated the ability to perform a variety of natural language processing\ntasks. However, it is not yet known whether LLMs can be served as a\nhigh-quality sentence simplification system. In this work, we empirically\nanalyze the zero-/few-shot learning ability of LLMs by evaluating them on a\nnumber of benchmark test sets. Experimental results show LLMs outperform\nstate-of-the-art sentence simplification methods, and are judged to be on a par\nwith human annotators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yutao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_J/0/1/0/all/0/1\">Jipeng Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yunhao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Deep Learning Learn to Abstract? A Systematic Probing Framework. (arXiv:2302.11978v1 [cs.LG])","link":"http://arxiv.org/abs/2302.11978","description":"<p>Abstraction is a desirable capability for deep learning models, which means\nto induce abstract concepts from concrete instances and flexibly apply them\nbeyond the learning context. At the same time, there is a lack of clear\nunderstanding about both the presence and further characteristics of this\ncapability in deep learning models. In this paper, we introduce a systematic\nprobing framework to explore the abstraction capability of deep learning models\nfrom a transferability perspective. A set of controlled experiments are\nconducted based on this framework, providing strong evidence that two probed\npre-trained language models (PLMs), T5 and GPT2, have the abstraction\ncapability. We also conduct in-depth analysis, thus shedding further light: (1)\nthe whole training phase exhibits a \"memorize-then-abstract\" two-stage process;\n(2) the learned abstract concepts are gathered in a few middle-layer attention\nheads, rather than being evenly distributed throughout the model; (3) the\nprobed abstraction capabilities exhibit robustness against concept mutations,\nand are more robust to low-level/source-side mutations than\nhigh-level/target-side ones; (4) generic pre-training is critical to the\nemergence of abstraction capability, and PLMs exhibit better abstraction with\nlarger model sizes and data scales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shengnan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metric-oriented Speech Enhancement using Diffusion Probabilistic Model. (arXiv:2302.11989v1 [cs.SD])","link":"http://arxiv.org/abs/2302.11989","description":"<p>Deep neural network based speech enhancement technique focuses on learning a\nnoisy-to-clean transformation supervised by paired training data. However, the\ntask-specific evaluation metric (e.g., PESQ) is usually non-differentiable and\ncan not be directly constructed in the training criteria. This mismatch between\nthe training objective and evaluation metric likely results in sub-optimal\nperformance. To alleviate it, we propose a metric-oriented speech enhancement\nmethod (MOSE), which leverages the recent advances in the diffusion\nprobabilistic model and integrates a metric-oriented training strategy into its\nreverse process. Specifically, we design an actor-critic based framework that\nconsiders the evaluation metric as a posterior reward, thus guiding the reverse\nprocess to the metric-increasing direction. The experimental results\ndemonstrate that MOSE obviously benefits from metric-oriented training and\nsurpasses the generative baselines in terms of all evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuchen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_W/0/1/0/all/0/1\">Weiwei Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing in the Legal Domain. (arXiv:2302.12039v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12039","description":"<p>In this paper, we summarize the current state of the field of NLP &amp; Law with\na specific focus on recent technical and substantive developments. To support\nour analysis, we construct and analyze a nearly complete corpus of more than\nsix hundred NLP &amp; Law related papers published over the past decade. Our\nanalysis highlights several major trends. Namely, we document an increasing\nnumber of papers written, tasks undertaken, and languages covered over the\ncourse of the past decade. We observe an increase in the sophistication of the\nmethods which researchers deployed in this applied context. Slowly but surely,\nLegal NLP is beginning to match not only the methodological sophistication of\ngeneral NLP but also the professional standards of data availability and code\nreproducibility observed within the broader scientific community. We believe\nall of these trends bode well for the future of the field, but many questions\nin both the academic and commercial sphere still remain open.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katz_D/0/1/0/all/0/1\">Daniel Martin Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartung_D/0/1/0/all/0/1\">Dirk Hartung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerlach_L/0/1/0/all/0/1\">Lauritz Gerlach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jana_A/0/1/0/all/0/1\">Abhik Jana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bommarito_M/0/1/0/all/0/1\">Michael J. Bommarito II</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Agents and Children: Let Children Learn. (arXiv:2302.12043v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12043","description":"<p>Using online information discovery as a case study, in this position paper we\ndiscuss the need to design, develop, and deploy (conversational) agents that\ncan -- non-intrusively -- guide children in their quest for online resources\nrather than simply finding resources for them. We argue that agents should \"let\nchildren learn\" and should be built to take on a teacher-facilitator function,\nallowing children to develop their technical and critical thinking abilities as\nthey interact with varied technology in a broad range of use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kennington_C/0/1/0/all/0/1\">Casey Kennington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fails_J/0/1/0/all/0/1\">Jerry Alan Fails</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_K/0/1/0/all/0/1\">Katherine Landau Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pera_M/0/1/0/all/0/1\">Maria Soledad Pera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Social Media for Early Detection of Depression in COVID-19 Patients. (arXiv:2302.12044v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12044","description":"<p>The COVID-19 pandemic has caused substantial damage to global health. Even\nthough three years have passed, the world continues to struggle with the virus.\nConcerns are growing about the impact of COVID-19 on the mental health of\ninfected individuals, who are more likely to experience depression, which can\nhave long-lasting consequences for both the affected individuals and the world.\nDetection and intervention at an early stage can reduce the risk of depression\nin COVID-19 patients. In this paper, we investigated the relationship between\nCOVID-19 infection and depression through social media analysis. Firstly, we\nmanaged a dataset of COVID-19 patients that contains information about their\nsocial media activity both before and after infection. Secondly,We conducted an\nextensive analysis of this dataset to investigate the characteristic of\nCOVID-19 patients with a higher risk of depression. Thirdly, we proposed a deep\nneural network for early prediction of depression risk. This model considers\ndaily mood swings as a psychiatric signal and incorporates textual and\nemotional characteristics via knowledge distillation. Experimental results\ndemonstrate that our proposed framework outperforms baselines in detecting\ndepression risk, with an AUROC of 0.9317 and an AUPRC of 0.8116. Our model has\nthe potential to enable public health organizations to initiate prompt\nintervention with high-risk patients\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiageng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yining Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shixu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Sentiment Transfer via Adaptive Masking. (arXiv:2302.12045v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12045","description":"<p>Sentiment transfer aims at revising the input text to satisfy a given\nsentiment polarity while retaining the original semantic content. The nucleus\nof sentiment transfer lies in precisely separating the sentiment information\nfrom the content information. Existing explicit approaches generally identify\nand mask sentiment tokens simply based on prior linguistic knowledge and\nmanually-defined rules, leading to low generality and undesirable transfer\nperformance. In this paper, we view the positions to be masked as the learnable\nparameters, and further propose a novel AM-ST model to learn adaptive\ntask-relevant masks based on the attention mechanism. Moreover, a\nsentiment-aware masked language model is further proposed to fill in the blanks\nin the masked positions by incorporating both context and sentiment polarity to\ncapture the multi-grained semantics comprehensively. AM-ST is thoroughly\nevaluated on two popular datasets, and the experimental results demonstrate the\nsuperiority of our proposal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yingze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1\">LiQiang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiren Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaozhuo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Automatic Speech Recognition in an Incremental Setting. (arXiv:2302.12049v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12049","description":"<p>The increasing reliability of automatic speech recognition has proliferated\nits everyday use. However, for research purposes, it is often unclear which\nmodel one should choose for a task, particularly if there is a requirement for\nspeed as well as accuracy. In this paper, we systematically evaluate six speech\nrecognizers using metrics including word error rate, latency, and the number of\nupdates to already recognized words on English test data, as well as propose\nand compare two methods for streaming audio into recognizers for incremental\nrecognition. We further propose Revokes per Second as a new metric for\nevaluating incremental recognition and demonstrate that it provides insights\ninto overall model performance. We find that, generally, local recognizers are\nfaster and require fewer updates than cloud-based recognizers. Finally, we find\nMeta's Wav2Vec model to be the fastest, and find Mozilla's DeepSpeech model to\nbe the most stable in its predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whetten_R/0/1/0/all/0/1\">Ryan Whetten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imtiaz_M/0/1/0/all/0/1\">Mir Tahsin Imtiaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennington_C/0/1/0/all/0/1\">Casey Kennington</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPINDLE: Spinning Raw Text into Lambda Terms with Graph Attention. (arXiv:2302.12050v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12050","description":"<p>This paper describes SPINDLE - an open source Python module implementing an\nefficient and accurate parser for written Dutch that transforms raw text input\nto programs for meaning composition, expressed as {\\lambda} terms. The parser\nintegrates a number of breakthrough advances made in recent years. Its output\nconsists of hi-res derivations of a multimodal type-logical grammar, capturing\ntwo orthogonal axes of syntax, namely deep function-argument structures and\ndependency relations. These are produced by three interdependent systems: a\nstatic type-checker asserting the well-formedness of grammatical analyses, a\nstate-of-the-art, structurally-aware supertagger based on heterogeneous graph\nconvolutions, and a massively parallel proof search component based on Sinkhorn\niterations. Packed in the software are also handy utilities and extras for\nproof visualization and inference, intended to facilitate end-user utilization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kogkalidis_K/0/1/0/all/0/1\">Konstantinos Kogkalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moortgat_M/0/1/0/all/0/1\">Michael Moortgat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moot_R/0/1/0/all/0/1\">Richard Moot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProsAudit, a prosodic benchmark for self-supervised speech models. (arXiv:2302.12057v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12057","description":"<p>We present ProsAudit, a benchmark in English to assess structural prosodic\nknowledge in self-supervised learning (SSL) speech models. It consists of two\nsubtasks, their corresponding metrics, an evaluation dataset. In the\nprotosyntax task, the model must correctly identify strong versus weak prosodic\nboundaries. In the lexical task, the model needs to correctly distinguish\nbetween pauses inserted between words and within words. We also provide human\nevaluation scores on this benchmark. We evaluated a series of SSL models and\nfound that they were all able to perform above chance on both tasks, even when\ntrained on an unseen language. However, non-native models performed\nsignificantly worse than native ones on the lexical task, highlighting the\nimportance of lexical knowledge in this task. We also found a clear effect of\nsize with models trained on more data performing better in the two subtasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seyssel_M/0/1/0/all/0/1\">Maureen de Seyssel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavechin_M/0/1/0/all/0/1\">Marvin Lavechin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titeux_H/0/1/0/all/0/1\">Hadrien Titeux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1\">Arthur Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virlet_G/0/1/0/all/0/1\">Gwendal Virlet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revilla_A/0/1/0/all/0/1\">Andrea Santos Revilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisniewski_G/0/1/0/all/0/1\">Guillaume Wisniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludusan_B/0/1/0/all/0/1\">Bogdan Ludusan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning model for Mongolian Citizens Feedback Analysis using Word Vector Embeddings. (arXiv:2302.12069v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12069","description":"<p>A large amount of feedback was collected over the years. Many feedback\nanalysis models have been developed focusing on the English language.\nRecognizing the concept of feedback is challenging and crucial in languages\nwhich do not have applicable corpus and tools employed in Natural Language\nProcessing (i.e., vocabulary corpus, sentence structure rules, etc). However,\nin this paper, we study a feedback classification in Mongolian language using\ntwo different word embeddings for deep learning. We compare the results of\nproposed approaches. We use feedback data in Cyrillic collected from 2012-2018.\nThe result indicates that word embeddings using their own dataset improve the\ndeep learning based proposed model with the best accuracy of 80.1% and 82.7%\nfor two classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dashdorj_Z/0/1/0/all/0/1\">Zolzaya Dashdorj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munkhbayar_T/0/1/0/all/0/1\">Tsetsentsengel Munkhbayar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grigorev_S/0/1/0/all/0/1\">Stanislav Grigorev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (arXiv:2302.12095v1 [cs.AI])","link":"http://arxiv.org/abs/2302.12095","description":"<p>ChatGPT is a recent chatbot service released by OpenAI and is receiving\nincreasing attention over the past few months. While evaluations of various\naspects of ChatGPT have been done, its robustness, i.e., the performance when\nfacing unexpected inputs, is still unclear to the public. Robustness is of\nparticular concern in responsible AI, especially for safety-critical\napplications. In this paper, we conduct a thorough evaluation of the robustness\nof ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To\ndo so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial\nrobustness and the Flipkart review and DDXPlus medical diagnosis datasets for\nOOD evaluation. We select several popular foundation models as baselines.\nResults show that ChatGPT does not show consistent advantages on adversarial\nand OOD classification tasks, while performing well on translation tasks. This\nsuggests that adversarial and OOD robustness remains a significant threat to\nfoundation models. Moreover, ChatGPT shows astounding performance in\nunderstanding dialogue-related texts and we find that it tends to provide\ninformal suggestions for medical tasks instead of definitive answers. Finally,\nwe present in-depth discussions of possible research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xixu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Runkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haojun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxin Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KHAN: Knowledge-Aware Hierarchical Attention Networks for Political Stance Prediction. (arXiv:2302.12126v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12126","description":"<p>The political stance prediction for news articles has been widely studied to\nmitigate the echo chamber effect -- people fall into their thoughts and\nreinforce their pre-existing beliefs. The previous works for the political\nstance problem focus on (1) identifying political factors that could reflect\nthe political stance of a news article and (2) capturing those factors\neffectively. Despite their empirical successes, they are not sufficiently\njustified in terms of how effective their identified factors are in the\npolitical stance prediction. Motivated by this, in this work, we conduct a user\nstudy to investigate important factors in political stance prediction, and\nobserve that the context and tone of a news article (implicit) and external\nknowledge for real-world entities appearing in the article (explicit) are\nimportant in determining its political stance. Based on this observation, we\npropose a novel knowledge-aware approach to political stance prediction (KHAN),\nemploying (1) hierarchical attention networks (HAN) to learn the relationships\namong words and sentences in three different levels and (2) knowledge encoding\n(KE) to incorporate external knowledge for real-world entities into the process\nof political stance prediction. Also, to take into account the subtle and\nimportant difference between opposite political stances, we build two\nindependent political knowledge graphs (KG) (i.e., KG-lib and KG-con) by\nourselves and learn to fuse the different political knowledge. Through\nextensive evaluations on three real-world datasets, we demonstrate the\nsuperiority of DASH in terms of (1) accuracy, (2) efficiency, and (3)\neffectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_Y/0/1/0/all/0/1\">Yunyong Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_S/0/1/0/all/0/1\">Seongeun Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soeun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_Y/0/1/0/all/0/1\">Yeongseung Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sohyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kyungsik Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sang-Wook Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Generalization Ability of Retrieval-Enhanced Transformers. (arXiv:2302.12128v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12128","description":"<p>Recent work on the Retrieval-Enhanced Transformer (RETRO) model has shown\nthat off-loading memory from trainable weights to a retrieval database can\nsignificantly improve language modeling and match the performance of\nnon-retrieval models that are an order of magnitude larger in size. It has been\nsuggested that at least some of this performance gain is due to non-trivial\ngeneralization based on both model weights and retrieval. In this paper, we try\nto better understand the relative contributions of these two components. We\nfind that the performance gains from retrieval largely originate from\noverlapping tokens between the database and the test data, suggesting less\nnon-trivial generalization than previously assumed. More generally, our results\npoint to the challenges of evaluating the generalization of retrieval-augmented\nlanguage models such as RETRO, as even limited token overlap may significantly\ndecrease test-time loss. We release our code and model at\nhttps://github.com/TobiasNorlund/retro\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Norlund_T/0/1/0/all/0/1\">Tobias Norlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doostmohammadi_E/0/1/0/all/0/1\">Ehsan Doostmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansson_R/0/1/0/all/0/1\">Richard Johansson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhlmann_M/0/1/0/all/0/1\">Marco Kuhlmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prosodic features improve sentence segmentation and parsing. (arXiv:2302.12165v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12165","description":"<p>Parsing spoken dialogue presents challenges that parsing text does not,\nincluding a lack of clear sentence boundaries. We know from previous work that\nprosody helps in parsing single sentences (Tran et al. 2018), but we want to\nshow the effect of prosody on parsing speech that isn't segmented into\nsentences. In experiments on the English Switchboard corpus, we find prosody\nhelps our model both with parsing and with accurately identifying sentence\nboundaries. However, we find that the best-performing parser is not necessarily\nthe parser that produces the best sentence segmentation performance. We suggest\nthat the best parses instead come from modelling sentence boundaries jointly\nwith other constituent boundaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_E/0/1/0/all/0/1\">Elizabeth Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwater_S/0/1/0/all/0/1\">Sharon Goldwater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models. (arXiv:2302.12173v1 [cs.CR])","link":"http://arxiv.org/abs/2302.12173","description":"<p>We are currently witnessing dramatic advances in the capabilities of Large\nLanguage Models (LLMs). They are already being adopted in practice and\nintegrated into many systems, including integrated development environments\n(IDEs) and search engines. The functionalities of current LLMs can be modulated\nvia natural language prompts, while their exact internal functionality remains\nimplicit and unassessable. This property, which makes them adaptable to even\nunseen tasks, might also make them susceptible to targeted adversarial\nprompting. Recently, several ways to misalign LLMs using Prompt Injection (PI)\nattacks have been introduced. In such attacks, an adversary can prompt the LLM\nto produce malicious content or override the original instructions and the\nemployed filtering schemes. Recent work showed that these attacks are hard to\nmitigate, as state-of-the-art LLMs are instruction-following. So far, these\nattacks assumed that the adversary is directly prompting the LLM.\n</p>\n<p>In this work, we show that augmenting LLMs with retrieval and API calling\ncapabilities (so-called Application-Integrated LLMs) induces a whole new set of\nattack vectors. These LLMs might process poisoned content retrieved from the\nWeb that contains malicious prompts pre-injected and selected by adversaries.\nWe demonstrate that an attacker can indirectly perform such PI attacks. Based\non this key insight, we systematically analyze the resulting threat landscape\nof Application-Integrated LLMs and discuss a variety of new attack vectors. To\ndemonstrate the practical viability of our attacks, we implemented specific\ndemonstrations of the proposed attacks within synthetic applications. In\nsummary, our work calls for an urgent evaluation of current mitigation\ntechniques and an investigation of whether new techniques are needed to defend\nLLMs against these threats.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Greshake_K/0/1/0/all/0/1\">Kai Greshake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelnabi_S/0/1/0/all/0/1\">Sahar Abdelnabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shailesh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Endres_C/0/1/0/all/0/1\">Christoph Endres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holz_T/0/1/0/all/0/1\">Thorsten Holz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple and Scalable Nearest Neighbor Machine Translation. (arXiv:2302.12188v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12188","description":"<p>$k$NN-MT is a straightforward yet powerful approach for fast domain\nadaptation, which directly plugs pre-trained neural machine translation (NMT)\nmodels with domain-specific token-level $k$-nearest-neighbor ($k$NN) retrieval\nto achieve domain adaptation without retraining. Despite being conceptually\nattractive, $k$NN-MT is burdened with massive storage requirements and high\ncomputational complexity since it conducts nearest neighbor searches over the\nentire reference corpus. In this paper, we propose a simple and scalable\nnearest neighbor machine translation framework to drastically promote the\ndecoding and storage efficiency of $k$NN-based models while maintaining the\ntranslation performance. To this end, we dynamically construct an extremely\nsmall datastore for each input via sentence-level retrieval to avoid searching\nthe entire datastore in vanilla $k$NN-MT, based on which we further introduce a\ndistance-aware adapter to adaptively incorporate the $k$NN retrieval results\ninto the pre-trained NMT models. Experiments on machine translation in two\ngeneral settings, static domain adaptation and online learning, demonstrate\nthat our proposed approach not only achieves almost 90% speed as the NMT model\nwithout performance degradation, but also significantly reduces the storage\nrequirements of $k$NN-MT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuhan Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiuzhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1\">Qu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yichao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HL Dataset: Grounding High-Level Linguistic Concepts in Vision. (arXiv:2302.12189v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12189","description":"<p>Current captioning datasets, focus on object-centric captions, describing the\nvisible objects in the image, often ending up stating the obvious (for humans),\ne.g. \"people eating food in a park\". Although these datasets are useful to\nevaluate the ability of Vision &amp; Language models to recognize the visual\ncontent, they lack in expressing trivial abstract concepts, e.g. \"people having\na picnic\". Such concepts are licensed by human's personal experience and\ncontribute to forming common sense assumptions. We present the High-Level\nDataset; a dataset extending 14997 images of the COCO dataset with 134973\nhuman-annotated (high-level) abstract captions collected along three axes:\nscenes, actions and rationales. We describe and release such dataset and we\nshow how it can be used to assess models' multimodal grounding of abstract\nconcepts and enrich models' visio-lingusitic representations. Moreover, we\ndescribe potential tasks enabled by this dataset involving high- and low-level\nconcepts interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cafagna_M/0/1/0/all/0/1\">Michele Cafagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCWDST: a Minimum-Cost Weighted Directed Spanning Tree Algorithm for Real-Time Fake News Mitigation in Social Media. (arXiv:2302.12190v1 [cs.SI])","link":"http://arxiv.org/abs/2302.12190","description":"<p>The widespread availability of internet access and handheld devices confers\nto social media a power similar to the one newspapers used to have. People seek\naffordable information on social media and can reach it within seconds. Yet\nthis convenience comes with dangers; any user may freely post whatever they\nplease and the content can stay online for a long period, regardless of its\ntruthfulness. A need to detect untruthful information, also known as fake news,\narises. In this paper, we present an end-to-end solution that accurately\ndetects fake news and immunizes network nodes that spread them in real-time. To\ndetect fake news, we propose two new stack deep learning architectures that\nutilize convolutional and bidirectional LSTM layers. To mitigate the spread of\nfake news, we propose a real-time network-aware strategy that (1) constructs a\nminimum-cost weighted directed spanning tree for a detected node, and (2)\nimmunizes nodes in that tree by scoring their harmfulness using a novel ranking\nfunction. We demonstrate the effectiveness of our solution on five real-world\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truica_C/0/1/0/all/0/1\">Ciprian-Octavian Truic&#x103;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostol_E/0/1/0/all/0/1\">Elena-Simona Apostol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolescu_R/0/1/0/all/0/1\">Radu-C&#x103;t&#x103;lin Nicolescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karras_P/0/1/0/all/0/1\">Panagiotis Karras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neural Span-Based Continual Named Entity Recognition Model. (arXiv:2302.12200v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12200","description":"<p>Named Entity Recognition (NER) models capable of Continual Learning (CL) are\nrealistically valuable in areas where entity types continuously increase (e.g.,\npersonal assistants). Meanwhile the learning paradigm of NER advances to new\npatterns such as the span-based methods. However, its potential to CL has not\nbeen fully explored. In this paper, we propose SpanKL1, a simple yet effective\nSpan-based model with Knowledge distillation (KD) to preserve memories and\nmulti-Label prediction to prevent conflicts in CL-NER. Unlike prior sequence\nlabeling approaches, the inherently independent modeling in span and entity\nlevel with the designed coherent optimization on SpanKL promotes its learning\nat each incremental step and mitigates the forgetting. Experiments on synthetic\nCL datasets derived from OntoNotes and Few-NERD show that SpanKL significantly\noutperforms previous SoTA in many aspects, and obtains the smallest gap from CL\nto the upper bound revealing its high practiced value.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Nearest Neighbor Machine Translation. (arXiv:2302.12211v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12211","description":"<p>To protect user privacy and meet legal regulations, federated learning (FL)\nis attracting significant attention. Training neural machine translation (NMT)\nmodels with traditional FL algorithm (e.g., FedAvg) typically relies on\nmulti-round model-based interactions. However, it is impractical and\ninefficient for machine translation tasks due to the vast communication\noverheads and heavy synchronization. In this paper, we propose a novel\nfederated nearest neighbor (FedNN) machine translation framework that, instead\nof multi-round model-based interactions, leverages one-round memorization-based\ninteraction to share knowledge across different clients to build low-overhead\nprivacy-preserving systems. The whole approach equips the public NMT model\ntrained on large-scale accessible data with a $k$-nearest-neighbor ($$kNN)\nclassifier and integrates the external datastore constructed by private text\ndata in all clients to form the final FL model. A two-phase datastore\nencryption strategy is introduced to achieve privacy-preserving during this\nprocess. Extensive experiments show that FedNN significantly reduces\ncomputational and communication costs compared with FedAvg, while maintaining\npromising performance in different FL settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yichao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bingzhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What makes a language easy to deep-learn?. (arXiv:2302.12239v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12239","description":"<p>Neural networks drive the success of natural language processing. A\nfundamental property of natural languages is their compositional structure,\nallowing us to describe new meanings systematically. However, neural networks\nnotoriously struggle with systematic generalization and do not necessarily\nbenefit from a compositional structure in emergent communication simulations.\nHere, we test how neural networks compare to humans in learning and\ngeneralizing a new language. We do this by closely replicating an artificial\nlanguage learning study (conducted originally with human participants) and\nevaluating the memorization and generalization capabilities of deep neural\nnetworks with respect to the degree of structure in the input language. Our\nresults show striking similarities between humans and deep neural networks:\nMore structured linguistic input leads to more systematic generalization and\nbetter convergence between humans and neural network agents and between\ndifferent neural agents. We then replicate this structure bias found in humans\nand our recurrent neural networks with a Transformer-based large language model\n(GPT-3), showing a similar benefit for structured linguistic input regarding\ngeneralization systematicity and memorization errors. These findings show that\nthe underlying structure of languages is crucial for systematic generalization.\nDue to the correlation between community size and linguistic structure in\nnatural languages, our findings underscore the challenge of automated\nprocessing of low-resource languages. Nevertheless, the similarity between\nhumans and machines opens new avenues for language evolution research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_Y/0/1/0/all/0/1\">Yoav Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviv_L/0/1/0/all/0/1\">Limor Raviv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Prompting with Chain-of-Thought for Large Language Models. (arXiv:2302.12246v1 [cs.CL])","link":"http://arxiv.org/abs/2302.12246","description":"<p>The increasing scale of large language models (LLMs) brings emergent\nabilities to various complex tasks requiring reasoning, such as arithmetic and\ncommonsense reasoning. It is known that the effective design of task-specific\nprompts is critical for LLMs' ability to produce high-quality answers. In\nparticular, an effective approach for complex question-and-answer tasks is\nexample-based prompting with chain-of-thought (CoT) reasoning, which\nsignificantly improves the performance of LLMs. However, current CoT methods\nrely on a fixed set of human-annotated exemplars, which are not necessarily the\nmost effective examples for different tasks. This paper proposes a new method,\nActive-Prompt, to adapt LLMs to different tasks with task-specific example\nprompts (annotated with human-designed CoT reasoning). For this purpose, we\npropose a solution to the key problem of determining which questions are the\nmost important and helpful ones to annotate from a pool of task-specific\nqueries. By borrowing ideas from the related problem of uncertainty-based\nactive learning, we introduce several metrics to characterize the uncertainty\nso as to select the most uncertain questions for annotation. Experimental\nresults demonstrate the superiority of our proposed method, achieving\nstate-of-the-art on eight complex reasoning tasks. Further analyses of\ndifferent uncertainty metrics, pool sizes, zero-shot learning, and\naccuracy-uncertainty relationship demonstrate the effectiveness of our method.\nOur code will be available at https://github.com/shizhediao/active-cot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1\">Shizhe Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengcheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying & Modeling Feature Interactions: An Information Decomposition Framework. (arXiv:2302.12247v1 [cs.LG])","link":"http://arxiv.org/abs/2302.12247","description":"<p>The recent explosion of interest in multimodal applications has resulted in a\nwide selection of datasets and methods for representing and integrating\ninformation from different signals. Despite these empirical advances, there\nremain fundamental research questions: how can we quantify the nature of\ninteractions that exist among input features? Subsequently, how can we capture\nthese interactions using suitable data-driven methods? To answer this question,\nwe propose an information-theoretic approach to quantify the degree of\nredundancy, uniqueness, and synergy across input features, which we term the\nPID statistics of a multimodal distribution. Using 2 newly proposed estimators\nthat scale to high-dimensional distributions, we demonstrate their usefulness\nin quantifying the interactions within multimodal datasets, the nature of\ninteractions captured by multimodal models, and principled approaches for model\nselection. We conduct extensive experiments on both synthetic datasets where\nthe PID statistics are known and on large-scale multimodal benchmarks where PID\nestimation was previously impossible. Finally, to demonstrate the real-world\napplicability of our approach, we present three case studies in pathology, mood\nprediction, and robotic perception where our framework accurately recommends\nstrong multimodal models for each application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1\">Chun Kai Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Suzanne Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Richard Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Explainable Phrasal Reasoning with Neural Fuzzy Logic. (arXiv:2109.08927v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08927","description":"<p>Natural language inference (NLI) aims to determine the logical relationship\nbetween two sentences, such as Entailment, Contradiction, and Neutral. In\nrecent years, deep learning models have become a prevailing approach to NLI,\nbut they lack interpretability and explainability. In this work, we address the\nexplainability of NLI by weakly supervised logical reasoning, and propose an\nExplainable Phrasal Reasoning (EPR) approach. Our model first detects phrases\nas the semantic unit and aligns corresponding phrases in the two sentences.\nThen, the model predicts the NLI label for the aligned phrases, and induces the\nsentence label by fuzzy logic formulas. Our EPR is almost everywhere\ndifferentiable and thus the system can be trained end to end. In this way, we\nare able to provide explicit explanations of phrasal logical relationships in a\nweakly supervised manner. We further show that such reasoning results help\ntextual explanation generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zi Xuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Atharva Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_Z/0/1/0/all/0/1\">Zhijian Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firdaus_M/0/1/0/all/0/1\">Mauajama Firdaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Black-box Prompt Learning for Pre-trained Language Models. (arXiv:2201.08531v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.08531","description":"<p>The increasing scale of general-purpose Pre-trained Language Models (PLMs)\nnecessitates the study of more efficient adaptation across different downstream\ntasks. In this paper, we establish a Black-box Discrete Prompt Learning (BDPL)\nto resonate with pragmatic interactions between the cloud infrastructure and\nedge devices. Particularly, instead of fine-tuning the model in the cloud, we\nadapt PLMs by prompt learning, which efficiently optimizes only a few\nparameters of the discrete prompts. Moreover, we consider the scenario that we\ndo not have access to the parameters and gradients of the pre-trained models,\nexcept for its outputs given inputs. This black-box setting secures the cloud\ninfrastructure from potential attack and misuse to cause a single-point\nfailure, which is preferable to the white-box counterpart by current\ninfrastructures. Under this black-box constraint, we apply a variance-reduced\npolicy gradient algorithm to estimate the gradients of parameters in the\ncategorical distribution of each discrete prompt. In light of our method, the\nuser devices can efficiently tune their tasks by querying the PLMs bounded by a\nrange of API calls. Our experiments on RoBERTa and GPT-3 demonstrate that the\nproposed algorithm achieves significant improvement on eight benchmarks in a\ncloud-device collaboration manner. Finally, we conduct in-depth case studies to\ncomprehensively analyze our method in terms of various data sizes, prompt\nlengths, training budgets, optimization objectives, prompt transferability, and\nexplanations of the learned prompts. Our code will be available at\nhttps://github.com/shizhediao/Black-Box-Prompt-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1\">Shizhe Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhichao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender Bias in Text: Labeled Datasets and Lexicons. (arXiv:2201.08675v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.08675","description":"<p>Language has a profound impact on our thoughts, perceptions, and conceptions\nof gender roles. Gender-inclusive language is, therefore, a key tool to promote\nsocial inclusion and contribute to achieving gender equality. Consequently,\ndetecting and mitigating gender bias in texts is instrumental in halting its\npropagation and societal implications. However, there is a lack of gender bias\ndatasets and lexicons for automating the detection of gender bias using\nsupervised and unsupervised machine learning (ML) and natural language\nprocessing (NLP) techniques. Therefore, the main contribution of this work is\nto publicly provide labeled datasets and exhaustive lexicons by collecting,\nannotating, and augmenting relevant sentences to facilitate the detection of\ngender bias in English text. Towards this end, we present an updated version of\nour previously proposed taxonomy by re-formalizing its structure, adding a new\nbias type, and mapping each bias subtype to an appropriate detection\nmethodology. The released datasets and lexicons span multiple bias subtypes\nincluding: Generic He, Generic She, Explicit Marking of Sex, and Gendered\nNeologisms. We leveraged the use of word embedding models to further augment\nthe collected lexicons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doughman_J/0/1/0/all/0/1\">Jad Doughman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khreich_W/0/1/0/all/0/1\">Wael Khreich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Words are all you need? Language as an approximation for human similarity judgments. (arXiv:2206.04105v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.04105","description":"<p>Human similarity judgments are a powerful supervision signal for machine\nlearning applications based on techniques such as contrastive learning,\ninformation retrieval, and model alignment, but classical methods for\ncollecting human similarity judgments are too expensive to be used at scale.\nRecent methods propose using pre-trained deep neural networks (DNNs) to\napproximate human similarity, but pre-trained DNNs may not be available for\ncertain domains (e.g., medical images, low-resource languages) and their\nperformance in approximating human similarity has not been extensively tested.\nWe conducted an evaluation of 611 pre-trained models across three domains --\nimages, audio, video -- and found that there is a large gap in performance\nbetween human similarity judgments and pre-trained DNNs. To address this gap,\nwe propose a new class of similarity approximation methods based on language.\nTo collect the language data required by these new methods, we also developed\nand validated a novel adaptive tag collection pipeline. We find that our\nproposed language-based methods are significantly cheaper, in the number of\nhuman judgments, than classical methods, but still improve performance over the\nDNN-based methods. Finally, we also develop `stacked' methods that combine\nlanguage embeddings with DNN embeddings, and find that these consistently\nprovide the best approximations for human similarity across all three of our\nmodalities. Based on the results of this comprehensive study, we provide a\nconcise guide for researchers interested in collecting or approximating human\nsimilarity data. To accompany this guide, we also release all of the similarity\nand language data, a total of 206,339 human judgments, that we collected in our\nexperiments, along with a detailed breakdown of all modeling results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1\">Raja Marjieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijn_P/0/1/0/all/0/1\">Pol van Rijn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sucholutsky_I/0/1/0/all/0/1\">Ilia Sucholutsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumers_T/0/1/0/all/0/1\">Theodore R. Sumers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Harin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BaIT: Barometer for Information Trustworthiness. (arXiv:2206.07535v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.07535","description":"<p>This paper presents a new approach to the FNC-1 fake news classification task\nwhich involves employing pre-trained encoder models from similar NLP tasks,\nnamely sentence similarity and natural language inference, and two neural\nnetwork architectures using this approach are proposed. Methods in data\naugmentation are explored as a means of tackling class imbalance in the\ndataset, employing common pre-existing methods and proposing a method for\nsample generation in the under-represented class using a novel sentence\nnegation algorithm. Comparable overall performance with existing baselines is\nachieved, while significantly increasing accuracy on an under-represented but\nnonetheless important class for FNC-1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nolan_O/0/1/0/all/0/1\">Ois&#xed;n Nolan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mourik_J/0/1/0/all/0/1\">Jeroen van Mourik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tilbury_C/0/1/0/all/0/1\">Callum Rhys Tilbury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamically Retrieving Knowledge via Query Generation for Informative Dialogue Generation. (arXiv:2208.00128v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.00128","description":"<p>Knowledge-driven dialog system has recently made remarkable breakthroughs.\nCompared with general dialog systems, superior knowledge-driven dialog systems\ncan generate more informative and knowledgeable responses with pre-provided\nknowledge. However, in practical applications, the dialog system cannot be\nprovided with corresponding knowledge in advance because it cannot know in\nadvance the development of the conversation. Therefore, in order to make the\nknowledge dialogue system more practical, it is vital to find a way to retrieve\nrelevant knowledge based on the dialogue history. To solve this problem, we\ndesign a knowledge-driven dialog system named DRKQG (Dynamically Retrieving\nKnowledge via Query Generation for informative dialog response). Specifically,\nthe system can be divided into two modules: the query generation module and the\ndialog generation module. First, a time-aware mechanism is utilized to capture\ncontext information, and a query can be generated for retrieving knowledge\nthrough search engine. Then, we integrate the copy mechanism and transformers,\nwhich allows the response generation module to produce responses derived from\nthe context and retrieved knowledge. Experimental results at LIC2022, Language\nand Intelligence Technology Competition, show that our module outperforms the\nbaseline model by a large margin on automatic evaluation metrics, while human\nevaluation by the Baidu Linguistics team shows that our system achieves\nimpressive results in Factually Correct and Knowledgeable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhongtian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lifang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yushuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ronghan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Meng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xinyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zejun Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PyABSA: A Modularized Framework for Reproducible Aspect-based Sentiment Analysis. (arXiv:2208.01368v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.01368","description":"<p>The advancement of aspect-based sentiment analysis (ABSA) has urged the lack\nof a user-friendly framework that can largely lower the difficulty of\nreproducing state-of-the-art ABSA performance, especially for beginners. To\nmeet the demand, we present \\our, a modularized framework built on PyTorch for\nreproducible ABSA. To facilitate ABSA research, PyABSA supports several ABSA\nsubtasks, including aspect term extraction, aspect sentiment classification,\nand end-to-end aspect-based sentiment analysis. Concretely, PyABSA integrates\n29 models and 26 datasets. With just a few lines of code, the result of a model\non a specific dataset can be reproduced. With a modularized design, PyABSA can\nalso be flexiblely extended to considered models, datasets, and other related\ntasks. Besides, PyABSA highlights its data augmentation and annotation\nfeatures, which significantly address data scarity. All are welcome to have a\ntry at \\url{https://github.com/yangheng95/PyABSA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Heng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Bi-Encoder for Named Entity Recognition via Contrastive Learning. (arXiv:2208.14565v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.14565","description":"<p>We present a bi-encoder framework for named entity recognition (NER), which\napplies contrastive learning to map candidate text spans and entity types into\nthe same vector representation space. Prior work predominantly approaches NER\nas sequence labeling or span classification. We instead frame NER as a\nrepresentation learning problem that maximizes the similarity between the\nvector representations of an entity mention and its type. This makes it easy to\nhandle nested and flat NER alike, and can better leverage noisy\nself-supervision signals. A major challenge to this bi-encoder formulation for\nNER lies in separating non-entity spans from entity mentions. Instead of\nexplicitly labeling all non-entity spans as the same class $\\texttt{Outside}$\n($\\texttt{O}$) as in most prior methods, we introduce a novel dynamic\nthresholding loss. Experiments show that our method performs well in both\nsupervised and distantly supervised settings, for nested and flat NER alike,\nestablishing new state of the art across standard datasets in the general\ndomain (e.g., ACE2004, ACE2005) and high-value verticals such as biomedicine\n(e.g., GENIA, NCBI, BC5CDR, JNLPBA). We release the code at\ngithub.com/microsoft/binder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation between Spoken Languages and Signed Languages Represented in SignWriting. (arXiv:2210.05404v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05404","description":"<p>This paper presents work on novel machine translation (MT) systems between\nspoken and signed languages, where signed languages are represented in\nSignWriting, a sign language writing system. Our work seeks to address the lack\nof out-of-the-box support for signed languages in current MT systems and is\nbased on the SignBank dataset, which contains pairs of spoken language text and\nSignWriting content. We introduce novel methods to parse, factorize, decode,\nand evaluate SignWriting, leveraging ideas from neural factored MT. In a\nbilingual setup--translating from American Sign Language to (American)\nEnglish--our method achieves over 30 BLEU, while in two multilingual\nsetups--translating in both directions between spoken languages and signed\nlanguages--we achieve over 20 BLEU. We find that common MT techniques used to\nimprove spoken language translation similarly affect the performance of sign\nlanguage translation. These findings validate our use of an intermediate text\nrepresentation for signed languages to include them in natural language\nprocessing research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moryossef_A/0/1/0/all/0/1\">Amit Moryossef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Mathias M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebling_S/0/1/0/all/0/1\">Sarah Ebling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook. (arXiv:2210.13623v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2210.13623","description":"<p>In recent years, reinforcement learning and bandits have transformed a wide\nrange of real-world applications including healthcare, finance, recommendation\nsystems, robotics, and last but not least, the speech and natural language\nprocessing. While most speech and language applications of reinforcement\nlearning algorithms are centered around improving the training of deep neural\nnetworks with its flexible optimization properties, there are still many\ngrounds to explore to utilize the benefits of reinforcement learning, such as\nits reward-driven adaptability, state representations, temporal structures and\ngeneralizability. In this survey, we present an overview of recent advancements\nof reinforcement learning and bandits, and discuss how they can be effectively\nemployed to solve speech and natural language processing problems with models\nthat are adaptive, interactive and scalable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Converge to the Truth: Factual Error Correction via Iterative Constrained Editing. (arXiv:2211.12130v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.12130","description":"<p>Given a possibly false claim sentence, how can we automatically correct it\nwith minimal editing? Existing methods either require a large number of pairs\nof false and corrected claims for supervised training or do not handle well\nerrors spanning over multiple tokens within an utterance. In this paper, we\npropose VENCE, a novel method for factual error correction (FEC) with minimal\nedits. VENCE formulates the FEC problem as iterative sampling editing actions\nwith respect to a target density function. We carefully design the target\nfunction with predicted truthfulness scores from an offline trained fact\nverification model. VENCE samples the most probable editing positions based on\nback-calculated gradients of the truthfulness score concerning input tokens and\nthe editing actions using a distantly-supervised language model (T5).\nExperiments on a public dataset show that VENCE improves the well-adopted SARI\nmetric by 5.3 (or a relative improvement of 11.8%) over the previous best\ndistantly-supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenxuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and accurate factorized neural transducer for text adaption of end-to-end speech recognition models. (arXiv:2212.01992v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01992","description":"<p>Neural transducer is now the most popular end-to-end model for speech\nrecognition, due to its naturally streaming ability. However, it is challenging\nto adapt it with text-only data. Factorized neural transducer (FNT) model was\nproposed to mitigate this problem. The improved adaptation ability of FNT on\ntext-only adaptation data came at the cost of lowered accuracy compared to the\nstandard neural transducer model. We propose several methods to improve the\nperformance of the FNT model. They are: adding CTC criterion during training,\nadding KL divergence loss during adaptation, using a pre-trained language model\nto seed the vocabulary predictor, and an efficient adaptation approach by\ninterpolating the vocabulary predictor with the n-gram language model. A\ncombination of these approaches results in a relative word-error-rate reduction\nof 9.48\\% from the standard FNT model. Furthermore, n-gram interpolation with\nthe vocabulary predictor improves the adaptation speed hugely with satisfactory\nadaptation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_P/0/1/0/all/0/1\">Partha Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miljanic_V/0/1/0/all/0/1\">Veljko Miljanic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03494","description":"<p>Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,\nwhich is the focus of this study. Eleven categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KG-ECO: Knowledge Graph Enhanced Entity Correction for Query Rewriting. (arXiv:2302.10454v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.10454","description":"<p>Query Rewriting (QR) plays a critical role in large-scale dialogue systems\nfor reducing frictions. When there is an entity error, it imposes extra\nchallenges for a dialogue system to produce satisfactory responses. In this\nwork, we propose KG-ECO: Knowledge Graph enhanced Entity COrrection for query\nrewriting, an entity correction system with corrupt entity span detection and\nentity retrieval/re-ranking functionalities. To boost the model performance, we\nincorporate Knowledge Graph (KG) to provide entity structural information\n(neighboring entities encoded by graph neural networks) and textual information\n(KG entity descriptions encoded by RoBERTa). Experimental results show that our\napproach yields a clear performance gain over two baselines: utterance level QR\nand entity correction without utilizing KG information. The proposed system is\nparticularly effective for few-shot learning cases where target entities are\nrarely seen in training or there is a KG relation between the target entity and\nother contextual entities in the query.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jinglun Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Ziyan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_E/0/1/0/all/0/1\">Eunah Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chenlei Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of Subword Pooling Strategy on Cross-lingual Event Detection. (arXiv:2302.11365v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.11365","description":"<p>Pre-trained multilingual language models (e.g., mBERT, XLM-RoBERTa) have\nsignificantly advanced the state-of-the-art for zero-shot cross-lingual\ninformation extraction. These language models ubiquitously rely on word\nsegmentation techniques that break a word into smaller constituent subwords.\nTherefore, all word labeling tasks (e.g. named entity recognition, event\ndetection, etc.), necessitate a pooling strategy that takes the subword\nrepresentations as input and outputs a representation for the entire word.\nTaking the task of cross-lingual event detection as a motivating example, we\nshow that the choice of pooling strategy can have a significant impact on the\ntarget language performance. For example, the performance varies by up to 16\nabsolute $f_{1}$ points depending on the pooling strategy when training in\nEnglish and testing in Arabic on the ACE task. We carry out our analysis with\nfive different pooling strategies across nine languages in diverse\nmulti-lingual datasets. Across configurations, we find that the canonical\nstrategy of taking just the first subword to represent the entire word is\nusually sub-optimal. On the other hand, we show that attention pooling is\nrobust to language and dataset variations by being either the best or close to\nthe optimal strategy. For reproducibility, we make our code available at\nhttps://github.com/isi-boston/ed-pooling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shantanu Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fincke_S/0/1/0/all/0/1\">Steven Fincke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_C/0/1/0/all/0/1\">Chris Jenkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_S/0/1/0/all/0/1\">Scott Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}}]}]}