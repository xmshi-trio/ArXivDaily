{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-05T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"An ensemble-based framework for mispronunciation detection of Arabic phonemes. (arXiv:2301.01378v1 [cs.SD])","link":"http://arxiv.org/abs/2301.01378","description":"<p>Determination of mispronunciations and ensuring feedback to users are\nmaintained by computer-assisted language learning (CALL) systems. In this work,\nwe introduce an ensemble model that defines the mispronunciation of Arabic\nphonemes and assists learning of Arabic, effectively. To the best of our\nknowledge, this is the very first attempt to determine the mispronunciations of\nArabic phonemes employing ensemble learning techniques and conventional machine\nlearning models, comprehensively. In order to observe the effect of feature\nextraction techniques, mel-frequency cepstrum coefficients (MFCC), and Mel\nspectrogram are blended with each learning algorithm. To show the success of\nproposed model, 29 letters in the Arabic phonemes, 8 of which are hafiz, are\nvoiced by a total of 11 different person. The amount of data set has been\nenhanced employing the methods of adding noise, time shifting, time stretching,\npitch shifting. Extensive experiment results demonstrate that the utilization\nof voting classifier as an ensemble algorithm with Mel spectrogram feature\nextraction technique exhibits remarkable classification result with 95.9% of\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calik_S/0/1/0/all/0/1\">Sukru Selim Calik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucukmanisa_A/0/1/0/all/0/1\">Ayhan Kucukmanisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilimci_Z/0/1/0/all/0/1\">Zeynep Hilal Kilimci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-Visual Efficient Conformer for Robust Speech Recognition. (arXiv:2301.01456v1 [cs.CV])","link":"http://arxiv.org/abs/2301.01456","description":"<p>End-to-end Automatic Speech Recognition (ASR) systems based on neural\nnetworks have seen large improvements in recent years. The availability of\nlarge scale hand-labeled datasets and sufficient computing resources made it\npossible to train powerful deep neural networks, reaching very low Word Error\nRate (WER) on academic benchmarks. However, despite impressive performance on\nclean audio samples, a drop of performance is often observed on noisy speech.\nIn this work, we propose to improve the noise robustness of the recently\nproposed Efficient Conformer Connectionist Temporal Classification (CTC)-based\narchitecture by processing both audio and visual modalities. We improve\nprevious lip reading methods using an Efficient Conformer back-end on top of a\nResNet-18 visual front-end and by adding intermediate CTC losses between\nblocks. We condition intermediate block features on early predictions using\nInter CTC residual modules to relax the conditional independence assumption of\nCTC-based models. We also replace the Efficient Conformer grouped attention by\na more efficient and simpler attention mechanism that we call patch attention.\nWe experiment with publicly available Lip Reading Sentences 2 (LRS2) and Lip\nReading Sentences 3 (LRS3) datasets. Our experiments show that using audio and\nvisual modalities allows to better recognize speech in the presence of\nenvironmental noise and significantly accelerate training, reaching lower WER\nwith 4 times less training steps. Our Audio-Visual Efficient Conformer (AVEC)\nmodel achieves state-of-the-art performance, reaching WER of 2.3% and 1.8% on\nLRS2 and LRS3 test sets. Code and pretrained models are available at\nhttps://github.com/burchim/AVEC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burchi_M/0/1/0/all/0/1\">Maxime Burchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Ambiguity from Crowd Sequential Annotations. (arXiv:2301.01579v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01579","description":"<p>Most crowdsourcing learning methods treat disagreement between annotators as\nnoisy labelings while inter-disagreement among experts is often a good\nindicator for the ambiguity and uncertainty that is inherent in natural\nlanguage. In this paper, we propose a framework called Learning Ambiguity from\nCrowd Sequential Annotations (LA-SCA) to explore the inter-disagreement between\nreliable annotators and effectively preserve confusing label information.\nFirst, a hierarchical Bayesian model is developed to infer ground-truth from\ncrowds and group the annotators with similar reliability together. By modeling\nthe relationship between the size of group the annotator involved in, the\nannotator's reliability and element's unambiguity in each sequence,\ninter-disagreement between reliable annotators on ambiguous elements is\ncomputed to obtain label confusing information that is incorporated to\ncost-sensitive sequence labeling. Experimental results on POS tagging and NER\ntasks show that our proposed framework achieves competitive performance in\ninferring ground-truth from crowds and predicting unknown sequences, and\ninterpreting hierarchical clustering results helps discover labeling patterns\nof annotators with similar reliability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaolei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammar construction methods for extended deterministic expressions. (arXiv:2301.01621v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01621","description":"<p>Extended regular expressions with counting and interleaving are widely used\nin practice. However the related theoretical studies for this kind of\nexpressions currently cannot meet the need of practical work. This paper\ndevelops syntax definitions for extended deterministic expressions and their\nsubclasses, hope to completely solve the long-standing problem that there are\nno syntax definitions for this kind of expressions, which has become an\nimportant reason for restricting the use of extended expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mou_X/0/1/0/all/0/1\">Xiaoying Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Aspect Explainable Inductive Relation Prediction by Sentence Transformer. (arXiv:2301.01664v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01664","description":"<p>Recent studies on knowledge graphs (KGs) show that path-based methods\nempowered by pre-trained language models perform well in the provision of\ninductive and explainable relation predictions. In this paper, we introduce the\nconcepts of relation path coverage and relation path confidence to filter out\nunreliable paths prior to model training to elevate the model performance.\nMoreover, we propose Knowledge Reasoning Sentence Transformer (KRST) to predict\ninductive relations in KGs. KRST is designed to encode the extracted reliable\npaths in KGs, allowing us to properly cluster paths and provide multi-aspect\nexplanations. We conduct extensive experiments on three real-world datasets.\nThe experimental results show that compared to SOTA models, KRST achieves the\nbest performance in most transductive and inductive test cases (4 of 6), and in\n11 of 12 few-shot test cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhixiang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lizhen Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chatbots as Problem Solvers: Playing Twenty Questions with Role Reversals. (arXiv:2301.01743v1 [cs.AI])","link":"http://arxiv.org/abs/2301.01743","description":"<p>New chat AI applications like ChatGPT offer an advanced understanding of\nquestion context and memory across multi-step tasks, such that experiments can\ntest its deductive reasoning. This paper proposes a multi-role and multi-step\nchallenge, where ChatGPT plays the classic twenty-questions game but\ninnovatively switches roles from the questioner to the answerer. The main\nempirical result establishes that this generation of chat applications can\nguess random object names in fewer than twenty questions (average, 12) and\ncorrectly guess 94% of the time across sixteen different experimental setups.\nThe research introduces four novel cases where the chatbot fields the\nquestions, asks the questions, both question-answer roles, and finally tries to\nguess appropriate contextual emotions. One task that humans typically fail but\ntrained chat applications complete involves playing bilingual games of twenty\nquestions (English answers to Spanish questions). Future variations address\ndirect problem-solving using a similar inquisitive format to arrive at novel\noutcomes deductively, such as patentable inventions or combination thinking.\nFeatured applications of this dialogue format include complex protein designs,\nneuroscience metadata, and child development educational materials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKee_F/0/1/0/all/0/1\">Forrest McKee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterated Decomposition: Improving Science Q&A by Supervising Reasoning Processes. (arXiv:2301.01751v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01751","description":"<p>Language models (LMs) can perform complex reasoning either end-to-end, with\nhidden latent state, or compositionally, with transparent intermediate state.\nComposition offers benefits for interpretability and safety, but may need\nworkflow support and infrastructure to remain competitive. We describe iterated\ndecomposition, a human-in-the-loop workflow for developing and refining\ncompositional LM programs. We improve the performance of compositions by\nzooming in on failing components and refining them through decomposition,\nadditional context, chain of thought, etc. To support this workflow, we develop\nICE, an open-source tool for visualizing the execution traces of LM programs.\nWe apply iterated decomposition to three real-world tasks and improve the\naccuracy of LM programs over less compositional baselines: describing the\nplacebo used in a randomized controlled trial (25% to 65%), evaluating\nparticipant adherence to a medical intervention (53% to 70%), and answering NLP\nquestions on the Qasper dataset (38% to 69%). These applications serve as case\nstudies for a workflow that, if automated, could keep ML systems interpretable\nand safe even as they scale to increasingly complex tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reppert_J/0/1/0/all/0/1\">Justin Reppert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rachbach_B/0/1/0/all/0/1\">Ben Rachbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_C/0/1/0/all/0/1\">Charlie George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_L/0/1/0/all/0/1\">Luke Stebbing Jungwon Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Appleton_M/0/1/0/all/0/1\">Maggie Appleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuhlmuller_A/0/1/0/all/0/1\">Andreas Stuhlm&#xfc;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification. (arXiv:2301.01764v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01764","description":"<p>Previous state-of-the-art models for lexical simplification consist of\ncomplex pipelines with several components, each of which requires deep\ntechnical knowledge and fine-tuned interaction to achieve its full potential.\nAs an alternative, we describe a frustratingly simple pipeline based on\nprompted GPT-3 responses, beating competing approaches by a wide margin in\nsettings with few training instances. Our best-performing submission to the\nEnglish language track of the TSAR-2022 shared task consists of an ``ensemble''\nof six different prompt templates with varying context levels. As a\nlate-breaking result, we further detail a language transfer technique that\nallows simplification in languages other than English. Applied to the Spanish\nand Portuguese subset, we achieve state-of-the-art results with only minor\nmodification to the original prompts. Aside from detailing the implementation\nand setup, we spend the remainder of this work discussing the particularities\nof prompting and implications for future work. Code for the experiments is\navailable online \\url{https://github.com/dennlinger/TSAR-2022-Shared-Task}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aumiller_D/0/1/0/all/0/1\">Dennis Aumiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gertz_M/0/1/0/all/0/1\">Michael Gertz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PMC-Patients: A Large-scale Dataset of Patient Notes and Relations Extracted from Case Reports in PubMed Central. (arXiv:2202.13876v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13876","description":"<p>Objective: Data unavailability has been one of the biggest barriers in\nclinical natural language processing. This paper is aimed at providing a\nlarge-scale and publicly available patient note dataset, named PMC-Patients,\nwith relevant articles and similar patients annotations. The ultimate goal of\nPMC-Patients is to facilitate the development of retrieval-based clinical\ndecision support systems. Materials and Methods: To collect PMC-Patients, we\nextract patient notes from case reports in PubMed Central by recognizing\ncertain section patterns. Patient-article relevance and patient-patient\nsimilarity are annotated by citation relationships in PubMed. In addition, we\nperform three tasks with PMC-Patients to demonstrate its utility in providing\nclinical decision support for a given patient, including (1) classifying\nwhether another patient is similar, (2) retrieving similar patients in\nPMC-Patients, and (3) retrieving relevant articles in PubMed. Results: We\ncollect and release PMC-Patients under the CC BY-NC-SA license, which becomes\nthe largest publicly available patient note dataset so far. PMC-Patients\ncontains 167k patient notes that are annotated with 3.1M relevant articles and\n293k similar patients. Qualitative and quantitative analyses reveal the high\nquality and richness of our dataset. Experiments show that classifying the\nsimilarity of patient pairs is relatively easy, but it is hard to retrieve\nsimilar patients or relevant articles for a given patient from a large set of\ncandidates. Conclusion: We present PMC-Patients, a large-scale dataset of\npatient notes with high quality, easy access, diverse conditions, and rich\nannotations. The proposed dataset can also serve as a hard benchmark for\nevaluating retrieval-based clinical decision support systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhengyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1\">Tuorui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GCNet: Graph Completion Network for Incomplete Multimodal Learning in Conversation. (arXiv:2203.02177v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.02177","description":"<p>Conversations have become a critical data format on social media platforms.\nUnderstanding conversation from emotion, content and other aspects also\nattracts increasing attention from researchers due to its widespread\napplication in human-computer interaction. In real-world environments, we often\nencounter the problem of incomplete modalities, which has become a core issue\nof conversation understanding. To address this problem, researchers propose\nvarious methods. However, existing approaches are mainly designed for\nindividual utterances rather than conversational data, which cannot fully\nexploit temporal and speaker information in conversations. To this end, we\npropose a novel framework for incomplete multimodal learning in conversations,\ncalled \"Graph Complete Network (GCNet)\", filling the gap of existing works. Our\nGCNet contains two well-designed graph neural network-based modules, \"Speaker\nGNN\" and \"Temporal GNN\", to capture temporal and speaker dependencies. To make\nfull use of complete and incomplete data, we jointly optimize classification\nand reconstruction tasks in an end-to-end manner. To verify the effectiveness\nof our method, we conduct experiments on three benchmark conversational\ndatasets. Experimental results demonstrate that our GCNet is superior to\nexisting state-of-the-art approaches in incomplete multimodal learning. Code is\navailable at https://github.com/zeroQiaoba/GCNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zheng Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Licai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaVocoder: Adaptive Vocoder for Custom Voice. (arXiv:2203.09825v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.09825","description":"<p>Custom voice is to construct a personal speech synthesis system by adapting\nthe source speech synthesis model to the target model through the target few\nrecordings. The solution to constructing a custom voice is to combine an\nadaptive acoustic model with a robust vocoder. However, training a robust\nvocoder usually requires a multi-speaker dataset, which should include various\nage groups and various timbres, so that the trained vocoder can be used for\nunseen speakers. Collecting such a multi-speaker dataset is difficult, and the\ndataset distribution always has a mismatch with the distribution of the target\nspeaker dataset. This paper proposes an adaptive vocoder for custom voice from\nanother novel perspective to solve the above problems. The adaptive vocoder\nmainly uses a cross-domain consistency loss to solve the overfitting problem\nencountered by the GAN-based neural vocoder in the transfer learning of\nfew-shot scenes. We construct two adaptive vocoders, AdaMelGAN and AdaHiFi-GAN.\nFirst, We pre-train the source vocoder model on AISHELL3 and CSMSC datasets,\nrespectively. Then, fine-tune it on the internal dataset VXI-children with few\nadaptation data. The empirical results show that a high-quality custom voice\nsystem can be built by combining a adaptive acoustic model with a adaptive\nvocoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yongbing Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mingming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuo_C/0/1/0/all/0/1\">Cheng Tuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. (arXiv:2211.10438v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.10438","description":"<p>Large language models (LLMs) show excellent performance but are compute- and\nmemory-intensive. Quantization can reduce memory and accelerate inference.\nHowever, for LLMs beyond 100 billion parameters, existing methods cannot\nmaintain accuracy or do not run efficiently on hardware. We propose\nSmoothQuant, a training-free, accuracy-preserving, and general-purpose\npost-training quantization (PTQ) solution to enable 8-bit weight, 8-bit\nactivation (W8A8) quantization for LLMs that can be implemented efficiently. We\nobserve that systematic outliers appear at fixed activation channels. Based on\nthe fact that weights are easy to quantize while activations are not,\nSmoothQuant smooths the activation outliers by offline migrating the\nquantization difficulty from activations to weights with a mathematically\nequivalent transformation. SmoothQuant enables an INT8 quantization of both\nweights and activations for all the GEMMs in LLMs, including OPT-175B,\nBLOOM-176B, and GLM-130B. SmoothQuant has better hardware efficiency than\nexisting techniques using mixed-precision activation quantization or\nweight-only quantization. We demonstrate up to 1.56x speedup and 2x memory\nreduction for LLMs with negligible loss in accuracy. Thanks to the\nhardware-friendly design, we integrate SmoothQuant into FasterTransformer, a\nstate-of-the-art LLM serving framework, and achieve faster inference speed with\nhalf the number of GPUs compared to FP16. Our work offers a turn-key solution\nthat reduces hardware costs and democratizes LLMs. Code is available at:\nhttps://github.com/mit-han-lab/smoothquant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Guangxuan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Ji Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seznec_M/0/1/0/all/0/1\">Mickael Seznec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demouth_J/0/1/0/all/0/1\">Julien Demouth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task. (arXiv:2211.11216v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2211.11216","description":"<p>Benefiting from large-scale datasets and pre-trained models, the field of\ngenerative models has recently gained significant momentum. However, most\ndatasets for symbolic music are very small, which potentially limits the\nperformance of data-driven multimodal models. An intuitive solution to this\nproblem is to leverage pre-trained models from other modalities (e.g., natural\nlanguage) to improve the performance of symbolic music-related multimodal\ntasks. In this paper, we carry out the first study of generating complete and\nsemantically consistent symbolic music scores from text descriptions, and\nexplore the efficacy of using publicly available checkpoints (i.e., BERT,\nGPT-2, and BART) for natural language processing in the task of text-to-music\ngeneration. Our experimental results show that the improvement from using\npre-trained checkpoints is statistically significant in terms of BLEU score and\nedit distance similarity. We analyse the capabilities and limitations of our\nmodel to better understand the potential of language-music models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangda Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement. (arXiv:2212.04523v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.04523","description":"<p>The long-distance agreement, evidence for syntactic structure, is\nincreasingly used to assess the syntactic generalization of Neural Language\nModels. Much work has shown that transformers are capable of high accuracy in\nvaried agreement tasks, but the mechanisms by which the models accomplish this\nbehavior are still not well understood. To better understand transformers'\ninternal working, this work contrasts how they handle two superficially similar\nbut theoretically distinct agreement phenomena: subject-verb and object-past\nparticiple agreement in French. Using probing and counterfactual analysis\nmethods, our experiments show that i) the agreement task suffers from several\nconfounders which partially question the conclusions drawn so far and ii)\ntransformers handle subject-verb and object-past participle agreements in a way\nthat is consistent with their modeling in theoretical linguistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisniewski_G/0/1/0/all/0/1\">Guillaume Wisniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crabbe_B/0/1/0/all/0/1\">Beno&#xee;t Crabb&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-Level Debiased Natural Language Understanding. (arXiv:2212.05421v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05421","description":"<p>Natural language understanding (NLU) models often rely on dataset biases\nrather than intended task-relevant features to achieve high performance on\nspecific datasets. As a result, these models perform poorly on datasets outside\nthe training distribution. Some recent studies address this issue by reducing\nthe weights of biased samples during the training process. However, these\nmethods still encode biased latent features in representations and neglect the\ndynamic nature of bias, which hinders model prediction. We propose an NLU\ndebiasing method, named debiasing contrastive learning (DCT), to simultaneously\nalleviate the above problems based on contrastive learning. We devise a\ndebiasing, positive sampling strategy to mitigate biased latent features by\nselecting the least similar biased positive samples. We also propose a dynamic\nnegative sampling strategy to capture the dynamic influence of biases by\nemploying a bias-only model to dynamically select the most similar biased\nnegative samples. We conduct experiments on three NLU benchmark datasets.\nExperimental results show that DCT outperforms state-of-the-art baselines on\nout-of-distribution datasets while maintaining in-distribution performance. We\nalso verify that DCT can reduce biased latent features from the model's\nrepresentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yougang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yechang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yukun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models as Corporate Lobbyists. (arXiv:2301.01181v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.01181","description":"<p>We demonstrate a proof-of-concept of a large language model conducting\ncorporate lobbying related activities. An autoregressive large language model\n(OpenAI's text-davinci-003) determines if proposed U.S. Congressional bills are\nrelevant to specific public companies and provides explanations and confidence\nlevels. For the bills the model deems as relevant, the model drafts a letter to\nthe sponsor of the bill in an attempt to persuade the congressperson to make\nchanges to the proposed legislation. We use hundreds of ground-truth labels of\nthe relevance of a bill to a company to benchmark the performance of the model,\nwhich outperforms the baseline of predicting the most common outcome of\nirrelevance. We also benchmark the performance of the previous OpenAI GPT-3\nmodel (text-davinci-002), which was state-of-the-art on many language tasks\nuntil text-davinci-003 was released on November 28, 2022. The performance of\ntext-davinci-002 is worse than simply always predicting that a bill is\nirrelevant to a company. These results suggest that, as large language models\ncontinue to exhibit improved core natural language understanding capabilities,\nperformance on corporate lobbying related tasks will continue to improve. We\nthen discuss why this could be problematic for societal-AI alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nay_J/0/1/0/all/0/1\">John J. Nay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StarGraph: Knowledge Representation Learning based on Incomplete Two-hop Subgraph. (arXiv:2205.14209v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2205.14209","description":"<p>Conventional representation learning algorithms for knowledge graphs (KG) map\neach entity to a unique embedding vector, ignoring the rich information\ncontained in the neighborhood. We propose a method named StarGraph, which gives\na novel way to utilize the neighborhood information for large-scale knowledge\ngraphs to obtain entity representations. An incomplete two-hop neighborhood\nsubgraph for each target node is at first generated, then processed by a\nmodified self-attention network to obtain the entity representation, which is\nused to replace the entity embedding in conventional methods. We achieved SOTA\nperformance on ogbl-wikikg2 and got competitive results on fb15k-237. The\nexperimental results proves that StarGraph is efficient in parameters, and the\nimprovement made on ogbl-wikikg2 demonstrates its great effectiveness of\nrepresentation learning on large-scale knowledge graphs. The code is now\navailable at \\url{https://github.com/hzli-ucas/StarGraph}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiangrui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Linhui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yafeng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuhui Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}