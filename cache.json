{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-02T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The Power of External Memory in Increasing Predictive Model Capacity. (arXiv:2302.00003v1 [cs.LG])","link":"http://arxiv.org/abs/2302.00003","description":"<p>One way of introducing sparsity into deep networks is by attaching an\nexternal table of parameters that is sparsely looked up at different layers of\nthe network. By storing the bulk of the parameters in the external table, one\ncan increase the capacity of the model without necessarily increasing the\ninference time. Two crucial questions in this setting are then: what is the\nlookup function for accessing the table and how are the contents of the table\nconsumed? Prominent methods for accessing the table include 1) using\nwords/wordpieces token-ids as table indices, 2) LSH hashing the token vector in\neach layer into a table of buckets, and 3) learnable softmax style routing to a\ntable entry. The ways to consume the contents include adding/concatenating to\ninput representation, and using the contents as expert networks that specialize\nto different inputs. In this work, we conduct rigorous experimental evaluations\nof existing ideas and their combinations. We also introduce a new method,\nalternating updates, that enables access to an increased token dimension\nwithout increasing the computation time, and demonstrate its effectiveness in\nlanguage modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baykal_C/0/1/0/all/0/1\">Cenk Baykal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cutler_D/0/1/0/all/0/1\">Dylan J Cutler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dikkala_N/0/1/0/all/0/1\">Nishanth Dikkala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_N/0/1/0/all/0/1\">Nikhil Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panigrahy_R/0/1/0/all/0/1\">Rina Panigrahy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Retrieval-Augmented Language Models. (arXiv:2302.00083v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00083","description":"<p>Retrieval-Augmented Language Modeling (RALM) methods, that condition a\nlanguage model (LM) on relevant documents from a grounding corpus during\ngeneration, have been shown to significantly improve language modeling while\nalso providing a natural source attribution mechanism. Existing RALM approaches\nfocus on modifying the LM architecture in order to facilitate the incorporation\nof external information, significantly complicating deployment. This paper\nproposes an under-explored alternative, which we dub In-Context RALM: leaving\nthe LM architecture unchanged and prepending grounding documents to the input.\nWe show that in-context RALM which uses off-the-shelf general purpose\nretrievers provides surprisingly large LM gains across model sizes and diverse\ncorpora. We also demonstrate that the document retrieval and ranking mechanism\ncan be specialized to the RALM setting to further boost performance. We\nconclude that in-context RALM has considerable potential to increase the\nprevalence of LM grounding, particularly in settings where a pretrained LM must\nbe used without modification or even via API access. To that end, we make our\ncode publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1\">Ori Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmedigos_I/0/1/0/all/0/1\">Itay Dalmedigos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhlgay_D/0/1/0/all/0/1\">Dor Muhlgay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leyton_Brown_K/0/1/0/all/0/1\">Kevin Leyton-Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoham_Y/0/1/0/all/0/1\">Yoav Shoham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Be Easily Distracted by Irrelevant Context. (arXiv:2302.00093v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00093","description":"<p>Large language models have achieved impressive performance on various natural\nlanguage processing tasks. However, so far they have been evaluated primarily\non benchmarks where all information in the input context is relevant for\nsolving the task. In this work, we investigate the distractibility of large\nlanguage models, i.e., how the model problem-solving accuracy can be influenced\nby irrelevant context. In particular, we introduce Grade-School Math with\nIrrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant\ninformation in the problem description. We use this benchmark to measure the\ndistractibility of cutting-edge prompting techniques for large language models,\nand find that the model performance is dramatically decreased when irrelevant\ninformation is included. We also identify several approaches for mitigating\nthis deficiency, such as decoding with self-consistency and adding to the\nprompt an instruction that tells the language model to ignore the irrelevant\ninformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Freda Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_K/0/1/0/all/0/1\">Kanishka Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scales_N/0/1/0/all/0/1\">Nathan Scales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharli_N/0/1/0/all/0/1\">Nathanael Sch&#xe4;rli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Harmful Agendas in News Articles. (arXiv:2302.00102v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00102","description":"<p>Manipulated news online is a growing problem which necessitates the use of\nautomated systems to curtail its spread. We argue that while misinformation and\ndisinformation detection have been studied, there has been a lack of investment\nin the important open challenge of detecting harmful agendas in news articles;\nidentifying harmful agendas is critical to flag news campaigns with the\ngreatest potential for real world harm. Moreover, due to real concerns around\ncensorship, harmful agenda detectors must be interpretable to be effective. In\nthis work, we propose this new task and release a dataset, NewsAgendas, of\nannotated news articles for agenda identification. We show how interpretable\nsystems can be effective on this task and demonstrate that they can perform\ncomparably to black-box models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subbiah_M/0/1/0/all/0/1\">Melanie Subbiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Amrita Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1\">Bobby Yilun Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1\">Tharindu Kumarage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation Impact in E-commerce Multilingual Search. (arXiv:2302.00119v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00119","description":"<p>Previous work suggests that performance of cross-lingual information\nretrieval correlates highly with the quality of Machine Translation. However,\nthere may be a threshold beyond which improving query translation quality\nyields little or no benefit to further improve the retrieval performance. This\nthreshold may depend upon multiple factors including the source and target\nlanguages, the existing MT system quality and the search pipeline. In order to\nidentify the benefit of improving an MT system for a given search pipeline, we\ninvestigate the sensitivity of retrieval quality to the presence of different\nlevels of MT quality using experimental datasets collected from actual traffic.\nWe systematically improve the performance of our MT systems quality on language\npairs as measured by MT evaluation metrics including Bleu and Chrf to determine\ntheir impact on search precision metrics and extract signals that help to guide\nthe improvement strategies. Using this information we develop techniques to\ncompare query translations for multiple language pairs and identify the most\npromising language pairs to invest and improve.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bryan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_A/0/1/0/all/0/1\">Amita Misra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Topological Regularities of Syntactic Structures: Decoupling Efficiency from Optimization. (arXiv:2302.00129v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00129","description":"<p>Human syntactic structures are usually represented as graphs. Much research\nhas focused on the mapping between such graphs and linguistic sequences, but\nless attention has been paid to the shapes of the graphs themselves: their\ntopologies. This study investigates how the topologies of syntactic graphs\nreveal traces of the processes that led to their emergence. I report a new\nuniversal regularity in syntactic structures: Their topology is communicatively\nefficient above chance. The pattern holds, without exception, for all 124\nlanguages studied, across linguistic families and modalities (spoken, written,\nand signed). This pattern can arise from a process optimizing for communicative\nefficiency or, alternatively, by construction, as a by-effect of a sublinear\npreferential attachment process reflecting language production mechanisms known\nfrom psycholinguistics. This dual explanation shows how communicative\nefficiency, per se, does not require optimization. Among the two options,\nefficiency without optimization offers the better explanation for the new\npattern.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martin_F/0/1/0/all/0/1\">Ferm&#xed;n Moscoso del Prado Mart&#xed;n</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Program Generation from Diverse Video Demonstrations. (arXiv:2302.00178v1 [cs.CV])","link":"http://arxiv.org/abs/2302.00178","description":"<p>The ability to use inductive reasoning to extract general rules from multiple\nobservations is a vital indicator of intelligence. As humans, we use this\nability to not only interpret the world around us, but also to predict the\noutcomes of the various interactions we experience. Generalising over multiple\nobservations is a task that has historically presented difficulties for\nmachines to grasp, especially when requiring computer vision. In this paper, we\npropose a model that can extract general rules from video demonstrations by\nsimultaneously performing summarisation and translation. Our approach differs\nfrom prior works by framing the problem as a multi-sequence-to-sequence task,\nwherein summarisation is learnt by the model. This allows our model to utilise\nedge cases that would otherwise be suppressed or discarded by traditional\nsummarisation techniques. Additionally, we show that our approach can handle\nnoisy specifications without the need for additional filtering methods. We\nevaluate our model by synthesising programs from video demonstrations in the\nVizdoom environment achieving state-of-the-art results with a relative increase\nof 11.75% program accuracy on prior works\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manchin_A/0/1/0/all/0/1\">Anthony Manchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherrah_J/0/1/0/all/0/1\">Jamie Sherrah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Lexical Borrowings from Dominant Languages in Multilingual Wordlists. (arXiv:2302.00189v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00189","description":"<p>Language contact is a pervasive phenomenon reflected in the borrowing of\nwords from donor to recipient languages. Most computational approaches to\nborrowing detection treat all languages under study as equally important, even\nthough dominant languages have a stronger impact on heritage languages than\nvice versa. We test new methods for lexical borrowing detection in contact\nsituations where dominant languages play an important role, applying two\nclassical sequence comparison methods and one machine learning method to a\nsample of seven Latin American languages which have all borrowed extensively\nfrom Spanish. All methods perform well, with the supervised machine learning\nsystem outperforming the classical systems. A review of detection errors shows\nthat borrowing detection could be substantially improved by taking into account\ndonor words with divergent meanings from recipient words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1\">John E. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+List_J/0/1/0/all/0/1\">Johann-Mattis List</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transaction Represented with Weighted Finite-State Transducers. (arXiv:2302.00200v1 [cs.FL])","link":"http://arxiv.org/abs/2302.00200","description":"<p>Not all contracts are good, but all good contracts can be expressed as a\nfinite-state transition system (\"State-Transition Contracts\"). Contracts that\ncan be represented as State-Transition Contracts discretize fat-tailed risk to\nforeseeable, managed risk, define the boundary of relevant events governed by\nthe relationship, and eliminate the potential of inconsistent contractual\nprovisions. Additionally, State-Transition Contracts reap the substantial\nbenefit of being able to be analyzed under the rules governing the science of\nthe theory of computation. Simple State-Transition Contracts can be represented\nas discrete finite automata; more complicated State-Transition Contracts, such\nas those that have downstream effects on other agreements or complicated\npathways of performance, benefit from representation as weighted finite-state\ntransducers, with weights assigned as costs, penalties, or probabilities of\ntransitions. This research paper (the \"Research\" or \"Paper\") presents a complex\nlegal transaction represented as weighted finite-state transducers.\nFurthermore, we show that the mathematics/algorithms permitted by the algebraic\nstructure of weighted finite-state transducers provides actionable, legal\ninsight into the transaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holmes_J/0/1/0/all/0/1\">J. Nathaniel Holmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beigi_H/0/1/0/all/0/1\">Homayoon Beigi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Filtering Context Mitigates Scarcity and Selection Bias in Political Ideology Prediction. (arXiv:2302.00239v1 [cs.LG])","link":"http://arxiv.org/abs/2302.00239","description":"<p>We propose a novel supervised learning approach for political ideology\nprediction (PIP) that is capable of predicting out-of-distribution inputs. This\nproblem is motivated by the fact that manual data-labeling is expensive, while\nself-reported labels are often scarce and exhibit significant selection bias.\nWe propose a novel statistical model that decomposes the document embeddings\ninto a linear superposition of two vectors; a latent neutral \\emph{context}\nvector independent of ideology, and a latent \\emph{position} vector aligned\nwith ideology. We train an end-to-end model that has intermediate contextual\nand positional vectors as outputs. At deployment time, our model predicts\nlabels for input documents by exclusively leveraging the predicted positional\nvectors. On two benchmark datasets we show that our model is capable of\noutputting predictions even when trained with as little as 5\\% biased data, and\nis significantly more accurate than the state-of-the-art. Through\ncrowd-sourcing we validate the neutrality of contextual vectors, and show that\ncontext filtering results in ideological concentration, allowing for prediction\non out-of-distribution examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_D/0/1/0/all/0/1\">Dylan Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1\">Venkatesh Saligrama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Evaluation of Persian-English Machine Translation Datasets with Transformers. (arXiv:2302.00321v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00321","description":"<p>Nowadays, many researchers are focusing their attention on the subject of\nmachine translation (MT). However, Persian machine translation has remained\nunexplored despite a vast amount of research being conducted in languages with\nhigh resources, such as English. Moreover, while a substantial amount of\nresearch has been undertaken in statistical machine translation for some\ndatasets in Persian, there is currently no standard baseline for\ntransformer-based text2text models on each corpus. This study collected and\nanalysed the most popular and valuable parallel corpora, which were used for\nPersian-English translation. Furthermore, we fine-tuned and evaluated two\nstate-of-the-art attention-based seq2seq models on each dataset separately (48\nresults). We hope this paper will assist researchers in comparing their Persian\nto English and vice versa machine translation results to a standard baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sartipi_A/0/1/0/all/0/1\">Amir Sartipi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghan_M/0/1/0/all/0/1\">Meghdad Dehghan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatemi_A/0/1/0/all/0/1\">Afsaneh Fatemi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating TCFD Reporting: A New Application of Zero-Shot Analysis to Climate-Related Financial Disclosures. (arXiv:2302.00326v1 [cs.CY])","link":"http://arxiv.org/abs/2302.00326","description":"<p>We examine climate-related disclosures in a large sample of reports published\nby banks that officially endorsed the recommendations of the Task Force for\nClimate-related Financial Disclosures (TCFD). In doing so, we introduce a new\napplication of the zero-shot text classification. By developing a set of\nfine-grained TCFD labels, we show that zero-shot analysis is a useful tool for\nclassifying climate-related disclosures without further model training.\nOverall, our findings indicate that corporate climate-related disclosures grew\ndynamically after the launch of the TCFD recommendations. However, there are\nmarked differences in the extent of reporting by recommended disclosure topic,\nsuggesting that some recommendations have not yet been fully met. Our findings\nyield important conclusions for the design of climate-related disclosure\nframeworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Auzepy_A/0/1/0/all/0/1\">Alix Auzepy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonjes_E/0/1/0/all/0/1\">Elena T&#xf6;njes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenz_D/0/1/0/all/0/1\">David Lenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funk_C/0/1/0/all/0/1\">Christoph Funk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Link: An Efficient Attention-Based Low Resource Machine Translation Architecture. (arXiv:2302.00340v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00340","description":"<p>Transformers have achieved great success in machine translation, but\ntransformer-based NMT models often require millions of bilingual parallel\ncorpus for training. In this paper, we propose a novel architecture named as\nattention link (AL) to help improve transformer models' performance, especially\nin low training resources. We theoretically demonstrate the superiority of our\nattention link architecture in low training resources. Besides, we have done a\nlarge number of experiments, including en-de, de-en, en-fr, en-it, it-en, en-ro\ntranslation tasks on the IWSLT14 dataset as well as real low resources scene on\nbn-gu and gu-ta translation tasks on the CVIT PIB dataset. All the experiment\nresults show our attention link is powerful and can lead to a significant\nimprovement. In addition, we achieve a 37.9 BLEU score, a new sota, on the\nIWSLT14 de-en task by combining our attention link and other advanced methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_Z/0/1/0/all/0/1\">Zeping Min</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on the Transferability of Transformer Modules in Parameter-Efficient Fine-Tuning. (arXiv:2302.00378v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00378","description":"<p>Parameter-efficient fine-tuning approaches have recently garnered a lot of\nattention. Having considerably lower number of trainable weights, these methods\ncan bring about scalability and computational effectiveness. In this paper, we\nlook for optimal sub-networks and investigate the capability of different\ntransformer modules in transferring knowledge from a pre-trained model to a\ndownstream task. Our empirical results suggest that every transformer module in\nBERT can act as a winning ticket: fine-tuning each specific module while\nkeeping the rest of the network frozen can lead to comparable performance to\nthe full fine-tuning. Among different modules, LayerNorms exhibit the best\ncapacity for knowledge transfer with limited trainable weights, to the extent\nthat, with only 0.003% of all parameters in the layer-wise analysis, they show\nacceptable performance on various target tasks. On the reasons behind their\neffectiveness, we argue that their notable performance could be attributed to\ntheir high-magnitude weights compared to that of the other modules in the\npre-trained BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+AkbarTajari_M/0/1/0/all/0/1\">Mohammad AkbarTajari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajaee_S/0/1/0/all/0/1\">Sara Rajaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video. (arXiv:2302.00402v1 [cs.CV])","link":"http://arxiv.org/abs/2302.00402","description":"<p>Recent years have witnessed a big convergence of language, vision, and\nmulti-modal pretraining. In this work, we present mPLUG-2, a new unified\nparadigm with modularized design for multi-modal pretraining, which can benefit\nfrom modality collaboration while addressing the problem of modality\nentanglement. In contrast to predominant paradigms of solely relying on\nsequence-to-sequence generation or encoder-based instance discrimination,\nmPLUG-2 introduces a multi-module composition network by sharing common\nuniversal modules for modality collaboration and disentangling different\nmodality modules to deal with modality entanglement. It is flexible to select\ndifferent modules for different understanding and generation tasks across all\nmodalities including text, image, and video. Empirical study shows that mPLUG-2\nachieves state-of-the-art or competitive results on a broad range of over 30\ndownstream tasks, spanning multi-modal tasks of image-text and video-text\nunderstanding and generation, and uni-modal tasks of text-only, image-only, and\nvideo-only understanding. Notably, mPLUG-2 shows new state-of-the-art results\nof 48.0 top-1 accuracy and 80.3 CIDEr on the challenging MSRVTT video QA and\nvideo caption tasks with a far smaller model size and data scale. It also\ndemonstrates strong zero-shot transferability on vision-language and\nvideo-language tasks. Code and models will be released in\nhttps://github.com/alibaba/AliceMind.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yaya Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiabo Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1\">Bin Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Role of Morphological Information for Contextual Lemmatization. (arXiv:2302.00407v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00407","description":"<p>Lemmatization is a Natural Language Processing (NLP) task which consists of\nproducing, from a given inflected word, its canonical form or lemma.\nLemmatization is one of the basic tasks that facilitate downstream NLP\napplications, and is of particular importance for high-inflected languages.\nGiven that the process to obtain a lemma from an inflected word can be\nexplained by looking at its morphosyntactic category, including fine-grained\nmorphosyntactic information to train contextual lemmatizers has become common\npractice, without analyzing whether that is the optimum in terms of downstream\nperformance. Thus, in this paper we empirically investigate the role of\nmorphological information to develop contextual lemmatizers in six languages\nwithin a varied spectrum of morphological complexity: Basque, Turkish, Russian,\nCzech, Spanish and English. Furthermore, and unlike the vast majority of\nprevious work, we also evaluate lemmatizers in out-of-domain settings, which\nconstitutes, after all, their most common application use. The results of our\nstudy are rather surprising: (i) providing lemmatizers with fine-grained\nmorphological features during training is not that beneficial, not even for\nagglutinative languages; (ii) in fact, modern contextual word representations\nseem to implicitly encode enough morphological information to obtain good\ncontextual lemmatizers without seeing any explicit morphological signal; (iii)\nthe best lemmatizers out-of-domain are those using simple UPOS tags or those\ntrained without morphology; (iv) current evaluation practices for lemmatization\nare not adequate to clearly discriminate between models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toporkov_O/0/1/0/all/0/1\">Olia Toporkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KNNs of Semantic Encodings for Rating Prediction. (arXiv:2302.00412v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00412","description":"<p>This paper explores a novel application of textual semantic similarity to\nuser-preference representation for rating prediction. The approach represents a\nuser's preferences as a graph of textual snippets from review text, where the\nedges are defined by semantic similarity. This textual, memory-based approach\nto rating prediction enables review-based explanations for recommendations. The\nmethod is evaluated quantitatively, highlighting that leveraging text in this\nway outperforms both strong memory-based and model-based collaborative\nfiltering baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laugier_L/0/1/0/all/0/1\">L&#xe9;o Laugier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonald_T/0/1/0/all/0/1\">Thomas Bonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixon_L/0/1/0/all/0/1\">Lucas Dixon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vadapalli_R/0/1/0/all/0/1\">Raghuram Vadapalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection. (arXiv:2302.00444v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00444","description":"<p>Knowledge distillation addresses the problem of transferring knowledge from a\nteacher model to a student model. In this process, we typically have multiple\ntypes of knowledge extracted from the teacher model. The problem is to make\nfull use of them to train the student model. Our preliminary study shows that:\n(1) not all of the knowledge is necessary for learning a good student model,\nand (2) knowledge distillation can benefit from certain knowledge at different\ntraining steps. In response to these, we propose an actor-critic approach to\nselecting appropriate knowledge to transfer during the process of knowledge\ndistillation. In addition, we offer a refinement of the training algorithm to\nease the computational burden. Experimental results on the GLUE datasets show\nthat our method outperforms several strong knowledge distillation baselines\nsignificantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yongyu Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yimin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HunSum-1: an Abstractive Summarization Dataset for Hungarian. (arXiv:2302.00455v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00455","description":"<p>We introduce HunSum-1: a dataset for Hungarian abstractive summarization,\nconsisting of 1.14M news articles. The dataset is built by collecting, cleaning\nand deduplicating data from 9 major Hungarian news sites through CommonCrawl.\nUsing this dataset, we build abstractive summarizer models based on huBERT and\nmT5. We demonstrate the value of the created dataset by performing a\nquantitative and qualitative analysis on the models' results. The HunSum-1\ndataset, all models used in our experiments and our code are available open\nsource.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barta_B/0/1/0/all/0/1\">Botond Barta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakatos_D/0/1/0/all/0/1\">Dorina Lakatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagy_A/0/1/0/all/0/1\">Attila Nagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyist_M/0/1/0/all/0/1\">Mil&#xe1;n Konor Nyist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acs_J/0/1/0/all/0/1\">Judit &#xc1;cs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feed-Forward Blocks Control Contextualization in Masked Language Models. (arXiv:2302.00456v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00456","description":"<p>Understanding the inner workings of neural network models is a crucial step\nfor rationalizing their output and refining their architecture.\nTransformer-based models are the core of recent natural language processing and\nhave been analyzed typically with attention patterns as their epoch-making\nfeature is contextualizing surrounding input words via attention mechanisms. In\nthis study, we analyze their inner contextualization by considering all the\ncomponents, including the feed-forward block (i.e., a feed-forward layer and\nits surrounding residual and normalization layers) as well as the attention.\nOur experiments with masked language models show that each of the previously\noverlooked components did modify the degree of the contextualization in case of\nprocessing special word-word pairs (e.g., consisting of named entities).\nFurthermore, we find that some components cancel each other's effects. Our\nresults could update the typical view about each component's roles (e.g.,\nattention performs contextualization, and the other components serve different\nroles) in the Transformer layer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_G/0/1/0/all/0/1\">Goro Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuribayashi_T/0/1/0/all/0/1\">Tatsuki Kuribayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoi_S/0/1/0/all/0/1\">Sho Yokoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Are What You Talk About: Inducing Evaluative Topics for Personality Analysis. (arXiv:2302.00493v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00493","description":"<p>Expressing attitude or stance toward entities and concepts is an integral\npart of human behavior and personality. Recently, evaluative language data has\nbecome more accessible with social media's rapid growth, enabling large-scale\nopinion analysis. However, surprisingly little research examines the\nrelationship between personality and evaluative language. To bridge this gap,\nwe introduce the notion of evaluative topics, obtained by applying topic models\nto pre-filtered evaluative text from social media. We then link evaluative\ntopics to individual text authors to build their evaluative profiles. We apply\nevaluative profiling to Reddit comments labeled with personality scores and\nconduct an exploratory study on the relationship between evaluative topics and\nBig Five personality facets, aiming for a more interpretable, facet-level\nanalysis. Finally, we validate our approach by observing correlations\nconsistent with prior research in personality psychology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jukic_J/0/1/0/all/0/1\">Josip Juki&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vukojevic_I/0/1/0/all/0/1\">Iva Vukojevi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1\">Jan &#x160;najder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Semantic Perturbations on Grover. (arXiv:2302.00509v1 [cs.LG])","link":"http://arxiv.org/abs/2302.00509","description":"<p>With news and information being as easy to access as they currently are, it\nis more important than ever to ensure that people are not mislead by what they\nread. Recently, the rise of neural fake news (AI-generated fake news) and its\ndemonstrated effectiveness at fooling humans has prompted the development of\nmodels to detect it. One such model is the Grover model, which can both detect\nneural fake news to prevent it, and generate it to demonstrate how a model\ncould be misused to fool human readers. In this work we explore the Grover\nmodel's fake news detection capabilities by performing targeted attacks through\nperturbations on input news articles. Through this we test Grover's resilience\nto these adversarial attacks and expose some potential vulnerabilities which\nshould be addressed in further iterations to ensure it can detect all types of\nfake news accurately.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1\">Pranav Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziqing Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neskovic_M/0/1/0/all/0/1\">Marko Neskovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nolan_K/0/1/0/all/0/1\">Kevin Nolan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-Writing with Opinionated Language Models Affects Users' Views. (arXiv:2302.00560v1 [cs.HC])","link":"http://arxiv.org/abs/2302.00560","description":"<p>If large language models like GPT-3 preferably produce a particular point of\nview, they may influence people's opinions on an unknown scale. This study\ninvestigates whether a language-model-powered writing assistant that generates\nsome opinions more often than others impacts what users write - and what they\nthink. In an online experiment, we asked participants (N=1,506) to write a post\ndiscussing whether social media is good for society. Treatment group\nparticipants used a language-model-powered writing assistant configured to\nargue that social media is good or bad for society. Participants then completed\na social media attitude survey, and independent judges (N=500) evaluated the\nopinions expressed in their writing. Using the opinionated language model\naffected the opinions expressed in participants' writing and shifted their\nopinions in the subsequent attitude survey. We discuss the wider implications\nof our results and argue that the opinions built into AI language technologies\nneed to be monitored and engineered more carefully.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jakesch_M/0/1/0/all/0/1\">Maurice Jakesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_A/0/1/0/all/0/1\">Advait Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buschek_D/0/1/0/all/0/1\">Daniel Buschek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zalmanson_L/0/1/0/all/0/1\">Lior Zalmanson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naaman_M/0/1/0/all/0/1\">Mor Naaman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The RW3D: A multi-modal panel dataset to understand the psychological impact of the pandemic. (arXiv:2302.00606v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00606","description":"<p>Besides far-reaching public health consequences, the COVID-19 pandemic had a\nsignificant psychological impact on people around the world. To gain further\ninsight into this matter, we introduce the Real World Worry Waves Dataset\n(RW3D). The dataset combines rich open-ended free-text responses with survey\ndata on emotions, significant life events, and psychological stressors in a\nrepeated-measures design in the UK over three years (2020: n=2441, 2021: n=1716\nand 2022: n=1152). This paper provides background information on the data\ncollection procedure, the recorded variables, participants' demographics, and\nhigher-order psychological and text-based derived variables that emerged from\nthe data. The RW3D is a unique primary data resource that could inspire new\nresearch questions on the psychological impact of the pandemic, especially\nthose that connect modalities (here: text data, psychological survey variables\nand demographics) over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vegt_I/0/1/0/all/0/1\">Isabelle van der Vegt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_B/0/1/0/all/0/1\">Bennett Kleinberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero Shot Transfer of Legal Judgement Prediction as Article-aware Entailment for the European Court of Human Rights. (arXiv:2302.00609v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00609","description":"<p>In this paper, we cast Legal Judgment Prediction (LJP) from text on European\nCourt of Human Rights cases as an entailment task, where the case outcome is\nclassified from a combined input of case facts and convention articles. This\nconfiguration facilitates the model learning legal reasoning ability in mapping\narticle text to specific fact text. It also provides the opportunity to\nevaluate the model's ability to generalize to zero-shot settings when asked to\nclassify the case outcome with respect to articles not seen during training. We\ndevise zero-shot LJP experiments and apply domain adaptation methods based on\ndomain discriminator and Wasserstein distance. Our results demonstrate that the\nentailment architecture outperforms straightforward fact classification. We\nalso find that domain adaptation methods improve zero-shot transfer\nperformance, with article relatedness and encoder pre-training influencing the\neffect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1\">Santosh T.Y.S.S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichim_O/0/1/0/all/0/1\">Oana Ichim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabmair_M/0/1/0/all/0/1\">Matthias Grabmair</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models. (arXiv:2302.00618v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00618","description":"<p>Large language models can perform various reasoning tasks by using\nchain-of-thought prompting, which guides them to find answers through\nstep-by-step demonstrations. However, the quality of the prompts depends on the\ndemonstrations given to the models, and creating many of them by hand is\ncostly. We introduce Synthetic prompting, a method that leverages a few\nhandcrafted examples to prompt the model to generate more examples by itself,\nand selects effective demonstrations to elicit better reasoning. Our method\nalternates between a backward and forward process to generate new examples. The\nbackward process generates a question that match a sampled reasoning chain, so\nthat the question is solvable and clear. The forward process produces a more\ndetailed reasoning chain for the question, improving the quality of the\nexample. We evaluate our method on numerical, symbolic, and algorithmic\nreasoning tasks, and show that it outperforms existing prompting techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhihong Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are UD Treebanks Getting More Consistent? A Report Card for English UD. (arXiv:2302.00636v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00636","description":"<p>Recent efforts to consolidate guidelines and treebanks in the Universal\nDependencies project raise the expectation that joint training and dataset\ncomparison is increasingly possible for high-resource languages such as\nEnglish, which have multiple corpora. Focusing on the two largest UD English\ntreebanks, we examine progress in data consolidation and answer several\nquestions: Are UD English treebanks becoming more internally consistent? Are\nthey becoming more like each other and to what extent? Is joint training a good\nidea, and if so, since which UD version? Our results indicate that while\nconsolidation has made progress, joint models may still suffer from\ninconsistencies, which hamper their ability to leverage a larger pool of\ntraining data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Vision Accelerate Hierarchical Generalization of Neural Language Learners?. (arXiv:2302.00667v1 [cs.CL])","link":"http://arxiv.org/abs/2302.00667","description":"<p>Neural language models (LMs) are arguably less data-efficient than humans --\nwhy does this gap occur? In this study, we hypothesize that this gap stems from\nthe learners' accessibility to modalities other than text, specifically,\nvision. We conducted two complementary experiments (using noisy, realistic data\nand a simplified, artificial one) toward the advantage of vision in the\nsyntactic generalization of LMs. Our results showed that vision accelerated a\nproper linguistic generalization in the simplified, artificial setting, but LMs\nstruggled with the noisy, realistic setting. These mixed results indicate\nseveral possibilities, e.g., vision can potentially boost language acquisition,\nbut learners' additional visual/linguistic prior knowledge should be needed to\nrobustly make use of raw images for efficient language acquisition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuribayashi_T/0/1/0/all/0/1\">Tatsuki Kuribayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"'Generative CI' through Collective Response Systems. (arXiv:2302.00672v1 [cs.HC])","link":"http://arxiv.org/abs/2302.00672","description":"<p>How can many people (who may disagree) come together to answer a question or\nmake a decision? \"Collective response systems\" are a type of generative\ncollective intelligence (CI) facilitation process meant to address this\nchallenge. They enable a form of \"generative voting\", where both the votes, and\nthe choices of what to vote on, are provided by the group. Such systems\novercome the traditional limitations of polling, town halls, standard voting,\nreferendums, etc. The generative CI outputs of collective response systems can\nalso be chained together into iterative \"collective dialogues\", analogously to\nsome kinds of generative AI.\n</p>\n<p>Technical advances across domains including recommender systems, language\nmodels, and human-computer interaction have led to the development of\ninnovative and scalable collective response systems. For example, Polis has\nbeen used around the world to support policy-making at different levels of\ngovernment, and Remesh has been used by the UN to understand the challenges and\nneeds of ordinary people across war-torn countries. This paper aims to develop\na shared language by defining the structure, processes, properties, and\nprinciples of such systems.\n</p>\n<p>Collective response systems allow non-confrontational exploration of divisive\nissues, help identify common ground, and elicit insights from those closest to\nthe issues. As a result, they can help overcome gridlock around conflict and\ngovernance challenges, increase trust, and develop mandates. Continued progress\ntoward their development and adoption could help revitalize democracies,\nreimagine corporate governance, transform conflict, and govern powerful AI\nsystems -- both as a complement to deeper deliberative democratic processes and\nas an option where deeper processes are not applicable or possible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ovadya_A/0/1/0/all/0/1\">Aviv Ovadya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data. (arXiv:2302.00674v1 [cs.LG])","link":"http://arxiv.org/abs/2302.00674","description":"<p>Few-shot learning involves learning an effective model from only a few\nlabeled datapoints. The use of a small training set makes it difficult to avoid\noverfitting but also makes few-shot learning applicable to many important\nreal-world settings. In this work, we focus on Few-shot Learning with Auxiliary\nData (FLAD), a training paradigm that assumes access to auxiliary data during\nfew-shot learning in hopes of improving generalization. Introducing auxiliary\ndata during few-shot learning leads to essential design choices where\nhand-designed heuristics can lead to sub-optimal performance. In this work, we\nfocus on automated sampling strategies for FLAD and relate them to the\nexplore-exploit dilemma that is central in multi-armed bandit settings. Based\non this connection we propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and\ncompare them with methods that either explore or exploit, finding that the\ncombination of exploration and exploitation is crucial. Using our proposed\nalgorithms to train T5 yields a 9% absolute improvement over the explicitly\nmulti-task pre-trained T0 model across 11 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic generation of semantic corpora for improving intent estimation of taxonomy-driven search engines. (arXiv:2203.16230v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16230","description":"<p>With the increasing demand of intelligent systems capable of operating in\ndifferent user contexts (e.g. users on the move) the correct interpretation of\nthe user-need by such systems has become crucial to give a consistent answer to\nthe user query. The most effective techniques which are used to address such\ntask are in the fields of natural language processing and semantic expansion of\nterms. Such systems are aimed at estimating the actual meaning of input\nqueries, addressing the concepts of the words which are expressed within the\nuser questions. The aim of this paper is to demonstrate which semantic relation\nimpacts the most in semantic expansion-based retrieval systems and to identify\nthe best tradeoff between accuracy and noise introduction when combining such\nrelations. The evaluations are made building a simple natural language\nprocessing system capable of querying any taxonomy-driven domain, making use of\nthe combination of different semantic expansions as knowledge resources. The\nproposed evaluation employs a wide and varied taxonomy as a use-case,\nexploiting its labels as basis for the expansions. To build the knowledge\nresources several corpora have been produced and integrated as gazetteers into\nthe NLP infrastructure with the purpose of estimating the pseudo-queries\ncorresponding to the taxonomy labels, considered as the possible intents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Massai_L/0/1/0/all/0/1\">Lorenzo Massai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Answering Open-ended Ethical Quandary Questions. (arXiv:2205.05989v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05989","description":"<p>Considerable advancements have been made in various NLP tasks based on the\nimpressive power of large language models (LLMs) and many NLP applications are\ndeployed in our daily lives. In this work, we challenge the capability of LLMs\nwith the new task of Ethical Quandary Generative Question Answering. Ethical\nquandary questions are more challenging to address because multiple conflicting\nanswers may exist to a single quandary. We explore the current capability of\nLLMs in providing an answer with a deliberative exchange of different\nperspectives to an ethical quandary, in the approach of Socratic philosophy,\ninstead of providing a closed answer like an oracle. We propose a model that\nsearches for different ethical principles applicable to the ethical quandary\nand generates an answer conditioned on the chosen principles through\nprompt-based few-shot learning. We also discuss the remaining challenges and\nethical issues involved in this task and suggest the direction toward\ndeveloping responsible NLP systems by incorporating human values explicitly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yejin Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalatbari_L/0/1/0/all/0/1\">Leila Khalatbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barraud_R/0/1/0/all/0/1\">Romain Barraud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barezi_E/0/1/0/all/0/1\">Elham J. Barezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kee_H/0/1/0/all/0/1\">Hayden Kee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Explanation: New Prompting Method to Generate Higher Quality Natural Language Explanation for Implicit Hate Speech. (arXiv:2209.04889v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.04889","description":"<p>Recent studies have exploited advanced generative language models to generate\nNatural Language Explanations (NLE) for why a certain text could be hateful. We\npropose the Chain of Explanation (CoE) Prompting method, using the target group\nand retrieved social norms, to generate high-quality NLE for implicit hate\nspeech. Providing accurate target information and high-quality related social\nnorms, we improved the BLUE score from 44.0 to 62.3 for NLE generation. We then\nevaluate the quality of generated NLE from various automatic metrics and human\nannotations of informativeness and clarity scores. The correlation analysis\nbetween auto-metrics and human perceptions reveals insights into how to select\nsuitable automatic metrics for Natural Language Generation tasks. To showcase a\npotential application of our proposed CoE method, we demonstrate the f1-score\nimprovements from 0.635 to 0.655 for the implicit hate speech classification\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Haewoon Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1\">Jisun An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WISE: Wavelet Transformation for Boosting Transformers' Long Sequence Learning Ability. (arXiv:2210.01989v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01989","description":"<p>Transformer and its variants are fundamental neural architectures in deep\nlearning. Recent works show that learning attention in the Fourier space can\nimprove the long sequence learning capability of Transformers. We argue that\nwavelet transform shall be a better choice because it captures both position\nand frequency information with a linear time complexity. Therefore, in this\npaper, we systematically study the synergy between wavelet transform and\nTransformers. Specifically, we focus on a new paradigm WISE, which replaces the\nattention in Transformers by (1) applying forward wavelet transform to project\nthe input sequences to multi-resolution bases, (2) conducting non-linear\ntransformations in the wavelet coefficient space, and (3) reconstructing the\nrepresentation in input space via backward wavelet transform. Extensive\nexperiments on the Long Range Arena benchmark demonstrate that learning\nattention in the wavelet space using either fixed or adaptive wavelets can\nconsistently improve Transformer's performance and also significantly\noutperform Fourier-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yufan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_F/0/1/0/all/0/1\">Fangbo Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FADO: Feedback-Aware Double COntrolling Network for Emotional Support Conversation. (arXiv:2211.00250v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.00250","description":"<p>Emotional Support Conversation (ESConv) aims to reduce help-seekers'emotional\ndistress with the supportive strategy and response. It is essential for the\nsupporter to select an appropriate strategy with the feedback of the\nhelp-seeker (e.g., emotion change during dialog turns, etc) in ESConv. However,\nprevious methods mainly focus on the dialog history to select the strategy and\nignore the help-seeker's feedback, leading to the wrong and user-irrelevant\nstrategy prediction. In addition, these approaches only model the\ncontext-to-strategy flow and pay less attention to the strategy-to-context flow\nthat can focus on the strategy-related context for generating the\nstrategy-constrain response. In this paper, we propose a Feedback-Aware Double\nCOntrolling Network (FADO) to make a strategy schedule and generate the\nsupportive response. The core module in FADO consists of a dual-level feedback\nstrategy selector and a double control reader. Specifically, the dual-level\nfeedback strategy selector leverages the turn-level and conversation-level\nfeedback to encourage or penalize strategies. The double control reader\nconstructs the novel strategy-to-context flow for generating the\nstrategy-constrain response. Furthermore, a strategy dictionary is designed to\nenrich the semantic information of the strategy and improve the quality of\nstrategy-constrain response. Experimental results on ESConv show that the\nproposed FADO has achieved the state-of-the-art performance in terms of both\nstrategy selection and response generation. Our code is available at\nhttps://github/after/reviewing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Ziyuan Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunpeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study of Slang Representation Methods. (arXiv:2212.05613v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05613","description":"<p>Considering the large amount of content created online by the minute,\nslang-aware automatic tools are critically needed to promote social good, and\nassist policymakers and moderators in restricting the spread of offensive\nlanguage, abuse, and hate speech. Despite the success of large language models\nand the spontaneous emergence of slang dictionaries, it is unclear how far\ntheir combination goes in terms of slang understanding for downstream social\ngood tasks. In this paper, we provide a framework to study different\ncombinations of representation learning models and knowledge resources for a\nvariety of downstream tasks that rely on slang understanding. Our experiments\nshow the superiority of models that have been pre-trained on social media data,\nwhile the impact of dictionaries is positive only for static word embeddings.\nOur error analysis identifies core challenges for slang representation\nlearning, including out-of-vocabulary words, polysemy, variance, and annotation\ndisagreements, which can be traced to characteristics of slang as a quickly\nevolving and highly subjective language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolla_A/0/1/0/all/0/1\">Aravinda Kolla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandlin_H/0/1/0/all/0/1\">H&#xf4;ng-&#xc2;n Sandlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mermoud_A/0/1/0/all/0/1\">Alain Mermoud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP as a Lens for Causal Analysis and Perception Mining to Infer Mental Health on Social Media. (arXiv:2301.11004v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11004","description":"<p>Interactions among humans on social media often convey intentions behind\ntheir actions, yielding a psychological language resource for Mental Health\nAnalysis (MHA) of online users. The success of Computational Intelligence\nTechniques (CIT) for inferring mental illness from such social media resources\npoints to NLP as a lens for causal analysis and perception mining. However, we\nargue that more consequential and explainable research is required for optimal\nimpact on clinical psychology practice and personalized mental healthcare. To\nbridge this gap, we posit two significant dimensions: (1) Causal analysis to\nillustrate a cause and effect relationship in the user generated text; (2)\nPerception mining to infer psychological perspectives of social effects on\nonline users intentions. Within the scope of Natural Language Processing (NLP),\nwe further explore critical areas of inquiry associated with these two\ndimensions, specifically through recent advancements in discourse analysis.\nThis position paper guides the community to explore solutions in this space and\nadvance the state of practice in developing conversational agents for inferring\nmental health from social media. We advocate for a more explainable approach\ntoward modeling computational psychology problems through the lens of language\nas we observe an increased number of research contributions in dataset and\nproblem formulation for causal relation extraction and perception enhancements\nwhile inferring mental states.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Muskan Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_C/0/1/0/all/0/1\">Chandni Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseem_U/0/1/0/all/0/1\">Usman Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorr_B/0/1/0/all/0/1\">Bonnie J Dorr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using novel data and ensemble models to improve automated labeling of Sustainable Development Goals. (arXiv:2301.11353v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11353","description":"<p>A number of labeling systems based on text have been proposed to help monitor\nwork on the United Nations (UN) Sustainable Development Goals (SDGs). Here, we\npresent a systematic comparison of systems using a variety of text sources and\nshow that systems differ considerably in their specificity (i.e., true-positive\nrate) and sensitivity (i.e., true-negative rate), have systematic biases (e.g.,\nare more sensitive to specific SDGs relative to others), and are susceptible to\nthe type and amount of text analyzed. We then show that an ensemble model that\npools labeling systems alleviates some of these limitations, exceeding the\nlabeling performance of all currently available systems. We conclude that\nresearchers and policymakers should care about the choice of labeling system\nand that ensemble methods should be favored when drawing conclusions about the\nabsolute and relative prevalence of work on the SDGs based on automated\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wulff_D/0/1/0/all/0/1\">Dirk U. Wulff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_D/0/1/0/all/0/1\">Dominik S. Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mata_R/0/1/0/all/0/1\">Rui Mata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REPLUG: Retrieval-Augmented Black-Box Language Models. (arXiv:2301.12652v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12652","description":"<p>We introduce REPLUG, a retrieval-augmented language modeling framework that\ntreats the language model (LM) as a black box and augments it with a tuneable\nretrieval model. Unlike prior retrieval-augmented LMs that train language\nmodels with special cross attention mechanisms to encode the retrieved text,\nREPLUG simply prepends retrieved documents to the input for the frozen\nblack-box LM. This simple design can be easily applied to any existing\nretrieval and language models. Furthermore, we show that the LM can be used to\nsupervise the retrieval model, which can then find documents that help the LM\nmake better predictions. Our experiments demonstrate that REPLUG with the tuned\nretriever significantly improves the performance of GPT-3 (175B) on language\nmodeling by 6.3%, as well as the performance of Codex on five-shot MMLU by\n5.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_R/0/1/0/all/0/1\">Rich James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}