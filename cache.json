{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-26T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Pre-trained Embeddings for Entity Resolution: An Experimental Analysis [Experiment, Analysis & Benchmark]. (arXiv:2304.12329v1 [cs.DB])","link":"http://arxiv.org/abs/2304.12329","description":"<p>Many recent works on Entity Resolution (ER) leverage Deep Learning techniques\ninvolving language models to improve effectiveness. This is applied to both\nmain steps of ER, i.e., blocking and matching. Several pre-trained embeddings\nhave been tested, with the most popular ones being fastText and variants of the\nBERT model. However, there is no detailed analysis of their pros and cons. To\ncover this gap, we perform a thorough experimental analysis of 12 popular\nlanguage models over 17 established benchmark datasets. First, we assess their\nvectorization overhead for converting all input entities into dense embeddings\nvectors. Second, we investigate their blocking performance, performing a\ndetailed scalability analysis, and comparing them with the state-of-the-art\ndeep learning-based blocking method. Third, we conclude with their relative\nperformance for both supervised and unsupervised matching. Our experimental\nresults provide novel insights into the strengths and weaknesses of the main\nlanguage models, facilitating researchers and practitioners to select the most\nsuitable ones in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeakis_A/0/1/0/all/0/1\">Alexandros Zeakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_G/0/1/0/all/0/1\">George Papadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skoutas_D/0/1/0/all/0/1\">Dimitrios Skoutas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koubarakis_M/0/1/0/all/0/1\">Manolis Koubarakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"USTEP: Structuration des logs en flux gr{\\^a}ce {\\`a} un arbre de recherche {\\'e}volutif. (arXiv:2304.12331v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12331","description":"<p>Logs record valuable system information at runtime. They are widely used by\ndata-driven approaches for development and monitoring purposes. Parsing log\nmessages to structure their format is a classic preliminary step for log-mining\ntasks. As they appear upstream, parsing operations can become a processing time\nbottleneck for downstream applications. The quality of parsing also has a\ndirect influence on their efficiency. Here, we propose USTEP, an online log\nparsing method based on an evolving tree structure. Evaluation results on a\nwide panel of datasets coming from different real-world systems demonstrate\nUSTEP superiority in terms of both effectiveness and robustness when compared\nto other online methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vervaet_A/0/1/0/all/0/1\">Arthur Vervaet</a> (ISEP), <a href=\"http://arxiv.org/find/cs/1/au:+Chiky_R/0/1/0/all/0/1\">Raja Chiky</a> (ISEP), <a href=\"http://arxiv.org/find/cs/1/au:+Callau_Zori_M/0/1/0/all/0/1\">Mar Callau-Zori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Question-Answering Models on a Budget. (arXiv:2304.12370v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12370","description":"<p>Low-rank adaptation (LoRA) and question-answer datasets from large language\nmodels have made it much easier for much smaller models to be finetuned to the\npoint where they display sophisticated conversational abilities. In this paper,\nwe present Eluwa, a family of LoRA models that use the Stanford Alpaca dataset\nand massively improve the capabilities of Facebook's OPT 1.3B, 2.7B and 6.7B\nmodels. We benchmark these models in multiple ways, including letting GPT-4\njudge their answers to prompts that span general knowledge, writing,\nprogramming and other tasks. We show that smaller models here can be fine-tuned\nto be as performant as models 3x larger - all for as little as 40 USD in\ncompute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wijeratne_Y/0/1/0/all/0/1\">Yudhanjaya Wijeratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marikar_I/0/1/0/all/0/1\">Ishan Marikar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extreme Classification for Answer Type Prediction in Question Answering. (arXiv:2304.12395v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12395","description":"<p>Semantic answer type prediction (SMART) is known to be a useful step towards\neffective question answering (QA) systems. The SMART task involves predicting\nthe top-$k$ knowledge graph (KG) types for a given natural language question.\nThis is challenging due to the large number of types in KGs. In this paper, we\npropose use of extreme multi-label classification using Transformer models\n(XBERT) by clustering KG types using structural and semantic features based on\nquestion text. We specifically improve the clustering stage of the XBERT\npipeline using textual and structural features derived from KGs. We show that\nthese features can improve end-to-end performance for the SMART task, and yield\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Setty_V/0/1/0/all/0/1\">Vinay Setty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research. (arXiv:2304.12397v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12397","description":"<p>Perception of toxicity evolves over time and often differs between\ngeographies and cultural backgrounds. Similarly, black-box commercially\navailable APIs for detecting toxicity, such as the Perspective API, are not\nstatic, but frequently retrained to address any unattended weaknesses and\nbiases. We evaluate the implications of these changes on the reproducibility of\nfindings that compare the relative merits of models and methods that aim to\ncurb toxicity. Our findings suggest that research that relied on inherited\nautomatic toxicity scores to compare models and techniques may have resulted in\ninaccurate findings. Rescoring all models from HELM, a widely respected living\nbenchmark, for toxicity with the recent version of the API led to a different\nranking of widely used foundation models. We suggest caution in applying\napples-to-apples comparisons between studies and lay recommendations for a more\nstructured approach to evaluating toxicity over time. Code and data are\navailable at https://github.com/for-ai/black-box-api-challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pozzobon_L/0/1/0/all/0/1\">Luiza Pozzobon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermis_B/0/1/0/all/0/1\">Beyza Ermis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Tokenizer for Enhanced Natural Language Processing. (arXiv:2304.12404v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12404","description":"<p>Traditionally, NLP performance improvement has been focused on improving\nmodels and increasing the number of model parameters. NLP vocabulary\nconstruction has remained focused on maximizing the number of words represented\nthrough subword regularization. We present a novel tokenizer that uses\nsemantics to drive vocabulary construction. The tokenizer includes a trainer\nthat uses stemming to enhance subword formation. Further optimizations and\nadaptations are implemented to minimize the number of words that cannot be\nencoded. The encoder is updated to integrate with the trainer. The tokenizer is\nimplemented as a drop-in replacement for the SentencePiece tokenizer. The new\ntokenizer more than doubles the number of wordforms represented in the\nvocabulary. The enhanced vocabulary significantly improves NLP model\nconvergence, and improves quality of word and sentence embeddings. Our\nexperimental results show top performance on two Glue tasks using BERT-base,\nimproving on models more than 50X in size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sandeep Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Darpan Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_R/0/1/0/all/0/1\">Ravindra Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques. (arXiv:2304.12410v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12410","description":"<p>Recent parameter-efficient finetuning (PEFT) techniques aim to improve over\nthe considerable cost of fully finetuning large pretrained language models\n(PLM). As different PEFT techniques proliferate, it is becoming difficult to\ncompare them, in particular in terms of (i) the structure and functionality\nthey add to the PLM, (ii) the different types and degrees of efficiency\nimprovements achieved, (iii) performance at different downstream tasks, and\n(iv) how differences in structure and functionality relate to efficiency and\ntask performance. To facilitate such comparisons, this paper presents a\nreference framework which standardises aspects shared by different PEFT\ntechniques, while isolating differences to specific locations and interactions\nwith the standard components. Through this process of standardising and\nisolating differences, a modular view of PEFT techniques emerges, supporting\nnot only direct comparison of different techniques and their efficiency and\ntask performance, but also systematic exploration of reusability and\ncomposability of the different types of finetuned modules. We demonstrate how\nthe reference framework can be applied to understand properties and relative\nadvantages of PEFT techniques, hence to inform selection of techniques for\nspecific tasks, and design choices for new PEFT techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabry_M/0/1/0/all/0/1\">Mohammed Sabry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belz_A/0/1/0/all/0/1\">Anya Belz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT (Feb 13 Version) is a Chinese Room. (arXiv:2304.12411v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12411","description":"<p>ChatGPT has gained both positive and negative publicity after reports\nsuggesting that it is able to pass various professional and licensing\nexaminations. This suggests that ChatGPT may pass Turing Test in the near\nfuture. However, a computer program that passing Turing Test can either mean\nthat it is a Chinese Room or artificially conscious. Hence, the question of\nwhether the current state of ChatGPT is more of a Chinese Room or approaching\nartificial consciousness remains. Here, I demonstrate that the current version\nof ChatGPT (Feb 13 version) is a Chinese Room. Despite potential evidence of\ncognitive connections, ChatGPT exhibits critical errors in causal reasoning. At\nthe same time, I demonstrate that ChatGPT can generate all possible categorical\nresponses to the same question and response with erroneous examples; thus,\nquestioning its utility as a learning tool. I also show that ChatGPT is capable\nof artificial hallucination, which is defined as generating confidently wrong\nreplies. It is likely that errors in causal reasoning leads to hallucinations.\nMore critically, ChatGPT generates false references to mimic real publications.\nTherefore, its utility is cautioned.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_M/0/1/0/all/0/1\">Maurice HT Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TIGTEC : Token Importance Guided TExt Counterfactuals. (arXiv:2304.12425v1 [cs.LG])","link":"http://arxiv.org/abs/2304.12425","description":"<p>Counterfactual examples explain a prediction by highlighting changes of\ninstance that flip the outcome of a classifier. This paper proposes TIGTEC, an\nefficient and modular method for generating sparse, plausible and diverse\ncounterfactual explanations for textual data. TIGTEC is a text editing\nheuristic that targets and modifies words with high contribution using local\nfeature importance. A new attention-based local feature importance is proposed.\nCounterfactual candidates are generated and assessed with a cost function\nintegrating semantic distance, while the solution space is efficiently explored\nin a beam search fashion. The conducted experiments show the relevance of\nTIGTEC in terms of success rate, sparsity, diversity and plausibility. This\nmethod can be used in both model-specific or model-agnostic way, which makes it\nvery convenient for generating counterfactual explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhan_M/0/1/0/all/0/1\">Milan Bhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vittaut_J/0/1/0/all/0/1\">Jean-Noel Vittaut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chesneau_N/0/1/0/all/0/1\">Nicolas Chesneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lesot_M/0/1/0/all/0/1\">Marie-Jeanne Lesot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding and Predicting Human Label Variation in Natural Language Inference through Explanation. (arXiv:2304.12443v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12443","description":"<p>Human label variation (Plank 2022), or annotation disagreement, exists in\nmany natural language processing (NLP) tasks. To be robust and trusted, NLP\nmodels need to identify such variation and be able to explain it. To this end,\nwe created the first ecologically valid explanation dataset with diverse\nreasoning, LiveNLI. LiveNLI contains annotators' highlights and free-text\nexplanations for the label(s) of their choice for 122 English Natural Language\nInference items, each with at least 10 annotations. We used its explanations\nfor chain-of-thought prompting, and found there is still room for improvement\nin GPT-3's ability to predict label distribution with in-context learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nan-Jiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marneffe_M/0/1/0/all/0/1\">Marie-Catherine de Marneffe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RenderDiffusion: Text Generation as Image Generation. (arXiv:2304.12519v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12519","description":"<p>Diffusion models have become a new generative paradigm for text generation.\nConsidering the discrete categorical nature of text, in this paper, we propose\n\\textsc{RenderDiffusion}, a novel diffusion approach for text generation via\ntext-guided image generation. Our key idea is to render the target text as a\n\\emph{glyph image} containing visual language content. In this way, conditional\ntext generation can be cast as a glyph image generation task, and it is then\nnatural to apply continuous diffusion models to discrete texts. Specially, we\nutilize a cascaded architecture (\\ie a base and a super-resolution diffusion\nmodel) to generate high-fidelity glyph images, conditioned on the input text.\nFurthermore, we design a text grounding module to transform and refine the\nvisual language content from generated glyph images into the final texts. In\nexperiments over four conditional text generation tasks and two classes of\nmetrics (\\ie quality and diversity), \\textsc{RenderDiffusion} can achieve\ncomparable or even better results than several baselines, including pretrained\nlanguage models. Our model also makes significant improvements compared to the\nrecent diffusion model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KINLP at SemEval-2023 Task 12: Kinyarwanda Tweet Sentiment Analysis. (arXiv:2304.12569v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12569","description":"<p>This paper describes the system entered by the author to the SemEval-2023\nTask 12: Sentiment analysis for African languages. The system focuses on the\nKinyarwanda language and uses a language-specific model. Kinyarwanda morphology\nis modeled in a two tier transformer architecture and the transformer model is\npre-trained on a large text corpus using multi-task masked morphology\nprediction. The model is deployed on an experimental platform that allows users\nto experiment with the pre-trained language model fine-tuning without the need\nto write machine learning code. Our final submission to the shared task\nachieves second ranking out of 34 teams in the competition, achieving 72.50%\nweighted F1 score. Our analysis of the evaluation results highlights challenges\nin achieving high accuracy on the task and identifies areas for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nzeyimana_A/0/1/0/all/0/1\">Antoine Nzeyimana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explain like I am BM25: Interpreting a Dense Model's Ranked-List with a Sparse Approximation. (arXiv:2304.12631v1 [cs.IR])","link":"http://arxiv.org/abs/2304.12631","description":"<p>Neural retrieval models (NRMs) have been shown to outperform their\nstatistical counterparts owing to their ability to capture semantic meaning via\ndense document representations. These models, however, suffer from poor\ninterpretability as they do not rely on explicit term matching. As a form of\nlocal per-query explanations, we introduce the notion of equivalent queries\nthat are generated by maximizing the similarity between the NRM's results and\nthe result set of a sparse retrieval system with the equivalent query. We then\ncompare this approach with existing methods such as RM3-based query expansion\nand contrast differences in retrieval effectiveness and in the terms generated\nby each approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Llordes_M/0/1/0/all/0/1\">Michael Llordes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_D/0/1/0/all/0/1\">Debasis Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Sumit Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PUNR: Pre-training with User Behavior Modeling for News Recommendation. (arXiv:2304.12633v1 [cs.IR])","link":"http://arxiv.org/abs/2304.12633","description":"<p>News recommendation aims to predict click behaviors based on user behaviors.\nHow to effectively model the user representations is the key to recommending\npreferred news. Existing works are mostly focused on improvements in the\nsupervised fine-tuning stage. However, there is still a lack of PLM-based\nunsupervised pre-training methods optimized for user representations. In this\nwork, we propose an unsupervised pre-training paradigm with two tasks, i.e.\nuser behavior masking and user behavior generation, both towards effective user\nbehavior modeling. Firstly, we introduce the user behavior masking pre-training\ntask to recover the masked user behaviors based on their contextual behaviors.\nIn this way, the model could capture a much stronger and more comprehensive\nuser news reading pattern. Besides, we incorporate a novel auxiliary user\nbehavior generation pre-training task to enhance the user representation vector\nderived from the user encoder. We use the above pre-trained user modeling\nencoder to obtain news and user representations in downstream fine-tuning.\nEvaluations on the real-world news benchmark show significant performance\nimprovements over existing baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guangyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_W/0/1/0/all/0/1\">Wanhui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1\">Zhepeng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compressing Sentence Representation with maximum Coding Rate Reduction. (arXiv:2304.12674v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12674","description":"<p>In most natural language inference problems, sentence representation is\nneeded for semantic retrieval tasks. In recent years, pre-trained large\nlanguage models have been quite effective for computing such representations.\nThese models produce high-dimensional sentence embeddings. An evident\nperformance gap between large and small models exists in practice. Hence, due\nto space and time hardware limitations, there is a need to attain comparable\nresults when using the smaller model, which is usually a distilled version of\nthe large language model. In this paper, we assess the model distillation of\nthe sentence representation model Sentence-BERT by augmenting the pre-trained\ndistilled model with a projection layer additionally learned on the Maximum\nCoding Rate Reduction (MCR2)objective, a novel approach developed for\ngeneral-purpose manifold clustering. We demonstrate that the new language model\nwith reduced complexity and sentence embedding size can achieve comparable\nresults on semantic retrieval benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Severdija_D/0/1/0/all/0/1\">Domagoj &#x160;everdija</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prusina_T/0/1/0/all/0/1\">Tomislav Prusina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jovanovic_A/0/1/0/all/0/1\">Antonio Jovanovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borozan_L/0/1/0/all/0/1\">Luka Borozan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maltar_J/0/1/0/all/0/1\">Jurica Maltar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matijevic_D/0/1/0/all/0/1\">Domagoj Matijevi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What does BERT learn about prosody?. (arXiv:2304.12706v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12706","description":"<p>Language models have become nearly ubiquitous in natural language processing\napplications achieving state-of-the-art results in many tasks including\nprosody. As the model design does not define predetermined linguistic targets\nduring training but rather aims at learning generalized representations of the\nlanguage, analyzing and interpreting the representations that models implicitly\ncapture is important in bridging the gap between interpretability and model\nperformance. Several studies have explored the linguistic information that\nmodels capture providing some insights on their representational capacity.\nHowever, the current studies have not explored whether prosody is part of the\nstructural information of the language that models learn. In this work, we\nperform a series of experiments on BERT probing the representations captured at\ndifferent layers. Our results show that information about prosodic prominence\nspans across many layers but is mostly focused in middle layers suggesting that\nBERT relies mostly on syntactic and semantic information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kakouros_S/0/1/0/all/0/1\">Sofoklis Kakouros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OMahony_J/0/1/0/all/0/1\">Johannah O&#x27;Mahony</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CitePrompt: Using Prompts to Identify Citation Intent in Scientific Papers. (arXiv:2304.12730v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12730","description":"<p>Citations in scientific papers not only help us trace the intellectual\nlineage but also are a useful indicator of the scientific significance of the\nwork. Citation intents prove beneficial as they specify the role of the\ncitation in a given context. In this paper, we present CitePrompt, a framework\nwhich uses the hitherto unexplored approach of prompt-based learning for\ncitation intent classification. We argue that with the proper choice of the\npretrained language model, the prompt template, and the prompt verbalizer, we\ncan not only get results that are better than or comparable to those obtained\nwith the state-of-the-art methods but also do it with much less exterior\ninformation about the scientific document. We report state-of-the-art results\non the ACL-ARC dataset, and also show significant improvement on the SciCite\ndataset over all baseline models except one. As suitably large labelled\ndatasets for citation intent classification can be quite hard to find, in a\nfirst, we propose the conversion of this task to the few-shot and zero-shot\nsettings. For the ACL-ARC dataset, we report a 53.86% F1 score for the\nzero-shot setting, which improves to 63.61% and 66.99% for the 5-shot and\n10-shot settings, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_A/0/1/0/all/0/1\">Avishek Lahiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_D/0/1/0/all/0/1\">Debarshi Kumar Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_I/0/1/0/all/0/1\">Imon Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-Time Adaptation with Perturbation Consistency Learning. (arXiv:2304.12764v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12764","description":"<p>Currently, pre-trained language models (PLMs) do not cope well with the\ndistribution shift problem, resulting in models trained on the training set\nfailing in real test scenarios. To address this problem, the test-time\nadaptation (TTA) shows great potential, which updates model parameters to suit\nthe test data at the testing time. Existing TTA methods rely on well-designed\nauxiliary tasks or self-training strategies based on pseudo-label. However,\nthese methods do not achieve good trade-offs regarding performance gains and\ncomputational costs. To obtain some insights into such a dilemma, we take two\nrepresentative TTA methods, i.e., Tent and OIL, for exploration and find that\nstable prediction is the key to achieving a good balance. Accordingly, in this\npaper, we propose perturbation consistency learning (PCL), a simple test-time\nadaptation method to promote the model to make stable predictions for samples\nwith distribution shifts. Extensive experiments on adversarial robustness and\ncross-lingual transferring demonstrate that our method can achieve higher or\ncomparable performance with less inference time over strong PLM backbones and\nprevious state-of-the-art TTA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yixin Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"State Spaces Aren't Enough: Machine Translation Needs Attention. (arXiv:2304.12776v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12776","description":"<p>Structured State Spaces for Sequences (S4) is a recently proposed sequence\nmodel with successful applications in various tasks, e.g. vision, language\nmodeling, and audio. Thanks to its mathematical formulation, it compresses its\ninput to a single hidden state, and is able to capture long range dependencies\nwhile avoiding the need for an attention mechanism. In this work, we apply S4\nto Machine Translation (MT), and evaluate several encoder-decoder variants on\nWMT'14 and WMT'16. In contrast with the success in language modeling, we find\nthat S4 lags behind the Transformer by approximately 4 BLEU points, and that it\ncounter-intuitively struggles with long sentences. Finally, we show that this\ngap is caused by S4's inability to summarize the full source sentence in a\nsingle hidden state, and show that we can close the gap by introducing an\nattention mechanism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vardasbi_A/0/1/0/all/0/1\">Ali Vardasbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pires_T/0/1/0/all/0/1\">Telmo Pessoa Pires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_R/0/1/0/all/0/1\">Robin M. Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peitz_S/0/1/0/all/0/1\">Stephan Peitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Voice Assistants Sound Cute? Towards a Model of Kawaii Vocalics. (arXiv:2304.12809v1 [cs.HC])","link":"http://arxiv.org/abs/2304.12809","description":"<p>The Japanese notion of \"kawaii\" or expressions of cuteness, vulnerability,\nand/or charm is a global cultural export. Work has explored kawaii-ness as a\ndesign feature and factor of user experience in the visual appearance,\nnonverbal behaviour, and sound of robots and virtual characters. In this\ninitial work, we consider whether voices can be kawaii by exploring the vocal\nqualities of voice assistant speech, i.e., kawaii vocalics. Drawing from an\nage-inclusive model of kawaii, we ran a user perceptions study on the\nkawaii-ness of younger- and older-sounding Japanese computer voices. We found\nthat kawaii-ness intersected with perceptions of gender and age, i.e., gender\nambiguous and girlish, as well as VA features, i.e., fluency and artificiality.\nWe propose an initial model of kawaii vocalics to be validated through the\nidentification and study of vocal qualities, cognitive appraisals, behavioural\nresponses, and affective reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seaborn_K/0/1/0/all/0/1\">Katie Seaborn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1\">Somang Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keckeis_J/0/1/0/all/0/1\">Julia Keckeis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itagaki_T/0/1/0/all/0/1\">Tatsuya Itagaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transcending the \"Male Code\": Implicit Masculine Biases in NLP Contexts. (arXiv:2304.12810v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12810","description":"<p>Critical scholarship has elevated the problem of gender bias in data sets\nused to train virtual assistants (VAs). Most work has focused on explicit\nbiases in language, especially against women, girls, femme-identifying people,\nand genderqueer folk; implicit associations through word embeddings; and\nlimited models of gender and masculinities, especially toxic masculinities,\nconflation of sex and gender, and a sex/gender binary framing of the masculine\nas diametric to the feminine. Yet, we must also interrogate how masculinities\nare \"coded\" into language and the assumption of \"male\" as the linguistic\ndefault: implicit masculine biases. To this end, we examined two natural\nlanguage processing (NLP) data sets. We found that when gendered language was\npresent, so were gender biases and especially masculine biases. Moreover, these\nbiases related in nuanced ways to the NLP context. We offer a new dictionary\ncalled AVA that covers ambiguous associations between gendered language and the\nlanguage of VAs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seaborn_K/0/1/0/all/0/1\">Katie Seaborn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_S/0/1/0/all/0/1\">Shruti Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabre_T/0/1/0/all/0/1\">Thibault Fabre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Dual of Shannon Information and Weighting Scheme. (arXiv:2304.12814v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12814","description":"<p>Shannon Information theory has achieved great success in not only\ncommunication technology where it was originally developed for but also many\nother science and engineering fields such as machine learning and artificial\nintelligence. Inspired by the famous weighting scheme TF-IDF, we discovered\nthat information entropy has a natural dual. We complement the classical\nShannon information theory by proposing a novel quantity, namely troenpy.\nTroenpy measures the certainty, commonness and similarity of the underlying\ndistribution. To demonstrate its usefulness, we propose a troenpy based\nweighting scheme for document with class labels, namely positive class\nfrequency (PCF). On a collection of public datasets we show the PCF based\nweighting scheme outperforms the classical TF-IDF and a popular Optimal\nTransportation based word moving distance algorithm in a kNN setting. We\nfurther developed a new odds-ratio type feature, namely Expected Class\nInformation Bias(ECIB), which can be regarded as the expected odds ratio of the\ninformation quantity entropy and troenpy. In the experiments we observe that\nincluding the new ECIB features and simple binary term features in a simple\nlogistic regression model can further significantly improve the performance.\nThe simple new weighting scheme and ECIB features are very effective and can be\ncomputed with linear order complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Arthur Jun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Information Theory of Certainty for Machine Learning. (arXiv:2304.12833v1 [cs.IT])","link":"http://arxiv.org/abs/2304.12833","description":"<p>Claude Shannon coined entropy to quantify the uncertainty of a random\ndistribution for communication coding theory. We observe that the uncertainty\nnature of entropy also limits its direct usage in mathematical modeling.\nTherefore we propose a new concept troenpy,as the canonical dual of entropy, to\nquantify the certainty of the underlying distribution. We demonstrate two\napplications in machine learning. The first is for the classical document\nclassification, we develop a troenpy based weighting scheme to leverage the\ndocument class label. The second is a self-troenpy weighting scheme for\nsequential data and show that it can be easily included in neural network based\nlanguage models and achieve dramatic perplexity reduction. We also define\nquantum troenpy as the dual of the Von Neumann entropy to quantify the\ncertainty of quantum systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Arthur Jun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lessons Learned from a Citizen Science Project for Natural Language Processing. (arXiv:2304.12836v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12836","description":"<p>Many Natural Language Processing (NLP) systems use annotated corpora for\ntraining and evaluation. However, labeled data is often costly to obtain and\nscaling annotation projects is difficult, which is why annotation tasks are\noften outsourced to paid crowdworkers. Citizen Science is an alternative to\ncrowdsourcing that is relatively unexplored in the context of NLP. To\ninvestigate whether and how well Citizen Science can be applied in this\nsetting, we conduct an exploratory study into engaging different groups of\nvolunteers in Citizen Science for NLP by re-annotating parts of a pre-existing\ncrowdsourced dataset. Our results show that this can yield high-quality\nannotations and attract motivated volunteers, but also requires considering\nfactors such as scalability, participation over time, and legal and ethical\nissues. We summarize lessons learned in the form of guidelines and provide our\ncode and data to aid future work on Citizen Science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klie_J/0/1/0/all/0/1\">Jan-Christoph Klie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Ji-Ung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stowe_K/0/1/0/all/0/1\">Kevin Stowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_G/0/1/0/all/0/1\">G&#xf6;zde G&#xfc;l &#x15e;ahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_N/0/1/0/all/0/1\">Nafise Sadat Moosavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bates_L/0/1/0/all/0/1\">Luke Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrak_D/0/1/0/all/0/1\">Dominic Petrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castilho_R/0/1/0/all/0/1\">Richard Eckart de Castilho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP-LTU at SemEval-2023 Task 10: The Impact of Data Augmentation and Semi-Supervised Learning Techniques on Text Classification Performance on an Imbalanced Dataset. (arXiv:2304.12847v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12847","description":"<p>In this paper, we propose a methodology for task 10 of SemEval23, focusing on\ndetecting and classifying online sexism in social media posts. The task is\ntackling a serious issue, as detecting harmful content on social media\nplatforms is crucial for mitigating the harm of these posts on users. Our\nsolution for this task is based on an ensemble of fine-tuned transformer-based\nmodels (BERTweet, RoBERTa, and DeBERTa). To alleviate problems related to class\nimbalance, and to improve the generalization capability of our model, we also\nexperiment with data augmentation and semi-supervised learning. In particular,\nfor data augmentation, we use back-translation, either on all classes, or on\nthe underrepresented classes only. We analyze the impact of these strategies on\nthe overall performance of the pipeline through extensive experiments. while\nfor semi-supervised learning, we found that with a substantial amount of\nunlabelled, in-domain data available, semi-supervised learning can enhance the\nperformance of certain models. Our proposed method (for which the source code\nis available on Github attains an F1-score of 0.8613 for sub-taskA, which\nranked us 10th in the competition\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Azzawi_S/0/1/0/all/0/1\">Sana Sabah Al-Azzawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovacs_G/0/1/0/all/0/1\">Gy&#xf6;rgy Kov&#xe1;cs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nilsson_F/0/1/0/all/0/1\">Filip Nilsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out-of-distribution Evidence-aware Fake News Detection via Dual Adversarial Debiasing. (arXiv:2304.12888v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12888","description":"<p>Evidence-aware fake news detection aims to conduct reasoning between news and\nevidence, which is retrieved based on news content, to find uniformity or\ninconsistency. However, we find evidence-aware detection models suffer from\nbiases, i.e., spurious correlations between news/evidence contents and\ntrue/fake news labels, and are hard to be generalized to Out-Of-Distribution\n(OOD) situations. To deal with this, we propose a novel Dual Adversarial\nLearning (DAL) approach. We incorporate news-aspect and evidence-aspect\ndebiasing discriminators, whose targets are both true/fake news labels, in DAL.\nThen, DAL reversely optimizes news-aspect and evidence-aspect debiasing\ndiscriminators to mitigate the impact of news and evidence content biases. At\nthe same time, DAL also optimizes the main fake news predictor, so that the\nnews-evidence interaction module can be learned. This process allows us to\nteach evidence-aware fake news detection models to better conduct news-evidence\nreasoning, and minimize the impact of content biases. To be noted, our proposed\nDAL approach is a plug-and-play module that works well with existing backbones.\nWe conduct comprehensive experiments under two OOD settings, and plug DAL in\nfour evidence-aware fake news detection backbones. Results demonstrate that,\nDAL significantly and stably outperforms the original backbones and some\ncompetitive debiasing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topological properties and organizing principles of semantic networks. (arXiv:2304.12940v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12940","description":"<p>Interpreting natural language is an increasingly important task in computer\nalgorithms due to the growing availability of unstructured textual data.\nNatural Language Processing (NLP) applications rely on semantic networks for\nstructured knowledge representation. The fundamental properties of semantic\nnetworks must be taken into account when designing NLP algorithms, yet they\nremain to be structurally investigated. We study the properties of semantic\nnetworks from ConceptNet, defined by 7 semantic relations from 11 different\nlanguages. We find that semantic networks have universal basic properties: they\nare sparse, highly clustered, and exhibit power-law degree distributions. Our\nfindings show that the majority of the considered networks are scale-free. Some\nnetworks exhibit language-specific properties determined by grammatical rules,\nfor example networks from highly inflected languages, such as e.g. Latin,\nGerman, French and Spanish, show peaks in the degree distribution that deviate\nfrom a power law. We find that depending on the semantic relation type and the\nlanguage, the link formation in semantic networks is guided by different\nprinciples. In some networks the connections are similarity-based, while in\nothers the connections are more complementarity-based. Finally, we demonstrate\nhow knowledge of similarity and complementarity in semantic networks can\nimprove NLP algorithms in missing link inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Budel_G/0/1/0/all/0/1\">Gabriel Budel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mieghem_P/0/1/0/all/0/1\">Piet Van Mieghem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitsak_M/0/1/0/all/0/1\">Maksim Kitsak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nondeterministic Stacks in Neural Networks. (arXiv:2304.12955v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12955","description":"<p>Human language is full of compositional syntactic structures, and although\nneural networks have contributed to groundbreaking improvements in computer\nsystems that process language, widely-used neural network architectures still\nexhibit limitations in their ability to process syntax. To address this issue,\nprior work has proposed adding stack data structures to neural networks,\ndrawing inspiration from theoretical connections between syntax and stacks.\nHowever, these methods employ deterministic stacks that are designed to track\none parse at a time, whereas syntactic ambiguity, which requires a\nnondeterministic stack to parse, is extremely common in language. In this\ndissertation, we remedy this discrepancy by proposing a method of incorporating\nnondeterministic stacks into neural networks. We develop a differentiable data\nstructure that efficiently simulates a nondeterministic pushdown automaton,\nrepresenting an exponential number of computations with a dynamic programming\nalgorithm. We incorporate this module into two predominant architectures:\nrecurrent neural networks (RNNs) and transformers. We show that this raises\ntheir formal recognition power to arbitrary context-free languages, and also\naids training, even on deterministic context-free languages. Empirically,\nneural networks with nondeterministic stacks learn context-free languages much\nmore effectively than prior stack-augmented models, including a language with\ntheoretically maximal parsing difficulty. We also show that an RNN augmented\nwith a nondeterminsitic stack is capable of surprisingly powerful behavior,\nsuch as learning cross-serial dependencies, a well-known non-context-free\npattern. We demonstrate improvements on natural language modeling and provide\nanalysis on a syntactic generalization benchmark. This work represents an\nimportant step toward building systems that learn to use syntax in more\nhuman-like fashion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DuSell_B/0/1/0/all/0/1\">Brian DuSell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Escaping the sentence-level paradigm in machine translation. (arXiv:2304.12959v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12959","description":"<p>It is well-known that document context is vital for resolving a range of\ntranslation ambiguities, and in fact the document setting is the most natural\nsetting for nearly all translation. It is therefore unfortunate that machine\ntranslation -- both research and production -- largely remains stuck in a\ndecades-old sentence-level translation paradigm. It is also an increasingly\nglaring problem in light of competitive pressure from large language models,\nwhich are natively document-based. Much work in document-context machine\ntranslation exists, but for various reasons has been unable to catch hold. This\npaper suggests a path out of this rut by addressing three impediments at once:\nwhat architectures should we use? where do we get document-level information\nfor training them? and how do we know whether they are any good? In contrast to\nwork on specialized architectures, we show that the standard Transformer\narchitecture is sufficient, provided it has enough capacity. Next, we address\nthe training data issue by taking document samples from back-translated data\nonly, where the data is not only more readily available, but is also of higher\nquality compared to parallel document data, which may contain machine\ntranslation output. Finally, we propose generative variants of existing\ncontrastive metrics that are better able to discriminate among document\nsystems. Results in four large-data language pairs (DE$\\rightarrow$EN,\nEN$\\rightarrow$DE, EN$\\rightarrow$FR, and EN$\\rightarrow$RU) establish the\nsuccess of these three pieces together in improving document-level performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junczys_Dowmunt_M/0/1/0/all/0/1\">Marcin Junczys-Dowmunt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GMNLP at SemEval-2023 Task 12: Sentiment Analysis with Phylogeny-Based Adapters. (arXiv:2304.12979v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12979","description":"<p>This report describes GMU's sentiment analysis system for the SemEval-2023\nshared task AfriSenti-SemEval. We participated in all three sub-tasks:\nMonolingual, Multilingual, and Zero-Shot. Our approach uses models initialized\nwith AfroXLMR-large, a pre-trained multilingual language model trained on\nAfrican languages and fine-tuned correspondingly. We also introduce augmented\ntraining data along with original training data. Alongside finetuning, we\nperform phylogeny-based adapter tuning to create several models and ensemble\nthe best models for the final submission. Our system achieves the best F1-score\non track 5: Amharic, with 6.2 points higher F1-score than the second-best\nperforming system on this track. Overall, our system ranks 5th among the 10\nsystems participating in all 15 tracks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Md Mahfuz Ibn Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Ruoyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faisal_F/0/1/0/all/0/1\">Fahim Faisal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intent Induction from Conversations for Task-Oriented Dialogue Track at DSTC 11. (arXiv:2304.12982v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12982","description":"<p>With increasing demand for and adoption of virtual assistants, recent work\nhas investigated ways to accelerate bot schema design through the automatic\ninduction of intents or the induction of slots and dialogue states. However, a\nlack of dedicated benchmarks and standardized evaluation has made progress\ndifficult to track and comparisons between systems difficult to make. This\nchallenge track, held as part of the Eleventh Dialog Systems Technology\nChallenge, introduces a benchmark that aims to evaluate methods for the\nautomatic induction of customer intents in a realistic setting of customer\nservice interactions between human agents and customers. We propose two\nsubtasks for progressively tackling the automatic induction of intents and\ncorresponding evaluation methodologies. We then present three datasets suitable\nfor evaluating the tasks and propose simple baselines. Finally, we summarize\nthe submissions and results of the challenge track, for which we received\nsubmissions from 34 teams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gung_J/0/1/0/all/0/1\">James Gung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_R/0/1/0/all/0/1\">Raphael Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeng_E/0/1/0/all/0/1\">Emily Moeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_W/0/1/0/all/0/1\">Wesley Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romeo_S/0/1/0/all/0/1\">Salvatore Romeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benajiba_Y/0/1/0/all/0/1\">Yassine Benajiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Arshit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Massive Multitask Chinese Understanding. (arXiv:2304.12986v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12986","description":"<p>The development of large-scale Chinese language models is flourishing, yet\nthere is a lack of corresponding capability assessments. Therefore, we propose\na test to measure the multitask accuracy of large Chinese language models. This\ntest encompasses four major domains, including medicine, law, psychology, and\neducation, with 15 subtasks in medicine and 8 subtasks in education. We found\nthat the best-performing models in the zero-shot setting outperformed the\nworst-performing models by nearly 22 percentage points on average. Across the\nfour major domains, the average zero-shot accuracy of all models did not exceed\n0.5. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shot\naccuracy of 0.703 in clinical medicine, which was the highest accuracy among\nall models across all subtasks. All models performed poorly in the legal\ndomain, with the highest zero-shot accuracy reaching only 0.259. By\ncomprehensively evaluating the breadth and depth of knowledge across multiple\ndisciplines, this test can more accurately identify the shortcomings of the\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hui Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head. (arXiv:2304.12995v1 [cs.CL])","link":"http://arxiv.org/abs/2304.12995","description":"<p>Large language models (LLMs) have exhibited remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. Despite the recent success, current LLMs are not capable of\nprocessing complex audio information or conducting spoken conversations (like\nSiri or Alexa). In this work, we propose a multi-modal AI system named\nAudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to\nprocess complex audio information and solve numerous understanding and\ngeneration tasks; and 2) the input/output interface (ASR, TTS) to support\nspoken dialogue. With an increasing demand to evaluate multi-modal LLMs of\nhuman intention understanding and cooperation with foundation models, we\noutline the principles and processes and test AudioGPT in terms of consistency,\ncapability, and robustness. Experimental results demonstrate the capabilities\nof AudioGPT in solving AI tasks with speech, music, sound, and talking head\nunderstanding and generation in multi-round dialogues, which empower humans to\ncreate rich and diverse audio content with unprecedented ease. Our system is\npublicly available at \\url{https://github.com/AIGC-Audio/AudioGPT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rongjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dongchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zhenhui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuning Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhiqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiawei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatLLM Network: More brains, More intelligence. (arXiv:2304.12998v1 [cs.AI])","link":"http://arxiv.org/abs/2304.12998","description":"<p>Dialogue-based language models mark a huge milestone in the field of\nartificial intelligence, by their impressive ability to interact with users, as\nwell as a series of challenging tasks prompted by customized instructions.\nHowever, the prevalent large-scale dialogue-based language models like ChatGPT\nstill have room for improvement, such as unstable responses to questions and\nthe inability to think cooperatively like humans. Considering the ability of\ndialogue-based language models in conversation and their inherent randomness in\nthinking, we propose ChatLLM network that allows multiple dialogue-based\nlanguage models to interact, provide feedback, and think together. We design\nthe network of ChatLLMs based on ChatGPT. Specifically, individual instances of\nChatGPT may possess distinct perspectives towards the same problem, and by\nconsolidating these diverse viewpoints via a separate ChatGPT, the ChatLLM\nnetwork system can conduct decision-making more objectively and\ncomprehensively. In addition, a language-based feedback mechanism comparable to\nbackpropagation is devised to update the ChatGPTs within the network.\nExperiments on two datasets demonstrate that our network attains significant\nimprovements in problem-solving, leading to observable progress amongst each\nmember.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_R/0/1/0/all/0/1\">Rui Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Linmei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Weijian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingliu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Inter-Bilingual Semantic Parsing for Indian Languages. (arXiv:2304.13005v1 [cs.CL])","link":"http://arxiv.org/abs/2304.13005","description":"<p>Despite significant progress in Natural Language Generation for Indian\nlanguages (IndicNLP), there is a lack of datasets around complex structured\ntasks such as semantic parsing. One reason for this imminent gap is the\ncomplexity of the logical form, which makes English to multilingual translation\ndifficult. The process involves alignment of logical forms, intents and slots\nwith translated unstructured utterance. To address this, we propose an\nInter-bilingual Seq2seq Semantic parsing dataset IE-SEMPARSE for 11 distinct\nIndian languages. We highlight the proposed task's practicality, and evaluate\nexisting multilingual seq2seq models across several train-test strategies. Our\nexperiment reveals a high correlation across performance of original\nmultilingual semantic parsing datasets (such as mTOP, multilingual TOP and\nmultiATIS++) and our proposed IE-SEMPARSE suite.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_D/0/1/0/all/0/1\">Divyanshu Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answering Questions by Meta-Reasoning over Multiple Chains of Thought. (arXiv:2304.13007v1 [cs.CL])","link":"http://arxiv.org/abs/2304.13007","description":"<p>Modern systems for multi-hop question answering (QA) typically break\nquestions into a sequence of reasoning steps, termed chain-of-thought (CoT),\nbefore arriving at a final answer. Often, multiple chains are sampled and\naggregated through a voting mechanism over the final answers, but the\nintermediate steps themselves are discarded. While such approaches improve\nperformance, they do not consider the relations between intermediate steps\nacross chains and do not provide a unified explanation for the predicted\nanswer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts\nlarge language models to meta-reason over multiple chains of thought, rather\nthan aggregating their answers. MCR examines different reasoning chains, mixes\ninformation between them and selects the most relevant facts in generating an\nexplanation and predicting the answer. MCR outperforms strong baselines on 7\nmulti-hop QA datasets. Moreover, our analysis reveals that MCR explanations\nexhibit high quality, enabling humans to verify its answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1\">Ori Yoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfson_T/0/1/0/all/0/1\">Tomer Wolfson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogin_B/0/1/0/all/0/1\">Ben Bogin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_U/0/1/0/all/0/1\">Uri Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutch_D/0/1/0/all/0/1\">Daniel Deutch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unstructured and structured data: Can we have the best of both worlds with large language models?. (arXiv:2304.13010v1 [cs.DB])","link":"http://arxiv.org/abs/2304.13010","description":"<p>This paper presents an opinion on the potential of using large language\nmodels to query on both unstructured and structured data. It also outlines some\nresearch challenges related to the topic of building question-answering systems\nfor both types of data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wang-Chiew Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Tradeoff Between Abstractiveness and Factuality in Abstractive Summarization. (arXiv:2108.02859v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02859","description":"<p>Neural models for abstractive summarization tend to generate output that is\nfluent and well-formed but lacks semantic faithfulness, or factuality, with\nrespect to the input documents. In this paper, we analyze the tradeoff between\nabstractiveness and factuality of generated summaries across multiple datasets\nand models, using extensive human evaluations of factuality. In our analysis,\nwe visualize the rates of change in factuality as we gradually increase\nabstractiveness using a decoding constraint, and we observe that, while\nincreased abstractiveness generally leads to a drop in factuality, the rate of\nfactuality decay depends on factors such as the data that the system was\ntrained on. We introduce two datasets with human factuality judgements; one\ncontaining 10.2k generated summaries with systematically varied degrees of\nabstractiveness; the other containing 4.2k summaries from five different\nsummarization models. We propose new factuality metrics that adjust for the\ndegree of abstractiveness, and we use them to compare the\nabstractiveness-adjusted factuality of previous summarization works, providing\nbaselines for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1\">Markus Dreyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_F/0/1/0/all/0/1\">Feng Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atluri_S/0/1/0/all/0/1\">Sandeep Atluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sujith Ravi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Hate Intensity of Twitter Conversation Threads. (arXiv:2206.08406v3 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2206.08406","description":"<p>Tweets are the most concise form of communication in online social media,\nwherein a single tweet has the potential to make or break the discourse of the\nconversation. Online hate speech is more accessible than ever, and stifling its\npropagation is of utmost importance for social media companies and users for\ncongenial communication. Most of the research barring a recent few has focused\non classifying an individual tweet regardless of the tweet thread/context\nleading up to that point. One of the classical approaches to curb hate speech\nis to adopt a reactive strategy after the hate speech postage. The ex-post\nfacto strategy results in neglecting subtle posts that do not show the\npotential to instigate hate speech on their own but may portend in the\nsubsequent discussion ensuing in the post's replies. In this paper, we propose\nDRAGNET++, which aims to predict the intensity of hatred that a tweet can bring\nin through its reply chain in the future. It uses the semantic and propagating\nstructure of the tweet threads to maximize the contextual information leading\nup to and the fall of hate intensity at each subsequent tweet. We explore three\npublicly available Twitter datasets -- Anti-Racism contains the reply tweets of\na collection of social media discourse on racist remarks during US political\nand Covid-19 background; Anti-Social presents a dataset of 40 million tweets\namidst the COVID-19 pandemic on anti-social behaviours; and Anti-Asian presents\nTwitter datasets collated based on anti-Asian behaviours during COVID-19\npandemic. All the curated datasets consist of structural graph information of\nthe Tweet threads. We show that DRAGNET++ outperforms all the state-of-the-art\nbaselines significantly. It beats the best baseline by an 11% margin on the\nPerson correlation coefficient and a decrease of 25% on RMSE for the\nAnti-Racism dataset with a similar performance on the other two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qing Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_T/0/1/0/all/0/1\">Tharun Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abstractive Meeting Summarization: A Survey. (arXiv:2208.04163v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.04163","description":"<p>A system that could reliably identify and sum up the most important points of\na conversation would be valuable in a wide variety of real-world contexts, from\nbusiness meetings to medical consultations to customer service calls. Recent\nadvances in deep learning, and especially the invention of encoder-decoder\narchitectures, has significantly improved language generation systems, opening\nthe door to improved forms of abstractive summarization, a form of\nsummarization particularly well-suited for multi-party conversation. In this\npaper, we provide an overview of the challenges raised by the task of\nabstractive meeting summarization and of the data sets, models and evaluation\nmetrics that have been used to tackle the problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rennard_V/0/1/0/all/0/1\">Virgile Rennard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_G/0/1/0/all/0/1\">Guokan Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hunter_J/0/1/0/all/0/1\">Julie Hunter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Interpretable Models with LLMs during Training. (arXiv:2209.11799v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2209.11799","description":"<p>Recent large language models (LLMs) have demonstrated remarkable prediction\nperformance for a growing array of tasks. However, their proliferation into\nhigh-stakes domains (e.g. medicine) and compute-limited settings has created a\nburgeoning need for interpretability and efficiency. We address this need by\nproposing Augmented Interpretable Models (Aug-imodels), a framework for\nleveraging the knowledge learned by LLMs to build extremely efficient and\ninterpretable models. Aug-imodels use LLMs during fitting but not during\ninference, allowing complete transparency and often a speed/memory improvement\nof greater than 1,000x for inference compared to LLMs. We explore two\ninstantiations of Aug-imodels in natural-language processing: (i) Aug-GAM,\nwhich augments a generalized additive model with decoupled embeddings from an\nLLM and (ii) Aug-Tree, which augments a decision tree with LLM feature\nexpansions. Across a variety of text-classification datasets, both outperform\ntheir non-augmented counterparts. Aug-GAM can even outperform much larger\nmodels (e.g. a 6-billion parameter GPT-J model), despite having 10,000x fewer\nparameters and being fully transparent. We further explore Aug-imodels in a\nnatural-language fMRI study, where they generate interesting interpretations\nfrom scientific data. All code for using Aug-imodels and reproducing results is\nmade available on Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1\">Chandan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askari_A/0/1/0/all/0/1\">Armin Askari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caruana_R/0/1/0/all/0/1\">Rich Caruana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects. (arXiv:2211.04604v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2211.04604","description":"<p>Robots operating in human environments must be able to rearrange objects into\nsemantically-meaningful configurations, even if these objects are previously\nunseen. In this work, we focus on the problem of building physically-valid\nstructures without step-by-step instructions. We propose StructDiffusion, which\ncombines a diffusion model and an object-centric transformer to construct\nstructures given partial-view point clouds and high-level language goals, such\nas \"set the table\". Our method can perform multiple challenging\nlanguage-conditioned multi-step 3D planning tasks using one model.\nStructDiffusion even improves the success rate of assembling physically-valid\nstructures out of unseen objects by on average 16% over an existing multi-modal\ntransformer model trained on specific structures. We show experiments on\nheld-out objects in both simulation and on real-world rearrangement tasks.\nImportantly, we show how integrating both a diffusion model and a\ncollision-discriminator model allows for improved generalization over other\nmethods when rearranging previously-unseen objects. For videos and additional\nresults, see our website: https://structdiffusion.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermans_T/0/1/0/all/0/1\">Tucker Hermans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernova_S/0/1/0/all/0/1\">Sonia Chernova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-Aware Language-Graph Transformer for Question Answering. (arXiv:2212.00975v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.00975","description":"<p>Question Answering (QA) is a task that entails reasoning over natural\nlanguage contexts, and many relevant works augment language models (LMs) with\ngraph neural networks (GNNs) to encode the Knowledge Graph (KG) information.\nHowever, most existing GNN-based modules for QA do not take advantage of rich\nrelational information of KGs and depend on limited information interaction\nbetween the LM and the KG. To address these issues, we propose Question\nAnswering Transformer (QAT), which is designed to jointly reason over language\nand graphs with respect to entity relations in a unified manner. Specifically,\nQAT constructs Meta-Path tokens, which learn relation-centric embeddings based\non diverse structural and semantic relations. Then, our Relation-Aware\nSelf-Attention module comprehensively integrates different modalities via the\nCross-Modal Relative Position Bias, which guides information exchange between\nrelevant entites of different modalities. We validate the effectiveness of QAT\non commonsense question answering datasets like CommonsenseQA and OpenBookQA,\nand on a medical question answering dataset, MedQA-USMLE. On all the datasets,\nour method achieves state-of-the-art performance. Our code is available at\n<a href=\"http://github.com/mlvlab/QAT.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hyeong Kyu Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Juyeon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyeonjin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Ji-Hoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jisu Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyungmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo J. Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.06373","description":"<p>Current approaches to empathetic response generation typically encode the\nentire dialogue history directly and put the output into a decoder to generate\nfriendly feedback. These methods focus on modelling contextual information but\nneglect capturing the direct intention of the speaker. We argue that the last\nutterance in the dialogue empirically conveys the intention of the speaker.\nConsequently, we propose a novel model named InferEM for empathetic response\ngeneration. We separately encode the last utterance and fuse it with the entire\ndialogue through the multi-head attention based intention fusion module to\ncapture the speaker's intention. Besides, we utilize previous utterances to\npredict the last utterance, which simulates human's psychology to guess what\nthe interlocutor may speak in advance. To balance the optimizing rates of the\nutterance prediction and response generation, a multi-task learning strategy is\ndesigned for InferEM. Experimental results demonstrate the plausibility and\nvalidity of InferEM in improving empathetic expression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1\">Guoqing Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhigang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Poor Man's Quality Estimation: Predicting Reference-Based MT Metrics Without the Reference. (arXiv:2301.09008v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.09008","description":"<p>Machine translation quality estimation (QE) predicts human judgements of a\ntranslation hypothesis without seeing the reference. State-of-the-art QE\nsystems based on pretrained language models have been achieving remarkable\ncorrelations with human judgements yet they are computationally heavy and\nrequire human annotations, which are slow and expensive to create. To address\nthese limitations, we define the problem of metric estimation (ME) where one\npredicts the automated metric scores also without the reference. We show that\neven without access to the reference, our model can estimate automated metrics\n($\\rho$=60% for BLEU, $\\rho$=51% for other metrics) at the sentence-level.\nBecause automated metrics correlate with human judgements, we can leverage the\nME task for pre-training a QE model. For the QE task, we find that pre-training\non TER is better ($\\rho$=23%) than training for scratch ($\\rho$=20%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zouhar_V/0/1/0/all/0/1\">Vil&#xe9;m Zouhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daheim_N/0/1/0/all/0/1\">Nico Daheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocmi_T/0/1/0/all/0/1\">Tom Kocmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Eleanor Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hitachi at SemEval-2023 Task 3: Exploring Cross-lingual Multi-task Strategies for Genre and Framing Detection in Online News. (arXiv:2303.01794v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.01794","description":"<p>This paper explains the participation of team Hitachi to SemEval-2023 Task 3\n\"Detecting the genre, the framing, and the persuasion techniques in online news\nin a multi-lingual setup.'' Based on the multilingual, multi-task nature of the\ntask and the low-resource setting, we investigated different cross-lingual and\nmulti-task strategies for training the pretrained language models. Through\nextensive experiments, we found that (a) cross-lingual/multi-task training, and\n(b) collecting an external balanced dataset, can benefit the genre and framing\ndetection. We constructed ensemble models from the results and achieved the\nhighest macro-averaged F1 scores in Italian and Russian genre categorization\nsubtasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koreeda_Y/0/1/0/all/0/1\">Yuta Koreeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokote_K/0/1/0/all/0/1\">Ken-ichi Yokote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozaki_H/0/1/0/all/0/1\">Hiroaki Ozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1\">Atsuki Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsunokake_M/0/1/0/all/0/1\">Masaya Tsunokake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogawa_Y/0/1/0/all/0/1\">Yasuhiro Sogawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a Good NLG Evaluator? A Preliminary Study. (arXiv:2303.04048v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.04048","description":"<p>Recently, the emergence of ChatGPT has attracted wide attention from the\ncomputational linguistics community. Many prior studies have shown that ChatGPT\nachieves remarkable performance on various NLP tasks in terms of automatic\nevaluation metrics. However, the ability of ChatGPT to serve as an evaluation\nmetric is still underexplored. Considering assessing the quality of natural\nlanguage generation (NLG) models is an arduous task and NLG metrics notoriously\nshow their poor correlation with human judgments, we wonder whether ChatGPT is\na good NLG evaluation metric. In this report, we provide a preliminary\nmeta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail,\nwe regard ChatGPT as a human evaluator and give task-specific (e.g.,\nsummarization) and aspect-specific (e.g., relevance) instruction to prompt\nChatGPT to evaluate the generated results of NLG models. We conduct experiments\non five NLG meta-evaluation datasets (including summarization, story generation\nand data-to-text tasks). Experimental results show that compared with previous\nautomatic metrics, ChatGPT achieves state-of-the-art or competitive correlation\nwith human judgments in most cases. In addition, we find that the effectiveness\nof the ChatGPT evaluator might be influenced by the creation method of the\nmeta-evaluation datasets. For the meta-evaluation datasets which are created\ngreatly depending on the reference and thus are biased, the ChatGPT evaluator\nmight lose its effectiveness. We hope our preliminary study could prompt the\nemergence of a general-purposed reliable NLG metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zengkui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haoxiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jianfeng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Large Language Models. (arXiv:2303.18223v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.18223","description":"<p>Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yupeng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Yingqian Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zican Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yushuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xinyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RPTQ: Reorder-based Post-training Quantization for Large Language Models. (arXiv:2304.01089v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01089","description":"<p>Large-scale language models (LLMs) have demonstrated outstanding performance\non various tasks, but their deployment poses challenges due to their enormous\nmodel size. In this paper, we identify that the main challenge in quantizing\nLLMs stems from the different activation ranges between the channels, rather\nthan just the issue of outliers.We propose a novel reorder-based quantization\napproach, RPTQ, that addresses the issue of quantizing the activations of LLMs.\nRPTQ rearranges the channels in the activations and then quantizing them in\nclusters, thereby reducing the impact of range difference of channels. In\naddition, we reduce the storage and computation overhead by avoiding explicit\nreordering. By implementing this approach, we achieved a significant\nbreakthrough by pushing LLM models to 3 bit activation for the first time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Lin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yuzhang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bingzhe Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Classification of Legal Document Pages. (arXiv:2304.02787v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02787","description":"<p>For many business applications that require the processing, indexing, and\nretrieval of professional documents such as legal briefs (in PDF format etc.),\nit is often essential to classify the pages of any given document into their\ncorresponding types beforehand. Most existing studies in the field of document\nimage classification either focus on single-page documents or treat multiple\npages in a document independently. Although in recent years a few techniques\nhave been proposed to exploit the context information from neighboring pages to\nenhance document page classification, they typically cannot be utilized with\nlarge pre-trained language models due to the constraint on input length. In\nthis paper, we present a simple but effective approach that overcomes the above\nlimitation. Specifically, we enhance the input with extra tokens carrying\nsequential information about previous pages - introducing recurrence - which\nenables the usage of pre-trained Transformer models like BERT for context-aware\npage classification. Our experiments conducted on two legal datasets in English\nand Portuguese respectively show that the proposed approach can significantly\nimprove the performance of document page classification compared to the\nnon-recurrent setup as well as the other context-aware baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fragkogiannis_P/0/1/0/all/0/1\">Pavlos Fragkogiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forster_M/0/1/0/all/0/1\">Martina Forster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Grace E. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dell Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective Data Augmentation for Robust Speech Translation. (arXiv:2304.03169v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.03169","description":"<p>Speech translation (ST) systems translate speech in one language to text in\nanother language. End-to-end ST systems (e2e-ST) have gained popularity over\ncascade systems because of their enhanced performance due to reduced latency\nand computational cost. Though resource intensive, e2e-ST systems have the\ninherent ability to retain para and non-linguistic characteristics of the\nspeech unlike cascade systems. In this paper, we propose to use an e2e\narchitecture for English-Hindi (en-hi) ST. We use two imperfect machine\ntranslation (MT) services to translate Libri-trans en text into hi text. While\neach service gives MT data individually to generate parallel ST data, we\npropose a data augmentation strategy of noisy MT data to aid robust ST. The\nmain contribution of this paper is the proposal of a data augmentation\nstrategy. We show that this results in better ST (BLEU score) compared to brute\nforce augmentation of MT data. We observed an absolute improvement of 1.59 BLEU\nscore with our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Acharya_R/0/1/0/all/0/1\">Rajul Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_A/0/1/0/all/0/1\">Ashish Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopparapu_S/0/1/0/all/0/1\">Sunil Kumar Kopparapu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Open Instruction Generalist: A Preliminary Release. (arXiv:2304.07987v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.07987","description":"<p>Instruction tuning is widely recognized as a key technique for building\ngeneralist language models, which has attracted the attention of researchers\nand the public with the release of InstructGPT~\\citep{ouyang2022training} and\nChatGPT\\footnote{\\url{https://chat.openai.com/}}. Despite impressive progress\nin English-oriented large-scale language models (LLMs), it is still\nunder-explored whether English-based foundation LLMs can perform similarly on\nmultilingual tasks compared to English tasks with well-designed instruction\ntuning and how we can construct the corpora needed for the tuning. To remedy\nthis gap, we propose the project as an attempt to create a Chinese instruction\ndataset by various methods adapted to the intrinsic characteristics of 4\nsub-tasks. We collect around 200k Chinese instruction tuning samples, which\nhave been manually checked to guarantee high quality. We also summarize the\nexisting English and Chinese instruction corpora and briefly describe some\npotential applications of the newly constructed Chinese instruction corpora.\nThe resulting \\textbf{C}hinese \\textbf{O}pen \\textbf{I}nstruction\n\\textbf{G}eneralist (\\textbf{COIG}) corpora are available in\nHuggingface\\footnote{\\url{https://huggingface.co/datasets/BAAI/COIG}} and\nGithub\\footnote{\\url{https://github.com/BAAI-Zlab/COIG}}, and will be\ncontinuously updated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yemin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Ruibin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yizhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Siwei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1\">Yu Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoqun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zekun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Autoregressive NLP Tasks via Modular Linearized Attention. (arXiv:2304.08453v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.08453","description":"<p>Various natural language processing (NLP) tasks necessitate models that are\nefficient and small based on their ultimate application at the edge or in other\nresource-constrained environments. While prior research has reduced the size of\nthese models, increasing computational efficiency without considerable\nperformance impacts remains difficult, especially for autoregressive tasks.\nThis paper proposes {modular linearized attention (MLA), which combines\nmultiple efficient attention mechanisms, including cosFormer, to maximize\ninference quality while achieving notable speedups. We validate this approach\non several autoregressive NLP tasks, including speech-to-text neural machine\ntranslation (S2T NMT), speech-to-text simultaneous translation (SimulST), and\nautoregressive text-to-spectrogram, noting efficiency gains on TTS and\ncompetitive performance for NMT and SimulST during training and inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agostinelli_V/0/1/0/all/0/1\">Victor Agostinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lizhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CB-Conformer: Contextual biasing Conformer for biased word recognition. (arXiv:2304.09607v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2304.09607","description":"<p>Due to the mismatch between the source and target domains, how to better\nutilize the biased word information to improve the performance of the automatic\nspeech recognition model in the target domain becomes a hot research topic.\nPrevious approaches either decode with a fixed external language model or\nintroduce a sizeable biasing module, which leads to poor adaptability and slow\ninference. In this work, we propose CB-Conformer to improve biased word\nrecognition by introducing the Contextual Biasing Module and the Self-Adaptive\nLanguage Model to vanilla Conformer. The Contextual Biasing Module combines\naudio fragments and contextual information, with only 0.2% model parameters of\nthe original Conformer. The Self-Adaptive Language Model modifies the internal\nweights of biased words based on their recall and precision, resulting in a\ngreater focus on biased words and more successful integration with the\nautomatic speech recognition model than the standard fixed language model. In\naddition, we construct and release an open-source Mandarin biased-word dataset\nbased on WenetSpeech. Experiments indicate that our proposed method brings a\n15.34% character error rate reduction, a 14.13% biased word recall increase,\nand a 6.80% biased word F1-score increase compared with the base Conformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yaoxun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Baiji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+and_Q/0/1/0/all/0/1\">Qiaochu Huang and</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingchen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Shiyin Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn What NOT to Learn: Towards Generative Safety in Chatbots. (arXiv:2304.11220v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.11220","description":"<p>Conversational models that are generative and open-domain are particularly\nsusceptible to generating unsafe content since they are trained on web-based\nsocial data. Prior approaches to mitigating this issue have drawbacks, such as\ndisrupting the flow of conversation, limited generalization to unseen toxic\ninput contexts, and sacrificing the quality of the dialogue for the sake of\nsafety. In this paper, we present a novel framework, named \"LOT\" (Learn NOT\nto), that employs a contrastive loss to enhance generalization by learning from\nboth positive and negative training signals. Our approach differs from the\nstandard contrastive learning framework in that it automatically obtains\npositive and negative signals from the safe and unsafe language distributions\nthat have been learned beforehand. The LOT framework utilizes divergence to\nsteer the generations away from the unsafe subspace and towards the safe\nsubspace while sustaining the flow of conversation. Our approach is memory and\ntime-efficient during decoding and effectively reduces toxicity while\npreserving engagingness and fluency. Empirical results indicate that LOT\nreduces toxicity by up to four-fold while achieving four to six-fold higher\nrates of engagingness and fluency compared to baseline models. Our findings are\nfurther corroborated by human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalatbari_L/0/1/0/all/0/1\">Leila Khalatbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yejin Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_W/0/1/0/all/0/1\">Willy Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadimi_S/0/1/0/all/0/1\">Saeed Ghadimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sameti_H/0/1/0/all/0/1\">Hossein Sameti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UBC-DLNLP at SemEval-2023 Task 12: Impact of Transfer Learning on African Sentiment Analysis. (arXiv:2304.11256v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.11256","description":"<p>We describe our contribution to the SemEVAl 2023 AfriSenti-SemEval shared\ntask, where we tackle the task of sentiment analysis in 14 different African\nlanguages. We develop both monolingual and multilingual models under a full\nsupervised setting (subtasks A and B). We also develop models for the zero-shot\nsetting (subtask C). Our approach involves experimenting with transfer learning\nusing six language models, including further pertaining of some of these models\nas well as a final finetuning stage. Our best performing models achieve an\nF1-score of 70.36 on development data and an F1-score of 66.13 on test data.\nUnsurprisingly, our results demonstrate the effectiveness of transfer learning\nand fine-tuning techniques for sentiment analysis across multiple languages.\nOur approach can be applied to other sentiment analysis tasks in different\nlanguages and domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_G/0/1/0/all/0/1\">Gagan Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adebara_I/0/1/0/all/0/1\">Ife Adebara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Theory-of-Mind Performance in Large Language Models via Prompting. (arXiv:2304.11490v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.11490","description":"<p>Large language models (LLMs) excel in many tasks in 2023, but they still face\nchallenges in complex reasoning. Theory-of-mind (ToM) tasks, which require\nunderstanding agents' beliefs, goals, and mental states, are essential for\ncommon-sense reasoning involving humans, making it crucial to enhance LLM\nperformance in this area. This study measures the ToM performance of GPT-4 and\nthree GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates\nthe effectiveness of in-context learning in improving their ToM comprehension.\nWe evaluated prompts featuring two-shot chain of thought reasoning and\nstep-by-step thinking instructions. We found that LLMs trained with\nReinforcement Learning from Human Feedback (RLHF) (all models excluding\nDavinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed\nbest in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell\nshort of the 87% human accuracy on the test set. However, when supplied with\nprompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM\naccuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate\nprompting enhances LLM ToM reasoning, and they underscore the context-dependent\nnature of LLM cognitive capacities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moghaddam_S/0/1/0/all/0/1\">Shima Rahimi Moghaddam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honey_C/0/1/0/all/0/1\">Christopher J. Honey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAIST-SIC-Aligned: Automatically-Aligned English-Japanese Simultaneous Interpretation Corpus. (arXiv:2304.11766v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.11766","description":"<p>It remains a question that how simultaneous interpretation (SI) data affects\nsimultaneous machine translation (SiMT). Research has been limited due to the\nlack of a large-scale training corpus. In this work, we aim to fill in the gap\nby introducing NAIST-SIC-Aligned, which is an automatically-aligned parallel\nEnglish-Japanese SI dataset. Starting with a non-aligned corpus NAIST-SIC, we\npropose a two-stage alignment approach to make the corpus parallel and thus\nsuitable for model training. The first stage is coarse alignment where we\nperform a many-to-many mapping between source and target sentences, and the\nsecond stage is fine-grained alignment where we perform intra- and\ninter-sentence filtering to improve the quality of aligned pairs. To ensure the\nquality of the corpus, each step has been validated either quantitatively or\nqualitatively. This is the first open-sourced large-scale parallel SI dataset\nin the literature. We also manually curated a small test set for evaluation\npurposes. We hope our work advances research on SI corpora construction and\nSiMT. Please find our data at \\url{https://github.com/mingzi151/AHC-SI}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_Y/0/1/0/all/0/1\">Yuka Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doi_K/0/1/0/all/0/1\">Kosuke Doi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukuda_R/0/1/0/all/0/1\">Ryo Fukuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1\">Katsuhito Sudoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by Identifying Important Nodes with Bridgeness. (arXiv:2304.12036v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2304.12036","description":"<p>Node representation learning in a network is an important machine learning\ntechnique for encoding relational information in a continuous vector space\nwhile preserving the inherent properties and structures of the network.\nRecently, unsupervised node embedding methods such as DeepWalk, LINE,\nstruc2vec, PTE, UserItem2vec, and RWJBG have emerged from the Skip-gram model\nand perform better performance in several downstream tasks such as node\nclassification and link prediction than the existing relational models.\nHowever, providing post-hoc explanations of Skip-gram-based embeddings remains\na challenging problem because of the lack of explanation methods and\ntheoretical studies applicable for embeddings. In this paper, we first show\nthat global explanations to the Skip-gram-based embeddings can be found by\ncomputing bridgeness under a spectral cluster-aware local perturbation.\nMoreover, a novel gradient-based explanation method, which we call GRAPH-wGD,\nis proposed that allows the top-q global explanations about learned graph\nembedding vectors more efficiently. Experiments show that the ranking of nodes\nby scores using GRAPH-wGD is highly correlated with true bridgeness scores. We\nalso observe that the top-q node-level explanations selected by GRAPH-wGD have\nhigher importance scores and produce more changes in class label prediction\nwhen perturbed, compared with the nodes selected by recent alternatives, using\nfive real-world graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hogun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neville_J/0/1/0/all/0/1\">Jennifer Neville</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}