{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-10-12T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Domain-Specific Word Embeddings with Structure Prediction. (arXiv:2210.04962v1 [cs.CL])","link":"http://arxiv.org/abs/2210.04962","description":"<p>Complementary to finding good general word embeddings, an important question\nfor representation learning is to find dynamic word embeddings, e.g., across\ntime or domain. Current methods do not offer a way to use or predict\ninformation on structure between sub-corpora, time or domain and dynamic\nembeddings can only be compared after post-alignment. We propose novel word\nembedding methods that provide general word representations for the whole\ncorpus, domain-specific representations for each sub-corpus, sub-corpus\nstructure, and embedding alignment simultaneously. We present an empirical\nevaluation on New York Times articles and two English Wikipedia datasets with\narticles on science and philosophy. Our method, called Word2Vec with Structure\nPrediction (W2VPred), provides better performance than baselines in terms of\nthe general analogy tests, domain-specific analogy tests, and multiple specific\nword embedding evaluations as well as structure prediction performance when no\nstructure is given a priori. As a use case in the field of Digital Humanities\nwe demonstrate how to raise novel research questions for high literature from\nthe German Text Archive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1\">Stephanie Brandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassner_D/0/1/0/all/0/1\">David Lassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baillot_A/0/1/0/all/0/1\">Anne Baillot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1\">Shinichi Nakajima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Every word counts: A multilingual analysis of individual human alignment with model attention. (arXiv:2210.04963v1 [cs.CL])","link":"http://arxiv.org/abs/2210.04963","description":"<p>Human fixation patterns have been shown to correlate strongly with\nTransformer-based attention. Those correlation analyses are usually carried out\nwithout taking into account individual differences between participants and are\nmostly done on monolingual datasets making it difficult to generalise findings.\nIn this paper, we analyse eye-tracking data from speakers of 13 different\nlanguages reading both in their native language (L1) and in English as language\nlearners (L2). We find considerable differences between languages but also that\nindividual reading behaviour such as skipping rate, total reading time and\nvocabulary knowledge (LexTALE) influence the alignment between humans and\nmodels to an extent that should be considered in future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1\">Stephanie Brandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollenstein_N/0/1/0/all/0/1\">Nora Hollenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Executable Action Plans with Environmentally-Aware Language Models. (arXiv:2210.04964v1 [cs.RO])","link":"http://arxiv.org/abs/2210.04964","description":"<p>Large Language Models (LLMs) trained using massive text datasets have\nrecently shown promise in generating action plans for robotic agents from high\nlevel text queries. However, these models typically do not consider the robot's\nenvironment, resulting in generated plans that may not actually be executable\ndue to ambiguities in the planned actions or environmental constraints. In this\npaper, we propose an approach to generate environmentally-aware action plans\nthat can be directly mapped to executable agent actions. Our approach involves\nintegrating environmental objects and object relations as additional inputs\ninto LLM action plan generation to provide the system with an awareness of its\nsurroundings, resulting in plans where each generated action is mapped to\nobjects present in the scene. We also design a novel scoring function that,\nalong with generating the action steps and associating them with objects, helps\nthe system disambiguate among object instances and take into account their\nstates. We evaluate our approach using the VirtualHome simulator and the\nActivityPrograms knowledge base. Our results show that the action plans\ngenerated from our system outperform prior work in terms of their correctness\nand executability by 5.3% and 8.9% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gramopadhye_M/0/1/0/all/0/1\">Maitrey Gramopadhye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szafir_D/0/1/0/all/0/1\">Daniel Szafir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REV: Information-Theoretic Evaluation of Free-Text Rationales. (arXiv:2210.04982v1 [cs.CL])","link":"http://arxiv.org/abs/2210.04982","description":"<p>Free-text rationales are a promising step towards explainable AI, yet their\nevaluation remains an open research problem. While existing metrics have mostly\nfocused on measuring the direct association between the rationale and a given\nlabel, we argue that an ideal metric should also be able to focus on the new\ninformation uniquely provided in the rationale that is otherwise not provided\nin the input or the label. We investigate this research problem from an\ninformation-theoretic perspective using the conditional V-information. More\nconcretely, we propose a metric called REV (Rationale Evaluation with\nconditional V-information), that can quantify the new information in a\nrationale supporting a given label beyond the information already available in\nthe input or the label. Experiments on reasoning tasks across four benchmarks,\nincluding few-shot prompting with GPT-3, demonstrate the effectiveness of REV\nin evaluating different types of rationale-label pairs, compared to existing\nmetrics. Through several quantitative comparisons, we demonstrate the\ncapability of REV in providing more sensitive measurements of new information\nin free-text rationales with respect to a label. Furthermore, REV is consistent\nwith human judgments on rationale evaluations. Overall, when used alongside\ntraditional performance metrics, REV provides deeper insights into a models'\nreasoning and prediction processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting or Guessing? Improving Faithfulness of Event Temporal Relation Extraction. (arXiv:2210.04992v1 [cs.CL])","link":"http://arxiv.org/abs/2210.04992","description":"<p>In this paper, we seek to improve the faithfulness of \\temprel extraction\nmodels from two perspectives. The first perspective is to extract genuinely\nbased on contextual description. To achieve this, we propose to conduct\ncounterfactual analysis to attenuate the effects of two significant types of\ntraining biases: the event trigger bias and the frequent label bias. We also\nadd tense information into event representations to explicitly place an\nemphasis on the contextual description. The second perspective is to provide\nproper uncertainty estimation and abstain from extraction when no relation is\ndescribed in the text. By parameterization of Dirichlet Prior over the\nmodel-predicted categorical distribution, we improve the model estimates of the\ncorrectness likelihood and make TempRel predictions more selective. We also\nemploy temperature scaling to recalibrate the model confidence measure after\nbias mitigation. Through experimental analysis on MATRES, MATRES-DS, and\nTDDiscourse, we demonstrate that our model extracts TempRel and timelines more\nfaithfully compared to SOTA methods, especially under distribution shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuqian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1\">Jacob R. Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Representation Distillation with Contrastive Learning. (arXiv:2210.05033v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05033","description":"<p>Multilingual sentence representations from large models can encode semantic\ninformation from two or more languages and can be used for different\ncross-lingual information retrieval tasks. In this paper, we integrate\ncontrastive learning into multilingual representation distillation and use it\nfor quality estimation of parallel sentences (find semantically similar\nsentences that can be used as translations of each other). We validate our\napproach with multilingual similarity search and corpus filtering tasks.\nExperiments across different low-resource languages show that our method\nsignificantly outperforms previous sentence encoders such as LASER, LASER3, and\nLaBSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weiting Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_K/0/1/0/all/0/1\">Kevin Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Errors are Equal: Learning Text Generation Metrics using Stratified Error Synthesis. (arXiv:2210.05035v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05035","description":"<p>Is it possible to build a general and automatic natural language generation\n(NLG) evaluation metric? Existing learned metrics either perform\nunsatisfactorily or are restricted to tasks where large human rating data is\nalready available. We introduce SESCORE, a model-based metric that is highly\ncorrelated with human judgements without requiring human annotation, by\nutilizing a novel, iterative error synthesis and severity scoring pipeline.\nThis pipeline applies a series of plausible errors to raw text and assigns\nseverity labels by simulating human judgements with entailment. We evaluate\nSESCORE against existing metrics by comparing how their scores correlate with\nhuman ratings. SESCORE outperforms all prior unsupervised metrics on multiple\ndiverse NLG tasks including machine translation, image captioning, and WebNLG\ntext generation. For WMT 20/21 En-De and Zh-En, SESCORE improve the average\nKendall correlation with human judgement from 0.154 to 0.195. SESCORE even\nachieves comparable performance to the best supervised metric COMET, despite\nreceiving no human-annotated training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1\">Yilin Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video Retrieval Benchmarks. (arXiv:2210.05038v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05038","description":"<p>Searching vast troves of videos with textual descriptions is a core\nmultimodal retrieval task. Owing to the lack of a purpose-built dataset for\ntext-to-video retrieval, video captioning datasets have been re-purposed to\nevaluate models by (1) treating captions as positive matches to their\nrespective videos and (2) all other videos as negatives. However, this\nmethodology leads to a fundamental flaw during evaluation: since captions are\nmarked as relevant only to their original video, many alternate videos also\nmatch the caption, which creates false-negative caption-video pairs. We show\nthat when these false negatives are corrected, a recent state-of-the-art model\ngains 25% recall points -- a difference that threatens the validity of the\nbenchmark itself. To diagnose and mitigate this issue, we annotate and release\n683K additional caption-video pairs. Using these, we recompute effectiveness\nscores for three models on two standard benchmarks (MSR-VTT and MSVD). We find\nthat (1) the recomputed metrics are up to 25% recall points higher for the best\nmodels, (2) these benchmarks are nearing saturation for Recall@10, (3) caption\nlength (generality) is related to the number of positives, and (4) annotation\ncosts can be mitigated by choosing evaluation sizes corresponding to desired\neffect size to detect. We recommend retiring these benchmarks in their current\nform and make recommendations for future text-to-video retrieval benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pedro Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azab_M/0/1/0/all/0/1\">Mahmoud Azab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvert_B/0/1/0/all/0/1\">Becka Silvert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1\">Renato Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labson_L/0/1/0/all/0/1\">Linzy Labson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1\">Hardik Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seungwhan Moon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling. (arXiv:2210.05043v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05043","description":"<p>Ensembling BERT models often significantly improves accuracy, but at the cost\nof significantly more computation and memory footprint. In this work, we\npropose Multi-CLS BERT, a novel ensembling method for CLS-based prediction\ntasks that is almost as efficient as a single BERT model. Multi-CLS BERT uses\nmultiple CLS tokens with a parameterization and objective that encourages their\ndiversity. Thus instead of fine-tuning each BERT model in an ensemble (and\nrunning them all at test time), we need only fine-tune our single Multi-CLS\nBERT model (and run the one model at test time, ensembling just the multiple\nfinal CLS embeddings). To test its effectiveness, we build Multi-CLS BERT on\ntop of a state-of-the-art pretraining method for BERT (Aroca-Ouellette and\nRudzicz, 2020). In experiments on GLUE and SuperGLUE we show that our Multi-CLS\nBERT reliably improves both overall accuracy and confidence estimation. When\nonly 100 training samples are available in GLUE, the Multi-CLS BERT_Base model\ncan even outperform the corresponding BERT_Large model. We analyze the behavior\nof our Multi-CLS BERT, showing that it has many of the same characteristics and\nbehavior as a typical BERT 5-way ensemble, but with nearly 4-times less\ncomputation and memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Haw-Shiuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruei-Yao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_K/0/1/0/all/0/1\">Kathryn Ricci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Retrieval Augmented Neural Machine Translation by Controlling Source and Fuzzy-Match Interactions. (arXiv:2210.05047v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05047","description":"<p>We explore zero-shot adaptation, where a general-domain model has access to\ncustomer or domain specific parallel data at inference time, but not during\ntraining. We build on the idea of Retrieval Augmented Translation (RAT) where\ntop-k in-domain fuzzy matches are found for the source sentence, and\ntarget-language translations of those fuzzy-matched sentences are provided to\nthe translation model at inference time. We propose a novel architecture to\ncontrol interactions between a source sentence and the top-k fuzzy\ntarget-language matches, and compare it to architectures from prior work. We\nconduct experiments in two language pairs (En-De and En-Fr) by training models\non WMT data and testing them with five and seven multi-domain datasets,\nrespectively. Our approach consistently outperforms the alternative\narchitectures, improving BLEU across language pair, domain, and number k of\nfuzzy matches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoang_C/0/1/0/all/0/1\">Cuong Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_D/0/1/0/all/0/1\">Devendra Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_P/0/1/0/all/0/1\">Prashant Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1\">Brian Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Robustness of Retrieval Augmented Translation via Shuffling of Suggestions. (arXiv:2210.05059v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05059","description":"<p>Several recent studies have reported dramatic performance improvements in\nneural machine translation (NMT) by augmenting translation at inference time\nwith fuzzy-matches retrieved from a translation memory (TM). However, these\nstudies all operate under the assumption that the TMs available at test time\nare highly relevant to the testset. We demonstrate that for existing retrieval\naugmented translation methods, using a TM with a domain mismatch to the test\nset can result in substantially worse performance compared to not using a TM at\nall. We propose a simple method to expose fuzzy-match NMT systems during\ntraining and show that it results in a system that is much more tolerant\n(regaining up to 5.8 BLEU) to inference with TMs with domain mismatch. Also,\nthe model is still competitive to the baseline when fed with suggestions from\nrelevant TMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoang_C/0/1/0/all/0/1\">Cuong Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_D/0/1/0/all/0/1\">Devendra Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_P/0/1/0/all/0/1\">Prashant Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1\">Brian Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reflection of Thought: Inversely Eliciting Numerical Reasoning in Language Models via Solving Linear Systems. (arXiv:2210.05075v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05075","description":"<p>Numerical reasoning over natural language has been a long-standing goal for\nthe research community. However, cutting-edge language models have proven\ndifficult to reliably generalize to a broad range of numbers, although they\nhave shown proficiency in reasoning over common and simple numbers. In this\npaper, we propose a novel method to elicit and exploit the numerical reasoning\nknowledge hidden in pre-trained language models using simple anchor numbers.\nConcretely, we first leverage simple numbers as anchors to probe the implicitly\ninferred arithmetic expressions from language models, and then explicitly apply\nthe expressions on complex numbers to get corresponding answers. To inversely\nelicit arithmetic expressions, we transform and formulate the task as an\nanalytically solvable linear system. Experimental results on several numerical\nreasoning benchmarks demonstrate that our approach significantly improves\nnumerical reasoning capabilities of existing LMs. More importantly, our\napproach is training-free and simply works in the inference phase, making it\nhighly portable and achieving consistent performance benefits across a variety\nof language models (GPT-3, T5, BART, etc) in all zero-shot, few-shot, and\nfine-tuning scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhoujun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Checks and Strategies for Enabling Code-Switched Machine Translation. (arXiv:2210.05096v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05096","description":"<p>Code-switching is a common phenomenon among multilingual speakers, where\nalternation between two or more languages occurs within the context of a single\nconversation. While multilingual humans can seamlessly switch back and forth\nbetween languages, multilingual neural machine translation (NMT) models are not\nrobust to such sudden changes in input. This work explores multilingual NMT\nmodels' ability to handle code-switched text. First, we propose checks to\nmeasure switching capability. Second, we investigate simple and effective data\naugmentation methods that can enhance an NMT model's ability to support\ncode-switching. Finally, by using a glass-box analysis of attention modules, we\ndemonstrate the effectiveness of these methods in improving robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_T/0/1/0/all/0/1\">Thamme Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gheini_M/0/1/0/all/0/1\">Mozhdeh Gheini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces. (arXiv:2210.05098v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05098","description":"<p>The ability to extract high-quality translation dictionaries from monolingual\nword embedding spaces depends critically on the geometric similarity of the\nspaces -- their degree of \"isomorphism.\" We address the root-cause of faulty\ncross-lingual mapping: that word embedding training resulted in the underlying\nspaces being non-isomorphic. We incorporate global measures of isomorphism\ndirectly into the skipgram loss function, successfully increasing the relative\nisomorphism of trained word embedding spaces and improving their ability to be\nmapped to a shared cross-lingual space. The result is improved bilingual\nlexicon induction in general data conditions, under domain mismatch, and with\ntraining algorithm dissimilarities. We release IsoVec at\nhttps://github.com/kellymarchisio/isovec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1\">Kelly Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_N/0/1/0/all/0/1\">Neha Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duh_K/0/1/0/all/0/1\">Kevin Duh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset. (arXiv:2210.05109v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05109","description":"<p>In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akil_A/0/1/0/all/0/1\">Ajwad Akil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultana_N/0/1/0/all/0/1\">Najrin Sultana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1\">Rifat Shahriyar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HUE: Pretrained Model and Dataset for Understanding Hanja Documents of Ancient Korea. (arXiv:2210.05112v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05112","description":"<p>Historical records in Korea before the 20th century were primarily written in\nHanja, an extinct language based on Chinese characters and not understood by\nmodern Korean or Chinese speakers. Historians with expertise in this time\nperiod have been analyzing the documents, but that process is very difficult\nand time-consuming, and language models would significantly speed up the\nprocess. Toward building and evaluating language models for Hanja, we release\nthe Hanja Understanding Evaluation dataset consisting of chronological\nattribution, topic classification, named entity recognition, and summary\nretrieval tasks. We also present BERT-based models continued training on the\ntwo major corpora from the 14th to the 19th centuries: the Annals of the Joseon\nDynasty and Diaries of the Royal Secretariats. We compare the models with\nseveral baselines on all tasks and show there are significant improvements\ngained by training on the two corpora. Additionally, we run zero-shot\nexperiments on the Daily Records of the Royal Court and Important Officials\n(DRRI). The DRRI dataset has not been studied much by the historians, and not\nat all by the NLP community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_H/0/1/0/all/0/1\">Haneul Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiho Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1\">Juhee Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bak_J/0/1/0/all/0/1\">JinYeong Bak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time-aware topic identification in social media with pre-trained language models: A case study of electric vehicles. (arXiv:2210.05143v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05143","description":"<p>Recent extensively competitive business environment makes companies to keep\ntheir eyes on social media, as there is a growing recognition over customer\nlanguages (e.g., needs, interests, and complaints) as source of future\nopportunities. This research avenue analysing social media data has received\nmuch attention in academia, but their utilities are limited as most of methods\nprovide retrospective results. Moreover, the increasing number of\ncustomer-generated contents and rapidly varying topics have made the necessity\nof time-aware topic evolution analyses. Recently, several researchers have\nshowed the applicability of pre-trained semantic language models to social\nmedia as an input feature, but leaving limitations in understanding evolving\ntopics. In this study, we propose a time-aware topic identification approach\nwith pre-trained language models. The proposed approach consists of two stages:\nthe dynamics-focused function for tracking time-varying topics with language\nmodels and the emergence-scoring function to examine future promising topics.\nHere we apply the proposed approach to reddit data on electric vehicles, and\nour findings highlight the feasibility of capturing emerging customer topics\nfrom voluminous social media in a time-aware manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_B/0/1/0/all/0/1\">Byeongki Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Janghyeok Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaewoong Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixture of Attention Heads: Selecting Attention Heads Per Token. (arXiv:2210.05144v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05144","description":"<p>Mixture-of-Experts (MoE) networks have been proposed as an efficient way to\nscale up model capacity and implement conditional computing. However, the study\nof MoE components mostly focused on the feedforward layer in Transformer\narchitecture. This paper proposes the Mixture of Attention Heads (MoA), a new\narchitecture that combines multi-head attention with the MoE mechanism. MoA\nincludes a set of attention heads that each has its own set of parameters.\nGiven an input, a router dynamically selects a subset of $k$ attention heads\nper token. This conditional computation schema allows MoA to achieve stronger\nperformance than the standard multi-head attention layer. Furthermore, the\nsparsely gated MoA can easily scale up the number of attention heads and the\nnumber of parameters while preserving computational efficiency. In addition to\nthe performance improvements, MoA also automatically differentiates heads'\nutilities, providing a new perspective to discuss the model's interpretability.\nWe conducted experiments on several important tasks, including Machine\nTranslation and Masked Language Modeling. Experiments have shown promising\nresults on several tasks against strong baselines that involve large and very\ndeep models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yikang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zeyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_W/0/1/0/all/0/1\">Wenge Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhang Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval Augmentation for T5 Re-ranker using External Sources. (arXiv:2210.05145v1 [cs.IR])","link":"http://arxiv.org/abs/2210.05145","description":"<p>Retrieval augmentation has shown promising improvements in different tasks.\nHowever, whether such augmentation can assist a large language model based\nre-ranker remains unclear. We investigate how to augment T5-based re-rankers\nusing high-quality information retrieved from two external corpora -- a\ncommercial web search engine and Wikipedia. We empirically demonstrate how\nretrieval augmentation can substantially improve the effectiveness of T5-based\nre-rankers for both in-domain and zero-shot out-of-domain re-ranking tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Kai Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1\">Honglei Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Mike Bendersky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Don Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSS: Combining Self-training and Self-supervised Learning for Few-shot Dialogue State Tracking. (arXiv:2210.05146v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05146","description":"<p>Few-shot dialogue state tracking (DST) is a realistic problem that trains the\nDST model with limited labeled data. Existing few-shot methods mainly transfer\nknowledge learned from external labeled dialogue data (e.g., from question\nanswering, dialogue summarization, machine reading comprehension tasks, etc.)\ninto DST, whereas collecting a large amount of external labeled data is\nlaborious, and the external data may not effectively contribute to the\nDST-specific task. In this paper, we propose a few-shot DST framework called\nCSS, which Combines Self-training and Self-supervised learning methods. The\nunlabeled data of the DST task is incorporated into the self-training\niterations, where the pseudo labels are predicted by a DST model trained on\nlimited labeled data in advance. Besides, a contrastive self-supervised method\nis used to learn better representations, where the data is augmented by the\ndropout operation to train the model. Experimental results on the MultiWOZ\ndataset show that our proposed CSS achieves competitive performance in several\nfew-shot scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haipeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Huaishao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Markup-to-Image Diffusion Models with Scheduled Sampling. (arXiv:2210.05147v1 [cs.LG])","link":"http://arxiv.org/abs/2210.05147","description":"<p>Building on recent advances in image generation, we present a fully\ndata-driven approach to rendering markup into images. The approach is based on\ndiffusion models, which parameterize the distribution of data using a sequence\nof denoising operations on top of a Gaussian noise distribution. We view the\ndiffusion denoising process as a sequential decision making process, and show\nthat it exhibits compounding errors similar to exposure bias issues in\nimitation learning problems. To mitigate these issues, we adapt the scheduled\nsampling algorithm to diffusion training. We conduct experiments on four markup\ndatasets: mathematical formulas (LaTeX), table layouts (HTML), sheet music\n(LilyPond), and molecular images (SMILES). These experiments each verify the\neffectiveness of the diffusion process and the use of scheduled sampling to fix\ngeneration issues. These results also show that the markup-to-image task\npresents a useful controlled compositional setting for diagnosing and analyzing\ngenerative image models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuntian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kojima_N/0/1/0/all/0/1\">Noriyuki Kojima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Failure of Batch Normalization for Transformers in NLP. (arXiv:2210.05153v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05153","description":"<p>Batch Normalization (BN) is a core and prevalent technique in accelerating\nthe training of deep neural networks and improving the generalization on\nComputer Vision (CV) tasks. However, it fails to defend its position in Natural\nLanguage Processing (NLP), which is dominated by Layer Normalization (LN). In\nthis paper, we are trying to answer why BN usually performs worse than LN in\nNLP tasks with Transformer models. We find that the inconsistency between\ntraining and inference of BN is the leading cause that results in the failure\nof BN in NLP. We define Training Inference Discrepancy (TID) to quantitatively\nmeasure this inconsistency and reveal that TID can indicate BN's performance,\nsupported by extensive experiments, including image classification, neural\nmachine translation, language modeling, sequence labeling, and text\nclassification tasks. We find that BN can obtain much better test performance\nthan LN when TID keeps small through training. To suppress the explosion of\nTID, we propose Regularized BN (RBN) that adds a simple regularization term to\nnarrow the gap between batch statistics and population statistics of BN. RBN\nimproves the performance of BN consistently and outperforms or is on par with\nLN on 17 out of 20 settings, involving ten datasets and two common variants of\nTransformer\\footnote{Our code is available at\n\\url{https://github.com/wjxts/RegularizedBN}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaxi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Ji Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering. (arXiv:2210.05156v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05156","description":"<p>Given its effectiveness on knowledge-intensive natural language processing\ntasks, dense retrieval models have become increasingly popular. Specifically,\nthe de-facto architecture for open-domain question answering uses two\nisomorphic encoders that are initialized from the same pretrained model but\nseparately parameterized for questions and passages. This bi-encoder\narchitecture is parameter-inefficient in that there is no parameter sharing\nbetween encoders. Further, recent studies show that such dense retrievers\nunderperform BM25 in various settings. We thus propose a new architecture,\nTask-aware Specialization for dense Retrieval (TASER), which enables parameter\nsharing by interleaving shared and specialized blocks in a single encoder. Our\nexperiments on five question answering datasets show that \\ourmodel\\ can\nachieve superior accuracy, surpassing BM25, while using about 60% of the\nparameters as bi-encoder dense retrievers. In out-of-domain evaluations, TASER\nis also empirically more robust than bi-encoder dense retrievers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Hao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Models Be Specific? How?. (arXiv:2210.05159v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05159","description":"<p>A good speaker not only needs to be correct, but also has the ability to be\nspecific when desired, and so are language models. In this paper, we propose to\nmeasure how specific the language of pre-trained language models (PLMs) is. To\nachieve this, we introduce a novel approach to build a benchmark for\nspecificity testing by forming masked token prediction tasks with prompts. For\ninstance, given ``J. K. Rowling was born in [MASK].'', we want to test whether\na more specific answer will be better filled in by PLMs, e.g., Yate instead of\nEngland. From our evaluations, we show that existing PLMs have only a slight\npreference for more specific answers. We identify underlying factors affecting\nthe specificity and design two prompt-based methods to improve the specificity.\nResults show that the specificity of the models can be improved by the proposed\nmethods without additional training. We believe this work can provide new\ninsights for language modeling and encourage the research community to further\nexplore this important but understudied problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwu_W/0/1/0/all/0/1\">Wen-mei Hwu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Legal Element-oriented Modeling with Multi-view Contrastive Learning for Legal Case Retrieval. (arXiv:2210.05188v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05188","description":"<p>Legal case retrieval, which aims to retrieve relevant cases given a query\ncase, plays an essential role in the legal system. While recent research\nefforts improve the performance of traditional ad-hoc retrieval models, legal\ncase retrieval is still challenging since queries are legal cases, which\ncontain hundreds of tokens. Legal cases are much longer and more complicated\nthan keywords queries. Apart from that, the definition of legal relevance is\nbeyond the general definition. In addition to general topical relevance, the\nrelevant cases also involve similar situations and legal elements, which can\nsupport the judgment of the current case. In this paper, we propose an\ninteraction-focused network for legal case retrieval with a multi-view\ncontrastive learning objective. The contrastive learning views, including\ncase-view and element-view, aim to overcome the above challenges. The case-view\ncontrastive learning minimizes the hidden space distance between relevant legal\ncase representations produced by a pre-trained language model (PLM) encoder.\nThe element-view builds positive and negative instances by changing legal\nelements of cases to help the network better compute legal relevance. To\nachieve this, we employ a legal element knowledge-aware indicator to detect\nlegal elements of cases. We conduct extensive experiments on the benchmark of\nrelevant case retrieval. Evaluation results indicate our proposed method\nobtains significant improvement over the existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation. (arXiv:2210.05193v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05193","description":"<p>Non-autoregressive models achieve significant decoding speedup in neural\nmachine translation but lack the ability to capture sequential dependency.\nDirected Acyclic Transformer (DA-Transformer) was recently proposed to model\nsequential dependency with a directed acyclic graph. Consequently, it has to\napply a sequential decision process at inference time, which harms the global\ntranslation accuracy. In this paper, we present a Viterbi decoding framework\nfor DA-Transformer, which guarantees to find the joint optimal solution for the\ntranslation and decoding path under any length constraint. Experimental results\ndemonstrate that our approach consistently improves the performance of\nDA-Transformer while maintaining a similar decoding speedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1\">Chenze Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengrui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIGAT: Modeling News Recommendation with Dual-Graph Interaction. (arXiv:2210.05196v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05196","description":"<p>News recommendation (NR) is essential for online news services. Existing NR\nmethods typically adopt a news-user representation learning framework, facing\ntwo potential limitations. First, in news encoder, single candidate news\nencoding suffers from an insufficient semantic information problem. Second,\nexisting graph-based NR methods are promising but lack effective news-user\nfeature interaction, rendering the graph-based recommendation suboptimal. To\novercome these limitations, we propose dual-interactive graph attention\nnetworks (DIGAT) consisting of news- and user-graph channels. In the news-graph\nchannel, we enrich the semantics of single candidate news by incorporating the\nsemantically relevant news information with a semantic-augmented graph (SAG).\nIn the user-graph channel, multi-level user interests are represented with a\nnews-topic graph. Most notably, we design a dual-graph interaction process to\nperform effective feature interaction between the news and user graphs, which\nfacilitates accurate news-user representation matching. Experiment results on\nthe benchmark dataset MIND show that DIGAT outperforms existing news\nrecommendation methods. Further ablation studies and analyses validate the\neffectiveness of (1) semantic-augmented news graph modeling and (2) dual-graph\ninteraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhiming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed-modality Representation Learning and Pre-training for Joint Table-and-Text Retrieval in OpenQA. (arXiv:2210.05197v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05197","description":"<p>Retrieving evidences from tabular and textual resources is essential for\nopen-domain question answering (OpenQA), which provides more comprehensive\ninformation. However, training an effective dense table-text retriever is\ndifficult due to the challenges of table-text discrepancy and data sparsity\nproblem. To address the above challenges, we introduce an optimized OpenQA\nTable-Text Retriever (OTTeR) to jointly retrieve tabular and textual evidences.\nFirstly, we propose to enhance mixed-modality representation learning via two\nmechanisms: modality-enhanced representation and mixed-modality negative\nsampling strategy. Secondly, to alleviate data sparsity problem and enhance the\ngeneral retrieval ability, we conduct retrieval-centric mixed-modality\nsynthetic pre-training. Experimental results demonstrate that OTTeR\nsubstantially improves the performance of table-and-text retrieval on the\nOTT-QA dataset. Comprehensive analyses examine the effectiveness of all the\nproposed mechanisms. Besides, equipped with OTTeR, our OpenQA system achieves\nthe state-of-the-art result on the downstream QA task, with 10.1\\% absolute\nimprovement in terms of the exact match over the previous best system.\n\\footnote{All the code and data are available at\n\\url{https://github.com/Jun-jie-Huang/OTTeR}.}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTC Alignments Improve Autoregressive Translation. (arXiv:2210.05200v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05200","description":"<p>Connectionist Temporal Classification (CTC) is a widely used approach for\nautomatic speech recognition (ASR) that performs conditionally independent\nmonotonic alignment. However for translation, CTC exhibits clear limitations\ndue to the contextual and non-monotonic nature of the task and thus lags behind\nattentional decoder approaches in terms of translation quality. In this work,\nwe argue that CTC does in fact make sense for translation if applied in a joint\nCTC/attention framework wherein CTC's core properties can counteract several\nkey weaknesses of pure-attention models during training and decoding. To\nvalidate this conjecture, we modify the Hybrid CTC/Attention model originally\nproposed for ASR to support text-to-text translation (MT) and speech-to-text\ntranslation (ST). Our proposed joint CTC/attention models outperform\npure-attention baselines across six benchmark translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higuchi_Y/0/1/0/all/0/1\">Yosuke Higuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Well Do Multi-hop Reading Comprehension Models Understand Date Information?. (arXiv:2210.05208v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05208","description":"<p>Several multi-hop reading comprehension datasets have been proposed to\nresolve the issue of reasoning shortcuts by which questions can be answered\nwithout performing multi-hop reasoning. However, the ability of multi-hop\nmodels to perform step-by-step reasoning when finding an answer to a comparison\nquestion remains unclear. It is also unclear how questions about the internal\nreasoning process are useful for training and evaluating question-answering\n(QA) systems. To evaluate the model precisely in a hierarchical manner, we\nfirst propose a dataset, \\textit{HieraDate}, with three probing tasks in\naddition to the main question: extraction, reasoning, and robustness. Our\ndataset is created by enhancing two previous multi-hop datasets, HotpotQA and\n2WikiMultiHopQA, focusing on multi-hop questions on date information that\ninvolve both comparison and numerical reasoning. We then evaluate the ability\nof existing models to understand date information. Our experimental results\nreveal that the multi-hop models do not have the ability to subtract two dates\neven when they perform well in date comparison and number subtraction tasks.\nOther results reveal that our probing questions can help to improve the\nperformance of the models (e.g., by +10.3 F1) on the main QA task and our\ndataset can be used for data augmentation to improve the robustness of the\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_X/0/1/0/all/0/1\">Xanh Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1\">Saku Sugawara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models. (arXiv:2210.05211v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05211","description":"<p>Despite the remarkable success of pre-trained language models (PLMs), they\nstill face two challenges: First, large-scale PLMs are inefficient in terms of\nmemory footprint and computation. Second, on the downstream tasks, PLMs tend to\nrely on the dataset bias and struggle to generalize to out-of-distribution\n(OOD) data. In response to the efficiency problem, recent studies show that\ndense PLMs can be replaced with sparse subnetworks without hurting the\nperformance. Such subnetworks can be found in three scenarios: 1) the\nfine-tuned PLMs, 2) the raw PLMs and then fine-tuned in isolation, and even\ninside 3) PLMs without any parameter fine-tuning. However, these results are\nonly obtained in the in-distribution (ID) setting. In this paper, we extend the\nstudy on PLMs subnetworks to the OOD setting, investigating whether sparsity\nand robustness to dataset bias can be achieved simultaneously. To this end, we\nconduct extensive experiments with the pre-trained BERT model on three natural\nlanguage understanding (NLU) tasks. Our results demonstrate that \\textbf{sparse\nand robust subnetworks (SRNets) can consistently be found in BERT}, across the\naforementioned three scenarios, using different training and compression\nmethods. Furthermore, we explore the upper bound of SRNets using the OOD\ninformation and show that \\textbf{there exist sparse and almost unbiased BERT\nsubnetworks}. Finally, we present 1) an analytical study that provides insights\non how to promote the efficiency of SRNets searching process and 2) a solution\nto improve subnetworks' performance at high sparsity. The code is available at\nhttps://github.com/llyx97/sparse-and-robust-PLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1\">Peng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHAE: Fine-Grained Controllable Story Generation with Characters, Actions and Emotions. (arXiv:2210.05221v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05221","description":"<p>Story generation has emerged as an interesting yet challenging NLP task in\nrecent years. Some existing studies aim at generating fluent and coherent\nstories from keywords and outlines; while others attempt to control the global\nfeatures of the story, such as emotion, style and topic. However, these works\nfocus on coarse-grained control on the story, neglecting control on the details\nof the story, which is also crucial for the task. To fill the gap, this paper\nproposes a model for fine-grained control on the story, which allows the\ngeneration of customized stories with characters, corresponding actions and\nemotions arbitrarily assigned. Extensive experimental results on both automatic\nand human manual evaluations show the superiority of our method. It has strong\ncontrollability to generate stories according to the fine-grained personalized\nguidance, unveiling the effectiveness of our methodology. Our code is available\nat https://github.com/victorup/CHAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Han Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhihua Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shanlin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Mimicking to Integrating: Knowledge Integration for Pre-Trained Language Models. (arXiv:2210.05230v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05230","description":"<p>Investigating better ways to reuse the released pre-trained language models\n(PLMs) can significantly reduce the computational cost and the potential\nenvironmental side-effects. This paper explores a novel PLM reuse paradigm,\nKnowledge Integration (KI). Without human annotations available, KI aims to\nmerge the knowledge from different teacher-PLMs, each of which specializes in a\ndifferent classification problem, into a versatile student model. To achieve\nthis, we first derive the correlation between virtual golden supervision and\nteacher predictions. We then design a Model Uncertainty--aware Knowledge\nIntegration (MUKI) framework to recover the golden supervision for the student.\nSpecifically, MUKI adopts Monte-Carlo Dropout to estimate model uncertainty for\nthe supervision integration. An instance-wise re-weighting mechanism based on\nthe margin of uncertainty scores is further incorporated, to deal with the\npotential conflicting supervision from teachers. Experimental results\ndemonstrate that MUKI achieves substantial improvements over baselines on\nbenchmark datasets. Further analysis shows that MUKI can generalize well for\nmerging teacher models with heterogeneous architectures, and even teachers\nmajor in cross-lingual datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PatternRank: Leveraging Pretrained Language Models and Part of Speech for Unsupervised Keyphrase Extraction. (arXiv:2210.05245v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05245","description":"<p>Keyphrase extraction is the process of automatically selecting a small set of\nmost relevant phrases from a given text. Supervised keyphrase extraction\napproaches need large amounts of labeled training data and perform poorly\noutside the domain of the training data (Bennani-Smires et al., 2018). In this\npaper, we present PatternRank, which leverages pretrained language models and\npart-of-speech for unsupervised keyphrase extraction from single documents. Our\nexperiments show PatternRank achieves higher precision, recall and F1 -scores\nthan previous state-of-the-art approaches. In addition, we present the\nKeyphraseVectorizers package, which allows easy modification of part-of-speech\npatterns for candidate keyphrase selection, and hence adaptation of our\napproach to any domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1\">Tim Schopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klimek_S/0/1/0/all/0/1\">Simon Klimek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Neural Network Policies and Imitation Learning for Multi-Domain Task-Oriented Dialogues. (arXiv:2210.05252v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05252","description":"<p>Task-oriented dialogue systems are designed to achieve specific goals while\nconversing with humans. In practice, they may have to handle simultaneously\nseveral domains and tasks. The dialogue manager must therefore be able to take\ninto account domain changes and plan over different domains/tasks in order to\ndeal with multidomain dialogues. However, learning with reinforcement in such\ncontext becomes difficult because the state-action dimension is larger while\nthe reward signal remains scarce. Our experimental results suggest that\nstructured policies based on graph neural networks combined with different\ndegrees of imitation learning can effectively handle multi-domain dialogues.\nThe reported experiments underline the benefit of structured policies over\nstandard policies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cordier_T/0/1/0/all/0/1\">Thibault Cordier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urvoy_T/0/1/0/all/0/1\">Tanguy Urvoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lefevre_F/0/1/0/all/0/1\">Fabrice Lef&#xe8;vre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1\">Lina M. Rojas-Barahona</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Event Coding Pipeline with Prompt Entailment. (arXiv:2210.05257v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05257","description":"<p>For monitoring crises, political events are extracted from the news. The\nlarge amount of unstructured full-text event descriptions makes a case-by-case\nanalysis unmanageable, particularly for low-resource humanitarian aid\norganizations. This creates a demand to classify events into event types, a\ntask referred to as event coding. Typically, domain experts craft an event type\nontology, annotators label a large dataset and technical experts develop a\nsupervised coding system. In this work, we propose PR-ENT, a new event coding\napproach that is more flexible and resource-efficient, while maintaining\ncompetitive accuracy: first, we extend an event description such as \"Military\ninjured two civilians'' by a template, e.g. \"People were [Z]\" and prompt a\npre-trained (cloze) language model to fill the slot Z. Second, we select answer\ncandidates Z* = {\"injured'', \"hurt\"...} by treating the event description as\npremise and the filled templates as hypothesis in a textual entailment task.\nThis allows domain experts to draft the codebook directly as labeled prompts\nand interpretable answer candidates. This human-in-the-loop process is guided\nby our interactive codebook design tool. We evaluate PR-ENT in several\nrobustness checks: perturbing the event description and prompt template,\nrestricting the vocabulary and removing contextual information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lefebvre_C/0/1/0/all/0/1\">Cl&#xe9;ment Lefebvre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoehr_N/0/1/0/all/0/1\">Niklas Stoehr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair Modeling. (arXiv:2210.05261v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05261","description":"<p>Transformer-based models have achieved great success on sentence pair\nmodeling tasks, such as answer selection and natural language inference (NLI).\nThese models generally perform cross-attention over input pairs, leading to\nprohibitive computational costs. Recent studies propose dual-encoder and late\ninteraction architectures for faster computation. However, the balance between\nthe expressive of cross-attention and computation speedup still needs better\ncoordinated. To this end, this paper introduces a novel paradigm MixEncoder for\nefficient sentence pair modeling. MixEncoder involves a light-weight\ncross-attention mechanism. It conducts query encoding only once while modeling\nthe query-candidate interaction in parallel. Extensive experiments conducted on\nfour tasks demonstrate that our MixEncoder can speed up sentence pairing by\nover 113x while achieving comparable performance as the more expensive\ncross-attention models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuanhang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+qi_s/0/1/0/all/0/1\">shiyi qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Cuiyun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuanyi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting and Advancing Chinese Natural Language Understanding with Accelerated Heterogeneous Knowledge Pre-training. (arXiv:2210.05287v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05287","description":"<p>Recently, knowledge-enhanced pre-trained language models (KEPLMs) improve\ncontext-aware representations via learning from structured relations in\nknowledge graphs, and/or linguistic knowledge from syntactic or dependency\nanalysis. Unlike English, there is a lack of high-performing open-source\nChinese KEPLMs in the natural language processing (NLP) community to support\nvarious language understanding applications. In this paper, we revisit and\nadvance the development of Chinese natural language understanding with a series\nof novel Chinese KEPLMs released in various parameter sizes, namely CKBERT\n(Chinese knowledge-enhanced BERT).Specifically, both relational and linguistic\nknowledge is effectively injected into CKBERT based on two novel pre-training\ntasks, i.e., linguistic-aware masked language modeling and contrastive\nmulti-hop relation modeling. Based on the above two pre-training paradigms and\nour in-house implemented TorchAccelerator, we have pre-trained base (110M),\nlarge (345M) and huge (1.3B) versions of CKBERT efficiently on GPU clusters.\nExperiments demonstrate that CKBERT outperforms strong baselines for Chinese\nover various benchmark NLP tasks and in terms of different model sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Taolin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DOng_J/0/1/0/all/0/1\">Junwei DOng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofeng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Use of Semantically-Aligned Speech Representations for Spoken Language Understanding. (arXiv:2210.05291v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05291","description":"<p>In this paper we examine the use of semantically-aligned speech\nrepresentations for end-to-end spoken language understanding (SLU). We employ\nthe recently-introduced SAMU-XLSR model, which is designed to generate a single\nembedding that captures the semantics at the utterance level, semantically\naligned across different languages. This model combines the acoustic\nframe-level speech representation learning model (XLS-R) with the Language\nAgnostic BERT Sentence Embedding (LaBSE) model. We show that the use of the\nSAMU-XLSR model instead of the initial XLS-R model improves significantly the\nperformance in the framework of end-to-end SLU. Finally, we present the\nbenefits of using this model towards language portability in SLU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laperriere_G/0/1/0/all/0/1\">Ga&#xeb;lle Laperri&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelloin_V/0/1/0/all/0/1\">Valentin Pelloin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouvier_M/0/1/0/all/0/1\">Micka&#xeb;l Rouvier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stafylakis_T/0/1/0/all/0/1\">Themos Stafylakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing for Cognitive Analysis of Emotions. (arXiv:2210.05296v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05296","description":"<p>Emotion analysis in texts suffers from two major limitations: annotated\ngold-standard corpora are mostly small and homogeneous, and emotion\nidentification is often simplified as a sentence-level classification problem.\nTo address these issues, we introduce a new annotation scheme for exploring\nemotions and their causes, along with a new French dataset composed of\nautobiographical accounts of an emotional scene. The texts were collected by\napplying the Cognitive Analysis of Emotions developed by A. Finkel to help\npeople improve on their emotion management. The method requires the manual\nanalysis of an emotional event by a coach trained in Cognitive Analysis. We\npresent a rule-based approach to automatically annotate emotions and their\nsemantic roles (e.g. emotion causes) to facilitate the identification of\nrelevant aspects by the coach. We investigate future directions for emotion\nanalysis using graph structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cortal_G/0/1/0/all/0/1\">Gustave Cortal</a> (LMF, ENS Paris Saclay), <a href=\"http://arxiv.org/find/cs/1/au:+Finkel_A/0/1/0/all/0/1\">Alain Finkel</a> (LMF, ENS Paris Saclay, IUF), <a href=\"http://arxiv.org/find/cs/1/au:+Paroubek_P/0/1/0/all/0/1\">Patrick Paroubek</a> (LISN), <a href=\"http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1\">Lina Ye</a> (LMF)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Structure-aware Paraphrase Identification with Phrase Alignment Using Sentence Encoders. (arXiv:2210.05302v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05302","description":"<p>Previous works have demonstrated the effectiveness of utilising pre-trained\nsentence encoders based on their sentence representations for meaning\ncomparison tasks. Though such representations are shown to capture hidden\nsyntax structures, the direct similarity comparison between them exhibits weak\nsensitivity to word order and structural differences in given sentences. A\nsingle similarity score further makes the comparison process hard to interpret.\nTherefore, we here propose to combine sentence encoders with an alignment\ncomponent by representing each sentence as a list of predicate-argument spans\n(where their span representations are derived from sentence encoders), and\ndecomposing the sentence-level meaning comparison into the alignment between\ntheir spans for paraphrase identification tasks. Empirical results show that\nthe alignment component brings in both improved performance and\ninterpretability for various sentence encoders. After closer investigation, the\nproposed approach indicates increased sensitivity to structural difference and\nenhanced ability to distinguish non-paraphrases with high lexical overlap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qiwei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weir_D/0/1/0/all/0/1\">David Weir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weeds_J/0/1/0/all/0/1\">Julie Weeds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAP: Modality-Agnostic Uncertainty-Aware Vision-Language Pre-training Model. (arXiv:2210.05335v1 [cs.CV])","link":"http://arxiv.org/abs/2210.05335","description":"<p>Multimodal semantic understanding often has to deal with uncertainty, which\nmeans the obtained message tends to refer to multiple targets. Such uncertainty\nis problematic for our interpretation, including intra-modal and inter-modal\nuncertainty. Little effort studies the modeling of this uncertainty,\nparticularly in pre-training on unlabeled datasets and fine-tuning in\ntask-specific downstream tasks. To address this, we project the representations\nof all modalities as probabilistic distributions via a Probability Distribution\nEncoder (PDE) by utilizing rich multimodal semantic information. Furthermore,\nwe integrate uncertainty modeling with popular pre-training frameworks and\npropose suitable pre-training tasks: Distribution-based Vision-Language\nContrastive learning (D-VLC), Distribution-based Masked Language Modeling\n(D-MLM), and Distribution-based Image-Text Matching (D-ITM). The fine-tuned\nmodels are applied to challenging downstream tasks, including image-text\nretrieval, visual question answering, visual reasoning, and visual entailment,\nand achieve state-of-the-art results. Code is released at\nhttps://github.com/IIGROUP/MAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yatai Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yuan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanru Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakai_T/0/1/0/all/0/1\">Tetsuya Sakai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind's Eye: Grounded Language Model Reasoning through Simulation. (arXiv:2210.05359v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05359","description":"<p>Successful and effective communication between humans and AI relies on a\nshared experience of the world. By training solely on written text, current\nlanguage models (LMs) miss the grounded experience of humans in the real-world\n-- their failure to relate language to the physical world causes knowledge to\nbe misrepresented and obvious mistakes in their reasoning. We present Mind's\nEye, a paradigm to ground language model reasoning in the physical world. Given\na physical reasoning question, we use a computational physics engine\n(DeepMind's MuJoCo) to simulate the possible outcomes, and then use the\nsimulation results as part of the input, which enables language models to\nperform reasoning. Experiments on 39 tasks in a physics alignment benchmark\ndemonstrate that Mind's Eye can improve reasoning ability by a large margin\n(27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average).\nSmaller language models armed with Mind's Eye can obtain similar performance to\nmodels that are 100x larger. Finally, we confirm the robustness of Mind's Eye\nthrough ablation studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Te-Yen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Claire Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepPerform: An Efficient Approach for Performance Testing of Resource-Constrained Neural Networks. (arXiv:2210.05370v1 [cs.LG])","link":"http://arxiv.org/abs/2210.05370","description":"<p>Today, an increasing number of Adaptive Deep Neural Networks (AdNNs) are\nbeing used on resource-constrained embedded devices. We observe that, similar\nto traditional software, redundant computation exists in AdNNs, resulting in\nconsiderable performance degradation. The performance degradation is dependent\non the input and is referred to as input-dependent performance bottlenecks\n(IDPBs). To ensure an AdNN satisfies the performance requirements of\nresource-constrained applications, it is essential to conduct performance\ntesting to detect IDPBs in the AdNN. Existing neural network testing methods\nare primarily concerned with correctness testing, which does not involve\nperformance testing. To fill this gap, we propose DeepPerform, a scalable\napproach to generate test samples to detect the IDPBs in AdNNs. We first\ndemonstrate how the problem of generating performance test samples detecting\nIDPBs can be formulated as an optimization problem. Following that, we\ndemonstrate how DeepPerform efficiently handles the optimization problem by\nlearning and estimating the distribution of AdNNs' computational consumption.\nWe evaluate DeepPerform on three widely used datasets against five popular AdNN\nmodels. The results show that DeepPerform generates test samples that cause\nmore severe performance degradation (FLOPs: increase up to 552\\%). Furthermore,\nDeepPerform is substantially more efficient than the baseline methods in\ngenerating test inputs(runtime overhead: only 6-10 milliseconds).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Simin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_M/0/1/0/all/0/1\">Mirazul Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEPTWEET: A Typology for Social Media Texts to Detect Depression Severities. (arXiv:2210.05372v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05372","description":"<p>Mental health research through data-driven methods has been hindered by a\nlack of standard typology and scarcity of adequate data. In this study, we\nleverage the clinical articulation of depression to build a typology for social\nmedia texts for detecting the severity of depression. It emulates the standard\nclinical assessment procedure Diagnostic and Statistical Manual of Mental\nDisorders (DSM-5) and Patient Health Questionnaire (PHQ-9) to encompass subtle\nindications of depressive disorders from tweets. Along with the typology, we\npresent a new dataset of 40191 tweets labeled by expert annotators. Each tweet\nis labeled as 'non-depressed' or 'depressed'. Moreover, three severity levels\nare considered for 'depressed' tweets: (1) mild, (2) moderate, and (3) severe.\nAn associated confidence score is provided with each label to validate the\nquality of annotation. We examine the quality of the dataset via representing\nsummary statistics while setting strong baseline results using attention-based\nmodels like BERT and DistilBERT. Finally, we extensively address the\nlimitations of the study to provide directions for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Mohsinul Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_T/0/1/0/all/0/1\">Tasnim Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Bakhtiar Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joarder_T/0/1/0/all/0/1\">Tarun Kumar Joarder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_H/0/1/0/all/0/1\">Hasan Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_K/0/1/0/all/0/1\">Kamrul Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not Good Times for Lies: Misinformation Detection on the Russia-Ukraine War, COVID-19, and Refugees. (arXiv:2210.05401v1 [cs.SI])","link":"http://arxiv.org/abs/2210.05401","description":"<p>Misinformation spread in online social networks is an urgent-to-solve problem\nhaving harmful consequences that threaten human health, public safety,\neconomics, and so on. In this study, we construct a novel dataset, called\nMiDe-22, having 5,284 English and 5,064 Turkish tweets with their\nmisinformation labels under several recent events, including the Russia-Ukraine\nwar, COVID-19 pandemic, and Refugees. Moreover, we provide the user engagements\nto the tweets in terms of likes, replies, retweets, and quotes. We present a\ndetailed data analysis with descriptive statistics and temporal analysis, and\nprovide the experimental results of a benchmark evaluation for misinformation\ndetection on our novel dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toraman_C/0/1/0/all/0/1\">Cagri Toraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozcelik_O/0/1/0/all/0/1\">Oguzhan Ozcelik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahinuc_F/0/1/0/all/0/1\">Furkan &#x15e;ahinu&#xe7;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Can_F/0/1/0/all/0/1\">Fazli Can</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation between Spoken Languages and Signed Languages Represented in SignWriting. (arXiv:2210.05404v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05404","description":"<p>This paper presents work on novel machine translation (MT) systems between\nspoken and signed languages, where signed languages are represented in\nSignWriting, a sign language writing system. Our work seeks to address the lack\nof out-of-the-box support for signed languages in current MT systems and is\nbased on the SignBank dataset, which contains pairs of spoken language text and\nSignWriting content. We introduce novel methods to parse, factorize, decode,\nand evaluate SignWriting, leveraging ideas from neural factored MT. In a\nbilingual setup--translating from American Sign Language to (American)\nEnglish--our method achieves over 30 BLEU, while in two multilingual\nsetups--translating in both directions between spoken languages and signed\nlanguages--we achieve over 20 BLEU. We find that common MT techniques used to\nimprove spoken language translation similarly affect the performance of sign\nlanguage translation. These findings validate our use of an intermediate text\nrepresentation for signed languages to include them in natural language\nprocessing research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moryossef_A/0/1/0/all/0/1\">Amit Moryossef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Mathias M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebling_S/0/1/0/all/0/1\">Sarah Ebling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Sense Induction with Hierarchical Clustering and Mutual Information Maximization. (arXiv:2210.05422v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05422","description":"<p>Word sense induction (WSI) is a difficult problem in natural language\nprocessing that involves the unsupervised automatic detection of a word's\nsenses (i.e. meanings). Recent work achieves significant results on the WSI\ntask by pre-training a language model that can exclusively disambiguate word\nsenses, whereas others employ previously pre-trained language models in\nconjunction with additional strategies to induce senses. In this paper, we\npropose a novel unsupervised method based on hierarchical clustering and\ninvariant information clustering (IIC). The IIC is used to train a small model\nto optimize the mutual information between two vector representations of a\ntarget word occurring in a pair of synthetic paraphrases. This model is later\nused in inference mode to extract a higher quality vector representation to be\nused in the hierarchical clustering. We evaluate our method on two WSI tasks\nand in two distinct clustering configurations (fixed and dynamic number of\nclusters). We empirically demonstrate that, in certain cases, our approach\noutperforms prior WSI state-of-the-art methods, while in others, it achieves a\ncompetitive performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdine_H/0/1/0/all/0/1\">Hadi Abdine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eddine_M/0/1/0/all/0/1\">Moussa Kamal Eddine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buscaldi_D/0/1/0/all/0/1\">Davide Buscaldi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Locate Visual Answer in Video Corpus Using Question. (arXiv:2210.05423v1 [cs.CV])","link":"http://arxiv.org/abs/2210.05423","description":"<p>We introduce a novel task, named video corpus visual answer localization\n(VCVAL), which aims to locate the visual answer in a large collection of\nuntrimmed, unsegmented instructional videos using a natural language question.\nThis task requires a range of skills - the interaction between vision and\nlanguage, video retrieval, passage comprehension, and visual answer\nlocalization. To solve these, we propose a cross-modal contrastive global-span\n(CCGS) method for the VCVAL, jointly training the video corpus retrieval and\nvisual answer localization tasks. More precisely, we enhance the video\nquestion-answer semantic by adding element-wise visual information into the\npre-trained language model, and designing a novel global-span predictor through\nfusion information to locate the visual answer point. The Global-span\ncontrastive learning is adopted to differentiate the span point in the positive\nand negative samples with the global-span matrix. We have reconstructed a new\ndataset named MedVidCQA and benchmarked the VCVAL task, where the proposed\nmethod achieves state-of-the-art (SOTA) both in the video corpus retrieval and\nvisual answer localization tasks. Most importantly, we pave a new path for\nunderstanding the instructional videos, performing detailed analyses on\nextensive experiments, which ushers in further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19-related Nepali Tweets Classification in a Low Resource Setting. (arXiv:2210.05425v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05425","description":"<p>Billions of people across the globe have been using social media platforms in\ntheir local languages to voice their opinions about the various topics related\nto the COVID-19 pandemic. Several organizations, including the World Health\nOrganization, have developed automated social media analysis tools that\nclassify COVID-19-related tweets into various topics. However, these tools that\nhelp combat the pandemic are limited to very few languages, making several\ncountries unable to take their benefit. While multi-lingual or low-resource\nlanguage-specific tools are being developed, they still need to expand their\ncoverage, such as for the Nepali language. In this paper, we identify the eight\nmost common COVID-19 discussion topics among the Twitter community using the\nNepali language, set up an online platform to automatically gather Nepali\ntweets containing the COVID-19-related keywords, classify the tweets into the\neight topics, and visualize the results across the period in a web-based\ndashboard. We compare the performance of two state-of-the-art multi-lingual\nlanguage models for Nepali tweet classification, one generic (mBERT) and the\nother Nepali language family-specific model (MuRIL). Our results show that the\nmodels' relative performance depends on the data size, with MuRIL doing better\nfor a larger dataset. The annotated data, models, and the web-based dashboard\nare open-sourced at https://github.com/naamiinepal/covid-tweet-classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adhikari_R/0/1/0/all/0/1\">Rabin Adhikari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thapaliya_S/0/1/0/all/0/1\">Safal Thapaliya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basnet_N/0/1/0/all/0/1\">Nirajan Basnet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poudel_S/0/1/0/all/0/1\">Samip Poudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakya_A/0/1/0/all/0/1\">Aman Shakya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanal_B/0/1/0/all/0/1\">Bishesh Khanal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Pretrained Multilingual Models Equally Fair Across Languages?. (arXiv:2210.05457v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05457","description":"<p>Pretrained multilingual language models can help bridge the digital language\ndivide, enabling high-quality NLP models for lower resourced languages. Studies\nof multilingual models have so far focused on performance, consistency, and\ncross-lingual generalisation. However, with their wide-spread application in\nthe wild and downstream societal impact, it is important to put multilingual\nmodels under the same scrutiny as monolingual models. This work investigates\nthe group fairness of multilingual models, asking whether these models are\nequally fair across languages. To this end, we create a new four-way\nmultilingual dataset of parallel cloze test examples (MozArt), equipped with\ndemographic information (balanced with regard to gender and native tongue)\nabout the test participants. We evaluate three multilingual models on MozArt --\nmBERT, XLM-R, and mT5 -- and show that across the four target languages, the\nthree models exhibit different levels of group disparity, e.g., exhibiting\nnear-equal risk for Spanish, but high levels of disparity for German.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piqueras_L/0/1/0/all/0/1\">Laura Cabello Piqueras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance Regularization for Discriminative Language Model Pre-training. (arXiv:2210.05471v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05471","description":"<p>Discriminative pre-trained language models (PrLMs) can be generalized as\ndenoising auto-encoders that work with two procedures, ennoising and denoising.\nFirst, an ennoising process corrupts texts with arbitrary noising functions to\nconstruct training instances. Then, a denoising language model is trained to\nrestore the corrupted tokens. Existing studies have made progress by optimizing\nindependent strategies of either ennoising or denosing. They treat training\ninstances equally throughout the training process, with little attention on the\nindividual contribution of those instances. To model explicit signals of\ninstance contribution, this work proposes to estimate the complexity of\nrestoring the original sentences from corrupted ones in language model\npre-training. The estimations involve the corruption degree in the ennoising\ndata construction process and the prediction confidence in the denoising\ncounterpart. Experimental results on natural language understanding and reading\ncomprehension benchmarks show that our approach improves pre-training\nefficiency, effectiveness, and robustness. Code is publicly available at\nhttps://github.com/cooelf/InstanceReg\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"T5 for Hate Speech, Augmented Data and Ensemble. (arXiv:2210.05480v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05480","description":"<p>We conduct relatively extensive investigations of automatic hate speech (HS)\ndetection using different state-of-the-art (SoTA) baselines over 11 subtasks of\n6 different datasets. Our motivation is to determine which of the recent SoTA\nmodels is best for automatic hate speech detection and what advantage methods\nlike data augmentation and ensemble may have on the best model, if any. We\ncarry out 6 cross-task investigations. We achieve new SoTA on two subtasks -\nmacro F1 scores of 91.73% and 53.21% for subtasks A and B of the HASOC 2020\ndataset, where previous SoTA are 51.52% and 26.52%, respectively. We achieve\nnear-SoTA on two others - macro F1 scores of 81.66% for subtask A of the OLID\n2019 dataset and 82.54% for subtask A of the HASOC 2021 dataset, where SoTA are\n82.9% and 83.05%, respectively. We perform error analysis and use two\nexplainable artificial intelligence (XAI) algorithms (IG and SHAP) to reveal\nhow two of the models (Bi-LSTM and T5) make the predictions they do by using\nexamples. Other contributions of this work are 1) the introduction of a simple,\nnovel mechanism for correcting out-of-class (OOC) predictions in T5, 2) a\ndetailed description of the data augmentation methods, 3) the revelation of the\npoor data annotations in the HASOC 2021 dataset by using several examples and\nXAI (buttressing the need for better quality control), and 4) the public\nrelease of our model checkpoints and codes to foster transparency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabry_S/0/1/0/all/0/1\">Sana Sabah Sabry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abid_N/0/1/0/all/0/1\">Nosheen Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_F/0/1/0/all/0/1\">Foteini Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Than Whitespace: Information Retrieval for Languages without Custom Tokenizers. (arXiv:2210.05481v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05481","description":"<p>Tokenization is a crucial step in information retrieval, especially for\nlexical matching algorithms, where the quality of indexable tokens directly\nimpacts the effectiveness of a retrieval system. Since different languages have\nunique properties, the design of the tokenization algorithm is usually\nlanguage-specific and requires at least some lingustic knowledge. However, only\na handful of the 7000+ languages on the planet benefit from specialized,\ncustom-built tokenization algorithms, while the other languages are stuck with\na \"default\" whitespace tokenizer, which cannot capture the intricacies of\ndifferent languages. To address this challenge, we propose a different approach\nto tokenization for lexical matching retrieval algorithms (e.g., BM25): using\nthe WordPiece tokenizer, which can be built automatically from unsupervised\ndata. We test the approach on 11 typologically diverse languages in the MrTyDi\ncollection: results show that the mBERT tokenizer provides strong relevance\nsignals for retrieval \"out of the box\", outperforming whitespace tokenization\non most languages. In many cases, our approach also improves retrieval\neffectiveness when combined with existing custom-built tokenizers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ogundepo_O/0/1/0/all/0/1\">Odunayo Ogundepo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Like a bilingual baby: The advantage of visually grounding a bilingual language model. (arXiv:2210.05487v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05487","description":"<p>Unlike most neural language models, humans learn language in a rich,\nmulti-sensory and, often, multi-lingual environment. Current language models\ntypically fail to fully capture the complexities of multilingual language use.\nWe train an LSTM language model on images and captions in English and Spanish\nfrom MS-COCO-ES. We find that the visual grounding improves the model's\nunderstanding of semantic similarity both within and across languages and\nimproves perplexity. However, we find no significant advantage of visual\ngrounding for abstract words. Our results provide additional evidence of the\nadvantages of visually grounded language models and point to the need for more\nnaturalistic language data from multilingual speakers and multilingual datasets\nwith perceptual grounding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khai-Nguyen Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zixin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mali_A/0/1/0/all/0/1\">Ankur Mali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_A/0/1/0/all/0/1\">Alex Kelly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models. (arXiv:2210.05497v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05497","description":"<p>Fine-tuning large pretrained language models on a limited training corpus\nusually suffers from poor generalization. Prior works show that the\nrecently-proposed sharpness-aware minimization (SAM) optimization method can\nimprove the model generalization. However, SAM adds a perturbation to each\nmodel parameter equally (but not all parameters contribute equally to the\noptimization of training), which we argue is sub-optimal and will lead to\nexcessive computation. In this paper, we propose a novel optimization\nprocedure, namely FSAM, which introduces a Fisher mask to improve the\nefficiency and performance of SAM. In short, instead of adding perturbation to\nall parameters, FSAM uses the Fisher information to identity the important\nparameters and formulates a Fisher mask to obtain the sparse perturbation,\ni.e., making the optimizer focus on these important parameters. Experiments on\nvarious tasks in GLUE and SuperGLUE benchmarks show that FSAM consistently\noutperforms the vanilla SAM by 0.67~1.98 average score among four different\npretrained models. We also empirically show that FSAM works well in other\ncomplex scenarios, e.g., fine-tuning on generation tasks or limited training\ndata. Encouragingly, when training data is limited, FSAM improves the SAM by a\nlarge margin, i.e., up to 15.1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qihuang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_P/0/1/0/all/0/1\">Peng Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Contrastive Learning for Evidence-aware Fake News Detection with Graph Neural Networks. (arXiv:2210.05498v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05498","description":"<p>The prevalence and perniciousness of fake news have been a critical issue on\nthe Internet, which stimulates the development of automatic fake news detection\nin turn. In this paper, we focus on evidence-based fake news detection, where\nseveral evidences are utilized to probe the veracity of news (i.e., a claim).\nMost previous methods first employ sequential models to embed the semantic\ninformation and then capture the claim-evidence interaction based on attention\nmechanisms. Despite their effectiveness, they still suffer from three\nweaknesses. Firstly, sequential models fail to integrate the relevant\ninformation that is scattered far apart in evidences. Secondly, they\nunderestimate much redundant information in evidences may be useless or\nharmful. Thirdly, insufficient data utilization limits the separability and\nreliability of representations captured by the model. To solve these problems,\nwe propose a unified Graph-based sEmantic structure mining framework with\nConTRAstive Learning, namely GETRAL in short. Specifically, we first model\nclaims and evidences as graph-structured data to capture the long-distance\nsemantic dependency. Consequently, we reduce information redundancy by\nperforming graph structure learning. Then the fine-grained semantic\nrepresentations are fed into the claim-evidence interaction module for\npredictions. Finally, an adversarial contrastive learning module is applied to\nmake full use of data and strengthen representation learning. Comprehensive\nexperiments have demonstrated the superiority of GETRAL over the\nstate-of-the-arts and validated the efficacy of semantic mining with graph\nstructure and contrastive learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weizhi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Global Structural Information in Long Document Question Answering with Compressive Graph Selector Network. (arXiv:2210.05499v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05499","description":"<p>Long document question answering is a challenging task due to its demands for\ncomplex reasoning over long text. Previous works usually take long documents as\nnon-structured flat texts or only consider the local structure in long\ndocuments. However, these methods usually ignore the global structure of the\nlong document, which is essential for long-range understanding. To tackle this\nproblem, we propose Compressive Graph Selector Network (CGSN) to capture the\nglobal structure in a compressive and iterative manner. Specifically, the\nproposed model consists of three modules: local graph network, global graph\nnetwork and evidence memory network. Firstly, the local graph network builds\nthe graph structure of the chunked segment in token, sentence, paragraph and\nsegment levels to capture the short-term dependency of the text. Secondly, the\nglobal graph network selectively receives the information of each level from\nthe local graph, compresses them into the global graph nodes and applies graph\nattention into the global graph nodes to build the long-range reasoning over\nthe entire text in an iterative way. Thirdly, the evidence memory network is\ndesigned to alleviate the redundancy problem in the evidence selection via\nsaving the selected result in the previous steps. Extensive experiments show\nthat the proposed model outperforms previous methods on two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yuxiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems. (arXiv:2210.05528v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05528","description":"<p>Do all instances need inference through the big models for a correct\nprediction? Perhaps not; some instances are easy and can be answered correctly\nby even small capacity models. This provides opportunities for improving the\ncomputational efficiency of systems. In this work, we present an explorative\nstudy on 'model cascading', a simple technique that utilizes a collection of\nmodels of varying capacities to accurately yet efficiently output predictions.\nThrough comprehensive experiments in multiple task settings that differ in the\nnumber of models available for cascading (K value), we show that cascading\nimproves both the computational efficiency and the prediction accuracy. For\ninstance, in K=3 setting, cascading saves up to 88.93% computation cost and\nconsistently achieves superior prediction accuracy with an improvement of up to\n2.18%. We also study the impact of introducing additional models in the cascade\nand show that it further increases the efficiency improvements. Finally, we\nhope that our work will facilitate development of efficient NLP systems making\ntheir widespread adoption in real-world applications possible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification. (arXiv:2210.05529v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05529","description":"<p>Non-hierarchical sparse attention Transformer-based models, such as\nLongformer and Big Bird, are popular approaches to working with long documents.\nThere are clear benefits to these approaches compared to the original\nTransformer in terms of efficiency, but Hierarchical Attention Transformer\n(HAT) models are a vastly understudied alternative. We develop and release\nfully pre-trained HAT models that use segment-wise followed by cross-segment\nencoders and compare them with Longformer models and partially pre-trained\nHATs. In several long document downstream classification tasks, our best HAT\nmodel outperforms equally-sized Longformer models while using 10-20% less GPU\nmemory and processing documents 40-45% faster. In a series of ablation studies,\nwe find that HATs perform best with cross-segment contextualization throughout\nthe model than alternative configurations that implement either early or late\ncross-segment contextualization. Our code is on GitHub:\nhttps://github.com/coastalcph/hierarchical-transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergadiotis_M/0/1/0/all/0/1\">Manos Fergadiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malakasiotis_P/0/1/0/all/0/1\">Prodromos Malakasiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Training of Language Models for Few-Shot Learning. (arXiv:2210.05549v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05549","description":"<p>Recent work on applying large language models (LMs) achieves impressive\nperformance in many NLP applications. Adapting or posttraining an LM using an\nunlabeled domain corpus can produce even better performance for end-tasks in\nthe domain. This paper proposes the problem of continually extending an LM by\nincrementally post-train the LM with a sequence of unlabeled domain corpora to\nexpand its knowledge without forgetting its previous skills. The goal is to\nimprove the few-shot end-task learning in these domains. The resulting system\nis called CPT (Continual PostTraining), which to our knowledge, is the first\ncontinual post-training system. Experimental results verify its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zixuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haowei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yijia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViLPAct: A Benchmark for Compositional Generalization on Multimodal Human Activities. (arXiv:2210.05556v1 [cs.CV])","link":"http://arxiv.org/abs/2210.05556","description":"<p>We introduce ViLPAct, a novel vision-language benchmark for human activity\nplanning. It is designed for a task where embodied AI agents can reason and\nforecast future actions of humans based on video clips about their initial\nactivities and intents in text. The dataset consists of 2.9k videos from\n\\charades extended with intents via crowdsourcing, a multi-choice question test\nset, and four strong baselines. One of the baselines implements a neurosymbolic\napproach based on a multi-modal knowledge base (MKB), while the other ones are\ndeep generative models adapted from recent state-of-the-art (SOTA) methods.\nAccording to our extensive experiments, the key challenges are compositional\ngeneralization and effective use of information from both modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yaqing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yuecheng Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approaching Neural Chinese Word Segmentation as a Low-Resource Machine Translation Task. (arXiv:2008.05348v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.05348","description":"<p>Chinese word segmentation has entered the deep learning era which greatly\nreduces the hassle of feature engineering. Recently, some researchers attempted\nto treat it as character-level translation, which further simplified model\ndesigning, but there is a performance gap between the translation-based\napproach and other methods. This motivates our work, in which we apply the best\npractices from low-resource neural machine translation to supervised Chinese\nsegmentation. We examine a series of techniques including regularization, data\naugmentation, objective weighting, transfer learning, and ensembling. Compared\nto previous works, our low-resource translation-based method maintains the\neffortless model design, yet achieves the same result as state of the art in\nthe constrained evaluation without using additional data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pinzhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heafield_K/0/1/0/all/0/1\">Kenneth Heafield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding-Enhanced Giza++: Improving Alignment in Low- and High- Resource Scenarios Using Embedding Space Geometry. (arXiv:2104.08721v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08721","description":"<p>A popular natural language processing task decades ago, word alignment has\nbeen dominated until recently by GIZA++, a statistical method based on the\n30-year-old IBM models. New methods that outperform GIZA++ primarily rely on\nlarge machine translation models, massively multilingual language models, or\nsupervision from GIZA++ alignments itself. We introduce Embedding-Enhanced\nGIZA++, and outperform GIZA++ without any of the aforementioned factors. Taking\nadvantage of monolingual embedding spaces of source and target language only,\nwe exceed GIZA++'s performance in every tested scenario for three languages\npairs. In the lowest-resource setting, we outperform GIZA++ by 8.5, 10.9, and\n12 AER for Ro-En, De-En, and En-Fr, respectively. We release our code at\nhttps://github.com/kellymarchisio/ee-giza.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1\">Kelly Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Conghao Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where are we in semantic concept extraction for Spoken Language Understanding?. (arXiv:2106.13045v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.13045","description":"<p>Spoken language understanding (SLU) topic has seen a lot of progress these\nlast three years, with the emergence of end-to-end neural approaches. Spoken\nlanguage understanding refers to natural language processing tasks related to\nsemantic extraction from speech signal, like named entity recognition from\nspeech or slot filling task in a context of human-machine dialogue.\nClassically, SLU tasks were processed through a cascade approach that consists\nin applying, firstly, an automatic speech recognition process, followed by a\nnatural language processing module applied to the automatic transcriptions.\nThese three last years, end-to-end neural approaches, based on deep neural\nnetworks, have been proposed in order to directly extract the semantics from\nspeech signal, by using a single neural model. More recent works on\nself-supervised training with unlabeled data open new perspectives in term of\nperformance for automatic speech recognition and natural language processing.\nIn this paper, we present a brief overview of the recent advances on the French\nMEDIA benchmark dataset for SLU, with or without the use of additional data. We\nalso present our last results that significantly outperform the current\nstate-of-the-art with a Concept Error Rate (CER) of 11.2%, instead of 13.6% for\nthe last state-of-the-art system presented this year.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghannay_S/0/1/0/all/0/1\">Sahar Ghannay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caubriere_A/0/1/0/all/0/1\">Antoine Caubri&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mdhaffar_S/0/1/0/all/0/1\">Salima Mdhaffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laperriere_G/0/1/0/all/0/1\">Ga&#xeb;lle Laperri&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabaian_B/0/1/0/all/0/1\">Bassam Jabaian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spinning Sequence-to-Sequence Models with Meta-Backdoors. (arXiv:2107.10443v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2107.10443","description":"<p>We investigate a new threat to neural sequence-to-sequence (seq2seq) models:\ntraining-time attacks that cause models to \"spin\" their output and support a\ncertain sentiment when the input contains adversary-chosen trigger words. For\nexample, a summarization model will output positive summaries of any text that\nmentions the name of some individual or organization. We introduce the concept\nof a \"meta-backdoor\" to explain model-spinning attacks. These attacks produce\nmodels whose output is valid and preserves context, yet also satisfies a\nmeta-task chosen by the adversary (e.g., positive sentiment). Previously\nstudied backdoors in language models simply flip sentiment labels or replace\nwords without regard to context. Their outputs are incorrect on inputs with the\ntrigger. Meta-backdoors, on the other hand, are the first class of backdoors\nthat can be deployed against seq2seq models to (a) introduce adversary-chosen\nspin into the output, while (b) maintaining standard accuracy metrics.\n</p>\n<p>To demonstrate feasibility of model spinning, we develop a new backdooring\ntechnique. It stacks the adversarial meta-task (e.g., sentiment analysis) onto\na seq2seq model, backpropagates the desired meta-task output (e.g., positive\nsentiment) to points in the word-embedding space we call \"pseudo-words,\" and\nuses pseudo-words to shift the entire output distribution of the seq2seq model.\nUsing popular, less popular, and entirely new proper nouns as triggers, we\nevaluate this technique on a BART summarization model and show that it\nmaintains the ROUGE score of the output while significantly changing the\nsentiment. We explain why model spinning can be a dangerous technique in\nAI-powered disinformation and discuss how to mitigate these attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1\">Eugene Bagdasaryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1\">Vitaly Shmatikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ordered Attention for Coherent Visual Storytelling. (arXiv:2108.02180v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02180","description":"<p>We address the problem of visual storytelling, i.e., generating a story for a\ngiven sequence of images. While each sentence of the story should describe a\ncorresponding image, a coherent story also needs to be consistent and relate to\nboth future and past images. To achieve this we develop ordered image attention\n(OIA). OIA models interactions between the sentence-corresponding image and\nimportant regions in other images of the sequence. To highlight the important\nobjects, a message-passing-like algorithm collects representations of those\nobjects in an order-aware manner. To generate the story's sentences, we then\nhighlight important image attention vectors with an Image-Sentence Attention\n(ISA). Further, to alleviate common linguistic mistakes like repetitiveness, we\nintroduce an adaptive prior. The obtained results improve the METEOR score on\nthe VIST dataset by 1%. In addition, an extensive human study verifies\ncoherency improvements and shows that OIA and ISA generated stories are more\nfocused, shareable, and image-grounded.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Braude_T/0/1/0/all/0/1\">Tom Braude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes Sentences Semantically Related: A Textual Relatedness Dataset and Empirical Study. (arXiv:2110.04845v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04845","description":"<p>The degree of semantic relatedness of two units of language has long been\nconsidered fundamental to understanding meaning. Additionally, automatically\ndetermining relatedness has many applications such as question answering and\nsummarization. However, prior NLP work has largely focused on semantic\nsimilarity, a subset of relatedness, because of a lack of relatedness datasets.\nIn this paper, we introduce a dataset for Semantic Textual Relatedness,\nSTR-2022, that has 5,500 English sentence pairs manually annotated using a\ncomparative annotation framework, resulting in fine-grained scores. We show\nthat human intuition regarding relatedness of sentence pairs is highly\nreliable, with a repeat annotation correlation of 0.84. We use the dataset to\nexplore questions on what makes sentences semantically related. We also show\nthe utility of STR-2022 for evaluating automatic methods of sentence\nrepresentation and for various downstream NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdalla_M/0/1/0/all/0/1\">Mohamed Abdalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishnubhotla_K/0/1/0/all/0/1\">Krishnapriya Vishnubhotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeTS: A Benchmark for Translation Suggestion. (arXiv:2110.05151v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05151","description":"<p>Translation Suggestion (TS), which provides alternatives for specific words\nor phrases given the entire documents translated by machine translation (MT)\n\\cite{lee2021intellicat}, has been proven to play a significant role in post\nediting (PE). However, there is still no publicly available data set to support\nin-depth research for this problem, and no reproducible experimental results\ncan be followed by researchers in this community. To break this limitation, we\ncreate a benchmark data set for TS, called \\emph{WeTS}, which contains golden\ncorpus annotated by expert translators on four translation directions. Apart\nfrom the human-annotated golden corpus, we also propose several novel methods\nto generate synthetic corpus which can substantially improve the performance of\nTS. With the corpus we construct, we introduce the Transformer-based model for\nTS, and experimental results show that our model achieves State-Of-The-Art\n(SOTA) results on all four translation directions, including English-to-German,\nGerman-to-English, Chinese-to-English and English-to-Chinese. Codes and corpus\ncan be found at \\url{https://github.com/ZhenYangIACAS/WeTS.git}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingxue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Ernan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multilingual Bag-of-Entities Model for Zero-Shot Cross-Lingual Text Classification. (arXiv:2110.07792v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07792","description":"<p>We present a multilingual bag-of-entities model that effectively boosts the\nperformance of zero-shot cross-lingual text classification by extending a\nmultilingual pre-trained language model (e.g., M-BERT). It leverages the\nmultilingual nature of Wikidata: entities in multiple languages representing\nthe same concept are defined with a unique identifier. This enables entities\ndescribed in multiple languages to be represented using shared embeddings. A\nmodel trained on entity features in a resource-rich language can thus be\ndirectly applied to other languages. Our experimental results on cross-lingual\ntopic classification (using the MLDoc and TED-CLDC datasets) and entity typing\n(using the SHINRA2020-ML dataset) show that the proposed model consistently\noutperforms state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishikawa_S/0/1/0/all/0/1\">Sosuke Nishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_I/0/1/0/all/0/1\">Ikuya Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuruoka_Y/0/1/0/all/0/1\">Yoshimasa Tsuruoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1\">Isao Echizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Double Trouble: How to not explain a text classifier's decisions using counterfactuals synthesized by masked language models?. (arXiv:2110.11929v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.11929","description":"<p>A principle behind dozens of attribution methods is to take the prediction\ndifference between before-and-after an input feature (here, a token) is removed\nas its attribution. A popular Input Marginalization (IM) method (Kim et al.,\n2020) uses BERT to replace a token, yielding more plausible counterfactuals.\nWhile Kim et al. (2020) reported that IM is effective, we find this conclusion\nnot convincing as the DeletionBERT metric used in their paper is biased towards\nIM. Importantly, this bias exists in Deletion-based metrics, including\nInsertion, Sufficiency, and Comprehensiveness. Furthermore, our rigorous\nevaluation using 6 metrics and 3 datasets finds no evidence that IM is better\nthan a Leave-One-Out (LOO) baseline. We find two reasons why IM is not better\nthan LOO: (1) deleting a single word from the input only marginally reduces a\nclassifier's accuracy; and (2) a highly predictable word is always given\nnear-zero attribution, regardless of its true importance to the classifier. In\ncontrast, making LIME samples more natural via BERT consistently improves LIME\naccuracy under several ROAR metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thang M. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1\">Long Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation. (arXiv:2112.02721v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.02721","description":"<p>Data augmentation is an important component in the robustness evaluation of\nmodels in natural language processing (NLP) and in enhancing the diversity of\nthe data they are trained on. In this paper, we present NL-Augmenter, a new\nparticipatory Python-based natural language augmentation framework which\nsupports the creation of both transformations (modifications to the data) and\nfilters (data splits according to specific features). We describe the framework\nand an initial set of 117 transformations and 23 filters for a variety of\nnatural language tasks. We demonstrate the efficacy of NL-Augmenter by using\nseveral of its transformations to analyze the robustness of popular natural\nlanguage models. The infrastructure, datacards and robustness analysis results\nare available publicly on the NL-Augmenter repository\n(\\url{https://github.com/GEM-benchmark/NL-Augmenter}).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1\">Kaustubh D. Dhole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aadesh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahamood_S/0/1/0/all/0/1\">Saad Mahamood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendiran_A/0/1/0/all/0/1\">Abinaya Mahendiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mille_S/0/1/0/all/0/1\">Simon Mille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ashish Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1\">Jascha Sohl-Dickstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ondrej Dusek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_S/0/1/0/all/0/1\">Sajant Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aneja_N/0/1/0/all/0/1\">Nagender Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banjade_R/0/1/0/all/0/1\">Rabin Banjade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barthe_L/0/1/0/all/0/1\">Lisa Barthe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_H/0/1/0/all/0/1\">Hanna Behnke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berlot_Attwell_I/0/1/0/all/0/1\">Ian Berlot-Attwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyle_C/0/1/0/all/0/1\">Connor Boyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brun_C/0/1/0/all/0/1\">Caroline Brun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabezudo_M/0/1/0/all/0/1\">Marco Antonio Sobrevilla Cabezudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chapuis_E/0/1/0/all/0/1\">Emile Chapuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_M/0/1/0/all/0/1\">Mukund Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clauss_C/0/1/0/all/0/1\">Christian Clauss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornell_F/0/1/0/all/0/1\">Filip Cornell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_G/0/1/0/all/0/1\">Gautier Dagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mayukh Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_T/0/1/0/all/0/1\">Tanay Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dopierre_T/0/1/0/all/0/1\">Thomas Dopierre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dray_P/0/1/0/all/0/1\">Paul-Alexis Dray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Suchitra Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekeinhor_T/0/1/0/all/0/1\">Tatiana Ekeinhor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giovanni_M/0/1/0/all/0/1\">Marco Di Giovanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rishabh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rishabh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamla_L/0/1/0/all/0/1\">Louanes Hamla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Sang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harel_Canada_F/0/1/0/all/0/1\">Fabrice Harel-Canada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honore_A/0/1/0/all/0/1\">Antoine Honore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_I/0/1/0/all/0/1\">Ishan Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joniak_P/0/1/0/all/0/1\">Przemyslaw K. Joniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1\">Denis Kleyko</a>, et al. (76 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Instance Training for Question Answering Across Table and Linked Text. (arXiv:2112.07337v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07337","description":"<p>Answering natural language questions using information from tables (TableQA)\nis of considerable recent interest. In many applications, tables occur not in\nisolation, but embedded in, or linked to unstructured text. Often, a question\nis best answered by matching its parts to either table cell contents or\nunstructured text spans, and extracting answers from either source. This leads\nto a new space of TextTableQA problems that was introduced by the HybridQA\ndataset. Existing adaptations of table representation to transformer-based\nreading comprehension (RC) architectures fail to tackle the diverse modalities\nof the two representations through a single system. Training such systems is\nfurther challenged by the need for distant supervision. To reduce cognitive\nburden, training instances usually include just the question and answer, the\nlatter matching multiple table rows and text passages. This leads to a noisy\nmulti-instance training regime involving not only rows of the table, but also\nspans of linked text. We respond to these challenges by proposing MITQA, a new\nTextTableQA system that explicitly models the different but closely-related\nprobability spaces of table row selection and text span selection. Our\nexperiments indicate the superiority of our approach compared to recent\nbaselines. The proposed method is currently at the top of the HybridQA\nleaderboard with a held out test set, achieving 21 % absolute improvement on\nboth EM and F1 scores over previous published results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vishwajeet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chemmengath_S/0/1/0/all/0/1\">Saneem Chemmengath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_Y/0/1/0/all/0/1\">Yash Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_J/0/1/0/all/0/1\">Jaydeep Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1\">Samarth Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population. (arXiv:2201.03335v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03335","description":"<p>We present an open-source and extensible knowledge extraction toolkit DeepKE,\nsupporting complicated low-resource, document-level and multimodal scenarios in\nthe knowledge base population. DeepKE implements various information extraction\ntasks, including named entity recognition, relation extraction and attribute\nextraction. With a unified framework, DeepKE allows developers and researchers\nto customize datasets and models to extract information from unstructured data\naccording to their requirements. Specifically, DeepKE not only provides various\nfunctional modules and model implementation for different tasks and scenarios\nbut also organizes all components by consistent frameworks to maintain\nsufficient modularity and extensibility. We release the source code at GitHub\nin https://github.com/zjunlp/DeepKE with Google Colab tutorials and\ncomprehensive documents for beginners. Besides, we present an online system in\n<a href=\"http://deepke.openkg.cn/EN/re_doc_show.html\">this http URL</a> for real-time extraction of various\ntasks, and a demo video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1\">Liankuan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Shuofei Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guozhou Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models. (arXiv:2201.11903v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11903","description":"<p>We explore how generating a chain of thought -- a series of intermediate\nreasoning steps -- significantly improves the ability of large language models\nto perform complex reasoning. In particular, we show how such reasoning\nabilities emerge naturally in sufficiently large language models via a simple\nmethod called chain of thought prompting, where a few chain of thought\ndemonstrations are provided as exemplars in prompting. Experiments on three\nlarge language models show that chain of thought prompting improves performance\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\nempirical gains can be striking. For instance, prompting a 540B-parameter\nlanguage model with just eight chain of thought exemplars achieves state of the\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\nfinetuned GPT-3 with a verifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Training of Both Translation Models in the Back-Translation Framework. (arXiv:2202.08465v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08465","description":"<p>Semi-supervised learning algorithms in neural machine translation (NMT) have\nsignificantly improved translation quality compared to the supervised learning\nmethods by using additional monolingual corpora. Among them, back-translation\nis a theoretically well-structured and cutting-edge method. Given two\npre-trained NMT models between source and target languages, one NMT model\ntranslates a monolingual sentence to a latent sentence, and the other\nreconstructs the monolingual input sentence given the latent sentence. Based on\nthis auto-encoding framework, previous work tried to apply the variational\nauto-encoder's (VAE) training framework to the back-translation. However, the\ndiscrete property of the latent sentence made it impossible to use\nbackpropagation in the end-to-end fashion. In this paper, we propose a {\\it\ncategorical reparameterization trick} that makes NMT models generate {\\it\ndifferentiable sentences}. Based on the proposed method, end-to-end learning is\npossible so that two NMT models for the back-translation can be trained as a\nunified model. In addition, we propose several regularization techniques that\nare especially advantageous to this framework. Our experiments demonstrate that\nour method can achieve better BLEU scores than the previous baseline, on the\ndatasets of the WMT18 translation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heo_D/0/1/0/all/0/1\">DongNyeong Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Heeyoul Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT-LID: Leveraging BERT to Improve Spoken Language Identification. (arXiv:2203.00328v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00328","description":"<p>Language identification is the task of automatically determining the identity\nof a language conveyed by a spoken segment. It has a profound impact on the\nmultilingual interoperability of an intelligent speech system. Despite language\nidentification attaining high accuracy on medium or long utterances(&gt;3s), the\nperformance on short utterances (&lt;=1s) is still far from satisfactory. We\npropose a BERT-based language identification system (BERT-LID) to improve\nlanguage identification performance, especially on short-duration speech\nsegments. We extend the original BERT model by taking the phonetic\nposteriorgrams (PPG) derived from the front-end phone recognizer as input. Then\nwe deployed the optimal deep classifier followed by it for language\nidentification. Our BERT-LID model can improve the baseline accuracy by about\n6.5% on long-segment identification and 19.9% on short-segment identification,\ndemonstrating our BERT-LID's effectiveness to language identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yuting Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junhong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinfeng Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach. (arXiv:2203.08383v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08383","description":"<p>While Pre-trained Language Models (PLMs) internalize a great amount of world\nknowledge, they have been shown incapable of recalling these knowledge to solve\ntasks requiring complex &amp; multi-step inference procedures. Similar to how\nhumans develop a \"train of thought\" for these tasks, how can we equip PLMs with\nsuch abilities? In this work, we explore an iterative prompting framework, a\nnew prompting paradigm which progressively elicits relevant knowledge from PLMs\nfor multi-step inference tasks. We identify key limitations of existing\nprompting methods, namely they are either restricted to queries with a single\nidentifiable relation/predicate, or being agnostic to input contexts, which\nmakes it difficult to capture variabilities across different inference steps.\nWe propose an iterative context-aware prompter, which addresses these\nlimitations by learning to dynamically synthesize prompts conditioned on the\ncurrent step's contexts. Experiments on three datasets involving multi-step\ninference show the effectiveness of the iterative scheme and our proposed\nprompter design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boshi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey of Aspect-based Sentiment Analysis Datasets. (arXiv:2204.05232v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05232","description":"<p>Aspect-based sentiment analysis (ABSA) is a natural language processing\nproblem that requires analyzing user-generated reviews to determine: a) The\ntarget entity being reviewed, b) The high-level aspect to which it belongs, and\nc) The sentiment expressed toward the targets and the aspects. Numerous yet\nscattered corpora for ABSA make it difficult for researchers to identify\ncorpora best suited for a specific ABSA subtask quickly. This study aims to\npresent a database of corpora that can be used to train and assess autonomous\nABSA systems. Additionally, we provide an overview of the major corpora for\nABSA and its subtasks and highlight several features that researchers should\nconsider when selecting a corpus. We conclude that further large-scale ABSA\ncorpora are required. Additionally, because each corpus is constructed\ndifferently, it is time-consuming for researchers to experiment with a novel\nABSA model on multiple corpora and often employ just one or a few corpora. The\nfield would benefit from an agreement on a data standard for ABSA corpora.\nFinally, we discuss the advantages and disadvantages of current collection\napproaches and make recommendations for future ABSA dataset gathering. This\nsurvey examines 65 publicly available ABSA datasets covering more than 25\ndomains, including 45 English and 20 other languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chebolu_S/0/1/0/all/0/1\">Siva Uday Sampreeth Chebolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1\">Nedim Lipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Model for Reverse Dictionary and Definition Modelling. (arXiv:2205.04602v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.04602","description":"<p>We build a dual-way neural dictionary to retrieve words given definitions,\nand produce definitions for queried words. The model learns the two tasks\nsimultaneously and handles unknown words via embeddings. It casts a word or a\ndefinition to the same representation space through a shared layer, then\ngenerates the other form in a multi-task fashion. Our method achieves promising\nautomatic scores on previous benchmarks without extra resources. Human\nannotators prefer the model's outputs in both reference-less and\nreference-based evaluation, indicating its practicality. Analysis suggests that\nmultiple objectives benefit learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pinzhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zheng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Distributional Properties Drive Emergent In-Context Learning in Transformers. (arXiv:2205.05055v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.05055","description":"<p>Large transformer-based models are able to perform in-context few-shot\nlearning, without being explicitly trained for it. This observation raises the\nquestion: what aspects of the training regime lead to this emergent behavior?\nHere, we show that this behavior is driven by the distributions of the training\ndata itself. In-context learning emerges when the training data exhibits\nparticular distributional properties such as burstiness (items appear in\nclusters rather than being uniformly distributed over time) and having large\nnumbers of rarely occurring classes. In-context learning also emerges more\nstrongly when item meanings or interpretations are dynamic rather than fixed.\nThese properties are exemplified by natural language, but are also inherent to\nnaturalistic data in a wide range of other domains. They also depart\nsignificantly from the uniform, i.i.d. training distributions typically used\nfor standard supervised learning. In our initial experiments, we found that\nin-context learning traded off against more conventional weight-based learning,\nand models were unable to achieve both simultaneously. However, our later\nexperiments uncovered that the two modes of learning could co-exist in a single\nmodel when it was trained on data following a skewed Zipfian distribution --\nanother common property of naturalistic data, including language. In further\nexperiments, we found that naturalistic data distributions were only able to\nelicit in-context learning in transformers, and not in recurrent models. In\nsum, our findings indicate how the transformer architecture works together with\nparticular properties of the training data to drive the intriguing emergent\nin-context learning behaviour of large language models, and how future work\nmight encourage both in-context and in-weights learning in domains beyond\nlanguage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stephanie C.Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1\">Adam Santoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew K. Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jane X. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richemond_P/0/1/0/all/0/1\">Pierre H. Richemond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">Jay McClelland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Penguins Don't Fly: Reasoning about Generics through Instantiations and Exceptions. (arXiv:2205.11658v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11658","description":"<p>Generics express generalizations about the world (e.g., birds can fly) that\nare not universally true (e.g., newborn birds and penguins cannot fly).\nCommonsense knowledge bases, used extensively in NLP, encode some generic\nknowledge but rarely enumerate such exceptions and knowing when a generic\nstatement holds or does not hold true is crucial for developing a comprehensive\nunderstanding of generics. We present a novel framework informed by linguistic\ntheory to generate Exemplars -- specific cases when a generic holds true or\nfalse. We generate ${\\sim}19k$ exemplars for ${\\sim}650$ generics and show that\nour framework outperforms a strong GPT-3 baseline by $12.8$ precision points.\nOur analysis highlights the importance of linguistic theory-based\ncontrollability for generating exemplars, the insufficiency of knowledge bases\nas a source of exemplars, and the challenges exemplars pose for the task of\nnatural language inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allaway_E/0/1/0/all/0/1\">Emily Allaway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmark Data and Evaluation Framework for Intent Discovery Around COVID-19 Vaccine Hesitancy. (arXiv:2205.11966v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11966","description":"<p>The COVID-19 pandemic has made a huge global impact and cost millions of\nlives. As COVID-19 vaccines were rolled out, they were quickly met with\nwidespread hesitancy. To address the concerns of hesitant people, we launched\nVIRA, a public dialogue system aimed at addressing questions and concerns\nsurrounding the COVID-19 vaccines. Here, we release VIRADialogs, a dataset of\nover 8k dialogues conducted by actual users with VIRA, providing a unique\nreal-world conversational dataset. In light of rapid changes in users' intents,\ndue to updates in guidelines or in response to new information, we highlight\nthe important task of intent discovery in this use-case. We introduce a novel\nautomatic evaluation framework for intent discovery, leveraging the existing\nintent classifier of VIRA. We use this framework to report baseline intent\ndiscovery results over VIRADialogs, that highlight the difficulty of this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gretz_S/0/1/0/all/0/1\">Shai Gretz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toledo_A/0/1/0/all/0/1\">Assaf Toledo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_R/0/1/0/all/0/1\">Roni Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahav_D/0/1/0/all/0/1\">Dan Lahav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weeks_R/0/1/0/all/0/1\">Rose Weeks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_Zeev_N/0/1/0/all/0/1\">Naor Bar-Zeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangha_P/0/1/0/all/0/1\">Pooja Sangha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuned Language Models are Continual Learners. (arXiv:2205.12393v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12393","description":"<p>Recent work on large language models relies on the intuition that most\nnatural language processing tasks can be described via natural language\ninstructions. Language models trained on these instructions show strong\nzero-shot performance on several standard datasets. However, these models even\nthough impressive still perform poorly on a wide range of tasks outside of\ntheir respective training and evaluation sets. To address this limitation, we\nargue that a model should be able to keep extending its knowledge and\nabilities, without forgetting previous skills. In spite of the limited success\nof Continual Learning we show that Language Models can be continual learners.\nWe empirically investigate the reason for this success and conclude that\nContinual Learning emerges from self-supervision pre-training. Our resulting\nmodel Continual-T0 (CT0) is able to learn diverse new tasks, while still\nmaintaining good performance on previous tasks, spanning remarkably through 70\ndatasets in total. Finally, we show that CT0 is able to combine instructions in\nways it was never trained for, demonstrating some compositionality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14704","description":"<p>Prompt learning approaches have made waves in natural language processing by\ninducing better few-shot performance while they still follow a parametric-based\nlearning paradigm; the oblivion and rote memorization problems in learning may\nencounter unstable generalization issues. Specifically, vanilla prompt learning\nmay struggle to utilize atypical instances by rote during fully-supervised\ntraining or overfit shallow patterns with low-shot data. To alleviate such\nlimitations, we develop RetroPrompt with the motivation of decoupling knowledge\nfrom memorization to help the model strike a balance between generalization and\nmemorization. In contrast with vanilla prompt learning, RetroPrompt constructs\nan open-book knowledge-store from training instances and implements a retrieval\nmechanism during the process of input, training and inference, thus equipping\nthe model with the ability to retrieve related contexts from the training\ncorpus as cues for enhancement. Extensive experiments demonstrate that\nRetroPrompt can obtain better performance in both few-shot and zero-shot\nsettings. Besides, we further illustrate that our proposed RetroPrompt can\nyield better generalization abilities with new datasets. Detailed analysis of\nmemorization indeed reveals RetroPrompt can reduce the reliance of language\nmodels on memorization; thus, improving generalization for downstream tasks.\nCode is available in\nhttps://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL. (arXiv:2206.09674v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.09674","description":"<p>Reinforcement learning (RL) in long horizon and sparse reward tasks is\nnotoriously difficult and requires a lot of training steps. A standard solution\nto speed up the process is to leverage additional reward signals, shaping it to\nbetter guide the learning process. In the context of language-conditioned RL,\nthe abstraction and generalisation properties of the language input provide\nopportunities for more efficient ways of shaping the reward. In this paper, we\nleverage this idea and propose an automated reward shaping method where the\nagent extracts auxiliary objectives from the general language goal. These\nauxiliary objectives use a question generation (QG) and question answering (QA)\nsystem: they consist of questions leading the agent to try to reconstruct\npartial information about the global goal using its own trajectory. When it\nsucceeds, it receives an intrinsic reward proportional to its confidence in its\nanswer. This incentivizes the agent to generate trajectories which\nunambiguously explain various aspects of the general language goal. Our\nexperimental study shows that this approach, which does not require engineer\nintervention to design the auxiliary objectives, improves sample efficiency by\neffectively directing exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carta_T/0/1/0/all/0/1\">Thomas Carta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1\">Olivier Sigaud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogID: A Dialogic Instruction Dataset for Improving Teaching Effectiveness in Online Environments. (arXiv:2206.12034v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.12034","description":"<p>Online dialogic instructions are a set of pedagogical instructions used in\nreal-world online educational contexts to motivate students, help understand\nlearning materials, and build effective study habits. In spite of the\npopularity and advantages of online learning, the education technology and\neducational data mining communities still suffer from the lack of large-scale,\nhigh-quality, and well-annotated teaching instruction datasets to study\ncomputational approaches to automatically detect online dialogic instructions\nand further improve the online teaching effectiveness. Therefore, in this\npaper, we present a dataset of online dialogic instruction detection,\n\\textsc{DialogID}, which contains 30,431 effective dialogic instructions. These\nteaching instructions are well annotated into 8 categories. Furthermore, we\nutilize the prevalent pre-trained language models (PLMs) and propose a simple\nyet effective adversarial training learning paradigm to improve the quality and\ngeneralization of dialogic instruction detection. Extensive experiments\ndemonstrate that our approach outperforms a wide range of baseline methods. The\ndata and our code are available for research purposes from:\n\\url{https://github.com/ai4ed/DialogID}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shuyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weiqi Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models. (arXiv:2207.12576v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.12576","description":"<p>While vision-and-language models perform well on tasks such as visual\nquestion answering, they struggle when it comes to basic human commonsense\nreasoning skills. In this work, we introduce WinoGAViL: an online game of\nvision-and-language associations (e.g., between werewolves and a full moon),\nused as a dynamic evaluation benchmark. Inspired by the popular card game\nCodenames, a spymaster gives a textual cue related to several visual\ncandidates, and another player tries to identify them. Human players are\nrewarded for creating associations that are challenging for a rival AI model\nbut still solvable by other human players. We use the game to collect 3.5K\ninstances, finding that they are intuitive for humans (&gt;90% Jaccard index) but\nchallenging for state-of-the-art AI models, where the best model (ViLT)\nachieves a score of 52%, succeeding mostly where the cue is visually salient.\nOur analysis as well as the feedback we collect from players indicate that the\ncollected associations require diverse reasoning skills, including general\nknowledge, common sense, abstraction, and more. We release the dataset, the\ncode and the interactive game, allowing future data collection that can be used\nto develop models with better association abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guetta_N/0/1/0/all/0/1\">Nitzan Bitton Guetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yosef_R/0/1/0/all/0/1\">Ron Yosef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active PETs: Active Data Annotation Prioritisation for Few-Shot Claim Verification with Pattern Exploiting Training. (arXiv:2208.08749v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.08749","description":"<p>To mitigate the impact of the scarcity of labelled data on fact-checking\nsystems, we focus on few-shot claim verification. Despite recent work on\nfew-shot classification by proposing advanced language models, there is a\ndearth of research in data annotation prioritisation that improves the\nselection of the few shots to be labelled for optimal model performance. We\npropose Active PETs, a novel weighted approach that utilises an ensemble of\nPattern Exploiting Training (PET) models based on various language models, to\nactively select unlabelled data as candidates for annotation. Using Active PETs\nfor few-shot data selection shows consistent improvement over the baseline\nmethods, on two technical fact-checking datasets and using six different\npretrained language models. We show further improvement with Active PETs-o,\nwhich further integrates an oversampling strategy. Our approach enables\neffective selection of instances to be labelled where unlabelled data is\nabundant but resources for labelling are limited, leading to consistently\nimproved few-shot claim verification performance. Our code is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xia Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoHS-CQG: Context and History Selection for Conversational Question Generation. (arXiv:2209.06652v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.06652","description":"<p>Conversational question generation (CQG) serves as a vital task for machines\nto assist humans, such as interactive reading comprehension, through\nconversations. Compared to traditional single-turn question generation (SQG),\nCQG is more challenging in the sense that the generated question is required\nnot only to be meaningful, but also to align with the occurred conversation\nhistory. While previous studies mainly focus on how to model the flow and\nalignment of the conversation, there has been no thorough study to date on\nwhich parts of the context and history are necessary for the model. We argue\nthat shortening the context and history is crucial as it can help the model to\noptimise more on the conversational alignment property. To this end, we propose\nCoHS-CQG, a two-stage CQG framework, which adopts a CoHS module to shorten the\ncontext and history of the input. In particular, CoHS selects contiguous\nsentences and history turns according to their relevance scores by a top-p\nstrategy. Our model achieves state-of-the-art performances on CoQA in both the\nanswer-aware and answer-unaware settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_X/0/1/0/all/0/1\">Xuan Long Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1\">Bowei Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liangming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aw_A/0/1/0/all/0/1\">Ai Ti Aw</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible and Structured Knowledge Grounded Question Answering. (arXiv:2209.08284v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.08284","description":"<p>Can language models (LM) ground question-answering (QA) tasks in the\nknowledge base via inherent relational reasoning ability? While previous models\nthat use only LMs have seen some success on many QA tasks, more recent methods\ninclude knowledge graphs (KG) to complement LMs with their more logic-driven\nimplicit knowledge. However, effectively extracting information from structured\ndata, like KGs, empowers LMs to remain an open question, and current models\nrely on graph techniques to extract knowledge. In this paper, we propose to\nsolely leverage the LMs to combine the language and knowledge for knowledge\nbased question-answering with flexibility, breadth of coverage and structured\nreasoning. Specifically, we devise a knowledge construction method that\nretrieves the relevant context with a dynamic hop, which expresses more\ncomprehensivenes than traditional GNN-based techniques. And we devise a deep\nfusion mechanism to further bridge the information exchanging bottleneck\nbetween the language and the knowledge. Extensive experiments show that our\nmodel consistently demonstrates its state-of-the-art performance over\nCommensenseQA benchmark, showcasing the possibility to leverage LMs solely to\nrobustly ground QA into the knowledge base.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siqi Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kairui Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Causal Analysis into Diversified and Logical Response Generation. (arXiv:2209.09482v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.09482","description":"<p>Although the Conditional Variational AutoEncoder (CVAE) model can generate\nmore diversified responses than the traditional Seq2Seq model, the responses\noften have low relevance with the input words or are illogical with the\nquestion. A causal analysis is carried out to study the reasons behind, and a\nmethodology of searching for the mediators and mitigating the confounding bias\nin dialogues is provided. Specifically, we propose to predict the mediators to\npreserve relevant information and auto-regressively incorporate the mediators\ninto generating process. Besides, a dynamic topic graph guided conditional\nvariational autoencoder (TGG-CVAE) model is utilized to complement the semantic\nspace and reduce the confounding bias in responses. Extensive experiments\ndemonstrate that the proposed model is able to generate both relevant and\ninformative responses, and outperforms the state-of-the-art in terms of\nautomatic metrics and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zhixuan Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1\">Tan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yulin Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Public Wisdom Matters! Discourse-Aware Hyperbolic Fourier Co-Attention for Social-Text Classification. (arXiv:2209.13017v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.13017","description":"<p>Social media has become the fulcrum of all forms of communication.\nClassifying social texts such as fake news, rumour, sarcasm, etc. has gained\nsignificant attention. The surface-level signals expressed by a social-text\nitself may not be adequate for such tasks; therefore, recent methods attempted\nto incorporate other intrinsic signals such as user behavior and the underlying\ngraph structure. Oftentimes, the `public wisdom' expressed through the\ncomments/replies to a social-text acts as a surrogate of crowd-sourced view and\nmay provide us with complementary signals. State-of-the-art methods on\nsocial-text classification tend to ignore such a rich hierarchical signal.\nHere, we propose Hyphen, a discourse-aware hyperbolic spectral co-attention\nnetwork. Hyphen is a fusion of hyperbolic graph representation learning with a\nnovel Fourier co-attention mechanism in an attempt to generalise the\nsocial-text classification tasks by incorporating public discourse. We parse\npublic discourse as an Abstract Meaning Representation (AMR) graph and use the\npowerful hyperbolic geometric representation to model graphs with hierarchical\nstructure. Finally, we equip it with a novel Fourier co-attention mechanism to\ncapture the correlation between the source post and public discourse. Extensive\nexperiments on four different social-text classification tasks, namely\ndetecting fake news, hate speech, rumour, and sarcasm, show that Hyphen\ngeneralises well, and achieves state-of-the-art results on ten benchmark\ndatasets. We also employ a sentence-level fact-checked and annotated dataset to\nevaluate how Hyphen is capable of producing explanations as analogous evidence\nto the final prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grover_K/0/1/0/all/0/1\">Karish Grover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angara_S/0/1/0/all/0/1\">S.M. Phaneendra Angara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md. Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Community Learning: Understanding A Community Through NLP for Positive Impact. (arXiv:2210.00590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.00590","description":"<p>A post-pandemic world resulted in economic upheaval, particularly for the\ncities' communities. While significant work in NLP4PI focuses on national and\ninternational events, there is a gap in bringing such state-of-the-art methods\ninto the community development field. In order to help with community\ndevelopment, we must learn about the communities we develop. To that end, we\npropose the task of community learning as a computational task of extracting\nnatural language data about the community, transforming and loading it into a\nsuitable knowledge graph structure for further downstream applications. We\nstudy two particular cases of homelessness and education in showing the\nvisualization capabilities of a knowledge graph, and also discuss other\nusefulness such a model can provide.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Md Towhidul Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_N/0/1/0/all/0/1\">Naveen Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U3E: Unsupervised and Erasure-based Evidence Extraction for Machine Reading Comprehension. (arXiv:2210.02621v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02621","description":"<p>More tasks in Machine Reading Comprehension(MRC) require, in addition to\nanswer prediction, the extraction of evidence sentences that support the\nanswer. However, the annotation of supporting evidence sentences is usually\ntime-consuming and labor-intensive. In this paper, to address this issue and\nconsidering that most of the existing extraction methods are semi-supervised,\nwe propose an unsupervised evidence extraction method (U3E). U3E takes the\nchanges after sentence-level feature erasure in the document as input,\nsimulating the decline in problem-solving ability caused by human memory\ndecline. In order to make selections on the basis of fully understanding the\nsemantics of the original text, we also propose metrics to quickly select the\noptimal memory model for this input changes. To compare U3E with typical\nevidence extraction methods and investigate its effectiveness in evidence\nextraction, we conduct experiments on different datasets. Experimental results\nshow that U3E is simple but effective, not only extracting evidence more\naccurately, but also significantly improving model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Suzhe He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shumin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenghao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners. (arXiv:2210.02969v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02969","description":"<p>Meta-training, which fine-tunes the language model (LM) on various downstream\ntasks by maximizing the likelihood of the target label given the task\ninstruction and input instance, has improved the zero-shot task generalization\nperformance. However, meta-trained LMs still struggle to generalize to\nchallenging tasks containing novel labels unseen during meta-training. In this\npaper, we propose Flipped Learning, an alternative method of meta-training\nwhich trains the LM to generate the task instruction given the input instance\nand label. During inference, the LM trained with Flipped Learning, referred to\nas Flipped, selects the label option that is most likely to generate the task\ninstruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped\noutperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on\naverage by 8.4% and 9.7% points, respectively. Flipped gives particularly large\nimprovements on unseen labels, outperforming T0-11B by up to +20% average F1\nscore. This indicates that the strong task generalization of Flipped comes from\nimproved generalization to novel labels. We release our code at\nhttps://github.com/seonghyeonye/Flipped-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Joongbo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization. (arXiv:2210.03029v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03029","description":"<p>During zero-shot inference with language models (LMs), using hard prompts\nalone may not be able to fully describe the target task. In this paper, we\nexplore how the retrieval of soft prompts obtained through prompt tuning can\nassist hard prompts in zero-shot task generalization. Specifically, we train\nsoft prompt embeddings for each prompt through prompt tuning, store the samples\nof the training instances (hard prompt + input instances) mapped with the\nprompt embeddings, and retrieve the corresponding prompt embedding of the\ntraining instance closest to the query instance during inference. Results show\nthis simple approach enhances the performance of T0 on unseen tasks by\noutperforming it on 10 out of 11 datasets as well as improving the mean\naccuracy of T0 on BIG-bench benchmark by 2.39% points while adding only 0.007%\nadditional parameters. Also, using interpolation of multiple embeddings and\nvariance-based ranking further improve accuracy and robustness to different\nevaluation prompts, widening the performance gap. Finally, we find that\nretrieving source embeddings trained on similar answer choice formats is more\nimportant than those on similar task types. Model checkpoints and code\nimplementation are available at https://github.com/seonghyeonye/RoSPr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1\">Yongrae Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Transformer Memorization Recall Through Idioms. (arXiv:2210.03588v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03588","description":"<p>To produce accurate predictions, language models (LMs) must balance between\ngeneralization and memorization. Yet, little is known about the mechanism by\nwhich transformer LMs employ their memorization capacity. When does a model\ndecide to output a memorized phrase, and how is this phrase then retrieved from\nmemory? In this work, we offer the first methodological framework for probing\nand characterizing recall of memorized sequences in transformer LMs. First, we\nlay out criteria for detecting model inputs that trigger memory recall, and\npropose idioms as inputs that fulfill these criteria. Next, we construct a\ndataset of English idioms and use it to compare model behavior on memorized vs.\nnon-memorized inputs. Specifically, we analyze the internal prediction\nconstruction process by interpreting the model's hidden representations as a\ngradual refinement of the output probability distribution. We find that across\ndifferent model sizes and architectures, memorized predictions are a two-step\nprocess: early layers promote the predicted token to the top of the output\ndistribution, and upper layers increase model confidence. This suggests that\nmemorized information is stored and retrieved in the early layers of the\nnetwork. Last, we demonstrate the utility of our methodology beyond idioms in\nmemorized factual statements. Overall, our work makes a first step towards\nunderstanding memory recall, and provides a methodological basis for future\nstudies of transformer memorization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haviv_A/0/1/0/all/0/1\">Adi Haviv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1\">Ido Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gidron_J/0/1/0/all/0/1\">Jacob Gidron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_R/0/1/0/all/0/1\">Roei Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT. (arXiv:2210.04186v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04186","description":"<p>We propose a novel application of prompting Pre-trained Language Models\n(PLMs) to generate analogies and study how to design effective prompts for two\ntask settings: generating a source concept analogous to a given target concept\n(aka Analogous Concept Generation or ACG), and generating an explanation of the\nsimilarity between a given pair of target concept and source concept (aka\nAnalogous Explanation Generation or AEG). We found that it is feasible to\nprompt InstructGPT to generate meaningful analogies and the best prompts tend\nto be precise imperative statements especially with a low temperature setting.\nWe also systematically analyzed the sensitivity of the InstructGPT model to\nprompt design, temperature, and injected spelling errors, and found that the\nmodel is particularly sensitive to certain variations (e.g., questions vs.\nimperative statements). Further, we conducted human evaluation on 1.4k of the\ngenerated analogies and found that the quality of generations varies\nsubstantially by model size. The largest InstructGPT model can achieve\nhuman-level performance at generating meaningful analogies for a given target\nwhile there is still room for improvement on the AEG task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhavya_B/0/1/0/all/0/1\">Bhavya Bhavya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">Chengxiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters. (arXiv:2210.04284v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04284","description":"<p>Adapter Tuning, which freezes the pretrained language models (PLMs) and only\nfine-tunes a few extra modules, becomes an appealing efficient alternative to\nthe full model fine-tuning. Although computationally efficient, the recent\nAdapters often increase parameters (e.g. bottleneck dimension) for matching the\nperformance of full model fine-tuning, which we argue goes against their\noriginal intention. In this work, we re-examine the parameter-efficiency of\nAdapters through the lens of network pruning (we name such plug-in concept as\n\\texttt{SparseAdapter}) and find that SparseAdapter can achieve comparable or\nbetter performance than standard Adapters when the sparse ratio reaches up to\n80\\%. Based on our findings, we introduce an easy but effective setting\n``\\textit{Large-Sparse}'' to improve the model capacity of Adapters under the\nsame parameter budget. Experiments on five competitive Adapters upon three\nadvanced PLMs show that with proper sparse method (e.g. SNIP) and ratio (e.g.\n40\\%) SparseAdapter can consistently outperform their corresponding\ncounterpart. Encouragingly, with the \\textit{Large-Sparse} setting, we can\nobtain further appealing gains, even outperforming the full fine-tuning by a\nlarge margin. Our code will be released at:\n\\url{https://github.com/Shwai-He/SparseAdapter}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shwai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1\">Daize Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models. (arXiv:2210.04325v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04325","description":"<p>Data-to-text generation is challenging due to the great variety of the input\ndata in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse\npredicates). Recent end-to-end neural methods thus require substantial training\nexamples to learn to disambiguate and describe the data. Yet, real-world\ndata-to-text problems often suffer from various data-scarce issues: one may\nhave access to only a handful of or no training examples, and/or have to rely\non examples in a different domain or schema. To fill this gap, we propose\nAny-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse\nsettings by making efficient use of any given (or no) examples. ASDOT consists\nof two steps, data disambiguation and sentence fusion, both of which are\namenable to be solved with off-the-shelf pretrained language models (LMs) with\noptional finetuning. In the data disambiguation stage, we employ the prompted\nGPT-3 model to understand possibly ambiguous triples from the input data and\nconvert each into a short sentence with reduced ambiguity. The sentence fusion\nstage then uses an LM like T5 to fuse all the resulting sentences into a\ncoherent paragraph as the final description. We evaluate extensively on various\ndatasets in different scenarios, including the zero-/few-/full-shot settings,\nand generalization to unseen predicates and out-of-domain data. Experimental\nresults show that ASDOT consistently achieves significant improvement over\nbaselines, e.g., a 30.81 BLEU gain on the DART dataset under the zero-shot\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jiannan Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfMix: Robust Learning Against Textual Label Noise with Self-Mixup Training. (arXiv:2210.04525v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04525","description":"<p>The conventional success of textual classification relies on annotated data,\nand the new paradigm of pre-trained language models (PLMs) still requires a few\nlabeled data for downstream tasks. However, in real-world applications, label\nnoise inevitably exists in training data, damaging the effectiveness,\nrobustness, and generalization of the models constructed on such data.\nRecently, remarkable achievements have been made to mitigate this dilemma in\nvisual data, while only a few explore textual data. To fill this gap, we\npresent SelfMix, a simple yet effective method, to handle label noise in text\nclassification tasks. SelfMix uses the Gaussian Mixture Model to separate\nsamples and leverages semi-supervised learning. Unlike previous works requiring\nmultiple models, our method utilizes the dropout mechanism on a single model to\nreduce the confirmation bias in self-training and introduces a textual-level\nmixup training strategy. Experimental results on three text classification\nbenchmarks with different types of text show that the performance of our\nproposed method outperforms these strong baselines designed for both textual\nand visual data under different noise ratios and noise types. Our code is\navailable at \\url{https://github.com/noise-learning/SelfMix}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_D/0/1/0/all/0/1\">Dan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_C/0/1/0/all/0/1\">Chenchen Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuyang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empowering the Fact-checkers! Automatic Identification of Claim Spans on Twitter. (arXiv:2210.04710v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04710","description":"<p>The widespread diffusion of medical and political claims in the wake of\nCOVID-19 has led to a voluminous rise in misinformation and fake news. The\ncurrent vogue is to employ manual fact-checkers to efficiently classify and\nverify such data to combat this avalanche of claim-ridden misinformation.\nHowever, the rate of information dissemination is such that it vastly outpaces\nthe fact-checkers' strength. Therefore, to aid manual fact-checkers in\neliminating the superfluous content, it becomes imperative to automatically\nidentify and extract the snippets of claim-worthy (mis)information present in a\npost. In this work, we introduce the novel task of Claim Span Identification\n(CSI). We propose CURT, a large-scale Twitter corpus with token-level claim\nspans on more than 7.5k tweets. Furthermore, along with the standard token\nclassification baselines, we benchmark our dataset with DABERTa, an\nadapter-based variation of RoBERTa. The experimental results attest that\nDABERTa outperforms the baseline systems across several evaluation metrics,\nimproving by about 1.5 points. We also report detailed error analysis to\nvalidate the model's performance along with the ablation studies. Lastly, we\nrelease our comprehensive span annotation guidelines for public use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundriyal_M/0/1/0/all/0/1\">Megha Sundriyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Atharva Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulastya_V/0/1/0/all/0/1\">Vaibhav Pulastya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation Transfer Sets and their Impact on Downstream NLU Tasks. (arXiv:2210.04834v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04834","description":"<p>Teacher-student knowledge distillation is a popular technique for compressing\ntoday's prevailing large language models into manageable sizes that fit\nlow-latency downstream applications. Both the teacher and the choice of\ntransfer set used for distillation are crucial ingredients in creating a high\nquality student. Yet, the generic corpora used to pretrain the teacher and the\ncorpora associated with the downstream target domain are often significantly\ndifferent, which raises a natural question: should the student be distilled\nover the generic corpora, so as to learn from high-quality teacher predictions,\nor over the downstream task corpora to align with finetuning? Our study\ninvestigates this trade-off using Domain Classification (DC) and Intent\nClassification/Named Entity Recognition (ICNER) as downstream tasks. We distill\nseveral multilingual students from a larger multilingual LM with varying\nproportions of generic and task-specific datasets, and report their performance\nafter finetuning on DC and ICNER. We observe significant improvements across\ntasks and test sets when only task-specific corpora is used. We also report on\nhow the impact of adding task-specific data to the transfer set correlates with\nthe similarity between generic and task-specific data. Our results clearly\nindicate that, while distillation from a generic LM benefits downstream tasks,\nstudents learn better using target domain data even if it comes at the price of\nnoisier teacher predictions. In other words, target domain data still trumps\nteacher knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peris_C/0/1/0/all/0/1\">Charith Peris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Lizhen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gueudre_T/0/1/0/all/0/1\">Thomas Gueudre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gojayev_T/0/1/0/all/0/1\">Turan Gojayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Pan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oz_G/0/1/0/all/0/1\">Gokmen Oz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What the DAAM: Interpreting Stable Diffusion Using Cross Attention. (arXiv:2210.04885v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.04885","description":"<p>Large-scale diffusion neural networks represent a substantial milestone in\ntext-to-image generation, with some performing similar to real photographs in\nhuman evaluation. However, they remain poorly understood, lacking\nexplainability and interpretability analyses, largely due to their proprietary,\nclosed-source nature. In this paper, to shine some much-needed light on\ntext-to-image diffusion models, we perform a text-image attribution analysis on\nStable Diffusion, a recently open-sourced large diffusion model. To produce\npixel-level attribution maps, we propose DAAM, a novel method based on\nupscaling and aggregating cross-attention activations in the latent denoising\nsubnetwork. We support its correctness by evaluating its unsupervised semantic\nsegmentation quality on its own generated imagery, compared to supervised\nsegmentation models. We show that DAAM performs strongly on COCO\ncaption-generated images, achieving an mIoU of 61.0, and it outperforms\nsupervised models on open-vocabulary segmentation, for an mIoU of 51.5. We\nfurther find that certain parts of speech, like punctuation and conjunctions,\ninfluence the generated imagery most, which agrees with the prior literature,\nwhile determiners and numerals the least, suggesting poor numeracy. To our\nknowledge, we are the first to propose and study word-pixel attribution for\nlarge-scale text-to-image diffusion models. Our code and data are at\nhttps://github.com/castorini/daam.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Raphael Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Akshat Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhiying Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Gefei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1\">Karun Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ture_F/0/1/0/all/0/1\">Ferhan Ture</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Label Errors in Token Classification Data. (arXiv:2210.03920v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2210.03920","description":"<p>Mislabeled examples are a common issue in real-world data, particularly for\ntasks like token classification where many labels must be chosen on a\nfine-grained basis. Here we consider the task of finding sentences that contain\nlabel errors in token classification datasets. We study 11 different\nstraightforward methods that score tokens/sentences based on the predicted\nclass probabilities output by a (any) token classification model (trained via\nany procedure). In precision-recall evaluations based on real-world label\nerrors in entity recognition data from CoNLL-2003, we identify a simple and\neffective method that consistently detects those sentences containing label\nerrors when applied with different token classification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei-Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Jonas Mueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}