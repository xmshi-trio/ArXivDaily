{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-08-10T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Changes in Policy Preferences in German Tweets during the COVID Pandemic. (arXiv:2308.04444v1 [cs.CY])","link":"http://arxiv.org/abs/2308.04444","description":"<p>Online social media have become an important forum for exchanging political\nopinions. In response to COVID measures citizens expressed their policy\npreferences directly on these platforms. Quantifying political preferences in\nonline social media remains challenging: The vast amount of content requires\nscalable automated extraction of political preferences -- however fine grained\npolitical preference extraction is difficult with current machine learning (ML)\ntechnology, due to the lack of data sets. Here we present a novel data set of\ntweets with fine grained political preference annotations. A text\nclassification model trained on this data is used to extract policy preferences\nin a German Twitter corpus ranging from 2019 to 2022. Our results indicate that\nin response to the COVID pandemic, expression of political opinions increased.\nUsing a well established taxonomy of policy preferences we analyse fine grained\npolitical views and highlight changes in distinct political categories. These\nanalyses suggest that the increase in policy preference expression is dominated\nby the categories pro-welfare, pro-education and pro-governmental\nadministration efficiency. All training data and code used in this study are\nmade publicly available to encourage other researchers to further improve\nautomated policy preference extraction methods. We hope that our findings\ncontribute to a better understanding of political statements in online social\nmedia and to a better assessment of how COVID measures impact political\npreferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biessmann_F/0/1/0/all/0/1\">Felix Biessmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogRE^C+: An Extension of DialogRE to Investigate How Much Coreference Helps Relation Extraction in Dialogs. (arXiv:2308.04498v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04498","description":"<p>Dialogue relation extraction (DRE) that identifies the relations between\nargument pairs in dialogue text, suffers much from the frequent occurrence of\npersonal pronouns, or entity and speaker coreference. This work introduces a\nnew benchmark dataset DialogRE^C+, introducing coreference resolution into the\nDRE scenario. With the aid of high-quality coreference knowledge, the reasoning\nof argument relations is expected to be enhanced. In DialogRE^C+ dataset, we\nmanually annotate total 5,068 coreference chains over 36,369 argument mentions\nbased on the existing DialogRE data, where four different coreference chain\ntypes namely speaker chain, person chain, location chain and organization chain\nare explicitly marked. We further develop 4 coreference-enhanced graph-based\nDRE models, which learn effective coreference representations for improving the\nDRE task. We also train a coreference resolution model based on our annotations\nand evaluate the effect of automatically extracted coreference chains\ndemonstrating the practicality of our dataset and its potential to other\ndomains and tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yiyun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1\">Mengwei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengqiong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1\">Chong Teng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Disentanglement and Fusion on Modality and Context in Conversational Multimodal Emotion Recognition. (arXiv:2308.04502v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04502","description":"<p>It has been a hot research topic to enable machines to understand human\nemotions in multimodal contexts under dialogue scenarios, which is tasked with\nmultimodal emotion analysis in conversation (MM-ERC). MM-ERC has received\nconsistent attention in recent years, where a diverse range of methods has been\nproposed for securing better task performance. Most existing works treat MM-ERC\nas a standard multimodal classification problem and perform multimodal feature\ndisentanglement and fusion for maximizing feature utility. Yet after revisiting\nthe characteristic of MM-ERC, we argue that both the feature multimodality and\nconversational contextualization should be properly modeled simultaneously\nduring the feature disentanglement and fusion steps. In this work, we target\nfurther pushing the task performance by taking full consideration of the above\ninsights. On the one hand, during feature disentanglement, based on the\ncontrastive learning technique, we devise a Dual-level Disentanglement\nMechanism (DDM) to decouple the features into both the modality space and\nutterance space. On the other hand, during the feature fusion stage, we propose\na Contribution-aware Fusion Mechanism (CFM) and a Context Refusion Mechanism\n(CRM) for multimodal and context integration, respectively. They together\nschedule the proper integrations of multimodal and context features.\nSpecifically, CFM explicitly manages the multimodal feature contributions\ndynamically, while CRM flexibly coordinates the introduction of dialogue\ncontexts. On two public MM-ERC datasets, our system achieves new\nstate-of-the-art performance consistently. Further analyses demonstrate that\nall our proposed mechanisms greatly facilitate the MM-ERC task by making full\nuse of the multimodal and context features adaptively. Note that our proposed\nmethods have the great potential to facilitate a broader range of other\nconversational multimodal tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lizi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1\">Chong Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Spectral and Long-term Contextual Information for Speech Emotion Recognition Using Deep Learning Techniques. (arXiv:2308.04517v1 [cs.SD])","link":"http://arxiv.org/abs/2308.04517","description":"<p>Traditional approaches in speech emotion recognition, such as LSTM, CNN, RNN,\nSVM, and MLP, have limitations such as difficulty capturing long-term\ndependencies in sequential data, capturing the temporal dynamics, and\nstruggling to capture complex patterns and relationships in multimodal data.\nThis research addresses these shortcomings by proposing an ensemble model that\ncombines Graph Convolutional Networks (GCN) for processing textual data and the\nHuBERT transformer for analyzing audio signals. We found that GCNs excel at\ncapturing Long-term contextual dependencies and relationships within textual\ndata by leveraging graph-based representations of text and thus detecting the\ncontextual meaning and semantic relationships between words. On the other hand,\nHuBERT utilizes self-attention mechanisms to capture long-range dependencies,\nenabling the modeling of temporal dynamics present in speech and capturing\nsubtle nuances and variations that contribute to emotion recognition. By\ncombining GCN and HuBERT, our ensemble model can leverage the strengths of both\napproaches. This allows for the simultaneous analysis of multimodal data, and\nthe fusion of these modalities enables the extraction of complementary\ninformation, enhancing the discriminative power of the emotion recognition\nsystem. The results indicate that the combined model can overcome the\nlimitations of traditional methods, leading to enhanced accuracy in recognizing\nemotions from speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Samiul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_M/0/1/0/all/0/1\">Md. Maksudul Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadat_A/0/1/0/all/0/1\">Abu Jobayer Md. Sadat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DisCoCat for Donkey Sentences. (arXiv:2308.04519v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04519","description":"<p>We demonstrate how to parse Geach's Donkey sentences in a compositional\ndistributional model of meaning. We build on previous work on the DisCoCat\n(Distributional Compositional Categorical) framework, including extensions that\nmodel discourse, determiners, and relative pronouns. We present a type-logical\nsyntax for parsing donkey sentences, for which we define both relational and\nvector space semantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McPheat_L/0/1/0/all/0/1\">Lachlan McPheat</a> (University College London), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daphne Wang</a> (University College London)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Who should I Collaborate with? A Comparative Study of Academia and Industry Research Collaboration in NLP. (arXiv:2308.04524v1 [cs.DL])","link":"http://arxiv.org/abs/2308.04524","description":"<p>The goal of our research was to investigate the effects of collaboration\nbetween academia and industry on Natural Language Processing (NLP). To do this,\nwe created a pipeline to extract affiliations and citations from NLP papers and\ndivided them into three categories: academia, industry, and hybrid\n(collaborations between academia and industry). Our empirical analysis found\nthat there is a trend towards an increase in industry and academia-industry\ncollaboration publications and that these types of publications tend to have a\nhigher impact compared to those produced solely within academia.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abuwala_H/0/1/0/all/0/1\">Hussain Sadiq Abuwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mushi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ahead of the Text: Leveraging Entity Preposition for Financial Relation Extraction. (arXiv:2308.04534v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04534","description":"<p>In the context of the ACM KDF-SIGIR 2023 competition, we undertook an entity\nrelation task on a dataset of financial entity relations called REFind. Our\ntop-performing solution involved a multi-step approach. Initially, we inserted\nthe provided entities at their corresponding locations within the text.\nSubsequently, we fine-tuned the transformer-based language model roberta-large\nfor text classification by utilizing a labeled training set to predict the\nentity relations. Lastly, we implemented a post-processing phase to identify\nand handle improbable predictions generated by the model. As a result of our\nmethodology, we achieved the 1st place ranking on the competition's public\nleaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasch_S/0/1/0/all/0/1\">Stefan Pasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petridis_D/0/1/0/all/0/1\">Dimitrios Petridis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-Sentence Reader: A Novel Approach for Addressing Answer Position Bias. (arXiv:2308.04566v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04566","description":"<p>Machine Reading Comprehension (MRC) models tend to take advantage of spurious\ncorrelations (also known as dataset bias or annotation artifacts in the\nresearch community). Consequently, these models may perform the MRC task\nwithout fully comprehending the given context and question, which is\nundesirable since it may result in low robustness against distribution shift.\nThis paper delves into the concept of answer-position bias, where a significant\npercentage of training questions have answers located solely in the first\nsentence of the context. We propose a Single-Sentence Reader as a new approach\nfor addressing answer position bias in MRC. We implement this approach using\nsix different models and thoroughly analyze their performance. Remarkably, our\nproposed Single-Sentence Readers achieve results that nearly match those of\nmodels trained on conventional training sets, proving their effectiveness. Our\nstudy also discusses several challenges our Single-Sentence Readers encounter\nand proposes a potential solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Quoc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kretchmar_M/0/1/0/all/0/1\">Matt Kretchmar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shepherd: A Critic for Language Model Generation. (arXiv:2308.04592v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04592","description":"<p>As large language models improve, there is increasing interest in techniques\nthat leverage these models' capabilities to refine their own outputs. In this\nwork, we introduce Shepherd, a language model specifically tuned to critique\nresponses and suggest refinements, extending beyond the capabilities of an\nuntuned model to identify diverse errors and provide suggestions to remedy\nthem. At the core of our approach is a high quality feedback dataset, which we\ncurate from community feedback and human annotations. Even though Shepherd is\nsmall (7B parameters), its critiques are either equivalent or preferred to\nthose from established models including ChatGPT. Using GPT-4 for evaluation,\nShepherd reaches an average win-rate of 53-87% compared to competitive\nalternatives. In human evaluation, Shepherd strictly outperforms other models\nand on average closely ties with ChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Ping Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiaoqing Ellen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_S/0/1/0/all/0/1\">Sean O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_Yu_J/0/1/0/all/0/1\">Jane Dwivedi-Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golovneva_O/0/1/0/all/0/1\">Olga Golovneva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazel_Zarandi_M/0/1/0/all/0/1\">Maryam Fazel-Zarandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating LLM Inference with Staged Speculative Decoding. (arXiv:2308.04623v1 [cs.AI])","link":"http://arxiv.org/abs/2308.04623","description":"<p>Recent advances with large language models (LLM) illustrate their diverse\ncapabilities. We propose a novel algorithm, staged speculative decoding, to\naccelerate LLM inference in small-batch, on-device scenarios. We address the\nlow arithmetic intensity of small-batch inference by improving upon previous\nwork in speculative decoding. First, we restructure the speculative batch as a\ntree, which reduces generation costs and increases the expected tokens per\nbatch. Second, we add a second stage of speculative decoding. Taken together,\nwe reduce single-batch decoding latency by 3.16x with a 762M parameter GPT-2-L\nmodel while perfectly preserving output quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spector_B/0/1/0/all/0/1\">Benjamin Spector</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Chris Re</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking LLM powered Chatbots: Methods and Metrics. (arXiv:2308.04624v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04624","description":"<p>Autonomous conversational agents, i.e. chatbots, are becoming an increasingly\ncommon mechanism for enterprises to provide support to customers and partners.\nIn order to rate chatbots, especially ones powered by Generative AI tools like\nLarge Language Models (LLMs) we need to be able to accurately assess their\nperformance. This is where chatbot benchmarking becomes important. In this\npaper, we propose the use of a novel benchmark that we call the E2E (End to\nEnd) benchmark, and show how the E2E benchmark can be used to evaluate accuracy\nand usefulness of the answers provided by chatbots, especially ones powered by\nLLMs. We evaluate an example chatbot at different levels of sophistication\nbased on both our E2E benchmark, as well as other available metrics commonly\nused in the state of art, and observe that the proposed benchmark show better\nresults compared to others. In addition, while some metrics proved to be\nunpredictable, the metric associated with the E2E benchmark, which uses cosine\nsimilarity performed well in evaluating chatbots. The performance of our best\nmodels shows that there are several benefits of using the cosine similarity\nscore as a metric in the E2E benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debarag Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Pooja Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avadhanam_A/0/1/0/all/0/1\">Arjun Avadhanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Saksham Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Sentence Embedding Models for Assessing Semantic Variation. (arXiv:2308.04625v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04625","description":"<p>Analyzing the pattern of semantic variation in long real-world texts such as\nbooks or transcripts is interesting from the stylistic, cognitive, and\nlinguistic perspectives. It is also useful for applications such as text\nsegmentation, document summarization, and detection of semantic novelty. The\nrecent emergence of several vector-space methods for sentence embedding has\nmade such analysis feasible. However, this raises the issue of how consistent\nand meaningful the semantic representations produced by various methods are in\nthemselves. In this paper, we compare several recent sentence embedding methods\nvia time-series of semantic similarity between successive sentences and\nmatrices of pairwise sentence similarity for multiple books of literature. In\ncontrast to previous work using target tasks and curated datasets to compare\nsentence embedding methods, our approach provides an evaluation of the methods\n'in the wild'. We find that most of the sentence embedding methods considered\ndo infer highly correlated patterns of semantic similarity in a given document,\nbut show interesting differences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mistry_D/0/1/0/all/0/1\">Deven M. Mistry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minai_A/0/1/0/all/0/1\">Ali A. Minai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Constituency Parsing for Middle High German: A Delexicalized Approach. (arXiv:2308.04645v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04645","description":"<p>Constituency parsing plays a fundamental role in advancing natural language\nprocessing (NLP) tasks. However, training an automatic syntactic analysis\nsystem for ancient languages solely relying on annotated parse data is a\nformidable task due to the inherent challenges in building treebanks for such\nlanguages. It demands extensive linguistic expertise, leading to a scarcity of\navailable resources. To overcome this hurdle, cross-lingual transfer techniques\nwhich require minimal or even no annotated data for low-resource target\nlanguages offer a promising solution. In this study, we focus on building a\nconstituency parser for $\\mathbf{M}$iddle $\\mathbf{H}$igh $\\mathbf{G}$erman\n$\\mathbf{MHG}$ under realistic conditions, where no annotated MHG treebank is\navailable for training. In our approach, we leverage the linguistic continuity\nand structural similarity between MHG and $\\mathbf{M}$odern $\\mathbf{G}$erman\n$\\mathbf{MG}$, along with the abundance of MG treebank resources. Specifically,\nby employing the $\\mathit{delexicalization}$ method, we train a constituency\nparser on MG parse datasets and perform cross-lingual transfer to MHG parsing.\nOur delexicalized constituency parser demonstrates remarkable performance on\nthe MHG test set, achieving an F1-score of 67.3%. It outperforms the best\nzero-shot cross-lingual baseline by a margin of 28.6% points. These encouraging\nresults underscore the practicality and potential for automatic syntactic\nanalysis in other ancient languages that face similar challenges as MHG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1\">Ercong Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_H/0/1/0/all/0/1\">Helmut Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sudowoodo: a Chinese Lyric Imitation System with Source Lyrics. (arXiv:2308.04665v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04665","description":"<p>Lyrics generation is a well-known application in natural language generation\nresearch, with several previous studies focusing on generating accurate lyrics\nusing precise control such as keywords, rhymes, etc. However, lyrics imitation,\nwhich involves writing new lyrics by imitating the style and content of the\nsource lyrics, remains a challenging task due to the lack of a parallel corpus.\nIn this paper, we introduce \\textbf{\\textit{Sudowoodo}}, a Chinese lyrics\nimitation system that can generate new lyrics based on the text of source\nlyrics. To address the issue of lacking a parallel training corpus for lyrics\nimitation, we propose a novel framework to construct a parallel corpus based on\na keyword-based lyrics model from source lyrics. Then the pairs \\textit{(new\nlyrics, source lyrics)} are used to train the lyrics imitation model. During\nthe inference process, we utilize a post-processing module to filter and rank\nthe generated lyrics, selecting the highest-quality ones. We incorporated audio\ninformation and aligned the lyrics with the audio to form the songs as a bonus.\nThe human evaluation results show that our framework can perform better lyric\nimitation. Meanwhile, the \\textit{Sudowoodo} system and demo video of the\nsystem is available at\n\\href{https://Sudowoodo.apps-hp.danlu.netease.com/}{Sudowoodo} and\n\\href{https://youtu.be/u5BBT_j1L5M}{https://youtu.be/u5BBT\\_j1L5M}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yongzhu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qihang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1\">Jiashu Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA. (arXiv:2308.04679v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04679","description":"<p>Large Language Models (LLMs) have shown outstanding performance across wide\nrange of downstream tasks. This competency is attributed to their substantial\nparameter size and pre-training on extensive corpus. Moreover, LLMs have\nexhibited enhanced reasoning capabilities in tackling complex reasoning tasks,\nowing to the utilization of a method named ``Chain-of-Thought (CoT)\nprompting''. This method is designed to generate intermediate reasoning steps\nthat guide the inference of the final answer. However, it is essential to\nhighlight that these advanced reasoning abilities appear to emerge in models\nwith a minimum of 10 billion parameters, thereby limiting its efficacy in\nsituations where computational resources are constrained. In this paper, we\ninvestigate the possibility of transferring the reasoning capabilities of LLMs\nto smaller models via knowledge distillation. Specifically, we propose Sci-CoT,\na two-stage framework that separates the processes of generating rationales and\ninferring answers. This method enables a more efficient use of rationales\nduring the answer inference stage, leading to improved performance on\nscientific question-answering tasks. Utilizing Sci-CoT, our 80-million\nparameter model is able to exceed the performance of BLOOM-176B in the ARC-Easy\ndataset under the few shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuhan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chenyou Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating News-Centric Crossword Puzzles As A Constraint Satisfaction and Optimization Problem. (arXiv:2308.04688v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04688","description":"<p>Crossword puzzles have traditionally served not only as entertainment but\nalso as an educational tool that can be used to acquire vocabulary and language\nproficiency. One strategy to enhance the educational purpose is\npersonalization, such as including more words on a particular topic. This paper\nfocuses on the case of encouraging people's interest in news and proposes a\nframework for automatically generating news-centric crossword puzzles. We\ndesigned possible scenarios and built a prototype as a constraint satisfaction\nand optimization problem, that is, containing as many news-derived words as\npossible. Our experiments reported the generation probabilities and time\nrequired under several conditions. The results showed that news-centric\ncrossword puzzles can be generated even with few news-derived words. We\nsummarize the current issues and future research directions through a\nqualitative evaluation of the prototype. This is the first proposal that a\nformulation of a constraint satisfaction and optimization problem can be\nbeneficial as an educational application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majima_K/0/1/0/all/0/1\">Kaito Majima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishihara_S/0/1/0/all/0/1\">Shotaro Ishihara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Open-Source Large Language Models, GPT-4 and Claude 2: Multiple-Choice Test Taking in Nephrology. (arXiv:2308.04709v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04709","description":"<p>In recent years, there have been significant breakthroughs in the field of\nnatural language processing, particularly with the development of large\nlanguage models (LLMs). These LLMs have showcased remarkable capabilities on\nvarious benchmarks. In the healthcare field, the exact role LLMs and other\nfuture AI models will play remains unclear. There is a potential for these\nmodels in the future to be used as part of adaptive physician training, medical\nco-pilot applications, and digital patient interaction scenarios. The ability\nof AI models to participate in medical training and patient care will depend in\npart on their mastery of the knowledge content of specific medical fields. This\nstudy investigated the medical knowledge capability of LLMs, specifically in\nthe context of internal medicine subspecialty multiple-choice test-taking\nability. We compared the performance of several open-source LLMs (Koala 7B,\nFalcon 7B, Stable-Vicuna 13B, and Orca Mini 13B), to GPT-4 and Claude 2 on\nmultiple-choice questions in the field of Nephrology. Nephrology was chosen as\nan example of a particularly conceptually complex subspecialty field within\ninternal medicine. The study was conducted to evaluate the ability of LLM\nmodels to provide correct answers to nephSAP (Nephrology Self-Assessment\nProgram) multiple-choice questions. The overall success of open-sourced LLMs in\nanswering the 858 nephSAP multiple-choice questions correctly was 17.1% -\n25.5%. In contrast, Claude 2 answered 54.4% of the questions correctly, whereas\nGPT-4 achieved a score of 73.3%. We show that current widely used open-sourced\nLLMs do poorly in their ability for zero-shot reasoning when compared to GPT-4\nand Claude 2. The findings of this study potentially have significant\nimplications for the future of subspecialty medical training and patient care.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sean Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_M/0/1/0/all/0/1\">Michael Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blum_L/0/1/0/all/0/1\">Lesley Blum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Andy Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_L/0/1/0/all/0/1\">Liyo Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scalzo_F/0/1/0/all/0/1\">Fabien Scalzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_I/0/1/0/all/0/1\">Ira Kurtz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answering Unseen Questions With Smaller Language\\\\Models Using Rationale Generation and Dense Retrieval. (arXiv:2308.04711v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04711","description":"<p>When provided with sufficient explanatory context, smaller Language Models\nhave been shown to exhibit strong reasoning ability on challenging short-answer\nquestion-answering tasks where the questions are unseen in training. We\nevaluate two methods for further improvement in this setting. Both methods\nfocus on combining rationales generated by a larger Language Model with longer\ncontexts created from a multi-hop dense retrieval system. The first method\n($\\textit{RR}$) involves training a Rationale Ranking model to score both\ngenerated rationales and retrieved contexts with respect to relevance and\ntruthfulness. We then use the scores to derive combined contexts from both\nknowledge sources using a number of combinatory strategies. For the second\nmethod ($\\textit{RATD}$) we train a smaller Reasoning model using\nretrieval-augmented training datasets such that it becomes proficient at\nutilising relevant information from longer text sequences that may be only\npartially evidential and frequently contain many irrelevant sentences.\nGenerally we find that both methods are effective but that the $\\textit{RATD}$\nmethod is more straightforward to apply and produces the strongest results in\nthe unseen setting on which we focus. Our single best Reasoning model using\nonly 440 million parameters materially improves upon strong comparable prior\nbaselines for unseen evaluation datasets (StrategyQA 58.9 $\\rightarrow$ 61.7\nacc., CommonsenseQA 63.6 $\\rightarrow$ 72.7 acc., ARC-DA 31.6 $\\rightarrow$\n52.1 F1, IIRC 25.5 $\\rightarrow$ 27.3 F1) and a version utilising our prior\nknowledge of each type of question in selecting a context combination strategy\ndoes even better. Our proposed models also generally outperform direct prompts\nagainst much larger models (BLOOM 175B and StableVicuna 13B) in both few-shot\nchain-of-thought and few-shot answer-only settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartill_T/0/1/0/all/0/1\">Tim Hartill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benavides_Prado_D/0/1/0/all/0/1\">Diana Benavides-Prado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1\">Michael Witbrock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riddle_P/0/1/0/all/0/1\">Patricia J. Riddle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning. (arXiv:2308.04712v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04712","description":"<p>Recent advanced methods in Natural Language Understanding for Task-oriented\nDialogue (TOD) Systems (e.g., intent detection and slot filling) require a\nlarge amount of annotated data to achieve competitive performance. In reality,\ntoken-level annotations (slot labels) are time-consuming and difficult to\nacquire. In this work, we study the Slot Induction (SI) task whose objective is\nto induce slot boundaries without explicit knowledge of token-level slot\nannotations. We propose leveraging Unsupervised Pre-trained Language Model\n(PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised\nsemantic knowledge extracted from PLM, and (2) additional sentence-level intent\nlabel signals available from TOD. Our approach is shown to be effective in SI\ntask and capable of bridging the gaps with token-level supervised models on two\nNLU benchmark datasets. When generalized to emerging intents, our SI objectives\nalso provide enhanced slot label representations, leading to improved\nperformance on the Slot Filling tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hoang H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Interpretable and Reliable Open Information Retriever for New Domains Overnight. (arXiv:2308.04756v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04756","description":"<p>Information retrieval (IR) or knowledge retrieval, is a critical component\nfor many down-stream tasks such as open-domain question answering (QA). It is\nalso very challenging, as it requires succinctness, completeness, and\ncorrectness. In recent works, dense retrieval models have achieved\nstate-of-the-art (SOTA) performance on in-domain IR and QA benchmarks by\nrepresenting queries and knowledge passages with dense vectors and learning the\nlexical and semantic similarity. However, using single dense vectors and\nend-to-end supervision are not always optimal because queries may require\nattention to multiple aspects and event implicit knowledge. In this work, we\npropose an information retrieval pipeline that uses entity/event linking model\nand query decomposition model to focus more accurately on different information\nunits of the query. We show that, while being more interpretable and reliable,\nour proposed pipeline significantly improves passage coverages and denotation\naccuracies across five IR and QA benchmarks. It will be the go-to system to use\nfor applications that need to perform IR on a new domain without much dedicated\neffort, because of its superior interpretability and cross-domain performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaodong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Ben Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatically measuring speech fluency in people with aphasia: first achievements using read-speech data. (arXiv:2308.04763v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04763","description":"<p>Background: Speech and language pathologists (SLPs) often relyon judgements\nof speech fluency for diagnosing or monitoringpatients with aphasia. However,\nsuch subjective methods havebeen criticised for their lack of reliability and\ntheir clinical cost interms of time. Aims: This study aims at assessing the\nrelevance of a signalprocessingalgorithm, initially developed in the field of\nlanguage acquisition, for the automatic measurement of speech fluency in people\nwith aphasia (PWA). Methods &amp; Procedures: Twenty-nine PWA and five control\nparticipantswere recruited via non-profit organizations and SLP networks. All\nparticipants were recorded while reading out loud a set ofsentences taken from\nthe French version of the Boston Diagnostic Aphasia Examination. Three trained\nSLPs assessed the fluency of each sentence on a five-point qualitative scale. A\nforward-backward divergence segmentation and a clustering algorithm were used\nto compute, for each sentence, four automatic predictors of speech fluency:\npseudo-syllable rate, speech ratio, rate of silent breaks, and standard\ndeviation of pseudo-syllable length. The four predictors were finally combined\ninto multivariate regression models (a multiplelinear regression - MLR, and two\nnon-linear models) to predict the average SLP ratings of speech fluency, using\na leave-one speaker-out validation scheme. Outcomes &amp; Results: All models\nachieved accurate predictions of speech fluency ratings, with average\nroot-mean-square errors as low as 0.5. The MLR yielded a correlation\ncoefficient of 0.87 with reference ratings at the sentence level, and of 0.93\nwhen aggregating the data for each participant. The inclusion of an additional\npredictor sensitive to repetitions improved further the predictions with a\ncorrelation coefficient of 0.91 at the sentence level, and of 0.96 at the\nparticipant level. Conclusions: The algorithms used in this study can\nconstitute a cost-effective and reliable tool for the assessment of the speech\nfluency of patients with aphasia in read-aloud tasks. Perspectives for the\nassessment of spontaneous speech are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fontan_L/0/1/0/all/0/1\">Lionel Fontan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prince_T/0/1/0/all/0/1\">Typhanie Prince</a> (Praxiling, LNPL), <a href=\"http://arxiv.org/find/cs/1/au:+Nowakowska_A/0/1/0/all/0/1\">Aleksandra Nowakowska</a> (Praxiling), <a href=\"http://arxiv.org/find/cs/1/au:+Sahraoui_H/0/1/0/all/0/1\">Halima Sahraoui</a> (LNPL), <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Ferreiro_S/0/1/0/all/0/1\">Silvia Martinez-Ferreiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADMUS: A Progressive Question Answering Framework Adaptable to Multiple Knowledge Sources. (arXiv:2308.04800v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04800","description":"<p>With the introduction of deep learning models, semantic parsingbased\nknowledge base question answering (KBQA) systems have achieved high performance\nin handling complex questions. However, most existing approaches primarily\nfocus on enhancing the model's effectiveness on individual benchmark datasets,\ndisregarding the high costs of adapting the system to disparate datasets in\nreal-world scenarios (e.g., multi-tenant platform). Therefore, we present\nADMUS, a progressive knowledge base question answering framework designed to\naccommodate a wide variety of datasets, including multiple languages, diverse\nbackbone knowledge bases, and disparate question answering datasets. To\naccomplish the purpose, we decouple the architecture of conventional KBQA\nsystems and propose this dataset-independent framework. Our framework supports\nthe seamless integration of new datasets with minimal effort, only requiring\ncreating a dataset-related micro-service at a negligible cost. To enhance the\nusability of ADUMS, we design a progressive framework consisting of three\nstages, ranges from executing exact queries, generating approximate queries and\nretrieving open-domain knowledge referring from large language models. An\nonline demonstration of ADUMS is available at:\nhttps://answer.gstore.cn/pc/index.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yirui Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanzeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Lei Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Bipartite Graph is All We Need for Enhancing Emotional Reasoning with Commonsense Knowledge. (arXiv:2308.04811v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04811","description":"<p>The context-aware emotional reasoning ability of AI systems, especially in\nconversations, is of vital importance in applications such as online opinion\nmining from social media and empathetic dialogue systems. Due to the implicit\nnature of conveying emotions in many scenarios, commonsense knowledge is widely\nutilized to enrich utterance semantics and enhance conversation modeling.\nHowever, most previous knowledge infusion methods perform empirical knowledge\nfiltering and design highly customized architectures for knowledge interaction\nwith the utterances, which can discard useful knowledge aspects and limit their\ngeneralizability to different knowledge sources. Based on these observations,\nwe propose a Bipartite Heterogeneous Graph (BHG) method for enhancing emotional\nreasoning with commonsense knowledge. In BHG, the extracted context-aware\nutterance representations and knowledge representations are modeled as\nheterogeneous nodes. Two more knowledge aggregation node types are proposed to\nperform automatic knowledge filtering and interaction. BHG-based knowledge\ninfusion can be directly generalized to multi-type and multi-grained knowledge\nsources. In addition, we propose a Multi-dimensional Heterogeneous Graph\nTransformer (MHGT) to perform graph reasoning, which can retain unchanged\nfeature spaces and unequal dimensions for heterogeneous node types during\ninference to prevent unnecessary loss of information. Experiments show that\nBHG-based methods significantly outperform state-of-the-art knowledge infusion\nmethods and show generalized knowledge infusion ability with higher efficiency.\nFurther analysis proves that previous empirical knowledge filtering methods do\nnot guarantee to provide the most useful knowledge information. Our code is\navailable at: https://github.com/SteveKGYang/BHG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLEVA: Chinese Language Models EVAluation Platform. (arXiv:2308.04813v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04813","description":"<p>With the continuous emergence of Chinese Large Language Models (LLMs), how to\nevaluate a model's capabilities has become an increasingly significant issue.\nThe absence of a comprehensive Chinese benchmark that thoroughly assesses a\nmodel's performance, the unstandardized and incomparable prompting procedure,\nand the prevalent risk of contamination pose major challenges in the current\nevaluation of Chinese LLMs. We present CLEVA, a user-friendly platform crafted\nto holistically evaluate Chinese LLMs. Our platform employs a standardized\nworkflow to assess LLMs' performance across various dimensions, regularly\nupdating a competitive leaderboard. To alleviate contamination, CLEVA curates a\nsignificant proportion of new data and develops a sampling strategy that\nguarantees a unique subset for each leaderboard round. Empowered by an\neasy-to-use interface that requires just a few mouse clicks and a model API,\nusers can conduct a thorough evaluation with minimal coding. Large-scale\nexperiments featuring 23 influential Chinese LLMs have validated CLEVA's\nefficacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianqiao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Duo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zi-Yuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xiaohui Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shijia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Generation Capabilities of Large Chinese Language Models. (arXiv:2308.04823v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04823","description":"<p>This paper presents CG-Eval, the first comprehensive evaluation of the\ngeneration capabilities of large Chinese language models across a wide range of\nacademic disciplines. The models' performance was assessed based on their\nability to generate accurate and relevant responses to different types of\nquestions in six disciplines, namely, Science and Engineering, Humanities and\nSocial Sciences, Mathematical Calculations, Medical Practitioner Qualification\nExamination, Judicial Examination, and Certified Public Accountant Examination.\nThis paper also presents Gscore, a composite index derived from the weighted\nsum of multiple metrics to measure the quality of model's generation against a\nreference. The test data and test results can be found at\n<a href=\"http://cgeval.besteasy.com/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hui Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jingyuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_M/0/1/0/all/0/1\">Meng Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_B/0/1/0/all/0/1\">Bin Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Na Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSSR: A Truncated and Signed Square Root Activation Function for Neural Networks. (arXiv:2308.04832v1 [cs.CV])","link":"http://arxiv.org/abs/2308.04832","description":"<p>Activation functions are essential components of neural networks. In this\npaper, we introduce a new activation function called the Truncated and Signed\nSquare Root (TSSR) function. This function is distinctive because it is odd,\nnonlinear, monotone and differentiable. Its gradient is continuous and always\npositive. Thanks to these properties, it has the potential to improve the\nnumerical stability of neural networks. Several experiments confirm that the\nproposed TSSR has better performance than other stat-of-the-art activation\nfunctions. The proposed function has significant implications for the\ndevelopment of neural network models and can be applied to a wide range of\napplications in fields such as computer vision, natural language processing,\nand speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yuanhao Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion-Conditioned Text Generation through Automatic Prompt Optimization. (arXiv:2308.04857v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04857","description":"<p>Conditional natural language generation methods often require either\nexpensive fine-tuning or training a large language model from scratch. Both are\nunlikely to lead to good results without a substantial amount of data and\ncomputational resources. Prompt learning without changing the parameters of a\nlarge language model presents a promising alternative. It is a cost-effective\napproach, while still achieving competitive results. While this procedure is\nnow established for zero- and few-shot text classification and structured\nprediction, it has received limited attention in conditional text generation.\nWe present the first automatic prompt optimization approach for\nemotion-conditioned text generation with instruction-fine-tuned models. Our\nmethod uses an iterative optimization procedure that changes the prompt by\nadding, removing, or replacing tokens. As objective function, we only require a\ntext classifier that measures the realization of the conditional variable in\nthe generated text. We evaluate the method on emotion-conditioned text\ngeneration with a focus on event reports and compare it to manually designed\nprompts that also act as the seed for the optimization procedure. The optimized\nprompts achieve 0.75 macro-average F1 to fulfill the emotion condition in\ncontrast to manually designed seed prompts with only 0.22 macro-average F1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Resendiz_Y/0/1/0/all/0/1\">Yarik Menchaca Resendiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information-Theoretic Characterization of Vowel Harmony: A Cross-Linguistic Study on Word Lists. (arXiv:2308.04885v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04885","description":"<p>We present a cross-linguistic study that aims to quantify vowel harmony using\ndata-driven computational modeling. Concretely, we define an\ninformation-theoretic measure of harmonicity based on the predictability of\nvowels in a natural language lexicon, which we estimate using phoneme-level\nlanguage models (PLMs). Prior quantitative studies have relied heavily on\ninflected word-forms in the analysis of vowel harmony. We instead train our\nmodels using cross-linguistically comparable lemma forms with little or no\ninflection, which enables us to cover more under-studied languages. Training\ndata for our PLMs consists of word lists with a maximum of 1000 entries per\nlanguage. Despite the fact that the data we employ are substantially smaller\nthan previously used corpora, our experiments demonstrate the neural PLMs\ncapture vowel harmony patterns in a set of languages that exhibit this\nphenomenon. Our work also demonstrates that word lists are a valuable resource\nfor typological research, and offers new possibilities for future studies on\nlow-resource, under-studied languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Steuer_J/0/1/0/all/0/1\">Julius Steuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_B/0/1/0/all/0/1\">Badr Abdullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+List_J/0/1/0/all/0/1\">Johann-Mattis List</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance. (arXiv:2308.04886v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04886","description":"<p>Dialect classification is used in a variety of applications, such as machine\ntranslation and speech recognition, to improve the overall performance of the\nsystem. In a real-world scenario, a deployed dialect classification model can\nencounter anomalous inputs that differ from the training data distribution,\nalso called out-of-distribution (OOD) samples. Those OOD samples can lead to\nunexpected outputs, as dialects of those samples are unseen during model\ntraining. Out-of-distribution detection is a new research area that has\nreceived little attention in the context of dialect classification. Towards\nthis, we proposed a simple yet effective unsupervised Mahalanobis distance\nfeature-based method to detect out-of-distribution samples. We utilize the\nlatent embeddings from all intermediate layers of a wav2vec 2.0\ntransformer-based dialect classifier model for multi-task learning. Our\nproposed approach outperforms other state-of-the-art OOD detection methods\nsignificantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sourya Dipta Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vadi_Y/0/1/0/all/0/1\">Yash Vadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unnam_A/0/1/0/all/0/1\">Abhishek Unnam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_K/0/1/0/all/0/1\">Kuldeep Yadav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following. (arXiv:2308.04913v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04913","description":"<p>E-commerce authoring involves creating attractive, abundant, and targeted\npromotional content to drive product sales. The emergence of large language\nmodels (LLMs) introduces an innovative paradigm, offering a unified solution to\naddress various authoring tasks within this scenario. However, mainstream LLMs\ntrained on general corpora with common sense knowledge reveal limitations in\nfitting complex and personalized features unique to e-commerce products and\ncustomers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility,\nraising concerns about safeguarding voluminous customer privacy data during\ntransmission. This paper proposes the LLaMA-E, the unified and customized\ninstruction-following language models focusing on diverse e-commerce authoring\ntasks. Specifically, the domain experts create the seed instruction set from\nthe tasks of ads generation, query-enhanced product title rewriting, product\nclassification, purchase intent speculation, and general Q&amp;A. These tasks\nenable the models to comprehensively understand precise e-commerce authoring\nknowledge by interleaving features covering typical service aspects of\ncustomers, sellers, and platforms. The GPT-3.5 is introduced as a teacher\nmodel, which expands the seed instructions to form a training set for the\nLLaMA-E models with various scales. The experimental results show that the\nproposed LLaMA-E models achieve state-of-the-art results in quantitative and\nqualitative evaluations, also exhibiting the advantage in zero-shot scenes. To\nthe best of our knowledge, this study is the first to serve the LLMs to\nspecific e-commerce authoring scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kaize Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xueyao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingxian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yinlin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guandong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating large language models and active inference to understand eye movements in reading and dyslexia. (arXiv:2308.04941v1 [q-bio.NC])","link":"http://arxiv.org/abs/2308.04941","description":"<p>We present a novel computational model employing hierarchical active\ninference to simulate reading and eye movements. The model characterizes\nlinguistic processing as inference over a hierarchical generative model,\nfacilitating predictions and inferences at various levels of granularity, from\nsyllables to sentences.\n</p>\n<p>Our approach combines the strengths of large language models for realistic\ntextual predictions and active inference for guiding eye movements to\ninformative textual information, enabling the testing of predictions. The model\nexhibits proficiency in reading both known and unknown words and sentences,\nadhering to the distinction between lexical and nonlexical routes in dual-route\ntheories of reading. Notably, our model permits the exploration of maladaptive\ninference effects on eye movements during reading, such as in dyslexia. To\nsimulate this condition, we attenuate the contribution of priors during the\nreading process, leading to incorrect inferences and a more fragmented reading\nstyle, characterized by a greater number of shorter saccades. This alignment\nwith empirical findings regarding eye movements in dyslexic individuals\nhighlights the model's potential to aid in understanding the cognitive\nprocesses underlying reading and eye movements, as well as how reading deficits\nassociated with dyslexia may emerge from maladaptive predictive processing.\n</p>\n<p>In summary, our model represents a significant advancement in comprehending\nthe intricate cognitive processes involved in reading and eye movements, with\npotential implications for understanding and addressing dyslexia through the\nsimulation of maladaptive inference. It may offer valuable insights into this\ncondition and contribute to the development of more effective interventions for\ntreatment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Donnarumma_F/0/1/0/all/0/1\">Francesco Donnarumma</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Frosolone_M/0/1/0/all/0/1\">Mirco Frosolone</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pezzulo_G/0/1/0/all/0/1\">Giovanni Pezzulo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking. (arXiv:2308.04945v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04945","description":"<p>The recent development and success of Large Language Models (LLMs)\nnecessitate an evaluation of their performance across diverse NLP tasks in\ndifferent languages. Although several frameworks have been developed and made\npublicly available, their customization capabilities for specific tasks and\ndatasets are often complex for different users. In this study, we introduce the\nLLMeBench framework. Initially developed to evaluate Arabic NLP tasks using\nOpenAI's GPT and BLOOM models; it can be seamlessly customized for any NLP task\nand model, regardless of language. The framework also features zero- and\nfew-shot learning settings. A new custom dataset can be added in less than 10\nminutes, and users can use their own model API keys to evaluate the task at\nhand. The developed framework has been already tested on 31 unique NLP tasks\nusing 53 publicly available datasets within 90 experimental setups, involving\napproximately 296K data points. We plan to open-source the framework for the\ncommunity (https://github.com/qcri/LLMeBench/). A video demonstrating the\nframework is available online (https://youtu.be/FkQn4UjYA0s).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanain_M/0/1/0/all/0/1\">Maram Hasanain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boughorbel_S/0/1/0/all/0/1\">Sabri Boughorbel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousi_B/0/1/0/all/0/1\">Basel Mousi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdaljalil_S/0/1/0/all/0/1\">Samir Abdaljalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazar_N/0/1/0/all/0/1\">Nizi Nazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawasly_M/0/1/0/all/0/1\">Majd Hawasly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extrapolating Large Language Models to Non-English by Aligning Languages. (arXiv:2308.04948v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04948","description":"<p>Due to the unbalanced training data distribution, the language ability of\nlarge language models (LLMs) is often biased towards English. In this paper, we\npropose to empower pre-trained LLMs on non-English languages by building\nsemantic alignment across languages. We perform instruction-tuning on LLaMA\nwith both translation task data and cross-lingual general task data to obtain\ncross-lingual models (x-LLaMA). Experiment results on cross-lingual benchmark\nXQUAD and MLQA show that x-LLaMA models outperform the English\ninstruction-tuned counterpart (Alpaca) by 42.50% on average on six non-English\nlanguages. Further experiments on Chinese benchmark C-Eval show that x-LLaMA\nachieves significant improvement on Chinese humanities tasks, outperforming\nAlpaca by 8.2%. We also discover that incorporating non-English text on the\ntarget side of translation data is particularly effective for boosting\nnon-English ability. Besides, we find that semantic alignment within LLM can be\nfurther strengthened as translation task data scales up and we present the\nformulation of the underlying scaling law. Evaluation results on translation\ndataset Flores-101 show that \\method outperforms previous LLaMA-based models in\nall evaluated directions. Code and data will be available at:\nhttps://github.com/OwenNJU/x-LLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yunzhe Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1\">Fei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Performance Analysis of Transformer Based Models (BERT, ALBERT and RoBERTa) in Fake News Detection. (arXiv:2308.04950v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04950","description":"<p>Fake news is fake material in a news media format but is not processed\nproperly by news agencies. The fake material can provoke or defame significant\nentities or individuals or potentially even for the personal interests of the\ncreators, causing problems for society. Distinguishing fake news and real news\nis challenging due to limited of domain knowledge and time constraints.\nAccording to the survey, the top three areas most exposed to hoaxes and\nmisinformation by residents are in Banten, DKI Jakarta and West Java. The model\nof transformers is referring to an approach in the field of artificial\nintelligence (AI) in natural language processing utilizing the deep learning\narchitectures. Transformers exercise a powerful attention mechanism to process\ntext in parallel and produce rich and contextual word representations. A\nprevious study indicates a superior performance of a transformer model known as\nBERT over and above non transformer approach. However, some studies suggest the\nperformance can be improved with the use of improved BERT models known as\nALBERT and RoBERTa. However, the modified BERT models are not well explored for\ndetecting fake news in Bahasa Indonesia. In this research, we explore those\ntransformer models and found that ALBERT outperformed other models with 87.6%\naccuracy, 86.9% precision, 86.9% F1-score, and 174.5 run-time (s/epoch)\nrespectively. Source code available at:\nhttps://github.com/Shafna81/fakenewsdetection.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azizah_S/0/1/0/all/0/1\">Shafna Fitria Nur Azizah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyono_H/0/1/0/all/0/1\">Hasan Dwi Cahyono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sihwi_S/0/1/0/all/0/1\">Sari Widya Sihwi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Widiarto_W/0/1/0/all/0/1\">Wisnu Widiarto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Multilingual Text Data Distillation. (arXiv:2308.04982v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04982","description":"<p>With the rise of deep learning, large datasets and complex models have become\ncommon, requiring significant computing power. To address this, data\ndistillation has emerged as a technique to quickly train models with lower\nmemory and time requirements. However, data distillation on text-based datasets\nhasn't been explored much because of the challenges rising due to its discrete\nnature. Additionally, existing dataset distillation methods often struggle to\ngeneralize to new architectures. In the paper, we propose several data\ndistillation techniques for multilingual text classification datasets using\nlanguage-model-based learning methods. We conduct experiments to analyze their\nperformance in terms of classification strength, and cross-architecture\ngeneralization. Furthermore, we investigate the language-specific fairness of\nthe data summaries generated by these methods. Our approach builds upon\nexisting techniques, enhancing cross-architecture generalization in the text\ndata distillation domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahni_S/0/1/0/all/0/1\">Shivam Sahni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1\">Harsh Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AspectMMKG: A Multi-modal Knowledge Graph with Aspect-aware Entities. (arXiv:2308.04992v1 [cs.CL])","link":"http://arxiv.org/abs/2308.04992","description":"<p>Multi-modal knowledge graphs (MMKGs) combine different modal data (e.g., text\nand image) for a comprehensive understanding of entities. Despite the recent\nprogress of large-scale MMKGs, existing MMKGs neglect the multi-aspect nature\nof entities, limiting the ability to comprehend entities from various\nperspectives. In this paper, we construct AspectMMKG, the first MMKG with\naspect-related images by matching images to different entity aspects.\nSpecifically, we collect aspect-related images from a knowledge base, and\nfurther extract aspect-related sentences from the knowledge base as queries to\nretrieve a large number of aspect-related images via an online image search\nengine. Finally, AspectMMKG contains 2,380 entities, 18,139 entity aspects, and\n645,383 aspect-related images. We demonstrate the usability of AspectMMKG in\nentity aspect linking (EAL) downstream task and show that previous EAL models\nachieve a new state-of-the-art performance with the help of AspectMMKG. To\nfacilitate the research on aspect-related MMKG, we further propose an\naspect-related image retrieval (AIR) model, that aims to correct and expand\naspect-related images in AspectMMKG. We train an AIR model to learn the\nrelationship between entity image and entity aspect-related images by\nincorporating entity image, aspect, and aspect image information. Experimental\nresults indicate that the AIR model could retrieve suitable images for a given\nentity w.r.t different aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingdan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaodan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction. (arXiv:2308.05046v1 [cs.CL])","link":"http://arxiv.org/abs/2308.05046","description":"<p>We present RadGraph2, a novel dataset for extracting information from\nradiology reports that focuses on capturing changes in disease state and device\nplacement over time. We introduce a hierarchical schema that organizes entities\nbased on their relationships and show that using this hierarchy during training\nimproves the performance of an information extraction model. Specifically, we\npropose a modification to the DyGIE++ framework, resulting in our model HGIE,\nwhich outperforms previous models in entity and relation extraction tasks. We\ndemonstrate that RadGraph2 enables models to capture a wider variety of\nfindings and perform better at relation extraction compared to those trained on\nthe original RadGraph dataset. Our work provides the foundation for developing\nautomated systems that can track disease progression over time and develop\ninformation extraction models that leverage the natural hierarchy of labels in\nthe medical domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanna_S/0/1/0/all/0/1\">Sameer Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dejl_A/0/1/0/all/0/1\">Adam Dejl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kibo Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_Q/0/1/0/all/0/1\">Quoc Hung Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_H/0/1/0/all/0/1\">Hanh Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenz_A/0/1/0/all/0/1\">Agustina Saenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Holistic Spatio-Temporal Scene Graph for Video Semantic Role Labeling. (arXiv:2308.05081v1 [cs.CV])","link":"http://arxiv.org/abs/2308.05081","description":"<p>Video Semantic Role Labeling (VidSRL) aims to detect the salient events from\ngiven videos, by recognizing the predict-argument event structures and the\ninterrelationships between events. While recent endeavors have put forth\nmethods for VidSRL, they can be mostly subject to two key drawbacks, including\nthe lack of fine-grained spatial scene perception and the insufficiently\nmodeling of video temporality. Towards this end, this work explores a novel\nholistic spatio-temporal scene graph (namely HostSG) representation based on\nthe existing dynamic scene graph structures, which well model both the\nfine-grained spatial semantics and temporal dynamics of videos for VidSRL.\nBuilt upon the HostSG, we present a nichetargeting VidSRL framework. A\nscene-event mapping mechanism is first designed to bridge the gap between the\nunderlying scene structure and the high-level event semantic structure,\nresulting in an overall hierarchical scene-event (termed ICE) graph structure.\nWe further perform iterative structure refinement to optimize the ICE graph,\nsuch that the overall structure representation can best coincide with end task\ndemand. Finally, three subtask predictions of VidSRL are jointly decoded, where\nthe end-to-end paradigm effectively avoids error propagation. On the benchmark\ndataset, our framework boosts significantly over the current best-performing\nmodel. Further analyses are shown for a better understanding of the advances of\nour methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jianguo Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Universal Question-Answering Platform for Knowledge Graphs. (arXiv:2303.00595v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2303.00595","description":"<p>Knowledge from diverse application domains is organized as knowledge graphs\n(KGs) that are stored in RDF engines accessible in the web via SPARQL\nendpoints. Expressing a well-formed SPARQL query requires information about the\ngraph structure and the exact URIs of its components, which is impractical for\nthe average user. Question answering (QA) systems assist by translating natural\nlanguage questions to SPARQL. Existing QA systems are typically based on\napplication-specific human-curated rules, or require prior information,\nexpensive pre-processing and model adaptation for each targeted KG. Therefore,\nthey are hard to generalize to a broad set of applications and KGs.\n</p>\n<p>In this paper, we propose KGQAn, a universal QA system that does not need to\nbe tailored to each target KG. Instead of curated rules, KGQAn introduces a\nnovel formalization of question understanding as a text generation problem to\nconvert a question into an intermediate abstract representation via a neural\nsequence-to-sequence model. We also develop a just-in-time linker that maps at\nquery time the abstract representation to a SPARQL query for a specific KG,\nusing only the publicly accessible APIs and the existing indices of the RDF\nstore, without requiring any pre-processing. Our experiments with several real\nKGs demonstrate that KGQAn is easily deployed and outperforms by a large margin\nthe state-of-the-art in terms of quality of answers and processing time,\nespecially for arbitrary KGs, unseen during the training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Omar_R/0/1/0/all/0/1\">Reham Omar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhall_I/0/1/0/all/0/1\">Ishika Dhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalnis_P/0/1/0/all/0/1\">Panos Kalnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_E/0/1/0/all/0/1\">Essam Mansour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference. (arXiv:2303.04673v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.04673","description":"<p>Large Language Models (LLMs) have sparked significant interest in their\ngenerative capabilities, leading to the development of various commercial\napplications. The high cost of using the models drives application builders to\nmaximize the value of generation under a limited inference budget. This paper\npresents a study of optimizing inference hyperparameters such as the number of\nresponses, temperature and max tokens, which significantly affects the\nutility/cost of text generation. We design a framework named EcoOptiGen which\nleverages economical hyperparameter optimization and cost-based pruning.\nExperiments with the GPT-3.5/GPT-4 models on a variety of tasks verify its\neffectiveness. EcoOptiGen is implemented in the `autogen' package of the FLAML\nlibrary: \\url{https://aka.ms/autogen}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Susan Xueqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed H. Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.16839","description":"<p>The development of language models have moved from encoder-decoder to\ndecoder-only designs. In addition, we observe that the two most popular\nmultimodal tasks, the generative and contrastive tasks, are nontrivial to\naccommodate in one architecture, and further need adaptations for downstream\ntasks. We propose a novel paradigm of training with a decoder-only model for\nmultimodal tasks, which is surprisingly effective in jointly learning of these\ndisparate vision-language tasks. This is done with a simple model, called\nMaMMUT. It consists of a single vision encoder and a text decoder, and is able\nto accommodate contrastive and generative learning by a novel two-pass approach\non the text decoder. We demonstrate that joint learning of these diverse\nobjectives is simple, effective, and maximizes the weight-sharing of the model\nacross these tasks. Furthermore, the same architecture enables straightforward\nextensions to open-vocabulary object detection and video-language tasks. The\nmodel tackles a diverse range of tasks, while being modest in capacity. Our\nmodel achieves the state of the art on image-text and text-image retrieval,\nvideo question answering and open-vocabulary detection tasks, outperforming\nmuch larger and more extensively trained foundational models. It shows very\ncompetitive results on VQA and Video Captioning, especially considering its\ncapacity. Ablations confirm the flexibility and advantages of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1\">Weicheng Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1\">AJ Piergiovanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1\">Ben Caine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogale_A/0/1/0/all/0/1\">Abhijit Ogale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Claire Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Prompt-based Multimodal Tabular Transformer Encoder For Medical Intervention Duration Estimation. (arXiv:2303.17408v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.17408","description":"<p>Objective: This study focuses on estimating the duration of medical\ninterventions using electronic health records (EHRs) in clinical decision\nsupport. Most existing models were designed for structured tabular data only\nand often suffer from data corruption problem. The unstructured clinical\nfree-text data that provides valuable insights and is more resistant to data\ncorruption is often overlooked. The objective of this research is to develop a\nmultimodal deep learning framework that integrates different data modalities\nfrom EHRs, thereby fully utilizing the predictive capability of EHRs for\nmedical intervention estimation.\n</p>\n<p>Materials and Methods: A novel prompt-based tabular transformer encoder\nframework is proposed for medical intervention duration estimation based on\nmultimodal EHR data. The framework leverages a pre-trained sentence encoder\nwith medical prompts to harmonize language representations of various clinical\ndata modalities, which a tabular transformer encoder is developed to further\nexplore.\n</p>\n<p>Results: The developed model demonstrates superior performance compared to\nthe baselines in two EHR datasets. Furthermore, the model exhibits resilience\nto data corruption in EHRs, with the RMSE curve increasing gradually with\nhigher corruption rates.\n</p>\n<p>Discussion: Other than the predictive effectiveness and robustness of the\nproposed framework, the ablation study highlights the significance of critical\ncomponents, such as medical prompts, free-text information, and the pre-trained\nsentence encoder, all contributing to the model's predictive ability.\n</p>\n<p>Conclusion: This research presents a promising pathway to enhance medical\nintervention estimation by incorporating diverse data modalities from language\nperspective, ultimately bolstering the reliability of deep learning models in\nclinical care.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1\">Yucheng Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xiang Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_D/0/1/0/all/0/1\">Daniel J. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_H/0/1/0/all/0/1\">Hairil Rizal Abdullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Mengling Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AttentionViz: A Global View of Transformer Attention. (arXiv:2305.03210v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2305.03210","description":"<p>Transformer models are revolutionizing machine learning, but their inner\nworkings remain mysterious. In this work, we present a new visualization\ntechnique designed to help researchers understand the self-attention mechanism\nin transformers that allows these models to learn rich, contextual\nrelationships between elements of a sequence. The main idea behind our method\nis to visualize a joint embedding of the query and key vectors used by\ntransformer models to compute attention. Unlike previous attention\nvisualization techniques, our approach enables the analysis of global patterns\nacross multiple input sequences. We create an interactive visualization tool,\nAttentionViz (demo: <a href=\"http://attentionviz.com\">this http URL</a>), based on these joint query-key\nembeddings, and use it to study attention mechanisms in both language and\nvision transformers. We demonstrate the utility of our approach in improving\nmodel understanding and offering new insights about query-key interactions\nthrough several application scenarios and expert feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Catherine Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yida Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Aoyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cynthia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viegas_F/0/1/0/all/0/1\">Fernanda Vi&#xe9;gas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1\">Martin Wattenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Word Dilution as Text Data Augmentation in Low-Resource Regime. (arXiv:2305.09287v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.09287","description":"<p>Data augmentation is widely used in text classification, especially in the\nlow-resource regime where a few examples for each class are available during\ntraining. Despite the success, generating data augmentations as hard positive\nexamples that may increase their effectiveness is under-explored. This paper\nproposes an Adversarial Word Dilution (AWD) method that can generate hard\npositive examples as text data augmentations to train the low-resource text\nclassification model efficiently. Our idea of augmenting the text data is to\ndilute the embedding of strong positive words by weighted mixing with\nunknown-word embedding, making the augmented inputs hard to be recognized as\npositive by the classification model. We adversarially learn the dilution\nweights through a constrained min-max optimization process with the guidance of\nthe labels. Empirical studies on three benchmark datasets show that AWD can\ngenerate more effective data augmentations and outperform the state-of-the-art\ntext data augmentation methods. The additional analysis demonstrates that the\ndata augmentations generated by AWD are interpretable and can flexibly extend\nto new examples without further training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junfan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zheyan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chunming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yongyi Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowing-how & Knowing-that: A New Task for Machine Comprehension of User Manuals. (arXiv:2306.04187v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.04187","description":"<p>The machine reading comprehension (MRC) of user manuals has huge potential in\ncustomer service. However, current methods have trouble answering complex\nquestions. Therefore, we introduce the Knowing-how &amp; Knowing-that task that\nrequires the model to answer factoid-style, procedure-style, and inconsistent\nquestions about user manuals. We resolve this task by jointly representing the\nsteps and facts in a graph TARA, which supports a unified inference of various\nquestions. Towards a systematical benchmarking study, we design a heuristic\nmethod to automatically parse user manuals into TARAs and build an annotated\ndataset to test the model's ability in answering real-world questions.\nEmpirical results demonstrate that representing user manuals as TARAs is a\ndesired solution for the MRC of user manuals. An in-depth investigation of TARA\nfurther sheds light on the issues and broader impacts of future representations\nof user manuals. We hope our work can move the MRC of user manuals to a more\ncomplex and realistic stage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hongru Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Weihong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Dingnan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zujie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Go Beyond The Obvious: Probing the gap of INFORMAL reasoning ability between Humanity and LLMs by Detective Reasoning Puzzle Benchmark. (arXiv:2307.05113v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.05113","description":"<p>Informal reasoning ability is the ability to reason based on common sense,\nexperience, and intuition.Humans use informal reasoning every day to extract\nthe most influential elements for their decision-making from a large amount of\nlife-like information.With the rapid development of language models, the\nrealization of general artificial intelligence has emerged with hope. Given the\noutstanding informal reasoning ability of humans, how much informal reasoning\nability language models have has not been well studied by scholars.In order to\nexplore the gap between humans and language models in informal reasoning\nability, this paper constructs a Detective Reasoning Benchmark, which is an\nassembly of 1,200 questions gathered from accessible online resources, aims at\nevaluating the model's informal reasoning ability in real-life\ncontext.Considering the improvement of the model's informal reasoning ability\nrestricted by the lack of benchmark, we further propose a Self-Question Prompt\nFramework that mimics human thinking to enhance the model's informal reasoning\nability.The goals of self-question are to find key elements, deeply investigate\nthe connections between these elements, encourage the relationship between each\nelement and the problem, and finally, require the model to reasonably answer\nthe problem.The experimental results show that human performance greatly\noutperforms the SoTA Language Models in Detective Reasoning Benchmark.Besides,\nSelf-Question is proven to be the most effective prompt engineering in\nimproving GPT-4's informal reasoning ability, but it still does not even\nsurpass the lowest score made by human participants.Upon acceptance of the\npaper, the source code for the benchmark will be made publicly accessible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhouhon Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zihan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhuozhi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Haoning Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yikai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Sihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shusen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Hongwei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models. (arXiv:2307.06713v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.06713","description":"<p>A wide variety of natural language tasks are currently being addressed with\nlarge-scale language models (LLMs). These models are usually trained with a\nvery large amount of unsupervised text data and adapted to perform a downstream\nnatural language task using methods like fine-tuning, calibration or in-context\nlearning. In this work, we propose an approach to adapt the prior class\ndistribution to perform text classification tasks without the need for labelled\nsamples and only few in-domain sample queries. The proposed approach treats the\nLLM as a black box, adding a stage where the model posteriors are calibrated to\nthe task. Results show that these methods outperform the un-adapted model for\ndifferent number of training shots in the prompt and a previous approach were\ncalibration is performed without using any adaptation data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Estienne_L/0/1/0/all/0/1\">Lautaro Estienne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1\">Luciana Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_M/0/1/0/all/0/1\">Mat&#xed;as Vera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2307.06775","description":"<p>Over the last decade, there has been a vast increase in eating disorder\ndiagnoses and eating disorder-attributed deaths, reaching their zenith during\nthe Covid-19 pandemic. This immense growth derived in part from the stressors\nof the pandemic but also from increased exposure to social media, which is rife\nwith content that promotes eating disorders. This study aimed to create a\nmultimodal deep learning model that can determine if a given social media post\npromotes eating disorders based on a combination of visual and textual data. A\nlabeled dataset of Tweets was collected from Twitter, upon which twelve deep\nlearning models were trained and tested. Based on model performance, the most\neffective deep learning model was the multimodal fusion of the RoBERTa natural\nlanguage processing model and the MaxViT image classification model, attaining\naccuracy and F1 scores of 95.9% and 0.959, respectively. The RoBERTa and MaxViT\nfusion model, deployed to classify an unlabeled dataset of posts from the\nsocial media sites Tumblr and Reddit, generated results akin to those of\nprevious research studies that did not employ artificial intelligence-based\ntechniques, indicating that deep learning models can develop insights congruent\nto those of researchers. Additionally, the model was used to conduct a\ntimeseries analysis of yet unseen Tweets from eight Twitter hashtags,\nuncovering that, since 2014, the relative abundance of content that promotes\neating disorders has decreased drastically within those communities. Despite\nthis reduction, by 2018, content that promotes eating disorders had either\nstopped declining or increased in ampleness anew on these hashtags.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feldman_J/0/1/0/all/0/1\">Jonathan Feldman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoHint: Automatic Prompt Optimization with Hint Generation. (arXiv:2307.07415v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.07415","description":"<p>This paper presents AutoHint, a novel framework for automatic prompt\nengineering and optimization for Large Language Models (LLM). While LLMs have\ndemonstrated remarkable ability in achieving high-quality annotation in various\ntasks, the key to applying this ability to specific tasks lies in developing\nhigh-quality prompts. Thus we propose a framework to inherit the merits of both\nin-context learning and zero-shot learning by incorporating enriched\ninstructions derived from input-output demonstrations to optimize original\nprompt. We refer to the enrichment as the hint and propose a framework to\nautomatically generate the hint from labeled data. More concretely, starting\nfrom an initial prompt, our method first instructs a LLM to deduce new hints\nfor selected samples from incorrect predictions, and then summarizes from\nper-sample hints and adds the results back to the initial prompt to form a new,\nenriched instruction. The proposed method is evaluated on the BIG-Bench\nInstruction Induction dataset for both zero-shot and few-short prompts, where\nexperiments demonstrate our method is able to significantly boost accuracy for\nmultiple tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinchuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homma_Y/0/1/0/all/0/1\">Youkow Homma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charles_D/0/1/0/all/0/1\">Denis Charles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retentive Network: A Successor to Transformer for Large Language Models. (arXiv:2307.08621v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.08621","description":"<p>In this work, we propose Retentive Network (RetNet) as a foundation\narchitecture for large language models, simultaneously achieving training\nparallelism, low-cost inference, and good performance. We theoretically derive\nthe connection between recurrence and attention. Then we propose the retention\nmechanism for sequence modeling, which supports three computation paradigms,\ni.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel\nrepresentation allows for training parallelism. The recurrent representation\nenables low-cost $O(1)$ inference, which improves decoding throughput, latency,\nand GPU memory without sacrificing performance. The chunkwise recurrent\nrepresentation facilitates efficient long-sequence modeling with linear\ncomplexity, where each chunk is encoded parallelly while recurrently\nsummarizing the chunks. Experimental results on language modeling show that\nRetNet achieves favorable scaling results, parallel training, low-cost\ndeployment, and efficient inference. The intriguing properties make RetNet a\nstrong successor to Transformer for large language models. Code will be\navailable at https://aka.ms/retnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yutao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuqing Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jilong Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.12267","description":"<p>The recent large language models (LLMs), e.g., ChatGPT, have been able to\ngenerate human-like and fluent responses when provided with specific\ninstructions. While admitting the convenience brought by technological\nadvancement, educators also have concerns that students might leverage LLMs to\ncomplete their writing assignments and pass them off as their original work.\nAlthough many AI content detection studies have been conducted as a result of\nsuch concerns, most of these prior studies modeled AI content detection as a\nclassification problem, assuming that a text is either entirely human-written\nor entirely AI-generated. In this study, we investigated AI content detection\nin a rarely explored yet realistic setting where the text to be detected is\ncollaboratively written by human and generative LLMs (i.e., hybrid text). We\nfirst formalized the detection task as identifying the transition points\nbetween human-written content and AI-generated content from a given hybrid text\n(boundary detection). Then we proposed a two-step approach where we (1)\nseparated AI-generated content from human-written content during the encoder\ntraining process; and (2) calculated the distances between every two adjacent\nprototypes and assumed that the boundaries exist between the two adjacent\nprototypes that have the furthest distance from each other. Through extensive\nexperiments, we observed the following main findings: (1) the proposed approach\nconsistently outperformed the baseline methods across different experiment\nsettings; (2) the encoder training process can significantly boost the\nperformance of the proposed approach; (3) when detecting boundaries for\nsingle-boundary hybrid essays, the proposed approach could be enhanced by\nadopting a relatively large prototype size, leading to a 22% improvement in the\nIn-Domain evaluation and an 18% improvement in the Out-of-Domain evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zijie Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1\">Lele Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaixun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasevic_D/0/1/0/all/0/1\">Dragan Ga&#x161;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanliang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.02582","description":"<p>Cross-domain and cross-compositional generalization of Text-to-SQL semantic\nparsing is a challenging task. Existing Large Language Model (LLM) based\nsolutions rely on inference-time retrieval of few-shot exemplars from the\ntraining set to synthesize a run-time prompt for each Natural Language (NL)\ntest query. In contrast, we devise an algorithm which performs offline sampling\nof a minimal set-of few-shots from the training data, with complete coverage of\nSQL clauses, operators and functions, and maximal domain coverage within the\nallowed token length. This allows for synthesis of a fixed Generic Prompt (GP),\nwith a diverse set-of exemplars common across NL test queries, avoiding\nexpensive test time exemplar retrieval. We further auto-adapt the GP to the\ntarget database domain (DA-GP), to better handle cross-domain generalization;\nfollowed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle\ncross-compositional generalization. The synthesis of LTMP-DA-GP is an offline\ntask, to be performed one-time per new database with minimal human\nintervention. Our approach demonstrates superior performance on the KaggleDBQA\ndataset, designed to evaluate generalizability for the Text-to-SQL task. We\nfurther showcase consistent performance improvement of LTMP-DA-GP over GP,\nacross LLMs and databases of KaggleDBQA, highlighting the efficacy and model\nagnostic benefits of our prompt based adapt and decompose approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Aseem Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaisaheb_S/0/1/0/all/0/1\">Shabbirhussain Bhaisaheb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nigam_H/0/1/0/all/0/1\">Harshit Nigam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwardhan_M/0/1/0/all/0/1\">Manasi Patwardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_L/0/1/0/all/0/1\">Lovekesh Vig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shroff_G/0/1/0/all/0/1\">Gautam Shroff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Multiple References Era -- Addressing Data Leakage and Limited Reference Diversity in NLG Evaluation. (arXiv:2308.03131v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.03131","description":"<p>N-gram matching-based evaluation metrics, such as BLEU and chrF, are widely\nutilized across a range of natural language generation (NLG) tasks. However,\nrecent studies have revealed a weak correlation between these matching-based\nmetrics and human evaluations, especially when compared with neural-based\nmetrics like BLEURT. In this paper, we conjecture that the performance\nbottleneck in matching-based metrics may be caused by the limited diversity of\nreferences. To address this issue, we propose to utilize \\textit{multiple\nreferences} to enhance the consistency between these metrics and human\nevaluations. Within the WMT Metrics benchmarks, we observe that the\nmulti-references F200spBLEU surpasses the conventional single-reference one by\nan accuracy improvement of 7.2\\%. Remarkably, it also exceeds the neural-based\nBERTscore by an accuracy enhancement of 3.9\\%. Moreover, we observe that the\ndata leakage issue in large language models (LLMs) can be mitigated to a large\nextent by our multi-reference metric. We release the code and data at\n\\url{https://github.com/SefaZeng/LLM-Ref}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xianfeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpinionConv: Conversational Product Search with Grounded Opinions. (arXiv:2308.04226v1 [cs.HC] CROSS LISTED)","link":"http://arxiv.org/abs/2308.04226","description":"<p>When searching for products, the opinions of others play an important role in\nmaking informed decisions. Subjective experiences about a product can be a\nvaluable source of information. This is also true in sales conversations, where\na customer and a sales assistant exchange facts and opinions about products.\nHowever, training an AI for such conversations is complicated by the fact that\nlanguage models do not possess authentic opinions for their lack of real-world\nexperience. We address this problem by leveraging product reviews as a rich\nsource of product opinions to ground conversational AI in true subjective\nnarratives. With OpinionConv, we develop the first conversational AI for\nsimulating sales conversations. To validate the generated conversations, we\nconduct several user studies showing that the generated opinions are perceived\nas realistic. Our assessors also confirm the importance of opinions as an\ninformative basis for decision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Javadi_V/0/1/0/all/0/1\">Vahid Sadiri Javadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1\">Lucie Flek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-08-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}