{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-12-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Labrador: Exploring the Limits of Masked Language Modeling for Laboratory Data. (arXiv:2312.11502v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11502","description":"<p>In this work we introduce Labrador, a pre-trained Transformer model for\nlaboratory data. Labrador and BERT were pre-trained on a corpus of 100 million\nlab test results from electronic health records (EHRs) and evaluated on various\ndownstream outcome prediction tasks. Both models demonstrate mastery of the\npre-training task but neither consistently outperform XGBoost on downstream\nsupervised tasks. Our ablation studies reveal that transfer learning shows\nlimited effectiveness for BERT and achieves marginal success with Labrador. We\nexplore the reasons for the failure of transfer learning and suggest that the\ndata generating process underlying each patient cannot be characterized\nsufficiently using labs alone, among other factors. We encourage future work to\nfocus on joint modeling of multiple EHR data categories and to include\ntree-based baselines in their evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bellamy_D/0/1/0/all/0/1\">David R. Bellamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_B/0/1/0/all/0/1\">Bhawesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cindy Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beam_A/0/1/0/all/0/1\">Andrew Beam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech and Text-Based Emotion Recognizer. (arXiv:2312.11503v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11503","description":"<p>Affective computing is a field of study that focuses on developing systems\nand technologies that can understand, interpret, and respond to human emotions.\nSpeech Emotion Recognition (SER), in particular, has got a lot of attention\nfrom researchers in the recent past. However, in many cases, the publicly\navailable datasets, used for training and evaluation, are scarce and imbalanced\nacross the emotion labels. In this work, we focused on building a balanced\ncorpus from these publicly available datasets by combining these datasets as\nwell as employing various speech data augmentation techniques. Furthermore, we\nexperimented with different architectures for speech emotion recognition. Our\nbest system, a multi-modal speech, and text-based model, provides a performance\nof UA(Unweighed Accuracy) + WA (Weighed Accuracy) of 157.57 compared to the\nbaseline algorithm performance of 119.66\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1\">Varun Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The performance of multiple language models in identifying offensive language on social media. (arXiv:2312.11504v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11504","description":"<p>Text classification is an important topic in the field of natural language\nprocessing. It has been preliminarily applied in information retrieval, digital\nlibrary, automatic abstracting, text filtering, word semantic discrimination\nand many other fields. The aim of this research is to use a variety of\nalgorithms to test the ability to identify offensive posts and evaluate their\nperformance against a variety of assessment methods. The motivation for this\nproject is to reduce the harm of these languages to human censors by automating\nthe screening of offending posts. The field is a new one, and despite much\ninterest in the past two years, there has been no focus on the object of the\noffence. Through the experiment of this project, it should inspire future\nresearch on identification methods as well as identification content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_B/0/1/0/all/0/1\">Brandon Bennett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variety and Quality over Quantity: Towards Versatile Instruction Curation. (arXiv:2312.11508v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11508","description":"<p>Instruction fine-tuning, involving the refinement of pre-trained LLMs using\ndatasets accompanied by natural instructions, is a powerful approach. However,\nits effectiveness is hindered by the redundancy and deficiencies in\nLLM-generated instruction datasets. In this paper, we introduce a highly\neffective and versatile paradigm for selecting diverse and high-quality\ninstruction-following data from fine-tuning datasets. We first employ the\ndataset enhancement and expansion to augment the dataset with more diverse and\nhigh-quality data, then we apply variety compression and quality compression\nsequentially to curate the desired dataset. Our experimental results showcase\nthat, even with a limited quantity of high-quality instruction data, LLMs\nconsistently maintain robust performance across both natural language\nunderstanding tasks and code generation tasks. Notably, they outperform models\ntrained on significantly larger instruction datasets in certain instances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yongqiang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1\">Mengnan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Maoquan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1\">Bin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency. (arXiv:2312.11509v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11509","description":"<p>We propose a Reinforcement-Learning-based system that would automatically\nprescribe a hypothetical patient medications that may help the patient with\ntheir mental-health-related speech disfluency, and adjust the medication and\nthe dosages in response to data from the patient. We demonstrate the components\nof the system: a module that detects and evaluates speech disfluency on a large\ndataset we built, and a Reinforcement Learning algorithm that automatically\nfinds good combinations of medications. To support the two modules, we collect\ndata on the effect of psychiatric medications for speech disfluency from the\nliterature, and build a plausible patient simulation system. We demonstrate\nthat the Reinforcement Learning system is, under some circumstances, able to\nconverge to a good medication regime. We collect and label a dataset of people\nwith possible speech disfluency and demonstrate our methods using that dataset.\nOur work is a proof of concept: we show that there is promise in the idea of\nusing automatic data collection to address disfluency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Constas_P/0/1/0/all/0/1\">Pavlos Constas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawal_V/0/1/0/all/0/1\">Vikram Rawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_M/0/1/0/all/0/1\">Matthew Honorio Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constas_A/0/1/0/all/0/1\">Andreas Constas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Aditya Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_K/0/1/0/all/0/1\">Kaison Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultani_N/0/1/0/all/0/1\">Najma Sultani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Carrie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altomare_M/0/1/0/all/0/1\">Micol Altomare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akzam_M/0/1/0/all/0/1\">Michael Akzam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiacheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_V/0/1/0/all/0/1\">Vhea He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altomare_L/0/1/0/all/0/1\">Lauren Altomare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murqi_H/0/1/0/all/0/1\">Heraa Murqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Asad Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanshali_N/0/1/0/all/0/1\">Nimit Amikumar Bhanshali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rachad_Y/0/1/0/all/0/1\">Youssef Rachad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerzhoy_M/0/1/0/all/0/1\">Michael Guerzhoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity. (arXiv:2312.11511v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11511","description":"<p>We present ComplexityNet, a streamlined language model designed for assessing\ntask complexity. This model predicts the likelihood of accurate output by\nvarious language models, each with different capabilities. Our initial\napplication of ComplexityNet involves the Mostly Basic Python Problems (MBPP)\ndataset. We pioneered the creation of the first set of labels to define task\ncomplexity. ComplexityNet achieved a notable 79% accuracy in determining task\ncomplexity, a significant improvement over the 34% accuracy of the original,\nnon fine-tuned model. Furthermore, ComplexityNet effectively reduces\ncomputational resource usage by 90% compared to using the highest complexity\nmodel, while maintaining a high code generation accuracy of 86.7%. This study\ndemonstrates that fine-tuning smaller models to categorize tasks based on their\ncomplexity can lead to a more balanced trade-off between accuracy and\nefficiency in the use of Large Language Models. Our findings suggest a\npromising direction for optimizing LLM applications, especially in\nresource-constrained environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bae_H/0/1/0/all/0/1\">Henry Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deeb_A/0/1/0/all/0/1\">Aghyad Deeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleury_A/0/1/0/all/0/1\">Alex Fleury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kehang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory. (arXiv:2312.11514v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11514","description":"<p>Large language models (LLMs) are central to modern natural language\nprocessing, delivering exceptional performance in various tasks. However, their\nintensive computational and memory requirements present challenges, especially\nfor devices with limited DRAM capacity. This paper tackles the challenge of\nefficiently running LLMs that exceed the available DRAM capacity by storing the\nmodel parameters on flash memory but bringing them on demand to DRAM. Our\nmethod involves constructing an inference cost model that harmonizes with the\nflash memory behavior, guiding us to optimize in two critical areas: reducing\nthe volume of data transferred from flash and reading data in larger, more\ncontiguous chunks. Within this flash memory-informed framework, we introduce\ntwo principal techniques. First, \"windowing'\" strategically reduces data\ntransfer by reusing previously activated neurons, and second, \"row-column\nbundling\", tailored to the sequential data access strengths of flash memory,\nincreases the size of data chunks read from flash memory. These methods\ncollectively enable running models up to twice the size of the available DRAM,\nwith a 4-5x and 20-25x increase in inference speed compared to naive loading\napproaches in CPU and GPU, respectively. Our integration of sparsity awareness,\ncontext-adaptive loading, and a hardware-oriented design paves the way for\neffective inference of LLMs on devices with limited memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alizadeh_K/0/1/0/all/0/1\">Keivan Alizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzadeh_I/0/1/0/all/0/1\">Iman Mirzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belenko_D/0/1/0/all/0/1\">Dmitry Belenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khatamifard_K/0/1/0/all/0/1\">Karen Khatamifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mundo_C/0/1/0/all/0/1\">Carlo C Del Mundo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1\">Mehrdad Farajtabar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking Musculoskeletal Disorder Risk Factors: NLP-Based Classification and Mode-Based Ranking. (arXiv:2312.11517v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11517","description":"<p>This research delves into the intricate landscape of Musculoskeletal Disorder\n(MSD) risk factors, employing a novel fusion of Natural Language Processing\n(NLP) techniques and mode-based ranking methodologies. The primary objective is\nto advance the comprehension of MSD risk factors, their classification, and\ntheir relative severity, facilitating more targeted preventive and management\ninterventions. The study utilizes eight diverse models, integrating pre-trained\ntransformers, cosine similarity, and various distance metrics to classify risk\nfactors into personal, biomechanical, workplace, psychological, and\norganizational classes. Key findings reveal that the BERT model with cosine\nsimilarity attains an overall accuracy of 28\\%, while the sentence transformer,\ncoupled with Euclidean, Bray-Curtis, and Minkowski distances, achieves a\nflawless accuracy score of 100\\%. In tandem with the classification efforts,\nthe research employs a mode-based ranking approach on survey data to discern\nthe severity hierarchy of MSD risk factors. Intriguingly, the rankings align\nprecisely with the previous literature, reaffirming the consistency and\nreliability of the approach. ``Working posture\" emerges as the most severe risk\nfactor, emphasizing the critical role of proper posture in preventing MSDs. The\ncollective perceptions of survey participants underscore the significance of\nfactors like ``Job insecurity,\" ``Effort reward imbalance,\" and ``Poor employee\nfacility\" in contributing to MSD risks. The convergence of rankings provides\nactionable insights for organizations aiming to reduce the prevalence of MSDs.\nThe study concludes with implications for targeted interventions,\nrecommendations for improving workplace conditions, and avenues for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahin_M/0/1/0/all/0/1\">Md Abrar Jahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talapatra_S/0/1/0/all/0/1\">Subrata Talapatra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User Modeling in the Era of Large Language Models: Current Research and Future Directions. (arXiv:2312.11518v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11518","description":"<p>User modeling (UM) aims to discover patterns or learn representations from\nuser data about the characteristics of a specific user, such as profile,\npreference, and personality. The user models enable personalization and\nsuspiciousness detection in many online applications such as recommendation,\neducation, and healthcare. Two common types of user data are text and graph, as\nthe data usually contain a large amount of user-generated content (UGC) and\nonline interactions. The research of text and graph mining is developing\nrapidly, contributing many notable solutions in the past two decades. Recently,\nlarge language models (LLMs) have shown superior performance on generating,\nunderstanding, and even reasoning over text data. The approaches of user\nmodeling have been equipped with LLMs and soon become outstanding. This article\nsummarizes existing research about how and why LLMs are great tools of modeling\nand understanding UGC. Then it reviews a few categories of large language\nmodels for user modeling (LLM-UM) approaches that integrate the LLMs with text\nand graph-based methods in different ways. Then it introduces specific LLM-UM\ntechniques for a variety of UM applications. Finally, it presents remaining\nchallenges and future directions in the LLM-UM research. We maintain the\nreading list at: https://github.com/TamSiuhin/LLM-UM-Reading\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhaoxuan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Complex Table Parsers. (arXiv:2312.11521v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11521","description":"<p>With the Generative Pre-trained Transformer 3.5 (GPT-3.5) exhibiting\nremarkable reasoning and comprehension abilities in Natural Language Processing\n(NLP), most Question Answering (QA) research has primarily centered around\ngeneral QA tasks based on GPT, neglecting the specific challenges posed by\nComplex Table QA. In this paper, we propose to incorporate GPT-3.5 to address\nsuch challenges, in which complex tables are reconstructed into tuples and\nspecific prompt designs are employed for dialogues. Specifically, we encode\neach cell's hierarchical structure, position information, and content as a\ntuple. By enhancing the prompt template with an explanatory description of the\nmeaning of each tuple and the logical reasoning process of the task, we\neffectively improve the hierarchical structure awareness capability of GPT-3.5\nto better parse the complex tables. Extensive experiments and results on\nComplex Table QA datasets, i.e., the open-domain dataset HiTAB and the aviation\ndomain dataset AIT-QA show that our approach significantly outperforms previous\nwork on both datasets, leading to state-of-the-art (SOTA) performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bowen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1\">Changkai Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuejie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Rui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaobo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToViLaG: Your Visual-Language Generative Model is Also An Evildoer. (arXiv:2312.11523v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11523","description":"<p>Warning: this paper includes model outputs showing offensive content. Recent\nlarge-scale Visual-Language Generative Models (VLGMs) have achieved\nunprecedented improvement in multimodal image/text generation. However, these\nmodels might also generate toxic content, e.g., offensive text and pornography\nimages, raising significant ethical risks. Despite exhaustive studies on toxic\ndegeneration of language models, this problem remains largely unexplored within\nthe context of visual-language generation. This work delves into the propensity\nfor toxicity generation and susceptibility to toxic data across various VLGMs.\nFor this purpose, we built ToViLaG, a dataset comprising 32K\nco-toxic/mono-toxic text-image pairs and 1K innocuous but evocative text that\ntends to stimulate toxicity. Furthermore, we propose WInToRe, a novel toxicity\nmetric tailored to visual-language generation, which theoretically reflects\ndifferent aspects of toxicity considering both input and output. On such a\nbasis, we benchmarked the toxicity of a diverse spectrum of VLGMs and\ndiscovered that some models do more evil than expected while some are more\nvulnerable to infection, underscoring the necessity of VLGMs detoxification.\nTherefore, we develop an innovative bottleneck-based detoxification method. Our\nmethod could reduce toxicity while maintaining comparable generation quality,\nproviding a promising initial solution to this line of research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Han Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shanlin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhihua Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing GPT4-V on Structured Reasoning Tasks. (arXiv:2312.11524v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11524","description":"<p>Multi-modality promises to unlock further uses for large language models.\nRecently, the state-of-the-art language model GPT-4 was enhanced with vision\ncapabilities. We carry out a prompting evaluation of GPT-4V and five other\nbaselines on structured reasoning tasks, such as mathematical reasoning, visual\ndata analysis, and code generation. We show that visual Chain-of-Thought, an\nextension of Chain-of-Thought to multi-modal LLMs, yields significant\nimprovements over the vanilla model. We also present a categorized analysis of\nscenarios where these models perform well and where they struggle, highlighting\nchallenges associated with coherent multimodal reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mukul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambronero_J/0/1/0/all/0/1\">Jos&#xe9; Cambronero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1\">Sumit Gulwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Vu Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbruggen_G/0/1/0/all/0/1\">Gust Verbruggen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation. (arXiv:2312.11532v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11532","description":"<p>This paper introduces a novel approach for topic modeling utilizing latent\ncodebooks from Vector-Quantized Variational Auto-Encoder~(VQ-VAE), discretely\nencapsulating the rich information of the pre-trained embeddings such as the\npre-trained language model. From the novel interpretation of the latent\ncodebooks and embeddings as conceptual bag-of-words, we propose a new\ngenerative topic model called Topic-VQ-VAE~(TVQ-VAE) which inversely generates\nthe original documents related to the respective latent codebook. The TVQ-VAE\ncan visualize the topics with various generative distributions including the\ntraditional BoW distribution and the autoregressive image generation. Our\nexperimental results on document analysis and image generation demonstrate that\nTVQ-VAE effectively captures the topic context which reveals the underlying\nstructures of the dataset and supports flexible forms of document generation.\nOfficial implementation of the proposed TVQ-VAE is available at\nhttps://github.com/clovaai/TVQ-VAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">YoungJoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jongwon Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know. (arXiv:2312.11539v1 [cs.AI])","link":"http://arxiv.org/abs/2312.11539","description":"<p>Current approaches to evaluating large language models (LLMs) with\npre-existing Knowledge Graphs (KG) mostly ignore the structure of the KG and\nmake arbitrary choices of which part of the graph to evaluate. In this paper,\nwe introduce KGLens, a method to evaluate LLMs by generating natural language\nquestions from a KG in a structure aware manner so that we can characterize its\nperformance on a more aggregated level. KGLens uses a parameterized KG, where\neach edge is augmented with a beta distribution that guides how to sample edges\nfrom the KG for QA testing. As the evaluation proceeds, different edges of the\nparameterized KG are sampled and assessed appropriately, converging to a more\nglobal picture of the performance of the LLMs on the KG as a whole. In our\nexperiments, we construct three domain-specific KGs for knowledge assessment,\ncomprising over 19,000 edges, 700 relations, and 21,000 entities. The results\ndemonstrate that KGLens can not only assess overall performance but also\nprovide topic, temporal, and relation analyses of LLMs. This showcases the\nadaptability and customizability of KGLens, emphasizing its ability to focus\nthe evaluation based on specific criteria.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shangshang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">He Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xiaochuan Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaitly_N/0/1/0/all/0/1\">Navdeep Jaitly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization in Healthcare. (arXiv:2312.11541v1 [cs.AI])","link":"http://arxiv.org/abs/2312.11541","description":"<p>In the era of modern healthcare, swiftly generating medical question\nsummaries is crucial for informed and timely patient care. Despite the\nincreasing complexity and volume of medical data, existing studies have focused\nsolely on text-based summarization, neglecting the integration of visual\ninformation. Recognizing the untapped potential of combining textual queries\nwith visual representations of medical conditions, we introduce the Multimodal\nMedical Question Summarization (MMQS) Dataset. This dataset, a major\ncontribution to our work, pairs medical queries with visual aids, facilitating\na richer and more nuanced understanding of patient needs. We also propose a\nframework, utilizing the power of Contrastive Language Image Pretraining(CLIP)\nand Large Language Models(LLMs), consisting of four modules that identify\nmedical disorders, generate relevant context, filter medical concepts, and\ncraft visually aware summaries. Our comprehensive framework harnesses the power\nof CLIP, a multimodal foundation model, and various general-purpose LLMs,\ncomprising four main modules: the medical disorder identification module, the\nrelevant context generation module, the context filtration module for\ndistilling relevant medical concepts and knowledge, and finally, a\ngeneral-purpose LLM to generate visually aware medical question summaries.\nLeveraging our MMQS dataset, we showcase how visual cues from images enhance\nthe generation of medically nuanced summaries. This multimodal approach not\nonly enhances the decision-making process in healthcare but also fosters a more\nnuanced understanding of patient queries, laying the groundwork for future\nresearch in personalized and responsive medical care\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Akash Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Arkadeep Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Raghav Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sriparna Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Setu Sinha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeGA: Preference-Aware Self-Contrastive Learning with Prompts for Anomalous User Detection on Twitter. (arXiv:2312.11553v1 [cs.SI])","link":"http://arxiv.org/abs/2312.11553","description":"<p>In the dynamic and rapidly evolving world of social media, detecting\nanomalous users has become a crucial task to address malicious activities such\nas misinformation and cyberbullying. As the increasing number of anomalous\nusers improves the ability to mimic normal users and evade detection, existing\nmethods only focusing on bot detection are ineffective in terms of capturing\nsubtle distinctions between users. To address these challenges, we proposed\nSeGA, preference-aware self-contrastive learning for anomalous user detection,\nwhich leverages heterogeneous entities and their relations in the Twittersphere\nto detect anomalous users with different malicious strategies. SeGA utilizes\nthe knowledge of large language models to summarize user preferences via posts.\nIn addition, integrating user preferences with prompts as pseudo-labels for\npreference-aware self-contrastive learning enables the model to learn\nmultifaceted aspects for describing the behaviors of users. Extensive\nexperiments on the proposed TwBNT benchmark demonstrate that SeGA significantly\noutperforms the state-of-the-art methods (+3.5\\% ~ 27.6\\%) and empirically\nvalidate the effectiveness of the model design and pre-training strategies. Our\ncode and data are publicly available at https://github.com/ying0409/SeGA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Ying-Ying Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei-Yao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wen-Chih Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deciphering Compatibility Relationships with Textual Descriptions via Extraction and Explanation. (arXiv:2312.11554v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11554","description":"<p>Understanding and accurately explaining compatibility relationships between\nfashion items is a challenging problem in the burgeoning domain of AI-driven\noutfit recommendations. Present models, while making strides in this area,\nstill occasionally fall short, offering explanations that can be elementary and\nrepetitive. This work aims to address these shortcomings by introducing the\nPair Fashion Explanation (PFE) dataset, a unique resource that has been curated\nto illuminate these compatibility relationships. Furthermore, we propose an\ninnovative two-stage pipeline model that leverages this dataset. This\nfine-tuning allows the model to generate explanations that convey the\ncompatibility relationships between items. Our experiments showcase the model's\npotential in crafting descriptions that are knowledgeable, aligned with\nground-truth matching correlations, and that produce understandable and\ninformative descriptions, as assessed by both automatic metrics and human\nevaluation. Our code and data are released at\nhttps://github.com/wangyu-ustc/PairFashionExplanation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zexue He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhankui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StarVector: Generating Scalable Vector Graphics Code from Images. (arXiv:2312.11556v1 [cs.CV])","link":"http://arxiv.org/abs/2312.11556","description":"<p>Scalable Vector Graphics (SVGs) have become integral in modern image\nrendering applications due to their infinite scalability in resolution,\nversatile usability, and editing capabilities. SVGs are particularly popular in\nthe fields of web development and graphic design. Existing approaches for SVG\nmodeling using deep learning often struggle with generating complex SVGs and\nare restricted to simpler ones that require extensive processing and\nsimplification. This paper introduces StarVector, a multimodal SVG generation\nmodel that effectively integrates Code Generation Large Language Models\n(CodeLLMs) and vision models. Our approach utilizes a CLIP image encoder to\nextract visual representations from pixel-based images, which are then\ntransformed into visual tokens via an adapter module. These visual tokens are\npre-pended to the SVG token embeddings, and the sequence is modeled by the\nStarCoder model using next-token prediction, effectively learning to align the\nvisual and code tokens. This enables StarVector to generate unrestricted SVGs\nthat accurately represent pixel images. To evaluate StarVector's performance,\nwe present SVG-Bench, a comprehensive benchmark for evaluating SVG methods\nacross multiple datasets and relevant metrics. Within this benchmark, we\nintroduce novel datasets including SVG-Stack, a large-scale dataset of\nreal-world SVG examples, and use it to pre-train StarVector as a large\nfoundation model for SVGs. Our results demonstrate significant enhancements in\nvisual quality and complexity handling over current methods, marking a notable\nadvancement in SVG generation technology. Code and models:\nhttps://github.com/joanrod/star-vector\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_J/0/1/0/all/0/1\">Juan A. Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shubham Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1\">Issam H. Laradji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pau Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1\">David Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Reasoning with Foundation Models. (arXiv:2312.11562v1 [cs.AI])","link":"http://arxiv.org/abs/2312.11562","description":"<p>Reasoning, a crucial ability for complex problem-solving, plays a pivotal\nrole in various real-world settings such as negotiation, medical diagnosis, and\ncriminal investigation. It serves as a fundamental methodology in the field of\nArtificial General Intelligence (AGI). With the ongoing development of\nfoundation models, there is a growing interest in exploring their abilities in\nreasoning tasks. In this paper, we introduce seminal foundation models proposed\nor adaptable for reasoning, highlighting the latest advancements in various\nreasoning tasks, methods, and benchmarks. We then delve into the potential\nfuture directions behind the emergence of reasoning abilities within foundation\nmodels. We also discuss the relevance of multimodal learning, autonomous\nagents, and super alignment in the context of reasoning. By discussing these\nfuture research directions, we hope to inspire researchers in their exploration\nof this field, stimulate further advancements in reasoning with foundation\nmodels, and contribute to the development of AGI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiankai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanyang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1\">Ruihang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiaqi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1\">Mengzhe Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junsong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhangyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Wu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xihui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng Ann Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A review-based study on different Text-to-Speech technologies. (arXiv:2312.11563v1 [cs.SD])","link":"http://arxiv.org/abs/2312.11563","description":"<p>This research paper presents a comprehensive review-based study on various\nText-to-Speech (TTS) technologies. TTS technology is an important aspect of\nhuman-computer interaction, enabling machines to convert written text into\naudible speech. The paper examines the different TTS technologies available,\nincluding concatenative TTS, formant synthesis TTS, and statistical parametric\nTTS. The study focuses on comparing the advantages and limitations of these\ntechnologies in terms of their naturalness of voice, the level of complexity of\nthe system, and their suitability for different applications. In addition, the\npaper explores the latest advancements in TTS technology, including neural TTS\nand hybrid TTS. The findings of this research will provide valuable insights\nfor researchers, developers, and users who want to understand the different TTS\ntechnologies and their suitability for specific applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Md. Jalal Uddin Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussan_A/0/1/0/all/0/1\">Ashab Hussan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularized Conditional Alignment for Multi-Domain Text Classification. (arXiv:2312.11572v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11572","description":"<p>The most successful multi-domain text classification (MDTC) approaches employ\nthe shared-private paradigm to facilitate the enhancement of domain-invariant\nfeatures through domain-specific attributes. Additionally, they employ\nadversarial training to align marginal feature distributions. Nevertheless,\nthese methodologies encounter two primary challenges: (1) Neglecting\nclass-aware information during adversarial alignment poses a risk of\nmisalignment; (2) The limited availability of labeled data across multiple\ndomains fails to ensure adequate discriminative capacity for the model. To\ntackle these issues, we propose a method called Regularized Conditional\nAlignment (RCA) to align the joint distributions of domains and classes, thus\nmatching features within the same category and amplifying the discriminative\nqualities of acquired features. Moreover, we employ entropy minimization and\nvirtual adversarial training to constrain the uncertainty of predictions\npertaining to unlabeled data and enhance the model's robustness. Empirical\nresults on two benchmark datasets demonstrate that our RCA approach outperforms\nstate-of-the-art MDTC techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Juntao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Language-Model Agents on Realistic Autonomous Tasks. (arXiv:2312.11671v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11671","description":"<p>In this report, we explore the ability of language model agents to acquire\nresources, create copies of themselves, and adapt to novel challenges they\nencounter in the wild. We refer to this cluster of capabilities as \"autonomous\nreplication and adaptation\" or ARA. We believe that systems capable of ARA\ncould have wide-reaching and hard-to-anticipate consequences, and that\nmeasuring and forecasting ARA may be useful for informing measures around\nsecurity, monitoring, and alignment. Additionally, once a system is capable of\nARA, placing bounds on a system's capabilities may become significantly more\ndifficult.\n</p>\n<p>We construct four simple example agents that combine language models with\ntools that allow them to take actions in the world. We then evaluate these\nagents on 12 tasks relevant to ARA. We find that these language model agents\ncan only complete the easiest tasks from this list, although they make some\nprogress on the more challenging tasks. Unfortunately, these evaluations are\nnot adequate to rule out the possibility that near-future agents will be\ncapable of ARA. In particular, we do not think that these evaluations provide\ngood assurance that the ``next generation'' of language models (e.g. 100x\neffective compute scaleup on existing models) will not yield agents capable of\nARA, unless intermediate evaluations are performed during pretraining.\nRelatedly, we expect that fine-tuning of the existing models could produce\nsubstantially more competent agents, even if the fine-tuning is not directly\ntargeted at ARA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kinniment_M/0/1/0/all/0/1\">Megan Kinniment</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_L/0/1/0/all/0/1\">Lucas Jun Koba Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Haoxing Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodrich_B/0/1/0/all/0/1\">Brian Goodrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasin_M/0/1/0/all/0/1\">Max Hasin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_L/0/1/0/all/0/1\">Lawrence Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miles_L/0/1/0/all/0/1\">Luke Harold Miles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tao R. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijk_H/0/1/0/all/0/1\">Hjalmar Wijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burget_J/0/1/0/all/0/1\">Joel Burget</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_A/0/1/0/all/0/1\">Aaron Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_E/0/1/0/all/0/1\">Elizabeth Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1\">Paul Christiano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows. (arXiv:2312.11681v1 [cs.HC])","link":"http://arxiv.org/abs/2312.11681","description":"<p>LLM chains enable complex tasks by decomposing work into a sequence of\nsub-tasks. Crowdsourcing workflows similarly decompose complex tasks into\nsmaller tasks for human crowdworkers. Chains address LLM errors analogously to\nthe way crowdsourcing workflows address human error. To characterize\nopportunities for LLM chaining, we survey 107 papers across the crowdsourcing\nand chaining literature to construct a design space for chain development. The\ndesign space connects an LLM designer's objectives to strategies they can use\nto achieve those objectives, and tactics to implement each strategy. To explore\nhow techniques from crowdsourcing may apply to chaining, we adapt crowdsourcing\nworkflows to implement LLM chains across three case studies: creating a\ntaxonomy, shortening text, and writing a short story. From the design space and\nour case studies, we identify which techniques transfer from crowdsourcing to\nLLM chaining and raise implications for future research and development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grunde_McLaughlin_M/0/1/0/all/0/1\">Madeleine Grunde-McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1\">Michelle S. Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heer_J/0/1/0/all/0/1\">Jeffrey Heer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opportunities and Challenges of Applying Large Language Models in Building Energy Efficiency and Decarbonization Studies: An Exploratory Overview. (arXiv:2312.11701v1 [eess.SY])","link":"http://arxiv.org/abs/2312.11701","description":"<p>In recent years, the rapid advancement and impressive capabilities of Large\nLanguage Models (LLMs) have been evident across various domains. This paper\nexplores the application, implications, and potential of LLMs in building\nenergy efficiency and decarbonization studies. The wide-ranging capabilities of\nLLMs are examined in the context of the building energy field, including\nintelligent control systems, code generation, data infrastructure, knowledge\nextraction, and education. Despite the promising potential of LLMs, challenges\nincluding complex and expensive computation, data privacy, security and\ncopyright, complexity in fine-tuned LLMs, and self-consistency are discussed.\nThe paper concludes with a call for future research focused on the enhancement\nof LLMs for domain-specific tasks, multi-modal LLMs, and collaborative research\nbetween AI and energy experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhelun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shaping Political Discourse using multi-source News Summarization. (arXiv:2312.11703v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11703","description":"<p>Multi-document summarization is the process of automatically generating a\nconcise summary of multiple documents related to the same topic. This summary\ncan help users quickly understand the key information from a large collection\nof documents. Multi-document summarization systems are more complex than\nsingle-document summarization systems due to the need to identify and combine\ninformation from multiple sources. In this paper, we have developed a machine\nlearning model that generates a concise summary of a topic from multiple news\ndocuments. The model is designed to be unbiased by sampling its input equally\nfrom all the different aspects of the topic, even if the majority of the news\nsources lean one way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajan_C/0/1/0/all/0/1\">Charles Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asnani_N/0/1/0/all/0/1\">Nishit Asnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Shreya Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models. (arXiv:2312.11720v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11720","description":"<p>Logical reasoning is central to complex human activities, such as thinking,\ndebating, and planning; it is also a central component of many AI systems as\nwell. In this paper, we investigate the extent to which encoder-only\ntransformer language models (LMs) can reason according to logical rules. We ask\nwhether those LMs can deduce theorems in propositional calculus and first-order\nlogic; if their relative success in these problems reflects general logical\ncapabilities; and which layers contribute the most to the task. First, we show\nfor several encoder-only LMs that they can be trained, to a reasonable degree,\nto determine logical validity on various datasets. Next, by cross-probing\nfine-tuned models on these datasets, we show that LMs have difficulty in\ntransferring their putative logical reasoning ability, which suggests that they\nmay have learned dataset-specific features, instead of a general capability.\nFinally, we conduct a layerwise probing experiment, which shows that the\nhypothesis classification task is mostly solved through higher layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pirozelli_P/0/1/0/all/0/1\">Paulo Pirozelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_M/0/1/0/all/0/1\">Marcos M. Jos&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filho_P/0/1/0/all/0/1\">Paulo de Tarso P. Filho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandao_A/0/1/0/all/0/1\">Anarosa A. F. Brand&#xe3;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cozman_F/0/1/0/all/0/1\">Fabio G. Cozman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are you talking to ['xem'] or ['x', 'em']? On Tokenization and Addressing Misgendering in LLMs with Pronoun Tokenization Parity. (arXiv:2312.11779v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11779","description":"<p>A large body of NLP research has documented the ways gender biases manifest\nand amplify within large language models (LLMs), though this research has\npredominantly operated within a gender binary-centric context. A growing body\nof work has identified the harmful limitations of this gender-exclusive\nframing; many LLMs cannot correctly and consistently refer to persons outside\nthe gender binary, especially if they use neopronouns. While data scarcity has\nbeen identified as a possible culprit, the precise mechanisms through which it\ninfluences LLM misgendering remain underexplored. Our work addresses this gap\nby studying data scarcity's role in subword tokenization and, consequently, the\nformation of LLM word representations. We uncover how the Byte-Pair Encoding\n(BPE) tokenizer, a backbone for many popular LLMs, contributes to neopronoun\nmisgendering through out-of-vocabulary behavior. We introduce pronoun\ntokenization parity (PTP), a novel approach to reduce LLM neopronoun\nmisgendering by preserving a token's functional structure. We evaluate PTP's\nefficacy using pronoun consistency-based metrics and a novel syntax-based\nmetric. Through several controlled experiments, finetuning LLMs with PTP\nimproves neopronoun consistency from 14.5% to 58.4%, highlighting the\nsignificant role tokenization plays in LLM pronoun consistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Anaelia Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehrabi_N/0/1/0/all/0/1\">Ninareh Mehrabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Palash Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1\">Jwala Dhamala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs. (arXiv:2312.11785v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11785","description":"<p>Despite progress in automated fact-checking, most systems require a\nsignificant amount of labeled training data, which is expensive. In this paper,\nwe propose a novel zero-shot method, which instead of operating directly on the\nclaim and evidence sentences, decomposes them into semantic triples augmented\nusing external knowledge graphs, and uses large language models trained for\nnatural language inference. This allows it to generalize to adversarial\ndatasets and domains that supervised models require specific training data for.\nOur empirical results show that our approach outperforms previous zero-shot\napproaches on FEVER, FEVER-Symmetric, FEVER 2.0, and Climate-FEVER, while being\ncomparable or better than supervised models on the adversarial and the\nout-of-domain datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhangdie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COOPER: Coordinating Specialized Agents towards a Complex Dialogue Goal. (arXiv:2312.11792v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11792","description":"<p>In recent years, there has been a growing interest in exploring dialogues\nwith more complex goals, such as negotiation, persuasion, and emotional\nsupport, which go beyond traditional service-focused dialogue systems. Apart\nfrom the requirement for much more sophisticated strategic reasoning and\ncommunication skills, a significant challenge of these tasks lies in the\ndifficulty of objectively measuring the achievement of their goals in a\nquantifiable way, making it difficult for existing research to directly\noptimize the dialogue procedure towards them. In our work, we emphasize the\nmultifaceted nature of complex dialogue goals and argue that it is more\nfeasible to accomplish them by comprehensively considering and jointly\npromoting their different aspects. To this end, we propose a novel dialogue\nframework, Cooper, which coordinates multiple specialized agents, each\ndedicated to a specific dialogue goal aspect separately, to approach the\ncomplex objective. Through this divide-and-conquer manner, we make complex\ndialogue goals more approachable and elicit greater intelligence via the\ncollaboration of individual agents. Experiments on persuasion and emotional\nsupport dialogues demonstrate the superiority of our method over a set of\ncompetitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenge Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_C/0/1/0/all/0/1\">Chak Tou Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1\">Yi Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MELO: Enhancing Model Editing with Neuron-Indexed Dynamic LoRA. (arXiv:2312.11795v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11795","description":"<p>Large language models (LLMs) have shown great success in various Natural\nLanguage Processing (NLP) tasks, whist they still need updates after deployment\nto fix errors or keep pace with the changing knowledge in the world.\nResearchers formulate such problem as Model Editing and have developed various\neditors focusing on different axes of editing properties. However, current\neditors can hardly support all properties and rely on heavy computational\nresources. In this paper, we propose a plug-in Model Editing method based on\nneuron-indexed dynamic LoRA (MELO), which alters the behavior of language\nmodels by dynamically activating certain LoRA blocks according to the index\nbuilt in an inner vector database. Our method satisfies various editing\nproperties with high efficiency and can be easily integrated into multiple LLM\nbackbones. Experimental results show that our proposed MELO achieves\nstate-of-the-art editing performance on three sequential editing tasks\n(document classification, question answering and hallucination correction),\nwhile requires the least trainable parameters and computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing Guiding Principles for NLP for Healthcare: A Case Study of Maternal Health. (arXiv:2312.11803v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11803","description":"<p>Objective: An ethical framework for the use of large language models (LLMs)\nis urgently needed to shape how natural language processing (NLP) tools are\nused for healthcare applications. Drawing directly from the voices of those\nmost affected, we propose a set of guiding principles for the use of NLP in\nhealthcare, with examples based on applications in maternal health.\n</p>\n<p>Materials and Methods: We led an interactive session centered on an LLM-based\nchatbot demonstration during a full-day workshop with 39 participants, and\nadditionally surveyed 30 healthcare workers and 30 birthing people about their\nvalues, needs, and perceptions of AI and LLMs. We conducted quantitative and\nqualitative analyses of the interactive discussions to consolidate our findings\ninto a set of guiding principles.\n</p>\n<p>Results: Using the case study of maternal health, we propose nine principles\nfor ethical use of LLMs, grouped into three categories: (i) contextual\nsignificance, (ii) measurements, and (iii) who/what is valued. We describe\nrationales underlying these principles and provide practical advice.\n</p>\n<p>Discussion: Healthcare faces existing challenges including the balance of\npower in clinician-patient relationships, systemic health disparities,\nhistorical injustices, and economic constraints. Our principles serve as a\nframework for surfacing key considerations when deploying LLMs in medicine, as\nwell as providing a methodological pattern for other researchers to follow.\n</p>\n<p>Conclusion: This set of principles can serve as a resource to practitioners\nworking on maternal health and other healthcare fields to emphasize the\nimportance of technical nuance, historical context, and inclusive design when\ndeveloping LLMs for use in clinical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antoniak_M/0/1/0/all/0/1\">Maria Antoniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Aakanksha Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarado_C/0/1/0/all/0/1\">Carla S. Alvarado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lucy Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_I/0/1/0/all/0/1\">Irene Y. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gemini: A Family of Highly Capable Multimodal Models. (arXiv:2312.11805v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11805","description":"<p>This report introduces a new family of multimodal models, Gemini, that\nexhibit remarkable capabilities across image, audio, video, and text\nunderstanding. The Gemini family consists of Ultra, Pro, and Nano sizes,\nsuitable for applications ranging from complex reasoning tasks to on-device\nmemory-constrained use-cases. Evaluation on a broad range of benchmarks shows\nthat our most-capable Gemini Ultra model advances the state of the art in 30 of\n32 of these benchmarks - notably being the first model to achieve human-expert\nperformance on the well-studied exam benchmark MMLU, and improving the state of\nthe art in every one of the 20 multimodal benchmarks we examined. We believe\nthat the new capabilities of Gemini models in cross-modal reasoning and\nlanguage understanding will enable a wide variety of use cases and we discuss\nour approach toward deploying them responsibly to users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Team_Gemini/0/1/0/all/0/1\">Gemini Team Google</a>: <a href=\"http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1\">Rohan Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schalkwyk_J/0/1/0/all/0/1\">Johan Schalkwyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauth_A/0/1/0/all/0/1\">Anja Hauth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millican_K/0/1/0/all/0/1\">Katie Millican</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silver_D/0/1/0/all/0/1\">David Silver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_S/0/1/0/all/0/1\">Slav Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonoglou_I/0/1/0/all/0/1\">Ioannis Antonoglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schrittwieser_J/0/1/0/all/0/1\">Julian Schrittwieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaese_A/0/1/0/all/0/1\">Amelia Glaese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jilin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitler_E/0/1/0/all/0/1\">Emily Pitler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lillicrap_T/0/1/0/all/0/1\">Timothy Lillicrap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaridou_A/0/1/0/all/0/1\">Angeliki Lazaridou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molloy_J/0/1/0/all/0/1\">James Molloy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isard_M/0/1/0/all/0/1\">Michael Isard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barham_P/0/1/0/all/0/1\">Paul R. Barham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigan_T/0/1/0/all/0/1\">Tom Hennigan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Benjamin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viola_F/0/1/0/all/0/1\">Fabio Viola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_M/0/1/0/all/0/1\">Malcolm Reynolds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doherty_R/0/1/0/all/0/1\">Ryan Doherty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_E/0/1/0/all/0/1\">Eli Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1\">Clemens Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rutherford_E/0/1/0/all/0/1\">Eliza Rutherford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_E/0/1/0/all/0/1\">Erica Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayoub_K/0/1/0/all/0/1\">Kareem Ayoub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_M/0/1/0/all/0/1\">Megha Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_G/0/1/0/all/0/1\">George Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piqueras_E/0/1/0/all/0/1\">Enrique Piqueras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krikun_M/0/1/0/all/0/1\">Maxim Krikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barr_I/0/1/0/all/0/1\">Iain Barr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savinov_N/0/1/0/all/0/1\">Nikolay Savinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danihelka_I/0/1/0/all/0/1\">Ivo Danihelka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_B/0/1/0/all/0/1\">Becca Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Ana&#xef;s White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreassen_A/0/1/0/all/0/1\">Anders Andreassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glehn_T/0/1/0/all/0/1\">Tamara von Glehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yagati_L/0/1/0/all/0/1\">Lakshman Yagati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazemi_M/0/1/0/all/0/1\">Mehran Kazemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_L/0/1/0/all/0/1\">Lucas Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalman_M/0/1/0/all/0/1\">Misha Khalman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sygnowski_J/0/1/0/all/0/1\">Jakub Sygnowski</a>, et al. (890 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training. (arXiv:2312.11819v1 [cs.LG])","link":"http://arxiv.org/abs/2312.11819","description":"<p>Recently, ChatGPT or InstructGPT like large language models (LLM) has made a\nsignificant impact in the AI world. These models are incredibly versatile,\ncapable of performing language tasks on par or even exceeding the capabilities\nof human experts. Many works have attempted to reproduce the complex\nInstructGPT's RLHF (Reinforcement Learning with Human Feedback) training\npipeline. However, the mainstream distributed RLHF training methods typically\nadopt a fixed model placement strategy, referred to as the Flattening strategy.\nThis strategy treats all four models involved in RLHF as a single entity and\nplaces them on all devices, regardless of their differences. Unfortunately,\nthis strategy exacerbates the generation bottlenecks in the RLHF training and\ndegrades the overall training efficiency. To address these issues, we propose\nan adaptive model placement framework that offers two flexible model placement\nstrategies. These strategies allow for the agile allocation of models across\ndevices in a fine-grained manner. The Interleaving strategy helps reduce memory\nredundancy and communication costs during RLHF training. On the other hand, the\nSeparation strategy improves the throughput of model training by separating the\ntraining and generation stages of the RLHF pipeline. Notably, this framework\nseamlessly integrates with other mainstream techniques for acceleration and\nenables automatic hyperparameter search. Extensive experiments have\ndemonstrated that our Interleaving and Separation strategies can achieve\nnotable improvements up to 11x, compared to the current state-of-the-art (SOTA)\napproaches. These experiments encompassed a wide range of training scenarios,\ninvolving models of varying sizes and devices of different scales. The results\nhighlight the effectiveness and superiority of our approaches in accelerating\nthe training of distributed RLHF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Youshao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weichang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhenglei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_F/0/1/0/all/0/1\">Fagui Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shangchun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1\">Lin Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Lei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaolu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TESS: A Multi-intent Parser for Conversational Multi-Agent Systems with Decentralized Natural Language Understanding Models. (arXiv:2312.11828v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11828","description":"<p>Chatbots have become one of the main pathways for the delivery of business\nautomation tools. Multi-agent systems offer a framework for designing chatbots\nat scale, making it easier to support complex conversations that span across\nmultiple domains as well as enabling developers to maintain and expand their\ncapabilities incrementally over time. However, multi-agent systems complicate\nthe natural language understanding (NLU) of user intents, especially when they\nrely on decentralized NLU models: some utterances (termed single intent) may\ninvoke a single agent while others (termed multi-intent) may explicitly invoke\nmultiple agents. Without correctly parsing multi-intent inputs, decentralized\nNLU approaches will not achieve high prediction accuracy. In this paper, we\npropose an efficient parsing and orchestration pipeline algorithm to service\nmulti-intent utterances from the user in the context of a multi-agent system.\nOur proposed approach achieved comparable performance to competitive deep\nlearning models on three different datasets while being up to 48 times faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aksar_B/0/1/0/all/0/1\">Burak Aksar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizk_Y/0/1/0/all/0/1\">Yara Rizk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborti_T/0/1/0/all/0/1\">Tathagata Chakraborti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Human Translation Difficulty with Neural Machine Translation. (arXiv:2312.11852v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11852","description":"<p>Human translators linger on some words and phrases more than others, and\npredicting this variation is a step towards explaining the underlying cognitive\nprocesses. Using data from the CRITT Translation Process Research Database, we\nevaluate the extent to which surprisal and attentional features derived from a\nNeural Machine Translation (NMT) model account for reading and production times\nof human translators. We find that surprisal and attention are complementary\npredictors of translation difficulty, and that surprisal derived from a NMT\nmodel is the single most successful predictor of production duration. Our\nanalyses draw on data from hundreds of translators operating across 13 language\npairs, and represent the most comprehensive investigation of human translation\ndifficulty to date.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lim_Z/0/1/0/all/0/1\">Zheng Wei Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vylomova_E/0/1/0/all/0/1\">Ekaterina Vylomova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1\">Charles Kemp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Revisit of Fake News Dataset with Augmented Fact-checking by ChatGPT. (arXiv:2312.11870v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11870","description":"<p>The proliferation of fake news has emerged as a critical issue in recent\nyears, requiring significant efforts to detect it. However, the existing fake\nnews detection datasets are sourced from human journalists, which are likely to\nhave inherent bias limitations due to the highly subjective nature of this\ntask. In this paper, we revisit the existing fake news dataset verified by\nhuman journalists with augmented fact-checking by large language models\n(ChatGPT), and we name the augmented fake news dataset ChatGPT-FC. We\nquantitatively analyze the distinctions and resemblances between human\njournalists and LLM in assessing news subject credibility, news creator\ncredibility, time-sensitive, and political framing. Our findings highlight\nLLM's potential to serve as a preliminary screening method, offering a\npromising avenue to mitigate the inherent biases of human journalists and\nenhance fake news detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zizhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse is Enough in Fine-tuning Pre-trained Large Language Model. (arXiv:2312.11875v1 [cs.LG])","link":"http://arxiv.org/abs/2312.11875","description":"<p>With the prevalence of pre-training-fine-tuning paradigm, how to efficiently\nadapt the pre-trained model to the downstream tasks has been an intriguing\nissue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed for\nlow-cost adaptation, including Adapters, Bia-only, and the recently widely used\nLow-Rank Adaptation. Although these methods have demonstrated their\neffectiveness to some extent and have been widely applied, the underlying\nprinciples are still unclear. In this paper, we reveal the transition of loss\nlandscape in the downstream domain from random initialization to pre-trained\ninitialization, that is, from low-amplitude oscillation to high-amplitude\noscillation. The parameter gradients exhibit a property akin to sparsity, where\na small fraction of components dominate the total gradient norm, for instance,\n1% of the components account for 99% of the gradient. This property ensures\nthat the pre-trained model can easily find a flat minimizer which guarantees\nthe model's ability to generalize even with a low number of trainable\nparameters. Based on this, we propose a gradient-based sparse fine-tuning\nalgorithm, named Sparse Increment Fine-Tuning (SIFT), and validate its\neffectiveness on a range of tasks including the GLUE Benchmark and\nInstruction-tuning. The code is accessible at https://github.com/song-wx/SIFT/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Weixi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lefei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Punctuation restoration Model and Spacing Model for Korean Ancient Document. (arXiv:2312.11881v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11881","description":"<p>In Korean ancient documents, there is no spacing or punctuation, and they are\nwritten in classical Chinese characters. This makes it challenging for modern\nindividuals and translation models to accurately interpret and translate them.\nWhile China has models predicting punctuation and spacing, applying them\ndirectly to Korean texts is problematic due to data differences. Therefore, we\ndeveloped the first models which predict punctuation and spacing for Korean\nhistorical texts and evaluated their performance. Our punctuation restoration\nmodel achieved an F1 score of 0.84, and Spacing model achieved a score of 0.96.\nIt has the advantage of enabling inference on low-performance GPUs with less\nVRAM while maintaining quite high accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_T/0/1/0/all/0/1\">Taehong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">Joonmo Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sojung Lucia Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for Accelerating Language Models Inference. (arXiv:2312.11882v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11882","description":"<p>Early Exiting is one of the most popular methods to achieve efficient\ninference. Current early exiting methods adopt the (weighted) sum of the cross\nentropy loss of all internal classifiers during training, imposing all these\nclassifiers to predict all instances correctly. However, during inference, as\nlong as one internal classifier predicts an instance correctly, it can\naccelerate without losing accuracy. Thus, there is a notable gap between\ntraining and inference. We propose ConsistentEE, an early exiting method that\nis consistent in training and inference. ConsistentEE formulates the early\nexiting process as a reinforcement learning problem. A policy network is added\nto decide whether an instance should exit or continue. The training objective\nof ConsistentEE only require each instance to be predicted correctly by one\ninternal classifier. Additionally, we introduce the concept Memorize Layer to\nmeasure the hardness of an instance. We incorporate memorized layer into reward\nfunction design, which allows ``easy'' instances to focus more on acceleration\nwhile ``hard'' instances to focus more on accuracy. Experimental results show\nthat our method outperforms other baselines on various natural language\nunderstanding and generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziqian Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yihuai Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hongliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1\">Huiping Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Difficulty-Focused Contrastive Learning for Knowledge Tracing with a Large Language Model-Based Difficulty Prediction. (arXiv:2312.11890v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11890","description":"<p>This paper presents novel techniques for enhancing the performance of\nknowledge tracing (KT) models by focusing on the crucial factor of question and\nconcept difficulty level. Despite the acknowledged significance of difficulty,\nprevious KT research has yet to exploit its potential for model optimization\nand has struggled to predict difficulty from unseen data. To address these\nproblems, we propose a difficulty-centered contrastive learning method for KT\nmodels and a Large Language Model (LLM)-based framework for difficulty\nprediction. These innovative methods seek to improve the performance of KT\nmodels and provide accurate difficulty estimates for unseen data. Our ablation\nstudy demonstrates the efficacy of these techniques by demonstrating enhanced\nKT model performance. Nonetheless, the complex relationship between language\nand difficulty merits further investigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_U/0/1/0/all/0/1\">Unggi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungjun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1\">Joon Seo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyoungsoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1\">YoungHoon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stratton_D/0/1/0/all/0/1\">Damji Stratton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyeoncheol Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Public Reactions, Perceptions, and Attitudes during the MPox Outbreak: Findings from Topic Modeling of Tweets. (arXiv:2312.11895v1 [cs.SI])","link":"http://arxiv.org/abs/2312.11895","description":"<p>The recent outbreak of the MPox virus has resulted in a tremendous increase\nin the usage of Twitter. Prior works in this area of research have primarily\nfocused on the sentiment analysis and content analysis of these Tweets, and the\nfew works that have focused on topic modeling have multiple limitations. This\npaper aims to address this research gap and makes two scientific contributions\nto this field. First, it presents the results of performing Topic Modeling on\n601,432 Tweets about the 2022 Mpox outbreak that were posted on Twitter between\n7 May 2022 and 3 March 2023. The results indicate that the conversations on\nTwitter related to Mpox during this time range may be broadly categorized into\nfour distinct themes - Views and Perspectives about Mpox, Updates on Cases and\nInvestigations about Mpox, Mpox and the LGBTQIA+ Community, and Mpox and\nCOVID-19. Second, the paper presents the findings from the analysis of these\nTweets. The results show that the theme that was most popular on Twitter (in\nterms of the number of Tweets posted) during this time range was Views and\nPerspectives about Mpox. This was followed by the theme of Mpox and the\nLGBTQIA+ Community, which was followed by the themes of Mpox and COVID-19 and\nUpdates on Cases and Investigations about Mpox, respectively. Finally, a\ncomparison with related studies in this area of research is also presented to\nhighlight the novelty and significance of this research work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duggal_Y/0/1/0/all/0/1\">Yuvraj Nihal Duggal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihui Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"External Knowledge Augmented Polyphone Disambiguation Using Large Language Model. (arXiv:2312.11920v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11920","description":"<p>One of the key issues in Mandarin Chinese text-to-speech (TTS) systems is\npolyphone disambiguation when doing grapheme-to-phoneme (G2P) conversion. In\nthis paper, we introduce a novel method to solve the problem as a generation\ntask. Following the trending research of large language models (LLM) and prompt\nlearning, the proposed method consists of three modules. Retrieval module\nincorporates external knowledge which is a multi-level semantic dictionary of\nChinese polyphonic characters to format the sentence into a prompt. Generation\nmodule adopts the decoder-only Transformer architecture to induce the target\ntext. Postprocess module corrects the generated text into a valid result if\nneeded. Experimental results show that our method outperforms the existing\nmethods on a public dataset called CPP. We also empirically study the impacts\nof different templates of the prompt, different sizes of training data, and\nwhether to incorporate external knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-Aware Question Answering for Heterogeneous Knowledge Graphs. (arXiv:2312.11922v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11922","description":"<p>Multi-hop Knowledge Base Question Answering(KBQA) aims to find the answer\nentity in a knowledge graph (KG), which requires multiple steps of reasoning.\nExisting retrieval-based approaches solve this task by concentrating on the\nspecific relation at different hops and predicting the intermediate entity\nwithin the reasoning path. During the reasoning process of these methods, the\nrepresentation of relations are fixed but the initial relation representation\nmay not be optimal. We claim they fail to utilize information from head-tail\nentities and the semantic connection between relations to enhance the current\nrelation representation, which undermines the ability to capture information of\nrelations in KGs. To address this issue, we construct a \\textbf{dual relation\ngraph} where each node denotes a relation in the original KG (\\textbf{primal\nentity graph}) and edges are constructed between relations sharing same head or\ntail entities. Then we iteratively do primal entity graph reasoning, dual\nrelation graph information propagation, and interaction between these two\ngraphs. In this way, the interaction between entity and relation is enhanced,\nand we derive better entity and relation representations. Experiments on two\npublic datasets, WebQSP and CWQ, show that our approach achieves a significant\nperformance gain over the prior state-of-the-art. Our code is available on\n\\url{https://github.com/yanmenxue/RAH-KBQA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Haowei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Quzhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Granularity Information Interaction Framework for Incomplete Utterance Rewriting. (arXiv:2312.11945v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11945","description":"<p>Recent approaches in Incomplete Utterance Rewriting (IUR) fail to capture the\nsource of important words, which is crucial to edit the incomplete utterance,\nand introduce words from irrelevant utterances. We propose a novel and\neffective multi-task information interaction framework including context\nselection, edit matrix construction, and relevance merging to capture the\nmulti-granularity of semantic information. Benefiting from fetching the\nrelevant utterance and figuring out the important words, our approach\noutperforms existing state-of-the-art models on two benchmark datasets\nRestoration-200K and CANAND in this field. Code will be provided on\n\\url{https://github.com/yanmenxue/QR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Haowei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Rendering for Conversational Speech Synthesis with Heterogeneous Graph-Based Context Modeling. (arXiv:2312.11947v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11947","description":"<p>Conversational Speech Synthesis (CSS) aims to accurately express an utterance\nwith the appropriate prosody and emotional inflection within a conversational\nsetting. While recognising the significance of CSS task, the prior studies have\nnot thoroughly investigated the emotional expressiveness problems due to the\nscarcity of emotional conversational datasets and the difficulty of stateful\nemotion modeling. In this paper, we propose a novel emotional CSS model, termed\nECSS, that includes two main components: 1) to enhance emotion understanding,\nwe introduce a heterogeneous graph-based emotional context modeling mechanism,\nwhich takes the multi-source dialogue history as input to model the dialogue\ncontext and learn the emotion cues from the context; 2) to achieve emotion\nrendering, we employ a contrastive learning-based emotion renderer module to\ninfer the accurate emotion style for the target utterance. To address the issue\nof data scarcity, we meticulously create emotional labels in terms of category\nand intensity, and annotate additional emotional information on the existing\nconversational dataset (DailyTalk). Both objective and subjective evaluations\nsuggest that our model outperforms the baseline models in understanding and\nrendering emotions. These evaluations also underscore the importance of\ncomprehensive emotional annotations. Code and audio samples can be found at:\nhttps://github.com/walker-hyf/ECSS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yifan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives. (arXiv:2312.11970v1 [cs.AI])","link":"http://arxiv.org/abs/2312.11970","description":"<p>Agent-based modeling and simulation has evolved as a powerful tool for\nmodeling complex systems, offering insights into emergent behaviors and\ninteractions among diverse agents. Integrating large language models into\nagent-based modeling and simulation presents a promising avenue for enhancing\nsimulation capabilities. This paper surveys the landscape of utilizing large\nlanguage models in agent-based modeling and simulation, examining their\nchallenges and promising future directions. In this survey, since this is an\ninterdisciplinary field, we first introduce the background of agent-based\nmodeling and simulation and large language model-empowered agents. We then\ndiscuss the motivation for applying large language models to agent-based\nsimulation and systematically analyze the challenges in environment perception,\nhuman alignment, action generation, and evaluation. Most importantly, we\nprovide a comprehensive overview of the recent works of large language\nmodel-empowered agent-based modeling and simulation in multiple scenarios,\nwhich can be divided into four domains: cyber, physical, social, and hybrid,\ncovering simulation of both real-world and virtual environments. Finally, since\nthis area is new and quickly evolving, we discuss the open problems and\npromising future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xiaochong Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jingtao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhilun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fengli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fluctuation-based Adaptive Structured Pruning for Large Language Models. (arXiv:2312.11983v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11983","description":"<p>Network Pruning is a promising way to address the huge computing resource\ndemands of the deployment and inference of Large Language Models (LLMs).\nRetraining-free is important for LLMs' pruning methods. However, almost all of\nthe existing retraining-free pruning approaches for LLMs focus on unstructured\npruning, which requires specific hardware support for acceleration. In this\npaper, we propose a novel retraining-free structured pruning framework for\nLLMs, named FLAP (FLuctuation-based Adaptive Structured Pruning). It is\nhardware-friendly by effectively reducing storage and enhancing inference\nspeed. For effective structured pruning of LLMs, we highlight three critical\nelements that demand the utmost attention: formulating structured importance\nmetrics, adaptively searching the global compressed model, and implementing\ncompensation mechanisms to mitigate performance loss. First, FLAP determines\nwhether the output feature map is easily recoverable when a column of weight is\nremoved, based on the fluctuation pruning metric. Then it standardizes the\nimportance scores to adaptively determine the global compressed model\nstructure. At last, FLAP adds additional bias terms to recover the output\nfeature maps using the baseline values. We thoroughly evaluate our approach on\na variety of language benchmarks. Without any retraining, our method\nsignificantly outperforms the state-of-the-art methods, including LLM-Pruner\nand the extension of Wanda in structured pruning. The code is released at\nhttps://github.com/CASIA-IVA-Lab/FLAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_Y/0/1/0/all/0/1\">Yongqi An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Ming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Climate Change from Large Language Models. (arXiv:2312.11985v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11985","description":"<p>Climate change presents significant challenges to the global community, and\nit is imperative to raise widespread awareness of the climate crisis and\neducate users about low-carbon living. Artificial intelligence, particularly\nlarge language models (LLMs), have emerged as powerful tools in mitigating the\nclimate crisis, leveraging their extensive knowledge, broad user base, and\nnatural language interaction capabilities. However, despite the growing body of\nresearch on climate change, there is a lack of comprehensive assessments of\nclimate crisis knowledge within LLMs. This paper aims to resolve this gap by\nproposing an automatic evaluation framework. We employ a hybrid approach to\ndata acquisition that combines data synthesis and manual collection to compile\na diverse set of questions related to the climate crisis. These questions cover\nvarious aspects of climate change, including its causes, impacts, mitigation\nstrategies, and adaptation measures. We then evaluate the model knowledge\nthrough prompt engineering based on the collected questions and generated\nanswers. We propose a set of comprehensive metrics to evaluate the climate\ncrisis knowledge, incorporating indicators from 10 different perspectives.\nExperimental results show that our method is effective in evaluating the\nknowledge of LLMs regarding the climate crisis. We evaluate several\nstate-of-the-art LLMs and find that their knowledge falls short in terms of\ntimeliness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1\">Prayag Tiwari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coreference Graph Guidance for Mind-Map Generation. (arXiv:2312.11997v1 [cs.CL])","link":"http://arxiv.org/abs/2312.11997","description":"<p>Mind-map generation aims to process a document into a hierarchical structure\nto show its central idea and branches. Such a manner is more conducive to\nunderstanding the logic and semantics of the document than plain text.\nRecently, a state-of-the-art method encodes the sentences of a document\nsequentially and converts them to a relation graph via sequence-to-graph.\nThough this method is efficient to generate mind-maps in parallel, its\nmechanism focuses more on sequential features while hardly capturing structural\ninformation. Moreover, it's difficult to model long-range semantic relations.\nIn this work, we propose a coreference-guided mind-map generation network\n(CMGN) to incorporate external structure knowledge. Specifically, we construct\na coreference graph based on the coreference semantic relationship to introduce\nthe graph structure information. Then we employ a coreference graph encoder to\nmine the potential governing relations between sentences. In order to exclude\nnoise and better utilize the information of the coreference graph, we adopt a\ngraph enhancement module in a contrastive learning manner. Experimental results\ndemonstrate that our model outperforms all the existing methods. The case study\nfurther proves that our model can more accurately and concisely reveal the\nstructure and semantics of a document. Code and data are available at\nhttps://github.com/Cyno2232/CMGN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuowei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Mengting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yinhao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can ChatGPT be Your Personal Medical Assistant?. (arXiv:2312.12006v1 [cs.CL])","link":"http://arxiv.org/abs/2312.12006","description":"<p>The advanced large language model (LLM) ChatGPT has shown its potential in\ndifferent domains and remains unbeaten due to its characteristics compared to\nother LLMs. This study aims to evaluate the potential of using a fine-tuned\nChatGPT model as a personal medical assistant in the Arabic language. To do so,\nthis study uses publicly available online questions and answering datasets in\nArabic language. There are almost 430K questions and answers for 20\ndisease-specific categories. GPT-3.5-turbo model was fine-tuned with a portion\nof this dataset. The performance of this fine-tuned model was evaluated through\nautomated and human evaluation. The automated evaluations include perplexity,\ncoherence, similarity, and token count. Native Arabic speakers with medical\nknowledge evaluated the generated text by calculating relevance, accuracy,\nprecision, logic, and originality. The overall result shows that ChatGPT has a\nbright future in medical assistance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_M/0/1/0/all/0/1\">Md. Rafiul Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1\">Ashhadul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_Z/0/1/0/all/0/1\">Zubair Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaghouani_W/0/1/0/all/0/1\">Wajdi Zaghouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belhaouari_S/0/1/0/all/0/1\">Samir Brahim Belhaouari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Preference Inference using Language Models and Probabilistic Reasoning. (arXiv:2312.12009v1 [cs.CL])","link":"http://arxiv.org/abs/2312.12009","description":"<p>Actively inferring user preferences, for example by asking good questions, is\nimportant for any human-facing decision-making system. Active inference allows\nsuch systems to adapt and personalize themselves to nuanced individual\npreferences. To enable this ability for instruction-tuned large language models\n(LLMs), one may prompt them to ask users questions to infer their preferences,\ntransforming the language models into more robust, interactive systems.\nHowever, out of the box, these models are not efficient at extracting\npreferences: the questions they generate are not informative, requiring a high\nnumber of user interactions and impeding the usability of the downstream\nsystem. In this work, we introduce an inference-time algorithm that helps LLMs\nquickly infer preferences by using more informative questions. Our algorithm\nuses a probabilistic model whose conditional distributions are defined by\nprompting an LLM, and returns questions that optimize expected entropy and\nexpected model change. Results in a simplified interactive web shopping setting\nwith real product items show that an LLM equipped with our entropy reduction\nalgorithm outperforms baselines with the same underlying LLM on task\nperformance while using fewer user interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piriyakulkij_T/0/1/0/all/0/1\">Top Piriyakulkij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuleshov_V/0/1/0/all/0/1\">Volodymyr Kuleshov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1\">Kevin Ellis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synergistic Anchored Contrastive Pre-training for Few-Shot Relation Extraction. (arXiv:2312.12021v1 [cs.CL])","link":"http://arxiv.org/abs/2312.12021","description":"<p>Few-shot Relation Extraction (FSRE) aims to extract relational facts from a\nsparse set of labeled corpora. Recent studies have shown promising results in\nFSRE by employing Pre-trained Language Models (PLMs) within the framework of\nsupervised contrastive learning, which considers both instances and label\nfacts. However, how to effectively harness massive instance-label pairs to\nencompass the learned representation with semantic richness in this learning\nparadigm is not fully explored. To address this gap, we introduce a novel\nsynergistic anchored contrastive pre-training framework. This framework is\nmotivated by the insight that the diverse viewpoints conveyed through\ninstance-label pairs capture incomplete yet complementary intrinsic textual\nsemantics. Specifically, our framework involves a symmetrical contrastive\nobjective that encompasses both sentence-anchored and label-anchored\ncontrastive losses. By combining these two losses, the model establishes a\nrobust and uniform representation space. This space effectively captures the\nreciprocal alignment of feature distributions among instances and relational\nfacts, simultaneously enhancing the maximization of mutual information across\ndiverse perspectives within the same relation. Experimental results demonstrate\nthat our framework achieves significant performance enhancements compared to\nbaseline models in downstream FSRE tasks. Furthermore, our approach exhibits\nsuperior adaptability to handle the challenges of domain shift and zero-shot\nrelation extraction. Our code is available online at\nhttps://github.com/AONE-NLP/FSRE-SaCon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DaLuo/0/1/0/all/0/1\">DaLuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yanglei Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Rui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1\">Run Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuxiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wannian Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Founder-GPT: Self-play to evaluate the Founder-Idea fit. (arXiv:2312.12037v1 [cs.CL])","link":"http://arxiv.org/abs/2312.12037","description":"<p>This research introduces an innovative evaluation method for the\n\"founder-idea\" fit in early-stage startups, utilizing advanced large language\nmodel techniques to assess founders' profiles against their startup ideas to\nenhance decision-making. Embeddings, self-play, tree-of-thought, and\ncritique-based refinement techniques show early promising results that each\nidea's success patterns are unique and they should be evaluated based on the\ncontext of the founder's background.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_S/0/1/0/all/0/1\">Sichao Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihlamur_Y/0/1/0/all/0/1\">Yigit Ihlamur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graphmax for Text Generation. (arXiv:2101.00153v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00153","description":"<p>In text generation, a large language model (LM) makes a choice of each new\nword based only on the former selection of its context using the softmax\nfunction. Nevertheless, the link statistics information of concurrent words\nbased on a scene-specific corpus is valuable in choosing the next word, which\ncan help to ensure the topic of the generated text to be aligned with the\ncurrent task. To fully explore the co-occurrence information,we propose a\ngraphmax function for task-specific text generation. Using the graph-based\nregularization, graphmax enables the final word choice to be determined by both\nthe global knowledge from the LM and the local knowledge from the\nscene-specific corpus. The traditional softmax function is regularized with a\ngraph total variation (GTV) term, which incorporates the local knowledge into\nthe LM and encourages the model to consider the statistical relationships\nbetween words in a scene-specific corpus. The proposed graphmax is versatile\nand can be readily plugged into any large pre-trained LM for text generation\nand machine translation. Through extensive experiments, we demonstrate that the\nnew GTV-based regularization can improve performances in various natural\nlanguage processing tasks in comparison with existing methods. Moreover,\nthrough human experiments, we observe that participants can easily distinguish\nthe text generated by graphmax or softmax.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bin_L/0/1/0/all/0/1\">Liu Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guosheng_Y/0/1/0/all/0/1\">Yin Guosheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction. (arXiv:2106.03518v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.03518","description":"<p>The Emotion Cause Extraction (ECE)} task aims to identify clauses which\ncontain emotion-evoking information for a particular emotion expressed in text.\nWe observe that a widely-used ECE dataset exhibits a bias that the majority of\nannotated cause clauses are either directly before their associated emotion\nclauses or are the emotion clauses themselves. Existing models for ECE tend to\nexplore such relative position information and suffer from the dataset bias. To\ninvestigate the degree of reliance of existing ECE models on clause relative\npositions, we propose a novel strategy to generate adversarial examples in\nwhich the relative position information is no longer the indicative feature of\ncause clauses. We test the performance of existing models on such adversarial\nexamples and observe a significant performance drop. To address the dataset\nbias, we propose a novel graph-based method to explicitly model the emotion\ntriggering paths by leveraging the commonsense knowledge to enhance the\nsemantic dependencies between a candidate clause and an emotion clause.\nExperimental results show that our proposed approach performs on par with the\nexisting state-of-the-art methods on the original ECE dataset, and is more\nrobust against adversarial attacks compared to existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hanqi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pergola_G/0/1/0/all/0/1\">Gabriele Pergola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.08012","description":"<p>Human beings use compositionality to generalise from past experiences to\nnovel experiences. We assume a separation of our experiences into fundamental\natomic components that can be recombined in novel ways to support our ability\nto engage with novel experiences. We frame this as the ability to learn to\ngeneralise compositionally, and we will refer to behaviours making use of this\nability as compositional learning behaviours (CLBs). A central problem to\nlearning CLBs is the resolution of a binding problem (BP). While it is another\nfeat of intelligence that human beings perform with ease, it is not the case\nfor state-of-the-art artificial agents. Thus, in order to build artificial\nagents able to collaborate with human beings, we propose to develop a novel\nbenchmark to investigate agents' abilities to exhibit CLBs by solving a\ndomain-agnostic version of the BP. We take inspiration from the language\nemergence and grounding framework of referential games and propose a\nmeta-learning extension of referential games, entitled Meta-Referential Games,\nand use this framework to build our benchmark, the Symbolic Behaviour Benchmark\n(S2B). We provide baseline results and error analysis showing that our\nbenchmark is a compelling challenge that we hope will spur the research\ncommunity towards developing more capable artificial agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Denamganai_K/0/1/0/all/0/1\">Kevin Denamgana&#xef;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Missaoui_S/0/1/0/all/0/1\">Sondess Missaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1\">James Alfred Walker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Addressing Token Uniformity in Transformers via Singular Value Transformation. (arXiv:2208.11790v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.11790","description":"<p>Token uniformity is commonly observed in transformer-based models, in which\ndifferent tokens share a large proportion of similar information after going\nthrough stacked multiple self-attention layers in a transformer. In this paper,\nwe propose to use the distribution of singular values of outputs of each\ntransformer layer to characterise the phenomenon of token uniformity and\nempirically illustrate that a less skewed singular value distribution can\nalleviate the `token uniformity' problem. Base on our observations, we define\nseveral desirable properties of singular value distributions and propose a\nnovel transformation function for updating the singular values. We show that\napart from alleviating token uniformity, the transformation function should\npreserve the local neighbourhood structure in the original embedding space. Our\nproposed singular value transformation function is applied to a range of\ntransformer-based language models such as BERT, ALBERT, RoBERTa and DistilBERT,\nand improved performance is observed in semantic textual similarity evaluation\nand a range of GLUE tasks. Our source code is available at\nhttps://github.com/hanqi-qi/tokenUni.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hanqi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training. (arXiv:2212.09897v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09897","description":"<p>Language tasks involving character-level manipulations (e.g., spelling\ncorrections, arithmetic operations, word games) are challenging for models\noperating on subword units. To address this, we develop a causal intervention\nframework to learn robust and interpretable character representations inside\nsubword-based language models. Our method treats each character as a typed\nvariable in a causal model and learns such causal structures by adapting the\ninterchange intervention training method of Geiger et al. (2021). We\nadditionally introduce a suite of character-level tasks that systematically\nvary in their dependence on meaning and sequence-level context. While\ncharacter-level models still perform best on purely form-based tasks like\nstring reversal, our method outperforms character-level models on more complex\ntasks that blend form, meaning, and context, such as spelling correction in\ncontext and word search games. Compared with standard subword-based models, our\napproach also significantly improves robustness on unseen token sequences and\nleads to human-interpretable internal representations of characters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word-Graph2vec: An efficient word embedding approach on word co-occurrence graph using random walk sampling. (arXiv:2301.04312v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.04312","description":"<p>Word embedding has become ubiquitous and is widely used in various text\nmining and natural language processing (NLP) tasks, such as information\nretrieval, semantic analysis, and machine translation, among many others.\nUnfortunately, it is prohibitively expensive to train the word embedding in a\nrelatively large corpus. We propose a graph-based word embedding algorithm,\ncalled Word-Graph2vec, which converts the large corpus into a word\nco-occurrence graph, then takes the word sequence samples from this graph by\nrandomly traveling and trains the word embedding on this sampling corpus in the\nend. We posit that because of the stable vocabulary, relative idioms, and fixed\nexpressions in English, the size and density of the word co-occurrence graph\nchange slightly with the increase in the training corpus. So that\nWord-Graph2vec has stable runtime on the large scale data set, and its\nperformance advantage becomes more and more obvious with the growth of the\ntraining corpus. Extensive experiments conducted on real-world datasets show\nthat the proposed algorithm outperforms traditional Skip-Gram by four-five\ntimes in terms of efficiency, while the error generated by the random walk\nsampling is small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jiahong Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huacan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanzhe Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-4 Technical Report. (arXiv:2303.08774v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08774","description":"<p>We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+OpenAI/0/1/0/all/0/1\">OpenAI</a>: <a href=\"http://arxiv.org/find/cs/1/au:+Achiam_J/0/1/0/all/0/1\">Josh Achiam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_S/0/1/0/all/0/1\">Steven Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sandhini Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_L/0/1/0/all/0/1\">Lama Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akkaya_I/0/1/0/all/0/1\">Ilge Akkaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aleman_F/0/1/0/all/0/1\">Florencia Leoni Aleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almeida_D/0/1/0/all/0/1\">Diogo Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altenschmidt_J/0/1/0/all/0/1\">Janko Altenschmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altman_S/0/1/0/all/0/1\">Sam Altman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anadkat_S/0/1/0/all/0/1\">Shyamal Anadkat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_R/0/1/0/all/0/1\">Red Avila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babuschkin_I/0/1/0/all/0/1\">Igor Babuschkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaji_S/0/1/0/all/0/1\">Suchir Balaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balcom_V/0/1/0/all/0/1\">Valerie Balcom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltescu_P/0/1/0/all/0/1\">Paul Baltescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Haiming Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bavarian_M/0/1/0/all/0/1\">Mo Bavarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belgum_J/0/1/0/all/0/1\">Jeff Belgum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_I/0/1/0/all/0/1\">Irwan Bello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berdine_J/0/1/0/all/0/1\">Jake Berdine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernadett_Shapiro_G/0/1/0/all/0/1\">Gabriel Bernadett-Shapiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berner_C/0/1/0/all/0/1\">Christopher Berner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogdonoff_L/0/1/0/all/0/1\">Lenny Bogdonoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boiko_O/0/1/0/all/0/1\">Oleg Boiko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_M/0/1/0/all/0/1\">Madelaine Boyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brakman_A/0/1/0/all/0/1\">Anna-Luisa Brakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockman_G/0/1/0/all/0/1\">Greg Brockman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brooks_T/0/1/0/all/0/1\">Tim Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brundage_M/0/1/0/all/0/1\">Miles Brundage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Button_K/0/1/0/all/0/1\">Kevin Button</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Trevor Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_R/0/1/0/all/0/1\">Rosie Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cann_A/0/1/0/all/0/1\">Andrew Cann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carey_B/0/1/0/all/0/1\">Brittany Carey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlson_C/0/1/0/all/0/1\">Chelsea Carlson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmichael_R/0/1/0/all/0/1\">Rory Carmichael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_B/0/1/0/all/0/1\">Brooke Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Che Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chantzis_F/0/1/0/all/0/1\">Fotis Chantzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Derek Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sully Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruby Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jason Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mark Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chess_B/0/1/0/all/0/1\">Ben Chess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_C/0/1/0/all/0/1\">Chester Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Casey Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummings_D/0/1/0/all/0/1\">Dave Cummings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Currier_J/0/1/0/all/0/1\">Jeremiah Currier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yunxing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decareaux_C/0/1/0/all/0/1\">Cory Decareaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Degry_T/0/1/0/all/0/1\">Thomas Degry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_N/0/1/0/all/0/1\">Noah Deutsch</a>, et al. (226 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models with Enhanced Adapter. (arXiv:2305.07490v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07490","description":"<p>In recent years, advancements in large language models have been remarkable,\nwith models such as ChatGPT demonstrating exceptional proficiency in diverse\nlinguistic tasks. The pre-training of large models with billions of parameters,\nposes a formidable challenge, primarily due to the scarcity of datasets of a\ncommensurate scale for effective training. Nevertheless, innovative strategies\nhave emerged, including methods to fine-tune these pre-trained models using\nfewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite\ntheir potential in various domains, these models remain limited in their\nunderstanding of artistic imagery. They have yet to fully grasp the intricate\nnuances of art images or to provide an objective articulation of the emotions\nthey evoke, in a manner akin to human perception. This work introduces\nArtGPT-4, a pioneering large vision-language model tailored to address the\ndeficiencies of contemporary models in artistic comprehension. ArtGPT-4\nunderwent training on image-text pairs utilizing a Tesla A100 device in a mere\n2 hours, with a dataset comprising approximately 0.52M entries. Impressively,\nthe model can render images with an artistic-understanding and convey the\nemotions they inspire, mirroring human interpretation. Additionally, this work\npresents a unique dataset designed to evaluate the efficacy of vision-language\nmodels. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art\nperformance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the\nestablished benchmarks introduced in This study, lagging behind professional\nartists' descriptions by a negligible 0.15 points on a 6-point scale. The code\nand the pre-trained model are accessible in\nhttps://huggingface.co/Tyrannosaurus/ArtGPT-4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhengqing Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yanfang Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning. (arXiv:2305.14160v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14160","description":"<p>In-context learning (ICL) emerges as a promising capability of large language\nmodels (LLMs) by providing them with demonstration examples to perform diverse\ntasks. However, the underlying mechanism of how LLMs learn from the provided\ncontext remains under-explored. In this paper, we investigate the working\nmechanism of ICL through an information flow lens. Our findings reveal that\nlabel words in the demonstration examples function as anchors: (1) semantic\ninformation aggregates into label word representations during the shallow\ncomputation layers' processing; (2) the consolidated information in label words\nserves as a reference for LLMs' final predictions. Based on these insights, we\nintroduce an anchor re-weighting method to improve ICL performance, a\ndemonstration compression technique to expedite inference, and an analysis\nframework for diagnosing ICL errors in GPT2-XL. The promising applications of\nour findings again validate the uncovered ICL working mechanism and pave the\nway for future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lean Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Deli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering. (arXiv:2305.14901v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14901","description":"<p>We train a language model (LM) to robustly answer multistep questions by\ngenerating and answering sub-questions. We propose Chain-of-Questions, a\nframework that trains a model to generate sub-questions and sub-answers one at\na time by leveraging human annotated question decomposition meaning\nrepresentation (QDMR). The key technical challenge is that QDMR only contains\nsub-questions but not answers to those sub-questions, so we treat sub-answers\nas latent variables and optimize them using a novel dynamic mixture of Hard-EM\nand MAPO. Chain-of-Questions greatly outperforms strong neuro-symbolic methods\nby 9.0 F1 on DROP contrast set, and outperforms GPT-3.5 by 24.3 F1 on HOTPOTQA\nadversarial set, thus demonstrating the effectiveness and robustness of our\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in Poetry Generation. (arXiv:2306.08456v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.08456","description":"<p>Controllable text generation is a challenging and meaningful field in natural\nlanguage generation (NLG). Especially, poetry generation is a typical one with\nwell-defined and strict conditions for text generation which is an ideal\nplayground for the assessment of current methodologies. While prior works\nsucceeded in controlling either semantic or metrical aspects of poetry\ngeneration, simultaneously addressing both remains a challenge. In this paper,\nwe pioneer the use of the Diffusion model for generating sonnets and Chinese\nSongCi poetry to tackle such challenges. In terms of semantics, our\nPoetryDiffusion model, built upon the Diffusion model, generates entire\nsentences or poetry by comprehensively considering the entirety of sentence\ninformation. This approach enhances semantic expression, distinguishing it from\nautoregressive and large language models (LLMs). For metrical control, the\nseparation feature of diffusion generation and its constraint control module\nenable us to flexibly incorporate a novel metrical controller to manipulate and\nevaluate metrics (format and rhythm). The denoising process in PoetryDiffusion\nallows for gradual enhancement of semantics and flexible integration of the\nmetrical controller which can calculate and impose penalties on states that\nstray significantly from the target control distribution. Experimental results\non two datasets demonstrate that our model outperforms existing models in\nautomatic evaluation of semantic, metrical, and overall performance as well as\nhuman evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chumin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Communicative Agents for Software Development. (arXiv:2307.07924v4 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2307.07924","description":"<p>Software engineering is a domain characterized by intricate decision-making\nprocesses, often relying on nuanced intuition and consultation. Recent\nadvancements in deep learning have started to revolutionize software\nengineering practices through elaborate designs implemented at various stages\nof software development. In this paper, we present an innovative paradigm that\nleverages large language models (LLMs) throughout the entire software\ndevelopment process, streamlining and unifying key processes through natural\nlanguage communication, thereby eliminating the need for specialized models at\neach phase. At the core of this paradigm lies ChatDev, a virtual chat-powered\nsoftware development company that mirrors the established waterfall model,\nmeticulously dividing the development process into four distinct chronological\nstages: designing, coding, testing, and documenting. Each stage engages a team\nof \"software agents\", such as programmers, code reviewers, and test engineers,\nfostering collaborative dialogue and facilitating a seamless workflow. The chat\nchain acts as a facilitator, breaking down each stage into atomic subtasks.\nThis enables dual roles, allowing for proposing and validating solutions\nthrough context-aware communication, leading to efficient resolution of\nspecific subtasks. The instrumental analysis of ChatDev highlights its\nremarkable efficacy in software generation, enabling the completion of the\nentire software development process in under seven minutes at a cost of less\nthan one dollar. It not only identifies and alleviates potential\nvulnerabilities but also rectifies potential hallucinations while maintaining\ncommendable efficiency and cost-effectiveness. The potential of ChatDev unveils\nfresh possibilities for integrating LLMs into the realm of software\ndevelopment. Our code is available at https://github.com/OpenBMB/ChatDev.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1\">Xin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yusheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1\">Yufan Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Juyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dahai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Transformer Extrapolation. (arXiv:2307.10156v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.10156","description":"<p>Length extrapolation has attracted considerable attention recently since it\nallows transformers to be tested on longer sequences than those used in\ntraining. Previous research has shown that this property can be attained by\nusing carefully designed Relative Positional Encodings (RPEs). While these\nmethods perform well on a variety of corpora, the conditions for length\nextrapolation have yet to be investigated. This paper attempts to determine\nwhat types of RPEs allow for length extrapolation through a thorough\nmathematical and empirical analysis. We discover that a transformer is certain\nto possess this property as long as the series that corresponds to the RPE's\nexponential converges. Two practices are derived from the conditions and\nexamined in language modeling tasks on a variety of corpora. As a bonus from\nthe conditions, we derive a new Theoretical Receptive Field (TRF) to measure\nthe receptive field of RPEs without taking any training steps. Extensive\nexperiments are conducted on the Wikitext-103, Books, Github, and WikiBook\ndatasets to demonstrate the viability of our discovered conditions. We also\ncompare TRF to Empirical Receptive Field (ERF) across different models, showing\nconsistently matched trends on the aforementioned datasets. The code is\navailable at https://github.com/OpenNLPLab/Rpe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hui Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.04766","description":"<p>We present SeaEval, a benchmark for multilingual foundation models. In\naddition to characterizing how these models understand and reason with natural\nlanguage, we also investigate how well they comprehend cultural practices,\nnuances, and values. Alongside standard accuracy metrics, we investigate the\nbrittleness of foundation models in the dimensions of semantics and\nmultilinguality. Our analyses span both open-sourced and closed models, leading\nto empirical results across classic NLP tasks, reasoning, and cultural\ncomprehension. Key findings indicate (1) Most models exhibit varied behavior\nwhen given paraphrased instructions. (2) Many models still suffer from exposure\nbias (e.g., positional bias, majority label bias). (3) For questions rooted in\nfactual, scientific, and commonsense knowledge, consistent responses are\nexpected across multilingual queries that are semantically equivalent. Yet,\nmost models surprisingly demonstrate inconsistent performance on these queries.\n(4) Multilingually-trained models have not attained \"balanced multilingual\"\ncapabilities. Our endeavors underscore the need for more generalizable semantic\nrepresentations and enhanced multilingual contextualization. SeaEval can serve\nas a launchpad for more thorough investigations and evaluations for\nmultilingual and multicultural scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_F/0/1/0/all/0/1\">Fangkai Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aw_A/0/1/0/all/0/1\">Ai Ti Aw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model. (arXiv:2309.06453v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.06453","description":"<p>Sentence Representation Learning (SRL) is a fundamental task in Natural\nLanguage Processing (NLP), with the Contrastive Learning of Sentence Embeddings\n(CSE) being the mainstream technique due to its superior performance. An\nintriguing phenomenon in CSE is the significant performance gap between\nsupervised and unsupervised methods, with their only difference lying in the\ntraining data. Previous works attribute this performance gap to differences in\ntwo representation properties (alignment and uniformity). However, since\nalignment and uniformity only measure the results, they fail to answer \"What\naspects of the training data contribute to the performance gap?\" and \"How can\nthe performance gap be narrowed?\", In this paper, we conduct empirical\nexperiments to answer these \"What\" and \"How\" questions. We first answer the\n\"What\" question by thoroughly comparing the behavior of supervised and\nunsupervised CSE during their respective training processes. From the\ncomparison, we identify the similarity pattern as a key factor to the\nperformance gap, and introduce a metric, called Relative Fitting Difficulty\n(RFD), to measure the complexity of the similarity pattern. Then, based on the\ninsights gained from the \"What\" question, we tackle the \"How\" question by\nincreasing the pattern complexity of the training data. We achieve this by\nleveraging the In-Context Learning (ICL) capability of the Large Language Model\n(LLM) to generate data that simulates complex patterns. By utilizing the\nhierarchical patterns in the LLM-generated data, we effectively narrow the gap\nbetween supervised and unsupervised CSE. We release our codes and appendix at\nhttps://github.com/BDBC-KG-NLP/NGCSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1\">Zhijie Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yongyi Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMR: Real-time Prompting of Interactive Worlds using Large Language Models. (arXiv:2309.12276v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2309.12276","description":"<p>We present Large Language Model for Mixed Reality (LLMR), a framework for the\nreal-time creation and modification of interactive Mixed Reality experiences\nusing LLMs. LLMR leverages novel strategies to tackle difficult cases where\nideal training data is scarce, or where the design goal requires the synthesis\nof internal dynamics, intuitive analysis, or advanced interactivity. Our\nframework relies on text interaction and the Unity game engine. By\nincorporating techniques for scene understanding, task planning,\nself-debugging, and memory management, LLMR outperforms the standard GPT-4 by\n4x in average error rate. We demonstrate LLMR's cross-platform interoperability\nwith several example worlds, and evaluate it on a variety of creation and\nmodification tasks to show that it can produce and edit diverse objects, tools,\nand scenes. Finally, we conducted a usability study (N=11) with a diverse set\nthat revealed participants had positive experiences with the system and would\nuse it again.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Torre_F/0/1/0/all/0/1\">Fernanda De La Torre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Cathy Mengying Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banburski_Fahey_A/0/1/0/all/0/1\">Andrzej Banburski-Fahey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_J/0/1/0/all/0/1\">Judith Amores Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanier_J/0/1/0/all/0/1\">Jaron Lanier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question-Answering Approach to Evaluating Legal Summaries. (arXiv:2309.15016v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.15016","description":"<p>Traditional evaluation metrics like ROUGE compare lexical overlap between the\nreference and generated summaries without taking argumentative structure into\naccount, which is important for legal summaries. In this paper, we propose a\nnovel legal summarization evaluation framework that utilizes GPT-4 to generate\na set of question-answer pairs that cover main points and information in the\nreference summary. GPT-4 is then used to generate answers based on the\ngenerated summary for the questions from the reference summary. Finally, GPT-4\ngrades the answers from the reference summary and the generated summary. We\nexamined the correlation between GPT-4 grading with human grading. The results\nsuggest that this question-answering approach with GPT-4 can be a useful tool\nfor gauging the quality of the summary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huihui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashley_K/0/1/0/all/0/1\">Kevin Ashley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond. (arXiv:2309.16583v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.16583","description":"<p>With the rapid advancement of large language models (LLMs), there is a\npressing need for a comprehensive evaluation suite to assess their capabilities\nand limitations. Existing LLM leaderboards often reference scores reported in\nother papers without consistent settings and prompts, which may inadvertently\nencourage cherry-picking favored settings and prompts for better results. In\nthis work, we introduce GPT-Fathom, an open-source and reproducible LLM\nevaluation suite built on top of OpenAI Evals. We systematically evaluate 10+\nleading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across\n7 capability categories, all under aligned settings. Our retrospective study on\nOpenAI's earlier models offers valuable insights into the evolutionary path\nfrom GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3\nprogressively improves to GPT-4, including technical details like whether\nadding code data improves LLM's reasoning capability, which aspects of LLM\ncapability can be improved by SFT and RLHF, how much is the alignment tax, etc.\nOur analysis sheds light on many of these questions, aiming to improve the\ntransparency of advanced LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_C/0/1/0/all/0/1\">Chenguang Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Neural Language Models as Probabilistic Finite-state Automata. (arXiv:2310.05161v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05161","description":"<p>Studying language models (LMs) in terms of well-understood formalisms allows\nus to precisely characterize their abilities and limitations. Previous work has\ninvestigated the representational capacity of recurrent neural network (RNN)\nLMs in terms of their capacity to recognize unweighted formal languages.\nHowever, LMs do not describe unweighted formal languages -- rather, they define\n\\emph{probability distributions} over strings. In this work, we study what\nclasses of such probability distributions RNN LMs can represent, which allows\nus to make more direct statements about their capabilities. We show that simple\nRNNs are equivalent to a subclass of probabilistic finite-state automata, and\ncan thus model a strict subset of probability distributions expressible by\nfinite-state models. Furthermore, we study the space complexity of representing\nfinite-state LMs with RNNs. We show that, to represent an arbitrary\ndeterministic finite-state LM with $N$ states over an alphabet $\\alphabet$, an\nRNN requires $\\Omega\\left(N |\\Sigma|\\right)$ neurons. These results present a\nfirst step towards characterizing the classes of distributions RNN LMs can\nrepresent and thus help us understand their capabilities and limitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Svete_A/0/1/0/all/0/1\">Anej Svete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLIS: Unimodal Language Models Guide Multimodal Language Generation. (arXiv:2310.09767v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.09767","description":"<p>Multimodal language generation, which leverages the synergy of language and\nvision, is a rapidly expanding field. However, existing vision-language models\nface challenges in tasks that require complex linguistic understanding. To\naddress this issue, we introduce Visual-Language models as Importance Sampling\nweights (VLIS), a novel framework that combines the visual conditioning\ncapability of vision-language models with the language understanding of\nunimodal text-only language models without further training. It extracts\npointwise mutual information of each image and text from a visual-language\nmodel and uses the value as an importance sampling weight to adjust the token\nlikelihood from a text-only model. VLIS improves vision-language models on\ndiverse tasks, including commonsense understanding (WHOOPS, OK-VQA, and\nScienceQA) and complex text generation (Concadia, Image Paragraph Captioning,\nand ROCStories). Our results suggest that VLIS represents a promising new\ndirection for multimodal language generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Jiwan Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphGPT: Graph Instruction Tuning for Large Language Models. (arXiv:2310.13023v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.13023","description":"<p>Graph Neural Networks (GNNs) have advanced graph structure understanding via\nrecursive information exchange and aggregation among graph nodes. To improve\nmodel robustness, self-supervised learning (SSL) has emerged as a promising\napproach for data augmentation. However, existing methods for generating\npre-trained graph embeddings often rely on fine-tuning with specific downstream\ntask labels, which limits their usability in scenarios where labeled data is\nscarce or unavailable. To address this, our research focuses on advancing the\ngeneralization capabilities of graph models in challenging zero-shot learning\nscenarios. Inspired by the success of large language models (LLMs), we aim to\ndevelop a graph-oriented LLM that can achieve high generalization across\ndiverse downstream datasets and tasks, even without any information available\nfrom the downstream graph data. In this work, we present the GraphGPT framework\nthat aligns LLMs with graph structural knowledge with a graph instruction\ntuning paradigm. Our framework incorporates a text-graph grounding component to\nestablish a connection between textual information and graph structures.\nAdditionally, we propose a dual-stage instruction tuning paradigm, accompanied\nby a lightweight graph-text alignment projector. This paradigm explores\nself-supervised graph structural signals and task-specific graph instructions,\nto guide LLMs in understanding complex graph structures and improving their\nadaptability across different downstream tasks. Our framework is evaluated on\nsupervised and zero-shot graph learning tasks, demonstrating superior\ngeneralization and outperforming state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiabin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Lixin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Suqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FP8-LM: Training FP8 Large Language Models. (arXiv:2310.18313v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2310.18313","description":"<p>In this paper, we explore FP8 low-bit data formats for efficient training of\nlarge language models (LLMs). Our key insight is that most variables, such as\ngradients and optimizer states, in LLM training can employ low-precision data\nformats without compromising model accuracy and requiring no changes to\nhyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision\nframework for training LLMs. This framework offers three levels of FP8\nutilization to streamline mixed-precision and distributed parallel training for\nLLMs. It gradually incorporates 8-bit gradients, optimizer states, and\ndistributed learning in an incremental manner. Experiment results show that,\nduring the training of GPT-175B model on H100 GPU platform, our FP8\nmixed-precision training framework not only achieved a remarkable 39% reduction\nin real memory usage but also ran 75% faster than the widely adopted BF16\nframework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer\nEngine by 37%. This largely reduces the training costs for large foundation\nmodels. Furthermore, our FP8 mixed-precision training methodology is generic.\nIt can be seamlessly applied to other tasks such as LLM instruction tuning and\nreinforcement learning with human feedback, offering savings in fine-tuning\nexpenses. Our FP8 low-precision training framework is open-sourced at\n{https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yixuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoshuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuxiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yifan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bolin Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingcheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaosen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1\">Jia Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruizhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_J/0/1/0/all/0/1\">Joe Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Peng Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs. (arXiv:2311.02775v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.02775","description":"<p>Responding to the thousands of student questions on online QA platforms each\nsemester has a considerable human cost, particularly in computing courses with\nrapidly growing enrollments. To address the challenges of scalable and\nintelligent question-answering (QA), we introduce an innovative solution that\nleverages open-source Large Language Models (LLMs) from the LLaMA-2 family to\nensure data privacy. Our approach combines augmentation techniques such as\nretrieval augmented generation (RAG), supervised fine-tuning (SFT), and\nlearning from human preferences data using Direct Preference Optimization\n(DPO). Through extensive experimentation on a Piazza dataset from an\nintroductory CS course, comprising 10,000 QA pairs and 1,500 pairs of\npreference data, we demonstrate a significant 30% improvement in the quality of\nanswers, with RAG being a particularly impactful addition. Our contributions\ninclude the development of a novel architecture for educational QA, extensive\nevaluations of LLM performance utilizing both human assessments and LLM-based\nmetrics, and insights into the challenges and future directions of educational\ndata processing. This work paves the way for the development of AI-TA, an\nintelligent QA assistant customizable for courses with an online QA platform\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hicke_Y/0/1/0/all/0/1\">Yann Hicke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Anmol Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianou Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1\">Paul Denny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Exemplars as Clues to Retrieving from Large Associative Memory. (arXiv:2311.03498v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.03498","description":"<p>Recently, large language models (LLMs) have made remarkable progress in\nnatural language processing. The most representative ability of LLMs is\nin-context learning (ICL), which enables LLMs to learn patterns from in-context\nexemplars without training. The performance of ICL greatly depends on the\nexemplars used. However, how to choose exemplars remains unclear due to the\nlack of understanding of how in-context learning works. In this paper, we\npresent a novel perspective on ICL by conceptualizing it as contextual\nretrieval from a model of associative memory. We establish a theoretical\nframework of ICL based on Hopfield Networks. Based on our framework, we look\ninto how in-context exemplars influence the performance of ICL and propose more\nefficient active exemplar selection. Our study sheds new light on the mechanism\nof ICL by connecting it to memory retrieval, with potential implications for\nadvancing the understanding of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiachen Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model. (arXiv:2311.07594v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.07594","description":"<p>This review paper explores Multimodal Large Language Models (MLLMs), which\nintegrate Large Language Models (LLMs) like GPT-4 to handle multimodal data\nsuch as text and vision. MLLMs demonstrate capabilities like generating image\nnarratives and answering image-based questions, bridging the gap towards\nreal-world human-computer interactions and hinting at a potential pathway to\nartificial general intelligence. However, MLLMs still face challenges in\nprocessing the semantic gap in multimodality, which may lead to erroneous\ngeneration, posing potential risks to society. Choosing the appropriate\nmodality alignment method is crucial, as improper methods might require more\nparameters with limited performance improvement. This paper aims to explore\nmodality alignment methods for LLMs and their existing capabilities.\nImplementing modality alignment allows LLMs to address environmental issues and\nenhance accessibility. The study surveys existing modal alignment methods in\nMLLMs into four groups: (1) Multimodal Converters that change data into\nsomething LLMs can understand; (2) Multimodal Perceivers to improve how LLMs\nperceive different types of data; (3) Tools Assistance for changing data into\none common format, usually text; and (4) Data-Driven methods that teach LLMs to\nunderstand specific types of data in a dataset. This field is still in a phase\nof exploration and experimentation, and we will organize and update various\nexisting research methods for multimodal information alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shezheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaoguang Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weimin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse Biomedical Tasks. (arXiv:2311.11608v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.11608","description":"<p>Objective: Most existing fine-tuned biomedical large language models (LLMs)\nfocus on enhancing performance in monolingual biomedical question answering and\nconversation tasks. To investigate the effectiveness of the fine-tuned LLMs on\ndiverse biomedical NLP tasks in different languages, We present Taiyi, a\nbilingual fine-tuned LLM for diverse biomedical tasks. Materials and Methods:\nWe first curated a comprehensive collection of 140 existing biomedical text\nmining datasets (102 English and 38 Chinese datasets) across over 10 task\ntypes. Subsequently, a two-stage strategy is proposed for supervised\nfine-tuning to optimize the model performance across varied tasks. Results:\nExperimental results on 13 test sets covering named entity recognition,\nrelation extraction, text classification, question answering tasks demonstrate\nthat Taiyi achieves superior performance compared to general LLMs. The case\nstudy involving additional biomedical NLP tasks further shows Taiyi's\nconsiderable potential for bilingual biomedical multi-tasking. Conclusion:\nLeveraging rich high-quality biomedical corpora and developing effective\nfine-tuning strategies can significantly improve the performance of LLMs within\nthe biomedical domain. Taiyi shows the bilingual multi-tasking capability\nthrough supervised fine-tuning. However, those tasks such as information\nextraction that are not generation tasks in nature remain challenging for\nLLM-based generative approaches, and they still underperform the conventional\ndiscriminative approaches of smaller language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Ling Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1\">Jinzhong Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingwen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhijun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zeyuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1\">Weiru Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qinyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangtao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yunzhi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_D/0/1/0/all/0/1\">Dinghao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wenduo Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1\">Senbo Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhihao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuanyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongfei Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.14743","description":"<p>Foundation models, specifically Large Language Models (LLM's), have lately\ngained wide-spread attention and adoption. Reinforcement Learning with Human\nFeedback (RLHF) involves training a reward model to capture desired behaviors,\nwhich is then used to align LLM's. These reward models are additionally used at\ninference-time to estimate LLM responses' adherence to those desired behaviors.\nHowever, there is little work measuring how robust these reward models are to\ndistribution shifts. In this work, we evaluate how reward model performance -\nmeasured via accuracy and calibration (i.e. alignment between accuracy and\nconfidence) - is affected by distribution shift. We show novel calibration\npatterns and accuracy drops due to OOD prompts and responses, and that the\nreward model is more sensitive to shifts in responses than prompts.\nAdditionally, we adapt an OOD detection technique commonly used in\nclassification to the reward model setting to detect these distribution shifts\nin prompts and responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1\">Will LeVine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1\">Ben Pikus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tony Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1\">Sean Hendryx</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. (arXiv:2311.16502v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.16502","description":"<p>We introduce MMMU: a new benchmark designed to evaluate multimodal models on\nmassive multi-discipline tasks demanding college-level subject knowledge and\ndeliberate reasoning. MMMU includes 11.5K meticulously collected multimodal\nquestions from college exams, quizzes, and textbooks, covering six core\ndisciplines: Art &amp; Design, Business, Science, Health &amp; Medicine, Humanities &amp;\nSocial Science, and Tech &amp; Engineering. These questions span 30 subjects and\n183 subfields, comprising 30 highly heterogeneous image types, such as charts,\ndiagrams, maps, tables, music sheets, and chemical structures. Unlike existing\nbenchmarks, MMMU focuses on advanced perception and reasoning with\ndomain-specific knowledge, challenging models to perform tasks akin to those\nfaced by experts. The evaluation of 14 open-source LMMs as well as the\nproprietary GPT-4V(ision) and Gemini highlights the substantial challenges\nposed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve\naccuracies of 56% and 59% respectively, indicating significant room for\nimprovement. We believe MMMU will stimulate the community to build\nnext-generation multimodal foundation models towards expert artificial general\nintelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yuansheng Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tianyu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruoqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1\">Samuel Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Dongfu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Weiming Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Cong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Botao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Ruibin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Renliang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Ming Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Boyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenzhu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?. (arXiv:2311.17280v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.17280","description":"<p>Data augmentation via back-translation is common when pretraining\nVision-and-Language Navigation (VLN) models, even though the generated\ninstructions are noisy. But: does that noise matter? We find that nonsensical\nor irrelevant language instructions during pretraining can have little effect\non downstream performance for both HAMT and VLN-BERT on R2R, and is still\nbetter than only using clean, human data. To underscore these results, we\nconcoct an efficient augmentation method, Unigram + Object, which generates\nnonsensical instructions that nonetheless improve downstream performance. Our\nfindings suggest that what matters for VLN R2R pretraining is the quantity of\nvisual trajectories, not the quality of instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1\">Ishika Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Explanations to Understand and Repair Embedding-based Entity Alignment. (arXiv:2312.04877v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.04877","description":"<p>Entity alignment (EA) seeks identical entities in different knowledge graphs,\nwhich is a long-standing task in the database research. Recent work leverages\ndeep learning to embed entities in vector space and align them via nearest\nneighbor search. Although embedding-based EA has gained marked success in\nrecent years, it lacks explanations for alignment decisions. In this paper, we\npresent the first framework that can generate explanations for understanding\nand repairing embedding-based EA results. Given an EA pair produced by an\nembedding model, we first compare its neighbor entities and relations to build\na matching subgraph as a local explanation. We then construct an alignment\ndependency graph to understand the pair from an abstract perspective. Finally,\nwe repair the pair by resolving three types of alignment conflicts based on\ndependency graphs. Experiments on a variety of EA datasets demonstrate the\neffectiveness, generalization, and robustness of our framework in explaining\nand repairing embedding-based EA results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xiaobin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zequn Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Shot Learning as Instruction Data Prospector for Large Language Models. (arXiv:2312.10302v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.10302","description":"<p>Aligning large language models(LLMs) with human is a critical step in\neffectively utilizing their pre-trained capabilities across a wide array of\nlanguage tasks. Current instruction tuning practices often rely on expanding\ndataset size without a clear strategy for ensuring data quality, which can\ninadvertently introduce noise and degrade model performance. To address this\nchallenge, we introduce Nuggets, a novel and efficient methodology that employs\none shot learning to select high-quality instruction data from expansive\ndatasets. Nuggets assesses the potential of individual instruction examples to\nact as effective one shot examples, thereby identifying those that can\nsignificantly enhance diverse task performance. Nuggets utilizes a scoring\nsystem based on the impact of candidate examples on the perplexity of a diverse\nanchor set, facilitating the selection of the most beneficial data for\ninstruction tuning. Through rigorous testing on two benchmarks, including\nMT-Bench and Alpaca-Eval, we demonstrate that instruction tuning with the top\n1% of Nuggets-curated examples substantially outperforms conventional methods\nthat use the full dataset. These findings advocate for a data selection\nparadigm that prioritizes quality, offering a more efficient pathway to align\nLLMs with humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunshui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xiaobo Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaxi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shuzheng Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiasing Multimodal Sarcasm Detection with Contrastive Learning. (arXiv:2312.10493v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.10493","description":"<p>Despite commendable achievements made by existing work, prevailing multimodal\nsarcasm detection studies rely more on textual content over visual information.\nIt unavoidably induces spurious correlations between textual words and labels,\nthereby significantly hindering the models' generalization capability. To\naddress this problem, we define the task of out-of-distribution (OOD)\nmultimodal sarcasm detection, which aims to evaluate models' generalizability\nwhen the word distribution is different in training and testing settings.\nMoreover, we propose a novel debiasing multimodal sarcasm detection framework\nwith contrastive learning, which aims to mitigate the harmful effect of biased\ntextual factors for robust OOD generalization. In particular, we first design\ncounterfactual data augmentation to construct the positive samples with\ndissimilar word biases and negative samples with similar word biases.\nSubsequently, we devise an adapted debiasing contrastive learning mechanism to\nempower the model to learn robust task-relevant features and alleviate the\nadverse effect of biased words. Extensive experiments show the superiority of\nthe proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Mengzhao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Can Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liqiang Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Instruction Mixture for Large Language Model Fine-tuning. (arXiv:2312.10793v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.10793","description":"<p>While instructions fine-tuning of large language models (LLMs) has been\nproven to enhance performance across various applications, the influence of the\ninstruction dataset mixture on LLMs has not been thoroughly explored. In this\nstudy, we classify instructions into three main types: NLP downstream tasks,\ncoding, and general chatting, and investigate their impact on LLMs. Our\nfindings reveal that specific types of instructions are more beneficial for\nparticular uses, while it may cause harms to other aspects, emphasizing the\nimportance of meticulously designing the instruction mixture to maximize model\nperformance. This study sheds light on the instruction mixture and paves the\nway for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renxi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xudong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haonan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Split and Rephrase with Large Language Models. (arXiv:2312.11075v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.11075","description":"<p>The Split and Rephrase task, which consists in splitting complex sentences\ninto a sequence of shorter grammatical sentences, while preserving the original\nmeaning, can facilitate the processing of complex texts for humans and machines\nalike. In this work, we describe an approach based on large language models,\nwhich improves over the state of the art by large margins on all the major\nmetrics for the task, on publicly available datasets. We also describe results\nfrom two human evaluations that further establish the significant improvements\nobtained with large language models and the viability of the approach. We\nevaluate different strategies, including fine-tuning pretrained language models\nof varying parameter size, and applying both zero-shot and few-shot in-context\nlearning on instruction-tuned language models. Although the latter were\nmarkedly outperformed by fine-tuned models, they still achieved promising\nresults overall. Our results thus demonstrate the strong potential of different\nvariants of large language models for the Split and Rephrase task, using\nrelatively small amounts of training samples and model parameters overall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ponce_D/0/1/0/all/0/1\">David Ponce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etchegoyhen_T/0/1/0/all/0/1\">Thierry Etchegoyhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Jes&#xfa;s Calleja P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gete_H/0/1/0/all/0/1\">Harritxu Gete</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Good, The Bad, and Why: Unveiling Emotions in Generative AI. (arXiv:2312.11111v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2312.11111","description":"<p>Emotion significantly impacts our daily behaviors and interactions. While\nrecent generative AI models, such as large language models, have shown\nimpressive performance in various tasks, it remains unclear whether they truly\ncomprehend emotions. This paper aims to address this gap by incorporating\npsychological theories to gain a holistic understanding of emotions in\ngenerative AI models. Specifically, we propose three approaches: 1)\nEmotionPrompt to enhance AI model performance, 2) EmotionAttack to impair AI\nmodel performance, and 3) EmotionDecode to explain the effects of emotional\nstimuli, both benign and malignant. Through extensive experiments involving\nlanguage and multi-modal models on semantic understanding, logical reasoning,\nand generation tasks, we demonstrate that both textual and visual EmotionPrompt\ncan boost the performance of AI models while EmotionAttack can hinder it.\nAdditionally, EmotionDecode reveals that AI models can comprehend emotional\nstimuli akin to the mechanism of dopamine in the human brain. Our work heralds\na novel avenue for exploring psychology to enhance our understanding of\ngenerative AI models. This paper is an extended version of our previous work\nEmotionPrompt (<a href=\"/abs/2307.11760\">arXiv:2307.11760</a>).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kaijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1\">Jianxun Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Paraphrasing The Original Text\" Makes High Accuracy Long-Context QA. (arXiv:2312.11193v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.11193","description":"<p>Although LLMs continue to iterate and improve, most open-source models still\nhave a context window of no more than 4k, limiting their ability to handle\nlong-context problems. Most existing open-source models for long-context chat\nstill lack satisfactory accuracy. To address this issue, I approach it from the\nperspective of training data and theoretically prove that training the\ncapability to handle long contexts requires \"effective\" rather than \"long\"\ndata. Based on this, I propose using the \"original text paraphrase\" task, and\nsuccessfully extend the context window of the existing model to 32k by a\nlow-cost and effective method, achieving extremely high accuracy in\nmulti-document-QA and surpassing all existing open-source models of the same\nscale. The model and training data have been open-sourced on\nHuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and\nWiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yijiong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Generalization for Multi-label Text Classification: A Data-Augmentation Approach. (arXiv:2312.11276v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.11276","description":"<p>Despite significant advancements in multi-label text classification, the\nability of existing models to generalize to novel and seldom-encountered\ncomplex concepts, which are compositions of elementary ones, remains\nunderexplored. This research addresses this gap. By creating unique data splits\nacross three benchmarks, we assess the compositional generalization ability of\nexisting multi-label text classification models. Our results show that these\nmodels often fail to generalize to compositional concepts encountered\ninfrequently during training, leading to inferior performance on tests with\nthese new combinations. To address this, we introduce a data augmentation\nmethod that leverages two innovative text generation models designed to enhance\nthe classification models' capacity for compositional generalization. Our\nexperiments show that this data augmentation approach significantly improves\nthe compositional generalization capabilities of classification models on our\nbenchmarks, with both generation models surpassing other text generation\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yuyang Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1\">Chong Teng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-12-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/"}}]}]}