{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-17T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ROSE: A Neurocomputational Architecture for Syntax. (arXiv:2303.08877v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08877","description":"<p>A comprehensive model of natural language processing in the brain must\naccommodate four components: representations, operations, structures and\nencoding. It further requires a principled account of how these components\nmechanistically, and causally, relate to each another. While previous models\nhave isolated regions of interest for structure-building and lexical access,\nmany gaps remain with respect to bridging distinct scales of neural complexity.\nBy expanding existing accounts of how neural oscillations can index various\nlinguistic processes, this article proposes a neurocomputational architecture\nfor syntax, termed the ROSE model (Representation, Operation, Structure,\nEncoding). Under ROSE, the basic data structures of syntax are atomic features,\ntypes of mental representations (R), and are coded at the single-unit and\nensemble level. Elementary computations (O) that transform these units into\nmanipulable objects accessible to subsequent structure-building levels are\ncoded via high frequency gamma activity. Low frequency synchronization and\ncross-frequency coupling code for recursive categorial inferences (S). Distinct\nforms of low frequency coupling and phase-amplitude coupling (delta-theta\ncoupling via pSTS-IFG; theta-gamma coupling via IFG to conceptual hubs) then\nencode these structures onto distinct workspaces (E). Causally connecting R to\nO is spike-phase/LFP coupling; connecting O to S is phase-amplitude coupling;\nconnecting S to E is a system of frontotemporal traveling oscillations;\nconnecting E to lower levels is low-frequency phase resetting of spike-LFP\ncoupling. ROSE is reliant on neurophysiologically plausible mechanisms, is\nsupported at all four levels by a range of recent empirical research, and\nprovides an anatomically precise and falsifiable grounding for the basic\nproperty of natural language syntax: hierarchical, recursive\nstructure-building.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murphy_E/0/1/0/all/0/1\">Elliot Murphy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Formalization of Operads in Coq. (arXiv:2303.08894v1 [math.CT])","link":"http://arxiv.org/abs/2303.08894","description":"<p>What provides the highest level of assurance for correctness of execution\nwithin a programming language? One answer, and our solution in particular, to\nthis problem is to provide a formalization for, if it exists, the denotational\nsemantics of a programming language. Achieving such a formalization provides a\ngold standard for ensuring a programming language is correct-by-construction.\nIn our effort on the DARPA V-SPELLS program, we worked to provide a foundation\nfor the denotational semantics of a meta-language using a mathematical object\nknown as an operad. This object has compositional properties which are vital to\nbuilding languages from smaller pieces. In this paper, we discuss our\nformalization of an operad in the proof assistant Coq. Moreover, our definition\nwithin Coq is capable of providing proofs that objects specified within Coq are\noperads. This work within Coq provides a formal mathematical basis for our\nmeta-language development within V-SPELLS. Our work also provides, to our\nknowledge, the first known formalization of operads within a proof assistant\nthat has significant automation, as well as a model that can be replicated\nwithout knowledge of Homotopy Type Theory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Flores_Z/0/1/0/all/0/1\">Zachary Flores</a>, <a href=\"http://arxiv.org/find/math/1/au:+Taranto_A/0/1/0/all/0/1\">Angelo Taranto</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bond_E/0/1/0/all/0/1\">Eric Bond</a>, <a href=\"http://arxiv.org/find/math/1/au:+Forman_Y/0/1/0/all/0/1\">Yakir Forman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. (arXiv:2303.08896v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08896","description":"<p>Generative Large Language Models (LLMs) such as GPT-3 are capable of\ngenerating highly fluent responses to a wide variety of user prompts. However,\nLLMs are known to hallucinate facts and make non-factual statements which can\nundermine trust in their output. Existing fact-checking approaches either\nrequire access to token-level output probability distribution (which may not be\navailable for systems such as ChatGPT) or external databases that are\ninterfaced via separate, often complex, modules. In this work, we propose\n\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check\nblack-box models in a zero-resource fashion, i.e. without an external database.\nSelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given\nconcept, sampled responses are likely to be similar and contain consistent\nfacts. However, for hallucinated facts, stochastically sampled responses are\nlikely to diverge and contradict one another. We investigate this approach by\nusing GPT-3 to generate passages about individuals from the WikiBio dataset,\nand manually annotate the factuality of the generated passages. We demonstrate\nthat SelfCheckGPT can: i) detect non-factual and factual sentences; and ii)\nrank passages in terms of factuality. We compare our approach to several\nexisting baselines and show that in sentence hallucination detection, our\napproach has AUC-PR scores comparable to grey-box methods, while SelfCheckGPT\nis best at passage factuality assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manakul_P/0/1/0/all/0/1\">Potsawee Manakul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liusie_A/0/1/0/all/0/1\">Adian Liusie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying unsupervised keyphrase methods on concepts extracted from discharge sheets. (arXiv:2303.08928v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08928","description":"<p>Clinical notes containing valuable patient information are written by\ndifferent health care providers with various scientific levels and writing\nstyles. It might be helpful for clinicians and researchers to understand what\ninformation is essential when dealing with extensive electronic medical\nrecords. Entities recognizing and mapping them to standard terminologies is\ncrucial in reducing ambiguity in processing clinical notes. Although named\nentity recognition and entity linking are critical steps in clinical natural\nlanguage processing, they can also result in the production of repetitive and\nlow-value concepts. In other hand, all parts of a clinical text do not share\nthe same importance or content in predicting the patient's condition. As a\nresult, it is necessary to identify the section in which each content is\nrecorded and also to identify key concepts to extract meaning from clinical\ntexts. In this study, these challenges have been addressed by using clinical\nnatural language processing techniques. In addition, in order to identify key\nconcepts, a set of popular unsupervised key phrase extraction methods has been\nverified and evaluated. Considering that most of the clinical concepts are in\nthe form of multi-word expressions and their accurate identification requires\nthe user to specify n-gram range, we have proposed a shortcut method to\npreserve the structure of the expression based on TF-IDF. In order to evaluate\nthe pre-processing method and select the concepts, we have designed two types\nof downstream tasks (multiple and binary classification) using the capabilities\nof transformer-based models. The obtained results show the superiority of\nproposed method in combination with SciBERT model, also offer an insight into\nthe efficacy of general extracting essential phrase methods for clinical notes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Memarzadeh_H/0/1/0/all/0/1\">Hoda Memarzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiri_N/0/1/0/all/0/1\">Nasser Ghadiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahreza_M/0/1/0/all/0/1\">Maryam Lotfi Shahreza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs. (arXiv:2303.08954v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08954","description":"<p>Research interest in task-oriented dialogs has increased as systems such as\nGoogle Assistant, Alexa and Siri have become ubiquitous in everyday life.\nHowever, the impact of academic research in this area has been limited by the\nlack of datasets that realistically capture the wide array of user pain points.\nTo enable research on some of the more challenging aspects of parsing realistic\nconversations, we introduce PRESTO, a public dataset of over 550K contextual\nmultilingual conversations between humans and virtual assistants. PRESTO\ncontains a diverse array of challenges that occur in real-world NLU tasks such\nas disfluencies, code-switching, and revisions. It is the only large scale\nhuman generated conversational parsing dataset that provides structured context\nsuch as a user's contacts and lists for each example. Our mT5 model based\nbaselines demonstrate that the conversational phenomenon present in PRESTO are\nchallenging to model, which is further pronounced in a low-resource setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rahul Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammar_W/0/1/0/all/0/1\">Waleed Ammar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aditya Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vashishtha_S/0/1/0/all/0/1\">Siddharth Vashishtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sano_M/0/1/0/all/0/1\">Motoki Sano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surani_F/0/1/0/all/0/1\">Faiz Surani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Max Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_H/0/1/0/all/0/1\">HyunJeong Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greene_D/0/1/0/all/0/1\">David Greene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kyle He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitisaroj_R/0/1/0/all/0/1\">Rattima Nitisaroj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trukhina_A/0/1/0/all/0/1\">Anna Trukhina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Shachi Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Pararth Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rushin Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-domain Sentiment Classification in Spanish. (arXiv:2303.08985v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08985","description":"<p>Sentiment Classification is a fundamental task in the field of Natural\nLanguage Processing, and has very important academic and commercial\napplications. It aims to automatically predict the degree of sentiment present\nin a text that contains opinions and subjectivity at some level, like product\nand movie reviews, or tweets. This can be really difficult to accomplish, in\npart, because different domains of text contains different words and\nexpressions. In addition, this difficulty increases when text is written in a\nnon-English language due to the lack of databases and resources. As a\nconsequence, several cross-domain and cross-language techniques are often\napplied to this task in order to improve the results. In this work we perform a\nstudy on the ability of a classification system trained with a large database\nof product reviews to generalize to different Spanish domains. Reviews were\ncollected from the MercadoLibre website from seven Latin American countries,\nallowing the creation of a large and balanced dataset. Results suggest that\ngeneralization across domains is feasible though very challenging when trained\nwith these product reviews, and can be improved by pre-training and fine-tuning\nthe classification model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Estienne_L/0/1/0/all/0/1\">Lautaro Estienne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_M/0/1/0/all/0/1\">Matias Vera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vega_L/0/1/0/all/0/1\">Leonardo Rey Vega</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeltaScore: Evaluating Story Generation with Differentiating Perturbations. (arXiv:2303.08991v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08991","description":"<p>Various evaluation metrics exist for natural language generation tasks, but\nthey have limited utility for story generation since they generally do not\ncorrelate well with human judgments and do not measure fine-grained story\naspects, such as fluency versus relatedness, as they are intended to assess\noverall generation quality. In this paper, we propose deltascore, an approach\nthat utilizes perturbation to evaluate fine-grained story aspects. Our core\nidea is based on the hypothesis that the better the story performs in a\nspecific aspect (e.g., fluency), the more it will be affected by a particular\nperturbation (e.g., introducing typos). To measure the impact, we calculate the\nlikelihood difference between the pre- and post-perturbation stories using a\nlanguage model. We evaluate deltascore against state-of-the-art model-based and\ntraditional similarity-based metrics across multiple story domains, and\ninvestigate its correlation with human judgments on five fine-grained story\naspects: fluency, coherence, relatedness, logicality, and interestingness. Our\nresults demonstrate that deltascore performs impressively in evaluating\nfine-grained story aspects, and we discovered a striking outcome where a\nspecific perturbation appears to be highly effective in measuring most aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhuohan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Miao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ART: Automatic multi-step reasoning and tool-use for large language models. (arXiv:2303.09014v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09014","description":"<p>Large language models (LLMs) can perform complex reasoning in few- and\nzero-shot settings by generating intermediate chain of thought (CoT) reasoning\nsteps. Further, each reasoning step can rely on external tools to support\ncomputation beyond the core LLM capabilities (e.g. search/running code). Prior\nwork on CoT prompting and tool use typically requires hand-crafting\ntask-specific demonstrations and carefully scripted interleaving of model\ngenerations with tool use. We introduce Automatic Reasoning and Tool-use (ART),\na framework that uses frozen LLMs to automatically generate intermediate\nreasoning steps as a program. Given a new task to solve, ART selects\ndemonstrations of multi-step reasoning and tool use from a task library. At\ntest time, ART seamlessly pauses generation whenever external tools are called,\nand integrates their output before resuming generation. ART achieves a\nsubstantial improvement over few-shot prompting and automatic CoT on unseen\ntasks in the BigBench and MMLU benchmarks, and matches performance of\nhand-crafted CoT prompts on a majority of these tasks. ART is also extensible,\nand makes it easy for humans to improve performance by correcting errors in\ntask-specific programs or incorporating new tools, which we demonstrate by\ndrastically improving performance on select tasks with minimal human\nintervention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_B/0/1/0/all/0/1\">Bhargavi Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundberg_S/0/1/0/all/0/1\">Scott Lundberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marco Tulio Ribeiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Picture is Worth a Thousand Words: Language Models Plan from Pixels. (arXiv:2303.09031v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09031","description":"<p>Planning is an important capability of artificial agents that perform\nlong-horizon tasks in real-world environments. In this work, we explore the use\nof pre-trained language models (PLMs) to reason about plan sequences from text\ninstructions in embodied visual environments. Prior PLM based approaches for\nplanning either assume observations are available in the form of text (e.g.,\nprovided by a captioning model), reason about plans from the instruction alone,\nor incorporate information about the visual environment in limited ways (such\nas a pre-trained affordance function). In contrast, we show that PLMs can\naccurately plan even when observations are directly encoded as input prompts\nfor the PLM. We show that this simple approach outperforms prior approaches in\nexperiments on the ALFWorld and VirtualHome benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anthony Z. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logeswaran_L/0/1/0/all/0/1\">Lajanugen Logeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1\">Sungryull Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential. (arXiv:2303.09038v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09038","description":"<p>The large language model called ChatGPT has drawn extensively attention\nbecause of its human-like expression and reasoning abilities. In this study, we\ninvestigate the feasibility of using ChatGPT in experiments on using ChatGPT to\ntranslate radiology reports into plain language for patients and healthcare\nproviders so that they are educated for improved healthcare. Radiology reports\nfrom 62 low-dose chest CT lung cancer screening scans and 76 brain MRI\nmetastases screening scans were collected in the first half of February for\nthis study. According to the evaluation by radiologists, ChatGPT can\nsuccessfully translate radiology reports into plain language with an average\nscore of 4.1 in the five-point system with 0.07 places of information missing\nand 0.11 places of misinformation. In terms of the suggestions provided by\nChatGPT, they are general relevant such as keeping following-up with doctors\nand closely monitoring any symptoms, and for about 37% of 138 cases in total\nChatGPT offers specific suggestions based on findings in the report. ChatGPT\nalso presents some randomness in its responses with occasionally\nover-simplified or neglected information, which can be mitigated using a more\ndetailed prompt. Furthermore, ChatGPT results are compared with a newly\nreleased large model GPT-4, showing that GPT-4 can significantly improve the\nquality of translated reports. Our results show that it is feasible to utilize\nlarge language models in clinical education, and further efforts are needed to\naddress limitations and maximize their potential.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Josh Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zapadka_M/0/1/0/all/0/1\">Mike E. Zapadka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponnatapuram_J/0/1/0/all/0/1\">Janardhana Ponnatapuram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitlow_C/0/1/0/all/0/1\">Christopher T. Whitlow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Secret-Keeping in Question Answering. (arXiv:2303.09067v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09067","description":"<p>Existing question-answering research focuses on unanswerable questions in the\ncontext of always providing an answer when a system can\\dots but what about\ncases where a system {\\bf should not} answer a question. This can either be to\nprotect sensitive users or sensitive information. Many models expose sensitive\ninformation under interrogation by an adversarial user. We seek to determine if\nit is possible to teach a question-answering system to keep a specific fact\nsecret. We design and implement a proof-of-concept architecture and through our\nevaluation determine that while possible, there are numerous directions for\nfuture research to reduce system paranoia (false positives), information\nleakage (false negatives) and extend the implementation of the work to more\ncomplex problems with preserving secrecy in the presence of information\naggregation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rollings_N/0/1/0/all/0/1\">Nathaniel W. Rollings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OSullivan_K/0/1/0/all/0/1\">Kent O&#x27;Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulshrestha_S/0/1/0/all/0/1\">Sakshum Kulshrestha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Consistent Learning: Cooperation between Generators and Discriminators. (arXiv:2303.09075v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09075","description":"<p>Using generated data to improve the performance of downstream discriminative\nmodels has recently gained popularity due to the great development of\npre-trained language models. In most previous studies, generative models and\ndiscriminative models are trained separately and thus could not adapt to any\nchanges in each other. As a result, the generated samples can easily deviate\nfrom the real data distribution, while the improvement of the discriminative\nmodel quickly reaches saturation. Generative adversarial networks (GANs) train\ngenerative models via an adversarial process with discriminative models to\nachieve joint training. However, the training of standard GANs is notoriously\nunstable and often falls short of convergence. In this paper, to address these\nissues, we propose a $\\textit{self-consistent learning}$ framework, in which a\ndiscriminator and a generator are cooperatively trained in a closed-loop form.\nThe discriminator and the generator enhance each other during multiple rounds\nof alternating training until a scoring consensus is reached. This framework\nproves to be easy to train and free from instabilities such as mode collapse\nand non-convergence. Extensive experiments on sentence semantic matching\ndemonstrate the effectiveness of the proposed framework: the discriminator\nachieves 10+ AP of improvement on the zero-shot setting and new\nstate-of-the-art performance on the full-data setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhongshen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Failures to Generalize for Coreference Resolution Models. (arXiv:2303.09092v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09092","description":"<p>Coreference resolution models are often evaluated on multiple datasets.\nDatasets vary, however, in how coreference is realized -- i.e., how the\ntheoretical concept of coreference is operationalized in the dataset -- due to\nfactors such as the choice of corpora and annotation guidelines. We investigate\nthe extent to which errors of current coreference resolution models are\nassociated with existing differences in operationalization across datasets\n(OntoNotes, PreCo, and Winogrande). Specifically, we distinguish between and\nbreak down model performance into categories corresponding to several types of\ncoreference, including coreferring generic mentions, compound modifiers, and\ncopula predicates, among others. This break down helps us investigate how\nstate-of-the-art models might vary in their ability to generalize across\ndifferent coreference types. In our experiments, for example, models trained on\nOntoNotes perform poorly on generic mentions and copula predicates in PreCo.\nOur findings help calibrate expectations of current coreference resolution\nmodels; and, future work can explicitly account for those types of coreference\nthat are empirically associated with poor generalization when developing\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Porada_I/0/1/0/all/0/1\">Ian Porada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olteanu_A/0/1/0/all/0/1\">Alexandra Olteanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suleman_K/0/1/0/all/0/1\">Kaheer Suleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trischler_A/0/1/0/all/0/1\">Adam Trischler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLEN: General-Purpose Event Detection for Thousands of Types. (arXiv:2303.09093v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09093","description":"<p>The development of event extraction systems has been hindered by the absence\nof wide-coverage, large-scale datasets. To make event extraction systems more\naccessible, we build a general-purpose event detection dataset GLEN, which\ncovers 3,465 different event types, making it over 20x larger in ontology than\nany current dataset. GLEN is created by utilizing the DWD Overlay, which\nprovides a mapping between Wikidata Qnodes and PropBank rolesets. This enables\nus to use the abundant existing annotation for PropBank as distant supervision.\nIn addition, we also propose a new multi-stage event detection model\nspecifically designed to handle the large ontology size and partial labels in\nGLEN. We show that our model exhibits superior performance (~10% F1 gain)\ncompared to both conventional classification baselines and newer\ndefinition-based models. Finally, we perform error analysis and show that label\nnoise is still the largest challenge for improving performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Q/0/1/0/all/0/1\">Qiusi Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conger_K/0/1/0/all/0/1\">Kathryn Conger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1\">Martha Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models. (arXiv:2303.09100v1 [cs.CV])","link":"http://arxiv.org/abs/2303.09100","description":"<p>For downstream applications of vision-language pre-trained models, there has\nbeen significant interest in constructing effective prompts. Existing works on\nprompt engineering, which either require laborious manual designs or optimize\nthe prompt tuning as a point estimation problem, may fail to describe diverse\ncharacteristics of categories and limit their applications. We introduce a\nBayesian probabilistic resolution to prompt learning, where the label-specific\nstochastic prompts are generated hierarchically by first sampling a latent\nvector from an underlying distribution and then employing a lightweight\ngenerative model. Importantly, we semantically regularize prompt learning with\nthe visual knowledge and view images and the corresponding prompts as patch and\ntoken sets under optimal transport, which pushes the prompt tokens to\nfaithfully capture the label-specific visual concepts, instead of overfitting\nthe training categories. Moreover, the proposed model can also be\nstraightforwardly extended to the conditional case where the\ninstance-conditional prompts are generated to improve the generalizability.\nExtensive experiments on 15 datasets show promising transferability and\ngeneralization performance of our proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongsheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Miaoge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zhibin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yishi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Distributional Shifts in Large Language Models for Code Analysis. (arXiv:2303.09128v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09128","description":"<p>We systematically study the capacity of two large language models for code -\nCodeT5 and Codex - to generalize to out-of-domain data. In this study, we\nconsider two fundamental applications - code summarization, and code\ngeneration. We split data into domains following its natural boundaries - by an\norganization, by a project, and by a module within the software project. This\nmakes recognition of in-domain vs out-of-domain data at the time of deployment\ntrivial. We establish that samples from each new domain present both models\nwith a significant challenge of distribution shift. We study how well different\nestablished methods can adapt models to better generalize to new domains. Our\nexperiments show that while multitask learning alone is a reasonable baseline,\ncombining it with few-shot finetuning on examples retrieved from training data\ncan achieve very strong performance. In fact, according to our experiments,\nthis solution can outperform direct finetuning for very low-data scenarios.\nFinally, we consider variations of this approach to create a more broadly\napplicable method to adapt to multiple domains at once. We find that in the\ncase of code generation, a model adapted to multiple domains simultaneously\nperforms on par with those adapted to each domain individually.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arakelyan_S/0/1/0/all/0/1\">Shushan Arakelyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rocktim Jyoti Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Short Survey of Viewing Large Language Models in Legal Aspect. (arXiv:2303.09136v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09136","description":"<p>Large language models (LLMs) have transformed many fields, including natural\nlanguage processing, computer vision, and reinforcement learning. These models\nhave also made a significant impact in the field of law, where they are being\nincreasingly utilized to automate various legal tasks, such as legal judgement\nprediction, legal document analysis, and legal document writing. However, the\nintegration of LLMs into the legal field has also raised several legal\nproblems, including privacy concerns, bias, and explainability. In this survey,\nwe explore the integration of LLMs into the field of law. We discuss the\nvarious applications of LLMs in legal tasks, examine the legal challenges that\narise from their use, and explore the data resources that can be used to\nspecialize LLMs in the legal domain. Finally, we discuss several promising\ndirections and conclude this paper. By doing so, we hope to provide an overview\nof the current state of LLMs in law and highlight the potential benefits and\nchallenges of their integration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhongxiang Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Block-wise Bit-Compression of Transformer-based Models. (arXiv:2303.09184v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09184","description":"<p>With the popularity of the recent Transformer-based models represented by\nBERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range\nof natural language processing tasks. However, the massive computations, huge\nmemory footprint, and thus high latency of Transformer-based models is an\ninevitable challenge for the cloud with high real-time requirement. To tackle\nthe issue, we propose BBCT, a method of block-wise bit-compression for\ntransformer without retraining. Our method achieves more fine-grained\ncompression of the whole transformer, including embedding, matrix\nmultiplication, GELU, softmax, layer normalization, and all the intermediate\nresults. As a case, we compress an efficient BERT with the method of BBCT. Our\nbenchmark test results on General Language Understanding Evaluation (GLUE) show\nthat BBCT can achieve less than 1% accuracy drop in most tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Gaochen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference. (arXiv:2303.09266v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09266","description":"<p>Dynamic early exiting has been proven to improve the inference speed of the\npre-trained language model like BERT. However, all samples must go through all\nconsecutive layers before early exiting and more complex samples usually go\nthrough more layers, which still exists redundant computation. In this paper,\nwe propose a novel dynamic early exiting combined with layer skipping for BERT\ninference named SmartBERT, which adds a skipping gate and an exiting operator\ninto each layer of BERT. SmartBERT can adaptively skip some layers and\nadaptively choose whether to exit. Besides, we propose cross-layer contrastive\nlearning and combine it into our training phases to boost the intermediate\nlayers and classifiers which would be beneficial for early exiting. To keep the\nconsistent usage of skipping gates between training and inference phases, we\npropose a hard weight mechanism during training phase. We conduct experiments\non eight classification datasets of the GLUE benchmark. Experimental results\nshow that SmartBERT achieves 2-3x computation reduction with minimal accuracy\ndrops compared with BERT and our method outperforms previous methods in both\nefficiency and accuracy. Moreover, in some complex datasets like RTE and WNLI,\nwe prove that the early exiting based on entropy hardly works, and the skipping\nmechanism is essential for reducing computation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Boren Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Bangla Complex Named Entity Recognition. (arXiv:2303.09306v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09306","description":"<p>Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n</p>\n<p>The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n</p>\n<p>We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahgir_H/0/1/0/all/0/1\">HAZ Sameen Shahgir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_R/0/1/0/all/0/1\">Ramisa Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Md. Zarif Ul Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TOT: Topology-Aware Optimal Transport For Multimodal Hate Detection. (arXiv:2303.09314v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09314","description":"<p>Multimodal hate detection, which aims to identify harmful content online such\nas memes, is crucial for building a wholesome internet environment. Previous\nwork has made enlightening exploration in detecting explicit hate remarks.\nHowever, most of their approaches neglect the analysis of implicit harm, which\nis particularly challenging as explicit text markers and demographic visual\ncues are often twisted or missing. The leveraged cross-modal attention\nmechanisms also suffer from the distributional modality gap and lack logical\ninterpretability. To address these semantic gaps issues, we propose TOT: a\ntopology-aware optimal transport framework to decipher the implicit harm in\nmemes scenario, which formulates the cross-modal aligning problem as solutions\nfor optimal transportation plans. Specifically, we leverage an optimal\ntransport kernel method to capture complementary information from multiple\nmodalities. The kernel embedding provides a non-linear transformation ability\nto reproduce a kernel Hilbert space (RKHS), which reflects significance for\neliminating the distributional modality gap. Moreover, we perceive the topology\ninformation based on aligned representations to conduct bipartite graph path\nreasoning. The newly achieved state-of-the-art performance on two publicly\navailable benchmark datasets, together with further visual analysis,\ndemonstrate the superiority of TOT in capturing implicit cross-modal alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Li Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangluan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zequn Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nayu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shiyao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?. (arXiv:2303.09325v1 [cs.AI])","link":"http://arxiv.org/abs/2303.09325","description":"<p>We evaluated the capability of generative pre-trained transformers (GPT), to\npass assessments in introductory and intermediate Python programming courses at\nthe postsecondary level. Discussions of potential uses (e.g., exercise\ngeneration, code explanation) and misuses (e.g., cheating) of this emerging\ntechnology in programming education have intensified, but to date there has not\nbeen a rigorous analysis of the models' capabilities in the realistic context\nof a full-fledged programming course with diverse set of assessment\ninstruments. We evaluated GPT on three Python courses that employ assessments\nranging from simple multiple-choice questions (no code involved) to complex\nprogramming projects with code bases distributed into multiple files (599\nexercises overall). Further, we studied if and how successfully GPT models\nleverage feedback provided by an auto-grader. We found that the current models\nare not capable of passing the full spectrum of assessments typically involved\nin a Python programming course (&lt;70% on even entry-level modules). Yet, it is\nclear that a straightforward application of these easily accessible models\ncould enable a learner to obtain a non-trivial portion of the overall available\nscore (&gt;55%) in introductory and intermediate courses alike. While the models\nexhibit remarkable capabilities, including correcting solutions based on\nauto-grader's feedback, some limitations exist (e.g., poor handling of\nexercises requiring complex chains of reasoning steps). These findings can be\nleveraged by instructors wishing to adapt their assessments so that GPT becomes\na valuable assistant for a learner as opposed to an end-to-end solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savelka_J/0/1/0/all/0/1\">Jaromir Savelka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Arav Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogart_C/0/1/0/all/0/1\">Christopher Bogart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakr_M/0/1/0/all/0/1\">Majd Sakr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tollywood Emotions: Annotation of Valence-Arousal in Telugu Song Lyrics. (arXiv:2303.09364v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09364","description":"<p>Emotion recognition from a given music track has heavily relied on acoustic\nfeatures, social tags, and metadata but is seldom focused on lyrics. There are\nno datasets of Indian language songs that contain both valence and arousal\nmanual ratings of lyrics. We present a new manually annotated dataset of Telugu\nsongs' lyrics collected from Spotify with valence and arousal annotated on a\ndiscrete scale. A fairly high inter-annotator agreement was observed for both\nvalence and arousal. Subsequently, we create two music emotion recognition\nmodels by using two classification techniques to identify valence, arousal and\nrespective emotion quadrant from lyrics. Support vector machine (SVM) with term\nfrequency-inverse document frequency (TF-IDF) features and fine-tuning the\npre-trained XLMRoBERTa (XLM-R) model were used for valence, arousal and\nquadrant classification tasks. Fine-tuned XLMRoBERTa performs better than the\nSVM by improving macro-averaged F1-scores of 54.69%, 67.61%, 34.13% to 77.90%,\n80.71% and 58.33% for valence, arousal and quadrant classifications,\nrespectively, on 10-fold cross-validation. In addition, we compare our lyrics\nannotations with Spotify's annotations of valence and energy (same as arousal),\nwhich are based on entire music tracks. The implications of our findings are\ndiscussed. Finally, we make the dataset publicly available with lyrics,\nannotations and Spotify IDs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shanker_R/0/1/0/all/0/1\">R Guru Ravi Shanker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_B/0/1/0/all/0/1\">B Manikanta Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koushik_B/0/1/0/all/0/1\">BV Koushik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alluri_V/0/1/0/all/0/1\">Vinoo Alluri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Scope of In-Context Learning for the Extraction of Medical Temporal Constraints. (arXiv:2303.09366v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09366","description":"<p>Medications often impose temporal constraints on everyday patient activity.\nViolations of such medical temporal constraints (MTCs) lead to a lack of\ntreatment adherence, in addition to poor health outcomes and increased\nhealthcare expenses. These MTCs are found in drug usage guidelines (DUGs) in\nboth patient education materials and clinical texts. Computationally\nrepresenting MTCs in DUGs will advance patient-centric healthcare applications\nby helping to define safe patient activity patterns. We define a novel taxonomy\nof MTCs found in DUGs and develop a novel context-free grammar (CFG) based\nmodel to computationally represent MTCs from unstructured DUGs. Additionally,\nwe release three new datasets with a combined total of N = 836 DUGs labeled\nwith normalized MTCs. We develop an in-context learning (ICL) solution for\nautomatically extracting and normalizing MTCs found in DUGs, achieving an\naverage F1 score of 0.62 across all datasets. Finally, we rigorously\ninvestigate ICL model performance against a baseline model, across datasets and\nMTC types, and through in-depth error analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seegmiller_P/0/1/0/all/0/1\">Parker Seegmiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatto_J/0/1/0/all/0/1\">Joseph Gatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basak_M/0/1/0/all/0/1\">Madhusudan Basak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cook_D/0/1/0/all/0/1\">Diane Cook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghasemzadeh_H/0/1/0/all/0/1\">Hassan Ghasemzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stankovic_J/0/1/0/all/0/1\">John Stankovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preum_S/0/1/0/all/0/1\">Sarah Preum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-to-ECG: 12-Lead Electrocardiogram Synthesis conditioned on Clinical Text Reports. (arXiv:2303.09395v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09395","description":"<p>Electrocardiogram (ECG) synthesis is the area of research focused on\ngenerating realistic synthetic ECG signals for medical use without concerns\nover annotation costs or clinical data privacy restrictions. Traditional ECG\ngeneration models consider a single ECG lead and utilize GAN-based generative\nmodels. These models can only generate single lead samples and require separate\ntraining for each diagnosis class. The diagnosis classes of ECGs are\ninsufficient to capture the intricate differences between ECGs depending on\nvarious features (e.g. patient demographic details, co-existing diagnosis\nclasses, etc.). To alleviate these challenges, we present a text-to-ECG task,\nin which textual inputs are used to produce ECG outputs. Then we propose\nAuto-TTE, an autoregressive generative model conditioned on clinical text\nreports to synthesize 12-lead ECGs, for the first time to our knowledge. We\ncompare the performance of our model with other representative models in\ntext-to-speech and text-to-image. Experimental results show the superiority of\nour model in various quantitative evaluations and qualitative analysis.\nFinally, we conduct a user study with three board-certified cardiologists to\nconfirm the fidelity and semantic alignment of generated samples. our code will\nbe available at https://github.com/TClife/text_to_ecg\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyunseung Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Joon-myoung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_K/0/1/0/all/0/1\">Ki-Hyun Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Min Sung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cryptocurrency Price Prediction using Twitter Sentiment Analysis. (arXiv:2303.09397v1 [q-fin.ST])","link":"http://arxiv.org/abs/2303.09397","description":"<p>The cryptocurrency ecosystem has been the centre of discussion on many social\nmedia platforms, following its noted volatility and varied opinions. Twitter is\nrapidly being utilised as a news source and a medium for bitcoin discussion.\nOur algorithm seeks to use historical prices and sentiment of tweets to\nforecast the price of Bitcoin. In this study, we develop an end-to-end model\nthat can forecast the sentiment of a set of tweets (using a Bidirectional\nEncoder Representations from Transformers - based Neural Network Model) and\nforecast the price of Bitcoin (using Gated Recurrent Unit) using the predicted\nsentiment and other metrics like historical cryptocurrency price data, tweet\nvolume, a user's following, and whether or not a user is verified. The\nsentiment prediction gave a Mean Absolute Percentage Error of 9.45%, an average\nof real-time data, and test data. The mean absolute percent error for the price\nprediction was 3.6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+GB_H/0/1/0/all/0/1\">Haritha GB</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+B_S/0/1/0/all/0/1\">Sahana N.B</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToxVis: Enabling Interpretability of Implicit vs. Explicit Toxicity Detection Models with Interactive Visualization. (arXiv:2303.09402v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09402","description":"<p>The rise of hate speech on online platforms has led to an urgent need for\neffective content moderation. However, the subjective and multi-faceted nature\nof hateful online content, including implicit hate speech, poses significant\nchallenges to human moderators and content moderation systems. To address this\nissue, we developed ToxVis, a visually interactive and explainable tool for\nclassifying hate speech into three categories: implicit, explicit, and\nnon-hateful. We fine-tuned two transformer-based models using RoBERTa, XLNET,\nand GPT-3 and used deep learning interpretation techniques to provide\nexplanations for the classification results. ToxVis enables users to input\npotentially hateful text and receive a classification result along with a\nvisual explanation of which words contributed most to the decision. By making\nthe classification process explainable, ToxVis provides a valuable tool for\nunderstanding the nuances of hateful content and supporting more effective\ncontent moderation. Our research contributes to the growing body of work aimed\nat mitigating the harms caused by online hate speech and demonstrates the\npotential for combining state-of-the-art natural language processing models\nwith interpretable deep learning techniques to address this critical issue.\nFinally, ToxVis can serve as a resource for content moderators, social media\nplatforms, and researchers working to combat the spread of hate speech online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunturi_U/0/1/0/all/0/1\">Uma Gunturi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rho_E/0/1/0/all/0/1\">Eugenia H. Rho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Team SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual approaches for news genre, topic and persuasion technique classification. (arXiv:2303.09421v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09421","description":"<p>This paper describes our approach for SemEval-2023 Task 3: Detecting the\ncategory, the framing, and the persuasion techniques in online news in a\nmulti-lingual setup. For Subtask 1 (News Genre), we propose an ensemble of\nfully trained and adapter mBERT models which was ranked joint-first for German,\nand had the highest mean rank of multi-language teams. For Subtask 2 (Framing),\nwe achieved first place in 3 languages, and the best average rank across all\nthe languages, by using two separate ensembles: a monolingual\nRoBERTa-MUPPETLARGE and an ensemble of XLM-RoBERTaLARGE with adapters and task\nadaptive pretraining. For Subtask 3 (Persuasion Techniques), we train a\nmonolingual RoBERTa-Base model for English and a multilingual mBERT model for\nthe remaining languages, which achieved top 10 for all languages, including 2nd\nfor English. For each subtask, we compare monolingual and multilingual\napproaches, and consider class imbalance techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Ben Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razuvayevskaya_O/0/1/0/all/0/1\">Olesya Razuvayevskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heppell_F/0/1/0/all/0/1\">Freddy Heppell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leite_J/0/1/0/all/0/1\">Jo&#xe3;o A. Leite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1\">Kalina Bontcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jump to Conclusions: Short-Cutting Transformers With Linear Transformations. (arXiv:2303.09435v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09435","description":"<p>Transformer-based language models (LMs) create hidden representations of\ntheir inputs at every layer, but only use final-layer representations for\nprediction. This obscures the internal decision-making process of the model and\nthe utility of its intermediate representations. One way to elucidate this is\nto cast the hidden representations as final representations, bypassing the\ntransformer computation in-between. In this work, we suggest a simple method\nfor such casting, by using linear transformations. We show that our approach\nproduces more accurate approximations than the prevailing practice of\ninspecting hidden representations from all layers in the space of the final\nlayer. Moreover, in the context of language modeling, our method allows\n\"peeking\" into early layer representations of GPT-2 and BERT, showing that\noften LMs already predict the final output in early layers. We then demonstrate\nthe practicality of our method to recent early exit strategies, showing that\nwhen aiming, for example, at retention of 95% accuracy, our approach saves\nadditional 7.9% layers for GPT-2 and 5.4% layers for BERT, on top of the\nsavings of the original approach. Last, we extend our method to linearly\napproximate sub-modules, finding that attention is most tolerant to this\nchange.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Din_A/0/1/0/all/0/1\">Alexander Yom Din</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karidi_T/0/1/0/all/0/1\">Taelin Karidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trustera: A Live Conversation Redaction System. (arXiv:2303.09438v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09438","description":"<p>Trustera, the first functional system that redacts personally identifiable\ninformation (PII) in real-time spoken conversations to remove agents' need to\nhear sensitive information while preserving the naturalness of live\ncustomer-agent conversations. As opposed to post-call redaction, audio masking\nstarts as soon as the customer begins speaking to a PII entity. This\nsignificantly reduces the risk of PII being intercepted or stored in insecure\ndata storage. Trustera's architecture consists of a pipeline of automatic\nspeech recognition, natural language understanding, and a live audio redactor\nmodule. The system's goal is three-fold: redact entities that are PII, mask the\naudio that goes to the agent, and at the same time capture the entity, so that\nthe captured PII can be used for a payment transaction or caller\nidentification. Trustera is currently being used by thousands of agents to\nsecure customers' sensitive information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gouvea_E/0/1/0/all/0/1\">Evandro Gouv&#xea;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dadgar_A/0/1/0/all/0/1\">Ali Dadgar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalalvand_S/0/1/0/all/0/1\">Shahab Jalalvand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chengalvarayan_R/0/1/0/all/0/1\">Rathi Chengalvarayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_B/0/1/0/all/0/1\">Badrinath Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_R/0/1/0/all/0/1\">Ryan Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1\">Nicholas Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGovern_J/0/1/0/all/0/1\">Jennifer McGovern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bangalore_S/0/1/0/all/0/1\">Srinivas Bangalore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stern_B/0/1/0/all/0/1\">Ben Stern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling High-Dimensional Data With Sparse Input. (arXiv:2303.09446v1 [eess.AS])","link":"http://arxiv.org/abs/2303.09446","description":"<p>We address the problem of human-in-the-loop control for generating\nhighly-structured data. This task is challenging because existing generative\nmodels lack an efficient interface through which users can modify the output.\nUsers have the option to either manually explore a non-interpretable latent\nspace, or to laboriously annotate the data with conditioning labels. To solve\nthis, we introduce a novel framework whereby an encoder maps a sparse, human\ninterpretable control space onto the latent space of a generative model. We\napply this framework to the task of controlling prosody in text-to-speech\nsynthesis. We propose a model, called Multiple-Instance CVAE (MICVAE), that is\nspecifically designed to encode sparse prosodic features and output complete\nwaveforms. We show empirically that MICVAE displays desirable qualities of a\nsparse human-in-the-loop control mechanism: efficiency, robustness, and\nfaithfulness. With even a very small number of input values (~4), MICVAE\nenables users to improve the quality of the output significantly, in terms of\nlistener preference (4:1).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Iliescu_D/0/1/0/all/0/1\">Dan Andrei Iliescu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohan_D/0/1/0/all/0/1\">Devang Savita Ram Mohan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teh_T/0/1/0/all/0/1\">Tian Huey Teh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hodari_Z/0/1/0/all/0/1\">Zack Hodari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Cross-lingual Visual Speech Representations. (arXiv:2303.09455v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09455","description":"<p>Cross-lingual self-supervised learning has been a growing research topic in\nthe last few years. However, current works only explored the use of audio\nsignals to create representations. In this work, we study cross-lingual\nself-supervised visual representation learning. We use the recently-proposed\nRaw Audio-Visual Speech Encoders (RAVEn) framework to pre-train an audio-visual\nmodel with unlabelled multilingual data, and then fine-tune the visual model on\nlabelled transcriptions. Our experiments show that: (1) multi-lingual models\nwith more data outperform monolingual ones, but, when keeping the amount of\ndata fixed, monolingual models tend to reach better performance; (2)\nmulti-lingual outperforms English-only pre-training; (3) using languages which\nare more similar yields better results; and (4) fine-tuning on unseen languages\nis competitive to using the target language in the pre-training set. We hope\nour study inspires future research on non-English-only speech representation\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zinonos_A/0/1/0/all/0/1\">Andreas Zinonos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haliassos_A/0/1/0/all/0/1\">Alexandros Haliassos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingchuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1\">Stavros Petridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Participates in a Computer Science Exam. (arXiv:2303.09461v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09461","description":"<p>We asked ChatGPT to participate in an undergraduate computer science exam on\n''Algorithms and Data Structures''. We evaluated the program on the entire exam\nas posed to the students. We hand-copied its answers onto an exam sheet, which\nwas subsequently graded in a blind setup alongside those of 200 participating\nstudents. We find that ChatGPT narrowly passed the exam, obtaining 20.5 out of\n40 points. This impressive performance indicates that ChatGPT can indeed\nsucceed in challenging tasks like university exams. At the same time, the tasks\nin our exam are structurally similar to those on other exams, solved homework\nproblems, and teaching materials that can be found online. Therefore, it would\nbe premature to conclude from this experiment that ChatGPT has any\nunderstanding of computer science. The transcript of our conversation with\nChatGPT is available at\n\\url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}, and the entire\ngraded exam is in the appendix of this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bordt_S/0/1/0/all/0/1\">Sebastian Bordt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luxburg_U/0/1/0/all/0/1\">Ulrike von Luxburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$P+$: Extended Textual Conditioning in Text-to-Image Generation. (arXiv:2303.09522v1 [cs.CV])","link":"http://arxiv.org/abs/2303.09522","description":"<p>We introduce an Extended Textual Conditioning space in text-to-image models,\nreferred to as $P+$. This space consists of multiple textual conditions,\nderived from per-layer prompts, each corresponding to a layer of the denoising\nU-net of the diffusion model.\n</p>\n<p>We show that the extended space provides greater disentangling and control\nover image synthesis. We further introduce Extended Textual Inversion (XTI),\nwhere the images are inverted into $P+$, and represented by per-layer tokens.\n</p>\n<p>We show that XTI is more expressive and precise, and converges faster than\nthe original Textual Inversion (TI) space. The extended inversion method does\nnot involve any noticeable trade-off between reconstruction and editability and\ninduces more regular inversions.\n</p>\n<p>We conduct a series of extensive experiments to analyze and understand the\nproperties of the new space, and to showcase the effectiveness of our method\nfor personalizing text-to-image models. Furthermore, we utilize the unique\nproperties of this space to achieve previously unattainable results in\nobject-style mixing using text-to-image models. Project page:\nhttps://prompt-plus.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1\">Andrey Voynov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qinghao Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1\">Kfir Aberman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v5 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08614","description":"<p>Question answering over knowledge graphs and other RDF data has been greatly\nadvanced, with a number of good systems providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, systems from the IR\nand NLP communities have addressed QA over text, but such systems barely\nutilize semantic data and knowledge. This paper presents the first system for\ncomplex questions that can seamlessly operate over a mixture of RDF datasets\nand text corpora, or individual sources, in a unified framework. Our method,\ncalled UNIQORN, builds a context graph on-the-fly, by retrieving\nquestion-relevant evidences from the RDF data and/or a text corpus, using\nfine-tuned BERT models. The resulting graph is typically rich but highly noisy.\nUNIQORN copes with this input by a graph algorithm for Group Steiner Trees,\nthat identifies the best answer candidates in the context graph. Experimental\nresults on several benchmarks of complex questions with multiple entities and\nrelations, show that \\uniqorn significantly outperforms state-of-the-art\nmethods for QA over heterogeneous sources. The graph-based methodology provides\nuser-interpretable evidence for the complete answering process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Soumajit Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01636","description":"<p>With the fast development of Deep Learning techniques, Named Entity\nRecognition (NER) is becoming more and more important in the information\nextraction task. The greatest difficulty that the NER task faces is to keep the\ndetectability even when types of NE and documents are unfamiliar. Realizing\nthat the specificity information may contain potential meanings of a word and\ngenerate semantic-related features for word embedding, we develop a\ndistribution-aware word embedding and implement three different methods to make\nuse of the distribution information in a NER framework. And the result shows\nthat the performance of NER will be improved if the word specificity is\nincorporated into existing NER methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-linguistic differences in gender congruency effects: Evidence from meta-analyses. (arXiv:2109.03490v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03490","description":"<p>It has been proposed that the order in which words are prepared for\nproduction depends on the speaker's language. When producing the translation\nequivalent of the small cat, speakers of German or Dutch select the\ngender-marked determiner at a relatively early stage of production. Speakers of\nFrench or Italian postpone the encoding of a determiner or adjective until the\nphonological form of the noun is available. Hence, even though the words are\nproduced in the same order (e.g., die kleine Katze in German, le petit chat in\nFrench), they are not planned in the same order and might require different\namounts of advanced planning prior to production onset. This distinction\nbetween early and late selection languages was proposed to account for the\nobservation that speakers of Germanic and Slavic languages, but not of Romance\nlanguages, are slower to name pictures in the context of a distractor word of a\ndifferent gender. Meta-analyses are conducted to provide the first direct test\nof this cross-linguistic difference and to test a prediction of the late\nselection hypothesis. They confirm the existence of the gender congruency\neffect in German/Slavic languages and its absence in Romance languages when\ntarget and distractor words are presented simultaneously. They do not allow\nconfirming the hypothesis that in the latter languages, a similar effect\nemerges when the presentation of the distractor is delayed. Overall, these\nanalyses confirm the cross-linguistic difference but show that the evidence\navailable to date is not sufficient to confirm or reject the late selection\nhypothesis as an explanation of this difference. We highlight specific\ndirections for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burki_A/0/1/0/all/0/1\">Audrey B&#xfc;rki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoven_E/0/1/0/all/0/1\">Emiel van den Hoven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiller_N/0/1/0/all/0/1\">Niels O. Schiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DImitrov_N/0/1/0/all/0/1\">Nikolay DImitrov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06537","description":"<p>The conventional wisdom behind learning deep classification models is to\nfocus on bad-classified examples and ignore well-classified examples that are\nfar from the decision boundary. For instance, when training with cross-entropy\nloss, examples with higher likelihoods (i.e., well-classified examples)\ncontribute smaller gradients in back-propagation. However, we theoretically\nshow that this common practice hinders representation learning, energy\noptimization, and margin growth. To counteract this deficiency, we propose to\nreward well-classified examples with additive bonuses to revive their\ncontribution to the learning process. This counterexample theoretically\naddresses these three issues. We empirically support this claim by directly\nverifying the theoretical results or significant performance improvement with\nour counterexample on diverse tasks, including image classification, graph\nclassification, and machine translation. Furthermore, this paper shows that we\ncan deal with complex scenarios, such as imbalanced classification, OOD\ndetection, and applications under adversarial attacks because our idea can\nsolve these three issues. Code is available at:\nhttps://github.com/lancopku/well-classified-examples-are-underestimated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Adults Understand What Young Children Say. (arXiv:2206.07807v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.07807","description":"<p>Children's early speech often bears little resemblance to that of adults, and\nyet parents and other caregivers are able to interpret that speech and react\naccordingly. Here we investigate how these adult inferences as listeners\nreflect sophisticated beliefs about what children are trying to communicate, as\nwell as how children are likely to pronounce words. Using a Bayesian framework\nfor modeling spoken word recognition, we find that computational models can\nreplicate adult interpretations of children's speech only when they include\nstrong, context-specific prior expectations about the messages that children\nwill want to communicate. This points to a critical role of adult cognitive\nprocesses in supporting early communication and reveals how children can\nactively prompt adults to take actions on their behalf even when they have only\na nascent understanding of the adult language. We discuss the wide-ranging\nimplications of the powerful listening capabilities of adults for theories of\nfirst language acquisition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meylan_S/0/1/0/all/0/1\">Stephan C. Meylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foushee_R/0/1/0/all/0/1\">Ruthe Foushee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1\">Nicole H. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergelson_E/0/1/0/all/0/1\">Elika Bergelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger P. Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Explanation: New Prompting Method to Generate Higher Quality Natural Language Explanation for Implicit Hate Speech. (arXiv:2209.04889v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.04889","description":"<p>Recent studies have exploited advanced generative language models to generate\nNatural Language Explanations (NLE) for why a certain text could be hateful. We\npropose the Chain of Explanation (CoE) Prompting method, using the heuristic\nwords and target group, to generate high-quality NLE for implicit hate speech.\nWe improved the BLUE score from 44.0 to 62.3 for NLE generation by providing\naccurate target information. We then evaluate the quality of generated NLE\nusing various automatic metrics and human annotations of informativeness and\nclarity scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Haewoon Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1\">Jisun An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing Methods to Identify Oncology Patients at High Risk for Acute Care with Clinical Notes. (arXiv:2209.13860v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.13860","description":"<p>Clinical notes are an essential component of a health record. This paper\nevaluates how natural language processing (NLP) can be used to identify the\nrisk of acute care use (ACU) in oncology patients, once chemotherapy starts.\nRisk prediction using structured health data (SHD) is now standard, but\npredictions using free-text formats are complex. This paper explores the use of\nfree-text notes for the prediction of ACU instead of SHD. Deep Learning models\nwere compared to manually engineered language features. Results show that SHD\nmodels minimally outperform NLP models; an l1-penalised logistic regression\nwith SHD achieved a C-statistic of 0.748 (95%-CI: 0.735, 0.762), while the same\nmodel with language features achieved 0.730 (95%-CI: 0.717, 0.745) and a\ntransformer-based model achieved 0.702 (95%-CI: 0.688, 0.717). This paper shows\nhow language models can be used in clinical applications and underlines how\nrisk bias is different for diverse patient groups, even using only free-text\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fanconi_C/0/1/0/all/0/1\">Claudio Fanconi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchem_M/0/1/0/all/0/1\">Marieke van Buchem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Boussard_T/0/1/0/all/0/1\">Tina Hernandez-Boussard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Decoding as Likelihood-Utility Alignment. (arXiv:2210.07228v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07228","description":"<p>A critical component of a successful language generation pipeline is the\ndecoding algorithm. However, the general principles that should guide the\nchoice of a decoding algorithm remain unclear. Previous works only compare\ndecoding algorithms in narrow scenarios, and their findings do not generalize\nacross tasks. We argue that the misalignment between the model's likelihood and\nthe task-specific notion of utility is the key factor to understanding the\neffectiveness of decoding algorithms. To structure the discussion, we introduce\na taxonomy of misalignment mitigation strategies (MMSs), providing a unifying\nview of decoding as a tool for alignment. The MMS taxonomy groups decoding\nalgorithms based on their implicit assumptions about likelihood--utility\nmisalignment, yielding general statements about their applicability across\ntasks. Specifically, by analyzing the correlation between the likelihood and\nthe utility of predictions across a diverse set of tasks, we provide empirical\nevidence supporting the proposed taxonomy and a set of principles to structure\nreasoning when choosing a decoding algorithm. Crucially, our analysis is the\nfirst to relate likelihood-based decoding algorithms with algorithms that rely\non external information, such as value-guided methods and prompting, and covers\nthe most diverse set of tasks to date. Code, data, and models are available at\nhttps://github.com/epfl-dlab/understanding-decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Josifoski_M/0/1/0/all/0/1\">Martin Josifoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajic_F/0/1/0/all/0/1\">Frano Rajic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1\">Debjit Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_V/0/1/0/all/0/1\">Valentin Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Barun Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiciman_E/0/1/0/all/0/1\">Emre K&#x131;c&#x131;man</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via Contrastive Prompt-Tuning. (arXiv:2211.11337v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.11337","description":"<p>Large-scale text-to-image generation models have achieved remarkable progress\nin synthesizing high-quality, feature-rich images with high resolution guided\nby texts. However, these models often struggle with novel concepts, eg, new\nstyles, object entities, etc. Although recent attempts have employed\nfine-tuning or prompt-tuning strategies to teach the pre-trained diffusion\nmodel novel concepts from a reference image set,they have the drawback of\noverfitting to the given reference images, particularly in one-shot\napplications, which is harmful to generate diverse and high-quality images\nwhile maintaining generation controllability.\n</p>\n<p>To tackle this challenge, we present a simple yet effective method called\nDreamArtist, which employs a positive-negative prompt-tuning learning strategy.\nSpecifically, DreamArtist incorporates both positive and negative embeddings\nand jointly trains them. The positive embedding aggressively captures the\nsalient characteristics of the reference image to drive diversified generation\nand the negative embedding rectifies inadequacies from the positive embedding.\nIt learns not only what is correct, but also what can be avoided or improved.\nWe have conducted extensive experiments and evaluated the proposed method from\nimage similarity and diversity, generation controllability, and style cloning.\nAnd our DreamArtist has achieved a superior generation performance over\nexisting methods. Besides, our additional evaluation on extended tasks,\nincluding concept compositions and prompt-guided image editing, demonstrates\nits effectiveness for more applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Ziyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Pengxu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automaton-Based Representations of Task Knowledge from Generative Language Models. (arXiv:2212.01944v3 [cs.FL] UPDATED)","link":"http://arxiv.org/abs/2212.01944","description":"<p>Automaton-based representations of task knowledge play an important role in\ncontrol and planning for sequential decision-making problems. However,\nobtaining the high-level task knowledge required to build such automata is\noften difficult. Meanwhile, large-scale generative language models (GLMs) can\nautomatically generate relevant task knowledge. However, the textual outputs\nfrom GLMs cannot be formally verified or used for sequential decision-making.\nWe propose a novel algorithm named GLM2FSA, which constructs a finite state\nautomaton (FSA) encoding high-level task knowledge from a brief\nnatural-language description of the task goal. GLM2FSA first sends queries to a\nGLM to extract task knowledge in textual form, and then it builds an FSA to\nrepresent this text-based knowledge. The proposed algorithm thus fills the gap\nbetween natural-language task descriptions and automaton-based representations,\nand the constructed FSA can be formally verified against user-defined\nspecifications. We accordingly propose a method to iteratively refine the\nqueries to the GLM based on the outcomes, e.g., counter-examples, from\nverification. We demonstrate GLM2FSA's ability to build and refine\nautomaton-based representations of everyday tasks (e.g., crossing a road or\nmaking a phone call), and also of tasks that require highly-specialized\nknowledge (e.g., executing secure multi-party computation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaglione_J/0/1/0/all/0/1\">Jean-Rapha&#xeb;l Gaglione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neary_C/0/1/0/all/0/1\">Cyrus Neary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training. (arXiv:2212.02691v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.02691","description":"<p>Transformers are widely used in NLP tasks. However, current approaches to\nleveraging transformers to understand language expose one weak spot: Number\nunderstanding. In some scenarios, numbers frequently occur, especially in\nsemi-structured data like tables. But current approaches to rich-number tasks\nwith transformer-based language models abandon or lose some of the numeracy\ninformation - e.g., breaking numbers into sub-word tokens - which leads to many\nnumber-related errors. In this paper, we propose the LUNA framework which\nimproves the numerical reasoning and calculation capabilities of\ntransformer-based language models. With the number plugin of NumTok and NumBed,\nLUNA represents each number as a whole to model input. With number\npre-training, including regression loss and model distillation, LUNA bridges\nthe gap between number and vocabulary embeddings. To the best of our knowledge,\nthis is the first work that explicitly injects numeracy capability into\nlanguage models using Number Plugins. Besides evaluating toy models on toy\ntasks, we evaluate LUNA on three large-scale transformer models (RoBERTa, BERT,\nTabBERT) over three different downstream tasks (TATQA, TabFact, CrediTrans),\nand observe the performances of language models are constantly improved by\nLUNA. The augmented models also improve the official baseline of TAT-QA (EM:\n50.15 -&gt; 59.58) and achieve SOTA performance on CrediTrans (F1 = 86.17).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hongwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jialiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yijia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-Guided Fair Classification for Natural Language Processing. (arXiv:2212.10154v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10154","description":"<p>Text classifiers have promising applications in high-stake tasks such as\nresume screening and content moderation. These classifiers must be fair and\navoid discriminatory decisions by being invariant to perturbations of sensitive\nattributes such as gender or ethnicity. However, there is a gap between human\nintuition about these perturbations and the formal similarity specifications\ncapturing them. While existing research has started to address this gap,\ncurrent methods are based on hardcoded word replacements, resulting in\nspecifications with limited expressivity or ones that fail to fully align with\nhuman intuition (e.g., in cases of asymmetric counterfactuals). This work\nproposes novel methods for bridging this gap by discovering expressive and\nintuitive individual fairness specifications. We show how to leverage\nunsupervised style transfer and GPT-3's zero-shot capabilities to automatically\ngenerate expressive candidate pairs of semantically similar sentences that\ndiffer along sensitive attributes. We then validate the generated pairs via an\nextensive crowdsourcing study, which confirms that a lot of these pairs align\nwith human intuition about fairness in the context of toxicity classification.\nFinally, we show how limited amounts of human feedback can be leveraged to\nlearn a similarity specification that can be used to train downstream\nfairness-aware models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dorner_F/0/1/0/all/0/1\">Florian E.Dorner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peychev_M/0/1/0/all/0/1\">Momchil Peychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstantinov_N/0/1/0/all/0/1\">Nikola Konstantinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_N/0/1/0/all/0/1\">Naman Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ash_E/0/1/0/all/0/1\">Elliott Ash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1\">Martin Vechev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing. (arXiv:2301.04558v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2301.04558","description":"<p>Self-supervised learning in vision-language processing exploits semantic\nalignment between imaging and text modalities. Prior work in biomedical VLP has\nmostly relied on the alignment of single image and report pairs even though\nclinical notes commonly refer to prior images. This does not only introduce\npoor alignment between the modalities but also a missed opportunity to exploit\nrich self-supervision through existing temporal content in the data. In this\nwork, we explicitly account for prior images and reports when available during\nboth training and fine-tuning. Our approach, named BioViL-T, uses a\nCNN-Transformer hybrid multi-image encoder trained jointly with a text model.\nIt is designed to be versatile to arising challenges such as pose variations\nand missing input images across time. The resulting model excels on downstream\ntasks both in single- and multi-image setups, achieving state-of-the-art\nperformance on (I) progression classification, (II) phrase grounding, and (III)\nreport generation, whilst offering consistent improvements on disease\nclassification and sentence-similarity tasks. We release a novel multi-modal\ntemporal benchmark dataset, MS-CXR-T, to quantify the quality of\nvision-language representations in terms of temporal semantics. Our\nexperimental results show the advantages of incorporating prior images and\nreports to make most use of the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyland_S/0/1/0/all/0/1\">Stephanie Hyland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianchu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1\">Fernando P&#xe9;rez-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilse_M/0/1/0/all/0/1\">Maximilian Ilse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1\">Harshita Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouzid_K/0/1/0/all/0/1\">Kenza Bouzid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thieme_A/0/1/0/all/0/1\">Anja Thieme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1\">Maria Wetscherek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Encoders for Streaming Sequence Tagging. (arXiv:2301.09244v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.09244","description":"<p>A naive application of state-of-the-art bidirectional encoders for streaming\nsequence tagging would require encoding each token from scratch for each new\ntoken in an incremental streaming input (like transcribed speech). The lack of\nre-usability of previous computation leads to a higher number of Floating Point\nOperations (or FLOPs) and higher number of unnecessary label flips. Increased\nFLOPs consequently lead to higher wall-clock time and increased label flipping\nleads to poorer streaming performance. In this work, we present a Hybrid\nEncoder with Adaptive Restart (HEAR) that addresses these issues while\nmaintaining the performance of bidirectional encoders over the offline (or\ncomplete) inputs while improving performance on streaming (or incomplete)\ninputs. HEAR has a Hybrid unidirectional-bidirectional encoder architecture to\nperform sequence tagging, along with an Adaptive Restart Module (ARM) to\nselectively guide the restart of bidirectional portion of the encoder. Across\nfour sequence tagging tasks, HEAR offers FLOP savings in streaming settings\nupto 71.1% and also outperforms bidirectional encoders for streaming\npredictions by upto +10% streaming exact match.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaushal_A/0/1/0/all/0/1\">Ayush Kaushal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aditya Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Shyam Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faruqui_M/0/1/0/all/0/1\">Manaal Faruqui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Very Large Pretrained Language Models Learn Storytelling With A Few Examples?. (arXiv:2301.09790v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.09790","description":"<p>While pre-trained language models can generate individually fluent sentences\nfor automatic story generation, they struggle to generate stories that are\ncoherent, sensible and interesting. Current state-of-the-art (SOTA) story\ngeneration models explore using higher-level features such as plots or\ncommonsense knowledge to improve the quality of generated stories. Prompt-based\nlearning using very large pre-trained language models (VLPLMs) such as GPT3 has\ndemonstrated impressive performance even across various NLP tasks. In this\npaper, we present an extensive study using automatic and human evaluation to\ncompare the story generation capability of VLPLMs to those SOTA models in three\ndifferent datasets where stories differ in style, register and length. Our\nresults show that VLPLMs generate much higher quality stories than other story\ngeneration models, and to a certain extent rival human authors, although\npreliminary investigation also reveals that they tend to ``plagiarise'' real\nstories in scenarios that involve world knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhuohan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Learning Siamese Network for Few-Shot Text Classification. (arXiv:2302.03507v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03507","description":"<p>Few-shot learning has been used to tackle the problem of label scarcity in\ntext classification, of which meta-learning based methods have shown to be\neffective, such as the prototypical networks (PROTO). Despite the success of\nPROTO, there still exist three main problems: (1) ignore the randomness of the\nsampled support sets when computing prototype vectors; (2) disregard the\nimportance of labeled samples; (3) construct meta-tasks in a purely random\nmanner. In this paper, we propose a Meta-Learning Siamese Network, namely,\nMeta-SN, to address these issues. Specifically, instead of computing prototype\nvectors from the sampled support sets, Meta-SN utilizes external knowledge\n(e.g. class names and descriptive texts) for class labels, which is encoded as\nthe low-dimensional embeddings of prototype vectors. In addition, Meta-SN\npresents a novel sampling strategy for constructing meta-tasks, which gives\nhigher sampling probabilities to hard-to-classify samples. Extensive\nexperiments are conducted on six benchmark datasets to show the clear\nsuperiority of Meta-SN over other state-of-the-art models. For reproducibility,\nall the datasets and codes are provided at https://github.com/hccngu/Meta-SN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chengcheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingnan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Ming Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aoying Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Chat Assistants can Improve Conversations about Divisive Topics. (arXiv:2302.07268v4 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2302.07268","description":"<p>A rapidly increasing amount of human conversation occurs online. But\ndivisiveness and conflict can fester in text-based interactions on social media\nplatforms, in messaging apps, and on other digital forums. Such toxicity\nincreases polarization and, importantly, corrodes the capacity of diverse\nsocieties to develop efficient solutions to complex social problems that impact\neveryone. Scholars and civil society groups promote interventions that can make\ninterpersonal conversations less divisive or more productive in offline\nsettings, but scaling these efforts to the amount of discourse that occurs\nonline is extremely challenging. We present results of a large-scale experiment\nthat demonstrates how online conversations about divisive topics can be\nimproved with artificial intelligence tools. Specifically, we employ a large\nlanguage model to make real-time, evidence-based recommendations intended to\nimprove participants' perception of feeling understood in conversations. We\nfind that these interventions improve the reported quality of the conversation,\nreduce political divisiveness, and improve the tone, without systematically\nchanging the content of the conversation or moving people's policy attitudes.\nThese findings have important implications for future research on social media,\npolitical deliberation, and the growing community of scholars interested in the\nplace of artificial intelligence within computational social science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Argyle_L/0/1/0/all/0/1\">Lisa P. Argyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busby_E/0/1/0/all/0/1\">Ethan Busby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gubler_J/0/1/0/all/0/1\">Joshua Gubler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bail_C/0/1/0/all/0/1\">Chris Bail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howe_T/0/1/0/all/0/1\">Thomas Howe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rytting_C/0/1/0/all/0/1\">Christopher Rytting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wingate_D/0/1/0/all/0/1\">David Wingate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech. (arXiv:2302.07736v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07736","description":"<p>Recent studies have alarmed that many online hate speeches are implicit. With\nits subtle nature, the explainability of the detection of such hateful speech\nhas been a challenging problem. In this work, we examine whether ChatGPT can be\nused for providing natural language explanations (NLEs) for implicit hateful\nspeech detection. We design our prompt to elicit concise ChatGPT-generated NLEs\nand conduct user studies to evaluate their qualities by comparison with\nhuman-written NLEs. We discuss the potential and limitations of ChatGPT in the\ncontext of implicit hateful speech research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Haewoon Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1\">Jisun An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"See Your Heart: Psychological states Interpretation through Visual Creations. (arXiv:2302.10276v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2302.10276","description":"<p>In psychoanalysis, generating interpretations to one's psychological state\nthrough visual creations is facing significant demands. The two main tasks of\nexisting studies in the field of computer vision, sentiment/emotion\nclassification and affective captioning, can hardly satisfy the requirement of\npsychological interpreting. To meet the demands for psychoanalysis, we\nintroduce a challenging task, \\textbf{V}isual \\textbf{E}motion\n\\textbf{I}nterpretation \\textbf{T}ask (VEIT). VEIT requires AI to generate\nreasonable interpretations of creator's psychological state through visual\ncreations. To support the task, we present a multimodal dataset termed SpyIn\n(\\textbf{S}and\\textbf{p}la\\textbf{y} \\textbf{In}terpretation Dataset), which is\npsychological theory supported and professional annotated. Dataset analysis\nillustrates that SpyIn is not only able to support VEIT, but also more\nchallenging compared with other captioning datasets. Building on SpyIn, we\nconduct experiments of several image captioning method, and propose a\nvisual-semantic combined model which obtains a SOTA result on SpyIn. The\nresults indicate that VEIT is a more challenging task requiring scene graph\ninformation and psychological knowledge. Our work also show a promise for AI to\nanalyze and explain inner world of humanity through visual creations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Likun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaokun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaotang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaiqi Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering. (arXiv:2303.01903v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.01903","description":"<p>Knowledge-based visual question answering (VQA) requires external knowledge\nbeyond the image to answer the question. Early studies retrieve required\nknowledge from explicit knowledge bases (KBs), which often introduces\nirrelevant information to the question, hence restricting the performance of\ntheir models. Recent works have sought to use a large language model (i.e.,\nGPT-3) as an implicit knowledge engine to acquire the necessary knowledge for\nanswering. Despite the encouraging results achieved by these methods, we argue\nthat they have not fully activated the capacity of GPT-3 as the provided input\ninformation is insufficient. In this paper, we present Prophet -- a\nconceptually simple framework designed to prompt GPT-3 with answer heuristics\nfor knowledge-based VQA. Specifically, we first train a vanilla VQA model on a\nspecific knowledge-based VQA dataset without external knowledge. After that, we\nextract two types of complementary answer heuristics from the model: answer\ncandidates and answer-aware examples. Finally, the two types of answer\nheuristics are encoded into the prompts to enable GPT-3 to better comprehend\nthe task thus enhancing its capacity. Prophet significantly outperforms all\nexisting state-of-the-art methods on two challenging knowledge-based VQA\ndatasets, OK-VQA and A-OKVQA, delivering 61.1% and 55.7% accuracies on their\ntesting sets, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhenwei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis. (arXiv:2303.02563v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.02563","description":"<p>This paper presents a novel approach for explainability in financial analysis\nby utilizing the Pearson correlation coefficient to establish a relationship\nbetween aspect-based sentiment analysis and stock prices. The proposed\nmethodology involves constructing an aspect list from financial news articles\nand analyzing sentiment intensity scores for each aspect. These scores are then\ncompared to the stock prices for the relevant companies using the Pearson\ncoefficient to determine any significant correlations. The results indicate\nthat the proposed approach provides a more detailed and accurate understanding\nof the relationship between sentiment analysis and stock prices, which can be\nuseful for investors and financial analysts in making informed decisions.\nAdditionally, this methodology offers a transparent and interpretable way to\nexplain the sentiment analysis results and their impact on stock prices.\nOverall, the findings of this paper demonstrate the importance of\nexplainability in financial analysis and highlight the potential benefits of\nutilizing the Pearson coefficient for analyzing aspect-based sentiment analysis\nand stock prices. The proposed approach offers a valuable tool for\nunderstanding the complex relationships between financial news sentiment and\nstock prices, providing a new perspective on the financial market and aiding in\nmaking informed investment decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ong_K/0/1/0/all/0/1\">Keane Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heever_W/0/1/0/all/0/1\">Wihan van der Heever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satapathy_R/0/1/0/all/0/1\">Ranjan Satapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mengaldo_G/0/1/0/all/0/1\">Gianmarco Mengaldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Study on Post-Training Quantization for Large Language Models. (arXiv:2303.08302v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.08302","description":"<p>Post-training quantization (\\ptq) had been recently shown as a compromising\nmethod to reduce memory consumption and/or compute cost for large language\nmodels. However, a comprehensive study about the effect of different\nquantization schemes, different model families, different \\ptq methods,\ndifferent quantization bit precision, etc, is still missing. In this work, we\nprovide an extensive study of those components over tens of thousands of\nzero-shot experiments. Our results show that (1) Fine-grained quantization and\n\\ptq methods (instead of naive round-to-nearest quantization) are necessary to\nachieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained\nquantization is more powerful than lower bits (e.g., 4 bits) with very\nfine-grained quantization (whose effective bit precision is similar to 5 bits).\nWe also present recommendations about how to utilize quantization for \\llms\nwith different sizes, and leave suggestions of future opportunities and system\nwork that are not resolved in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youn_S/0/1/0/all/0/1\">Stephen Youn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FactReranker: Fact-guided Reranker for Faithful Radiology Report Summarization. (arXiv:2303.08335v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08335","description":"<p>Automatic radiology report summarization is a crucial clinical task, whose\nkey challenge is to maintain factual accuracy between produced summaries and\nground truth radiology findings. Existing research adopts reinforcement\nlearning to directly optimize factual consistency metrics such as CheXBert or\nRadGraph score. However, their decoding method using greedy search or beam\nsearch considers no factual consistency when picking the optimal candidate,\nleading to limited factual consistency improvement. To address it, we propose a\nnovel second-stage summarizing approach FactReranker, the first attempt that\nlearns to choose the best summary from all candidates based on their estimated\nfactual consistency score. We propose to extract medical facts of the input\nmedical report, its gold summary, and candidate summaries based on the RadGraph\nschema and design the fact-guided reranker to efficiently incorporate the\nextracted medical facts for selecting the optimal summary. We decompose the\nfact-guided reranker into the factual knowledge graph generation and the\nfactual scorer, which allows the reranker to model the mapping between the\nmedical facts of the input text and its gold summary, thus can select the\noptimal summary even the gold summary can't be observed during inference. We\nalso present a fact-based ranking metric (RadMRR) for measuring the ability of\nthe reranker on selecting factual consistent candidates. Experimental results\non two benchmark datasets demonstrate the superiority of our method in\ngenerating summaries with higher factual consistency scores when compared with\nexisting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiayu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-4 Technical Report. (arXiv:2303.08774v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08774","description":"<p>We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+OpenAI/0/1/0/all/0/1\">OpenAI</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Sea of Words: An In-Depth Analysis of Anchors for Text Data. (arXiv:2205.13789v2 [stat.ML] CROSS LISTED)","link":"http://arxiv.org/abs/2205.13789","description":"<p>Anchors (Ribeiro et al., 2018) is a post-hoc, rule-based interpretability\nmethod. For text data, it proposes to explain a decision by highlighting a\nsmall set of words (an anchor) such that the model to explain has similar\noutputs when they are present in a document. In this paper, we present the\nfirst theoretical analysis of Anchors, considering that the search for the best\nanchor is exhaustive. After formalizing the algorithm for text classification,\nwe present explicit results on different classes of models when the\nvectorization step is TF-IDF, and words are replaced by a fixed\nout-of-dictionary token when removed. Our inquiry covers models such as\nelementary if-then rules and linear classifiers. We then leverage this analysis\nto gain insights on the behavior of Anchors for any differentiable classifiers.\nFor neural networks, we empirically show that the words corresponding to the\nhighest partial derivatives of the model with respect to the input, reweighted\nby the inverse document frequencies, are selected by Anchors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Lopardo_G/0/1/0/all/0/1\">Gianluigi Lopardo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Precioso_F/0/1/0/all/0/1\">Frederic Precioso</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Garreau_D/0/1/0/all/0/1\">Damien Garreau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Feature Importance and Rule Extraction for Interpretability on Text Data. (arXiv:2207.01420v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2207.01420","description":"<p>Complex machine learning algorithms are used more and more often in critical\ntasks involving text data, leading to the development of interpretability\nmethods. Among local methods, two families have emerged: those computing\nimportance scores for each feature and those extracting simple logical rules.\nIn this paper we show that using different methods can lead to unexpectedly\ndifferent explanations, even when applied to simple models for which we would\nexpect qualitative coincidence. To quantify this effect, we propose a new\napproach to compare explanations produced by different methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopardo_G/0/1/0/all/0/1\">Gianluigi Lopardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garreau_D/0/1/0/all/0/1\">Damien Garreau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Formal Algebraic Framework for DSL Composition. (arXiv:2302.00744v1 [math.CT] CROSS LISTED)","link":"http://arxiv.org/abs/2302.00744","description":"<p>We discuss a formal framework for using algebraic structures to model a\nmeta-language that can write, compose, and provide interoperability between\nabstractions of DSLs. The purpose of this formal framework is to provide a\nverification of compositional properties of the meta-language. Throughout our\npaper we discuss the construction of this formal framework, as well its\nrelation to our team's work on the DARPA V-SPELLS program via the pipeline we\nhave developed for completing our verification tasking on V-SPELLS. We aim to\ngive a broad overview of this verification pipeline in our paper. The pipeline\ncan be split into four main components: the first is providing a formal model\nof the meta-language in Coq; the second is to give a specification in Coq of\nour chosen algebraic structures; third, we need to implement specific instances\nof our algebraic structures in Coq, as well as give a proof in Coq that this\nimplementation is an algebraic structure according to our specification in the\nsecond step; and lastly, we need to give a proof in Coq that the formal model\nfor the meta-language in the first step is an instance of the implementation in\nthe third step.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Flores_Z/0/1/0/all/0/1\">Zachary Flores</a>, <a href=\"http://arxiv.org/find/math/1/au:+Taranto_A/0/1/0/all/0/1\">Angelo Taranto</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bond_E/0/1/0/all/0/1\">Eric Bond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Post-hoc Explainers: The Case of Anchors. (arXiv:2303.08806v1 [stat.ML] CROSS LISTED)","link":"http://arxiv.org/abs/2303.08806","description":"<p>In many scenarios, the interpretability of machine learning models is a\nhighly required but difficult task. To explain the individual predictions of\nsuch models, local model-agnostic approaches have been proposed. However, the\nprocess generating the explanations can be, for a user, as mysterious as the\nprediction to be explained. Furthermore, interpretability methods frequently\nlack theoretical guarantees, and their behavior on simple models is frequently\nunknown. While it is difficult, if not impossible, to ensure that an explainer\nbehaves as expected on a cutting-edge model, we can at least ensure that\neverything works on simple, already interpretable models. In this paper, we\npresent a theoretical analysis of Anchors (Ribeiro et al., 2018): a popular\nrule-based interpretability method that highlights a small set of words to\nexplain a text classifier's decision. After formalizing its algorithm and\nproviding useful insights, we demonstrate mathematically that Anchors produces\nmeaningful results when used with linear text classifiers on top of a TF-IDF\nvectorization. We believe that our analysis framework can aid in the\ndevelopment of new explainability methods based on solid theoretical\nfoundations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Lopardo_G/0/1/0/all/0/1\">Gianluigi Lopardo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Precioso_F/0/1/0/all/0/1\">Frederic Precioso</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Garreau_D/0/1/0/all/0/1\">Damien Garreau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}