{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-10T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Detecting and Reasoning of Deleted Tweets before they are Posted. (arXiv:2305.04927v1 [cs.CL])","link":"http://arxiv.org/abs/2305.04927","description":"<p>Social media platforms empower us in several ways, from information\ndissemination to consumption. While these platforms are useful in promoting\ncitizen journalism, public awareness etc., they have misuse potentials.\nMalicious users use them to disseminate hate-speech, offensive content, rumor\netc. to gain social and political agendas or to harm individuals, entities and\norganizations. Often times, general users unconsciously share information\nwithout verifying it, or unintentionally post harmful messages. Some of such\ncontent often get deleted either by the platform due to the violation of terms\nand policies, or users themselves for different reasons, e.g., regrets. There\nis a wide range of studies in characterizing, understanding and predicting\ndeleted content. However, studies which aims to identify the fine-grained\nreasons (e.g., posts are offensive, hate speech or no identifiable reason)\nbehind deleted content, are limited. In this study we address this gap, by\nidentifying deleted tweets, particularly within the Arabic context, and\nlabeling them with a corresponding fine-grained disinformation category. We\nthen develop models that can predict the potentiality of tweets getting\ndeleted, as well as the potential reasons behind deletion. Such models can help\nin moderating social media posts before even posting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdaljalil_S/0/1/0/all/0/1\">Samir Abdaljalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassar_A/0/1/0/all/0/1\">Azza Nassar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])","link":"http://arxiv.org/abs/2305.04928","description":"<p>Supervised named entity recognition (NER) in the biomedical domain is\ndependent on large sets of annotated texts with the given named entities, whose\ncreation can be time-consuming and expensive. Furthermore, the extraction of\nnew entities often requires conducting additional annotation tasks and\nretraining the model. To address these challenges, this paper proposes a\ntransformer-based method for zero- and few-shot NER in the biomedical domain.\nThe method is based on transforming the task of multi-class token\nclassification into binary token classification (token contains the searched\nentity or does not contain the searched entity) and pre-training on a larger\namount of datasets and biomedical entities, from where the method can learn\nsemantic relations between the given and potential classes. We have achieved\naverage F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94%\nfor 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical\nentities with PubMedBERT fine-tuned model. The results demonstrate the\neffectiveness of the proposed method for recognizing new entities with limited\nexamples, with comparable or better results from the state-of-the-art zero- and\nfew-shot NER methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kosprdic_M/0/1/0/all/0/1\">Milo&#x161; Ko&#x161;prdi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prodanovic_N/0/1/0/all/0/1\">Nikola Prodanovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ljajic_A/0/1/0/all/0/1\">Adela Ljaji&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basaragin_B/0/1/0/all/0/1\">Bojana Ba&#x161;aragin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milosevic_N/0/1/0/all/0/1\">Nikola Milo&#x161;evi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification. (arXiv:2305.04940v1 [cs.SE])","link":"http://arxiv.org/abs/2305.04940","description":"<p>The use of modern Natural Language Processing (NLP) techniques has shown to\nbe beneficial for software engineering tasks, such as vulnerability detection\nand type inference. However, training deep NLP models requires significant\ncomputational resources. This paper explores techniques that aim at achieving\nthe best usage of resources and available information in these models.\n</p>\n<p>We propose a generic approach, EarlyBIRD, to build composite representations\nof code from the early layers of a pre-trained transformer model. We\nempirically investigate the viability of this approach on the CodeBERT model by\ncomparing the performance of 12 strategies for creating composite\nrepresentations with the standard practice of only using the last encoder\nlayer.\n</p>\n<p>Our evaluation on four datasets shows that several early layer combinations\nyield better performance on defect detection, and some combinations improve\nmulti-class classification. More specifically, we obtain a +2 average\nimprovement of detection accuracy on Devign with only 3 out of 12 layers of\nCodeBERT and a 3.3x speed-up of fine-tuning. These findings show that early\nlayers can be used to obtain better results using the same resources, as well\nas to reduce resource usage during fine-tuning and inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grishina_A/0/1/0/all/0/1\">Anastasiia Grishina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hort_M/0/1/0/all/0/1\">Max Hort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moonen_L/0/1/0/all/0/1\">Leon Moonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Moment Retrieval and Highlight Detection Via Natural Language Queries. (arXiv:2305.04961v1 [cs.CV])","link":"http://arxiv.org/abs/2305.04961","description":"<p>Video summarization has become an increasingly important task in the field of\ncomputer vision due to the vast amount of video content available on the\ninternet. In this project, we propose a new method for natural language query\nbased joint video summarization and highlight detection using multi-modal\ntransformers. This approach will use both visual and audio cues to match a\nuser's natural language query to retrieve the most relevant and interesting\nmoments from a video. Our approach employs multiple recent techniques used in\nVision Transformers (ViTs) to create a transformer-like encoder-decoder model.\nWe evaluated our approach on multiple datasets such as YouTube Highlights and\nTVSum to demonstrate the flexibility of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Richard Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1\">Austin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yap_H/0/1/0/all/0/1\">Heidi Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beard_K/0/1/0/all/0/1\">Koby Beard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization. (arXiv:2305.04971v1 [cs.LG])","link":"http://arxiv.org/abs/2305.04971","description":"<p>Regularization techniques are crucial to improving the generalization\nperformance and training efficiency of deep neural networks. Many deep learning\nalgorithms rely on weight decay, dropout, batch/layer normalization to converge\nfaster and generalize. Label Smoothing (LS) is another simple, versatile and\nefficient regularization which can be applied to various supervised\nclassification tasks. Conventional LS, however, regardless of the training\ninstance assumes that each non-target class is equally likely. In this work, we\npresent a general framework for training with label regularization, which\nincludes conventional LS but can also model instance-specific variants. Based\non this formulation, we propose an efficient way of learning LAbel\nregularization by devising a Bi-level Optimization (LABO) problem. We derive a\ndeterministic and interpretable solution of the inner loop as the optimal label\nsmoothing without the need to store the parameters or the output of a trained\nmodel. Finally, we conduct extensive experiments and demonstrate our LABO\nconsistently yields improvement over conventional label regularization on\nvarious fields, including seven machine translation and three image\nclassification tasks across various\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Peng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobyzev_I/0/1/0/all/0/1\">Ivan Kobyzev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge. (arXiv:2305.04978v1 [cs.CL])","link":"http://arxiv.org/abs/2305.04978","description":"<p>Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is\nan essential component of our world knowledge, yet understudied in prior\nliterature. In this paper, we study the task of comparative knowledge\nacquisition, motivated by the dramatic improvements in the capabilities of\nextreme-scale language models like GPT-3, which have fueled efforts towards\nharvesting their knowledge into knowledge bases. However, access to inference\nAPI for such models is limited, thereby restricting the scope and the diversity\nof the knowledge acquisition. We thus ask a seemingly implausible question:\nwhether more accessible, yet considerably smaller and weaker models such as\nGPT-2, can be utilized to acquire comparative knowledge, such that the\nresulting quality is on par with their large-scale counterparts?\n</p>\n<p>We introduce NeuroComparatives, a novel framework for comparative knowledge\ndistillation using lexically-constrained decoding, followed by stringent\nfiltering of generated knowledge. Our framework acquires comparative knowledge\nbetween everyday objects and results in a corpus of 8.7M comparisons over 1.74M\nentity pairs - 10X larger and 30% more diverse than existing resources.\nMoreover, human evaluations show that NeuroComparatives outperform existing\nresources (up to 32% absolute improvement), even including GPT-3, despite using\na 100X smaller model. Our results motivate neuro-symbolic manipulation of\nsmaller models as a cost-effective alternative to the currently dominant\npractice of relying on extreme-scale language models with limited inference\naccess.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Howard_P/0/1/0/all/0/1\">Phillip Howard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junlin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_G/0/1/0/all/0/1\">Gadi Singer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust. (arXiv:2305.04989v1 [cs.CL])","link":"http://arxiv.org/abs/2305.04989","description":"<p>A fundamental question in natural language processing is - what kind of\nlanguage structure and semantics is the language model capturing? Graph formats\nsuch as knowledge graphs are easy to evaluate as they explicitly express\nlanguage semantics and structure. This study evaluates the semantics encoded in\nthe self-attention transformers by leveraging explicit knowledge graph\nstructures. We propose novel metrics to measure the reconstruction error when\nproviding graph path sequences from a knowledge graph and trying to\nreproduce/reconstruct the same from the outputs of the self-attention\ntransformer models. The opacity of language models has an immense bearing on\nsocietal issues of trust and explainable decision outcomes. Our findings\nsuggest that language models are models of stochastic control processes for\nplausible language pattern generation. However, they do not ascribe object and\nconcept-level meaning and semantics to the learned stochastic patterns such as\nthose described in knowledge graphs. Furthermore, to enable robust evaluation\nof concept understanding by language models, we construct and make public an\naugmented language understanding benchmark built on the General Language\nUnderstanding Evaluation (GLUE) benchmark. This has significant\napplication-level user trust implications as stochastic patterns without a\nstrong sense of meaning cannot be trusted in high-stakes applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_T/0/1/0/all/0/1\">Tarun Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palit_V/0/1/0/all/0/1\">Vedant Palit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_Y/0/1/0/all/0/1\">Yuxin Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vignesh Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explanation-based Finetuning Makes Models More Robust to Spurious Cues. (arXiv:2305.04990v1 [cs.CL])","link":"http://arxiv.org/abs/2305.04990","description":"<p>Large Language Models (LLMs) are so powerful that they sometimes learn\ncorrelations between labels and features that are irrelevant to the task,\nleading to poor generalization on out-of-distribution data. We propose\nexplanation-based finetuning as a novel and general approach to mitigate LLMs'\nreliance on spurious correlations. Unlike standard finetuning where the model\nonly predicts the answer given the input, we finetune the model to additionally\ngenerate a free-text explanation supporting its answer. To evaluate our method,\nwe finetune the model on artificially constructed training sets containing\ndifferent types of spurious cues, and test it on a test set without these cues.\nCompared to standard finetuning, our method makes models remarkably more robust\nagainst spurious cues in terms of accuracy drop across four classification\ntasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). Moreover,\nour method works equally well with explanations generated by the model,\nimplying its applicability to more datasets without human-written explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ludan_J/0/1/0/all/0/1\">Josh Magnus Ludan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yixuan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Saurabh Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1\">Marianna Apidianaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning. (arXiv:2305.05001v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05001","description":"<p>This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared\ntask, encompassing both subtask A and subtask B. We approach the task as a\ndialogue summarization problem and implement two distinct pipelines: (a) a\nfine-tuning of a pre-trained dialogue summarization model and GPT-3, and (b)\nfew-shot in-context learning (ICL) using a large language model, GPT-4. Both\nmethods achieve excellent results in terms of ROUGE-1 F1, BERTScore F1\n(deberta-xlarge-mnli), and BLEURT, with scores of 0.4011, 0.7058, and 0.5421,\nrespectively. Additionally, we predict the associated section headers using\nRoBERTa and SciBERT based classification models. Our team ranked fourth among\nall teams, while each team is allowed to submit three runs as part of their\nsubmission. We also utilize expert annotations to demonstrate that the notes\ngenerated through the ICL GPT-4 are better than all other baselines. The code\nfor our submission is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1\">Andrew Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jeffrey Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1\">Mark Gerstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Relation Extraction in the era of Large Language Models. (arXiv:2305.05003v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05003","description":"<p>Relation extraction (RE) is the core NLP task of inferring semantic\nrelationships between entities from text. Standard supervised RE techniques\nentail training modules to tag tokens comprising entity spans and then predict\nthe relationship between them. Recent work has instead treated the problem as a\n\\emph{sequence-to-sequence} task, linearizing relations between entities as\ntarget strings to be generated conditioned on the input. Here we push the\nlimits of this approach, using larger language models (GPT-3 and Flan-T5 large)\nthan considered in prior work and evaluating their performance on standard RE\ntasks under varying levels of supervision. We address issues inherent to\nevaluating generative approaches to RE by doing human evaluations, in lieu of\nrelying on exact matching. Under this refined evaluation, we find that: (1)\nFew-shot prompting with GPT-3 achieves near SOTA performance, i.e., roughly\nequivalent to existing fully supervised models; (2) Flan-T5 is not as capable\nin the few-shot setting, but supervising and fine-tuning it with\nChain-of-Thought (CoT) style explanations (generated via GPT-3) yields SOTA\nresults. We release this model as a new baseline for RE tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadhwa_S/0/1/0/all/0/1\">Somin Wadhwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amir_S/0/1/0/all/0/1\">Silvio Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Not Blindly Imitate the Teacher: Using Perturbed Loss for Knowledge Distillation. (arXiv:2305.05010v1 [cs.LG])","link":"http://arxiv.org/abs/2305.05010","description":"<p>Knowledge distillation is a popular technique to transfer knowledge from\nlarge teacher models to a small student model. Typically, the student learns to\nimitate the teacher by minimizing the KL divergence of its output distribution\nwith the teacher's output distribution. In this work, we argue that such a\nlearning objective is sub-optimal because there exists a discrepancy between\nthe teacher's output distribution and the ground truth label distribution.\nTherefore, forcing the student to blindly imitate the unreliable teacher output\ndistribution leads to inferior performance. To this end, we propose a novel\nknowledge distillation objective PTLoss by first representing the vanilla\nKL-based distillation loss function via a Maclaurin series and then perturbing\nthe leading-order terms in this series. This perturbed loss implicitly\ntransforms the original teacher into a proxy teacher with a distribution closer\nto the ground truth distribution. We establish the theoretical connection\nbetween this \"distribution closeness\" and the student model generalizability,\nwhich enables us to select the PTLoss's perturbation coefficients in a\nprincipled way. Extensive experiments on five datasets demonstrate PTLoss can\nsignificantly improve the distillation effectiveness for teachers of various\nscales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jialu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Michael Bendersky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1\">Marc Najork</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Web Content Filtering through knowledge distillation of Large Language Models. (arXiv:2305.05027v1 [cs.LG])","link":"http://arxiv.org/abs/2305.05027","description":"<p>We introduce a state-of-the-art approach for URL categorization that\nleverages the power of Large Language Models (LLMs) to address the primary\nobjectives of web content filtering: safeguarding organizations from legal and\nethical risks, limiting access to high-risk or suspicious websites, and\nfostering a secure and professional work environment. Our method utilizes LLMs\nto generate accurate classifications and then employs established knowledge\ndistillation techniques to create smaller, more specialized student models\ntailored for web content filtering. Distillation results in a student model\nwith a 9\\% accuracy rate improvement in classifying websites, sourced from\ncustomer telemetry data collected by a large security vendor, into 30 distinct\ncontent categories based on their URLs, surpassing the current state-of-the-art\napproach. Our student model matches the performance of the teacher LLM with 175\ntimes less parameters, allowing the model to be used for in-line scanning of\nlarge volumes of URLs, and requires 3 orders of magnitude less manually labeled\ntraining data than the current state-of-the-art approach. Depending on the\nspecific use case, the output generated by our approach can either be directly\nreturned or employed as a pre-filter for more resource-intensive operations\ninvolving website images or HTML.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voros_T/0/1/0/all/0/1\">Tam&#xe1;s V&#xf6;r&#xf6;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergeron_S/0/1/0/all/0/1\">Sean Paul Bergeron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berlin_K/0/1/0/all/0/1\">Konstantin Berlin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANALOGICAL -- A New Benchmark for Analogy of Long Text for Large Language Models. (arXiv:2305.05050v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05050","description":"<p>Over the past decade, analogies, in the form of word-level analogies, have\nplayed a significant role as an intrinsic measure of evaluating the quality of\nword embedding methods such as word2vec. Modern large language models (LLMs),\nhowever, are primarily evaluated on extrinsic measures based on benchmarks such\nas GLUE and SuperGLUE, and there are only a few investigations on whether LLMs\ncan draw analogies between long texts. In this paper, we present ANALOGICAL, a\nnew benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of\nlong text with six levels of complexity -- (i) word, (ii) word vs. sentence,\n(iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using\nthirteen datasets and three different distance measures, we evaluate the\nabilities of eight LLMs in identifying analogical pairs in the semantic vector\nspace (e.g., \"I can speak two languages\" should be closer to \"I am bilingual\"\nwhile \"I like chocolate\" and \"I do not like chocolate\" should be orthogonal).\nOur evaluation finds that it is increasingly challenging for LLMs to identify\nanalogies when going up the analogy taxonomy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wijesiriwardene_T/0/1/0/all/0/1\">Thilini Wijesiriwardene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wickramarachchi_R/0/1/0/all/0/1\">Ruwan Wickramarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gajera_B/0/1/0/all/0/1\">Bimal G. Gajera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gowaikar_S/0/1/0/all/0/1\">Shreeyash Mukul Gowaikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1\">Chandan Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reganti_A/0/1/0/all/0/1\">Aishwarya Naresh Reganti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Amitava Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dreams Are More \"Predictable'' Than You Think. (arXiv:2305.05054v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05054","description":"<p>A consistent body of evidence suggests that dream reports significantly vary\nfrom other types of textual transcripts with respect to semantic content.\nFurthermore, it appears to be a widespread belief in the dream/sleep research\ncommunity that dream reports constitute rather ``unique'' strings of text. This\nmight be a notable issue for the growing amount of approaches using natural\nlanguage processing (NLP) tools to automatically analyse dream reports, as they\nlargely rely on neural models trained on non-dream corpora scraped from the\nweb. In this work, I will adopt state-of-the-art (SotA) large language models\n(LLMs), to study if and how dream reports deviate from other human-generated\ntext strings, such as Wikipedia. Results show that, taken as a whole, DreamBank\ndoes not deviate from Wikipedia. Moreover, on average, single dream reports are\nsignificantly more predictable than Wikipedia articles. Preliminary evidence\nsuggests that word count, gender, and visual impairment can significantly shape\nhow predictable a dream report can appear to the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bertolini_L/0/1/0/all/0/1\">Lorenzo Bertolini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coherent Wave Dynamics and Language Generation of a Generative Pre-trained Transformer. (arXiv:2305.05061v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05061","description":"<p>Large Language Models (LLMs), such as the Generative Pretrained Transformer\n(GPT), have achieved tremendous success in various language tasks, but their\nemergent abilities have also raised many questions, concerns, and challenges\nthat need to be addressed. To gain a better understanding of the models' inner\nmechanisms, we analyze the hidden state and channel wave dynamics in a small\nGPT, focusing on the coherence of wave patterns in terms of cross-channel\ncorrelation and individual auto-correlation. Our findings suggest that wave\ndynamics offer consistent and repeatable intrinsic oscillation modes, along\nwith context-aware plasticity and expressiveness in language generation. By\nanalyzing wave patterns, coherence, and clustering, we provide a systematic way\nto identify and interpret the functionality of the hidden state channels,\npaving the way to understand and control higher-level language pattern\nformation. In addition, we investigate the Poisson statistics of spelling\nerrors in text sequence generation across various levels of model training and\nobserve a phase-transition-like process. As coherence builds up, there is a\ncompetition between the generation of correct and misspelled words. However,\nonce the model is adequately trained and significant coherence has emerged, the\ncoherent process becomes strong enough to effectively suppress spelling errors,\npreventing the cascade amplification of defects. The distribution of correct\nspellings transitions from Poissonian to Sub-Poissonian, while the distribution\nof misspellings shows the opposite trend. By leveraging concepts and techniques\nfrom quantum physics, we gain novel insights into the dynamics of the small\nGPT. This approach can be extended to larger language models that exhibit more\ncomplex coherent language patterns, opening up opportunities to interpret their\nemergent capabilities and develop more specialized models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1\">Tao Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Evaluation Framework for Novelty Detection and Accommodation in NLP with an Instantiation in Authorship Attribution. (arXiv:2305.05079v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05079","description":"<p>State-of-the-art natural language processing models have been shown to\nachieve remarkable performance in 'closed-world' settings where all the labels\nin the evaluation set are known at training time. However, in real-world\nsettings, 'novel' instances that do not belong to any known class are often\nobserved. This renders the ability to deal with novelties crucial. To initiate\na systematic research in this important area of 'dealing with novelties', we\nintroduce 'NoveltyTask', a multi-stage task to evaluate a system's performance\non pipelined novelty 'detection' and 'accommodation' tasks. We provide\nmathematical formulation of NoveltyTask and instantiate it with the authorship\nattribution task that pertains to identifying the correct author of a given\ntext. We use Amazon reviews corpus and compile a large dataset (consisting of\n250k instances across 200 authors/labels) for NoveltyTask. We conduct\ncomprehensive experiments and explore several baseline methods for the task.\nOur results show that the methods achieve considerably low performance making\nthe task challenging and leaving sufficient room for improvement. Finally, we\nbelieve our work will encourage research in this underexplored area of dealing\nwith novelties, an important step en route to developing robust systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_E/0/1/0/all/0/1\">Eric Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-enhanced Agents for Interactive Text Games. (arXiv:2305.05091v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05091","description":"<p>Communication via natural language is a crucial aspect of intelligence, and\nit requires computational models to learn and reason about world concepts, with\nvarying levels of supervision. While there has been significant progress made\non fully-supervised non-interactive tasks, such as question-answering and\nprocedural text understanding, much of the community has turned to various\nsequential interactive tasks, as in semi-Markov text-based games, which have\nrevealed limitations of existing approaches in terms of coherence, contextual\nawareness, and their ability to learn effectively from the environment. In this\npaper, we propose a framework for enabling improved functional grounding of\nagents in text-based games. Specifically, we consider two forms of domain\nknowledge that we inject into learning-based agents: memory of previous correct\nactions and affordances of relevant objects in the environment. Our framework\nsupports three representative model classes: `pure' reinforcement learning (RL)\nagents, RL agents enhanced with knowledge graphs, and agents equipped with\nlanguage models. Furthermore, we devise multiple injection strategies for the\nabove domain knowledge types and agent architectures, including injection via\nknowledge graphs and augmentation of the existing input encoding strategies. We\nperform all experiments on the ScienceWorld text-based game environment, to\nillustrate the performance of various model configurations in challenging\nscience-related instruction-following tasks. Our findings provide crucial\ninsights on the development of effective natural language processing systems\nfor interactive contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chhikara_P/0/1/0/all/0/1\">Prateek Chhikara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiarui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Concept Learning for Uncovering Latent Themes in Large Text Collections. (arXiv:2305.05094v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05094","description":"<p>Experts across diverse disciplines are often interested in making sense of\nlarge text collections. Traditionally, this challenge is approached either by\nnoisy unsupervised techniques such as topic models, or by following a manual\ntheme discovery process. In this paper, we expand the definition of a theme to\naccount for more than just a word distribution, and include generalized\nconcepts deemed relevant by domain experts. Then, we propose an interactive\nframework that receives and encodes expert feedback at different levels of\nabstraction. Our framework strikes a balance between automation and manual\ncoding, allowing experts to maintain control of their study while reducing the\nmanual effort required.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_M/0/1/0/all/0/1\">Maria Leonor Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tunazzina Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Ming Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Who Needs Decoders? Efficient Estimation of Sequence-level Attributes. (arXiv:2305.05098v1 [cs.LG])","link":"http://arxiv.org/abs/2305.05098","description":"<p>State-of-the-art sequence-to-sequence models often require autoregressive\ndecoding, which can be highly expensive. However, for some downstream tasks\nsuch as out-of-distribution (OOD) detection and resource allocation, the actual\ndecoding output is not needed just a scalar attribute of this sequence. In\nthese scenarios, where for example knowing the quality of a system's output to\npredict poor performance prevails over knowing the output itself, is it\npossible to bypass the autoregressive decoding? We propose Non-Autoregressive\nProxy (NAP) models that can efficiently predict general scalar-valued\nsequence-level attributes. Importantly, NAPs predict these metrics directly\nfrom the encodings, avoiding the expensive autoregressive decoding stage. We\nconsider two sequence-to-sequence task: Machine Translation (MT); and Automatic\nSpeech Recognition (ASR). In OOD for MT, NAPs outperform a deep ensemble while\nbeing significantly faster. NAPs are also shown to be able to predict\nperformance metrics such as BERTScore (MT) or word error rate (ASR). For\ndownstream tasks, such as data filtering and resource optimization, NAPs\ngenerate performance predictions that outperform predictive uncertainty while\nbeing highly inference efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fathullah_Y/0/1/0/all/0/1\">Yassir Fathullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radmard_P/0/1/0/all/0/1\">Puria Radmard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liusie_A/0/1/0/all/0/1\">Adian Liusie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Phishing Attacks using ChatGPT. (arXiv:2305.05133v1 [cs.CR])","link":"http://arxiv.org/abs/2305.05133","description":"<p>The ability of ChatGPT to generate human-like responses and understand\ncontext has made it a popular tool for conversational agents, content creation,\ndata analysis, and research and innovation. However, its effectiveness and ease\nof accessibility makes it a prime target for generating malicious content, such\nas phishing attacks, that can put users at risk. In this work, we identify\nseveral malicious prompts that can be provided to ChatGPT to generate\nfunctional phishing websites. Through an iterative approach, we find that these\nphishing websites can be made to imitate popular brands and emulate several\nevasive tactics that have been known to avoid detection by anti-phishing\nentities. These attacks can be generated using vanilla ChatGPT without the need\nof any prior adversarial exploits (jailbreaking).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sayak Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naragam_K/0/1/0/all/0/1\">Krishna Vamsi Naragam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nilizadeh_S/0/1/0/all/0/1\">Shirin Nilizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Read, Diagnose and Chat: Towards Explainable and Interactive LLMs-Augmented Depression Detection in Social Media. (arXiv:2305.05138v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05138","description":"<p>This paper proposes a new depression detection system based on LLMs that is\nboth interpretable and interactive. It not only provides a diagnosis, but also\ndiagnostic evidence and personalized recommendations based on natural language\ndialogue with the user. We address challenges such as the processing of large\namounts of text and integrate professional diagnostic criteria. Our system\noutperforms traditional methods across various settings and is demonstrated\nthrough case studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_W/0/1/0/all/0/1\">Wei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zetong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yunshi Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Weijieying Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Richang Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Medical Code Prediction via Label Internal Alignment. (arXiv:2305.05162v1 [cs.LG])","link":"http://arxiv.org/abs/2305.05162","description":"<p>The clinical notes are usually typed into the system by physicians. They are\ntypically required to be marked by standard medical codes, and each code\nrepresents a diagnosis or medical treatment procedure. Annotating these notes\nis time consuming and prone to error. In this paper, we proposed a multi-view\nattention based Neural network to predict medical codes from clinical texts.\nOur method incorporates three aspects of information, the semantic context of\nthe clinical text, the relationship among the label (medical codes) space, and\nthe alignment between each pair of a clinical text and medical code. Our method\nis verified to be effective on the open source dataset. The experimental result\nshows that our method achieves better performance against the prior\nstate-of-art on multiple metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guodong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E2TIMT: Efficient and Effective Modal Adapter for Text Image Machine Translation. (arXiv:2305.05166v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05166","description":"<p>Text image machine translation (TIMT) aims to translate texts embedded in\nimages from one source language to another target language. Existing methods,\nboth two-stage cascade and one-stage end-to-end architectures, suffer from\ndifferent issues. The cascade models can benefit from the large-scale optical\ncharacter recognition (OCR) and MT datasets but the two-stage architecture is\nredundant. The end-to-end models are efficient but suffer from training data\ndeficiency. To this end, in our paper, we propose an end-to-end TIMT model\nfully making use of the knowledge from existing OCR and MT datasets to pursue\nboth an effective and efficient framework. More specifically, we build a novel\nmodal adapter effectively bridging the OCR encoder and MT decoder. End-to-end\nTIMT loss and cross-modal contrastive loss are utilized jointly to align the\nfeature distribution of the OCR and MT tasks. Extensive experiments show that\nthe proposed method outperforms the existing two-stage cascade models and\none-stage end-to-end models with a lighter and faster architecture.\nFurthermore, the ablation studies verify the generalization of our method,\nwhere the proposed modal adapter is effective to bridge various OCR and MT\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Cong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_M/0/1/0/all/0/1\">Mei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarization with Precise Length Control. (arXiv:2305.05171v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05171","description":"<p>Many applications of text generation such as summarization benefit from\naccurately controlling the text length. Existing approaches on\nlength-controlled summarization either result in degraded performance or can\nonly control the length approximately. In this work, we present a framework to\ngenerate summaries with precisely the specified number of tokens or sentences,\nwhile maintaining or even improving the text quality. In addition, we jointly\ntrain the models to predict the lengths, so our model can generate summaries\nwith optimal length. We evaluate the proposed framework on the CNNDM dataset\nand show improved performance compared to existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miculicich_L/0/1/0/all/0/1\">Lesly Miculicich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance. (arXiv:2305.05176v1 [cs.LG])","link":"http://arxiv.org/abs/2305.05176","description":"<p>There is a rapidly growing number of large language models (LLMs) that users\ncan query for a fee. We review the cost associated with querying popular LLM\nAPIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have\nheterogeneous pricing structures, with fees that can differ by two orders of\nmagnitude. In particular, using LLMs on large collections of queries and text\ncan be expensive. Motivated by this, we outline and discuss three types of\nstrategies that users can exploit to reduce the inference cost associated with\nusing LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As\nan example, we propose FrugalGPT, a simple yet flexible instantiation of LLM\ncascade which learns which combinations of LLMs to use for different queries in\norder to reduce cost and improve accuracy. Our experiments show that FrugalGPT\ncan match the performance of the best individual LLM (e.g. GPT-4) with up to\n98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost.\nThe ideas and findings presented here lay a foundation for using LLMs\nsustainably and efficiently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lingjiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoT: Pre-thinking and Recalling Enable ChatGPT to Self-Improve with Memory-of-Thoughts. (arXiv:2305.05181v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05181","description":"<p>Large Language Models have shown impressive abilities on various tasks.\nHowever, fundamentally improving them depends on high-quality datasets or\ncomputationally expensive fine-tuning. On the contrary, human can easily\nimprove themselves by thinking and memory, without external resources. In this\npaper, we propose a framework, MoT, to let the LLM self-improve through Memory\nof Thoughts, without annotated datasets and parameter updates. Specifically,\nthe framework is divided into two stages: 1. before the test stage, we let the\nLLM pre-think on the unlabeled dataset and save the high-confidence thoughts as\nexternal memory; 2. during inference, given a test question, we let the LLM\nrecall relevant memory to help itself reason and answer it. Experimental\nresults show that the proposed framework can help ChatGPT significantly improve\nits abilities in math reasoning, commonsense reasoning, factual reasoning and\nnatural language inference. Further analyses show that each component\ncontributes critically to the improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSED: A Chinese Semantic Error Diagnosis Corpus. (arXiv:2305.05183v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05183","description":"<p>Recently, much Chinese text error correction work has focused on Chinese\nSpelling Check (CSC) and Chinese Grammatical Error Diagnosis (CGED). In\ncontrast, little attention has been paid to the complicated problem of Chinese\nSemantic Error Diagnosis (CSED), which lacks relevant datasets. The study of\nsemantic errors is important because they are very common and may lead to\nsyntactic irregularities or even problems of comprehension. To investigate\nthis, we build the CSED corpus, which includes two datasets. The one is for the\nCSED-Recognition (CSED-R) task. The other is for the CSED-Correction (CSED-C)\ntask. Our annotation guarantees high-quality data through quality assurance\nmechanisms. Our experiments show that powerful pre-trained models perform\npoorly on this corpus. We also find that the CSED task is challenging, as\nevidenced by the fact that even humans receive a low score. This paper proposes\nsyntax-aware models to specifically adapt to the CSED task. The experimental\nresults show that the introduction of the syntax-aware approach is meaningful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baoxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dayong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models. (arXiv:2305.05189v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05189","description":"<p>Diffusion models, which have emerged to become popular text-to-image\ngeneration models, can produce high-quality and content-rich images guided by\ntextual prompts. However, there are limitations to semantic understanding and\ncommonsense reasoning in existing models when the input prompts are concise\nnarrative, resulting in low-quality image generation. To improve the capacities\nfor narrative prompts, we propose a simple-yet-effective parameter-efficient\nfine-tuning approach called the Semantic Understanding and Reasoning adapter\n(SUR-adapter) for pre-trained diffusion models. To reach this goal, we first\ncollect and annotate a new dataset SURD which consists of more than 57,000\nsemantically corrected multi-modal samples. Each sample contains a simple\nnarrative prompt, a complex keyword-based prompt, and a high-quality image.\nThen, we align the semantic representation of narrative prompts to the complex\nprompts and transfer knowledge of large language models (LLMs) to our\nSUR-adapter via knowledge distillation so that it can acquire the powerful\nsemantic understanding and reasoning capabilities to build a high-quality\ntextual semantic representation for text-to-image generation. We conduct\nexperiments by integrating multiple LLMs and popular pre-trained diffusion\nmodels to show the effectiveness of our approach in enabling diffusion models\nto understand and reason concise natural language without image quality\ndegradation. Our approach can make text-to-image diffusion models easier to use\nwith better user experience, which demonstrates our approach has the potential\nfor further advancing the development of user-friendly text-to-image generation\nmodels by bridging the semantic gap between simple narrative prompts and\ncomplex keyword-based prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Shanshan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1\">Wushao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jinghui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COLA: Contextualized Commonsense Causal Reasoning from the Causal Inference Perspective. (arXiv:2305.05191v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05191","description":"<p>Detecting commonsense causal relations (causation) between events has long\nbeen an essential yet challenging task. Given that events are complicated, an\nevent may have different causes under various contexts. Thus, exploiting\ncontext plays an essential role in detecting causal relations. Meanwhile,\nprevious works about commonsense causation only consider two events and ignore\ntheir context, simplifying the task formulation. This paper proposes a new task\nto detect commonsense causation between two events in an event sequence (i.e.,\ncontext), called contextualized commonsense causal reasoning. We also design a\nzero-shot framework: COLA (Contextualized Commonsense Causality Reasoner) to\nsolve the task from the causal inference perspective. This framework obtains\nrich incidental supervision from temporality and balances covariates from\nmultiple timestamps to remove confounding effects. Our extensive experiments\nshow that COLA can detect commonsense causality more accurately than baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_Q/0/1/0/all/0/1\">Quyet V. Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_G/0/1/0/all/0/1\">Ginny Y. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1\">Simon See</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploration of Language Dependency for Japanese Self-Supervised Speech Representation Models. (arXiv:2305.05201v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05201","description":"<p>Self-supervised learning (SSL) has been dramatically successful not only in\nmonolingual but also in cross-lingual settings. However, since the two settings\nhave been studied individually in general, there has been little research\nfocusing on how effective a cross-lingual model is in comparison with a\nmonolingual model. In this paper, we investigate this fundamental question\nempirically with Japanese automatic speech recognition (ASR) tasks. First, we\nbegin by comparing the ASR performance of cross-lingual and monolingual models\nfor two different language tasks while keeping the acoustic domain as identical\nas possible. Then, we examine how much unlabeled data collected in Japanese is\nneeded to achieve performance comparable to a cross-lingual model pre-trained\nwith tens of thousands of hours of English and/or multilingual data. Finally,\nwe extensively investigate the effectiveness of SSL in Japanese and demonstrate\nstate-of-the-art performance on multiple ASR tasks. Since there is no\ncomprehensive SSL study for Japanese, we hope this study will guide Japanese\nSSL research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashihara_T/0/1/0/all/0/1\">Takanori Ashihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moriya_T/0/1/0/all/0/1\">Takafumi Moriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuura_K/0/1/0/all/0/1\">Kohei Matsuura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages. (arXiv:2305.05214v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05214","description":"<p>We address the task of machine translation from an extremely low-resource\nlanguage (LRL) to English using cross-lingual transfer from a closely related\nhigh-resource language (HRL). For many of these languages, no parallel corpora\nare available, even monolingual corpora are limited and representations in\npre-trained sequence-to-sequence models are absent. These factors limit the\nbenefits of cross-lingual transfer from shared embedding spaces in multilingual\nmodels. However, many extremely LRLs have a high level of lexical similarity\nwith related HRLs. We utilize this property by injecting character and\ncharacter-span noise into the training data of the HRL prior to learning the\nvocabulary. This serves as a regularizer which makes the model more robust to\nlexical divergences between the HRL and LRL and better facilitates\ncross-lingual transfer. On closely related HRL and LRL pairs from multiple\nlanguage families, we observe that our method significantly outperforms the\nbaseline MT as well as approaches proposed previously to address cross-lingual\ntransfer between closely related languages. We also show that the proposed\ncharacter-span noise injection performs better than the unigram-character noise\ninjection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maurya_K/0/1/0/all/0/1\">Kaushal Kumar Maurya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kejriwal_R/0/1/0/all/0/1\">Rahul Kejriwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desarkar_M/0/1/0/all/0/1\">Maunendra Sankar Desarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Teacher Knowledge Distillation For Text Image Machine Translation. (arXiv:2305.05226v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05226","description":"<p>Text image machine translation (TIMT) has been widely used in various\nreal-world applications, which translates source language texts in images into\nanother target language sentence. Existing methods on TIMT are mainly divided\ninto two categories: the recognition-then-translation pipeline model and the\nend-to-end model. However, how to transfer knowledge from the pipeline model\ninto the end-to-end model remains an unsolved problem. In this paper, we\npropose a novel Multi-Teacher Knowledge Distillation (MTKD) method to\neffectively distillate knowledge into the end-to-end TIMT model from the\npipeline model. Specifically, three teachers are utilized to improve the\nperformance of the end-to-end TIMT model. The image encoder in the end-to-end\nTIMT model is optimized with the knowledge distillation guidance from the\nrecognition teacher encoder, while the sequential encoder and decoder are\nimproved by transferring knowledge from the translation sequential and decoder\nteacher models. Furthermore, both token and sentence-level knowledge\ndistillations are incorporated to better boost the translation performance.\nExtensive experimental results show that our proposed MTKD effectively improves\nthe text image translation performance and outperforms existing end-to-end and\npipeline models with fewer parameters and less decoding time, illustrating that\nMTKD can take advantage of both pipeline and end-to-end models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Cong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_M/0/1/0/all/0/1\">Mei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05252","description":"<p>In everyday life, humans often plan their actions by following step-by-step\ninstructions in the form of goal-oriented scripts. Previous work has exploited\nlanguage models (LMs) to plan for abstract goals of stereotypical activities\n(e.g., \"make a cake\"), but leaves more specific goals with multi-facet\nconstraints understudied (e.g., \"make a cake for diabetics\"). In this paper, we\ndefine the task of constrained language planning for the first time. We propose\nan overgenerate-then-filter approach to improve large language models (LLMs) on\nthis task, and use it to distill a novel constrained language planning dataset,\nCoScript, which consists of 55,000 scripts. Empirical results demonstrate that\nour method significantly improves the constrained language planning ability of\nLLMs, especially on constraint faithfulness. Furthermore, CoScript is\ndemonstrated to be quite effective in endowing smaller LMs with constrained\nlanguage planning ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Siyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Ziquan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuyang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Soham Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jankowski_C/0/1/0/all/0/1\">Charles Robert Jankowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Deqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attack Named Entity Recognition by Entity Boundary Interference. (arXiv:2305.05253v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05253","description":"<p>Named Entity Recognition (NER) is a cornerstone NLP task while its robustness\nhas been given little attention. This paper rethinks the principles of NER\nattacks derived from sentence classification, as they can easily violate the\nlabel consistency between the original and adversarial NER examples. This is\ndue to the fine-grained nature of NER, as even minor word changes in the\nsentence can result in the emergence or mutation of any entities, resulting in\ninvalid adversarial examples. To this end, we propose a novel one-word\nmodification NER attack based on a key insight, NER models are always\nvulnerable to the boundary position of an entity to make their decision. We\nthus strategically insert a new boundary into the sentence and trigger the\nEntity Boundary Interference that the victim model makes the wrong prediction\neither on this boundary word or on other words in the sentence. We call this\nattack Virtual Boundary Attack (ViBA), which is shown to be remarkably\neffective when attacking both English and Chinese models with a 70%-90% attack\nsuccess rate on state-of-the-art language models (e.g. RoBERTa, DeBERTa) and\nalso significantly faster than previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yifei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongqiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Acoustic and Semantic Contextual Biasing in Neural Transducers for Speech Recognition. (arXiv:2305.05271v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05271","description":"<p>Attention-based contextual biasing approaches have shown significant\nimprovements in the recognition of generic and/or personal rare-words in\nEnd-to-End Automatic Speech Recognition (E2E ASR) systems like neural\ntransducers. These approaches employ cross-attention to bias the model towards\nspecific contextual entities injected as bias-phrases to the model. Prior\napproaches typically relied on subword encoders for encoding the bias phrases.\nHowever, subword tokenizations are coarse and fail to capture granular\npronunciation information which is crucial for biasing based on acoustic\nsimilarity. In this work, we propose to use lightweight character\nrepresentations to encode fine-grained pronunciation features to improve\ncontextual biasing guided by acoustic similarity between the audio and the\ncontextual entities (termed acoustic biasing). We further integrate pretrained\nneural language model (NLM) based encoders to encode the utterance's semantic\ncontext along with contextual entities to perform biasing informed by the\nutterance's semantic context (termed semantic biasing). Experiments using a\nConformer Transducer model on the Librispeech dataset show a 4.62% - 9.26%\nrelative WER improvement on different biasing list sizes over the baseline\ncontextual model when incorporating our proposed acoustic and semantic biasing\napproach. On a large-scale in-house dataset, we observe 7.91% relative WER\nimprovement compared to our baseline model. On tail utterances, the\nimprovements are even more pronounced with 36.80% and 23.40% relative WER\nimprovements on Librispeech rare words and an in-house testset respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xuandi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathyendra_K/0/1/0/all/0/1\">Kanthashree Mysore Sathyendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1\">Grant P. Strimel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGowan_R/0/1/0/all/0/1\">Ross McGowan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1\">Athanasios Mouchtaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VCSUM: A Versatile Chinese Meeting Summarization Dataset. (arXiv:2305.05280v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05280","description":"<p>Compared to news and chat summarization, the development of meeting\nsummarization is hugely decelerated by the limited data. To this end, we\nintroduce a versatile Chinese meeting summarization dataset, dubbed VCSum,\nconsisting of 239 real-life meetings, with a total duration of over 230 hours.\nWe claim our dataset is versatile because we provide the annotations of topic\nsegmentation, headlines, segmentation summaries, overall meeting summaries, and\nsalient sentences for each meeting transcript. As such, the dataset can adapt\nto various summarization tasks or methods, including segmentation-based\nsummarization, multi-granularity summarization and retrieval-then-generate\nsummarization. Our analysis confirms the effectiveness and robustness of VCSum.\nWe also provide a set of benchmark models regarding different downstream\nsummarization tasks on VCSum to facilitate further research. The dataset and\ncode will be released at \\url{https://github.com/hahahawu/VCSum}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_M/0/1/0/all/0/1\">Mingjie Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Haochen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhaohui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue. (arXiv:2305.05290v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05290","description":"<p>Goal-directed dialogue systems aim to proactively reach a pre-determined\ntarget through multi-turn conversations. The key to achieving this task lies in\nplanning dialogue paths that smoothly and coherently direct conversations\ntowards the target. However, this is a challenging and under-explored task. In\nthis work, we propose a coherent dialogue planning approach that uses a\nstochastic process to model the temporal dynamics of dialogue paths. We define\na latent space that captures the coherence of goal-directed behavior using a\nBrownian bridge process, which allows us to incorporate user feedback flexibly\nin dialogue planning. Based on the derived latent trajectories, we generate\ndialogue paths explicitly using pre-trained language models. We finally employ\nthese paths as natural language prompts to guide dialogue generation. Our\nexperiments show that our approach generates more coherent utterances and\nachieves the goal with a higher success rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dongding Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data. (arXiv:2305.05295v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05295","description":"<p>Transferring information retrieval (IR) models from a high-resource language\n(typically English) to other languages in a zero-shot fashion has become a\nwidely adopted approach. In this work, we show that the effectiveness of\nzero-shot rankers diminishes when queries and documents are present in\ndifferent languages. Motivated by this, we propose to train ranking models on\nartificially code-switched data instead, which we generate by utilizing\nbilingual lexicons. To this end, we experiment with lexicons induced from (1)\ncross-lingual word embeddings and (2) parallel Wikipedia page titles. We use\nthe mMARCO dataset to extensively evaluate reranking models on 36 language\npairs spanning Monolingual IR (MoIR), Cross-lingual IR (CLIR), and Multilingual\nIR (MLIR). Our results show that code-switching can yield consistent and\nsubstantial gains of 5.1 MRR@10 in CLIR and 3.9 MRR@10 in MLIR, while\nmaintaining stable performance in MoIR. Encouragingly, the gains are especially\npronounced for distant languages (up to 2x absolute gain). We further show that\nour approach is robust towards the ratio of code-switched tokens and also\nextends to unseen languages. Our results demonstrate that training on\ncode-switched data is a cheap and effective way of generalizing zero-shot\nrankers for cross-lingual and multilingual retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Litschko_R/0/1/0/all/0/1\">Robert Litschko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Perfect Victim: Computational Analysis of Judicial Attitudes towards Victims of Sexual Violence. (arXiv:2305.05302v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05302","description":"<p>We develop computational models to analyze court statements in order to\nassess judicial attitudes toward victims of sexual violence in the Israeli\ncourt system. The study examines the resonance of \"rape myths\" in the criminal\njustice system's response to sex crimes, in particular in judicial assessment\nof victim's credibility. We begin by formulating an ontology for evaluating\njudicial attitudes toward victim's credibility, with eight ordinal labels and\nbinary categorizations. Second, we curate a manually annotated dataset for\njudicial assessments of victim's credibility in the Hebrew language, as well as\na model that can extract credibility labels from court cases. The dataset\nconsists of 855 verdict decision documents in sexual assault cases from\n1990-2021, annotated with the help of legal experts and trained law students.\nThe model uses a combined approach of syntactic and latent structures to find\nsentences that convey the judge's attitude towards the victim and classify them\naccording to the credibility label set. Our ontology, data, and models will be\nmade available upon request, in the hope they spur future progress in this\njudicial important task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Habba_E/0/1/0/all/0/1\">Eliya Habba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keydar_R/0/1/0/all/0/1\">Renana Keydar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bareket_D/0/1/0/all/0/1\">Dan Bareket</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Sentiment Analysis as Transition-based Dependency Parsing. (arXiv:2305.05311v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05311","description":"<p>Structured sentiment analysis (SSA) aims to automatically extract people's\nopinions from a text in natural language and adequately represent that\ninformation in a graph structure. One of the most accurate methods for\nperforming SSA was recently proposed and consists of approaching it as a\ndependency parsing task. Although we can find in the literature how\ntransition-based algorithms excel in dependency parsing in terms of accuracy\nand efficiency, all proposed attempts to tackle SSA following that approach\nwere based on graph-based models. In this article, we present the first\ntransition-based method to address SSA as dependency parsing. Specifically, we\ndesign a transition system that processes the input text in a left-to-right\npass, incrementally generating the graph structure containing all identified\nopinions. To effectively implement our final transition-based model, we resort\nto a Pointer Network architecture as a backbone. From an extensive evaluation,\nwe demonstrate that our model offers the best performance to date in\npractically all cases among prior dependency-based methods, and surpass recent\ntask-specific techniques on the most challenging datasets. We additionally\ninclude an in-depth analysis and empirically prove that the overall\ntime-complexity cost of our approach is quadratic in the sentence length, being\nmore efficient than top-performing graph-based parsers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1\">Daniel Fern&#xe1;ndez-Gonz&#xe1;lez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of depression on social networks using transformers and ensembles. (arXiv:2305.05325v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05325","description":"<p>As the impact of technology on our lives is increasing, we witness increased\nuse of social media that became an essential tool not only for communication\nbut also for sharing information with community about our thoughts and\nfeelings. This can be observed also for people with mental health disorders\nsuch as depression where they use social media for expressing their thoughts\nand asking for help. This opens a possibility to automatically process social\nmedia posts and detect signs of depression. We build several large pre-trained\nlanguage model based classifiers for depression detection from social media\nposts. Besides fine-tuning BERT, RoBERTA, BERTweet, and mentalBERT were also\nconstruct two types of ensembles. We analyze the performance of our models on\ntwo data sets of posts from social platforms Reddit and Twitter, and\ninvestigate also the performance of transfer learning across the two data sets.\nThe results show that transformer ensembles improve over the single\ntransformer-based classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tavchioski_I/0/1/0/all/0/1\">Ilija Tavchioski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollak_S/0/1/0/all/0/1\">Senja Pollak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Recommender with Geometric Information Bottleneck. (arXiv:2305.05331v1 [cs.IR])","link":"http://arxiv.org/abs/2305.05331","description":"<p>Explainable recommender systems can explain their recommendation decisions,\nenhancing user trust in the systems. Most explainable recommender systems\neither rely on human-annotated rationales to train models for explanation\ngeneration or leverage the attention mechanism to extract important text spans\nfrom reviews as explanations. The extracted rationales are often confined to an\nindividual review and may fail to identify the implicit features beyond the\nreview text. To avoid the expensive human annotation process and to generate\nexplanations beyond individual reviews, we propose to incorporate a geometric\nprior learnt from user-item interactions into a variational network which\ninfers latent factors from user-item reviews. The latent factors from an\nindividual user-item pair can be used for both recommendation and explanation\ngeneration, which naturally inherit the global characteristics encoded in the\nprior knowledge. Experimental results on three e-commerce datasets show that\nour model significantly improves the interpretability of a variational\nrecommender using the Wasserstein distance while achieving performance\ncomparable to existing content-based recommender systems in terms of\nrecommendation behaviours.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hanqi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Menghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArgU: A Controllable Factual Argument Generator. (arXiv:2305.05334v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05334","description":"<p>Effective argumentation is essential towards a purposeful conversation with a\nsatisfactory outcome. For example, persuading someone to reconsider smoking\nmight involve empathetic, well founded arguments based on facts and expert\nopinions about its ill-effects and the consequences on one's family. However,\nthe automatic generation of high-quality factual arguments can be challenging.\nAddressing existing controllability issues can make the recent advances in\ncomputational models for argument generation a potential solution. In this\npaper, we introduce ArgU: a neural argument generator capable of producing\nfactual arguments from input facts and real-world concepts that can be\nexplicitly controlled for stance and argument structure using Walton's argument\nscheme-based control codes. Unfortunately, computational argument generation is\na relatively new field and lacks datasets conducive to training. Hence, we have\ncompiled and released an annotated corpora of 69,428 arguments spanning six\ntopics and six argument schemes, making it the largest publicly available\ncorpus for identifying argument schemes; the paper details our annotation and\ndataset creation framework. We further experiment with an argument generation\nstrategy that establishes an inference strategy by generating an ``argument\ntemplate'' before actual argument generation. Our results demonstrate that it\nis possible to automatically generate diverse arguments exhibiting different\ninference patterns for the same set of facts by using control codes based on\nargument schemes and stance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sougata Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_R/0/1/0/all/0/1\">Rohini Srihari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rudolf Christoph Eucken at SemEval-2023 Task 4: An Ensemble Approach for Identifying Human Values from Arguments. (arXiv:2305.05335v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05335","description":"<p>The subtle human values we acquire through life experiences govern our\nthoughts and gets reflected in our speech. It plays an integral part in\ncapturing the essence of our individuality and making it imperative to identify\nsuch values in computational systems that mimic human actions. Computational\nargumentation is a field that deals with the argumentation capabilities of\nhumans and can benefit from identifying such values. Motivated by that, we\npresent an ensemble approach for detecting human values from argument text. Our\nensemble comprises three models: (i) An entailment-based model for determining\nthe human values based on their descriptions, (ii) A Roberta-based classifier\nthat predicts the set of human values from an argument. (iii) A Roberta-based\nclassifier to predict a reduced set of human values from an argument. We\nexperiment with different ways of combining the models and report our results.\nFurthermore, our best combination achieves an overall F1 score of 0.48 on the\nmain test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sougata Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_R/0/1/0/all/0/1\">Rohini Srihari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])","link":"http://arxiv.org/abs/2305.05352","description":"<p>The recent release of large language model (LLM) based chatbots, such as\nChatGPT, has attracted significant attention on foundations models. It is\nwidely believed that foundation models will serve as the fundamental building\nblocks for future AI systems. As foundation models are in their early stages,\nthe design of foundation model based systems has not yet been systematically\nexplored. There is little understanding about the impact of introducing\nfoundation models in software architecture. Therefore, in this paper, we\npropose a taxonomy of foundation model based systems, which classifies and\ncompares the characteristics of foundation models and foundation model based\nsystems. Our taxonomy comprises three categories: foundation model pretraining\nand fine-tuning, architecture design of foundation model based systems, and\nresponsible-AI-by-design. This taxonomy provides concrete guidance for making\nmajor design decisions when designing foundation model based systems and\nhighlights trade-offs arising from design decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhenchang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whittle_J/0/1/0/all/0/1\">Jon Whittle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model Programs. (arXiv:2305.05364v1 [cs.LG])","link":"http://arxiv.org/abs/2305.05364","description":"<p>In recent years, large pre-trained language models (LLMs) have demonstrated\nthe ability to follow instructions and perform novel tasks from a few examples.\nThe possibility to parameterise an LLM through such in-context examples widens\ntheir capability at a much lower cost than finetuning. We extend this line of\nreasoning and present a method which further expands the capabilities of an LLM\nby embedding it within an algorithm or program. To demonstrate the benefits of\nthis approach, we present an illustrative example of evidence-supported\nquestion-answering. We obtain a 6.4\\% improvement over the chain of thought\nbaseline through a more algorithmic approach without any finetuning.\nFurthermore, we highlight recent work from this perspective and discuss the\nadvantages and disadvantages in comparison to the standard approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlag_I/0/1/0/all/0/1\">Imanol Schlag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1\">Sainbayar Sukhbaatar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLM-GNN: A Webpage Classification Method based on Joint Pre-trained Language Model and Graph Neural Network. (arXiv:2305.05378v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05378","description":"<p>The number of web pages is growing at an exponential rate, accumulating\nmassive amounts of data on the web. It is one of the key processes to classify\nwebpages in web information mining. Some classical methods are based on\nmanually building features of web pages and training classifiers based on\nmachine learning or deep learning. However, building features manually requires\nspecific domain knowledge and usually takes a long time to validate the\nvalidity of features. Considering webpages generated by the combination of text\nand HTML Document Object Model(DOM) trees, we propose a representation and\nclassification method based on a pre-trained language model and graph neural\nnetwork, named PLM-GNN. It is based on the joint encoding of text and HTML DOM\ntrees in the web pages. It performs well on the KI-04 and SWDE datasets and on\npractical dataset AHS for the project of scholar's homepage crawling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_Q/0/1/0/all/0/1\">Qiwei Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shiqi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Execution with Pre-trained Language Models. (arXiv:2305.05383v1 [cs.PL])","link":"http://arxiv.org/abs/2305.05383","description":"<p>Code execution is a fundamental aspect of programming language semantics that\nreflects the exact behavior of the code. However, most pre-trained models for\ncode intelligence ignore the execution trace and only rely on source code and\nsyntactic structures. In this paper, we investigate how well pre-trained models\ncan understand and perform code execution. We develop a mutation-based data\naugmentation technique to create a large-scale and realistic Python dataset and\ntask for code execution, which challenges existing models such as Codex. We\nthen present CodeExecutor, a Transformer model that leverages code execution\npre-training and curriculum learning to enhance its semantic comprehension. We\nevaluate CodeExecutor on code execution and show its promising performance and\nlimitations. We also demonstrate its potential benefits for code intelligence\ntasks such as zero-shot code-to-code search and text-to-code generation. Our\nanalysis provides insights into the learning and generalization abilities of\npre-trained models for code execution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shuai Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1\">Alexey Svyatkovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Shengyu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COKE: A Cognitive Knowledge Graph for Machine Theory of Mind. (arXiv:2305.05390v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05390","description":"<p>Theory of mind (ToM) refers to humans' ability to understand and infer the\ndesires, beliefs, and intentions of others. The acquisition of ToM plays a key\nrole in humans' social cognition and interpersonal relations. Though\nindispensable for social intelligence, ToM is still lacking for modern AI and\nNLP systems since they cannot access the human mental state and cognitive\nprocess beneath the training corpus. To empower AI systems with the ToM ability\nand narrow the gap between them and humans, in this paper, we propose COKE: the\nfirst cognitive knowledge graph for machine theory of mind. Specifically, COKE\nformalizes ToM as a collection of 45k+ manually verified cognitive chains that\ncharacterize human mental activities and subsequent behavioral/affective\nresponses when facing specific social circumstances. Beyond that, we further\ngeneralize COKE using pre-trained language models and build a powerful\ncognitive generation model COKE+. Experimental results in both automatic and\nhuman evaluation demonstrate the high quality of COKE and the superior ToM\nability of COKE+.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jincenzi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiawen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1\">Sahand Sabour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaseEncoder: A Knowledge-enhanced Pre-trained Model for Legal Case Encoding. (arXiv:2305.05393v1 [cs.IR])","link":"http://arxiv.org/abs/2305.05393","description":"<p>Legal case retrieval is a critical process for modern legal information\nsystems. While recent studies have utilized pre-trained language models (PLMs)\nbased on the general domain self-supervised pre-training paradigm to build\nmodels for legal case retrieval, there are limitations in using general domain\nPLMs as backbones. Specifically, these models may not fully capture the\nunderlying legal features in legal case documents. To address this issue, we\npropose CaseEncoder, a legal document encoder that leverages fine-grained legal\nknowledge in both the data sampling and pre-training phases. In the data\nsampling phase, we enhance the quality of the training data by utilizing\nfine-grained law article information to guide the selection of positive and\nnegative examples. In the pre-training phase, we design legal-specific\npre-training tasks that align with the judging criteria of relevant legal\ncases. Based on these tasks, we introduce an innovative loss function called\nBiased Circle Loss to enhance the model's ability to recognize case relevance\nin fine grains. Experimental results on multiple benchmarks demonstrate that\nCaseEncoder significantly outperforms both existing general pre-training models\nand legal-specific pre-training models in zero-shot legal case retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yixiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yueyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weihang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiqun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Text Categorization using Data Augmentation in e-Commerce. (arXiv:2305.05402v1 [cs.LG])","link":"http://arxiv.org/abs/2305.05402","description":"<p>The categorization of massive e-Commerce data is a crucial, well-studied\ntask, which is prevalent in industrial settings. In this work, we aim to\nimprove an existing product categorization model that is already in use by a\nmajor web company, serving multiple applications. At its core, the product\ncategorization model is a text classification model that takes a product title\nas an input and outputs the most suitable category out of thousands of\navailable candidates. Upon a closer inspection, we found inconsistencies in the\nlabeling of similar items. For example, minor modifications of the product\ntitle pertaining to colors or measurements majorly impacted the model's output.\nThis phenomenon can negatively affect downstream recommendation or search\napplications, leading to a sub-optimal user experience.\n</p>\n<p>To address this issue, we propose a new framework for consistent text\ncategorization. Our goal is to improve the model's consistency while\nmaintaining its production-level performance. We use a semi-supervised approach\nfor data augmentation and presents two different methods for utilizing\nunlabeled samples. One method relies directly on existing catalogs, while the\nother uses a generative model. We compare the pros and cons of each approach\nand present our experimental results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horowitz_G/0/1/0/all/0/1\">Guy Horowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daye_S/0/1/0/all/0/1\">Stav Yanovsky Daye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avigdor_Elgrabli_N/0/1/0/all/0/1\">Noa Avigdor-Elgrabli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviv_A/0/1/0/all/0/1\">Ariel Raviv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey. (arXiv:2305.05403v1 [cs.AI])","link":"http://arxiv.org/abs/2305.05403","description":"<p>General-purpose knowledge bases (KBs) are a cornerstone of knowledge-centric\nAI. Many of them are constructed pragmatically from Web sources, and are thus\nfar from complete. This poses challenges for the consumption as well as the\ncuration of their content. While several surveys target the problem of\ncompleting incomplete KBs, the first problem is arguably to know whether and\nwhere the KB is incomplete in the first place, and to which degree.\n</p>\n<p>In this survey we discuss how knowledge about completeness, recall, and\nnegation in KBs can be expressed, extracted, and inferred. We cover (i) the\nlogical foundations of knowledge representation and querying under partial\nclosed-world semantics; (ii) the estimation of this information via statistical\npatterns; (iii) the extraction of information about recall from KBs and text;\n(iv) the identification of interesting negative statements; and (v) relaxed\nnotions of relative recall.\n</p>\n<p>This survey is targeted at two types of audiences: (1) practitioners who are\ninterested in tracking KB quality, focusing extraction efforts, and building\nquality-aware downstream applications; and (2) data management, knowledge base\nand semantic web researchers who wish to understand the state of the art of\nknowledge bases beyond the open-world assumption. Consequently, our survey\npresents both fundamental methodologies and their working, and gives\npractice-oriented recommendations on how to choose between different approaches\nfor a problem at hand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnaout_H/0/1/0/all/0/1\">Hiba Arnaout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shrestha Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suchanek_F/0/1/0/all/0/1\">Fabian Suchanek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Need Holistically Thought in Medical Conversational QA. (arXiv:2305.05410v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05410","description":"<p>The medical conversational question answering (CQA) system aims at providing\na series of professional medical services to improve the efficiency of medical\ncare. Despite the success of large language models (LLMs) in complex reasoning\ntasks in various fields, such as mathematics, logic, and commonsense QA, they\nstill need to improve with the increased complexity and specialization of the\nmedical field. This is because medical CQA tasks require not only strong\nmedical reasoning, but also the ability to think broadly and deeply. In this\npaper, to address these challenges in medical CQA tasks that need to be\nconsidered and understood in many aspects, we propose the Holistically Thought\n(HoT) method, which is designed to guide the LLMs to perform the diffused and\nfocused thinking for generating high-quality medical responses. The proposed\nHoT method has been evaluated through automated and manual assessments in three\ndifferent medical CQA datasets containing the English and Chinese languages.\nThe extensive experimental results show that our method can produce more\ncorrectness, professional, and considerate answers than several\nstate-of-the-art (SOTA) methods, manifesting its effectiveness. Our code in\nhttps://github.com/WENGSYX/HoT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minjun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shizhu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating related words computationally using language model from the Mahabharata -- an Indian epic. (arXiv:2305.05420v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05420","description":"<p>'Mahabharata' is the most popular among many Indian pieces of literature\nreferred to in many domains for completely different purposes. This text itself\nis having various dimension and aspects which is useful for the human being in\ntheir personal life and professional life. This Indian Epic is originally\nwritten in the Sanskrit Language. Now in the era of Natural Language\nProcessing, Artificial Intelligence, Machine Learning, and Human-Computer\ninteraction this text can be processed according to the domain requirement. It\nis interesting to process this text and get useful insights from Mahabharata.\nThe limitation of the humans while analyzing Mahabharata is that they always\nhave a sentiment aspect towards the story narrated by the author. Apart from\nthat, the human cannot memorize statistical or computational details, like\nwhich two words are frequently coming in one sentence? What is the average\nlength of the sentences across the whole literature? Which word is the most\npopular word across the text, what are the lemmas of the words used across the\nsentences? Thus, in this paper, we propose an NLP pipeline to get some\nstatistical and computational insights along with the most relevant word\nsearching method from the largest epic 'Mahabharata'. We stacked the different\ntext-processing approaches to articulate the best results which can be further\nused in the various domain where Mahabharata needs to be referred.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gadesha_V/0/1/0/all/0/1\">Vrunda Gadesha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_K/0/1/0/all/0/1\">Keyur D Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_S/0/1/0/all/0/1\">Shefali Naik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WikiWeb2M: A Page-Level Multimodal Wikipedia Dataset. (arXiv:2305.05432v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05432","description":"<p>Webpages have been a rich resource for language and vision-language tasks.\nYet only pieces of webpages are kept: image-caption pairs, long text articles,\nor raw HTML, never all in one place. Webpage tasks have resultingly received\nlittle attention and structured image-text data underused. To study multimodal\nwebpage understanding, we introduce the Wikipedia Webpage 2M (WikiWeb2M) suite;\nthe first to retain the full set of images, text, and structure data available\nin a page. WikiWeb2M can be used for tasks like page description generation,\nsection summarization, and contextual image captioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burns_A/0/1/0/all/0/1\">Andrea Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_K/0/1/0/all/0/1\">Krishna Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1\">Geoff Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What is the best recipe for character-level encoder-only modelling?. (arXiv:2305.05461v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05461","description":"<p>This paper aims to benchmark recent progress in language understanding models\nthat output contextualised representations at the character level. Many such\nmodelling architectures and methods to train those architectures have been\nproposed, but it is currently unclear what the relative contributions of the\narchitecture vs. the pretraining objective are to final model performance. We\nexplore the design space of such models, comparing architectural innovations\nand a variety of different pretraining objectives on a suite of evaluation\ntasks with a fixed training procedure in order to find the currently optimal\nway to build and train character-level BERT-like models. We find that our best\nperforming character-level model exceeds the performance of a token-based model\ntrained with the same settings on the same data, suggesting that\ncharacter-level models are ready for more widespread adoption. Unfortunately,\nthe best method to train character-level models still relies on a subword-level\ntokeniser during pretraining, and final model performance is highly dependent\non tokeniser quality. We believe our results demonstrate the readiness of\ncharacter-level models for multilingual language representation, and encourage\nNLP practitioners to try them as drop-in replacements for token-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1\">Kris Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good. (arXiv:2305.05471v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05471","description":"<p>With the recent advances in natural language processing (NLP), a vast number\nof applications have emerged across various use cases. Among the plethora of\nNLP applications, many academic researchers are motivated to do work that has a\npositive social impact, in line with the recent initiatives of NLP for Social\nGood (NLP4SG). However, it is not always obvious to researchers how their\nresearch efforts are tackling today's big social problems. Thus, in this paper,\nwe introduce NLP4SGPAPERS, a scientific dataset with three associated tasks\nthat can help identify NLP4SG papers and characterize the NLP4SG landscape by:\n(1) identifying the papers that address a social problem, (2) mapping them to\nthe corresponding UN Sustainable Development Goals (SDGs), and (3) identifying\nthe task they are solving and the methods they are using. Using\nstate-of-the-art NLP models, we address each of these tasks and use them on the\nentire ACL Anthology, resulting in a visualization workspace that gives\nresearchers a comprehensive overview of the field of NLP4SG. Our website is\navailable at https://nlp4sg.vercel.app . We released our data at\nhttps://huggingface.co/datasets/feradauto/NLP4SGPapers and code at\nhttps://github.com/feradauto/nlp4sg .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Fernando Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beydoun_J/0/1/0/all/0/1\">Jad Beydoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Going beyond research datasets: Novel intent discovery in the industry setting. (arXiv:2305.05474v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05474","description":"<p>Novel intent discovery automates the process of grouping similar messages\n(questions) to identify previously unknown intents. However, current research\nfocuses on publicly available datasets which have only the question field and\nsignificantly differ from real-life datasets. This paper proposes methods to\nimprove the intent discovery pipeline deployed in a large e-commerce platform.\nWe show the benefit of pre-training language models on in-domain data: both\nself-supervised and with weak supervision. We also devise the best method to\nutilize the conversational structure (i.e., question and answer) of real-life\ndatasets during fine-tuning for clustering tasks, which we call Conv. All our\nmethods combined to fully utilize real-life datasets give up to 33pp\nperformance boost over state-of-the-art Constrained Deep Adaptive Clustering\n(CDAC) model for question only. By comparison CDAC model for the question data\nonly gives only up to 13pp performance boost over the naive baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chrabrowa_A/0/1/0/all/0/1\">Aleksandra Chrabrowa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadeliya_T/0/1/0/all/0/1\">Tsimur Hadeliya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajtoch_D/0/1/0/all/0/1\">Dariusz Kajtoch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mroczkowski_R/0/1/0/all/0/1\">Robert Mroczkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybak_P/0/1/0/all/0/1\">Piotr Rybak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05480","description":"<p>We would like to explore how morphemes can affect the performance of a\nlanguage model. We trained GPT-2 and Bert model with StateMorph for both\nFinnish and Russian, which is a morpheme segmenting algorithm. As a comparison,\nwe also trained a model with BPE and Morfessor. Our preliminary result shows\nthat StateMorph can help the model to converge more efficiently and achieve a\nbetter validation score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jue Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katinskaia_A/0/1/0/all/0/1\">Anisia Katinskaia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_A/0/1/0/all/0/1\">Anh-Duc Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yangarber_R/0/1/0/all/0/1\">Roman Yangarber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAUPQA: Massive Automatically-created Polish Question Answering Dataset. (arXiv:2305.05486v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05486","description":"<p>Recently, open-domain question answering systems have begun to rely heavily\non annotated datasets to train neural passage retrievers. However, manually\nannotating such datasets is both difficult and time-consuming, which limits\ntheir availability for less popular languages. In this work, we experiment with\nseveral methods for automatically collecting weakly labeled datasets and show\nhow they affect the performance of the neural passage retrieval models. As a\nresult of our work, we publish the MAUPQA dataset, consisting of nearly 400,000\nquestion-passage pairs for Polish, as well as the HerBERT-QA neural retriever.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rybak_P/0/1/0/all/0/1\">Piotr Rybak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Pseudo Image Captions for Multimodal Summarization. (arXiv:2305.05496v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05496","description":"<p>Cross-modal contrastive learning in vision language pretraining (VLP) faces\nthe challenge of (partial) false negatives. In this paper, we study this\nproblem from the perspective of Mutual Information (MI) optimization. It is\ncommon sense that InfoNCE loss used in contrastive learning will maximize the\nlower bound of MI between anchors and their positives, while we theoretically\nprove that MI involving negatives also matters when noises commonly exist.\nGuided by a more general lower bound form for optimization, we propose a\ncontrastive learning strategy regulated by progressively refined cross-modal\nsimilarity, to more accurately optimize MI between an image/text anchor and its\nnegative texts/images instead of improperly minimizing it. Our method performs\ncompetitively on four downstream cross-modal tasks and systematically balances\nthe beneficial and harmful effects of (partial) false negative samples under\ntheoretical guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chaoya Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Rui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jinan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Humanize Technology. (arXiv:2305.05576v1 [cs.CY])","link":"http://arxiv.org/abs/2305.05576","description":"<p>Large Language Models (LLMs) have made rapid progress in recent months and\nweeks, garnering significant public attention. This has sparked concerns about\naligning these models with human values, their impact on labor markets, and the\npotential need for regulation in further research and development. However, the\ndiscourse often lacks a focus on the imperative to widely diffuse the societal\nbenefits of LLMs. To qualify this societal benefit, we assert that LLMs exhibit\nemergent abilities to humanize technology more effectively than previous\ntechnologies, and for people across language, occupation, and accessibility\ndivides. We argue that they do so by addressing three mechanizing bottlenecks\nin today's computing technologies: creating diverse and accessible content,\nlearning complex digital tools, and personalizing machine learning algorithms.\nWe adopt a case-based approach and illustrate each bottleneck with two examples\nwhere current technology imposes bottlenecks that LLMs demonstrate the ability\nto address. Given this opportunity to humanize technology widely, we advocate\nfor more widespread understanding of LLMs, tools and methods to simplify use of\nLLMs, and cross-cutting institutional capacity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure. (arXiv:2305.05588v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05588","description":"<p>This work explores the utility of explicit structure for representation\nlearning in NLP by developing StrAE -- an autoencoding framework that\nfaithfully leverages sentence structure to learn multi-level node embeddings in\nan unsupervised fashion. We use StrAE to train models across different types of\nsentential structure and objectives, including a novel contrastive loss over\nstructure, and evaluate the learnt embeddings on a series of both intrinsic and\nextrinsic tasks. Our experiments indicate that leveraging explicit structure\nthrough StrAE leads to improved embeddings over prior work, and that our novel\ncontrastive objective over structure outperforms the standard cross-entropy\nobjective. Moreover, in contrast to findings from prior work that weakly\nleverages structure, we find that being completely faithful to structure does\nenable disambiguation between types of structure based on the corresponding\nmodel's performance. As further evidence of StrAE's utility, we develop a\nsimple proof-of-concept approach to simultaneously induce structure while\nlearning embeddings, rather than being given structure, and find that\nperformance is comparable to that of the best-performing models where structure\nis given. Finally, we contextualise these results by comparing StrAE against\nstandard unstructured baselines learnt in similar settings, and show that\nfaithfully leveraging explicit structure can be beneficial in lexical and\nsentence-level semantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opper_M/0/1/0/all/0/1\">Mattia Opper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prokhorov_V/0/1/0/all/0/1\">Victor Prokhorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DomainInv: Domain Invariant Fine Tuning and Adversarial Label Correction For QA Domain Adaptation. (arXiv:2305.05589v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05589","description":"<p>Existing Question Answering (QA) systems limited by the capability of\nanswering questions from unseen domain or any out-of-domain distributions\nmaking them less reliable for deployment to real scenarios. Most importantly\nall the existing QA domain adaptation methods are either based on generating\nsynthetic data or pseudo labeling the target domain data. The domain adaptation\nmethods based on synthetic data and pseudo labeling suffers either from the\nrequirement of computational resources or an extra overhead of carefully\nselecting the confidence threshold to separate the noisy examples from being in\nthe training dataset. In this paper, we propose the unsupervised domain\nadaptation for unlabeled target domain by transferring the target\nrepresentation near to source domain while still using the supervision from\nsource domain. Towards that we proposed the idea of domain invariant fine\ntuning along with adversarial label correction to identify the target instances\nwhich lie far apart from the source domain, so that the feature encoder can be\nlearnt to minimize the distance between such target instances and source\ninstances class wisely, removing the possibility of learning the features of\ntarget domain which are still near to source support but are ambiguous.\nEvaluation of our QA domain adaptation method namely, DomainInv on multiple\ntarget QA dataset reveal the performance improvement over the strongest\nbaseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Anant Khandelwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Case Records of ChatGPT: Language Models and Complex Clinical Questions. (arXiv:2305.05609v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05609","description":"<p>Background: Artificial intelligence language models have shown promise in\nvarious applications, including assisting with clinical decision-making as\ndemonstrated by strong performance of large language models on medical\nlicensure exams. However, their ability to solve complex, open-ended cases,\nwhich may be representative of clinical practice, remains unexplored. Methods:\nIn this study, the accuracy of large language AI models GPT4 and GPT3.5 in\ndiagnosing complex clinical cases was investigated using published Case Records\nof the Massachusetts General Hospital. A total of 50 cases requiring a\ndiagnosis and diagnostic test published from January 1, 2022 to April 16, 2022\nwere identified. For each case, models were given a prompt requesting the top\nthree specific diagnoses and associated diagnostic tests, followed by case\ntext, labs, and figure legends. Model outputs were assessed in comparison to\nthe final clinical diagnosis and whether the model-predicted test would result\nin a correct diagnosis. Results: GPT4 and GPT3.5 accurately provided the\ncorrect diagnosis in 26% and 22% of cases in one attempt, and 46% and 42%\nwithin three attempts, respectively. GPT4 and GPT3.5 provided a correct\nessential diagnostic test in 28% and 24% of cases in one attempt, and 44% and\n50% within three attempts, respectively. No significant differences were found\nbetween the two models, and multiple trials with identical prompts using the\nGPT3.5 model provided similar results. Conclusions: In summary, these models\ndemonstrate potential usefulness in generating differential diagnoses but\nremain limited in their ability to provide a single unifying diagnosis in\ncomplex, open-ended cases. Future research should focus on evaluating model\nperformance in larger datasets of open-ended clinical challenges and exploring\npotential human-AI collaboration strategies to enhance clinical\ndecision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poterucha_T/0/1/0/all/0/1\">Timothy Poterucha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elias_P/0/1/0/all/0/1\">Pierre Elias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haggerty_C/0/1/0/all/0/1\">Christopher M. Haggerty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text. (arXiv:2305.05627v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05627","description":"<p>Standard methods for multi-label text classification largely rely on\nencoder-only pre-trained language models, whereas encoder-decoder models have\nproven more effective in other classification tasks. In this study, we compare\nfour methods for multi-label classification, two based on an encoder only, and\ntwo based on an encoder-decoder. We carry out experiments on four datasets --\ntwo in the legal domain and two in the biomedical domain, each with two levels\nof label granularity -- and always depart from the same pre-trained model, T5.\nOur results show that encoder-decoder methods outperform encoder-only methods,\nwith a growing advantage on more complex datasets and labeling schemes of finer\ngranularity. Using encoder-decoder models in a non-autoregressive fashion, in\nparticular, yields the best performance overall, so we further study this\napproach through ablations to better understand its strengths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kementchedjhieva_Y/0/1/0/all/0/1\">Yova Kementchedjhieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare. (arXiv:2305.05640v1 [cs.AI])","link":"http://arxiv.org/abs/2305.05640","description":"<p>Knowledge graphs (KGs) are a popular way to organise information based on\nontologies or schemas and have been used across a variety of scenarios from\nsearch to recommendation. Despite advances in KGs, representing knowledge\nremains a non-trivial task across industries and it is especially challenging\nin the biomedical and healthcare domains due to complex interdependent\nrelations between entities, heterogeneity, lack of standardization, and\nsparseness of data. KGs are used to discover diagnoses or prioritize genes\nrelevant to disease, but they often rely on schemas that are not centred around\na node or entity of interest, such as a person. Entity-centric KGs are\nrelatively unexplored but hold promise in representing important facets\nconnected to a central node and unlocking downstream tasks beyond graph\ntraversal and reasoning, such as generating graph embeddings and training graph\nneural networks for a wide range of predictive tasks. This paper presents an\nend-to-end representation learning framework to extract entity-centric KGs from\nstructured and unstructured data. We introduce a star-shaped ontology to\nrepresent the multiple facets of a person and use it to guide KG creation.\nCompact representations of the graphs are created leveraging graph neural\nnetworks and experiments are conducted using different levels of heterogeneity\nor explicitness. A readmission prediction task is used to evaluate the results\nof the proposed framework, showing a stable system, robust to missing data,\nthat outperforms a range of baseline machine learning classifiers. We highlight\nthat this approach has several potential applications across domains and is\nopen-sourced. Lastly, we discuss lessons learned, challenges, and next steps\nfor the adoption of the framework in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theodoropoulos_C/0/1/0/all/0/1\">Christos Theodoropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulligan_N/0/1/0/all/0/1\">Natasha Mulligan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stappenbeck_T/0/1/0/all/0/1\">Thaddeus Stappenbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bettencourt_Silva_J/0/1/0/all/0/1\">Joao Bettencourt-Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Building the Federated GPT: Federated Instruction Tuning. (arXiv:2305.05644v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05644","description":"<p>While ``instruction-tuned\" generative large language models (LLMs) have\ndemonstrated an impressive ability to generalize to new tasks, the training\nphases heavily rely on large amounts of diverse and high-quality instruction\ndata (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data,\nespecially when it comes to human-written data, can pose significant challenges\nboth in terms of cost and accessibility. Moreover, concerns related to privacy\ncan further limit access to such data, making the process of obtaining it a\ncomplex and nuanced undertaking. Consequently, this hinders the generality of\nthe tuned models and may restrict their effectiveness in certain contexts. To\ntackle this issue, our study introduces a new approach called Federated\nInstruction Tuning (FedIT), which leverages federated learning (FL) as the\nlearning framework for the instruction tuning of LLMs. This marks the first\nexploration of FL-based instruction tuning for LLMs. This is especially\nimportant since text data is predominantly generated by end users. Therefore,\nit is imperative to design and adapt FL approaches to effectively leverage\nthese users' diverse instructions stored on local devices, while preserving\nprivacy and ensuring data security. In the current paper, by conducting widely\nused GPT-4 auto-evaluation, we demonstrate that by exploiting the heterogeneous\nand diverse sets of instructions on the client's end with the proposed\nframework FedIT, we improved the performance of LLMs compared to centralized\ntraining with only limited local instructions. Further, in this paper, we\ndeveloped a Github repository named Shepherd. This repository offers a\nfoundational framework for exploring federated fine-tuning of LLMs using\nheterogeneous instructions across diverse categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahidian_S/0/1/0/all/0/1\">Saeed Vahidian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_M/0/1/0/all/0/1\">Martin Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiran Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TidyBot: Personalized Robot Assistance with Large Language Models. (arXiv:2305.05658v1 [cs.RO])","link":"http://arxiv.org/abs/2305.05658","description":"<p>For a robot to personalize physical assistance effectively, it must learn\nuser preferences that can be generally reapplied to future scenarios. In this\nwork, we investigate personalization of household cleanup with robots that can\ntidy up rooms by picking up objects and putting them away. A key challenge is\ndetermining the proper place to put each object, as people's preferences can\nvary greatly depending on personal taste or cultural background. For instance,\none person may prefer storing shirts in the drawer, while another may prefer\nthem on the shelf. We aim to build systems that can learn such preferences from\njust a handful of examples via prior interactions with a particular person. We\nshow that robots can combine language-based planning and perception with the\nfew-shot summarization capabilities of large language models (LLMs) to infer\ngeneralized user preferences that are broadly applicable to future\ninteractions. This approach enables fast adaptation and achieves 91.2% accuracy\non unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away\n85.0% of objects in real-world test scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jimmy Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonova_R/0/1/0/all/0/1\">Rika Antonova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_A/0/1/0/all/0/1\">Adam Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepert_M/0/1/0/all/0/1\">Marion Lepert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1\">Jeannette Bohg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusinkiewicz_S/0/1/0/all/0/1\">Szymon Rusinkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1\">Thomas Funkhouser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A transfer learning based approach for pronunciation scoring. (arXiv:2111.00976v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00976","description":"<p>Phone-level pronunciation scoring is a challenging task, with performance far\nfrom that of human annotators. Standard systems generate a score for each phone\nin a phrase using models trained for automatic speech recognition (ASR) with\nnative data only. Better performance has been shown when using systems that are\ntrained specifically for the task using non-native data. Yet, such systems face\nthe challenge that datasets labelled for this task are scarce and usually\nsmall. In this paper, we present a transfer learning-based approach that\nleverages a model trained for ASR, adapting it for the task of pronunciation\nscoring. We analyze the effect of several design choices and compare the\nperformance with a state-of-the-art goodness of pronunciation (GOP) system. Our\nfinal system is 20% better than the GOP system on EpaDB, a database for\npronunciation scoring research, for a cost function that prioritizes low rates\nof unnecessary corrections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sancinetti_M/0/1/0/all/0/1\">Marcelo Sancinetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1\">Jazmin Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonomi_C/0/1/0/all/0/1\">Cyntia Bonomi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1\">Luciana Ferrer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vector Space Semantics for Lambek Calculus with Soft Subexponentials. (arXiv:2111.11331v2 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2111.11331","description":"<p>We develop a vector space semantics for Lambek Calculus with Soft\nSubexponentials, apply the calculus to construct compositional vector\ninterpretations for parasitic gap noun phrases and discourse units with\nanaphora and ellipsis, and experiment with the constructions in a\ndistributional sentence similarity task. As opposed to previous work, which\nused Lambek Calculus with a Relevant Modality the calculus used in this paper\nuses a bounded version of the modality and is decidable. The vector space\nsemantics of this new modality allows us to meaningfully define contraction as\nprojection and provide a linear theory behind what we could previously only\nachieve via nonlinear maps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McPheat_L/0/1/0/all/0/1\">Lachlan McPheat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wazni_H/0/1/0/all/0/1\">Hadi Wazni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1\">Mehrnoosh Sadrzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT WEAVER: Using WEight AVERaging to Enable Lifelong Learning for Transformer-based Models in the Biomedical Domain. (arXiv:2202.10101v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.10101","description":"<p>Recent developments in transfer learning have boosted the advancements in\nnatural language processing tasks. The performance is, however, dependent on\nhigh-quality, manually annotated training data. Especially in the biomedical\ndomain, it has been shown that one training corpus is not enough to learn\ngeneric models that are able to efficiently predict on new data. Therefore,\nstate-of-the-art models need the ability of lifelong learning in order to\nimprove performance as soon as new data are available - without the need of\nre-training the whole model from scratch. We present WEAVER, a simple, yet\nefficient post-processing method that infuses old knowledge into the new model,\nthereby reducing catastrophic forgetting. We show that applying WEAVER in a\nsequential manner results in similar word embedding distributions as doing a\ncombined training on all data at once, while being computationally more\nefficient. Because there is no need of data sharing, the presented method is\nalso easily applicable to federated learning settings and can for example be\nbeneficial for the mining of electronic health records from different clinics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuhnel_L/0/1/0/all/0/1\">Lisa K&#xfc;hnel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1\">Alexander Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fluck_J/0/1/0/all/0/1\">Juliane Fluck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multimodal Transformer: Fusing Clinical Notes with Structured EHR Data for Interpretable In-Hospital Mortality Prediction. (arXiv:2208.10240v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.10240","description":"<p>Deep-learning-based clinical decision support using structured electronic\nhealth records (EHR) has been an active research area for predicting risks of\nmortality and diseases. Meanwhile, large amounts of narrative clinical notes\nprovide complementary information, but are often not integrated into predictive\nmodels. In this paper, we provide a novel multimodal transformer to fuse\nclinical notes and structured EHR data for better prediction of in-hospital\nmortality. To improve interpretability, we propose an integrated gradients (IG)\nmethod to select important words in clinical notes and discover the critical\nstructured EHR features with Shapley values. These important words and clinical\nfeatures are visualized to assist with interpretation of the prediction\noutcomes. We also investigate the significance of domain adaptive pretraining\nand task adaptive fine-tuning on the Clinical BERT, which is used to learn the\nrepresentations of clinical notes. Experiments demonstrated that our model\noutperforms other methods (AUCPR: 0.538, AUCROC: 0.877, F1:0.490).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_W/0/1/0/all/0/1\">Weimin Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_R/0/1/0/all/0/1\">Rachel Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Songzhu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abell_Hart_K/0/1/0/all/0/1\">Kayley Abell-Hart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fusheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Reality and the Limits of Language Data: Aligning LLMs with Human Norms. (arXiv:2208.11981v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.11981","description":"<p>Recent advancements in Large Language Models (LLMs) harness linguistic\nassociations in vast natural language data for practical applications. However,\ntheir ability to understand the physical world using only language data remains\na question. After reviewing existing protocols, we explore this question using\na novel and tightly controlled reasoning test (ART) and compare human norms\nagainst versions of GPT-3. Our findings highlight the categories of\ncommon-sense relations models that could learn directly from data and areas of\nweakness. GPT-3 offers evidence for verbal reasoning on a par with human\nsubjects for several relations including Synonymy, Antonymy, and Default\ninheritance, Without reinforcement learning from human judgements, it appears\nGPT-3 performs at the lower end of the reference interval for Has-part and\nContained-in. Weaknesses were observed also in affordance characteristics\nthrough Necessary-quality, Order-of-size and Order-of-intensity. Combining LLMs\nwith symbolic world grounding is a promising direction to address associative\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel H. Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1\">Ehsan Shareghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach. (arXiv:2209.06995v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.06995","description":"<p>Large Language Models have demonstrated remarkable few-shot performance, but\nthe performance can be sensitive to the selection of few-shot instances. We\npropose PATRON, a new method that uses prompt-based uncertainty estimation for\ndata selection for pre-trained language model fine-tuning under cold-start\nscenarios, i.e., no initial labeled data are available. In PATRON, we design\n(1) a prompt-based uncertainty propagation approach to estimate the importance\nof data points and (2) a partition-then-rewrite (PTR) strategy to promote\nsample diversity when querying for annotations. Experiments on six text\nclassification datasets show that PATRON outperforms the strongest cold-start\ndata selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON\nachieves 91.0% and 92.1% of the fully supervised performance based on vanilla\nfine-tuning and prompt-based learning respectively. Our implementation of\nPATRON is available at \\url{https://github.com/yueyu1030/Patron}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jieyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Span Representations for Named Entity Recognition. (arXiv:2210.04182v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04182","description":"<p>Span-based models are one of the most straightforward methods for named\nentity recognition (NER). Existing span-based NER systems shallowly aggregate\nthe token representations to span representations. However, this typically\nresults in significant ineffectiveness for long-span entities, a coupling\nbetween the representations of overlapping spans, and ultimately a performance\ndegradation. In this study, we propose DSpERT (Deep Span Encoder\nRepresentations from Transformers), which comprises a standard Transformer and\na span Transformer. The latter uses low-layered span representations as\nqueries, and aggregates the token representations as keys and values, layer by\nlayer from bottom to top. Thus, DSpERT produces span representations of deep\nsemantics.\n</p>\n<p>With weight initialization from pretrained language models, DSpERT achieves\nperformance higher than or competitive with recent state-of-the-art systems on\neight NER benchmarks. Experimental results verify the importance of the depth\nfor span representations, and show that DSpERT performs particularly well on\nlong-span entities and nested structures. Further, the deep span\nrepresentations are well structured and easily separable in the feature space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">Enwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Agnostic Multilingual Information Retrieval with Contrastive Learning. (arXiv:2210.06633v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2210.06633","description":"<p>Multilingual information retrieval (IR) is challenging since annotated\ntraining data is costly to obtain in many languages. We present an effective\nmethod to train multilingual IR systems when only English IR training data and\nsome parallel corpora between English and other languages are available. We\nleverage parallel and non-parallel corpora to improve the pretrained\nmultilingual language models' cross-lingual transfer ability. We design a\nsemantic contrastive loss to align representations of parallel sentences that\nshare the same semantics in different languages, and a new language contrastive\nloss to leverage parallel sentence pairs to remove language-specific\ninformation in sentence representations from non-parallel corpora. When trained\non English IR data with these losses and evaluated zero-shot on non-English\ndata, our model demonstrates significant improvement to prior work on retrieval\nperformance, while it requires much less computational effort. We also\ndemonstrate the value of our model for a practical setting when a parallel\ncorpus is only available for a few languages, but a lack of parallel corpora\nresources persists for many other low-resource languages. Our model can work\nwell even with a small number of parallel sentences, and be used as an add-on\nmodule to any backbones and other tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiyang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinchi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1\">Peng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Deguang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunlun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiheng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Demographic Factors Improve Text Classification? Revisiting Demographic Adaptation in the Age of Transformers. (arXiv:2210.07362v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07362","description":"<p>Demographic factors (e.g., gender or age) shape our language. Previous work\nshowed that incorporating demographic factors can consistently improve\nperformance for various NLP tasks with traditional NLP models. In this work, we\ninvestigate whether these previous findings still hold with state-of-the-art\npretrained Transformer-based language models (PLMs). We use three common\nspecialization methods proven effective for incorporating external knowledge\ninto pretrained Transformers (e.g., domain-specific or geographic knowledge).\nWe adapt the language representations for the demographic dimensions of gender\nand age, using continuous language modeling and dynamic multi-task learning for\nadaptation, where we couple language modeling objectives with the prediction of\ndemographic classes. Our results, when employing a multilingual PLM, show\nsubstantial gains in task performance across four languages (English, German,\nFrench, and Danish), which is consistent with the results of previous work.\nHowever, controlling for confounding factors - primarily domain and language\nproficiency of Transformer-based PLMs - shows that downstream performance gains\nfrom our demographic adaptation do not actually stem from demographic\nknowledge. Our results indicate that demographic specialization of PLMs, while\nholding promise for positive societal impact, still represents an unsolved\nproblem for (modern) NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hung_C/0/1/0/all/0/1\">Chia-Chien Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Learning for Analyzing Political Campaigns on Facebook. (arXiv:2210.10669v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10669","description":"<p>Social media platforms are currently the main channel for political\nmessaging, allowing politicians to target specific demographics and adapt based\non their reactions. However, making this communication transparent is\nchallenging, as the messaging is tightly coupled with its intended audience and\noften echoed by multiple stakeholders interested in advancing specific\npolicies. Our goal in this paper is to take a first step towards understanding\nthese highly decentralized settings. We propose a weakly supervised approach to\nidentify the stance and issue of political ads on Facebook and analyze how\npolitical campaigns use some kind of demographic targeting by location, gender,\nor age. Furthermore, we analyze the temporal dynamics of the political ads on\nelection polls.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tunazzina Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shamik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Once-for-All Sequence Compression for Self-Supervised Speech Models. (arXiv:2211.02332v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.02332","description":"<p>The sequence length along the time axis is often the dominant factor of the\ncomputation in speech processing. Works have been proposed to reduce the\nsequence length for lowering the computational cost in self-supervised speech\nmodels. However, different downstream tasks have different tolerance of\nsequence compressing, so a model that produces a fixed compressing rate may not\nfit all tasks. In this work, we introduce a once-for-all (OFA) sequence\ncompression framework for self-supervised speech models that supports a\ncontinuous range of operating compressing rates. The framework is evaluated on\nvarious tasks, showing marginal degradation compared to the fixed compressing\nrate variants with a smooth performance-efficiency trade-off. We further\nexplore adaptive compressing rate learning, demonstrating the ability to select\ntask-specific preferred frame periods without needing a grid search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsuan-Jui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yen Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse Relation Recognition. (arXiv:2211.13873v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.13873","description":"<p>Due to the absence of explicit connectives, implicit discourse relation\nrecognition (IDRR) remains a challenging task in discourse analysis. The\ncritical step for IDRR is to learn high-quality discourse relation\nrepresentations between two arguments. Recent methods tend to integrate the\nwhole hierarchical information of senses into discourse relation\nrepresentations for multi-level sense recognition. Nevertheless, they\ninsufficiently incorporate the static hierarchical structure containing all\nsenses (defined as global hierarchy), and ignore the hierarchical sense label\nsequence corresponding to each instance (defined as local hierarchy). For the\npurpose of sufficiently exploiting global and local hierarchies of senses to\nlearn better discourse relation representations, we propose a novel GlObal and\nLocal Hierarchy-aware Contrastive Framework (GOLF), to model two kinds of\nhierarchies with the aid of multi-task learning and contrastive learning.\nExperimental results on PDTB 2.0 and PDTB 3.0 datasets demonstrate that our\nmethod remarkably outperforms current state-of-the-art models at all\nhierarchical levels. Our code is publicly available at\nhttps://github.com/YJiangcm/GOLF_for_IDRR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Validating Large Language Models with ReLM. (arXiv:2211.15458v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2211.15458","description":"<p>Although large language models (LLMs) have been touted for their ability to\ngenerate natural-sounding text, there are growing concerns around possible\nnegative effects of LLMs such as data memorization, bias, and inappropriate\nlanguage. Unfortunately, the complexity and generation capacities of LLMs make\nvalidating (and correcting) such concerns difficult. In this work, we introduce\nReLM, a system for validating and querying LLMs using standard regular\nexpressions. ReLM formalizes and enables a broad range of language model\nevaluations, reducing complex evaluation rules to simple regular expression\nqueries. Our results exploring queries surrounding memorization, gender bias,\ntoxicity, and language understanding show that ReLM achieves up to 15x higher\nsystem efficiency, 2.5x data efficiency, and increased statistical and\nprompt-tuning coverage compared to state-of-the-art ad-hoc queries. ReLM offers\na competitive and general baseline for the increasingly important problem of\nLLM validation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuchnik_M/0/1/0/all/0/1\">Michael Kuchnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1\">Virginia Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amvrosiadis_G/0/1/0/all/0/1\">George Amvrosiadis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator. (arXiv:2212.10218v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10218","description":"<p>Pre-trained models have achieved remarkable success in natural language\nprocessing (NLP). However, existing pre-training methods underutilize the\nbenefits of language understanding for generation. Inspired by the idea of\nGenerative Adversarial Networks (GANs), we propose a GAN-style model for\nencoder-decoder pre-training by introducing an auxiliary discriminator,\nunifying the ability of language understanding and generation in a single\nmodel. Our model, named as GanLM, is trained with two pre-training objectives:\nreplaced token detection and replaced token denoising. Specifically, given\nmasked source sentences, the generator outputs the target distribution and the\ndiscriminator predicts whether the target sampled tokens from distribution are\nincorrect. The target sentence is replaced with misclassified tokens to\nconstruct noisy previous context, which is used to generate the gold sentence.\nIn general, both tasks improve the ability of language understanding and\ngeneration by selectively using the denoising data. Extensive experiments in\nlanguage generation benchmarks show that GanLM with the powerful language\nunderstanding capability outperforms various strong pre-trained language models\n(PLMs) and achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuwei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liqun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers. (arXiv:2212.10325v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10325","description":"<p>Diffusion model, a new generative modelling paradigm, has achieved great\nsuccess in image, audio, and video generation. However, considering the\ndiscrete categorical nature of text, it is not trivial to extend continuous\ndiffusion models to natural language, and text diffusion models are less\nstudied. Sequence-to-sequence text generation is one of the essential natural\nlanguage processing topics. In this work, we apply diffusion models to approach\nsequence-to-sequence text generation, and explore whether the superiority\ngeneration performance of diffusion model can transfer to natural language\ndomain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence\ngeneration. SeqDiffuSeq uses an encoder-decoder Transformers architecture to\nmodel denoising function. In order to improve generation quality, SeqDiffuSeq\ncombines the self-conditioning technique and a newly proposed adaptive noise\nschedule technique. The adaptive noise schedule has the difficulty of denoising\nevenly distributed across time steps, and considers exclusive noise schedules\nfor tokens at different positional order. Experiment results illustrate the\ngood performance on sequence-to-sequence generation in terms of text quality\nand inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretraining Without Attention. (arXiv:2212.10544v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10544","description":"<p>Transformers have been essential to pretraining success in NLP. While other\narchitectures have been used, downstream accuracy is either significantly\nworse, or requires attention layers to match standard benchmarks such as GLUE.\nThis work explores pretraining without attention by using recent advances in\nsequence routing based on state-space models (SSMs). Our proposed model,\nBidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative\ngating architecture that has been effective in simplified sequence modeling\narchitectures. The model learns static layers that do not consider pair-wise\ninteractions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE\nand can be extended to long-form pretraining of 4096 tokens without\napproximation. Analysis shows that while the models have similar average\naccuracy, the approach has different inductive biases than BERT in terms of\ninteractions and syntactic representations. All models from this work are\navailable at https://github.com/jxiw/BiGS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jing Nathan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1\">Albert Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Machine Translation with Large Language Models. (arXiv:2301.13294v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.13294","description":"<p>Consistency is a key requirement of high-quality translation. It is\nespecially important to adhere to pre-approved terminology and adapt to\ncorrected translations in domain-specific projects. Machine translation (MT)\nhas achieved significant progress in the area of domain adaptation. However,\nreal-time adaptation remains challenging. Large-scale language models (LLMs)\nhave recently shown interesting capabilities of in-context learning, where they\nlearn to replicate certain input-output text generation patterns, without\nfurther fine-tuning. By feeding an LLM at inference time with a prompt that\nconsists of a list of translation pairs, it can then simulate the domain and\nstyle characteristics. This work aims to investigate how we can utilize\nin-context learning to improve real-time adaptive MT. Our extensive experiments\nshow promising results at translation time. For example, LLMs can adapt to a\nset of in-domain sentence pairs and/or terminology while translating a new\nsentence. We observe that the translation quality with few-shot in-context\nlearning can surpass that of strong encoder-decoder MT systems, especially for\nhigh-resource languages. Moreover, we investigate whether we can combine MT\nfrom strong encoder-decoder models with fuzzy matches, which can further\nimprove translation quality, especially for less supported languages. We\nconduct our experiments across five diverse language pairs, namely\nEnglish-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French\n(EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moslem_Y/0/1/0/all/0/1\">Yasmin Moslem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_R/0/1/0/all/0/1\">Rejwanul Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelleher_J/0/1/0/all/0/1\">John D. Kelleher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Way_A/0/1/0/all/0/1\">Andy Way</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creating a Large Language Model of a Philosopher. (arXiv:2302.01339v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.01339","description":"<p>Can large language models be trained to produce philosophical texts that are\ndifficult to distinguish from texts produced by human philosophers? To address\nthis question, we fine-tuned OpenAI's GPT-3 with the works of philosopher\nDaniel C. Dennett as additional training data. To explore the Dennett model, we\nasked the real Dennett ten philosophical questions and then posed the same\nquestions to the language model, collecting four responses for each question\nwithout cherry-picking. We recruited 425 participants to distinguish Dennett's\nanswer from the four machine-generated answers. Experts on Dennett's work (N =\n25) succeeded 51% of the time, above the chance rate of 20% but short of our\nhypothesized rate of 80% correct. For two of the ten questions, the language\nmodel produced at least one answer that experts selected more frequently than\nDennett's own answer. Philosophy blog readers (N = 302) performed similarly to\nthe experts, while ordinary research participants (N = 98) were near chance\ndistinguishing GPT-3's responses from those of an \"actual human philosopher\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwitzgebel_E/0/1/0/all/0/1\">Eric Schwitzgebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwitzgebel_D/0/1/0/all/0/1\">David Schwitzgebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strasser_A/0/1/0/all/0/1\">Anna Strasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distinguishability Calibration to In-Context Learning. (arXiv:2302.06198v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.06198","description":"<p>Recent years have witnessed increasing interests in prompt-based learning in\nwhich models can be trained on only a few annotated instances, making them\nsuitable in low-resource settings. When using prompt-based learning for text\nclassification, the goal is to use a pre-trained language model (PLM) to\npredict a missing token in a pre-defined template given an input text, which\ncan be mapped to a class label. However, PLMs built on the transformer\narchitecture tend to generate similar output embeddings, making it difficult to\ndiscriminate between different class labels. The problem is further exacerbated\nwhen dealing with classification tasks involving many fine-grained class\nlabels. In this work, we alleviate this information diffusion issue, i.e.,\ndifferent tokens share a large proportion of similar information after going\nthrough stacked multiple self-attention layers in a transformer, by proposing a\ncalibration method built on feature transformations through rotation and\nscaling to map a PLM-encoded embedding into a new metric space to guarantee the\ndistinguishability of the resulting embeddings. Furthermore, we take the\nadvantage of hyperbolic embeddings to capture the hierarchical relations among\nfine-grained class-associated token embedding by a coarse-to-fine metric\nlearning strategy to enhance the distinguishability of the learned output\nembeddings. Extensive experiments on the three datasets under various settings\ndemonstrate the effectiveness of our approach. Our code can be found at\nhttps://github.com/donttal/TARA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hanqi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1\">Li Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.06675","description":"<p>We present a method to formulate algorithm discovery as program search, and\napply it to discover optimization algorithms for deep neural network training.\nWe leverage efficient search techniques to explore an infinite and sparse\nprogram space. To bridge the large generalization gap between proxy and target\ntasks, we also introduce program selection and simplification strategies. Our\nmethod discovers a simple and effective optimization algorithm, $\\textbf{Lion}$\n($\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}$).\nIt is more memory-efficient than Adam as it only keeps track of the momentum.\nDifferent from adaptive optimizers, its update has the same magnitude for each\nparameter calculated through the sign operation. We compare Lion with widely\nused optimizers, such as Adam and Adafactor, for training a variety of models\non different tasks. On image classification, Lion boosts the accuracy of ViT by\nup to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On\nvision-language contrastive learning, we achieve 88.3% $\\textit{zero-shot}$ and\n91.1% $\\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best\nresults by 2% and 0.1%, respectively. On diffusion models, Lion outperforms\nAdam by achieving a better FID score and reducing the training compute by up to\n2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion\nexhibits a similar or better performance compared to Adam. Our analysis of Lion\nreveals that its performance gain grows with the training batch size. It also\nrequires a smaller learning rate than Adam due to the larger norm of the update\nproduced by the sign function. Additionally, we examine the limitations of Lion\nand identify scenarios where its improvements are small or not statistically\nsignificant. Lion is also successfully deployed in production systems such as\nGoogle search ads CTR model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Da Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Real_E/0/1/0/all/0/1\">Esteban Real</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xuanyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_T/0/1/0/all/0/1\">Thang Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yifeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Translation Performance of a Large Multilingual Language Model: the Case of BLOOM. (arXiv:2303.01911v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.01911","description":"<p>The NLP community recently saw the release of a new large open-access\nmultilingual language model, BLOOM (BigScience et al., 2022) covering 46\nlanguages. We focus on BLOOM's multilingual ability by evaluating its machine\ntranslation performance across several datasets (WMT, Flores-101 and DiaBLa)\nand language pairs (high- and low-resourced). Our results show that 0-shot\nperformance suffers from overgeneration and generating in the wrong language,\nbut this is greatly improved in the few-shot setting, with very good results\nfor a number of language pairs. We study several aspects including prompt\ndesign, model sizes, cross-lingual transfer and the use of discursive context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1\">Rachel Bawden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascading and Direct Approaches to Unsupervised Constituency Parsing on Spoken Sentences. (arXiv:2303.08809v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08809","description":"<p>Past work on unsupervised parsing is constrained to written form. In this\npaper, we present the first study on unsupervised spoken constituency parsing\ngiven unlabeled spoken sentences and unpaired textual data. The goal is to\ndetermine the spoken sentences' hierarchical syntactic structure in the form of\nconstituency parse trees, such that each node is a span of audio that\ncorresponds to a constituent. We compare two approaches: (1) cascading an\nunsupervised automatic speech recognition (ASR) model and an unsupervised\nparser to obtain parse trees on ASR transcripts, and (2) direct training an\nunsupervised parser on continuous word-level speech representations. This is\ndone by first splitting utterances into sequences of word-level segments, and\naggregating self-supervised speech representations within segments to obtain\nsegment embeddings. We find that separately training a parser on the unpaired\ntext and directly applying it on ASR transcripts for inference produces better\nresults for unsupervised parsing. Additionally, our results suggest that\naccurate segmentation alone may be sufficient to parse spoken sentences\naccurately. Finally, we show the direct approach may learn head-directionality\ncorrectly for both head-initial and head-final languages without any explicit\ninductive bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tseng_Y/0/1/0/all/0/1\">Yuan Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Cheng-I Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual approaches for news genre, topic and persuasion technique classification. (arXiv:2303.09421v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.09421","description":"<p>This paper describes our approach for SemEval-2023 Task 3: Detecting the\ncategory, the framing, and the persuasion techniques in online news in a\nmulti-lingual setup. For Subtask 1 (News Genre), we propose an ensemble of\nfully trained and adapter mBERT models which was ranked joint-first for German,\nand had the highest mean rank of multi-language teams. For Subtask 2 (Framing),\nwe achieved first place in 3 languages, and the best average rank across all\nthe languages, by using two separate ensembles: a monolingual\nRoBERTa-MUPPETLARGE and an ensemble of XLM-RoBERTaLARGE with adapters and task\nadaptive pretraining. For Subtask 3 (Persuasion Techniques), we train a\nmonolingual RoBERTa-Base model for English and a multilingual mBERT model for\nthe remaining languages, which achieved top 10 for all languages, including 2nd\nfor English. For each subtask, we compared monolingual and multilingual\napproaches, and considered class imbalance techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Ben Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razuvayevskaya_O/0/1/0/all/0/1\">Olesya Razuvayevskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heppell_F/0/1/0/all/0/1\">Freddy Heppell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leite_J/0/1/0/all/0/1\">Jo&#xe3;o A. Leite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1\">Kalina Bontcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BloombergGPT: A Large Language Model for Finance. (arXiv:2303.17564v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.17564","description":"<p>The use of NLP in the realm of financial technology is broad and complex,\nwith applications ranging from sentiment analysis and named entity recognition\nto question answering. Large Language Models (LLMs) have been shown to be\neffective on a variety of tasks; however, no LLM specialized for the financial\ndomain has been reported in literature. In this work, we present BloombergGPT,\na 50 billion parameter language model that is trained on a wide range of\nfinancial data. We construct a 363 billion token dataset based on Bloomberg's\nextensive data sources, perhaps the largest domain-specific dataset yet,\naugmented with 345 billion tokens from general purpose datasets. We validate\nBloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite\nof internal benchmarks that most accurately reflect our intended usage. Our\nmixed dataset training leads to a model that outperforms existing models on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. Additionally, we explain our modeling choices, training\nprocess, and evaluation methodology. We release Training Chronicles (Appendix\nC) detailing our experience in training BloombergGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shijie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irsoy_O/0/1/0/all/0/1\">Ozan Irsoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Steven Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabravolski_V/0/1/0/all/0/1\">Vadim Dabravolski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kambadur_P/0/1/0/all/0/1\">Prabhanjan Kambadur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1\">David Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_G/0/1/0/all/0/1\">Gideon Mann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParroT: Translating During Chat Using Large Language Models. (arXiv:2304.02426v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02426","description":"<p>Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable\nabilities on a wide range of natural language processing (NLP) tasks, including\nvarious machine translation abilities accomplished during chat. However, these\nmodels are only accessible through restricted APIs, which creates barriers to\nnew research and advancements in the field. Therefore, we propose the\n$\\mathbf{ParroT}$ framework to enhance and regulate the translation abilities\nduring chat based on open-sourced LLMs (i.e., LLaMA-7b, BLOOMZ-7b-mt) and human\nwritten translation and evaluation data. Specifically, ParroT reformulates\ntranslation data into the instruction-following style, and introduces a\n\"$\\mathbf{Hint}$\" field for incorporating extra requirements to regulate the\ntranslation process. Accordingly, we propose three instruction types for\nfinetuning ParroT models, including translation instruction, contrastive\ninstruction, and error-guided instruction. We can finetune either the full\nmodels or partial parameters via low rank adaptation (LoRA). Experiments on\nFlores subsets and WMT22 test sets suggest that translation instruction\nimproves the translation performance of vanilla LLMs significantly while\nerror-guided instruction can lead to a further improvement, which demonstrates\nthe importance of learning from low-quality translations annotated by human.\nMeanwhile, the ParroT models can also preserve the ability on general tasks\nwith the Alpaca multi-task dataset involved in finetuning. Please refer to our\nGithub project for more implementation details:\nhttps://github.com/wxjiao/ParroT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tailoring Domain Adaptation for Machine Translation Quality Estimation. (arXiv:2304.08891v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.08891","description":"<p>While quality estimation (QE) can play an important role in the translation\nprocess, its effectiveness relies on the availability and quality of training\ndata. For QE in particular, high-quality labeled data is often lacking due to\nthe high cost and effort associated with labeling such data. Aside from the\ndata scarcity challenge, QE models should also be generalizable, i.e., they\nshould be able to handle data from different domains, both generic and\nspecific. To alleviate these two main issues -- data scarcity and domain\nmismatch -- this paper combines domain adaptation and data augmentation within\na robust QE system. Our method first trains a generic QE model and then\nfine-tunes it on a specific domain while retaining generic knowledge. Our\nresults show a significant improvement for all the language pairs investigated,\nbetter cross-lingual inference, and a superior performance in zero-shot\nlearning scenarios as compared to state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharami_J/0/1/0/all/0/1\">Javad Pourmostafa Roshan Sharami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shterionov_D/0/1/0/all/0/1\">Dimitar Shterionov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blain_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Blain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1\">Eva Vanmassenhove</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sisto_M/0/1/0/all/0/1\">Mirella De Sisto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emmery_C/0/1/0/all/0/1\">Chris Emmery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spronck_P/0/1/0/all/0/1\">Pieter Spronck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology. (arXiv:2304.11957v2 [physics.med-ph] UPDATED)","link":"http://arxiv.org/abs/2304.11957","description":"<p>The potential of large language models in medicine for education and decision\nmaking purposes has been demonstrated as they achieve decent scores on medical\nexams such as the United States Medical Licensing Exam (USMLE) and the MedQA\nexam. In this work, we evaluate the performance of ChatGPT-4 in the specialized\nfield of radiation oncology using the 38th American College of Radiology (ACR)\nradiation oncology in-training (TXIT) exam and the 2022 red journal gray zone\ncases. For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of\n63.65% and 74.57%, respectively, highlighting the advantage of the latest\nChatGPT-4 model. Based on the TXIT exam, ChatGPT-4's strong and weak areas in\nradiation oncology are identified to some extent. Specifically, ChatGPT-4\ndemonstrates good knowledge of statistics, CNS &amp; eye, pediatrics, biology, and\nphysics but has limitations in bone &amp; soft tissue and gynecology, as per the\nACR knowledge domain. Regarding clinical care paths, ChatGPT-4 performs well in\ndiagnosis, prognosis, and toxicity but lacks proficiency in topics related to\nbrachytherapy and dosimetry, as well as in-depth questions from clinical\ntrials. For the gray zone cases, ChatGPT-4 is able to suggest a personalized\ntreatment approach to each case and achieves comparable votes (28.76% on\naverage) to human experts in general. Most importantly, it provides\ncomplementary suggestion to the recommendation from a single expert. Both\nevaluations have demonstrated the potential of ChatGPT in medical education for\nthe general public and cancer patients, as well as the potential to aid\nclinical decision-making, while acknowledging its limitations in certain\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Huang_Y/0/1/0/all/0/1\">Yixing Huang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gomaa_A/0/1/0/all/0/1\">Ahmed Gomaa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Weissmann_T/0/1/0/all/0/1\">Thomas Weissmann</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Grigo_J/0/1/0/all/0/1\">Johanna Grigo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tkhayat_H/0/1/0/all/0/1\">Hassen Ben Tkhayat</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Frey_B/0/1/0/all/0/1\">Benjamin Frey</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gaipl_U/0/1/0/all/0/1\">Udo S. Gaipl</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Distel_L/0/1/0/all/0/1\">Luitpold V. Distel</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bert_C/0/1/0/all/0/1\">Christoph Bert</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fietkau_R/0/1/0/all/0/1\">Rainer Fietkau</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Putz_F/0/1/0/all/0/1\">Florian Putz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus & Walkden (2023). (arXiv:2305.00217v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.00217","description":"<p>In a recent paper published in the Journal of Language Evolution, Kauhanen,\nEinhaus &amp; Walkden (https://doi.org/10.1093/jole/lzad005, KEW) challenge the\nresults presented in one of my papers (Koplenig, Royal Society Open Science, 6,\n181274 (2019), https://doi.org/10.1098/rsos.181274), in which I tried to show\nthrough a series of statistical analyses that large numbers of L2 (second\nlanguage) speakers do not seem to affect the (grammatical or statistical)\ncomplexity of a language. To this end, I focus on the way in which the\nEthnologue assesses language status: a language is characterised as vehicular\nif, in addition to being used by L1 (first language) speakers, it should also\nhave a significant number of L2 users. KEW criticise both the use of\nvehicularity as a (binary) indicator of whether a language has a significant\nnumber of L2 users and the idea of imputing a zero proportion of L2 speakers to\nnon-vehicular languages whenever a direct estimate of that proportion is\nunavailable. While I recognise the importance of post-publication commentary on\npublished research, I show in this rejoinder that both points of criticism are\nexplicitly mentioned and analysed in my paper. In addition, I also comment on\nother points raised by KEW and demonstrate that both alternative analyses\noffered by KEW do not stand up to closer scrutiny.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koplenig_A/0/1/0/all/0/1\">Alexander Koplenig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01219","description":"<p>The prompt-based learning paradigm, which bridges the gap between\npre-training and fine-tuning, achieves state-of-the-art performance on several\nNLP tasks, particularly in few-shot settings. Despite being widely applied,\nprompt-based learning is vulnerable to backdoor attacks. Textual backdoor\nattacks are designed to introduce targeted vulnerabilities into models by\npoisoning a subset of training samples through trigger injection and label\nmodification. However, they suffer from flaws such as abnormal natural language\nexpressions resulting from the trigger and incorrect labeling of poisoned\nsamples. In this study, we propose ProAttack, a novel and efficient method for\nperforming clean-label backdoor attacks based on the prompt, which uses the\nprompt itself as a trigger. Our method does not require external triggers and\nensures correct labeling of poisoned samples, improving the stealthy nature of\nthe backdoor attack. With extensive experiments on rich-resource and few-shot\ntext classification tasks, we empirically validate ProAttack's competitive\nperformance in textual backdoor attacks. Notably, in the rich-resource setting,\nProAttack achieves state-of-the-art attack success rates in the clean-label\nbackdoor attack benchmark without external triggers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jinming Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1\">Luu Anh Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information. (arXiv:2305.01528v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01528","description":"<p>Dungeons &amp; Dragons (D&amp;D) is a tabletop roleplaying game with complex natural\nlanguage interactions between players and hidden state information. Recent work\nhas shown that large language models (LLMs) that have access to state\ninformation can generate higher quality game turns than LLMs that use dialog\nhistory alone. However, previous work used game state information that was\nheuristically created and was not a true gold standard game state. We present\nFIREBALL, a large dataset containing nearly 25,000 unique sessions from real\nD&amp;D gameplay on Discord with true game state info. We recorded game play\nsessions of players who used the Avrae bot, which was developed to aid people\nin playing D&amp;D online, capturing language, game commands and underlying game\nstate information. We demonstrate that FIREBALL can improve natural language\ngeneration (NLG) by using Avrae state information, improving both automated\nmetrics and human judgments of quality. Additionally, we show that LLMs can\ngenerate executable Avrae commands, particularly after finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1\">Andrew Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1\">Karmanya Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1\">Alexander Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Lara J. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects. (arXiv:2305.02750v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02750","description":"<p>Proactive dialogue systems, related to a wide range of real-world\nconversational applications, equip the conversational agent with the capability\nof leading the conversation direction towards achieving pre-defined targets or\nfulfilling certain goals from the system side. It is empowered by advanced\ntechniques to progress to more complicated tasks that require strategical and\nmotivational interactions. In this survey, we provide a comprehensive overview\nof the prominent problems and advanced designs for conversational agent's\nproactivity in different types of dialogues. Furthermore, we discuss challenges\nthat meet the real-world application needs but require a greater research focus\nin the future. We hope that this first survey of proactive dialogue systems can\nprovide the community with a quick access and an overall picture to this\npractical problem, and stimulate more progresses on conversational AI to the\nnext level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research. (arXiv:2305.02797v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02797","description":"<p>Recent advances in deep learning methods for natural language processing\n(NLP) have created new business opportunities and made NLP research critical\nfor industry development. As one of the big players in the field of NLP,\ntogether with governments and universities, it is important to track the\ninfluence of industry on research. In this study, we seek to quantify and\ncharacterize industry presence in the NLP community over time. Using a corpus\nwith comprehensive metadata of 78,187 NLP publications and 701 resumes of NLP\npublication authors, we explore the industry presence in the field since the\nearly 90s. We find that industry presence among NLP authors has been steady\nbefore a steep increase over the past five years (180% growth from 2017 to\n2022). A few companies account for most of the publications and provide funding\nto academic researchers through grants and internships. Our study shows that\nthe presence and impact of the industry on natural language processing research\nare significant and fast-growing. This work calls for increased transparency of\nindustry influence in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdalla_M/0/1/0/all/0/1\">Mohamed Abdalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neveol_A/0/1/0/all/0/1\">Aur&#xe9;lie N&#xe9;v&#xe9;ol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ducel_F/0/1/0/all/0/1\">Fanny Ducel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_K/0/1/0/all/0/1\">Kar&#xeb;n Fort</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering. (arXiv:2305.03453v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03453","description":"<p>Large Language Models (LLMs) have recently demonstrated exceptional\nperformance in various Natural Language Processing (NLP) tasks. They have also\nshown the ability to perform chain-of-thought (CoT) reasoning to solve complex\nproblems. Recent studies have explored CoT reasoning in complex multimodal\nscenarios, such as the science question answering task, by fine-tuning\nmultimodal models with high-quality human-annotated CoT rationales. However,\ncollecting high-quality COT rationales is usually time-consuming and costly.\nBesides, the annotated rationales are hardly accurate due to the redundant\ninformation involved or the essential information missed. To address these\nissues, we propose a novel method termed \\emph{T-SciQ} that aims at teaching\nscience question answering with LLM signals. The T-SciQ approach generates\nhigh-quality CoT rationales as teaching signals and is advanced to train much\nsmaller models to perform CoT reasoning in complex modalities. Additionally, we\nintroduce a novel data mixing strategy to produce more effective teaching data\nsamples for simple and complex science question answer problems. Extensive\nexperimental results show that our T-SciQ method achieves a new\nstate-of-the-art performance on the ScienceQA benchmark, with an accuracy of\n96.18%. Moreover, our approach outperforms the most powerful fine-tuned\nbaseline by 4.5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiabang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03509","description":"<p>Diffusion-based generative models' impressive ability to create convincing\nimages has captured global attention. However, their complex internal\nstructures and operations often make them difficult for non-experts to\nunderstand. We present Diffusion Explainer, the first interactive visualization\ntool that explains how Stable Diffusion transforms text prompts into images.\nDiffusion Explainer tightly integrates a visual overview of Stable Diffusion's\ncomplex components with detailed explanations of their underlying operations,\nenabling users to fluidly transition between multiple levels of abstraction\nthrough animations and interactive elements. By comparing the evolutions of\nimage representations guided by two related text prompts over refinement\ntimesteps, users can discover the impact of prompts on image generation.\nDiffusion Explainer runs locally in users' web browsers without the need for\ninstallation or specialized hardware, broadening the public's education access\nto modern AI techniques. Our open-sourced tool is available at:\nhttps://poloclub.github.io/diffusion-explainer/. A video demo is available at\nhttps://youtu.be/Zg4gxdIWDds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seongmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1\">Benjamin Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijie J. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">ShengYun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_A/0/1/0/all/0/1\">Austin Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kevin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Haekyu Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoyang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition. (arXiv:2305.03688v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03688","description":"<p>The MultiCoNER \\RNum{2} shared task aims to tackle multilingual named entity\nrecognition (NER) in fine-grained and noisy scenarios, and it inherits the\nsemantic ambiguity and low-context setting of the MultiCoNER \\RNum{1} task. To\ncope with these problems, the previous top systems in the MultiCoNER \\RNum{1}\neither incorporate the knowledge bases or gazetteers. However, they still\nsuffer from insufficient knowledge, limited context length, single retrieval\nstrategy. In this paper, our team \\textbf{DAMO-NLP} proposes a unified\nretrieval-augmented system (U-RaNER) for fine-grained multilingual NER. We\nperform error analysis on the previous top systems and reveal that their\nperformance bottleneck lies in insufficient knowledge. Also, we discover that\nthe limited context length causes the retrieval knowledge to be invisible to\nthe model. To enhance the retrieval context, we incorporate the entity-centric\nWikidata knowledge base, while utilizing the infusion approach to broaden the\ncontextual scope of the model. Also, we explore various search strategies and\nrefine the quality of retrieval knowledge. Our system\\footnote{We will release\nthe dataset, code, and scripts of our system at {\\small\n\\url{https://github.com/modelscope/AdaSeq/tree/master/examples/U-RaNER}}.} wins\n9 out of 13 tracks in the MultiCoNER \\RNum{2} shared task. Additionally, we\ncompared our system with ChatGPT, one of the large language models which have\nunlocked strong capabilities on many tasks. The results show that there is\nstill much room for improvement for ChatGPT on the extraction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zeqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zixia Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in Vietnamese. (arXiv:2305.04166v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.04166","description":"<p>Image Captioning is one of the vision-language tasks that still interest the\nresearch community worldwide in the 2020s. MS-COCO Caption benchmark is\ncommonly used to evaluate the performance of advanced captioning models,\nalthough it was published in 2015. Recent captioning models trained on the\nMS-COCO Caption dataset only have good performance in language patterns of\nEnglish; they do not have such good performance in contexts captured in Vietnam\nor fluently caption images using Vietnamese. To contribute to the low-resources\nresearch community as in Vietnam, we introduce a novel image captioning dataset\nin Vietnamese, the Open-domain Vietnamese Image Captioning dataset\n(UIT-OpenViIC). The introduced dataset includes complex scenes captured in\nVietnam and manually annotated by Vietnamese under strict rules and\nsupervision. In this paper, we present in more detail the dataset creation\nprocess. From preliminary analysis, we show that our dataset is challenging to\nrecent state-of-the-art (SOTA) Transformer-based baselines, which performed\nwell on the MS COCO dataset. Then, the modest results prove that UIT-OpenViIC\nhas room to grow, which can be one of the standard benchmarks in Vietnamese for\nthe research community to evaluate their captioning models. Furthermore, we\npresent a CAMO approach that effectively enhances the image representation\nability by a multi-level encoder output fusion mechanism, which helps improve\nthe quality of generated captions compared to previous captioning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bui_D/0/1/0/all/0/1\">Doanh C. Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nghia Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khang Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment. (arXiv:2305.04476v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2305.04476","description":"<p>The speech-to-singing (STS) voice conversion task aims to generate singing\nsamples corresponding to speech recordings while facing a major challenge: the\nalignment between the target (singing) pitch contour and the source (speech)\ncontent is difficult to learn in a text-free situation. This paper proposes\nAlignSTS, an STS model based on explicit cross-modal alignment, which views\nspeech variance such as pitch and content as different modalities. Inspired by\nthe mechanism of how humans will sing the lyrics to the melody, AlignSTS: 1)\nadopts a novel rhythm adaptor to predict the target rhythm representation to\nbridge the modality gap between content and pitch, where the rhythm\nrepresentation is computed in a simple yet effective way and is quantized into\na discrete space; and 2) uses the predicted rhythm representation to re-align\nthe content based on cross-attention and conducts a cross-modal fusion for\nre-synthesize. Extensive experiments show that AlignSTS achieves superior\nperformance in terms of both objective and subjective metrics. Audio samples\nare available at https://alignsts.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1\">Ruiqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_R/0/1/0/all/0/1\">Rongjie Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lichao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PreCog: Exploring the Relation between Memorization and Performance in Pre-trained Language Models. (arXiv:2305.04673v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04673","description":"<p>Pre-trained Language Models such as BERT are impressive machines with the\nability to memorize, possibly generalized learning examples. We present here a\nsmall, focused contribution to the analysis of the interplay between\nmemorization and performance of BERT in downstream tasks. We propose PreCog, a\nmeasure for evaluating memorization from pre-training, and we analyze its\ncorrelation with the BERT's performance. Our experiments show that highly\nmemorized examples are better classified, suggesting memorization is an\nessential key to success for BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranaldi_L/0/1/0/all/0/1\">Leonardo Ranaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruzzetti_E/0/1/0/all/0/1\">Elena Sofia Ruzzetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanzotto_F/0/1/0/all/0/1\">Fabio Massimo Zanzotto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiModal-GPT: A Vision and Language Model for Dialogue with Humans. (arXiv:2305.04790v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.04790","description":"<p>We present a vision and language model named MultiModal-GPT to conduct\nmulti-round dialogue with humans. MultiModal-GPT can follow various\ninstructions from humans, such as generating a detailed caption, counting the\nnumber of interested objects, and answering general questions from users.\nMultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with\nLow-rank Adapter (LoRA) added both in the cross-attention part and the\nself-attention part of the language model. We first construct instruction\ntemplates with vision and language data for multi-modality instruction tuning\nto make the model understand and follow human instructions. We find the quality\nof training data is vital for the dialogue performance, where few data\ncontaining short answers can lead the model to respond shortly to any\ninstructions. To further enhance the ability to chat with humans of the\nMultiModal-GPT, we utilize language-only instruction-following data to train\nthe MultiModal-GPT jointly. The joint training of language-only and\nvisual-language instructions with the \\emph{same} instruction template\neffectively improves dialogue performance. Various demos show the ability of\ncontinuous dialogue of MultiModal-GPT with humans. Code, dataset, and demo are\nat https://github.com/open-mmlab/Multimodal-GPT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1\">Tao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chengqi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Miao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kuikun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}