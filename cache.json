{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-10T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"CRL+: A Novel Semi-Supervised Deep Active Contrastive Representation Learning-Based Text Classification Model for Insurance Data. (arXiv:2302.04343v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04343","description":"<p>Financial sector and especially the insurance industry collect vast volumes\nof text on a daily basis and through multiple channels (their agents, customer\ncare centers, emails, social networks, and web in general). The information\ncollected includes policies, expert and health reports, claims and complaints,\nresults of surveys, and relevant social media posts. It is difficult to\neffectively extract label, classify, and interpret the essential information\nfrom such varied and unstructured material. Therefore, the Insurance Industry\nis among the ones that can benefit from applying technologies for the\nintelligent analysis of free text through Natural Language Processing (NLP).\n</p>\n<p>In this paper, CRL+, a novel text classification model combining Contrastive\nRepresentation Learning (CRL) and Active Learning is proposed to handle the\nchallenge of using semi-supervised learning for text classification. In this\nmethod, supervised (CRL) is used to train a RoBERTa transformer model to encode\nthe textual data into a contrastive representation space and then classify\nusing a classification layer. This (CRL)-based transformer model is used as the\nbase model in the proposed Active Learning mechanism to classify all the data\nin an iterative manner. The proposed model is evaluated using unstructured\nobituary data with objective to determine the cause of the death from the data.\nThis model is compared with the CRL model and an Active Learning model with the\nRoBERTa base model. The experiment shows that the proposed method can\noutperform both methods for this specific task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahromi_A/0/1/0/all/0/1\">Amir Namavar Jahromi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pourjafari_E/0/1/0/all/0/1\">Ebrahim Pourjafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimipour_H/0/1/0/all/0/1\">Hadis Karimipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satpathy_A/0/1/0/all/0/1\">Amit Satpathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodge_L/0/1/0/all/0/1\">Lovell Hodge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment analysis and opinion mining on educational data: A survey. (arXiv:2302.04359v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04359","description":"<p>Sentiment analysis AKA opinion mining is one of the most widely used NLP\napplications to identify human intentions from their reviews. In the education\nsector, opinion mining is used to listen to student opinions and enhance their\nlearning-teaching practices pedagogically. With advancements in sentiment\nannotation techniques and AI methodologies, student comments can be labelled\nwith their sentiment orientation without much human intervention. In this\nreview article, (1) we consider the role of emotional analysis in education\nfrom four levels: document level, sentence level, entity level, and aspect\nlevel, (2) sentiment annotation techniques including lexicon-based and\ncorpus-based approaches for unsupervised annotations are explored, (3) the role\nof AI in sentiment analysis with methodologies like machine learning, deep\nlearning, and transformers are discussed, (4) the impact of sentiment analysis\non educational procedures to enhance pedagogy, decision-making, and evaluation\nare presented. Educational institutions have been widely invested to build\nsentiment analysis tools and process their student feedback to draw their\nopinions and insights. Applications built on sentiment analysis of student\nfeedback are reviewed in this study. Challenges in sentiment analysis like\nmulti-polarity, polysemous, negation words, and opinion spam detection are\nexplored and their trends in the research space are discussed. The future\ndirections of sentiment analysis in education are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaik_T/0/1/0/all/0/1\">Thanveer Shaik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xiaohui Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dann_C/0/1/0/all/0/1\">Christopher Dann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galligan_L/0/1/0/all/0/1\">Linda Galligan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v1 [cs.LG])","link":"http://arxiv.org/abs/2302.04391","description":"<p>In industry deep learning application, our manually labeled data has a\ncertain number of noisy data. To solve this problem and achieve more than 90\nscore in dev dataset, we present a simple method to find the noisy data and\nre-label the noisy data by human, given the model predictions as references in\nhuman labeling. In this paper, we illustrate our idea for a broad set of deep\nlearning tasks, includes classification, sequence tagging, object detection,\nsequence generation, click-through rate prediction. The experimental results\nand human evaluation results verify our idea.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge Memorization. (arXiv:2302.04415v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04415","description":"<p>Pre-trained language models (PLM) have achieved remarkable advancement in\ntable-to-text generation tasks. However, the lack of labeled domain-specific\nknowledge and the topology gap between tabular data and text make it difficult\nfor PLMs to yield faithful text. Low-resource generation likewise faces unique\nchallenges in this domain. Inspired by how humans descript tabular data with\nprior knowledge, we suggest a new framework: PromptMize, which targets\ntable-to-text generation under few-shot settings. The design of our framework\nconsists of two aspects: a prompt planner and a knowledge adapter. The prompt\nplanner aims to generate a prompt signal that provides instance guidance for\nPLMs to bridge the topology gap between tabular data and text. Moreover, the\nknowledge adapter memorizes domain-specific knowledge from the unlabelled\ncorpus to supply essential information during generation. Extensive experiments\nand analyses are investigated on three open domain few-shot NLG datasets:\nhuman, song, and book. Compared with previous state-of-the-art approaches, our\nmodel achieves remarkable performance in generating quality as judged by human\nand automatic evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhixin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Minyxuan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiexing Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Ziwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guanjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-based Response Evaluator for Open-Domain Spoken Conversation. (arXiv:2302.04424v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04424","description":"<p>Many open-domain dialogue systems rely on multiple response generators, any\nof which can contribute a response to the dialogue in a particular context.\nThus the ability to compare potential responses and then select the best plays\nan important role in ensuring a dialogue system is coherent and engaging.\nDialogue coherence goes beyond simply remaining on topic -- some trivia may be\non topic and engaging when mentioned out of the blue, but may not be coherent\nand grounded in the context of the conversation. We carry out experiments on\nresponse selection in the Athena system, an Alexa Prize SocialBot that has\ndedicated content and multiple topic-specific response generators for a large\nnumber of topics. First, we collect a corpus of Athena conversations with live\nhuman traffic, where potential responses from all enabled response generators\nare logged and subsequently annotated for response quality. We compare several\noff-the-shelf response ranking methods for open-domain dialogue to\nAthena-Heuristic, a heuristic response ranker that was field-tested in Athena\nduring the third Alexa Prize competition. We also compare these to a\ntransformer-based response ranker we call Athena-RR, that we train on our\nAthena conversations. Athena-RR uses both the conversational context and the\ndialogue state to rank the potential responses. We find that Athena-RR with a\nRecall@1 of 70.79\\% outperforms Athena-Heuristic and all of the off-the-shelf\nrankers by a large margin. We then conduct a live A/B study comparing\nAthena-Heuristic to Athena-RR in a 6,358 conversations with Alexa users. We\nshow that Athena-RR leads to significantly longer conversations that receive\nsignificantly higher user ratings than the heuristic rule-based ranker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harrison_V/0/1/0/all/0/1\">Vrindavan Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekaran_R/0/1/0/all/0/1\">Rishi Rajasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_M/0/1/0/all/0/1\">Marilyn Walker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Visual Feedback to Guide Benchmark Creation: A Human-and-Metric-in-the-Loop Workflow. (arXiv:2302.04434v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04434","description":"<p>Recent research has shown that language models exploit `artifacts' in\nbenchmarks to solve tasks, rather than truly learning them, leading to inflated\nmodel performance. In pursuit of creating better benchmarks, we propose VAIDA,\na novel benchmark creation paradigm for NLP, that focuses on guiding\ncrowdworkers, an under-explored facet of addressing benchmark idiosyncrasies.\nVAIDA facilitates sample correction by providing realtime visual feedback and\nrecommendations to improve sample quality. Our approach is domain, model, task,\nand metric agnostic, and constitutes a paradigm shift for robust, validated,\nand dynamic benchmark creation via human-and-metric-in-the-loop workflows. We\nevaluate via expert review and a user study with NASA TLX. We find that VAIDA\ndecreases effort, frustration, mental, and temporal demands of crowdworkers and\nanalysts, simultaneously increasing the performance of both user groups with a\n45.8% decrease in the level of artifacts in created samples. As a by product of\nour user study, we observe that created samples are adversarial across models,\nleading to decreases of 31.3% (BERT), 22.5% (RoBERTa), 14.98% (GPT-3 fewshot)\nin performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arunkumar_A/0/1/0/all/0/1\">Anjana Arunkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_B/0/1/0/all/0/1\">Bhavdeep Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryan_C/0/1/0/all/0/1\">Chris Bryan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing E-Commerce Recommendation using Pre-Trained Language Model and Fine-Tuning. (arXiv:2302.04443v1 [cs.LG])","link":"http://arxiv.org/abs/2302.04443","description":"<p>Pretrained Language Models (PLM) have been greatly successful on a board\nrange of natural language processing (NLP) tasks. However, it has just started\nbeing applied to the domain of recommendation systems. Traditional\nrecommendation algorithms failed to incorporate the rich textual information in\ne-commerce datasets, which hinderss the performance of those models. We present\na thorough investigation on the effect of various strategy of incorporating\nPLMs into traditional recommender algorithms on one of the e-commerce datasets,\nand we compare the results with vanilla recommender baseline models. We show\nthat the application of PLMs and domain specific fine-tuning lead to an\nincrease on the predictive capability of combined models. These results\naccentuate the importance of utilizing textual information in the context of\ne-commerce, and provides insight on how to better apply PLMs alongside\ntraditional recommender system algorithms. The code used in this paper is\navailable on Github: https://github.com/NuofanXu/bert_retail_recommender.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Nuofan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chenhui Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v1 [cs.LG])","link":"http://arxiv.org/abs/2302.04449","description":"<p>High sample complexity has long been a challenge for RL. On the other hand,\nhumans learn to perform tasks not only from interaction or demonstrations, but\nalso by reading unstructured text documents, e.g., instruction manuals.\nInstruction manuals and wiki pages are among the most abundant data that could\ninform agents of valuable features and policies or task-specific environmental\ndynamics and reward structures. Therefore, we hypothesize that the ability to\nutilize human-written instruction manuals to assist learning policies for\nspecific tasks should lead to a more efficient and better-performing agent.\n</p>\n<p>We propose the Read and Reward framework. Read and Reward speeds up RL\nalgorithms on Atari games by reading manuals released by the Atari game\ndevelopers. Our framework consists of a QA Extraction module that extracts and\nsummarizes relevant information from the manual and a Reasoning module that\nevaluates object-agent interactions based on information from the manual.\nAuxiliary reward is then provided to a standard A2C RL agent, when interaction\nis detected. When assisted by our design, A2C improves on 4 games in the Atari\nenvironment with sparse rewards, and requires 1000x less training frames\ncompared to the previous SOTA Agent 57 on Skiing, the hardest game in Atari.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yewen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azaria_A/0/1/0/all/0/1\">Amos Azaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1\">Tom M. Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models. (arXiv:2302.04456v1 [cs.SD])","link":"http://arxiv.org/abs/2302.04456","description":"<p>In recent years, there has been an increased popularity in image and speech\ngeneration using diffusion models. However, directly generating music waveforms\nfrom free-form text prompts is still under-explored. In this paper, we propose\nthe first text-to-waveform music generation model that can receive arbitrary\ntexts using diffusion models. We incorporate the free-form textual prompt as\nthe condition to guide the waveform generation process of diffusion models. To\nsolve the problem of lacking such text-music parallel data, we collect a\ndataset of text-music pairs from the Internet with weak supervision. Besides,\nwe compare the effect of two prompt formats of conditioning texts (music tags\nand free-form texts) and prove the superior performance of our method in terms\nof text-music relevance. We further demonstrate that our generated music in the\nwaveform domain outperforms previous works by a large margin in terms of\ndiversity, quality, and text-music relevance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengfei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_C/0/1/0/all/0/1\">Chao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yekun Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Constraints with Prompting for Zero-Shot Event Argument Classification. (arXiv:2302.04459v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04459","description":"<p>Determining the role of event arguments is a crucial subtask of event\nextraction. Most previous supervised models leverage costly annotations, which\nis not practical for open-domain applications. In this work, we propose to use\nglobal constraints with prompting to effectively tackles event argument\nclassification without any annotation and task-specific training. Specifically,\ngiven an event and its associated passage, the model first creates several new\npassages by prefix prompts and cloze prompts, where prefix prompts indicate\nevent type and trigger span, and cloze prompts connect each candidate role with\nthe target argument span. Then, a pre-trained language model scores the new\npassages, making the initial prediction. Our novel prompt templates can easily\nadapt to all events and argument types without manual effort. Next, the model\nregularizes the prediction by global constraints exploiting cross-task,\ncross-argument, and cross-event relations. Extensive experiments demonstrate\nour model's effectiveness: it outperforms the best zero-shot baselines by 12.5%\nand 10.9% F1 on ACE and ERE with given argument spans and by 4.3% and 3.3% F1,\nrespectively, without given argument spans. We have made our code publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zizheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bag of Tricks for Training Data Extraction from Language Models. (arXiv:2302.04460v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04460","description":"<p>With the advance of language models, privacy protection is receiving more\nattention. Training data extraction is therefore of great importance, as it can\nserve as a potential tool to assess privacy leakage. However, due to the\ndifficulty of this task, most of the existing methods are proof-of-concept and\nstill not effective enough. In this paper, we investigate and benchmark tricks\nfor improving training data extraction using a publicly available dataset.\nBecause most existing extraction methods use a pipeline of\ngenerating-then-ranking, i.e., generating text candidates as potential training\ndata and then ranking them based on specific criteria, our research focuses on\nthe tricks for both text generation (e.g., sampling strategy) and text ranking\n(e.g., token-level criteria). The experimental results show that several\npreviously overlooked tricks can be crucial to the success of training data\nextraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricks\noutperform the baseline by a large margin in most cases, providing a much\nstronger baseline for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weichen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1\">Chao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Bingyi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Min Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Contextomized Quotes in News Headlines by Contrastive Learning. (arXiv:2302.04465v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04465","description":"<p>Quotes are critical for establishing credibility in news articles. A direct\nquote enclosed in quotation marks has a strong visual appeal and is a sign of a\nreliable citation. Unfortunately, this journalistic practice is not strictly\nfollowed, and a quote in the headline is often \"contextomized.\" Such a quote\nuses words out of context in a way that alters the speaker's intention so that\nthere is no semantically matching quote in the body text. We present QuoteCSE,\na contrastive learning framework that represents the embedding of news quotes\nbased on domain-driven positive and negative samples to identify such an\neditorial strategy. The dataset and code are available at\nhttps://github.com/ssu-humane/contextomized-quote-contrastive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Seonyeong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hyeonho Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kunwoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiyoung Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Meeyoung Cha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Large-Scale Analysis of Persian Tweets Regarding Covid-19 Vaccination. (arXiv:2302.04511v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04511","description":"<p>The Covid-19 pandemic had an enormous effect on our lives, especially on\npeople's interactions. By introducing Covid-19 vaccines, both positive and\nnegative opinions were raised over the subject of taking vaccines or not. In\nthis paper, using data gathered from Twitter, including tweets and user\nprofiles, we offer a comprehensive analysis of public opinion in Iran about the\nCoronavirus vaccines. For this purpose, we applied a search query technique\ncombined with a topic modeling approach to extract vaccine-related tweets. We\nutilized transformer-based models to classify the content of the tweets and\nextract themes revolving around vaccination. We also conducted an emotion\nanalysis to evaluate the public happiness and anger around this topic. Our\nresults demonstrate that Covid-19 vaccination has attracted considerable\nattention from different angles, such as governmental issues, safety or\nhesitancy, and side effects. Moreover, Coronavirus-relevant phenomena like\npublic vaccination and the rate of infection deeply impacted public emotional\nstatus and users' interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+ShabaniMirzaei_T/0/1/0/all/0/1\">Taha ShabaniMirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chamani_H/0/1/0/all/0/1\">Houmaan Chamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadeh_Z/0/1/0/all/0/1\">Zhivar Sourati Hassan Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahrak_B/0/1/0/all/0/1\">Behnam Bahrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Attention via Control Variates. (arXiv:2302.04542v1 [cs.LG])","link":"http://arxiv.org/abs/2302.04542","description":"<p>Random-feature-based attention (RFA) is an efficient approximation of softmax\nattention with linear runtime and space complexity. However, the approximation\ngap between RFA and conventional softmax attention is not well studied. Built\nupon previous progress of RFA, we characterize this gap through the lens of\ncontrol variates and show that RFA can be decomposed into a sum of multiple\ncontrol variate estimators for each element in the sequence. This new framework\nreveals that exact softmax attention can be recovered from RFA by manipulating\neach control variate. Besides, it allows us to develop a more flexible form of\ncontrol variates, resulting in a novel attention mechanism that significantly\nreduces the approximation gap while maintaining linear complexity. Extensive\nexperiments demonstrate that our model outperforms state-of-the-art efficient\nattention mechanisms on both vision and language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Robust Character Detection in Fantasy Novels. (arXiv:2302.04555v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04555","description":"<p>Named Entity Recognition (NER) is a low-level task often used as a foundation\nfor solving higher level NLP problems. In the context of character detection in\nnovels, NER false negatives can be an issue as they possibly imply missing\ncertain characters or relationships completely. In this article, we demonstrate\nthat applying a straightforward data augmentation technique allows training a\nmodel achieving higher recall, at the cost of a certain amount of precision\nregarding ambiguous entities. We show that this decrease in precision can be\nmitigated by giving the model more local context, which resolves some of the\nambiguities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amalvy_A/0/1/0/all/0/1\">Arthur Amalvy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufour_R/0/1/0/all/0/1\">Richard Dufour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP-based Decision Support System for Examination of Eligibility Criteria from Securities Prospectuses at the German Central Bank. (arXiv:2302.04562v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04562","description":"<p>As part of its digitization initiative, the German Central Bank (Deutsche\nBundesbank) wants to examine the extent to which natural Language Processing\n(NLP) can be used to make independent decisions upon the eligibility criteria\nof securities prospectuses. Every month, the Directorate General Markets at the\nGerman Central Bank receives hundreds of scanned prospectuses in PDF format,\nwhich must be manually processed to decide upon their eligibility. We found\nthat this tedious and time-consuming process can be (semi-)automated by\nemploying modern NLP model architectures, which learn the linguistic feature\nrepresentation in text to identify the present eligible and ineligible\ncriteria. The proposed Decision Support System provides decisions of\ndocument-level eligibility criteria accompanied by human-understandable\nexplanations of the decisions. The aim of this project is to model the\ndescribed use case and to evaluate the extent to which current research results\nfrom the field of NLP can be applied to this problem. After creating a\nheterogeneous domain-specific dataset containing annotations of eligible and\nnon-eligible mentions of relevant criteria, we were able to successfully build,\ntrain and deploy a semi-automatic decider model. This model is based on\ntransformer-based language models and decision trees, which integrate the\nestablished rule-based parts of the decision processes. Results suggest that it\nis possible to efficiently model the problem and automate decision making to\nmore than 90% for many of the considered eligibility criteria.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanig_C/0/1/0/all/0/1\">Christian H&#xe4;nig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlosser_M/0/1/0/all/0/1\">Markus Schl&#xf6;sser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamotskyi_S/0/1/0/all/0/1\">Serhii Hamotskyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zambaku_G/0/1/0/all/0/1\">Gent Zambaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blankenburg_J/0/1/0/all/0/1\">Janek Blankenburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating a Structured Summary of Numerous Academic Papers: Dataset and Method. (arXiv:2302.04580v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04580","description":"<p>Writing a survey paper on one research topic usually needs to cover the\nsalient content from numerous related papers, which can be modeled as a\nmulti-document summarization (MDS) task. Existing MDS datasets usually focus on\nproducing the structureless summary covering a few input documents. Meanwhile,\nprevious structured summary generation works focus on summarizing a single\ndocument into a multi-section summary. These existing datasets and methods\ncannot meet the requirements of summarizing numerous academic papers into a\nstructured summary. To deal with the scarcity of available data, we propose\nBigSurvey, the first large-scale dataset for generating comprehensive summaries\nof numerous academic papers on each topic. We collect target summaries from\nmore than seven thousand survey papers and utilize their 430 thousand reference\npapers' abstracts as input documents. To organize the diverse content from\ndozens of input documents and ensure the efficiency of processing long text\nsequences, we propose a summarization method named category-based alignment and\nsparse transformer (CAST). The experimental results show that our CAST method\noutperforms various advanced summarization methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaiqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiannong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhiyuan Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study. (arXiv:2302.04618v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04618","description":"<p>A deployed question answering (QA) model can easily fail when the test data\nhas a distribution shift compared to the training data. Robustness tuning (RT)\nmethods have been widely studied to enhance model robustness against\ndistribution shifts before model deployment. However, can we improve a model\nafter deployment? To answer this question, we evaluate test-time adaptation\n(TTA) to improve a model after deployment. We first introduce COLDQA, a unified\nevaluation benchmark for robust QA against text corruption and changes in\nlanguage and domain. We then evaluate previous TTA methods on COLDQA and\ncompare them to RT methods. We also propose a novel TTA method called online\nimitation learning (OIL). Through extensive experiments, we find that TTA is\ncomparable to RT methods, and applying TTA after RT can significantly boost the\nperformance on COLDQA. Our proposed OIL improves TTA to be more robust to\nvariation in hyper-parameters and test distributions over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuyang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Approach for Auto-Formulation of Optimization Problems. (arXiv:2302.04643v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04643","description":"<p>In the Natural Language for Optimization (NL4Opt) NeurIPS 2022 competition,\ncompetitors focus on improving the accessibility and usability of optimization\nsolvers, with the aim of subtask 1: recognizing the semantic entities that\ncorrespond to the components of the optimization problem; subtask 2: generating\nformulations for the optimization problem. In this paper, we present the\nsolution of our team. First, we treat subtask 1 as a named entity recognition\n(NER) problem with the solution pipeline including pre-processing methods,\nadversarial training, post-processing methods and ensemble learning. Besides,\nwe treat subtask 2 as a generation problem with the solution pipeline including\nspecially designed prompts, adversarial training, post-processing methods and\nensemble learning. Our proposed methods have achieved the F1-score of 0.931 in\nsubtask 1 and the accuracy of 0.867 in subtask 2, which won the fourth and\nthird places respectively in this competition. Our code is available at\nhttps://github.com/bigdata-ustc/nl4opt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1\">Yuting Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Longhu Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1\">Shangzi Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhenya Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jinze Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models. (arXiv:2302.04662v1 [cs.PL])","link":"http://arxiv.org/abs/2302.04662","description":"<p>Large language models trained on code (LLMCs), such as Codex, hold great\npromise in enhancing programming education by automatically generating feedback\nfor students. We investigate using LLMCs to generate feedback for fixing syntax\nerrors in Python programs, a key scenario in introductory programming. More\nconcretely, given a student's buggy program, our goal is to generate feedback\ncomprising a fixed program along with a natural language explanation describing\nthe errors/fixes, inspired by how a human tutor would give feedback. While\nusing LLMCs is promising, the critical challenge is to ensure high precision in\nthe generated feedback, which is imperative before deploying such technology in\nclassrooms. The main research question we study is: Can we develop LLMCs-based\nfeedback generation techniques with a tunable precision parameter, giving\neducators quality control over the feedback that students receive? To this end,\nwe introduce PyFiXV, our technique to generate high-precision feedback powered\nby Codex. The key idea behind PyFiXV is to use a novel run-time validation\nmechanism to decide whether the generated feedback is suitable for sharing with\nthe student; notably, this validation mechanism also provides a precision knob\nto educators. We perform an extensive evaluation using two real-world datasets\nof Python programs with syntax errors and show the efficacy of PyFiXV in\ngenerating high-precision feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phung_T/0/1/0/all/0/1\">Tung Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambronero_J/0/1/0/all/0/1\">Jos&#xe9; Cambronero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1\">Sumit Gulwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohn_T/0/1/0/all/0/1\">Tobias Kohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_R/0/1/0/all/0/1\">Rupak Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1\">Adish Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_G/0/1/0/all/0/1\">Gustavo Soares</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting NLP data to counter Annotation Artifacts for NLI Tasks. (arXiv:2302.04700v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04700","description":"<p>In this paper, we explore Annotation Artifacts - the phenomena wherein large\npre-trained NLP models achieve high performance on benchmark datasets but do\nnot actually \"solve\" the underlying task and instead rely on some dataset\nartifacts (same across train, validation, and test sets) to figure out the\nright answer. We explore this phenomenon on the well-known Natural Language\nInference task by first using contrast and adversarial examples to understand\nlimitations to the model's performance and show one of the biases arising from\nannotation artifacts (the way training data was constructed by the annotators).\nWe then propose a data augmentation technique to fix this bias and measure its\neffectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhullar_A/0/1/0/all/0/1\">Armaan Singh Bhullar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Generalizability of Collaborative Dialogue Analysis with Multi-Feature Embeddings. (arXiv:2302.04716v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04716","description":"<p>Conflict prediction in communication is integral to the design of virtual\nagents that support successful teamwork by providing timely assistance. The aim\nof our research is to analyze discourse to predict collaboration success.\nUnfortunately, resource scarcity is a problem that teamwork researchers\ncommonly face since it is hard to gather a large number of training examples.\nTo alleviate this problem, this paper introduces a multi-feature embedding\n(MFeEmb) that improves the generalizability of conflict prediction models\ntrained on dialogue sequences. MFeEmb leverages textual, structural, and\nsemantic information from the dialogues by incorporating lexical, dialogue\nacts, and sentiment features. The use of dialogue acts and sentiment features\nreduces performance loss from natural distribution shifts caused mainly by\nchanges in vocabulary.\n</p>\n<p>This paper demonstrates the performance of MFeEmb on domain adaptation\nproblems in which the model is trained on discourse from one task domain and\napplied to predict team performance in a different domain. The generalizability\nof MFeEmb is quantified using the similarity measure proposed by Bontonou et\nal. (2021). Our results show that MFeEmb serves as an excellent domain-agnostic\nrepresentation for meta-pretraining a few-shot model on collaborative\nmultiparty dialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Enayet_A/0/1/0/all/0/1\">Ayesha Enayet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukthankar_G/0/1/0/all/0/1\">Gita Sukthankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Transformers for Clinical Natural Language Processing. (arXiv:2302.04725v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04725","description":"<p>Specialised pre-trained language models are becoming more frequent in NLP\nsince they can potentially outperform models trained on generic texts. BioBERT\nand BioClinicalBERT are two examples of such models that have shown promise in\nmedical NLP tasks. Many of these models are overparametrised and\nresource-intensive, but thanks to techniques like Knowledge Distillation (KD),\nit is possible to create smaller versions that perform almost as well as their\nlarger counterparts. In this work, we specifically focus on development of\ncompact language models for processing clinical texts (i.e. progress notes,\ndischarge summaries etc). We developed a number of efficient lightweight\nclinical transformers using knowledge distillation and continual learning, with\nthe number of parameters ranging from 15 million to 65 million. These models\nperformed comparably to larger models such as BioBERT and ClinicalBioBERT and\nsignificantly outperformed other compact models trained on general or\nbiomedical data. Our extensive evaluation was done across several standard\ndatasets and covered a wide range of clinical text-mining tasks, including\nNatural Language Inference, Relation Extraction, Named Entity Recognition, and\nSequence Classification. To our knowledge, this is the first comprehensive\nstudy specifically focused on creating efficient and compact transformers for\nclinical NLP tasks. The models and code used in this study can be found on our\nHuggingface profile at https://huggingface.co/nlpie and Github page at\nhttps://github.com/nlpie-research/Lightweight-Clinical-Transformers,\nrespectively, promoting reproducibility of our results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rohanian_O/0/1/0/all/0/1\">Omid Rohanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouriborji_M/0/1/0/all/0/1\">Mohammadmahdi Nouriborji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jauncey_H/0/1/0/all/0/1\">Hannah Jauncey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouchaki_S/0/1/0/all/0/1\">Samaneh Kouchaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Group_ISARIC_Clinical_Characterisation/0/1/0/all/0/1\">ISARIC Clinical Characterisation Group</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_L/0/1/0/all/0/1\">Lei Clifton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merson_L/0/1/0/all/0/1\">Laura Merson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David A. Clifton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toolformer: Language Models Can Teach Themselves to Use Tools. (arXiv:2302.04761v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04761","description":"<p>Language models (LMs) exhibit remarkable abilities to solve new tasks from\njust a few examples or textual instructions, especially at scale. They also,\nparadoxically, struggle with basic functionality, such as arithmetic or factual\nlookup, where much simpler and smaller models excel. In this paper, we show\nthat LMs can teach themselves to use external tools via simple APIs and achieve\nthe best of both worlds. We introduce Toolformer, a model trained to decide\nwhich APIs to call, when to call them, what arguments to pass, and how to best\nincorporate the results into future token prediction. This is done in a\nself-supervised way, requiring nothing more than a handful of demonstrations\nfor each API. We incorporate a range of tools, including a calculator, a Q\\&amp;A\nsystem, two different search engines, a translation system, and a calendar.\nToolformer achieves substantially improved zero-shot performance across a\nvariety of downstream tasks, often competitive with much larger models, without\nsacrificing its core language modeling abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_Yu_J/0/1/0/all/0/1\">Jane Dwivedi-Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1\">Roberto Dess&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1\">Roberta Raileanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomeli_M/0/1/0/all/0/1\">Maria Lomeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cancedda_N/0/1/0/all/0/1\">Nicola Cancedda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Massively Multilingual Language Models for Cross Lingual Fact Extraction from Low Resource Indian Languages. (arXiv:2302.04790v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04790","description":"<p>Massive knowledge graphs like Wikidata attempt to capture world knowledge\nabout multiple entities. Recent approaches concentrate on automatically\nenriching these KGs from text. However a lot of information present in the form\nof natural text in low resource languages is often missed out. Cross Lingual\nInformation Extraction aims at extracting factual information in the form of\nEnglish triples from low resource Indian Language text. Despite its massive\npotential, progress made on this task is lagging when compared to Monolingual\nInformation Extraction. In this paper, we propose the task of Cross Lingual\nFact Extraction(CLFE) from text and devise an end-to-end generative approach\nfor the same which achieves an overall F1 score of 77.46.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1\">Bhavyajeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandru_P/0/1/0/all/0/1\">Pavan Kandru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anubhav Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Large-Scale Multilingual Study of Visual Constraints on Linguistic Selection of Descriptions. (arXiv:2302.04811v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04811","description":"<p>We present a large, multilingual study into how vision constrains linguistic\nchoice, covering four languages and five linguistic properties, such as verb\ntransitivity or use of numerals. We propose a novel method that leverages\nexisting corpora of images with captions written by native speakers, and apply\nit to nine corpora, comprising 600k images and 3M captions. We study the\nrelation between visual input and linguistic choices by training classifiers to\npredict the probability of expressing a property from raw images, and find\nevidence supporting the claim that linguistic properties are constrained by\nvisual context across languages. We complement this investigation with a corpus\nstudy, taking the test case of numerals. Specifically, we use existing\nannotations (number or type of objects) to investigate the effect of different\nvisual conditions on the use of numeral expressions in captions, and show that\nsimilar patterns emerge across languages. Our methods and findings both confirm\nand extend existing research in the cognitive literature. We additionally\ndiscuss possible applications for language generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berger_U/0/1/0/all/0/1\">Uri Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frermann_L/0/1/0/all/0/1\">Lea Frermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explanation Selection Using Unlabeled Data for In-Context Learning. (arXiv:2302.04813v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04813","description":"<p>Recent work has addressed textual reasoning tasks by prompting large language\nmodels with explanations via the chain-of-thought paradigm. However, subtly\ndifferent explanations can yield widely varying downstream task accuracy, so\nexplanations that have not been \"tuned\" for a task, such as off-the-shelf\nexplanations written by nonexperts, may lead to mediocre performance. This\npaper tackles the problem of how to optimize explanation-infused prompts in a\nblack-box fashion. We first generate sets of candidate explanations for each\nexample in the prompt using a leave-one-out scheme. We then use a two-stage\nframework where we first evaluate explanations for each in-context example in\nisolation according to proxy metrics. Finally, we search over sets of\nexplanations to find a set which yields high performance against a\nsilver-labeled development set, drawing inspiration from recent work on\nbootstrapping language models on unlabeled data. Across four textual reasoning\ntasks spanning question answering, mathematical reasoning, and natural language\ninference, results show that our proxy metrics correlate with ground truth\naccuracy and our overall method can effectively improve prompts over\ncrowdworker annotations and naive search strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Cognitive Dynamics of Artificial Intelligence in the Post-COVID-19 and Learning 3.0 Era: A Case Study of ChatGPT. (arXiv:2302.04818v1 [cs.CY])","link":"http://arxiv.org/abs/2302.04818","description":"<p>The emergence of artificial intelligence has incited a paradigm shift across\nthe spectrum of human endeavors, with ChatGPT serving as a catalyst for the\ntransformation of various established domains, including but not limited to\neducation, journalism, security, and ethics. In the post-pandemic era, the\nwidespread adoption of remote work has prompted the educational sector to\nreassess conventional pedagogical methods. This paper is to scrutinize the\nunderlying psychological principles of ChatGPT, delve into the factors that\ncaptivate user attention, and implicate its ramifications on the future of\nlearning. The ultimate objective of this study is to instigate a scholarly\ndiscourse on the interplay between technological advancements in education and\nthe evolution of human learning patterns, raising the question of whether\ntechnology is driving human evolution or vice versa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luan_L/0/1/0/all/0/1\">Lingfei Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbiao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FrameBERT: Conceptual Metaphor Detection with Frame Embedding Learning. (arXiv:2302.04834v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04834","description":"<p>In this paper, we propose FrameBERT, a RoBERTa-based model that can\nexplicitly learn and incorporate FrameNet Embeddings for concept-level metaphor\ndetection. FrameBERT not only achieves better or comparable performance to the\nstate-of-the-art, but also is more explainable and interpretable compared to\nexisting models, attributing to its ability of accounting for external\nknowledge of FrameNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yucheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning. (arXiv:2302.04858v1 [cs.CV])","link":"http://arxiv.org/abs/2302.04858","description":"<p>Augmenting pretrained language models (LMs) with a vision encoder (e.g.,\nFlamingo) has obtained state-of-the-art results in image-to-text generation.\nHowever, these models store all the knowledge within their parameters, thus\noften requiring enormous model parameters to model the abundant visual concepts\nand very rich textual descriptions. Additionally, they are inefficient in\nincorporating new data, requiring a computational-expensive fine-tuning\nprocess. In this work, we introduce a Retrieval-augmented Visual Language\nModel, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant\nknowledge from the external database for zero and in-context few-shot\nimage-to-text generations. By storing certain knowledge explicitly in the\nexternal database, our approach reduces the number of model parameters and can\neasily accommodate new data during evaluation by simply updating the database.\nWe also construct an interleaved image and text data that facilitates\nin-context few-shot learning capabilities. We demonstrate that Re-ViLM\nsignificantly boosts performance for image-to-text generation tasks, especially\nfor zero-shot and few-shot generation in out-of-domain settings with 4 times\nless parameters compared with baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuolin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korthikanti_V/0/1/0/all/0/1\">Vijay Korthikanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1\">Weili Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">De-An Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1\">Shiyi Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming-Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge is a Region in Weight Space for Fine-tuned Language Models. (arXiv:2302.04863v1 [cs.LG])","link":"http://arxiv.org/abs/2302.04863","description":"<p>Research on neural networks has largely focused on understanding a single\nmodel trained on a single dataset. However, relatively little is known about\nthe relationships between different models, especially those trained or tested\non different datasets. We address this by studying how the weight space and\nunderlying loss landscape of different models are interconnected.\n</p>\n<p>Specifically, we demonstrate that fine-tuned models that were optimized for\nhigh performance, reside in well-defined regions in weight space, and vice\nversa -- that any model that resides anywhere in those regions also has high\nperformance. Specifically, we show that language models that have been\nfine-tuned on the same dataset form a tight cluster in the weight space and\nthat models fine-tuned on different datasets from the same underlying task form\na looser cluster. Moreover, traversing around the region between the models\nreaches new models that perform comparably or even better than models found via\nfine-tuning, even on tasks that the original models were not fine-tuned on.\n</p>\n<p>Our findings provide insight into the relationships between models,\ndemonstrating that a model positioned between two similar models can acquire\nthe knowledge of both. We leverage this finding and design a method to pick a\nbetter model for efficient fine-tuning. Specifically, we show that starting\nfrom the center of the region is as good or better than the pre-trained model\nin 11 of 12 datasets and improves accuracy by 3.06 on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gueta_A/0/1/0/all/0/1\">Almog Gueta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venezian_E/0/1/0/all/0/1\">Elad Venezian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning by Asking for Embodied Visual Navigation and Task Completion. (arXiv:2302.04865v1 [cs.CV])","link":"http://arxiv.org/abs/2302.04865","description":"<p>The research community has shown increasing interest in designing intelligent\nembodied agents that can assist humans in accomplishing tasks. Despite recent\nprogress on related vision-language benchmarks, most prior work has focused on\nbuilding agents that follow instructions rather than endowing agents the\nability to ask questions to actively resolve ambiguities arising naturally in\nembodied environments. To empower embodied agents with the ability to interact\nwith humans, in this work, we propose an Embodied Learning-By-Asking (ELBA)\nmodel that learns when and what questions to ask to dynamically acquire\nadditional information for completing the task. We evaluate our model on the\nTEACH vision-dialog navigation and task completion dataset. Experimental\nresults show that ELBA achieves improved task performance compared to baseline\nmodels without question-answering capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Offsite-Tuning: Transfer Learning without Full Model. (arXiv:2302.04870v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04870","description":"<p>Transfer learning is important for foundation models to adapt to downstream\ntasks. However, many foundation models are proprietary, so users must share\ntheir data with model owners to fine-tune the models, which is costly and raise\nprivacy concerns. Moreover, fine-tuning large foundation models is\ncomputation-intensive and impractical for most downstream users. In this paper,\nwe propose Offsite-Tuning, a privacy-preserving and efficient transfer learning\nframework that can adapt billion-parameter foundation models to downstream data\nwithout access to the full model. In offsite-tuning, the model owner sends a\nlight-weight adapter and a lossy compressed emulator to the data owner, who\nthen fine-tunes the adapter on the downstream data with the emulator's\nassistance. The fine-tuned adapter is then returned to the model owner, who\nplugs it into the full model to create an adapted foundation model.\nOffsite-tuning preserves both parties' privacy and is computationally more\nefficient than the existing fine-tuning methods that require access to the full\nmodel weights. We demonstrate the effectiveness of offsite-tuning on various\nlarge language and vision foundation models. Offsite-tuning can achieve\ncomparable accuracy as full model fine-tuning while being privacy-preserving\nand efficient, achieving 6.5x speedup and 5.6x memory reduction. Code is\navailable at https://github.com/mit-han-lab/offsite-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Guangxuan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Ji Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning From How Humans Correct. (arXiv:2102.00225v13 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.00225","description":"<p>In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and re-label\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we re-label the\nnoisy data in our dataset for our industry application. The experiment result\nshows that our method improve the classification accuracy from 91.7% to 92.5%.\nThe 91.7% accuracy is trained on the corrected dataset, which improve the\nbaseline from 83.3% to 91.7%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes Sentences Semantically Related: A Textual Relatedness Dataset and Empirical Study. (arXiv:2110.04845v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04845","description":"<p>The degree of semantic relatedness of two units of language has long been\nconsidered fundamental to understanding meaning. Additionally, automatically\ndetermining relatedness has many applications such as question answering and\nsummarization. However, prior NLP work has largely focused on semantic\nsimilarity, a subset of relatedness, because of a lack of relatedness datasets.\nIn this paper, we introduce a dataset for Semantic Textual Relatedness,\nSTR-2022, that has 5,500 English sentence pairs manually annotated using a\ncomparative annotation framework, resulting in fine-grained scores. We show\nthat human intuition regarding relatedness of sentence pairs is highly\nreliable, with a repeat annotation correlation of 0.84. We use the dataset to\nexplore questions on what makes sentences semantically related. We also show\nthe utility of STR-2022 for evaluating automatic methods of sentence\nrepresentation and for various downstream NLP tasks.\n</p>\n<p>Our dataset, data statement, and annotation questionnaire can be found at:\nhttps://doi.org/10.5281/zenodo.7599667\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdalla_M/0/1/0/all/0/1\">Mohamed Abdalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishnubhotla_K/0/1/0/all/0/1\">Krishnapriya Vishnubhotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Composable Sparse Fine-Tuning for Cross-Lingual Transfer. (arXiv:2110.07560v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07560","description":"<p>Fine-tuning the entire set of parameters of a large pretrained model has\nbecome the mainstream approach for transfer learning. To increase its\nefficiency and prevent catastrophic forgetting and interference, techniques\nlike adapters and sparse fine-tuning have been developed. Adapters are modular,\nas they can be combined to adapt a model towards different facets of knowledge\n(e.g., dedicated language and/or task adapters). Sparse fine-tuning is\nexpressive, as it controls the behavior of all model components. In this work,\nwe introduce a new fine-tuning method with both these desirable properties. In\nparticular, we learn sparse, real-valued masks based on a simple variant of the\nLottery Ticket Hypothesis. Task-specific masks are obtained from annotated data\nin a source language, and language-specific masks from masked language modeling\nin a target language. Both these masks can then be composed with the pretrained\nmodel. Unlike adapter-based fine-tuning, this method neither increases the\nnumber of parameters at inference time nor alters the original model\narchitecture. Most importantly, it outperforms adapters in zero-shot\ncross-lingual transfer by a large margin in a series of multilingual\nbenchmarks, including Universal Dependencies, MasakhaNER, and AmericasNLI.\nBased on an in-depth analysis, we additionally find that sparsity is crucial to\nprevent both 1) interference between the fine-tunings to be composed and 2)\noverfitting. We release the code and models at\nhttps://github.com/cambridgeltl/composable-sft.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ansell_A/0/1/0/all/0/1\">Alan Ansell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Latent-Variable Model for Intrinsic Probing. (arXiv:2201.08214v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.08214","description":"<p>The success of pre-trained contextualized representations has prompted\nresearchers to analyze them for the presence of linguistic information. Indeed,\nit is natural to assume that these pre-trained representations do encode some\nlevel of linguistic knowledge as they have brought about large empirical\nimprovements on a wide variety of NLP tasks, which suggests they are learning\ntrue linguistic generalization. In this work, we focus on intrinsic probing, an\nanalysis technique where the goal is not only to identify whether a\nrepresentation encodes a linguistic attribute but also to pinpoint where this\nattribute is encoded. We propose a novel latent-variable formulation for\nconstructing intrinsic probes and derive a tractable variational approximation\nto the log-likelihood. Our results show that our model is versatile and yields\ntighter mutual information estimates than two intrinsic probes previously\nproposed in the literature. Finally, we find empirical evidence that\npre-trained representations develop a cross-lingually entangled notion of\nmorphosyntax.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stanczak_K/0/1/0/all/0/1\">Karolina Sta&#x144;czak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1\">Lucas Torroba Hennigen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Spatial Reasoning. (arXiv:2205.00363v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00363","description":"<p>Spatial relations are a basic part of human cognition. However, they are\nexpressed in natural language in a variety of ways, and previous work has\nsuggested that current vision-and-language models (VLMs) struggle to capture\nrelational information. In this paper, we present Visual Spatial Reasoning\n(VSR), a dataset containing more than 10k natural text-image pairs with 65\ntypes of spatial relations in English (such as: under, in front of, and\nfacing). While using a seemingly simple annotation format, we show how the\ndataset includes challenging linguistic phenomena, such as varying reference\nframes. We demonstrate a large gap between human and model performance: the\nhuman ceiling is above 95%, while state-of-the-art models only achieve around\n70%. We observe that VLMs' by-relation performances have little correlation\nwith the number of training examples and the tested models are in general\nincapable of recognising relations concerning the orientations of objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emerson_G/0/1/0/all/0/1\">Guy Emerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Performance Disparities Between English-Language Accents in Automatic Speech Recognition. (arXiv:2208.01157v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.01157","description":"<p>Past research has identified discriminatory automatic speech recognition\n(ASR) performance as a function of the racial group and nationality of the\nspeaker. In this paper, we expand the discussion beyond bias as a function of\nthe individual national origin of the speaker to look for bias as a function of\nthe geopolitical orientation of their nation of origin. We audit some of the\nmost popular English language ASR services using a large and global data set of\nspeech from The Speech Accent Archive, which includes over 2,700 speakers of\nEnglish born in 171 different countries. We show that, even when controlling\nfor multiple linguistic covariates, ASR service performance has a statistically\nsignificant relationship to the political alignment of the speaker's birth\ncountry with respect to the United States' geopolitical power. This holds for\nall ASR services tested. We discuss this bias in the context of the historical\nuse of language to maintain global and political power.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DiChristofano_A/0/1/0/all/0/1\">Alex DiChristofano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_H/0/1/0/all/0/1\">Henry Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_S/0/1/0/all/0/1\">Shefali Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwari_N/0/1/0/all/0/1\">Neal Patwari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Approaches to Multilingual Information Retrieval. (arXiv:2209.01335v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2209.01335","description":"<p>Providing access to information across languages has been a goal of\nInformation Retrieval (IR) for decades. While progress has been made on Cross\nLanguage IR (CLIR) where queries are expressed in one language and documents in\nanother, the multilingual (MLIR) task to create a single ranked list of\ndocuments across many languages is considerably more challenging. This paper\ninvestigates whether advances in neural document translation and pretrained\nmultilingual neural language models enable improvements in the state of the art\nover earlier MLIR techniques. The results show that although combining neural\ndocument translation with neural ranking yields the best Mean Average Precision\n(MAP), 98% of that MAP score can be achieved with an 84% reduction in indexing\ntime by using a pretrained XLM-R multilingual language model to index documents\nin their native language, and that 2% difference in effectiveness is not\nstatistically significant. Key to achieving these results for MLIR is to\nfine-tune XLM-R using mixed-language batches from neural translations of MS\nMARCO passages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lawrie_D/0/1/0/all/0/1\">Dawn Lawrie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eugene Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oard_D/0/1/0/all/0/1\">Douglas W. Oard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayfield_J/0/1/0/all/0/1\">James Mayfield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COMPS: Conceptual Minimal Pair Sentences for testing Robust Property Knowledge and its Inheritance in Pre-trained Language Models. (arXiv:2210.01963v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01963","description":"<p>A characteristic feature of human semantic cognition is its ability to not\nonly store and retrieve the properties of concepts observed through experience,\nbut to also facilitate the inheritance of properties (can breathe) from\nsuperordinate concepts (animal) to their subordinates (dog) -- i.e. demonstrate\nproperty inheritance. In this paper, we present COMPS, a collection of minimal\npair sentences that jointly tests pre-trained language models (PLMs) on their\nability to attribute properties to concepts and their ability to demonstrate\nproperty inheritance behavior. Analyses of 22 different PLMs on COMPS reveal\nthat they can easily distinguish between concepts on the basis of a property\nwhen they are trivially different, but find it relatively difficult when\nconcepts are related on the basis of nuanced knowledge representations.\nFurthermore, we find that PLMs can demonstrate behavior consistent with\nproperty inheritance to a great extent, but fail in the presence of distracting\ninformation, which decreases the performance of many models, sometimes even\nbelow chance. This lack of robustness in demonstrating simple reasoning raises\nimportant questions about PLMs' capacity to make correct inferences even when\nthey appear to possess the prerequisite knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Misra_K/0/1/0/all/0/1\">Kanishka Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayz_J/0/1/0/all/0/1\">Julia Taylor Rayz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ettinger_A/0/1/0/all/0/1\">Allyson Ettinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Pre-Training by Reducing Representation Confusion. (arXiv:2210.04246v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04246","description":"<p>In this work, we revisit the Transformer-based pre-trained language models\nand identify two different types of information confusion in position encoding\nand model representations, respectively. Firstly, we show that in the relative\nposition encoding, the joint modeling about relative distances and directions\nbrings confusion between two heterogeneous information. It may make the model\nunable to capture the associative semantics of the same distance and the\nopposite directions, which in turn affects the performance of downstream tasks.\nSecondly, we notice the BERT with Mask Language Modeling (MLM) pre-training\nobjective outputs similar token representations (last hidden states of\ndifferent tokens) and head representations (attention weights of different\nheads), which may make the diversity of information expressed by different\ntokens and heads limited. Motivated by the above investigation, we propose two\nnovel techniques to improve pre-trained language models: Decoupled Directional\nRelative Position (DDRP) encoding and MTH pre-training objective. DDRP\ndecouples the relative distance features and the directional features in\nclassical relative position encoding. MTH applies two novel auxiliary\nregularizers besides MLM to enlarge the dissimilarities between (a) last hidden\nstates of different tokens, and (b) attention weights of different heads. These\ndesigns allow the model to capture different categories of information more\nclearly, as a way to alleviate information confusion in representation learning\nfor better optimization. Extensive experiments and ablation studies on GLUE\nbenchmark demonstrate the effectiveness of our proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haojie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingfei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Ruobing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Leyu Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Best Practices in the Creation and Use of Emotion Lexicons. (arXiv:2210.07206v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07206","description":"<p>Words play a central role in how we express ourselves. Lexicons of\nword-emotion associations are widely used in research and real-world\napplications for sentiment analysis, tracking emotions associated with products\nand policies, studying health disorders, tracking emotional arcs of stories,\nand so on. However, inappropriate and incorrect use of these lexicons can lead\nto not just sub-optimal results, but also inferences that are directly harmful\nto people. This paper brings together ideas from Affective Computing and AI\nEthics to present, some of the practical and ethical considerations involved in\nthe creation and use of emotion lexicons -- best practices. The goal is to\nprovide a comprehensive set of relevant considerations, so that readers\n(especially those new to work with emotions) can find relevant information in\none place. We hope this work will facilitate more thoughtfulness when one is\ndeciding on what emotions to work on, how to create an emotion lexicon, how to\nuse an emotion lexicon, how to draw meaningful inferences, and how to judge\nsuccess.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TPU-MLIR: A Compiler For TPU Using MLIR. (arXiv:2210.15016v2 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2210.15016","description":"<p>Multi-level intermediate representations (MLIR) show great promise for\nreducing the cost of building domain-specific compilers by providing a reusable\nand extensible compiler infrastructure. This work presents TPU-MLIR, an\nend-to-end compiler based on MLIR that deploys pre-trained neural network (NN)\nmodels to a custom ASIC called a Tensor Processing Unit (TPU). TPU-MLIR defines\ntwo new dialects to implement its functionality: 1. a Tensor operation (TOP)\ndialect that encodes the deep learning graph semantics and independent of the\ndeep learning framework and 2. a TPU kernel dialect to provide a standard\nkernel computation on TPU. A NN model is translated to the TOP dialect and then\nlowered to the TPU dialect for different TPUs according to the chip's\nconfiguration. We demonstrate how to use the MLIR pass pipeline to organize and\nperform optimization on TPU to generate machine code. The paper also presents a\nverification procedure to ensure the correctness of each transform stage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Pengchao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Man Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Guoyue Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Average Token Delay: A Latency Metric for Simultaneous Translation. (arXiv:2211.13173v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.13173","description":"<p>Simultaneous translation is a task in which translation begins before the\nspeaker has finished speaking. In its evaluation, we have to consider the\nlatency of the translation in addition to the quality. The latency is\npreferably as small as possible for users to comprehend what the speaker says\nwith a small delay. Existing latency metrics focus on when the translation\nstarts but do not consider adequately when the translation ends. This means\nsuch metrics do not penalize the latency caused by a long translation output,\nwhich actually delays users' comprehension. In this work, we propose a novel\nlatency evaluation metric called Average Token Delay (ATD) that focuses on the\nend timings of partial translations in simultaneous translation. We discuss the\nadvantage of ATD using simulated examples and also investigate the differences\nbetween ATD and Average Lagging with simultaneous translation experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kano_Y/0/1/0/all/0/1\">Yasumasa Kano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1\">Katsuhito Sudoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposing a Recurrent Neural Network into Modules for Enabling Reusability and Replacement. (arXiv:2212.05970v3 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2212.05970","description":"<p>Can we take a recurrent neural network (RNN) trained to translate between\nlanguages and augment it to support a new natural language without retraining\nthe model from scratch? Can we fix the faulty behavior of the RNN by replacing\nportions associated with the faulty behavior? Recent works on decomposing a\nfully connected neural network (FCNN) and convolutional neural network (CNN)\ninto modules have shown the value of engineering deep models in this manner,\nwhich is standard in traditional SE but foreign for deep learning models.\nHowever, prior works focus on the image-based multiclass classification\nproblems and cannot be applied to RNN due to (a) different layer structures,\n(b) loop structures, (c) different types of input-output architectures, and (d)\nusage of both nonlinear and logistic activation functions. In this work, we\npropose the first approach to decompose an RNN into modules. We study different\ntypes of RNNs, i.e., Vanilla, LSTM, and GRU. Further, we show how such RNN\nmodules can be reused and replaced in various scenarios. We evaluate our\napproach against 5 canonical datasets (i.e., Math QA, Brown Corpus,\nWiki-toxicity, Clinc OOS, and Tatoeba) and 4 model variants for each dataset.\nWe found that decomposing a trained model has a small cost (Accuracy: -0.6%,\nBLEU score: +0.10%). Also, the decomposed modules can be reused and replaced\nwithout needing to retrain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imtiaz_S/0/1/0/all/0/1\">Sayem Mohammad Imtiaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batole_F/0/1/0/all/0/1\">Fraol Batole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Astha Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1\">Rangeet Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_B/0/1/0/all/0/1\">Breno Dantas Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1\">Hridesh Rajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Structured Object Sequence Encoders. (arXiv:2301.01015v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2301.01015","description":"<p>In this paper we explore the task of modeling (semi) structured object\nsequences; in particular we focus our attention on the problem of developing a\nstructure-aware input representation for such sequences. In such sequences, we\nassume that each structured object is represented by a set of key-value pairs\nwhich encode the attributes of the structured object. Given a universe of keys,\na sequence of structured objects can then be viewed as an evolution of the\nvalues for each key, over time. We encode and construct a sequential\nrepresentation using the values for a particular key (Temporal Value Modeling -\nTVM) and then self-attend over the set of key-conditioned value sequences to a\ncreate a representation of the structured object sequence (Key Aggregation -\nKA). We pre-train and fine-tune the two components independently and present an\ninnovative training schedule that interleaves the training of both modules with\nshared attention heads. We find that this iterative two part-training results\nin better performance than a unified network with hierarchical encoding as well\nas over, other methods that use a {\\em record-view} representation of the\nsequence \\cite{de2021transformers4rec} or a simple {\\em flattened}\nrepresentation of the sequence. We conduct experiments using real-world data to\ndemonstrate the advantage of interleaving TVM-KA on multiple tasks and detailed\nablation studies motivating our modeling choices. We find that our approach\nperforms better than flattening sequence objects and also allows us to operate\non significantly larger sequences than existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1\">Rudra Murthy V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Riyaz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasekara_C/0/1/0/all/0/1\">Chulaka Gunasekara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Siva Sankalp Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Hui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamecha_T/0/1/0/all/0/1\">Tejas Indulal Dhamecha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contractor_D/0/1/0/all/0/1\">Danish Contractor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danilevsky_M/0/1/0/all/0/1\">Marina Danilevsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Questions for Zero-Shot Relation Extraction. (arXiv:2301.09640v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.09640","description":"<p>Zero-Shot Relation Extraction (ZRE) is the task of Relation Extraction where\nthe training and test sets have no shared relation types. This very challenging\ndomain is a good test of a model's ability to generalize. Previous approaches\nto ZRE reframed relation extraction as Question Answering (QA), allowing for\nthe use of pre-trained QA models. However, this method required manually\ncreating gold question templates for each new relation. Here, we do away with\nthese gold templates and instead learn a model that can generate questions for\nunseen relations. Our technique can successfully translate relation\ndescriptions into relevant questions, which are then leveraged to generate the\ncorrect tail entity. On tail entity extraction, we outperform the previous\nstate-of-the-art by more than 16 F1 points without using gold question\ntemplates. On the RE-QA dataset where no previous baseline for relation\nextraction exists, our proposed algorithm comes within 0.7 F1 points of a\nsystem that uses gold question templates. Our model also outperforms the\nstate-of-the-art ZRE baselines on the FewRel and WikiZSL datasets, showing that\nQA models no longer need template questions to match the performance of models\nspecifically tailored to the ZRE task. Our implementation is available at\nhttps://github.com/fyshelab/QA-ZRE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Najafi_S/0/1/0/all/0/1\">Saeed Najafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fyshe_A/0/1/0/all/0/1\">Alona Fyshe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Chain-of-Thought Reasoning in Language Models. (arXiv:2302.00923v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00923","description":"<p>Large language models (LLMs) have shown impressive performance on complex\nreasoning by leveraging chain-of-thought (CoT) prompting to generate\nintermediate reasoning chains as the rationale to infer the answer. However,\nexisting CoT studies are mostly isolated in the language modality with LLMs,\nwhere LLMs are hard to deploy. To elicit CoT reasoning in multimodality, a\npossible solution is to fine-tune small language models by fusing the vision\nand language features to perform CoT reasoning. The key challenge is that those\nlanguage models tend to generate hallucinated reasoning chains that mislead the\nanswer inference. To mitigate the effect of such mistakes, we propose\nMultimodal-CoT that incorporates vision features. The framework separates the\nrationale generation and answer inference into two stages. By incorporating the\nvision features in both stages, the model is able to generate effective\nrationales that contribute to answer inference. With Multimodal-CoT, our model\nunder 1 billion parameters outperforms the previous state-of-the-art LLM\n(GPT-3.5) by 16% (75.17%-&gt;91.68%) on the ScienceQA benchmark and even surpasses\nhuman performance. Code is publicly available at\nhttps://github.com/amazon-science/mm-cot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alex Smola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Many and Which Training Points Would Need to be Removed to Flip this Prediction?. (arXiv:2302.02169v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.02169","description":"<p>We consider the problem of identifying a minimal subset of training data\n$\\mathcal{S}_t$ such that if the instances comprising $\\mathcal{S}_t$ had been\nremoved prior to training, the categorization of a given test point $x_t$ would\nhave been different. Identifying such a set may be of interest for a few\nreasons. First, the cardinality of $\\mathcal{S}_t$ provides a measure of\nrobustness (if $|\\mathcal{S}_t|$ is small for $x_t$, we might be less confident\nin the corresponding prediction), which we show is correlated with but\ncomplementary to predicted probabilities. Second, interrogation of\n$\\mathcal{S}_t$ may provide a novel mechanism for contesting a particular model\nprediction: If one can make the case that the points in $\\mathcal{S}_t$ are\nwrongly labeled or irrelevant, this may argue for overturning the associated\nprediction. Identifying $\\mathcal{S}_t$ via brute-force is intractable. We\npropose comparatively fast approximation methods to find $\\mathcal{S}_t$ based\non influence functions, and find that -- for simple convex text classification\nmodels -- these approaches can often successfully identify relatively small\nsets of training examples which, if removed, would flip the prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinghan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sarthak Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Benefits of Training Expert Language Models over Instruction Tuning. (arXiv:2302.03202v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03202","description":"<p>Recently, Language Models (LMs) instruction-tuned on multiple tasks, also\nknown as multitask-prompted fine-tuning (MT), have shown the capability to\ngeneralize to unseen tasks. Previous work has shown that scaling the number of\ntraining tasks is the key component in making stronger MT LMs. In this work, we\nreport an unexpected finding that an expert LM fine-tuned on just a single task\ncan outperform an MT LM trained with 300+ different tasks on 11 different\nunseen datasets and on 13 datasets of the BIG-bench benchmark by a mean\naccuracy of 3.20% and 1.29%, respectively. This finding casts doubt on the\npreviously held belief that simply scaling the number of tasks makes stronger\nMT LMs. Leveraging this finding, we further show that this distributed approach\nof training a separate expert LM per training task instead of a single MT LM\nfor zero-shot inference possesses many benefits including (1) avoiding negative\ntask transfer that often occurs during instruction tuning, (2) being able to\ncontinually learn new tasks without having to re-train on previous tasks to\navoid catastrophic forgetting, and (3) showing compositional capabilities when\nmerging individual experts together. The code is available at\nhttps://github.com/joeljang/ELM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungone Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logeswaran_L/0/1/0/all/0/1\">Lajanugen Logeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Moontae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyungjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-Learning: An Adversarial Process of Two Pre-trained Models for Natural Language Generation. (arXiv:2302.03896v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03896","description":"<p>Pre-trained models have been used in many fields in recent years, ranging\nfrom natural language understanding to computer vision and natural language\ngeneration. Nowadays, the performance of these natural language generation\nmodels is overly dependent on the model's scale and the dataset's size. While\nthe larger language model is excellent in some respects, it cannot learn\nup-to-date knowledge and is relatively difficult to relearn. In this paper, a\nnew adversarial process learning method is called Auto-Learning, which can\nimprove the performance of any natural language generation model without the\nhelp of additional datasets. Auto-Learning includes two models: $G$ is a text\ngeneration model, and $D$ can test whether the data generated by G is\nlegitimate. Firstly, the fine-tuned $D$ model is used as the brain's knowledge\nbase before the process. Then the text generated by the $G$ model is used as\nthe input of $D$ to determine whether the text is legitimate. Finally, $G$ is\nfine-tuned according to the output of $D$. This adversarial process is like a\nself-escalation of the brain through some a priori knowledge. When this\nadversarial system wants to learn something new, simply fine-tune the $D$\nmodel. Our approach applies to Autoregressive Language Modeling for all\nTransformer classes. Auto-Learning enables 8 models to achieve stable\nimprovement in 10 natural language processing tasks without any change in\nstructure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhengqing Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Huiwen Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuelin Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEBERT: Efficient and robust binary ensemble BERT. (arXiv:2210.15976v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2210.15976","description":"<p>Pre-trained BERT models have achieved impressive accuracy on natural language\nprocessing (NLP) tasks. However, their excessive amount of parameters hinders\nthem from efficient deployment on edge devices. Binarization of the BERT models\ncan significantly alleviate this issue but comes with a severe accuracy drop\ncompared with their full-precision counterparts. In this paper, we propose an\nefficient and robust binary ensemble BERT (BEBERT) to bridge the accuracy gap.\nTo the best of our knowledge, this is the first work employing ensemble\ntechniques on binary BERTs, yielding BEBERT, which achieves superior accuracy\nwhile retaining computational efficiency. Furthermore, we remove the knowledge\ndistillation procedures during ensemble to speed up the training process\nwithout compromising accuracy. Experimental results on the GLUE benchmark show\nthat the proposed BEBERT significantly outperforms the existing binary BERT\nmodels in accuracy and robustness with a 2x speedup on training time. Moreover,\nour BEBERT has only a negligible accuracy loss of 0.3% compared to the\nfull-precision baseline while saving 15x and 13x in FLOPs and model size,\nrespectively. In addition, BEBERT also outperforms other compressed BERTs in\naccuracy by up to 6.7%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jiayi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}