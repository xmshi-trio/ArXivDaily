{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-09-25T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Considerations for health care institutions training large language models on electronic health records. (arXiv:2309.12339v1 [cs.CY])","link":"http://arxiv.org/abs/2309.12339","description":"<p>Large language models (LLMs) like ChatGPT have excited scientists across\nfields; in medicine, one source of excitement is the potential applications of\nLLMs trained on electronic health record (EHR) data. But there are tough\nquestions we must first answer if health care institutions are interested in\nhaving LLMs trained on their own data; should they train an LLM from scratch or\nfine-tune it from an open-source model? For healthcare institutions with a\npredefined budget, what are the biggest LLMs they can afford? In this study, we\ntake steps towards answering these questions with an analysis on dataset sizes,\nmodel sizes, and costs for LLM training using EHR data. This analysis provides\na framework for thinking about these questions in terms of data scale, compute\nscale, and training budgets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitterman_D/0/1/0/all/0/1\">Danielle Bitterman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afshar_M/0/1/0/all/0/1\">Majid Afshar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1\">Timothy A. Miller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions. (arXiv:2309.12342v1 [cs.CY])","link":"http://arxiv.org/abs/2309.12342","description":"<p>The deployment of large language models (LLMs) raises concerns regarding\ntheir cultural misalignment and potential ramifications on individuals from\nvarious cultural norms. Existing work investigated political and social biases\nand public opinions rather than their cultural values. To address this\nlimitation, the proposed Cultural Alignment Test (CAT) quantifies cultural\nalignment using Hofstede's cultural dimension framework, which offers an\nexplanatory cross-cultural comparison through the latent variable analysis. We\napply our approach to assess the cultural values embedded in state-of-the-art\nLLMs, such as: ChatGPT and Bard, across diverse cultures of countries: United\nStates (US), Saudi Arabia, China, and Slovakia, using different prompting\nstyles and hyperparameter settings. Our results not only quantify cultural\nalignment of LLMs with certain countries, but also reveal the difference\nbetween LLMs in explanatory cultural dimensions. While all LLMs did not provide\nsatisfactory results in understanding cultural values, GPT-4 exhibited the\nhighest CAT score for the cultural values of the US.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Masoud_R/0/1/0/all/0/1\">Reem I. Masoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferianc_M/0/1/0/all/0/1\">Martin Ferianc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treleaven_P/0/1/0/all/0/1\">Philip Treleaven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1\">Miguel Rodrigues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Social Choice via NLP and Sampling. (arXiv:2309.12360v1 [cs.CY])","link":"http://arxiv.org/abs/2309.12360","description":"<p>Attention-Aware Social Choice tackles the fundamental conflict faced by some\nagent communities between their desire to include all members in the decision\nmaking processes and the limited time and attention that are at the disposal of\nthe community members. Here, we investigate a combination of two techniques for\nattention-aware social choice, namely Natural Language Processing (NLP) and\nSampling. Essentially, we propose a system in which each governance proposal to\nchange the status quo is first sent to a trained NLP model that estimates the\nprobability that the proposal would pass if all community members directly vote\non it; then, based on such an estimation, a population sample of a certain size\nis being selected and the proposal is decided upon by taking the sample\nmajority. We develop several concrete algorithms following the scheme described\nabove and evaluate them using various data, including such from several\nDecentralized Autonomous Organizations (DAOs).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashkenazy_L/0/1/0/all/0/1\">Lior Ashkenazy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talmon_N/0/1/0/all/0/1\">Nimrod Talmon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Assisting Diagnosis of Neuro-ophthalmology Diseases Based on Case Reports. (arXiv:2309.12361v1 [cs.CY])","link":"http://arxiv.org/abs/2309.12361","description":"<p>Objective: To evaluate the efficiency of large language models (LLMs) such as\nChatGPT to assist in diagnosing neuro-ophthalmic diseases based on detailed\ncase descriptions. Methods: We selected 22 different case reports of\nneuro-ophthalmic diseases from a publicly available online database. These\ncases included a wide range of chronic and acute diseases that are commonly\nseen by neuro-ophthalmic sub-specialists. We inserted the text from each case\nas a new prompt into both ChatGPT v3.5 and ChatGPT Plus v4.0 and asked for the\nmost probable diagnosis. We then presented the exact information to two\nneuro-ophthalmologists and recorded their diagnoses followed by comparison to\nresponses from both versions of ChatGPT. Results: ChatGPT v3.5, ChatGPT Plus\nv4.0, and the two neuro-ophthalmologists were correct in 13 (59%), 18 (82%), 19\n(86%), and 19 (86%) out of 22 cases, respectively. The agreement between the\nvarious diagnostic sources were as follows: ChatGPT v3.5 and ChatGPT Plus v4.0,\n13 (59%); ChatGPT v3.5 and the first neuro-ophthalmologist, 12 (55%); ChatGPT\nv3.5 and the second neuro-ophthalmologist, 12 (55%); ChatGPT Plus v4.0 and the\nfirst neuro-ophthalmologist, 17 (77%); ChatGPT Plus v4.0 and the second\nneuro-ophthalmologist, 16 (73%); and first and second neuro-ophthalmologists 17\n(17%). Conclusions: The accuracy of ChatGPT v3.5 and ChatGPT Plus v4.0 in\ndiagnosing patients with neuro-ophthalmic diseases was 59% and 82%,\nrespectively. With further development, ChatGPT Plus v4.0 may have potential to\nbe used in clinical care settings to assist clinicians in providing quick,\naccurate diagnoses of patients in neuro-ophthalmology. The applicability of\nusing LLMs like ChatGPT in clinical settings that lack access to subspeciality\ntrained neuro-ophthalmologists deserves further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madadi_Y/0/1/0/all/0/1\">Yeganeh Madadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delsoz_M/0/1/0/all/0/1\">Mohammad Delsoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_P/0/1/0/all/0/1\">Priscilla A. Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_J/0/1/0/all/0/1\">Joseph W. Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollingsworth_T/0/1/0/all/0/1\">TJ Hollingsworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahook_M/0/1/0/all/0/1\">Malik Y. Kahook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousefi_S/0/1/0/all/0/1\">Siamak Yousefi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining the Influence of Varied Levels of Domain Knowledge Base Inclusion in GPT-based Intelligent Tutors. (arXiv:2309.12367v1 [cs.HC])","link":"http://arxiv.org/abs/2309.12367","description":"<p>Recent advancements in large language models (LLMs) have facilitated the\ndevelopment of chatbots with sophisticated conversational capabilities.\nHowever, LLMs exhibit frequent inaccurate responses to queries, hindering\napplications in educational settings. In this paper, we investigate the\neffectiveness of integrating a knowledge base (KB) with LLM intelligent tutors\nto increase response reliability. To achieve this, we design a scaleable KB\nthat affords educational supervisors seamless integration of lesson curricula,\nwhich is automatically processed by the intelligent tutoring system. We then\ndetail an evaluation, where student participants were presented with questions\nabout the artificial intelligence curriculum to respond to. GPT-4 intelligent\ntutors with varying hierarchies of KB access and human domain experts then\nassessed these responses. Lastly, students cross-examined the intelligent\ntutors' responses to the domain experts' and ranked their various pedagogical\nabilities. Results suggest that, although these intelligent tutors still\ndemonstrate a lower accuracy compared to domain experts, the accuracy of the\nintelligent tutors increases when access to a KB is granted. We also observe\nthat the intelligent tutors with KB access exhibit better pedagogical abilities\nto speak like a teacher and understand students than those of domain experts,\nwhile their ability to help students remains lagging behind domain experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Castleman_B/0/1/0/all/0/1\">Blake Castleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turkcan_M/0/1/0/all/0/1\">Mehmet Kerem Turkcan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constraints First: A New MDD-based Model to Generate Sentences Under Constraints. (arXiv:2309.12415v1 [cs.AI])","link":"http://arxiv.org/abs/2309.12415","description":"<p>This paper introduces a new approach to generating strongly constrained\ntexts. We consider standardized sentence generation for the typical application\nof vision screening. To solve this problem, we formalize it as a discrete\ncombinatorial optimization problem and utilize multivalued decision diagrams\n(MDD), a well-known data structure to deal with constraints. In our context,\none key strength of MDD is to compute an exhaustive set of solutions without\nperforming any search. Once the sentences are obtained, we apply a language\nmodel (GPT-2) to keep the best ones. We detail this for English and also for\nFrench where the agreement and conjugation rules are known to be more complex.\nFinally, with the help of GPT-2, we get hundreds of bona-fide candidate\nsentences. When compared with the few dozen sentences usually available in the\nwell-known vision screening test (MNREAD), this brings a major breakthrough in\nthe field of standardized sentence generation. Also, as it can be easily\nadapted for other languages, it has the potential to make the MNREAD test even\nmore valuable and usable. More generally, this paper highlights MDD as a\nconvincing alternative for constrained text generation, especially when the\nconstraints are hard to satisfy, but also for many other prospects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonlarron_A/0/1/0/all/0/1\">Alexandre Bonlarron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calabrese_A/0/1/0/all/0/1\">Aur&#xe9;lie Calabr&#xe8;se</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornprobst_P/0/1/0/all/0/1\">Pierre Kornprobst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regin_J/0/1/0/all/0/1\">Jean-Charles R&#xe9;gin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges. (arXiv:2309.12426v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12426","description":"<p>Large Language Models (LLMs) have demonstrated impressive zero shot\nperformance on a wide range of NLP tasks, demonstrating the ability to reason\nand apply commonsense. A relevant application is to use them for creating high\nquality synthetic datasets for downstream tasks. In this work, we probe whether\nGPT-4 can be used to augment existing extractive reading comprehension\ndatasets. Automating data annotation processes has the potential to save large\namounts of time, money and effort that goes into manually labelling datasets.\nIn this paper, we evaluate the performance of GPT-4 as a replacement for human\nannotators for low resource reading comprehension tasks, by comparing\nperformance after fine tuning, and the cost associated with annotation. This\nwork serves to be the first analysis of LLMs as synthetic data augmenters for\nQA systems, highlighting the unique opportunities and challenges. Additionally,\nwe release augmented versions of low resource datasets, that will allow the\nresearch community to create further benchmarks for evaluation of generated\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samuel_V/0/1/0/all/0/1\">Vinay Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aynaou_H/0/1/0/all/0/1\">Houda Aynaou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1\">Arijit Ghosh Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_K/0/1/0/all/0/1\">Karthik Venkat Ramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Multilingual Fingerspelling Corpora. (arXiv:2309.12443v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12443","description":"<p>We apply active learning to help with data scarcity problems in sign\nlanguages. In particular, we perform a novel analysis of the effect of\npre-training. Since many sign languages are linguistic descendants of French\nsign language, they share hand configurations, which pre-training can hopefully\nexploit. We test this hypothesis on American, Chinese, German, and Irish\nfingerspelling corpora. We do observe a benefit from pre-training, but this may\nbe due to visual rather than linguistic similarities\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalisnick_E/0/1/0/all/0/1\">Eric Nalisnick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI. (arXiv:2309.12444v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12444","description":"<p>Generative Artificial Intelligence is set to revolutionize healthcare\ndelivery by transforming traditional patient care into a more personalized,\nefficient, and proactive process. Chatbots, serving as interactive\nconversational models, will probably drive this patient-centered transformation\nin healthcare. Through the provision of various services, including diagnosis,\npersonalized lifestyle recommendations, and mental health support, the\nobjective is to substantially augment patient health outcomes, all the while\nmitigating the workload burden on healthcare providers. The life-critical\nnature of healthcare applications necessitates establishing a unified and\ncomprehensive set of evaluation metrics for conversational models. Existing\nevaluation metrics proposed for various generic large language models (LLMs)\ndemonstrate a lack of comprehension regarding medical and health concepts and\ntheir significance in promoting patients' well-being. Moreover, these metrics\nneglect pivotal user-centered aspects, including trust-building, ethics,\npersonalization, empathy, user comprehension, and emotional support. The\npurpose of this paper is to explore state-of-the-art LLM-based evaluation\nmetrics that are specifically applicable to the assessment of interactive\nconversational models in healthcare. Subsequently, we present an comprehensive\nset of evaluation metrics designed to thoroughly assess the performance of\nhealthcare chatbots from an end-user perspective. These metrics encompass an\nevaluation of language processing abilities, impact on real-world clinical\ntasks, and effectiveness in user-interactive conversations. Finally, we engage\nin a discussion concerning the challenges associated with defining and\nimplementing these metrics, with particular emphasis on confounding factors\nsuch as the target audience, evaluation methods, and prompt techniques involved\nin the evaluation process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abbasian_M/0/1/0/all/0/1\">Mahyar Abbasian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khatibi_E/0/1/0/all/0/1\">Elahe Khatibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azimi_I/0/1/0/all/0/1\">Iman Azimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1\">David Oniani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abad_Z/0/1/0/all/0/1\">Zahra Shakeri Hossein Abad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thieme_A/0/1/0/all/0/1\">Alexander Thieme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhongqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanshan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bryant Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1\">Olivier Gevaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li-Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Ramesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1\">Amir M. Rahmani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation. (arXiv:2309.12455v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12455","description":"<p>Maintaining factual consistency is a critical issue in abstractive text\nsummarisation, however, it cannot be assessed by traditional automatic metrics\nused for evaluating text summarisation, such as ROUGE scoring. Recent efforts\nhave been devoted to developing improved metrics for measuring factual\nconsistency using pre-trained language models, but these metrics have\nrestrictive token limits, and are therefore not suitable for evaluating long\ndocument text summarisation. Moreover, there is limited research evaluating\nwhether existing automatic evaluation metrics are fit for purpose when applied\nto long document data sets. In this work, we evaluate the efficacy of automatic\nmetrics at assessing factual consistency in long document text summarisation\nand propose a new evaluation framework LongDocFACTScore. This framework allows\nmetrics to be extended to any length document. This framework outperforms\nexisting state-of-the-art metrics in its ability to correlate with human\nmeasures of factuality when used to evaluate long document summarisation data\nsets. Furthermore, we show LongDocFACTScore has performance comparable to\nstate-of-the-art metrics when evaluated against human measures of factual\nconsistency on short document data sets. We make our code and annotated data\npublicly available: https://github.com/jbshp/LongDocFACTScore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bishop_J/0/1/0/all/0/1\">Jennifer A Bishop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Deep Learning for Scientific Imaging Interpretation. (arXiv:2309.12460v1 [cs.LG])","link":"http://arxiv.org/abs/2309.12460","description":"<p>In the domain of scientific imaging, interpreting visual data often demands\nan intricate combination of human expertise and deep comprehension of the\nsubject materials. This study presents a novel methodology to linguistically\nemulate and subsequently evaluate human-like interactions with Scanning\nElectron Microscopy (SEM) images, specifically of glass materials. Leveraging a\nmultimodal deep learning framework, our approach distills insights from both\ntextual and visual data harvested from peer-reviewed articles, further\naugmented by the capabilities of GPT-4 for refined data synthesis and\nevaluation. Despite inherent challenges--such as nuanced interpretations and\nthe limited availability of specialized datasets--our model (GlassLLaVA) excels\nin crafting accurate interpretations, identifying key features, and detecting\ndefects in previously unseen SEM images. Moreover, we introduce versatile\nevaluation metrics, suitable for an array of scientific imaging applications,\nwhich allows for benchmarking against research-grounded answers. Benefiting\nfrom the robustness of contemporary Large Language Models, our model adeptly\naligns with insights from research papers. This advancement not only\nunderscores considerable progress in bridging the gap between human and machine\ninterpretation in scientific imaging, but also hints at expansive avenues for\nfuture research and broader application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alshehri_A/0/1/0/all/0/1\">Abdulelah S. Alshehri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_F/0/1/0/all/0/1\">Franklin L. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shihu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HANS, are you clever? Clever Hans Effect Analysis of Neural Systems. (arXiv:2309.12481v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12481","description":"<p>Instruction-tuned Large Language Models (It-LLMs) have been exhibiting\noutstanding abilities to reason around cognitive states, intentions, and\nreactions of all people involved, letting humans guide and comprehend\nday-to-day social interactions effectively. In fact, several multiple-choice\nquestions (MCQ) benchmarks have been proposed to construct solid assessments of\nthe models' abilities. However, earlier works are demonstrating the presence of\ninherent \"order bias\" in It-LLMs, posing challenges to the appropriate\nevaluation. In this paper, we investigate It-LLMs' resilience abilities towards\na series of probing tests using four MCQ benchmarks. Introducing adversarial\nexamples, we show a significant performance gap, mainly when varying the order\nof the choices, which reveals a selection bias and brings into discussion\nreasoning abilities. Following a correlation between first positions and model\nchoices due to positional bias, we hypothesized the presence of structural\nheuristics in the decision-making process of the It-LLMs, strengthened by\nincluding significant examples in few-shot scenarios. Finally, by using the\nChain-of-Thought (CoT) technique, we elicit the model to reason and mitigate\nthe bias by obtaining more robust models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranaldi_L/0/1/0/all/0/1\">Leonardo Ranaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanzotto_F/0/1/0/all/0/1\">Fabio Massimo Zanzotto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Studying and improving reasoning in humans and machines. (arXiv:2309.12485v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12485","description":"<p>In the present study, we investigate and compare reasoning in large language\nmodels (LLM) and humans using a selection of cognitive psychology tools\ntraditionally dedicated to the study of (bounded) rationality. To do so, we\npresented to human participants and an array of pretrained LLMs new variants of\nclassical cognitive experiments, and cross-compared their performances. Our\nresults showed that most of the included models presented reasoning errors akin\nto those frequently ascribed to error-prone, heuristic-based human reasoning.\nNotwithstanding this superficial similarity, an in-depth comparison between\nhumans and LLMs indicated important differences with human-like reasoning, with\nmodels limitations disappearing almost entirely in more recent LLMs releases.\nMoreover, we show that while it is possible to devise strategies to induce\nbetter performance, humans and machines are not equally-responsive to the same\nprompting schemes. We conclude by discussing the epistemological implications\nand challenges of comparing human and machine behavior for both artificial\nintelligence and cognitive psychology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yax_N/0/1/0/all/0/1\">Nicolas Yax</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anllo_H/0/1/0/all/0/1\">Hernan Anll&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palminteri_S/0/1/0/all/0/1\">Stefano Palminteri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation. (arXiv:2309.12491v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12491","description":"<p>We study the effect of tokenization on gender bias in machine translation, an\naspect that has been largely overlooked in previous works. Specifically, we\nfocus on the interactions between the frequency of gendered profession names in\ntraining data, their representation in the subword tokenizer's vocabulary, and\ngender bias. We observe that female and non-stereotypical gender inflections of\nprofession names (e.g., Spanish \"doctora\" for \"female doctor\") tend to be split\ninto multiple subword tokens. Our results indicate that the imbalance of gender\nforms in the model's training corpus is a major factor contributing to gender\nbias and has a greater impact than subword splitting. We show that analyzing\nsubword splits provides good estimates of gender-form imbalance in the training\ndata and can be used even when the corpus is not publicly available. We also\ndemonstrate that fine-tuning just the token embedding layer can decrease the\ngap in gender prediction accuracy between female and male forms without\nimpairing the translation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iluz_B/0/1/0/all/0/1\">Bar Iluz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Limisiewicz_T/0/1/0/all/0/1\">Tomasz Limisiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marecek_D/0/1/0/all/0/1\">David Mare&#x10d;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Embedding: An Overview. (arXiv:2309.12501v1 [cs.AI])","link":"http://arxiv.org/abs/2309.12501","description":"<p>Many mathematical models have been leveraged to design embeddings for\nrepresenting Knowledge Graph (KG) entities and relations for link prediction\nand many downstream tasks. These mathematically-inspired models are not only\nhighly scalable for inference in large KGs, but also have many explainable\nadvantages in modeling different relation patterns that can be validated\nthrough both formal proofs and empirical results. In this paper, we make a\ncomprehensive overview of the current state of research in KG completion. In\nparticular, we focus on two main branches of KG embedding (KGE) design: 1)\ndistance-based methods and 2) semantic matching-based methods. We discover the\nconnections between recently proposed models and present an underlying trend\nthat might help researchers invent novel and more effective models. Next, we\ndelve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D\naffine operations, respectively. They encompass a broad spectrum of techniques\nincluding distance-based and semantic-based methods. We will also discuss an\nemerging approach for KG completion which leverages pre-trained language models\n(PLMs) and textual descriptions of entities and relations and offer insights\ninto the integration of KGE embedding methods with PLMs for KG completion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xiou Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yun-Cheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Answerability Evaluation for Question Generation. (arXiv:2309.12546v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12546","description":"<p>Conventional automatic evaluation metrics, such as BLEU and ROUGE, developed\nfor natural language generation (NLG) tasks, are based on measuring the n-gram\noverlap between the generated and reference text. These simple metrics may be\ninsufficient for more complex tasks, such as question generation (QG), which\nrequires generating questions that are answerable by the reference answers.\nDeveloping a more sophisticated automatic evaluation metric, thus, remains as\nan urgent problem in QG research. This work proposes a Prompting-based Metric\non ANswerability (PMAN), a novel automatic evaluation metric to assess whether\nthe generated questions are answerable by the reference answers for the QG\ntasks. Extensive experiments demonstrate that its evaluation results are\nreliable and align with human evaluations. We further apply our metric to\nevaluate the performance of QG models, which shows our metric complements\nconventional metrics. Our implementation of a ChatGPT-based QG model achieves\nstate-of-the-art (SOTA) performance in generating answerable questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funakoshi_K/0/1/0/all/0/1\">Kotaro Funakoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okumura_M/0/1/0/all/0/1\">Manabu Okumura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is it Possible to Modify Text to a Target Readability Level? An Initial Investigation Using Zero-Shot Large Language Models. (arXiv:2309.12551v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12551","description":"<p>Text simplification is a common task where the text is adapted to make it\neasier to understand. Similarly, text elaboration can make a passage more\nsophisticated, offering a method to control the complexity of reading\ncomprehension tests. However, text simplification and elaboration tasks are\nlimited to only relatively alter the readability of texts. It is useful to\ndirectly modify the readability of any text to an absolute target readability\nlevel to cater to a diverse audience. Ideally, the readability of\nreadability-controlled generated text should be independent of the source text.\nTherefore, we propose a novel readability-controlled text modification task.\nThe task requires the generation of 8 versions at various target readability\nlevels for each input text. We introduce novel readability-controlled text\nmodification metrics. The baselines for this task use ChatGPT and Llama-2, with\nan extension approach introducing a two-step process (generating paraphrases by\npassing through the language model twice). The zero-shot approaches are able to\npush the readability of the paraphrases in the desired direction but the final\nreadability remains correlated with the original text's readability. We also\nfind greater drops in semantic and lexical similarity between the source and\ntarget texts with greater shifts in the readability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farajidizaji_A/0/1/0/all/0/1\">Asma Farajidizaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vatsal Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PlanFitting: Tailoring Personalized Exercise Plans with Large Language Models. (arXiv:2309.12555v1 [cs.HC])","link":"http://arxiv.org/abs/2309.12555","description":"<p>A personally tailored exercise regimen is crucial to ensuring sufficient\nphysical activities, yet challenging to create as people have complex schedules\nand considerations and the creation of plans often requires iterations with\nexperts. We present PlanFitting, a conversational AI that assists in\npersonalized exercise planning. Leveraging generative capabilities of large\nlanguage models, PlanFitting enables users to describe various constraints and\nqueries in natural language, thereby facilitating the creation and refinement\nof their weekly exercise plan to suit their specific circumstances while\nstaying grounded in foundational principles. Through a user study where\nparticipants (N=18) generated a personalized exercise plan using PlanFitting\nand expert planners (N=3) evaluated these plans, we identified the potential of\nPlanFitting in generating personalized, actionable, and evidence-based exercise\nplans. We discuss future design opportunities for AI assistants in creating\nplans that better comply with exercise principles and accommodate personal\nconstraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1\">Donghoon Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_G/0/1/0/all/0/1\">Gary Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Ho Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers. (arXiv:2309.12570v1 [cs.HC])","link":"http://arxiv.org/abs/2309.12570","description":"<p>The development of large language models (LLMs) capable of following\ninstructions and engaging in conversational interactions sparked increased\ninterest in their utilization across various support tools. We investigate the\nutility of modern LLMs in assisting professional writers via an empirical user\nstudy (n=30). The design of our collaborative writing interface is grounded in\nthe cognitive process model of writing that views writing as a goal-oriented\nthinking process encompassing non-linear cognitive activities: planning,\ntranslating, and reviewing. Participants are asked to submit a post-completion\nsurvey to provide feedback on the potential and pitfalls of LLMs as writing\ncollaborators. Upon analyzing the writer-LLM interactions, we find that while\nwriters seek LLM's help across all three types of cognitive activities, they\nfind LLMs more helpful in translation and reviewing. Our findings from\nanalyzing both the interactions and the survey responses highlight future\nresearch directions in creative writing assistance using LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1\">Vishakh Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking Model Insights: A Dataset for Automated Model Card Generation. (arXiv:2309.12616v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12616","description":"<p>Language models (LMs) are no longer restricted to ML community, and\ninstruction-tuned LMs have led to a rise in autonomous AI agents. As the\naccessibility of LMs grows, it is imperative that an understanding of their\ncapabilities, intended usage, and development cycle also improves. Model cards\nare a popular practice for documenting detailed information about an ML model.\nTo automate model card generation, we introduce a dataset of 500\nquestion-answer pairs for 25 ML models that cover crucial aspects of the model,\nsuch as its training configurations, datasets, biases, architecture details,\nand training resources. We employ annotators to extract the answers from the\noriginal paper. Further, we explore the capabilities of LMs in generating model\ncards by answering questions. Our initial experiments with ChatGPT-3.5, LLaMa,\nand Galactica showcase a significant gap in the understanding of research\npapers by these aforementioned LMs as well as generating factual textual\nresponses. We posit that our dataset can be used to train models to automate\nthe generation of model cards from paper text and reduce human effort in the\nmodel card curation process. The complete dataset is available on\nhttps://osf.io/hqt7p/?view_only=3b9114e3904c4443bcd9f5c270158d37\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Shruti Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lodwal_H/0/1/0/all/0/1\">Hitesh Lodwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malwat_H/0/1/0/all/0/1\">Husain Malwat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_R/0/1/0/all/0/1\">Rakesh Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Diversify Neural Text Generation via Degenerative Model. (arXiv:2309.12619v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12619","description":"<p>Neural language models often fail to generate diverse and informative texts,\nlimiting their applicability in real-world problems. While previous approaches\nhave proposed to address these issues by identifying and penalizing undesirable\nbehaviors (e.g., repetition, overuse of frequent words) from language models,\nwe propose an alternative approach based on an observation: models primarily\nlearn attributes within examples that are likely to cause degeneration\nproblems. Based on this observation, we propose a new approach to prevent\ndegeneration problems by training two models. Specifically, we first train a\nmodel that is designed to amplify undesirable patterns. We then enhance the\ndiversity of the second model by focusing on patterns that the first model\nfails to learn. Extensive experiments on two tasks, namely language modeling\nand dialogue generation, demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jimin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">ChaeHun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients. (arXiv:2309.12625v1 [cs.AI])","link":"http://arxiv.org/abs/2309.12625","description":"<p>In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) plays\na key role but its current assignment process is time-consuming. We introduce\nDRG-LLaMA, a large language model (LLM) fine-tuned on clinical notes for\nimproved DRG prediction. Using Meta's LLaMA as the base model, we optimized it\nwith Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries. With\nan input token length of 512, DRG-LLaMA-7B achieved a macro-averaged F1 score\nof 0.327, a top-1 prediction accuracy of 52.0% and a macro-averaged Area Under\nthe Curve (AUC) of 0.986. Impressively, DRG-LLaMA-7B surpassed previously\nreported leading models on this task, demonstrating a relative improvement in\nmacro-averaged F1 score of 40.3% compared to ClinicalBERT and 35.7% compared to\nCAML. When DRG-LLaMA is applied to predict base DRGs and complication or\ncomorbidity (CC) / major complication or comorbidity (MCC), the top-1\nprediction accuracy reached 67.8% for base DRGs and 67.5% for CC/MCC status.\nDRG-LLaMA performance exhibits improvements in correlation with larger model\nparameters and longer input context lengths. Furthermore, usage of LoRA enables\ntraining even on smaller GPUs with 48 GB of VRAM, highlighting the viability of\nadapting LLMs for DRGs prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chufan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dantona_C/0/1/0/all/0/1\">Christopher Dantona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hull_B/0/1/0/all/0/1\">Bryan Hull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction contract risk identification based on knowledge-augmented language model. (arXiv:2309.12626v1 [cs.AI])","link":"http://arxiv.org/abs/2309.12626","description":"<p>Contract review is an essential step in construction projects to prevent\npotential losses. However, the current methods for reviewing construction\ncontracts lack effectiveness and reliability, leading to time-consuming and\nerror-prone processes. While large language models (LLMs) have shown promise in\nrevolutionizing natural language processing (NLP) tasks, they struggle with\ndomain-specific knowledge and addressing specialized issues. This paper\npresents a novel approach that leverages LLMs with construction contract\nknowledge to emulate the process of contract review by human experts. Our\ntuning-free approach incorporates construction contract domain knowledge to\nenhance language models for identifying construction contract risks. The use of\na natural language when building the domain knowledge base facilitates\npractical implementation. We evaluated our method on real construction\ncontracts and achieved solid performance. Additionally, we investigated how\nlarge language models employ logical thinking during the task and provide\ninsights and recommendations for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_S/0/1/0/all/0/1\">Saika Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chunmo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yinqiu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoding Affect in Dyadic Conversations: Leveraging Semantic Similarity through Sentence Embedding. (arXiv:2309.12646v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12646","description":"<p>Recent advancements in Natural Language Processing (NLP) have highlighted the\npotential of sentence embeddings in measuring semantic similarity. Yet, its\napplication in analyzing real-world dyadic interactions and predicting the\naffect of conversational participants remains largely uncharted. To bridge this\ngap, the present study utilizes verbal conversations within 50 married couples\ntalking about conflicts and pleasant activities. Transformer-based model\nall-MiniLM-L6-v2 was employed to obtain the embeddings of the utterances from\neach speaker. The overall similarity of the conversation was then quantified by\nthe average cosine similarity between the embeddings of adjacent utterances.\nResults showed that semantic similarity had a positive association with wives'\naffect during conflict (but not pleasant) conversations. Moreover, this\nassociation was not observed with husbands' affect regardless of conversation\ntypes. Two validation checks further provided support for the validity of the\nsimilarity measure and showed that the observed patterns were not mere\nartifacts of data. The present study underscores the potency of sentence\nembeddings in understanding the association between interpersonal dynamics and\nindividual affect, paving the way for innovative applications in affective and\nrelationship sciences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chen-Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yun-Shiuan Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotsos_A/0/1/0/all/0/1\">Alexandros N. Lotsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haase_C/0/1/0/all/0/1\">Claudia M. Haase</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering. (arXiv:2309.12669v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12669","description":"<p>Answering numerical questions over hybrid contents from the given tables and\ntext(TextTableQA) is a challenging task. Recently, Large Language Models (LLMs)\nhave gained significant attention in the NLP community. With the emergence of\nlarge language models, In-Context Learning and Chain-of-Thought prompting have\nbecome two particularly popular research topics in this field. In this paper,\nwe introduce a new prompting strategy called Hybrid prompt strategy and\nRetrieval of Thought for TextTableQA. Through In-Context Learning, we prompt\nthe model to develop the ability of retrieval thinking when dealing with hybrid\ndata. Our method achieves superior performance compared to the fully-supervised\nSOTA on the MultiHiertt dataset in the few-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tongxu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_F/0/1/0/all/0/1\">Fangyu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jiahe Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shihu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JCoLA: Japanese Corpus of Linguistic Acceptability. (arXiv:2309.12676v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12676","description":"<p>Neural language models have exhibited outstanding performance in a range of\ndownstream tasks. However, there is limited understanding regarding the extent\nto which these models internalize syntactic knowledge, so that various datasets\nhave recently been constructed to facilitate syntactic evaluation of language\nmodels across languages. In this paper, we introduce JCoLA (Japanese Corpus of\nLinguistic Acceptability), which consists of 10,020 sentences annotated with\nbinary acceptability judgments. Specifically, those sentences are manually\nextracted from linguistics textbooks, handbooks and journal articles, and split\ninto in-domain data (86 %; relatively simple acceptability judgments extracted\nfrom textbooks and handbooks) and out-of-domain data (14 %; theoretically\nsignificant acceptability judgments extracted from journal articles), the\nlatter of which is categorized by 12 linguistic phenomena. We then evaluate the\nsyntactic knowledge of 9 different types of Japanese language models on JCoLA.\nThe results demonstrated that several models could surpass human performance\nfor the in-domain data, while no models were able to exceed human performance\nfor the out-of-domain data. Error analyses by linguistic phenomena further\nrevealed that although neural language models are adept at handling local\nsyntactic dependencies like argument structure, their performance wanes when\nconfronted with long-distance syntactic dependencies like verbal agreement and\nNPI licensing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Someya_T/0/1/0/all/0/1\">Taiga Someya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_Y/0/1/0/all/0/1\">Yushi Sugimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseki_Y/0/1/0/all/0/1\">Yohei Oseki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer. (arXiv:2309.12689v1 [cs.LG])","link":"http://arxiv.org/abs/2309.12689","description":"<p>Mixup is an effective data augmentation method that generates new augmented\nsamples by aggregating linear combinations of different original samples.\nHowever, if there are noises or aberrant features in the original samples,\nMixup may propagate them to the augmented samples, leading to over-sensitivity\nof the model to these outliers . To solve this problem, this paper proposes a\nnew Mixup method called AMPLIFY. This method uses the Attention mechanism of\nTransformer itself to reduce the influence of noises and aberrant values in the\noriginal samples on the prediction results, without increasing additional\ntrainable parameters, and the computational cost is very low, thereby avoiding\nthe problem of high resource consumption in common Mixup methods such as\nSentence Mixup . The experimental results show that, under a smaller\ncomputational resource cost, AMPLIFY outperforms other Mixup methods in text\nclassification tasks on 7 benchmark datasets, providing new ideas and new ways\nto further improve the performance of pre-trained models based on the Attention\nmechanism, such as BERT, ALBERT, RoBERTa, and GPT. Our code can be obtained at\nhttps://github.com/kiwi-lilo/AMPLIFY.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Leixin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yu Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic similarity prediction is better than other semantic similarity measures. (arXiv:2309.12697v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12697","description":"<p>Semantic similarity between natural language texts is typically measured\neither by looking at the overlap between subsequences (e.g., BLEU) or by using\nembeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we\nare only interested in measuring the semantic similarity, it is better to\ndirectly predict the similarity using a fine-tuned model for such a task. Using\na fine-tuned model for the STS-B from the GLUE benchmark, we define the\nSTSScore approach and show that the resulting similarity is better aligned with\nour expectations on a robust semantic similarity measure than other approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Herbold_S/0/1/0/all/0/1\">Steffen Herbold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-context Interference in Chat-based Large Language Models. (arXiv:2309.12727v1 [cs.AI])","link":"http://arxiv.org/abs/2309.12727","description":"<p>Large language models (LLMs) have had a huge impact on society due to their\nimpressive capabilities and vast knowledge of the world. Various applications\nand tools have been created that allow users to interact with these models in a\nblack-box scenario. However, one limitation of this scenario is that users\ncannot modify the internal knowledge of the model, and the only way to add or\nmodify internal knowledge is by explicitly mentioning it to the model during\nthe current interaction. This learning process is called in-context training,\nand it refers to training that is confined to the user's current session or\ncontext. In-context learning has significant applications, but also has\nlimitations that are seldom studied. In this paper, we present a study that\nshows how the model can suffer from interference between information that\ncontinually flows in the context, causing it to forget previously learned\nknowledge, which can reduce the model's performance. Along with showing the\nproblem, we propose an evaluation benchmark based on the bAbI dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coleman_E/0/1/0/all/0/1\">Eric Nuertey Coleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurtado_J/0/1/0/all/0/1\">Julio Hurtado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reduce, Reuse, Recycle: Is Perturbed Data better than Other Language augmentation for Low Resource Self-Supervised Speech Models. (arXiv:2309.12763v1 [eess.AS])","link":"http://arxiv.org/abs/2309.12763","description":"<p>Self-supervised representation learning (SSRL) has improved the performance\non downstream phoneme recognition versus supervised models. Training SSRL\nmodels requires a large amount of pre-training data and this poses a challenge\nfor low resource languages. A common approach is transferring knowledge from\nother languages. Instead, we propose to use audio augmentation to pre-train\nSSRL models in a low resource condition and evaluate phoneme recognition as\ndownstream task. We performed a systematic comparison of augmentation\ntechniques, namely: pitch variation, noise addition, accented target-language\nspeech and other language speech. We found combined augmentations (noise/pitch)\nwas the best augmentation strategy outperforming accent and language knowledge\ntransfer. We compared the performance with various quantities and types of\npre-training data. We examined the scaling factor of augmented data to achieve\nequivalent performance to models pre-trained with target domain speech. Our\nfindings suggest that for resource constrained languages, in-domain synthetic\naugmentation can outperform knowledge transfer from accented or other language\nspeech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ullah_A/0/1/0/all/0/1\">Asad Ullah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ragano_A/0/1/0/all/0/1\">Alessandro Ragano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hines_A/0/1/0/all/0/1\">Andrew Hines</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Furthest Reasoning with Plan Assessment: Stable Reasoning Path with Retrieval-Augmented Large Language Models. (arXiv:2309.12767v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12767","description":"<p>Large Language Models (LLMs), acting as a powerful reasoner and generator,\nexhibit extraordinary performance across various natural language tasks, such\nas question answering (QA). Among these tasks, Multi-Hop Question Answering\n(MHQA) stands as a widely discussed category, necessitating seamless\nintegration between LLMs and the retrieval of external knowledge. Existing\nmethods employ LLM to generate reasoning paths and plans, and utilize IR to\niteratively retrieve related knowledge, but these approaches have inherent\nflaws. On one hand, Information Retriever (IR) is hindered by the low quality\nof generated queries by LLM. On the other hand, LLM is easily misguided by the\nirrelevant knowledge by IR. These inaccuracies, accumulated by the iterative\ninteraction between IR and LLM, lead to a disaster in effectiveness at the end.\nTo overcome above barriers, in this paper, we propose a novel pipeline for MHQA\ncalled Furthest-Reasoning-with-Plan-Assessment (FuRePA), including an improved\nframework (Furthest Reasoning) and an attached module (Plan Assessor). 1)\nFurthest reasoning operates by masking previous reasoning path and generated\nqueries for LLM, encouraging LLM generating chain of thought from scratch in\neach iteration. This approach enables LLM to break the shackle built by\nprevious misleading thoughts and queries (if any). 2) The Plan Assessor is a\ntrained evaluator that selects an appropriate plan from a group of candidate\nplans proposed by LLM. Our methods are evaluated on three highly recognized\npublic multi-hop question answering datasets and outperform state-of-the-art on\nmost metrics (achieving a 10%-12% in answer accuracy).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhiling Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatPRCS: A Personalized Support System for English Reading Comprehension based on ChatGPT. (arXiv:2309.12808v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12808","description":"<p>As a common approach to learning English, reading comprehension primarily\nentails reading articles and answering related questions. However, the\ncomplexity of designing effective exercises results in students encountering\nstandardized questions, making it challenging to align with individualized\nlearners' reading comprehension ability. By leveraging the advanced\ncapabilities offered by large language models, exemplified by ChatGPT, this\npaper presents a novel personalized support system for reading comprehension,\nreferred to as ChatPRCS, based on the Zone of Proximal Development theory.\nChatPRCS employs methods including reading comprehension proficiency\nprediction, question generation, and automatic evaluation, among others, to\nenhance reading comprehension instruction. First, we develop a new algorithm\nthat can predict learners' reading comprehension abilities using their\nhistorical data as the foundation for generating questions at an appropriate\nlevel of difficulty. Second, a series of new ChatGPT prompt patterns is\nproposed to address two key aspects of reading comprehension objectives:\nquestion generation, and automated evaluation. These patterns further improve\nthe quality of generated questions. Finally, by integrating personalized\nability and reading comprehension prompt patterns, ChatPRCS is systematically\nvalidated through experiments. Empirical results demonstrate that it provides\nlearners with high-quality reading comprehension questions that are broadly\naligned with expert-crafted questions at a statistical level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xizhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yihua Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Changqin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaodi Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyloMetrix: An Open-Source Multilingual Tool for Representing Stylometric Vectors. (arXiv:2309.12810v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12810","description":"<p>This work aims to provide an overview on the open-source multilanguage tool\ncalled StyloMetrix. It offers stylometric text representations that cover\nvarious aspects of grammar, syntax and lexicon. StyloMetrix covers four\nlanguages: Polish as the primary language, English, Ukrainian and Russian. The\nnormalized output of each feature can become a fruitful course for machine\nlearning models and a valuable addition to the embeddings layer for any deep\nlearning algorithm. We strive to provide a concise, but exhaustive overview on\nthe application of the StyloMetrix vectors as well as explain the sets of the\ndeveloped linguistic features. The experiments have shown promising results in\nsupervised content classification with simple algorithms as Random Forest\nClassifier, Voting Classifier, Logistic Regression and others. The deep\nlearning assessments have unveiled the usefulness of the StyloMetrix vectors at\nenhancing an embedding layer extracted from Transformer architectures. The\nStyloMetrix has proven itself to be a formidable source for the machine\nlearning and deep learning algorithms to execute different classification\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Okulska_I/0/1/0/all/0/1\">Inez Okulska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stetsenko_D/0/1/0/all/0/1\">Daria Stetsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolos_A/0/1/0/all/0/1\">Anna Ko&#x142;os</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinska_A/0/1/0/all/0/1\">Agnieszka Karli&#x144;ska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glabinska_K/0/1/0/all/0/1\">Kinga G&#x142;&#x105;bi&#x144;ska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowakowski_A/0/1/0/all/0/1\">Adam Nowakowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography. (arXiv:2309.12829v1 [cs.CV])","link":"http://arxiv.org/abs/2309.12829","description":"<p>Accurate segmentation is essential for echocardiography-based assessment of\ncardiovascular diseases (CVDs). However, the variability among sonographers and\nthe inherent challenges of ultrasound images hinder precise segmentation. By\nleveraging the joint representation of image and text modalities,\nVision-Language Segmentation Models (VLSMs) can incorporate rich contextual\ninformation, potentially aiding in accurate and explainable segmentation.\nHowever, the lack of readily available data in echocardiography hampers the\ntraining of VLSMs. In this study, we explore using synthetic datasets from\nSemantic Diffusion Models (SDMs) to enhance VLSMs for echocardiography\nsegmentation. We evaluate results for two popular VLSMs (CLIPSeg and CRIS)\nusing seven different kinds of language prompts derived from several\nattributes, automatically extracted from echocardiography images, segmentation\nmasks, and their metadata. Our results show improved metrics and faster\nconvergence when pretraining VLSMs on SDM-generated synthetic images before\nfinetuning on real images. The code, configs, and prompts are available at\nhttps://github.com/naamiinepal/synthetic-boost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adhikari_R/0/1/0/all/0/1\">Rabin Adhikari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhakal_M/0/1/0/all/0/1\">Manish Dhakal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thapaliya_S/0/1/0/all/0/1\">Safal Thapaliya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poudel_K/0/1/0/all/0/1\">Kanchan Poudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandari_P/0/1/0/all/0/1\">Prasiddha Bhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanal_B/0/1/0/all/0/1\">Bishesh Khanal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation for Arabic Machine Translation: The Case of Financial Texts. (arXiv:2309.12863v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12863","description":"<p>Neural machine translation (NMT) has shown impressive performance when\ntrained on large-scale corpora. However, generic NMT systems have demonstrated\npoor performance on out-of-domain translation. To mitigate this issue, several\ndomain adaptation methods have recently been proposed which often lead to\nbetter translation quality than genetic NMT systems. While there has been some\ncontinuous progress in NMT for English and other European languages, domain\nadaption in Arabic has received little attention in the literature. The current\nstudy, therefore, aims to explore the effectiveness of domain-specific\nadaptation for Arabic MT (AMT), in yet unexplored domain, financial news\narticles. To this end, we developed carefully a parallel corpus for\nArabic-English (AR- EN) translation in the financial domain for benchmarking\ndifferent domain adaptation methods. We then fine-tuned several pre-trained NMT\nand Large Language models including ChatGPT-3.5 Turbo on our dataset. The\nresults showed that the fine-tuning is successful using just a few well-aligned\nin-domain AR-EN segments. The quality of ChatGPT translation was superior than\nother models based on automatic and human evaluations. To the best of our\nknowledge, this is the first work on fine-tuning ChatGPT towards financial\ndomain transfer learning. To contribute to research in domain translation, we\nmade our datasets and fine-tuned models available at\nhttps://huggingface.co/asas-ai/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alghamdi_E/0/1/0/all/0/1\">Emad A. Alghamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakraoui_J/0/1/0/all/0/1\">Jezia Zakraoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abanmy_F/0/1/0/all/0/1\">Fares A. Abanmy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12871","description":"<p>High-quality text embedding is pivotal in improving semantic textual\nsimilarity (STS) tasks, which are crucial components in Large Language Model\n(LLM) applications. However, a common challenge existing text embedding models\nface is the problem of vanishing gradients, primarily due to their reliance on\nthe cosine function in the optimization objective, which has saturation zones.\nTo address this issue, this paper proposes a novel angle-optimized text\nembedding model called AnglE. The core idea of AnglE is to introduce angle\noptimization in a complex space. This novel approach effectively mitigates the\nadverse effects of the saturation zone in the cosine function, which can impede\ngradient and hinder optimization processes. To set up a comprehensive STS\nevaluation, we experimented on existing short-text STS datasets and a newly\ncollected long-text STS dataset from GitHub Issues. Furthermore, we examine\ndomain-specific STS scenarios with limited labeled data and explore how AnglE\nworks with LLM-annotated data. Extensive experiments were conducted on various\ntasks including short-text STS, long-text STS, and domain-specific STS tasks.\nThe results show that AnglE outperforms the state-of-the-art (SOTA) STS models\nthat ignore the cosine saturation zone. These findings demonstrate the ability\nof AnglE to generate high-quality text embeddings and the usefulness of angle\noptimization in STS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affect Recognition in Conversations Using Large Language Models. (arXiv:2309.12881v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12881","description":"<p>Affect recognition, encompassing emotions, moods, and feelings, plays a\npivotal role in human communication. In the realm of conversational artificial\nintelligence (AI), the ability to discern and respond to human affective cues\nis a critical factor for creating engaging and empathetic interactions. This\nstudy delves into the capacity of large language models (LLMs) to recognise\nhuman affect in conversations, with a focus on both open-domain chit-chat\ndialogues and task-oriented dialogues. Leveraging three diverse datasets,\nnamely IEMOCAP, EmoWOZ, and DAIC-WOZ, covering a spectrum of dialogues from\ncasual conversations to clinical interviews, we evaluated and compared LLMs'\nperformance in affect recognition. Our investigation explores the zero-shot and\nfew-shot capabilities of LLMs through in-context learning (ICL) as well as\ntheir model capacities through task-specific fine-tuning. Additionally, this\nstudy takes into account the potential impact of automatic speech recognition\n(ASR) errors on LLM predictions. With this work, we aim to shed light on the\nextent to which LLMs can replicate human-like affect recognition capabilities\nin conversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoEM: A Prototype-Enhanced Matching Framework for Event Relation Extraction. (arXiv:2309.12892v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12892","description":"<p>Event Relation Extraction (ERE) aims to extract multiple kinds of relations\namong events in texts. However, existing methods singly categorize event\nrelations as different classes, which are inadequately capturing the intrinsic\nsemantics of these relations. To comprehensively understand their intrinsic\nsemantics, in this paper, we obtain prototype representations for each type of\nevent relation and propose a Prototype-Enhanced Matching (ProtoEM) framework\nfor the joint extraction of multiple kinds of event relations. Specifically,\nProtoEM extracts event relations in a two-step manner, i.e., prototype\nrepresenting and prototype matching. In the first step, to capture the\nconnotations of different event relations, ProtoEM utilizes examples to\nrepresent the prototypes corresponding to these relations. Subsequently, to\ncapture the interdependence among event relations, it constructs a dependency\ngraph for the prototypes corresponding to these relations and utilized a Graph\nNeural Network (GNN)-based module for modeling. In the second step, it obtains\nthe representations of new event pairs and calculates their similarity with\nthose prototypes obtained in the first step to evaluate which types of event\nrelations they belong to. Experimental results on the MAVEN-ERE dataset\ndemonstrate that the proposed ProtoEM framework can effectively represent the\nprototypes of event relations and further obtain a significant improvement over\nbaseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhilei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Daozhu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Long Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Cheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaolong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Separate Normalization in Self-supervised Transformers. (arXiv:2309.12931v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12931","description":"<p>Self-supervised training methods for transformers have demonstrated\nremarkable performance across various domains. Previous transformer-based\nmodels, such as masked autoencoders (MAE), typically utilize a single\nnormalization layer for both the [CLS] symbol and the tokens. We propose in\nthis paper a simple modification that employs separate normalization layers for\nthe tokens and the [CLS] symbol to better capture their distinct\ncharacteristics and enhance downstream task performance. Our method aims to\nalleviate the potential negative effects of using the same normalization\nstatistics for both token types, which may not be optimally aligned with their\nindividual roles. We empirically show that by utilizing a separate\nnormalization layer, the [CLS] embeddings can better encode the global\ncontextual information and are distributed more uniformly in its anisotropic\nspace. When replacing the conventional normalization layer with the two\nseparate layers, we observe an average 2.7% performance improvement over the\nimage, natural language, and graph domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yinkai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuanqi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassoun_S/0/1/0/all/0/1\">Soha Hassoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li-Ping Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TopRoBERTa: Topology-Aware Authorship Attribution of Deepfake Texts. (arXiv:2309.12934v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12934","description":"<p>Recent advances in Large Language Models (LLMs) have enabled the generation\nof open-ended high-quality texts, that are non-trivial to distinguish from\nhuman-written texts. We refer to such LLM-generated texts as \\emph{deepfake\ntexts}. There are currently over 11K text generation models in the huggingface\nmodel repo. As such, users with malicious intent can easily use these\nopen-sourced LLMs to generate harmful texts and misinformation at scale. To\nmitigate this problem, a computational method to determine if a given text is a\ndeepfake text or not is desired--i.e., Turing Test (TT). In particular, in this\nwork, we investigate the more general version of the problem, known as\n\\emph{Authorship Attribution (AA)}, in a multi-class setting--i.e., not only\ndetermining if a given text is a deepfake text or not but also being able to\npinpoint which LLM is the author. We propose \\textbf{TopRoBERTa} to improve\nexisting AA solutions by capturing more linguistic patterns in deepfake texts\nby including a Topological Data Analysis (TDA) layer in the RoBERTa model. We\nshow the benefits of having a TDA layer when dealing with noisy, imbalanced,\nand heterogeneous datasets, by extracting TDA features from the reshaped\n$pooled\\_output$ of RoBERTa as input. We use RoBERTa to capture contextual\nrepresentations (i.e., semantic and syntactic linguistic features), while using\nTDA to capture the shape and structure of data (i.e., linguistic structures).\nFinally, \\textbf{TopRoBERTa}, outperforms the vanilla RoBERTa in 2/3 datasets,\nachieving up to 7\\% increase in Macro F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uchendu_A/0/1/0/all/0/1\">Adaku Uchendu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thai Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models. (arXiv:2309.12940v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12940","description":"<p>Task-oriented dialogue (TOD) systems facilitate users in executing various\nactivities via multi-turn dialogues, but Large Language Models (LLMs) often\nstruggle to comprehend these intricate contexts. In this study, we propose a\nnovel \"Self-Explanation\" prompting strategy to enhance the comprehension\nabilities of LLMs in multi-turn dialogues. This task-agnostic approach requires\nthe model to analyze each dialogue utterance before task execution, thereby\nimproving performance across various dialogue-centric tasks. Experimental\nresults from six benchmark datasets confirm that our method consistently\noutperforms other zero-shot prompts and matches or exceeds the efficacy of\nfew-shot prompts, demonstrating its potential as a powerful tool in enhancing\nLLMs' comprehension in complex dialogue tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Haoyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hangyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wentao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nested Event Extraction upon Pivot Element Recogniton. (arXiv:2309.12960v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12960","description":"<p>Nested Event Extraction (NEE) aims to extract complex event structures where\nan event contains other events as its arguments recursively. Nested events\ninvolve a kind of Pivot Elements (PEs) that simultaneously act as arguments of\nouter events and as triggers of inner events, and thus connect them into nested\nstructures. This special characteristic of PEs brings challenges to existing\nNEE methods, as they cannot well cope with the dual identities of PEs.\nTherefore, this paper proposes a new model, called PerNee, which extracts\nnested events mainly based on recognizing PEs. Specifically, PerNee first\nrecognizes the triggers of both inner and outer events and further recognizes\nthe PEs via classifying the relation type between trigger pairs. In order to\nobtain better representations of triggers and arguments to further improve NEE\nperformance, it incorporates the information of both event types and argument\nroles into PerNee through prompt learning. Since existing NEE datasets (e.g.,\nGenia11) are limited to specific domains and contain a narrow range of event\ntypes with nested structures, we systematically categorize nested events in\ngeneric domain and construct a new NEE dataset, namely ACE2005-Nest.\nExperimental results demonstrate that PerNee consistently achieves\nstate-of-the-art performance on ACE2005-Nest, Genia11 and Genia13.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Weicheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaolong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Long Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_M/0/1/0/all/0/1\">Miao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yantao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Saiping Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wordification: A New Way of Teaching English Spelling Patterns. (arXiv:2309.12981v1 [cs.OH])","link":"http://arxiv.org/abs/2309.12981","description":"<p>Literacy, or the ability to read and write, is a crucial indicator of success\nin life and greater society. It is estimated that 85% of people in juvenile\ndelinquent systems cannot adequately read or write, that more than half of\nthose with substance abuse issues have complications in reading or writing and\nthat two-thirds of those who do not complete high school lack proper literacy\nskills. Furthermore, young children who do not possess reading skills matching\ngrade level by the fourth grade are approximately 80% likely to not catch up at\nall. Many may believe that in a developed country such as the United States,\nliteracy fails to be an issue; however, this is a dangerous misunderstanding.\nGlobally an estimated 1.19 trillion dollars are lost every year due to issues\nin literacy; in the USA, the loss is an estimated 300 billion. To put it in\nmore shocking terms, one in five American adults still fail to comprehend basic\nsentences. Making matters worse, the only tools available now to correct a lack\nof reading and writing ability are found in expensive tutoring or other\nprograms that oftentimes fail to be able to reach the required audience. In\nthis paper, our team puts forward a new way of teaching English spelling and\nword recognitions to grade school students in the United States: Wordification.\nWordification is a web application designed to teach English literacy using\nprinciples of linguistics applied to the orthographic and phonological\nproperties of words in a manner not fully utilized previously in any\ncomputer-based teaching application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whalen_L/0/1/0/all/0/1\">Lexington Whalen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bickel_N/0/1/0/all/0/1\">Nathan Bickel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comandur_S/0/1/0/all/0/1\">Shash Comandur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Craven_D/0/1/0/all/0/1\">Dalton Craven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubinsky_S/0/1/0/all/0/1\">Stanley Dubinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valafar_H/0/1/0/all/0/1\">Homayoun Valafar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audience-specific Explanations for Machine Translation. (arXiv:2309.12998v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12998","description":"<p>In machine translation, a common problem is that the translation of certain\nwords even if translated can cause incomprehension of the target language\naudience due to different cultural backgrounds. A solution to solve this\nproblem is to add explanations for these words. In a first step, we therefore\nneed to identify these words or phrases. In this work we explore techniques to\nextract example explanations from a parallel corpus. However, the sparsity of\nsentences containing words that need to be explained makes building the\ntraining dataset extremely difficult. In this work, we propose a semi-automatic\ntechnique to extract these explanations from a large parallel corpus.\nExperiments on English-&gt;German language pair show that our method is able to\nextract sentence so that more than 10% of the sentences contain explanation,\nwhile only 1.9% of the original sentences contain explanations. In addition,\nexperiments on English-&gt;French and English-&gt;Chinese language pairs also show\nsimilar conclusions. This is therefore an essential first automatic step to\ncreate a explanation dataset. Furthermore we show that the technique is robust\nfor all three language pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1\">Renhan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs. (arXiv:2309.13007v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13007","description":"<p>Large Language Models (LLMs) still struggle with complex reasoning tasks.\nMotivated by the society of minds (Minsky, 1988), we propose ReConcile, a\nmulti-model multi-agent framework designed as a round table conference among\ndiverse LLM agents to foster diverse thoughts and discussion for improved\nconsensus. ReConcile enhances the reasoning capabilities of LLMs by holding\nmultiple rounds of discussion, learning to convince other agents to improve\ntheir answers, and employing a confidence-weighted voting mechanism. In each\nround, ReConcile initiates discussion between agents via a 'discussion prompt'\nthat consists of (a) grouped answers and explanations generated by each agent\nin the previous round, (b) their uncertainties, and (c) demonstrations of\nanswer-rectifying human explanations, used for convincing other agents. This\ndiscussion prompt enables each agent to revise their responses in light of\ninsights from other agents. Once a consensus is reached and the discussion\nends, ReConcile determines the final answer by leveraging the confidence of\neach agent in a weighted voting scheme. We implement ReConcile with ChatGPT,\nBard, and Claude2 as the three agents. Our experimental results on various\nbenchmarks demonstrate that ReConcile significantly enhances the reasoning\nperformance of the agents (both individually and as a team), surpassing prior\nsingle-agent and multi-agent baselines by 7.7% and also outperforming GPT-4 on\nsome of these datasets. We also experiment with GPT-4 itself as one of the\nagents in ReConcile and demonstrate that its initial performance also improves\nby absolute 10.0% through discussion and feedback from other agents. Finally,\nwe also analyze the accuracy after every round and observe that ReConcile\nachieves better and faster consensus between agents, compared to a multi-agent\ndebate baseline. Our code is available at: https://github.com/dinobby/ReConcile\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Justin Chih-Yao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Swarnadeep Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model. (arXiv:2309.13018v1 [eess.AS])","link":"http://arxiv.org/abs/2309.13018","description":"<p>Neural network pruning offers an effective method for compressing a\nmultilingual automatic speech recognition (ASR) model with minimal performance\nloss. However, it entails several rounds of pruning and re-training needed to\nbe run for each language. In this work, we propose the use of an adaptive\nmasking approach in two scenarios for pruning a multilingual ASR model\nefficiently, each resulting in sparse monolingual models or a sparse\nmultilingual model (named as Dynamic ASR Pathways). Our approach dynamically\nadapts the sub-network, avoiding premature decisions about a fixed sub-network\nstructure. We show that our approach outperforms existing pruning methods when\ntargeting sparse monolingual models. Further, we illustrate that Dynamic ASR\nPathways jointly discovers and trains better sub-networks (pathways) of a\nsingle multilingual model by adapting from different sub-network\ninitializations, thereby reducing the need for language-specific pruning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xie_J/0/1/0/all/0/1\">Jiamin Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1\">Jinxi Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tjandra_A/0/1/0/all/0/1\">Andros Tjandra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shangguan_Y/0/1/0/all/0/1\">Yuan Shangguan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sari_L/0/1/0/all/0/1\">Leda Sari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chunyang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_J/0/1/0/all/0/1\">Junteng Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahadeokar_J/0/1/0/all/0/1\">Jay Mahadeokar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Automated Clinical Language Simplification: Dataset, Algorithm, and Evaluation. (arXiv:2012.02420v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.02420","description":"<p>Patients with low health literacy usually have difficulty understanding\nmedical jargon and the complex structure of professional medical language.\nAlthough some studies are proposed to automatically translate expert language\ninto layperson-understandable language, only a few of them focus on both\naccuracy and readability aspects simultaneously in the clinical domain. Thus,\nsimplification of the clinical language is still a challenging task, but\nunfortunately, it is not yet fully addressed in previous work. To benchmark\nthis task, we construct a new dataset named MedLane to support the development\nand evaluation of automated clinical language simplification approaches.\nBesides, we propose a new model called DECLARE that follows the human\nannotation procedure and achieves state-of-the-art performance compared with\neight strong baselines. To fairly evaluate the performance, we also propose\nthree specific evaluation metrics. Experimental results demonstrate the utility\nof the annotated MedLane dataset and the effectiveness of the proposed model\nDECLARE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Junyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zifei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hanzhong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Muchao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1\">Quanzeng You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Cao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fenglong Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smoothing Entailment Graphs with Language Models. (arXiv:2208.00318v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.00318","description":"<p>The diversity and Zipfian frequency distribution of natural language\npredicates in corpora leads to sparsity in Entailment Graphs (EGs) built by\nOpen Relation Extraction (ORE). EGs are computationally efficient and\nexplainable models of natural language inference, but as symbolic models, they\nfail if a novel premise or hypothesis vertex is missing at test-time. We\npresent theory and methodology for overcoming such sparsity in symbolic models.\nFirst, we introduce a theory of optimal smoothing of EGs by constructing\ntransitive chains. We then demonstrate an efficient, open-domain, and\nunsupervised smoothing method using an off-the-shelf Language Model to find\napproximations of missing premise predicates. This improves recall by 25.1 and\n16.3 percentage points on two difficult directional entailment datasets, while\nraising average precision and maintaining model explainability. Further, in a\nQA task we show that EG smoothing is most useful for answering questions with\nlesser supporting text, where missing premise predicates are more costly.\nFinally, controlled experiments with WordNet confirm our theory and show that\nhypothesis smoothing is difficult, but possible in principle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McKenna_N/0/1/0/all/0/1\">Nick McKenna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Mark Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of Hybrid ASR Systems for Low Resource Medical Domain Conversational Telephone Speech. (arXiv:2210.13397v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13397","description":"<p>Language barriers present a great challenge in our increasingly connected and\nglobal world. Especially within the medical domain, e.g. hospital or emergency\nroom, communication difficulties and delays may lead to malpractice and\nnon-optimal patient care. In the HYKIST project, we consider patient-physician\ncommunication, more specifically between a German-speaking physician and an\nArabic- or Vietnamese-speaking patient. Currently, a doctor can call the\nTriaphon service to get assistance from an interpreter in order to help\nfacilitate communication. The HYKIST goal is to support the usually\nnon-professional bilingual interpreter with an automatic speech translation\nsystem to improve patient care and help overcome language barriers. In this\nwork, we present our ASR system development efforts for this conversational\ntelephone speech translation task in the medical domain for two languages\npairs, data collection, various acoustic model architectures and\ndialect-induced difficulties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luscher_C/0/1/0/all/0/1\">Christoph L&#xfc;scher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zijian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raissi_T/0/1/0/all/0/1\">Tina Raissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieting_P/0/1/0/all/0/1\">Peter Vieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Duc_K/0/1/0/all/0/1\">Khai Le-Duc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lessons learned from the evaluation of Spanish Language Models. (arXiv:2212.08390v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08390","description":"<p>Given the impact of language models on the field of Natural Language\nProcessing, a number of Spanish encoder-only masked language models (aka BERTs)\nhave been trained and released. These models were developed either within large\nprojects using very large private corpora or by means of smaller scale academic\nefforts leveraging freely available data. In this paper we present a\ncomprehensive head-to-head comparison of language models for Spanish with the\nfollowing results: (i) Previously ignored multilingual models from large\ncompanies fare better than monolingual models, substantially changing the\nevaluation landscape of language models in Spanish; (ii) Results across the\nmonolingual models are not conclusive, with supposedly smaller and inferior\nmodels performing competitively. Based on these empirical results, we argue for\nthe need of more research to understand the factors underlying them. In this\nsense, the effect of corpus size, quality and pre-training techniques need to\nbe further investigated to be able to obtain Spanish monolingual models\nsignificantly better than the multilingual ones released by large private\ncompanies, specially in the face of rapid ongoing progress in the field. The\nrecent activity in the development of language technology for Spanish is to be\nwelcomed, but our results show that building language models remains an open,\nresource-heavy problem which requires to marry resources (monetary and/or\ncomputational) with the best research expertise and practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversation Style Transfer using Few-Shot Learning. (arXiv:2302.08362v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08362","description":"<p>Conventional text style transfer approaches focus on sentence-level style\ntransfer without considering contextual information, and the style is described\nwith attributes (e.g., formality). When applying style transfer in\nconversations such as task-oriented dialogues, existing approaches suffer from\nthese limitations as context can play an important role and the style\nattributes are often difficult to define in conversations. In this paper, we\nintroduce conversation style transfer as a few-shot learning problem, where the\nmodel learns to perform style transfer by observing only a few example\ndialogues in the target style. We propose a novel in-context learning approach\nto solve the task with style-free dialogues as a pivot. Human evaluation shows\nthat by incorporating multi-turn context, the model is able to match the target\nstyle while having better appropriateness and semantic correctness compared to\nutterance/sentence-level style transfer. Additionally, we show that\nconversation style transfer can also benefit downstream tasks. For example, in\nmulti-domain intent classification tasks, the F1 scores improve after\ntransferring the style of training data to match the style of the test data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shamik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_R/0/1/0/all/0/1\">Raphael Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1\">Nikolaos Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1\">Elman Mansimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes a Language Easy to Deep-Learn?. (arXiv:2302.12239v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12239","description":"<p>Neural networks drive the success of natural language processing. A\nfundamental property of language is its compositional structure, allowing\nhumans to produce forms for new meanings systematically. However, unlike\nhumans, neural networks notoriously struggle with systematic generalization,\nand do not necessarily benefit from compositional structure in emergent\ncommunication simulations. This poses a problem for using neural networks to\nsimulate human language learning and evolution, and suggests crucial\ndifferences in the biases of the different learning systems. Here, we directly\ntest how neural networks compare to humans in learning and generalizing\ndifferent input languages that vary in their degree of structure. We evaluate\nthe memorization and generalization capabilities of a pre-trained language\nmodel GPT-3.5 (analagous to an adult second language learner) and recurrent\nneural networks trained from scratch (analaogous to a child first language\nlearner). Our results show striking similarities between deep neural networks\nand adult human learners, with more structured linguistic input leading to more\nsystematic generalization and to better convergence between neural networks and\nhumans. These findings suggest that all the learning systems are sensitive to\nthe structure of languages in similar ways with compositionality being\nadvantageous for learning. Our findings draw a clear prediction regarding\nchildren's learning biases, as well as highlight the challenges of automated\nprocessing of languages spoken by small communities. Notably, the similarity\nbetween humans and machines opens new avenues for research on language learning\nand evolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_Y/0/1/0/all/0/1\">Yoav Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviv_L/0/1/0/all/0/1\">Limor Raviv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks. (arXiv:2304.14732v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14732","description":"<p>Making the contents generated by Large Language Model (LLM) such as ChatGPT,\naccurate, credible and traceable is crucial, especially in complex\nknowledge-intensive tasks that require multi-step reasoning and each of which\nneeds knowledge to solve. Introducing Information Retrieval (IR) to provide LLM\nwith external knowledge is good potential to solve this problem. However, where\nand how to introduce IR into LLM is a big challenge. Previous work has the\ndisadvantage that the wrong knowledge retrieved by IR misleads the LLM or\nbreaks the reasoning chain of LLM. In this paper, we propose a novel framework\ncalled Search-in-the-Chain (SearChain) for the interaction between LLM and IR\nto solve the challenges. First, LLM generates the global reasoning chain called\nChain-of-Query (CoQ) where each node consists of an IR-oriented query and the\nanswer to the query. Second, IR verifies the answer of each node of CoQ, it\ncorrects the answer that is not consistent with the retrieved information when\nIR gives high confidence, which improves the credibility. Third, LLM can mark\nits missing knowledge in CoQ and IR can provide this knowledge to LLM. These\nthree operations improve the accuracy of LLM for complex knowledge-intensive\ntasks in terms of reasoning ability and knowledge. Finally, SearChain generates\nthe reasoning process and marks references to supporting documents for each\nreasoning step, which improves traceability. SearChain transforms the topology\nof reasoning from chain to tree, which can modify the reasoning direction.\nExperiment shows that SearChain outperforms baselines on complex\nknowledge-intensive tasks including multi-hop question-answering, slot filling,\nfact checking, and long-form question-answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shicheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v5 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2305.00969","description":"<p>This paper describes the Ubenwa CryCeleb dataset - a labeled collection of\ninfant cries - and the accompanying CryCeleb 2023 task, which is a public\nspeaker verification challenge based on cry sounds. We released more than 6\nhours of manually segmented cry sounds from 786 newborns for academic use,\naiming to encourage research in infant cry analysis. The inaugural public\ncompetition attracted 59 participants, 11 of whom improved the baseline\nperformance. The top-performing system achieved a significant improvement\nscoring 25.8% equal error rate, which is still far from the performance of\nstate-of-the-art adult speaker verification systems. Therefore, we believe\nthere is room for further research on this dataset, potentially extending\nbeyond the verification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Budaghyan_D/0/1/0/all/0/1\">David Budaghyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onu_C/0/1/0/all/0/1\">Charles C. Onu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorin_A/0/1/0/all/0/1\">Arsenii Gorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subakan_C/0/1/0/all/0/1\">Cem Subakan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner. (arXiv:2305.01711v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01711","description":"<p>Language models (LMs) trained on vast quantities of unlabelled data have\ngreatly advanced the field of natural language processing (NLP). In this study,\nwe re-visit the widely accepted notion in NLP that continued pre-training LMs\non task-related texts improves the performance of fine-tuning (FT) in\ndownstream tasks. Through experiments on eight single-sentence tasks and eight\nsentence-pair tasks in both semi-supervised and fully-supervised settings, we\nfind that conventional continued pre-training does not consistently provide\nbenefits and can even be detrimental for sentence-pair tasks or when\nprompt-based FT is used. To tackle these issues, we propose Prompt-based\nContinued Pre-training (PCP), which combines the idea of instruction tuning\nwith conventional continued pre-training. Our approach aims to improve the\nperformance of prompt-based FT by presenting both task-related texts and prompt\ntemplates to LMs through unsupervised pre-training objectives before\nfine-tuning for the target task. Our empirical evaluations on 21 benchmarks\ndemonstrate that the PCP consistently improves the performance of\nstate-of-the-art prompt-based FT approaches (up to 20.1% absolute) in both\nsemi-supervised and fully-supervised settings, even with only hundreds of\nunlabelled examples. Additionally, prompt-based FT with the PCP outperforms\nstate-of-the-art semi-supervised approaches with greater simplicity,\neliminating the need for an iterative process and extra data augmentation. Our\nfurther analysis explores the performance lower bound of the PCP and reveals\nthat the advantages of PCP persist across different sizes of models and\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhengxiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1\">Aldo Lipani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v5 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2305.06569","description":"<p>Recommendation foundation model utilizes large language models (LLM) for\nrecommendation by converting recommendation tasks into natural language tasks.\nIt enables generative recommendation which directly generates the item(s) to\nrecommend rather than calculating a ranking score for each and every candidate\nitem in traditional recommendation models, simplifying the recommendation\npipeline from multi-stage filtering to single-stage filtering. To avoid\ngenerating excessively long text and hallucinated recommendation when deciding\nwhich item(s) to recommend, creating LLM-compatible item IDs to uniquely\nidentify each item is essential for recommendation foundation models. In this\nstudy, we systematically examine the item indexing problem for recommendation\nfoundation models, using P5 as an example of backbone model. To emphasize the\nimportance of item indexing, we first discuss the issues of several trivial\nitem indexing methods, such as independent indexing, title indexing, and random\nindexing. We then propose four simple yet effective solutions, including\nsequential indexing, collaborative indexing, semantic (content-based) indexing,\nand hybrid indexing. Our study highlights the significant influence of item\nindexing methods on the performance of LLM-based recommendation, and our\nresults on real-world datasets validate the effectiveness of our proposed\nsolutions. The research also demonstrates how recent advances on language\nmodeling and traditional IR principles such as indexing can help each other for\nbetter learning and inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy. (arXiv:2305.10307v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10307","description":"<p>Measuring the distance between machine-produced and human language is a\ncritical open problem. Inspired by empirical findings from psycholinguistics on\nthe periodicity of entropy in language, we propose FACE, a set of metrics based\non Fourier Analysis of the estimated Cross-Entropy of language, for measuring\nthe similarity between model-generated and human-written languages. Based on an\nopen-ended generation task and the experimental data from previous studies, we\nfind that FACE can effectively identify the human-model gap, scales with model\nsize, reflects the outcomes of different sampling methods for decoding,\ncorrelates well with other evaluation metrics and with human judgment scores.\nFACE is computationally efficient and provides intuitive interpretations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zuhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yingfang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_S/0/1/0/all/0/1\">Shuo Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Huajun Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kefan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GENTLE: A Genre-Diverse Multilayer Challenge Set for English NLP and Linguistic Evaluation. (arXiv:2306.01966v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01966","description":"<p>We present GENTLE, a new mixed-genre English challenge corpus totaling 17K\ntokens and consisting of 8 unusual text types for out-of domain evaluation:\ndictionary entries, esports commentaries, legal documents, medical notes,\npoetry, mathematical proofs, syllabuses, and threat letters. GENTLE is manually\nannotated for a variety of popular NLP tasks, including syntactic dependency\nparsing, entity recognition, coreference resolution, and discourse parsing. We\nevaluate state-of-the-art NLP systems on GENTLE and find severe degradation for\nat least some genres in their performance on all tasks, which indicates\nGENTLE's utility as an evaluation dataset for NLP systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aoyama_T/0/1/0/all/0/1\">Tatsuya Aoyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behzad_S/0/1/0/all/0/1\">Shabnam Behzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gessler_L/0/1/0/all/0/1\">Luke Gessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_L/0/1/0/all/0/1\">Lauren Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jessica Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Janet Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Siyao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yilun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.03341","description":"<p>We introduce Inference-Time Intervention (ITI), a technique designed to\nenhance the truthfulness of large language models (LLMs). ITI operates by\nshifting model activations during inference, following a set of directions\nacross a limited number of attention heads. This intervention significantly\nimproves the performance of LLaMA models on the TruthfulQA benchmark. On an\ninstruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from\n32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and\ndemonstrate how to balance it by tuning the intervention strength. ITI is\nminimally invasive and computationally inexpensive. Moreover, the technique is\ndata efficient: while approaches like RLHF require extensive annotations, ITI\nlocates truthful directions using only few hundred examples. Our findings\nsuggest that LLMs may have an internal representation of the likelihood of\nsomething being true, even as they produce falsehoods on the surface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kenneth Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_O/0/1/0/all/0/1\">Oam Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viegas_F/0/1/0/all/0/1\">Fernanda Vi&#xe9;gas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1\">Martin Wattenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AVIS: Autonomous Visual Information Seeking with Large Language Model Agent. (arXiv:2306.08129v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2306.08129","description":"<p>In this paper, we propose an autonomous information seeking visual question\nanswering framework, AVIS. Our method leverages a Large Language Model (LLM) to\ndynamically strategize the utilization of external tools and to investigate\ntheir outputs, thereby acquiring the indispensable knowledge needed to provide\nanswers to the posed questions. Responding to visual questions that necessitate\nexternal knowledge, such as \"What event is commemorated by the building\ndepicted in this image?\", is a complex task. This task presents a combinatorial\nsearch space that demands a sequence of actions, including invoking APIs,\nanalyzing their responses, and making informed decisions. We conduct a user\nstudy to collect a variety of instances of human decision-making when faced\nwith this task. This data is then used to design a system comprised of three\ncomponents: an LLM-powered planner that dynamically determines which tool to\nuse next, an LLM-powered reasoner that analyzes and extracts key information\nfrom the tool outputs, and a working memory component that retains the acquired\ninformation throughout the process. The collected user behavior serves as a\nguide for our system in two key ways. First, we create a transition graph by\nanalyzing the sequence of decisions made by users. This graph delineates\ndistinct states and confines the set of actions available at each state.\nSecond, we use examples of user decision-making to provide our LLM-powered\nplanner and reasoner with relevant contextual instances, enhancing their\ncapacity to make informed decisions. We show that AVIS achieves\nstate-of-the-art results on knowledge-intensive visual question answering\nbenchmarks such as Infoseek and OK-VQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Ziniu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iscen_A/0/1/0/all/0/1\">Ahmet Iscen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1\">David A Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathi_A/0/1/0/all/0/1\">Alireza Fathi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personality Traits in Large Language Models. (arXiv:2307.00184v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.00184","description":"<p>The advent of large language models (LLMs) has revolutionized natural\nlanguage processing, enabling the generation of coherent and contextually\nrelevant human-like text. As LLMs increasingly power conversational agents used\nby the general public world-wide, the synthetic personality embedded in these\nmodels, by virtue of training on large amounts of human data, is becoming\nincreasingly important. Since personality is a key factor determining the\neffectiveness of communication, we present a comprehensive method for\nadministering and validating personality tests on widely-used LLMs, as well as\nfor shaping personality in the generated text of such LLMs. Applying this\nmethod, we found: 1) personality measurements in the outputs of some LLMs under\nspecific prompting configurations are reliable and valid; 2) evidence of\nreliability and validity of synthetic LLM personality is stronger for larger\nand instruction fine-tuned models; and 3) personality in LLM outputs can be\nshaped along desired dimensions to mimic specific human personality profiles.\nWe discuss application and ethical implications of the measurement and shaping\nmethod, in particular regarding responsible AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Serapio_Garcia_G/0/1/0/all/0/1\">Greg Serapio-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safdari_M/0/1/0/all/0/1\">Mustafa Safdari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crepy_C/0/1/0/all/0/1\">Cl&#xe9;ment Crepy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Luning Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fitz_S/0/1/0/all/0/1\">Stephen Fitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_P/0/1/0/all/0/1\">Peter Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulhai_M/0/1/0/all/0/1\">Marwa Abdulhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1\">Aleksandra Faust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mataric_M/0/1/0/all/0/1\">Maja Matari&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions. (arXiv:2307.03941v3 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2307.03941","description":"<p>The Right to be Forgotten (RTBF) was first established as the result of the\nruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\\'alez, and\nwas later included as the Right to Erasure under the General Data Protection\nRegulation (GDPR) of European Union to allow individuals the right to request\npersonal data be deleted by organizations. Specifically for search engines,\nindividuals can send requests to organizations to exclude their information\nfrom the query results. It was a significant emergent right as the result of\nthe evolution of technology. With the recent development of Large Language\nModels (LLMs) and their use in chatbots, LLM-enabled software systems have\nbecome popular. But they are not excluded from the RTBF. Compared with the\nindexing approach used by search engines, LLMs store, and process information\nin a completely different way. This poses new challenges for compliance with\nthe RTBF. In this paper, we explore these challenges and provide our insights\non how to implement technical solutions for the RTBF, including the use of\ndifferential privacy, machine unlearning, model editing, and prompt\nengineering. With the rapid advancement of AI and the increasing need of\nregulating this powerful technology, learning from the case of RTBF can provide\nvaluable lessons for technical practitioners, legal experts, organizations, and\nauthorities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dawen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finckenberg_Broman_P/0/1/0/all/0/1\">Pamela Finckenberg-Broman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Thong Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shidong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhenchang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staples_M/0/1/0/all/0/1\">Mark Staples</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiwei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2308.07706","description":"<p>Medical image segmentation with deep learning is an important and widely\nstudied topic because segmentation enables quantifying target structure size\nand shape that can help in disease diagnosis, prognosis, surgery planning, and\nunderstanding. Recent advances in the foundation VLMs and their adaptation to\nsegmentation tasks in natural images with VLSMs have opened up a unique\nopportunity to build potentially powerful segmentation models for medical\nimages that enable providing helpful information via language prompt as input,\nleverage the extensive range of other medical imaging datasets by pooled\ndataset training, adapt to new classes, and be robust against\nout-of-distribution data with human-in-the-loop prompting during inference.\nAlthough transfer learning from natural to medical images for image-only\nsegmentation models has been studied, no studies have analyzed how the joint\nrepresentation of vision-language transfers to medical images in segmentation\nproblems and understand gaps in leveraging their full potential. We present the\nfirst benchmark study on transfer learning of VLSMs to 2D medical images with\nthoughtfully collected 11 existing 2D medical image datasets of diverse\nmodalities with carefully presented 9 types of language prompts from 14\nattributes. Our results indicate that VLSMs trained in natural image-text pairs\ntransfer reasonably to the medical domain in zero-shot settings when prompted\nappropriately for non-radiology photographic modalities; when finetuned, they\nobtain comparable performance to conventional architectures, even in X-rays and\nultrasound modalities. However, the additional benefit of language prompts\nduring finetuning may be limited, with image features playing a more dominant\nrole; they can better handle training on pooled datasets combining diverse\nmodalities and are potentially more robust to domain shift than the\nconventional segmentation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poudel_K/0/1/0/all/0/1\">Kanchan Poudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhakal_M/0/1/0/all/0/1\">Manish Dhakal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandari_P/0/1/0/all/0/1\">Prasiddha Bhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adhikari_R/0/1/0/all/0/1\">Rabin Adhikari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thapaliya_S/0/1/0/all/0/1\">Safal Thapaliya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanal_B/0/1/0/all/0/1\">Bishesh Khanal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Biasing of Named-Entities with Large Language Models. (arXiv:2309.00723v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.00723","description":"<p>This paper studies contextual biasing with Large Language Models (LLMs),\nwhere during second-pass rescoring additional contextual information is\nprovided to a LLM to boost Automatic Speech Recognition (ASR) performance. We\npropose to leverage prompts for a LLM without fine tuning during rescoring\nwhich incorporate a biasing list and few-shot examples to serve as additional\ninformation when calculating the score for the hypothesis. In addition to\nfew-shot prompt learning, we propose multi-task training of the LLM to predict\nboth the entity class and the next token. To improve the efficiency for\ncontextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we\npropose dynamic prompting, where we select the most likely class using the\nclass tag prediction, and only use entities in this class as contexts for next\ntoken prediction. Word Error Rate (WER) evaluation is performed on i) an\ninternal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli\ndataset. Results indicate that biasing lists and few-shot examples can achieve\n17.8% and 9.6% relative improvement compared to first pass ASR, and that\nmulti-task training and dynamic prompting can achieve 20.0% and 11.3% relative\nWER improvement, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chuanneng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1\">Zeeshan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yingyi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabela_L/0/1/0/all/0/1\">Lucas Kabela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yutong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3). (arXiv:2309.08636v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.08636","description":"<p>Historically, proficient writing was deemed essential for human advancement,\nwith creative expression viewed as one of the hallmarks of human achievement.\nHowever, recent advances in generative AI have marked an inflection point in\nthis narrative, including for scientific writing. This article provides a\ncomprehensive analysis of the capabilities and limitations of six AI chatbots\nin scholarly writing in the humanities and archaeology. The methodology was\nbased on tagging AI generated content for quantitative accuracy and qualitative\nprecision by human experts. Quantitative accuracy assessed the factual\ncorrectness, while qualitative precision gauged the scientific contribution.\nWhile the AI chatbots, especially ChatGPT-4, demonstrated proficiency in\nrecombining existing knowledge, they failed in generating original scientific\ncontent. As a side note, our results also suggest that with ChatGPT-4 the size\nof the LLMs has plateaued. Furthermore, the paper underscores the intricate and\nrecursive nature of human research. This process of transforming raw data into\nrefined knowledge is computationally irreducible, which highlights the\nchallenges AI chatbots face in emulating human originality in scientific\nwriting. In conclusion, while large language models have revolutionised content\ngeneration, their ability to produce original scientific contributions in the\nhumanities remains limited. We expect that this will change in the near future\nwith the evolution of current LLM-based AI chatbots towards LLM-powered\nsoftware.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lozic_E/0/1/0/all/0/1\">Edisa Lozi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stular_B/0/1/0/all/0/1\">Benjamin &#x160;tular</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.09357","description":"<p>Despite the plethora of telehealth applications to assist home-based older\nadults and healthcare providers, basic messaging and phone calls are still the\nmost common communication methods, which suffer from limited availability,\ninformation loss, and process inefficiencies. One promising solution to\nfacilitate patient-provider communication is to leverage large language models\n(LLMs) with their powerful natural conversation and summarization capability.\nHowever, there is a limited understanding of LLMs' role during the\ncommunication. We first conducted two interview studies with both older adults\n(N=10) and healthcare providers (N=9) to understand their needs and\nopportunities for LLMs in patient-provider asynchronous communication. Based on\nthe insights, we built an LLM-powered communication system, Talk2Care, and\ndesigned interactive components for both groups: (1) For older adults, we\nleveraged the convenience and accessibility of voice assistants (VAs) and built\nan LLM-powered VA interface for effective information collection. (2) For\nhealth providers, we built an LLM-based dashboard to summarize and present\nimportant health information based on older adults' conversations with the VA.\nWe further conducted two user studies with older adults and providers to\nevaluate the usability of the system. The results showed that Talk2Care could\nfacilitate the communication process, enrich the health information collected\nfrom older adults, and considerably save providers' efforts and time. We\nenvision our work as an initial exploration of LLMs' capability in the\nintersection of healthcare and interpersonal communication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuhai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_E/0/1/0/all/0/1\">Ethan Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Intille_S/0/1/0/all/0/1\">Stephen Intille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shara_N/0/1/0/all/0/1\">Nawar Shara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guodong Gordon Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CFGPT: Chinese Financial Assistant with Large Language Model. (arXiv:2309.10654v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.10654","description":"<p>Large language models (LLMs) have demonstrated great potential in natural\nlanguage processing tasks within the financial domain. In this work, we present\na Chinese Financial Generative Pre-trained Transformer framework, named CFGPT,\nwhich includes a dataset~(CFData) for pre-training and supervised fine-tuning,\na financial LLM~(CFLLM) to adeptly manage financial texts, and a deployment\nframework~(CFAPP) designed to navigate real-world financial applications. The\nCFData comprising both a pre-training dataset and a supervised fine-tuning\ndataset, where the pre-training dataset collates Chinese financial data and\nanalytics, alongside a smaller subset of general-purpose text with 584M\ndocuments and 141B tokens in total, and the supervised fine-tuning dataset is\ntailored for six distinct financial tasks, embodying various facets of\nfinancial analysis and decision-making with 1.5M instruction pairs and 1.5B\ntokens in total. The CFLLM, which is based on InternLM-7B to balance the model\ncapability and size, is trained on CFData in two stage, continued pre-training\nand supervised fine-tuning. The CFAPP is centered on large language models\n(LLMs) and augmented with additional modules to ensure multifaceted\nfunctionality in real-world application. Our codes are released at\nhttps://github.com/TongjiFinLab/CFGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangtong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1\">Yuxuan Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Dawei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhijun Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Changjun Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio Contrastive based Fine-tuning. (arXiv:2309.11895v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2309.11895","description":"<p>Audio classification plays a crucial role in speech and sound processing\ntasks with a wide range of applications. There still remains a challenge of\nstriking the right balance between fitting the model to the training data\n(avoiding overfitting) and enabling it to generalise well to a new domain.\nLeveraging the transferability of contrastive learning, we introduce Audio\nContrastive-based Fine-tuning (AudioConFit), an efficient approach\ncharacterised by robust generalisability. Empirical experiments on a variety of\naudio classification tasks demonstrate the effectiveness and robustness of our\napproach, which achieves state-of-the-art results in various settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Q/0/1/0/all/0/1\">Qibin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chenghao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yizhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1\">Noura Al Moubayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework. (arXiv:2309.11911v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.11911","description":"<p>The development of emotion recognition in dialogue (ERC) has been\nconsistently hindered by the complexity of pipeline designs, leading to ERC\nmodels that often overfit to specific datasets and dialogue patterns. In this\nstudy, we propose a novel approach, namely\n</p>\n<p>InstructERC, to reformulates the ERC task from a discriminative framework to\na generative framework based on Large Language Models (LLMs) . InstructERC has\ntwo significant contributions: Firstly, InstructERC introduces a simple yet\neffective retrieval template module, which helps the model explicitly integrate\nmulti-granularity dialogue supervision information by concatenating the\nhistorical dialog content, label statement, and emotional domain demonstrations\nwith high semantic similarity. Furthermore, we introduce two additional emotion\nalignment tasks, namely speaker identification and emotion prediction tasks, to\nimplicitly model the dialogue role relationships and future emotional\ntendencies in conversations. Our LLM-based plug-and-play plugin framework\nsignificantly outperforms all previous models and achieves comprehensive SOTA\non three commonly used ERC datasets. Extensive analysis of parameter-efficient\nand data-scaling experiments provide empirical guidance for applying\nInstructERC in practical scenarios. Our code will be released after blind\nreview.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shanglin Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guanting Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics. (arXiv:2309.11981v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.11981","description":"<p>In the burgeoning field of artificial intelligence (AI), the unprecedented\nprogress of large language models (LLMs) in natural language processing (NLP)\noffers an opportunity to revisit the entire approach of traditional metrics of\nmachine intelligence, both in form and content. As the realm of machine\ncognitive evaluation has already reached Imitation, the next step is an\nefficient Language Acquisition and Understanding. Our paper proposes a paradigm\nshift from the established Turing Test towards an all-embracing framework that\nhinges on language acquisition, taking inspiration from the recent advancements\nin LLMs. The present contribution is deeply tributary of the excellent work\nfrom various disciplines, point out the need to keep interdisciplinary bridges\nopen, and delineates a more robust and sustainable approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vera_P/0/1/0/all/0/1\">Patricio Vera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moya_P/0/1/0/all/0/1\">Pedro Moya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barraza_L/0/1/0/all/0/1\">Lisa Barraza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. (arXiv:2309.11998v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.11998","description":"<p>Studying how people interact with large language models (LLMs) in real-world\nscenarios is increasingly important due to their widespread use in various\napplications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset\ncontaining one million real-world conversations with 25 state-of-the-art LLMs.\nThis dataset is collected from 210K unique IP addresses in the wild on our\nVicuna demo and Chatbot Arena website. We offer an overview of the dataset's\ncontent, including its curation process, basic statistics, and topic\ndistribution, highlighting its diversity, originality, and scale. We\ndemonstrate its versatility through four use cases: developing content\nmoderation models that perform similarly to GPT-4, building a safety benchmark,\ntraining instruction-following models that perform similarly to Vicuna, and\ncreating challenging benchmark questions. We believe that this dataset will\nserve as a valuable resource for understanding and advancing LLM capabilities.\nThe dataset is publicly available at\nhttps://huggingface.co/datasets/lmsys/lmsys-chat-1m.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lianmin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_W/0/1/0/all/0/1\">Wei-Lin Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1\">Ying Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianle Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Siyuan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhanghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yonghao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric. P Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AceGPT, Localizing Large Language Models in Arabic. (arXiv:2309.12053v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.12053","description":"<p>This paper explores the imperative need and methodology for developing a\nlocalized Large Language Model (LLM) tailored for Arabic, a language with\nunique cultural characteristics that are not adequately addressed by current\nmainstream models like ChatGPT. Key concerns additionally arise when\nconsidering cultural sensitivity and local values. To this end, the paper\noutlines a packaged solution, including further pre-training with Arabic texts,\nsupervised fine-tuning (SFT) using native Arabic instructions and GPT-4\nresponses in Arabic, and reinforcement learning with AI feedback (RLAIF) using\na reward model that is sensitive to local culture and values. The objective is\nto train culturally aware and value-aligned Arabic LLMs that can serve the\ndiverse application-specific needs of Arabic-speaking communities.\n</p>\n<p>Extensive evaluations demonstrated that the resulting LLM called `AceGPT' is\nthe SOTA open Arabic LLM in various benchmarks, including instruction-following\nbenchmark (i.e., Arabic Vicuna-80 and Arabic AlpacaEval), knowledge benchmark\n(i.e., Arabic MMLU and EXAMs), as well as the newly-proposed Arabic cultural \\&amp;\nvalue alignment benchmark. Notably, AceGPT outperforms ChatGPT in the popular\nVicuna-80 benchmark when evaluated with GPT-4, despite the benchmark's limited\nscale. % Natural Language Understanding (NLU) benchmark (i.e., ALUE)\n</p>\n<p>Codes, data, and models are in https://github.com/FreedomIntelligence/AceGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xuening Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dingjie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alharthi_A/0/1/0/all/0/1\">Abdulmohsen Alharthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziche Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruoyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinchao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.12284","description":"<p>Large language models (LLMs) have pushed the limits of natural language\nunderstanding and exhibited excellent problem-solving ability. Despite the\ngreat success, most existing open-source LLMs (e.g., LLaMA-2) are still far\naway from satisfactory for solving mathematical problem due to the complex\nreasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned\nlanguage model that specializes in mathematical reasoning. Specifically, we\nstart by bootstrapping mathematical questions by rewriting the question from\nmultiple perspectives without extra knowledge, which results in a new dataset\ncalled MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA.\nExperimental results on two popular benchmarks (i.e., GSM8K and MATH) for\nmathematical reasoning demonstrate that MetaMath outperforms a suite of\nopen-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4%\non GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same\nsize by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of\n82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release the MetaMathQA\ndataset, the MetaMath models with different model sizes and the training code\nfor public use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Longhui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Weisen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Han Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jincheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1\">James T. Kwok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-09-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}