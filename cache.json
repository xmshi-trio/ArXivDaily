{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-08-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The Bias Amplification Paradox in Text-to-Image Generation. (arXiv:2308.00755v1 [cs.LG])","link":"http://arxiv.org/abs/2308.00755","description":"<p>Bias amplification is a phenomenon in which models increase imbalances\npresent in the training data. In this paper, we study bias amplification in the\ntext-to-image domain using Stable Diffusion by comparing gender ratios in\ntraining vs. generated images. We find that the model appears to amplify\ngender-occupation biases found in the training data (LAION). However, we\ndiscover that amplification can largely be attributed to discrepancies between\ntraining captions and model prompts. For example, an inherent difference is\nthat captions from the training data often contain explicit gender information\nwhile the prompts we use do not, which leads to a distribution shift and\nconsequently impacts bias measures. Once we account for various distributional\ndifferences between texts used for training and generation, we observe that\namplification decreases considerably. Our findings illustrate the challenges of\ncomparing biases in models and the data they are trained on, and highlight\nconfounding factors that contribute to bias amplification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seshadri_P/0/1/0/all/0/1\">Preethi Seshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Contrastive BERT Fine-tuning for Fusion-based Reviewed-Item Retrieval. (arXiv:2308.00762v1 [cs.IR])","link":"http://arxiv.org/abs/2308.00762","description":"<p>As natural language interfaces enable users to express increasingly complex\nnatural language queries, there is a parallel explosion of user review content\nthat can allow users to better find items such as restaurants, books, or movies\nthat match these expressive queries. While Neural Information Retrieval (IR)\nmethods have provided state-of-the-art results for matching queries to\ndocuments, they have not been extended to the task of Reviewed-Item Retrieval\n(RIR), where query-review scores must be aggregated (or fused) into item-level\nscores for ranking. In the absence of labeled RIR datasets, we extend Neural IR\nmethodology to RIR by leveraging self-supervised methods for contrastive\nlearning of BERT embeddings for both queries and reviews. Specifically,\ncontrastive learning requires a choice of positive and negative samples, where\nthe unique two-level structure of our item-review data combined with meta-data\naffords us a rich structure for the selection of these samples. For contrastive\nlearning in a Late Fusion scenario, we investigate the use of positive review\nsamples from the same item and/or with the same rating, selection of hard\npositive samples by choosing the least similar reviews from the same anchor\nitem, and selection of hard negative samples by choosing the most similar\nreviews from different items. We also explore anchor sub-sampling and\naugmenting with meta-data. For a more end-to-end Early Fusion approach, we\nintroduce contrastive item embedding learning to fuse reviews into single item\nembeddings. Experimental results show that Late Fusion contrastive learning for\nNeural RIR outperforms all other contrastive IR configurations, Neural IR, and\nsparse retrieval baselines, thus demonstrating the power of exploiting the\ntwo-level structure in Neural RIR approaches as well as the importance of\npreserving the nuance of individual review content via Late Fusion methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pour_M/0/1/0/all/0/1\">Mohammad Mahdi Abdollah Pour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinneya_P/0/1/0/all/0/1\">Parsa Farinneya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toroghi_A/0/1/0/all/0/1\">Armin Toroghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korikov_A/0/1/0/all/0/1\">Anton Korikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pesaranghader_A/0/1/0/all/0/1\">Ali Pesaranghader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajed_T/0/1/0/all/0/1\">Touqir Sajed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_M/0/1/0/all/0/1\">Manasa Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavrin_B/0/1/0/all/0/1\">Borislav Mavrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1\">Scott Sanner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRDD: A Dataset for Greek Dialectal NLP. (arXiv:2308.00802v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00802","description":"<p>In this paper, we present a dataset for the computational study of a number\nof Modern Greek dialects. It consists of raw text data from four dialects of\nModern Greek, Cretan, Pontic, Northern Greek and Cypriot Greek. The dataset is\nof considerable size, albeit imbalanced, and presents the first attempt to\ncreate large scale dialectal resources of this type for Modern Greek dialects.\nWe then use the dataset to perform dialect idefntification. We experiment with\ntraditional ML algorithms, as well as simple DL architectures. The results show\nvery good performance on the task, potentially revealing that the dialects in\nquestion have distinct enough characteristics allowing even simple ML models to\nperform well on the task. Error analysis is performed for the top performing\nalgorithms showing that in a number of cases the errors are due to insufficient\ndataset cleaning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chatzikyriakidis_S/0/1/0/all/0/1\">Stergios Chatzikyriakidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qwaider_C/0/1/0/all/0/1\">Chatrine Qwaider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolokousis_I/0/1/0/all/0/1\">Ilias Kolokousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koula_C/0/1/0/all/0/1\">Christina Koula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_D/0/1/0/all/0/1\">Dimitris Papadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakellariou_E/0/1/0/all/0/1\">Efthymia Sakellariou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiactTOD: Learning Generalizable Latent Dialogue Acts for Controllable Task-Oriented Dialogue Systems. (arXiv:2308.00878v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00878","description":"<p>Dialogue act annotations are important to improve response generation quality\nin task-oriented dialogue systems. However, it can be challenging to use\ndialogue acts to control response generation in a generalizable way because\ndifferent datasets and tasks may have incompatible annotations. While\nalternative methods that utilize latent action spaces or reinforcement learning\ndo not require explicit annotations, they may lack interpretability or face\ndifficulties defining task-specific rewards. In this work, we present a novel\nend-to-end latent dialogue act model (DiactTOD) that represents dialogue acts\nin a latent space. DiactTOD, when pre-trained on a large corpus, is able to\npredict and control dialogue acts to generate controllable responses using\nthese latent representations in a zero-shot fashion. Our approach demonstrates\nstate-of-the-art performance across a wide range of experimental settings on\nthe MultiWOZ dataset, including zero-shot, few-shot, and full data fine-tuning\nwith both end-to-end and policy optimization configurations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gung_J/0/1/0/all/0/1\">James Gung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_R/0/1/0/all/0/1\">Raphael Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-aware conditional GAN for category text generation. (arXiv:2308.00939v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00939","description":"<p>Category text generation receives considerable attentions since it is\nbeneficial for various natural language processing tasks. Recently, the\ngenerative adversarial network (GAN) has attained promising performance in text\ngeneration, attributed to its adversarial training process. However, there are\nseveral issues in text GANs, including discreteness, training instability, mode\ncollapse, lack of diversity and controllability etc. To address these issues,\nthis paper proposes a novel GAN framework, the feature-aware conditional GAN\n(FA-GAN), for controllable category text generation. In FA-GAN, the generator\nhas a sequence-to-sequence structure for improving sentence diversity, which\nconsists of three encoders including a special feature-aware encoder and a\ncategory-aware encoder, and one relational-memory-core-based decoder with the\nGumbel SoftMax activation function. The discriminator has an additional\ncategory classification head. To generate sentences with specified categories,\nthe multi-class classification loss is supplemented in the adversarial\ntraining. Comprehensive experiments have been conducted, and the results show\nthat FA-GAN consistently outperforms 10 state-of-the-art text generation\napproaches on 6 text classification datasets. The case study demonstrates that\nthe synthetic sentences generated by FA-GAN can match the required categories\nand are aware of the features of conditioned sentences, with good readability,\nfluency, and text authenticity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1\">Kezhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fanfan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zijian Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Smaller Language Models To Generalise To Unseen Compositional Questions. (arXiv:2308.00946v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00946","description":"<p>We equip a smaller Language Model to generalise to answering challenging\ncompositional questions that have not been seen in training. To do so we\npropose a combination of multitask supervised pretraining on up to 93 tasks\ndesigned to instill diverse reasoning abilities, and a dense retrieval system\nthat aims to retrieve a set of evidential paragraph fragments. Recent progress\nin question-answering has been achieved either through prompting methods\nagainst very large pretrained Language Models in zero or few-shot fashion, or\nby fine-tuning smaller models, sometimes in conjunction with information\nretrieval. We focus on the less explored question of the extent to which\nzero-shot generalisation can be enabled in smaller models with retrieval\nagainst a corpus within which sufficient information to answer a particular\nquestion may not exist. We establish strong baselines in this setting for\ndiverse evaluation datasets (StrategyQA, CommonsenseQA, IIRC, DROP, Musique and\nARC-DA), and show that performance can be significantly improved by adding\nretrieval-augmented training datasets which are designed to expose our models\nto a variety of heuristic reasoning strategies such as weighing partial\nevidence or ignoring an irrelevant context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartill_T/0/1/0/all/0/1\">Tim Hartill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+TAN_N/0/1/0/all/0/1\">Neset TAN</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1\">Michael Witbrock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riddle_P/0/1/0/all/0/1\">Patricia J. Riddle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis. (arXiv:2308.01018v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01018","description":"<p>While FastSpeech2 aims to integrate aspects of speech such as pitch, energy,\nand duration as conditional inputs, it still leaves scope for richer\nrepresentations. As a part of this work, we leverage representations from\nvarious Self-Supervised Learning (SSL) models to enhance the quality of the\nsynthesized speech. In particular, we pass the FastSpeech2 encoder's\nlength-regulated outputs through a series of encoder layers with the objective\nof reconstructing the SSL representations. In the SALTTS-parallel\nimplementation, the representations from this second encoder are used for an\nauxiliary reconstruction loss with the SSL features. The SALTTS-cascade\nimplementation, however, passes these representations through the decoder in\naddition to having the reconstruction loss. The richness of speech\ncharacteristics from the SSL features reflects in the output speech quality,\nwith the objective and subjective evaluation measures of the proposed approach\noutperforming the baseline FastSpeech2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sivaguru_R/0/1/0/all/0/1\">Ramanan Sivaguru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lodagala_V/0/1/0/all/0/1\">Vasista Sai Lodagala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chat Translation Error Detection for Assisting Cross-lingual Communications. (arXiv:2308.01044v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01044","description":"<p>In this paper, we describe the development of a communication support system\nthat detects erroneous translations to facilitate crosslingual communications\ndue to the limitations of current machine chat translation methods. We trained\nan error detector as the baseline of the system and constructed a new\nJapanese-English bilingual chat corpus, BPersona-chat, which comprises\nmultiturn colloquial chats augmented with crowdsourced quality ratings. The\nerror detector can serve as an encouraging foundation for more advanced\nerroneous translation detection systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunmeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1\">Jun Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morishita_M/0/1/0/all/0/1\">Makoto Morishita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abe_K/0/1/0/all/0/1\">Kaori Abe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokuhisa_R/0/1/0/all/0/1\">Ryoko Tokuhisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brassard_A/0/1/0/all/0/1\">Ana Brassard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Few-Shot Data Augmentation and Waterfall Prompting for Response Generation. (arXiv:2308.01080v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01080","description":"<p>This paper discusses our approaches for task-oriented conversational\nmodelling using subjective knowledge, with a particular emphasis on response\ngeneration. Our methodology was shaped by an extensive data analysis that\nevaluated key factors such as response length, sentiment, and dialogue acts\npresent in the provided dataset. We used few-shot learning to augment the data\nwith newly generated subjective knowledge items and present three approaches\nfor DSTC11: (1) task-specific model exploration, (2) incorporation of the most\nfrequent question into all generated responses, and (3) a waterfall prompting\ntechnique using a combination of both GPT-3 and ChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krause_L/0/1/0/all/0/1\">Lea Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santamaria_S/0/1/0/all/0/1\">Selene B&#xe1;ez Santamar&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meer_M/0/1/0/all/0/1\">Michiel van der Meer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khurana_U/0/1/0/all/0/1\">Urja Khurana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Generic: Enhancing Image Captioning with Real-World Knowledge using Vision-Language Pre-Training Model. (arXiv:2308.01126v1 [cs.CV])","link":"http://arxiv.org/abs/2308.01126","description":"<p>Current captioning approaches tend to generate correct but \"generic\"\ndescriptions that lack real-world knowledge, e.g., named entities and\ncontextual information. Considering that Vision-Language Pre-Training (VLP)\nmodels master massive such knowledge from large-scale web-harvested data, it is\npromising to utilize the generalizability of VLP models to incorporate\nknowledge into image descriptions. However, using VLP models faces challenges:\nzero-shot inference suffers from knowledge hallucination that leads to\nlow-quality descriptions, but the generic bias in downstream task fine-tuning\nhinders the VLP model from expressing knowledge. To address these concerns, we\npropose a simple yet effective method called Knowledge-guided Replay\n(K-Replay), which enables the retention of pre-training knowledge during\nfine-tuning. Our approach consists of two parts: (1) a knowledge prediction\ntask on automatically collected replay exemplars to continuously awaken the VLP\nmodel's memory about knowledge, thus preventing the model from collapsing into\nthe generic pattern; (2) a knowledge distillation constraint to improve the\nfaithfulness of generated descriptions hence alleviating the knowledge\nhallucination. To evaluate knowledge-enhanced descriptions, we construct a\nnovel captioning benchmark KnowCap, containing knowledge of landmarks, famous\nbrands, special foods and movie characters. Experimental results show that our\napproach effectively incorporates knowledge into descriptions, outperforming\nstrong VLP baseline by 20.9 points (78.7-&gt;99.6) in CIDEr score and 20.5\npercentage points (34.0%-&gt;54.5%) in knowledge recognition accuracy. Our code\nand data is available at https://github.com/njucckevin/KnowCap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kanzhi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wenpo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zixuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianbing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADS-Cap: A Framework for Accurate and Diverse Stylized Captioning with Unpaired Stylistic Corpora. (arXiv:2308.01143v1 [cs.CV])","link":"http://arxiv.org/abs/2308.01143","description":"<p>Generating visually grounded image captions with specific linguistic styles\nusing unpaired stylistic corpora is a challenging task, especially since we\nexpect stylized captions with a wide variety of stylistic patterns. In this\npaper, we propose a novel framework to generate Accurate and Diverse Stylized\nCaptions (ADS-Cap). Our ADS-Cap first uses a contrastive learning module to\nalign the image and text features, which unifies paired factual and unpaired\nstylistic corpora during the training process. A conditional variational\nauto-encoder is then used to automatically memorize diverse stylistic patterns\nin latent space and enhance diversity through sampling. We also design a simple\nbut effective recheck module to boost style accuracy by filtering\nstyle-specific captions. Experimental results on two widely used stylized image\ncaptioning datasets show that regarding consistency with the image, style\naccuracy and diversity, ADS-Cap achieves outstanding performances compared to\nvarious baselines. We finally conduct extensive analyses to understand the\neffectiveness of our method. Our code is available at\nhttps://github.com/njucckevin/ADS-Cap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kanzhi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_S/0/1/0/all/0/1\">Shi Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianbing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xinyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arithmetic with Language Models: from Memorization to Computation. (arXiv:2308.01154v1 [cs.AI])","link":"http://arxiv.org/abs/2308.01154","description":"<p>A better understanding of the emergent computation and problem-solving\ncapabilities of recent large language models is of paramount importance to\nfurther improve them and broaden their applicability. This work investigates\nhow a language model, trained to predict the next token, can perform arithmetic\ncomputations generalizing beyond training data. Binary addition and\nmultiplication constitute a good testbed for this purpose, since they require a\nvery small vocabulary and exhibit relevant input/output discontinuities making\nsmooth input interpolation ineffective for novel data. We successfully trained\na light language model to learn these tasks and ran a number of experiments to\ninvestigate the extrapolation capabilities and internal information processing.\nOur findings support the hypotheses that the language model works as an\nEncoding-Regression-Decoding machine where the computation takes place in the\nvalue space once the input token representation is mapped to an appropriate\ninternal representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maltoni_D/0/1/0/all/0/1\">Davide Maltoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_M/0/1/0/all/0/1\">Matteo Ferrara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysing the Resourcefulness of the Paragraph for Precedence Retrieval. (arXiv:2308.01203v1 [cs.IR])","link":"http://arxiv.org/abs/2308.01203","description":"<p>Developing methods for extracting relevant legal information to aid legal\npractitioners is an active research area. In this regard, research efforts are\nbeing made by leveraging different kinds of information, such as meta-data,\ncitations, keywords, sentences, paragraphs, etc. Similar to any text document,\nlegal documents are composed of paragraphs. In this paper, we have analyzed the\nresourcefulness of paragraph-level information in capturing similarity among\njudgments for improving the performance of precedence retrieval. We found that\nthe paragraph-level methods could capture the similarity among the judgments\nwith only a few paragraph interactions and exhibit more discriminating power\nover the baseline document-level method. Moreover, the comparison results on\ntwo benchmark datasets for the precedence retrieval on the Indian supreme court\njudgments task show that the paragraph-level methods exhibit comparable\nperformance with the state-of-the-art methods\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sisodiya_B/0/1/0/all/0/1\">Bhoomeendra Singh Sisodiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unnam_N/0/1/0/all/0/1\">Narendra Babu Unnam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_P/0/1/0/all/0/1\">P. Krishna Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Apala Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santhy_K/0/1/0/all/0/1\">K.V.K. Santhy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_V/0/1/0/all/0/1\">V. Balakista Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Hierarchical Neural Networks using Hierarchical Softmax. (arXiv:2308.01210v1 [stat.ML])","link":"http://arxiv.org/abs/2308.01210","description":"<p>This paper presents a framework in which hierarchical softmax is used to\ncreate a global hierarchical classifier. The approach is applicable for any\nclassification task where there is a natural hierarchy among classes. We show\nempirical results on four text classification datasets. In all datasets the\nhierarchical softmax improved on the regular softmax used in a flat classifier\nin terms of macro-F1 and macro-recall. In three out of four datasets\nhierarchical softmax achieved a higher micro-accuracy and macro-precision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Schuurmans_J/0/1/0/all/0/1\">Jetze Schuurmans</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Frasincar_F/0/1/0/all/0/1\">Flavius Frasincar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Multilingual Language Models Think Better in English?. (arXiv:2308.01223v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01223","description":"<p>Translate-test is a popular technique to improve the performance of\nmultilingual language models. This approach works by translating the input into\nEnglish using an external machine translation system, and running inference\nover the translated input. However, these improvements can be attributed to the\nuse of a separate translation system, which is typically trained on large\namounts of parallel data not seen by the language model. In this work, we\nintroduce a new approach called self-translate, which overcomes the need of an\nexternal translation system by leveraging the few-shot translation capabilities\nof multilingual language models. Experiments over 5 tasks show that\nself-translate consistently outperforms direct inference, demonstrating that\nlanguage models are unable to leverage their full multilingual potential when\nprompted in non-English languages. Our code is available at\nhttps://github.com/juletx/self-translate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Etxaniz_J/0/1/0/all/0/1\">Julen Etxaniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azkune_G/0/1/0/all/0/1\">Gorka Azkune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1\">Aitor Soroa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacalle_O/0/1/0/all/0/1\">Oier Lopez de Lacalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounded Image Text Matching with Mismatched Relation Reasoning. (arXiv:2308.01236v1 [cs.CV])","link":"http://arxiv.org/abs/2308.01236","description":"<p>This paper introduces Grounded Image Text Matching with Mismatched Relation\n(GITM-MR), a novel visual-linguistic joint task that evaluates the relation\nunderstanding capabilities of transformer-based pre-trained models. GITM-MR\nrequires a model to first determine if an expression describes an image, then\nlocalize referred objects or ground the mismatched parts of the text. We\nprovide a benchmark for evaluating pre-trained models on this task, with a\nfocus on the challenging settings of limited data and out-of-distribution\nsentence lengths. Our evaluation demonstrates that pre-trained models lack data\nefficiency and length generalization ability. To address this, we propose the\nRelation-sensitive Correspondence Reasoning Network (RCRN), which incorporates\nrelation-aware reasoning via bi-directional message propagation guided by\nlanguage structure. RCRN can be interpreted as a modular program and delivers\nstrong performance in both length generalization and data efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yana Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haozhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sibei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation. (arXiv:2308.01240v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01240","description":"<p>In this work, we evaluate 10 open-source instructed LLMs on four\nrepresentative code comprehension and generation tasks. We have the following\nmain findings. First, for the zero-shot setting, instructed LLMs are very\ncompetitive on code comprehension and generation tasks and sometimes even\nbetter than small SOTA models specifically fine-tuned on each downstream task.\nWe also find that larger instructed LLMs are not always better on code-related\ntasks. Second, for the few-shot setting, we find that adding demonstration\nexamples substantially helps instructed LLMs perform better on most code\ncomprehension and generation tasks; however, the examples would sometimes\ninduce unstable or even worse performance. Furthermore, we find widely-used\nBM25-based shot selection strategy significantly outperforms the basic random\nselection or fixed selection only on generation problems. Third, for the\nfine-tuning setting, we find that fine-tuning could further improve the model\nperformance on downstream code comprehension and generation tasks compared to\nthe zero-shot/one-shot performance. In addition, after being fine-tuned on the\nsame downstream task dataset, instructed LLMs outperform both the small SOTA\nmodels and similar-scaled LLMs without instruction tuning. Based on our\nfindings, we further present practical implications on model and usage\nrecommendation, performance and cost trade-offs, and future direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhiqiang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_Q/0/1/0/all/0/1\">Qiancheng Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yiling Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. (arXiv:2308.01263v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01263","description":"<p>Without proper safeguards, large language models will readily follow\nmalicious instructions and generate toxic content. This motivates safety\nefforts such as red-teaming and large-scale feedback learning, which aim to\nmake models both helpful and harmless. However, there is a tension between\nthese two objectives, since harmlessness requires models to refuse complying\nwith unsafe prompts, and thus not be helpful. Recent anecdotal evidence\nsuggests that some models may have struck a poor balance, so that even clearly\nsafe prompts are refused if they use similar language to unsafe prompts or\nmention sensitive topics. In this paper, we introduce a new test suite called\nXSTest to identify such eXaggerated Safety behaviours in a structured and\nsystematic way. In its current form, XSTest comprises 200 safe prompts across\nten prompt types that well-calibrated models should not refuse to comply with.\nWe describe XSTest's creation and composition, and use the test suite to\nhighlight systematic failure modes in a recently-released state-of-the-art\nlanguage model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertie Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1\">Giuseppe Attanasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the psychology of GPT-4's Moral and Legal Reasoning. (arXiv:2308.01264v1 [cs.AI])","link":"http://arxiv.org/abs/2308.01264","description":"<p>Large language models have been used as the foundation of highly\nsophisticated artificial intelligences, capable of delivering human-like\nresponses to probes about legal and moral issues. However, these models are\nunreliable guides to their own inner workings, and even the engineering teams\nbehind their creation are unable to explain exactly how they came to develop\nall of the capabilities they currently have. The emerging field of machine\npsychology seeks to gain insight into the processes and concepts that these\nmodels possess. In this paper, we employ the methods of psychology to probe\ninto GPT-4's moral and legal reasoning. More specifically, we investigate the\nsimilarities and differences between GPT-4 and humans when it comes to\nintentionality ascriptions, judgments about causation, the morality of\ndeception, moral foundations, the impact of moral luck on legal judgments, the\nconcept of consent, and rule violation judgments. We find high correlations\nbetween human and AI responses, but also several significant systematic\ndifferences between them. We conclude with a discussion of the philosophical\nimplications of our findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almeida_G/0/1/0/all/0/1\">Guilherme F. C. F. Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunes_J/0/1/0/all/0/1\">Jos&#xe9; Luiz Nunes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelmann_N/0/1/0/all/0/1\">Neele Engelmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiegmann_A/0/1/0/all/0/1\">Alex Wiegmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_M/0/1/0/all/0/1\">Marcelo de Ara&#xfa;jo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?. (arXiv:2308.01284v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01284","description":"<p>Large language models (LLMs) such as ChatGPT are increasingly being used for\nvarious use cases, including text content generation at scale. Although\ndetection methods for such AI-generated text exist already, we investigate\nChatGPT's performance as a detector on such AI-generated text, inspired by\nworks that use ChatGPT as a data labeler or annotator. We evaluate the\nzero-shot performance of ChatGPT in the task of human-written vs. AI-generated\ntext detection, and perform experiments on publicly available datasets. We\nempirically investigate if ChatGPT is symmetrically effective in detecting\nAI-generated or human-written text. Our findings provide insight on how ChatGPT\nand similar LLMs may be leveraged in automated detection pipelines by simply\nfocusing on solving a specific aspect of the problem and deriving the rest from\nthat solution. All code and data is available at\n\\url{https://github.com/AmritaBh/ChatGPT-as-Detector}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Amrita Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])","link":"http://arxiv.org/abs/2308.01313","description":"<p>CLIP, as a foundational vision language model, is widely used in zero-shot\nimage classification due to its ability to understand various visual concepts\nand natural language descriptions. However, how to fully leverage CLIP's\nunprecedented human-like understanding capabilities to achieve better zero-shot\nclassification is still an open question. This paper draws inspiration from the\nhuman visual perception process: a modern neuroscience view suggests that in\nclassifying an object, humans first infer its class-independent attributes\n(e.g., background and orientation) which help separate the foreground object\nfrom the background, and then make decisions based on this information.\nInspired by this, we observe that providing CLIP with contextual attributes\nimproves zero-shot classification and mitigates reliance on spurious features.\nWe also observe that CLIP itself can reasonably infer the attributes from an\nimage. With these observations, we propose a training-free, two-step zero-shot\nclassification method named PerceptionCLIP. Given an image, it first infers\ncontextual attributes (e.g., background) and then performs object\nclassification conditioning on them. Our experiments show that PerceptionCLIP\nachieves better generalization, group robustness, and better interpretability.\nFor example, PerceptionCLIP with ViT-L/14 improves the worst group accuracy by\n16.5% on the Waterbirds dataset and by 3.5% on CelebA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sicheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panaitescu_Liess_M/0/1/0/all/0/1\">Michael-Andrei Panaitescu-Liess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mummadi_C/0/1/0/all/0/1\">Chaithanya Kumar Mummadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Is All You Need. (arXiv:1706.03762v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1706.03762","description":"<p>The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1\">Ashish Vaswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_N/0/1/0/all/0/1\">Niki Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1\">Jakob Uszkoreit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_L/0/1/0/all/0/1\">Llion Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1\">Aidan N. Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_L/0/1/0/all/0/1\">Lukasz Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polosukhin_I/0/1/0/all/0/1\">Illia Polosukhin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DePA: Improving Non-autoregressive Machine Translation with Dependency-Aware Decoder. (arXiv:2203.16266v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16266","description":"<p>Non-autoregressive machine translation (NAT) models have lower translation\nquality than autoregressive translation (AT) models because NAT decoders do not\ndepend on previous target tokens in the decoder input. We propose a novel and\ngeneral Dependency-Aware Decoder (DePA) to enhance target dependency modeling\nin the decoder of fully NAT models from two perspectives: decoder\nself-attention and decoder input. First, we propose an autoregressive\nforward-backward pre-training phase before NAT training, which enables the NAT\ndecoder to gradually learn bidirectional target dependencies for the final NAT\ntraining. Second, we transform the decoder input from the source language\nrepresentation space to the target language representation space through a\nnovel attentive transformation process, which enables the decoder to better\ncapture target dependencies. DePA can be applied to any fully NAT models.\nExtensive experiments show that DePA consistently improves highly competitive\nand state-of-the-art fully NAT models on widely used WMT and IWSLT benchmarks\nby up to 1.88 BLEU gain, while maintaining the inference latency comparable to\nother fully NAT models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1\">Jiaao Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improve Event Extraction via Self-Training with Gradient Guidance. (arXiv:2205.12490v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12490","description":"<p>Data scarcity has been the main factor that hinders the progress of event\nextraction. To overcome this issue, we propose a Self-Training with Feedback\n(STF) framework that leverages the large-scale unlabeled data and acquires\nfeedback for each new event prediction from the unlabeled data by comparing it\nto the Abstract Meaning Representation (AMR) graph of the same sentence.\nSpecifically, STF consists of (1) a base event extraction model trained on\nexisting event annotations and then applied to large-scale unlabeled corpora to\npredict new event mentions as pseudo training samples, and (2) a novel scoring\nmodel that takes in each new predicted event trigger, an argument, its argument\nrole, as well as their paths in the AMR graph to estimate a compatibility score\nindicating the correctness of the pseudo label. The compatibility scores\nfurther act as feedback to encourage or discourage the model learning on the\npseudo labels during self-training. Experimental results on three benchmark\ndatasets, including ACE05-E, ACE05-E+, and ERE, demonstrate the effectiveness\nof the STF framework on event extraction, especially event argument extraction,\nwith significant performance gain over the base event extraction models and\nstrong baselines. Our experimental analysis further shows that STF is a generic\nframework as it can be applied to improve most, if not all, event extraction\nmodels by leveraging large-scale unlabeled data, even when high-quality AMR\ngraph annotations are not available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jay-Yoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mass-Editing Memory in a Transformer. (arXiv:2210.07229v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07229","description":"<p>Recent work has shown exciting promise in updating large language models with\nnew memories, so as to replace obsolete information or add specialized\nknowledge. However, this line of work is predominantly limited to updating\nsingle associations. We develop MEMIT, a method for directly updating a\nlanguage model with many memories, demonstrating experimentally that it can\nscale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B),\nexceeding prior work by orders of magnitude. Our code and data are at\nhttps://memit.baulab.info.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_K/0/1/0/all/0/1\">Kevin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Arnab Sen Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andonian_A/0/1/0/all/0/1\">Alex Andonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntactic Surprisal From Neural Models Predicts, But Underestimates, Human Processing Difficulty From Syntactic Ambiguities. (arXiv:2210.12187v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12187","description":"<p>Humans exhibit garden path effects: When reading sentences that are\ntemporarily structurally ambiguous, they slow down when the structure is\ndisambiguated in favor of the less preferred alternative. Surprisal theory\n(Hale, 2001; Levy, 2008), a prominent explanation of this finding, proposes\nthat these slowdowns are due to the unpredictability of each of the words that\noccur in these sentences. Challenging this hypothesis, van Schijndel &amp; Linzen\n(2021) find that estimates of the cost of word predictability derived from\nlanguage models severely underestimate the magnitude of human garden path\neffects. In this work, we consider whether this underestimation is due to the\nfact that humans weight syntactic factors in their predictions more highly than\nlanguage models do. We propose a method for estimating syntactic predictability\nfrom a language model, allowing us to weigh the cost of lexical and syntactic\npredictability independently. We find that treating syntactic predictability\nindependently from lexical predictability indeed results in larger estimates of\ngarden path. At the same time, even when syntactic predictability is\nindependently weighted, surprisal still greatly underestimate the magnitude of\nhuman garden path effects. Our results support the hypothesis that\npredictability is not the only factor responsible for the processing cost\nassociated with garden path sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arehalli_S/0/1/0/all/0/1\">Suhas Arehalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dillon_B/0/1/0/all/0/1\">Brian Dillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thinking Fast and Slow in Large Language Models. (arXiv:2212.05206v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05206","description":"<p>Large language models (LLMs) are currently at the forefront of intertwining\nAI systems with human communication and everyday life. Therefore, it is of\ngreat importance to evaluate their emerging abilities. In this study, we show\nthat LLMs like GPT-3 exhibit behavior that strikingly resembles human-like\nintuition - and the cognitive errors that come with it. However, LLMs with\nhigher cognitive capabilities, in particular ChatGPT and GPT-4, learned to\navoid succumbing to these errors and perform in a hyperrational manner. For our\nexperiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as\nsemantic illusions that were originally designed to investigate intuitive\ndecision-making in humans. Our study demonstrates that investigating LLMs with\nmethods from psychology has the potential to reveal otherwise unknown emergent\ntraits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagendorff_T/0/1/0/all/0/1\">Thilo Hagendorff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabi_S/0/1/0/all/0/1\">Sarah Fabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosinski_M/0/1/0/all/0/1\">Michal Kosinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Detecting Harmful Agendas in News Articles. (arXiv:2302.00102v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00102","description":"<p>Manipulated news online is a growing problem which necessitates the use of\nautomated systems to curtail its spread. We argue that while misinformation and\ndisinformation detection have been studied, there has been a lack of investment\nin the important open challenge of detecting harmful agendas in news articles;\nidentifying harmful agendas is critical to flag news campaigns with the\ngreatest potential for real world harm. Moreover, due to real concerns around\ncensorship, harmful agenda detectors must be interpretable to be effective. In\nthis work, we propose this new task and release a dataset, NewsAgendas, of\nannotated news articles for agenda identification. We show how interpretable\nsystems can be effective on this task and demonstrate that they can perform\ncomparably to black-box models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subbiah_M/0/1/0/all/0/1\">Melanie Subbiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Amrita Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yilun Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1\">Tharindu Kumarage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Prompt for Few-shot Table-to-Text Generation. (arXiv:2302.12468v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12468","description":"<p>Pretrained language models (PLMs) have made remarkable progress in\ntable-to-text generation tasks. However, the lack of domain-specific knowledge\nmakes it challenging to bridge the topological gap between tabular data and\ntext, especially in real-world applications with limited resources. To mitigate\nthe limitation of insufficient labeled data, we propose a novel framework:\nAdapt-Prompt-to-Generate (AdaPTGen). The core insight of AdaPTGen is to adapt\nprompt templates of domain-specific knowledge into the model, which brings at\nleast three benefits: (1) it injects representation of normal table-related\ndescriptions to bridge the topological gap between tabular data and texts; (2)\nit enables us to use large amounts of unlabeled domain-specific knowledge\nfully, which can alleviate the PLMs' inherent shortcomings of lacking domain\nknowledge; (3) it allows us to design various tasks to explore the\ndomain-specific knowledge. Extensive experiments and analyses are conducted on\nthree open-domain few-shot natural language generation (NLG) data sets: Humans,\nSongs, and Books. Compared to previous state-of-the-art approaches, our model\nachieves superior performance in terms of both fluency and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhixin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Minyxuan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiexing Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Ziwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guanjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technical report: Graph Neural Networks go Grammatical. (arXiv:2303.01590v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.01590","description":"<p>This paper proposes a framework to formally link a fragment of an algebraic\nlanguage to a Graph Neural Network (GNN). It relies on Context Free Grammars\n(CFG) to organise algebraic operations into generative rules that can be\ntranslated into a GNN layer model. Since the rules and variables of a CFG\ndirectly derived from a language contain redundancies, a grammar reduction\nscheme is presented making tractable the translation into a GNN layer. Applying\nthis strategy, a grammar compliant with the third-order Weisfeiler-Lehman\n(3-WL) test is defined from MATLANG. From this 3-WL CFG, we derive a provably\n3-WL GNN model called G$^2$N$^2$. Moreover, this grammatical approach allows us\nto provide algebraic formulas to count the cycles of length up to six and\nchordal cycles at the edge level, which enlightens the counting power of 3-WL.\nSeveral experiments illustrate that G$^2$N$^2$ efficiently outperforms other\n3-WL GNNs on many downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piquenot_J/0/1/0/all/0/1\">Jason Piquenot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moscatelli_A/0/1/0/all/0/1\">Aldo Moscatelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berar_M/0/1/0/all/0/1\">Maxime B&#xe9;rar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heroux_P/0/1/0/all/0/1\">Pierre H&#xe9;roux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+raveaux_R/0/1/0/all/0/1\">Romain raveaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramel_J/0/1/0/all/0/1\">Jean-Yves Ramel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_S/0/1/0/all/0/1\">S&#xe9;bastien Adam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Strong Zero-Shot Retriever. (arXiv:2304.14233v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14233","description":"<p>In this work, we propose a simple method that applies a large language model\n(LLM) to large-scale retrieval in zero-shot scenarios. Our method, the Language\nlanguage model as Retriever (LameR), is built upon no other neural models but\nan LLM, while breaking brute-force combinations of retrievers with LLMs and\nlifting the performance of zero-shot retrieval to be very competitive on\nbenchmark datasets. Essentially, we propose to augment a query with its\npotential answers by prompting LLMs with a composition of the query and the\nquery's in-domain candidates. The candidates, regardless of correct or wrong,\nare obtained by a vanilla retrieval procedure on the target collection. As a\npart of the prompts, they are likely to help LLM generate more precise answers\nby pattern imitation or candidate summarization. Even if all the candidates are\nwrong, the prompts at least make LLM aware of in-collection patterns and\ngenres. Moreover, due to the low performance of a self-supervised retriever,\nthe LLM-based query augmentation becomes less effective as the retriever\nbottlenecks the whole pipeline. Therefore, we propose to leverage a\nnon-parametric lexicon-based method (e.g., BM25) as the retrieval module to\ncapture query-document overlap in a literal fashion. As such, LameR makes the\nretrieval procedure transparent to the LLM, thus circumventing the performance\nbottleneck.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Small Language Models on PubMedQA via Generative Data Augmentation. (arXiv:2305.07804v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07804","description":"<p>Large Language Models (LLMs) have made remarkable advancements in the field\nof natural language processing. However, their increasing size poses challenges\nin terms of computational cost. On the other hand, Small Language Models (SLMs)\nare known for their efficiency, but they often struggle with limited capacity\nand training data, especially in specific domains. In this paper, we introduce\na novel method aimed at improving SLMs in the medical domain using LLM-based\ngenerative data augmentation. The objective of our approach is to develop more\nefficient and capable models that are specifically tailored for specialized\napplications. Through experiments conducted on the PubMedQA dataset, we\ndemonstrate the effectiveness of LLMs in refining and diversifying existing\nquestion-answer pairs. This refinement process leads to improved performance in\na significantly smaller model after fine-tuning. Notably, our best SLM, with\nunder 1.6 billion parameters, outperforms the few-shot GPT-4 on the PubMedQA\ndataset. Our code and generated data are publicly available to facilitate\nfurther explorations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shangdi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR. (arXiv:2305.15386v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15386","description":"<p>Improving ASR systems is necessary to make new LLM-based use-cases accessible\nto people across the globe. In this paper, we focus on Indian languages, and\nmake the case that diverse benchmarks are required to evaluate and improve ASR\nsystems for Indian languages. To address this, we collate Vistaar as a set of\n59 benchmarks across various language and domain combinations, on which we\nevaluate 3 publicly available ASR systems and 2 commercial systems. We also\ntrain IndicWhisper models by fine-tuning the Whisper models on publicly\navailable training datasets across 12 Indian languages totalling to 10.7K\nhours. We show that IndicWhisper significantly improves on considered ASR\nsystems on the Vistaar benchmark. Indeed, IndicWhisper has the lowest WER in 39\nout of the 59 benchmarks, with an average reduction of 4.1 WER. We open-source\nall datasets, code and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhogale_K/0/1/0/all/0/1\">Kaushal Santosh Bhogale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_S/0/1/0/all/0/1\">Sai Sundaresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_A/0/1/0/all/0/1\">Abhigyan Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_T/0/1/0/all/0/1\">Tahir Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CamemBERT-bio: a Tasty French Language Model Better for your Health. (arXiv:2306.15550v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.15550","description":"<p>Clinical data in hospitals are increasingly accessible for research through\nclinical data warehouses, however these documents are unstructured. It is\ntherefore necessary to extract information from medical reports to conduct\nclinical studies. Transfer learning with BERT-like models such as CamemBERT has\nallowed major advances, especially for named entity recognition. However, these\nmodels are trained for plain language and are less efficient on biomedical\ndata. This is why we propose a new French public biomedical dataset on which we\nhave continued the pre-training of CamemBERT. Thus, we introduce a first\nversion of CamemBERT-bio, a specialized public model for the French biomedical\ndomain that shows 2.54 points of F1 score improvement on average on different\nbiomedical named entity recognition tasks. Our findings demonstrate the success\nof continual pre-training from a French model and contrast with recent\nproposals on the same domain and language. One of our key contributions\nhighlights the importance of using a standard evaluation protocol that enables\na clear view of the current state-of-the-art for French biomedical models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Touchent_R/0/1/0/all/0/1\">Rian Touchent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romary_L/0/1/0/all/0/1\">Laurent Romary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clergerie_E/0/1/0/all/0/1\">Eric de la Clergerie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.00925","description":"<p>Semantic similarity measures are widely used in natural language processing\nto catalyze various computer-related tasks. However, no single semantic\nsimilarity measure is the most appropriate for all tasks, and researchers often\nuse ensemble strategies to ensure performance. This research work proposes a\nmethod for automatically designing semantic similarity ensembles. In fact, our\nproposed method uses grammatical evolution, for the first time, to\nautomatically select and aggregate measures from a pool of candidates to create\nan ensemble that maximizes correlation to human judgment. The method is\nevaluated on several benchmark datasets and compared to state-of-the-art\nensembles, showing that it can significantly improve similarity assessment\naccuracy and outperform existing methods in some cases. As a result, our\nresearch demonstrates the potential of using grammatical evolution to\nautomatically compare text and prove the benefits of using ensembles for\nsemantic similarity tasks. The source code that illustrates our approach can be\ndownloaded from https://github.com/jorge-martinez-gil/sesige.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Gil_J/0/1/0/all/0/1\">Jorge Martinez-Gil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.03109","description":"<p>Large language models (LLMs) are gaining increasing popularity in both\nacademia and industry, owing to their unprecedented performance in various\napplications. As LLMs continue to play a vital role in both research and daily\nuse, their evaluation becomes increasingly critical, not only at the task\nlevel, but also at the society level for better understanding of their\npotential risks. Over the past years, significant efforts have been made to\nexamine LLMs from various perspectives. This paper presents a comprehensive\nreview of these evaluation methods for LLMs, focusing on three key dimensions:\nwhat to evaluate, where to evaluate, and how to evaluate. Firstly, we provide\nan overview from the perspective of evaluation tasks, encompassing general\nnatural language processing tasks, reasoning, medical usage, ethics,\neducations, natural and social sciences, agent applications, and other areas.\nSecondly, we answer the `where' and `how' questions by diving into the\nevaluation methods and benchmarks, which serve as crucial components in\nassessing performance of LLMs. Then, we summarize the success and failure cases\nof LLMs in different tasks. Finally, we shed light on several future challenges\nthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to\nresearchers in the realm of LLMs evaluation, thereby aiding the development of\nmore proficient LLMs. Our key point is that evaluation should be treated as an\nessential discipline to better assist the development of LLMs. We consistently\nmaintain the related open-source materials at:\nhttps://github.com/MLGroupJLU/LLM-eval-survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yupeng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kaijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cunxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Integrated NPL Approach to Sentiment Analysis in Satisfaction Surveys. (arXiv:2307.11771v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.11771","description":"<p>The research project aims to apply an integrated approach to natural language\nprocessing NLP to satisfaction surveys. It will focus on understanding and\nextracting relevant information from survey responses, analyzing feelings, and\nidentifying recurring word patterns. NLP techniques will be used to determine\nemotional polarity, classify responses into positive, negative, or neutral\ncategories, and use opinion mining to highlight participants opinions. This\napproach will help identify the most relevant aspects for participants and\nunderstand their opinions in relation to those specific aspects. A key\ncomponent of the research project will be the analysis of word patterns in\nsatisfaction survey responses using NPL. This analysis will provide a deeper\nunderstanding of feelings, opinions, and themes and trends present in\nrespondents responses. The results obtained from this approach can be used to\nidentify areas for improvement, understand respondents preferences, and make\nstrategic decisions based on analysis to improve respondent satisfaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pinto_Luque_E/0/1/0/all/0/1\">Edson B. Pinto-Luque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback. (arXiv:2307.16039v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.16039","description":"<p>A key technology for the development of large language models (LLMs) involves\ninstruction tuning that helps align the models' responses with human\nexpectations to realize impressive learning abilities. Two major approaches for\ninstruction tuning characterize supervised fine-tuning (SFT) and reinforcement\nlearning from human feedback (RLHF), which are currently applied to produce the\nbest commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for\nresearch and development efforts, various instruction-tuned open-source LLMs\nhave also been introduced recently, e.g., Alpaca, Vicuna, to name a few.\nHowever, existing open-source LLMs have only been instruction-tuned for English\nand a few popular languages, thus hindering their impacts and accessibility to\nmany other languages in the world. Among a few very recent work to explore\ninstruction tuning for LLMs in multiple languages, SFT has been used as the\nonly approach to instruction-tune LLMs for multiple languages. This has left a\nsignificant gap for fine-tuned LLMs based on RLHF in diverse languages and\nraised important questions on how RLHF can boost the performance of\nmultilingual instruction tuning. To overcome this issue, we present Okapi, the\nfirst system with instruction-tuned LLMs based on RLHF for multiple languages.\nOkapi introduces instruction and response-ranked data in 26 diverse languages\nto facilitate the experiments and development of future multilingual LLM\nresearch. We also present benchmark datasets to enable the evaluation of\ngenerative LLMs in multiple languages. Our experiments demonstrate the\nadvantages of RLHF for multilingual instruction over SFT for different base\nmodels and datasets. Our framework and resources are released at\nhttps://github.com/nlp-uoregon/Okapi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1\">Viet Dac Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chien Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_N/0/1/0/all/0/1\">Nghia Trung Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuat Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1\">Ryan A. Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension. (arXiv:2307.16125v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.16125","description":"<p>Based on powerful Large Language Models (LLMs), recent generative Multimodal\nLarge Language Models (MLLMs) have gained prominence as a pivotal research\narea, exhibiting remarkable capability for both comprehension and generation.\nIn this work, we address the evaluation of generative comprehension in MLLMs as\na preliminary step towards a comprehensive assessment of generative models, by\nintroducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple\nchoice questions with accurate human annotations (x 6 larger than existing\nbenchmarks), which spans 12 evaluation dimensions including the comprehension\nof both the image and video modality. We develop an advanced pipeline for\ngenerating multiple-choice questions that target specific evaluation\ndimensions, integrating both automatic filtering and manual verification\nprocesses. Multiple-choice questions with groundtruth options derived from\nhuman annotation enables an objective and efficient assessment of model\nperformance, eliminating the need for human or GPT intervention during\nevaluation. We further evaluate the performance of 18 models across all 12\ndimensions, covering both the spatial and temporal understanding. By revealing\nthe limitations of existing MLLMs through evaluation results, we aim for\nSEED-Bench to provide insights for motivating future research. We will launch\nand consistently maintain a leaderboard to provide a platform for the community\nto assess and investigate model capability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yuying Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Private Watermark for Large Language Models. (arXiv:2307.16230v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.16230","description":"<p>Recently, text watermarking algorithms for large language models (LLMs) have\nbeen mitigating the potential harms of text generated by the LLMs, including\nfake news and copyright issues. However, the watermark detection of current\ntext algorithms requires the key from the generation process, making them\nsusceptible to breaches and counterfeiting. In this work, we propose the first\nprivate watermarking algorithm, which extends the current text watermarking\nalgorithms by using two different neural networks respectively for watermark\ngeneration and detection, rather than using the same key at both stages.\nMeanwhile, part of the parameters of the watermark generation and detection\nnetworks are shared, which makes the detection network achieve a high accuracy\nvery efficiently. Experiments show that our algorithm ensures high detection\naccuracy with minimal impact on generation and detection speed, due to the\nsmall parameter size of both networks. Additionally, our subsequent analysis\ndemonstrates the difficulty of reverting the watermark generation rules from\nthe detection network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Leyi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shu&#x27;ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMs4OL: Large Language Models for Ontology Learning. (arXiv:2307.16648v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2307.16648","description":"<p>We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs)\nfor Ontology Learning (OL). LLMs have shown significant advancements in natural\nlanguage processing, demonstrating their ability to capture complex language\npatterns in different knowledge domains. Our LLMs4OL paradigm investigates the\nfollowing hypothesis: \\textit{Can LLMs effectively apply their language pattern\ncapturing capability to OL, which involves automatically extracting and\nstructuring knowledge from natural language text?} To test this hypothesis, we\nconduct a comprehensive evaluation using the zero-shot prompting method. We\nevaluate nine different LLM model families for three main OL tasks: term\ntyping, taxonomy discovery, and extraction of non-taxonomic relations.\nAdditionally, the evaluations encompass diverse genres of ontological\nknowledge, including lexicosemantic knowledge in WordNet, geographical\nknowledge in GeoNames, and medical knowledge in UMLS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giglou_H/0/1/0/all/0/1\">Hamed Babaei Giglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jennifer D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1\">S&#xf6;ren Auer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AsdKB: A Chinese Knowledge Base for the Early Screening and Diagnosis of Autism Spectrum Disorder. (arXiv:2307.16773v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2307.16773","description":"<p>To easily obtain the knowledge about autism spectrum disorder and help its\nearly screening and diagnosis, we create AsdKB, a Chinese knowledge base on\nautism spectrum disorder. The knowledge base is built on top of various\nsources, including 1) the disease knowledge from SNOMED CT and ICD-10 clinical\ndescriptions on mental and behavioural disorders, 2) the diagnostic knowledge\nfrom DSM-5 and different screening tools recommended by social organizations\nand medical institutes, and 3) the expert knowledge on professional physicians\nand hospitals from the Web. AsdKB contains both ontological and factual\nknowledge, and is accessible as Linked Data at https://w3id.org/asdkb/. The\npotential applications of AsdKB are question answering, auxiliary diagnosis,\nand expert recommendation, and we illustrate them with a prototype which can be\naccessed at <a href=\"http://asdkb.org.cn/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianxing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xudong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yipeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feiyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1\">Tianling Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_S/0/1/0/all/0/1\">Shenqi Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Semantically Enriched Embeddings for Knowledge Graph Completion. (arXiv:2308.00081v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2308.00081","description":"<p>Embedding based Knowledge Graph (KG) Completion has gained much attention\nover the past few years. Most of the current algorithms consider a KG as a\nmultidirectional labeled graph and lack the ability to capture the semantics\nunderlying the schematic information. In a separate development, a vast amount\nof information has been captured within the Large Language Models (LLMs) which\nhas revolutionized the field of Artificial Intelligence. KGs could benefit from\nthese LLMs and vice versa. This vision paper discusses the existing algorithms\nfor KG completion based on the variations for generating KG embeddings. It\nstarts with discussing various KG completion algorithms such as transductive\nand inductive link prediction and entity type prediction algorithms. It then\nmoves on to the algorithms utilizing type information within the KGs, LLMs, and\nfinally to algorithms capturing the semantics represented in different\ndescription logic axioms. We conclude the paper with a critical reflection on\nthe current state of work in the community and give recommendations for future\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mehwish Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harmelen_F/0/1/0/all/0/1\">Frank van Harmelen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acosta_M/0/1/0/all/0/1\">Maribel Acosta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZRIGF: An Innovative Multimodal Framework for Zero-Resource Image-Grounded Dialogue Generation. (arXiv:2308.00400v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.00400","description":"<p>Image-grounded dialogue systems benefit greatly from integrating visual\ninformation, resulting in high-quality response generation. However, current\nmodels struggle to effectively utilize such information in zero-resource\nscenarios, mainly due to the disparity between image and text modalities. To\novercome this challenge, we propose an innovative multimodal framework, called\nZRIGF, which assimilates image-grounded information for dialogue generation in\nzero-resource situations. ZRIGF implements a two-stage learning strategy,\ncomprising contrastive pre-training and generative pre-training. Contrastive\npre-training includes a text-image matching module that maps images and texts\ninto a unified encoded vector space, along with a text-assisted masked image\nmodeling module that preserves pre-training visual features and fosters further\nmultimodal feature alignment. Generative pre-training employs a multimodal\nfusion module and an information transfer module to produce insightful\nresponses based on harmonized multimodal representations. Comprehensive\nexperiments conducted on both text-based and image-grounded dialogue datasets\ndemonstrate ZRIGF's efficacy in generating contextually pertinent and\ninformative responses. Furthermore, we adopt a fully zero-resource scenario in\nthe image-grounded dialogue dataset to demonstrate our framework's robust\ngeneralization capabilities in novel domains. The code is available at\nhttps://github.com/zhangbo-nlp/ZRIGF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongfei Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2308.00436","description":"<p>The recent progress in large language models (LLMs), especially the invention\nof chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning\nproblems. However, even the strongest LLMs are still struggling with more\ncomplicated problems that require non-linear thinking and multi-step reasoning.\nIn this work, we explore whether LLMs have the ability to recognize their own\nerrors, without resorting to external resources. In particular, we investigate\nwhether they can be used to identify individual errors within a step-by-step\nreasoning. To this end, we propose a zero-shot verification scheme to recognize\nsuch errors. We then use this verification scheme to improve question-answering\nperformance, by using it to perform weighted voting on different generated\nanswers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and\nfind that it successfully recognizes errors and, in turn, increases final\npredictive performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_N/0/1/0/all/0/1\">Ning Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-08-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}