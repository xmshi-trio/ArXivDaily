{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"DeltaNet:Conditional Medical Report Generation for COVID-19 Diagnosis. (arXiv:2211.13229v1 [eess.IV])","link":"http://arxiv.org/abs/2211.13229","description":"<p>Fast screening and diagnosis are critical in COVID-19 patient treatment. In\naddition to the gold standard RT-PCR, radiological imaging like X-ray and CT\nalso works as an important means in patient screening and follow-up. However,\ndue to the excessive number of patients, writing reports becomes a heavy burden\nfor radiologists. To reduce the workload of radiologists, we propose DeltaNet\nto generate medical reports automatically. Different from typical image\ncaptioning approaches that generate reports with an encoder and a decoder,\nDeltaNet applies a conditional generation process. In particular, given a\nmedical image, DeltaNet employs three steps to generate a report: 1) first\nretrieving related medical reports, i.e., the historical reports from the same\nor similar patients; 2) then comparing retrieved images and current image to\nfind the differences; 3) finally generating a new report to accommodate\nidentified differences based on the conditional report. We evaluate DeltaNet on\na COVID-19 dataset, where DeltaNet outperforms state-of-the-art approaches.\nBesides COVID-19, the proposed DeltaNet can be applied to other diseases as\nwell. We validate its generalization capabilities on the public IU-Xray and\nMIMIC-CXR datasets for chest-related diseases. Code is available at\n\\url{https://github.com/LX-doctorAI1/DeltaNet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shuxin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhaopeng Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yangtian Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xingwang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Multimodal Model with Unlikelihood Training for Visual Dialog. (arXiv:2211.13235v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13235","description":"<p>The task of visual dialog requires a multimodal chatbot to answer sequential\nquestions from humans about image content. Prior work performs the standard\nlikelihood training for answer generation on the positive instances (involving\ncorrect answers). However, the likelihood objective often leads to frequent and\ndull outputs and fails to exploit the useful knowledge from negative instances\n(involving incorrect answers). In this paper, we propose a Unified Multimodal\nModel with UnLikelihood Training, named UniMM-UL, to tackle this problem.\nFirst, to improve visual dialog understanding and generation by multi-task\nlearning, our model extends ViLBERT from only supporting answer discrimination\nto holding both answer discrimination and answer generation seamlessly by\ndifferent attention masks. Specifically, in order to make the original\ndiscriminative model compatible with answer generation, we design novel\ngenerative attention masks to implement the autoregressive Masked Language\nModeling (autoregressive MLM) task. And to attenuate the adverse effects of the\nlikelihood objective, we exploit unlikelihood training on negative instances to\nmake the model less likely to generate incorrect answers. Then, to utilize\ndense annotations, we adopt different fine-tuning methods for both generating\nand discriminating answers, rather than just for discriminating answers as in\nthe prior work. Finally, on the VisDial dataset, our model achieves the best\ngenerative results (69.23 NDCG score). And our model also yields comparable\ndiscriminative results with the state-of-the-art in both single-model and\nensemble settings (75.92 and 76.17 NDCG scores).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Changjun Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask the Correct Tokens: An Embarrassingly Simple Approach for Error Correction. (arXiv:2211.13252v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13252","description":"<p>Text error correction aims to correct the errors in text sequences such as\nthose typed by humans or generated by speech recognition models. Previous error\ncorrection methods usually take the source (incorrect) sentence as encoder\ninput and generate the target (correct) sentence through the decoder. Since the\nerror rate of the incorrect sentence is usually low (e.g., 10\\%), the\ncorrection model can only learn to correct on limited error tokens but\ntrivially copy on most tokens (correct tokens), which harms the effective\ntraining of error correction. In this paper, we argue that the correct tokens\nshould be better utilized to facilitate effective training and then propose a\nsimple yet effective masking strategy to achieve this goal. Specifically, we\nrandomly mask out a part of the correct tokens in the source sentence and let\nthe model learn to not only correct the original error tokens but also predict\nthe masked tokens based on their context information. Our method enjoys several\nadvantages: 1) it alleviates trivial copy; 2) it leverages effective training\nsignals from correct tokens; 3) it is a plug-and-play module and can be applied\nto different models and tasks. Experiments on spelling error correction and\nspeech recognition error correction on Mandarin datasets and grammar error\ncorrection on English datasets with both autoregressive and non-autoregressive\ngeneration models show that our method improves the correction accuracy\nconsistently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1\">Kai Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Edward Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Device Directedness with Contextual Cues for Spoken Dialog Systems. (arXiv:2211.13280v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13280","description":"<p>In this work, we define barge-in verification as a supervised learning task\nwhere audio-only information is used to classify user spoken dialogue into true\nand false barge-ins. Following the success of pre-trained models, we use\nlow-level speech representations from a self-supervised representation learning\nmodel for our downstream classification task. Further, we propose a novel\ntechnique to infuse lexical information directly into speech representations to\nimprove the domain-specific language information implicitly learned during\npre-training. Experiments conducted on spoken dialog data show that our\nproposed model trained to validate barge-in entirely from speech\nrepresentations is faster by 38% relative and achieves 4.5% relative F1 score\nimprovement over a baseline LSTM model that uses both audio and Automatic\nSpeech Recognition (ASR) 1-best hypotheses. On top of this, our best proposed\nmodel with lexically infused representations along with contextual features\nprovides a further relative improvement of 5.7% in the F1 score but only 22%\nfaster than the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bekal_D/0/1/0/all/0/1\">Dhanush Bekal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sundararajan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1\">Sravan Bodapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronanki_S/0/1/0/all/0/1\">Srikanth Ronanki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEAT: Stable and Explainable Attention. (arXiv:2211.13290v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13290","description":"<p>Currently, attention mechanism becomes a standard fixture in most\nstate-of-the-art natural language processing (NLP) models, not only due to\noutstanding performance it could gain, but also due to plausible innate\nexplanation for the behaviors of neural architectures it provides, which is\nnotoriously difficult to analyze. However, recent studies show that attention\nis unstable against randomness and perturbations during training or testing,\nsuch as random seeds and slight perturbation of embedding vectors, which\nimpedes it from becoming a faithful explanation tool. Thus, a natural question\nis whether we can find some substitute of the current attention which is more\nstable and could keep the most important characteristics on explanation and\nprediction of attention. In this paper, to resolve the problem, we provide a\nfirst rigorous definition of such alternate namely SEAT (Stable and Explainable\nAttention). Specifically, a SEAT should has the following three properties: (1)\nIts prediction distribution is enforced to be close to the distribution based\non the vanilla attention; (2) Its top-k indices have large overlaps with those\nof the vanilla attention; (3) It is robust w.r.t perturbations, i.e., any\nslight perturbation on SEAT will not change the prediction distribution too\nmuch, which implicitly indicates that it is stable to randomness and\nperturbations. Finally, through intensive experiments on various datasets, we\ncompare our SEAT with other baseline methods using RNN, BiLSTM and BERT\narchitectures via six different evaluation metrics for model interpretation,\nstability and accuracy. Results show that SEAT is more stable against different\nperturbations and randomness while also keeps the explainability of attention,\nwhich indicates it is a more faithful explanation. Moreover, compared with\nvanilla attention, there is almost no utility (accuracy) degradation for SEAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lijie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huai_M/0/1/0/all/0/1\">Mengdi Huai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13308","description":"<p>Learned representations of scientific documents can serve as valuable input\nfeatures for downstream tasks, without the need for further fine-tuning.\nHowever, existing benchmarks for evaluating these representations fail to\ncapture the diversity of relevant tasks. In response, we introduce SciRepEval,\nthe first comprehensive benchmark for training and evaluating scientific\ndocument representations. It includes 25 challenging and realistic tasks, 11 of\nwhich are new, across four formats: classification, regression, ranking and\nsearch. We then use the benchmark to study and improve the generalization\nability of scientific document representation models. We show how\nstate-of-the-art models struggle to generalize across task formats, and that\nsimple multi-task training fails to improve them. However, a new approach that\nlearns multiple embeddings per document, each tailored to a different format,\ncan improve performance. We experiment with task-format-specific control codes\nand adapters in a multi-task setting and find that they outperform the existing\nsingle-embedding state-of-the-art by up to 1.5 points absolute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DArcy_M/0/1/0/all/0/1\">Mike D&#x27;Arcy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1\">Sergey Feldman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rank-One Editing of Encoder-Decoder Models. (arXiv:2211.13317v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13317","description":"<p>Large sequence to sequence models for tasks such as Neural Machine\nTranslation (NMT) are usually trained over hundreds of millions of samples.\nHowever, training is just the origin of a model's life-cycle. Real-world\ndeployments of models require further behavioral adaptations as new\nrequirements emerge or shortcomings become known. Typically, in the space of\nmodel behaviors, behavior deletion requests are addressed through model\nretrainings whereas model finetuning is done to address behavior addition\nrequests, both procedures being instances of data-based model intervention. In\nthis work, we present a preliminary study investigating rank-one editing as a\ndirect intervention method for behavior deletion requests in encoder-decoder\ntransformer models. We propose four editing tasks for NMT and show that the\nproposed editing algorithm achieves high efficacy, while requiring only a\nsingle instance of positive example to fix an erroneous (negative) model\nbehavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raunak_V/0/1/0/all/0/1\">Vikas Raunak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Arul Menezes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Report on the Euphemisms Detection Shared Task. (arXiv:2211.13327v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13327","description":"<p>This paper presents The Shared Task on Euphemism Detection for the Third\nWorkshop on Figurative Language Processing (FigLang 2022) held in conjunction\nwith EMNLP 2022. Participants were invited to investigate the euphemism\ndetection task: given input text, identify whether it contains a euphemism. The\ninput data is a corpus of sentences containing potentially euphemistic terms\n(PETs) collected from the GloWbE corpus (Davies and Fuchs, 2015), and are\nhuman-annotated as containing either a euphemistic or literal usage of a PET.\nIn this paper, we present the results and analyze the common themes, methods\nand findings of the participating teams\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Patrick Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_A/0/1/0/all/0/1\">Anna Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jing Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Focal Loss to Fight Shallow Heuristics: An Empirical Analysis of Modulated Cross-Entropy in Natural Language Inference. (arXiv:2211.13331v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13331","description":"<p>There is no such thing as a perfect dataset. In some datasets, deep neural\nnetworks discover underlying heuristics that allow them to take shortcuts in\nthe learning process, resulting in poor generalization capability. Instead of\nusing standard cross-entropy, we explore whether a modulated version of\ncross-entropy called focal loss can constrain the model so as not to use\nheuristics and improve generalization performance. Our experiments in natural\nlanguage inference show that focal loss has a regularizing impact on the\nlearning process, increasing accuracy on out-of-distribution data, but slightly\ndecreasing performance on in-distribution data. Despite the improved\nout-of-distribution performance, we demonstrate the shortcomings of focal loss\nand its inferiority in comparison to the performance of methods such as\nunbiased focal loss and self-debiasing ensembles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajic_F/0/1/0/all/0/1\">Frano Raji&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stresec_I/0/1/0/all/0/1\">Ivan Stresec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marmet_A/0/1/0/all/0/1\">Axel Marmet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Postuvan_T/0/1/0/all/0/1\">Tim Po&#x161;tuvan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tapping the Potential of Coherence and Syntactic Features in Neural Models for Automatic Essay Scoring. (arXiv:2211.13373v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13373","description":"<p>In the prompt-specific holistic score prediction task for Automatic Essay\nScoring, the general approaches include pre-trained neural model, coherence\nmodel, and hybrid model that incorporate syntactic features with neural model.\nIn this paper, we propose a novel approach to extract and represent essay\ncoherence features with prompt-learning NSP that shows to match the\nstate-of-the-art AES coherence model, and achieves the best performance for\nlong essays. We apply syntactic feature dense embedding to augment BERT-based\nmodel and achieve the best performance for hybrid methodology for AES. In\naddition, we explore various ideas to combine coherence, syntactic information\nand semantic embeddings, which no previous study has done before. Our combined\nmodel also performs better than the SOTA available for combined model, even\nthough it does not outperform our syntactic enhanced neural model. We further\noffer analyses that can be useful for future study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xinying Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shuxuan Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiajun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InDEX: Indonesian Idiom and Expression Dataset for Cloze Test. (arXiv:2211.13376v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13376","description":"<p>We propose InDEX, an Indonesian Idiom and Expression dataset for cloze test.\nThe dataset contains 10438 unique sentences for 289 idioms and expressions for\nwhich we generate 15 different types of distractors, resulting in a large\ncloze-style corpus. Many baseline models of cloze test reading comprehension\napply BERT with random initialization to learn embedding representation. But\nidioms and fixed expressions are different such that the literal meaning of the\nphrases may or may not be consistent with their contextual meaning. Therefore,\nwe explore different ways to combine static and contextual representations for\na stronger baseline model. Experimentations show that combining definition and\nrandom initialization will better support cloze test model performance for\nidioms whether independently or mixed with fixed expressions. While for fixed\nexpressions with no special meaning, static embedding with random\ninitialization is sufficient for cloze test model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xinying Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Guofeng Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning. (arXiv:2211.13437v1 [cs.CV])","link":"http://arxiv.org/abs/2211.13437","description":"<p>Cross-modal alignment is essential for vision-language pre-training (VLP)\nmodels to learn the correct corresponding information across different\nmodalities. For this purpose, inspired by the success of masked language\nmodeling (MLM) tasks in the NLP pre-training area, numerous masked modeling\ntasks have been proposed for VLP to further promote cross-modal interactions.\nThe core idea of previous masked modeling tasks is to focus on reconstructing\nthe masked tokens based on visible context for learning local-to-local\nalignment. However, most of them pay little attention to the global semantic\nfeatures generated for the masked data, resulting in the limited cross-modal\nalignment ability of global representations. Therefore, in this paper, we\npropose a novel Semantic Completion Learning (SCL) task, complementary to\nexisting masked modeling tasks, to facilitate global-to-local alignment.\nSpecifically, the SCL task complements the missing semantics of masked data by\ncapturing the corresponding information from the other modality, promoting\nlearning more representative global features which have a great impact on the\nperformance of downstream tasks. Moreover, we present a flexible vision\nencoder, which enables our model to perform image-text and video-text\nmultimodal tasks simultaneously. Experimental results show that our proposed\nmethod obtains state-of-the-art performance on various vision-language\nbenchmarks, such as visual question answering, image-text retrieval, and\nvideo-text retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yatai Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1\">Rongcheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1\">Weijie Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSGP: Two-Stage Generative Prompting for Unsupervised Commonsense Question Answering. (arXiv:2211.13515v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13515","description":"<p>Unsupervised commonsense question answering requires mining effective\ncommonsense knowledge without the rely on the labeled task data. Previous\nmethods typically retrieved from traditional knowledge bases or used\npre-trained language models (PrLMs) to generate fixed types of knowledge, which\nhave poor generalization ability. In this paper, we aim to address the above\nlimitation by leveraging the implicit knowledge stored in PrLMs and propose a\ntwo-stage prompt-based unsupervised commonsense question answering framework\n(TSGP). Specifically, we first use knowledge generation prompts to generate the\nknowledge required for questions with unlimited types and possible candidate\nanswers independent of specified choices. Then, we further utilize answer\ngeneration prompts to generate possible candidate answers independent of\nspecified choices. Experimental results and analysis on three different\ncommonsense reasoning tasks, CommonsenseQA, OpenBookQA, and SocialIQA,\ndemonstrate that TSGP significantly improves the reasoning ability of language\nmodels in unsupervised settings. Our code is available at:\nhttps://github.com/Yueqing-Sun/TSGP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yueqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Le Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qi Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Mahalanobis-Based Scores for Textual OOD Detection. (arXiv:2211.13527v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13527","description":"<p>Deep learning methods have boosted the adoption of NLP systems in real-life\napplications. However, they turn out to be vulnerable to distribution shifts\nover time which may cause severe dysfunctions in production systems, urging\npractitioners to develop tools to detect out-of-distribution (OOD) samples\nthrough the lens of the neural network. In this paper, we introduce TRUSTED, a\nnew OOD detector for classifiers based on Transformer architectures that meets\noperational requirements: it is unsupervised and fast to compute. The\nefficiency of TRUSTED relies on the fruitful idea that all hidden layers carry\nrelevant information to detect OOD examples. Based on this, for a given input,\nTRUSTED consists in (i) aggregating this information and (ii) computing a\nsimilarity score by exploiting the training distribution, leveraging the\npowerful concept of data depth. Our extensive numerical experiments involve 51k\nmodel configurations, including various checkpoints, seeds, and datasets, and\ndemonstrate that TRUSTED achieves state-of-the-art performances. In particular,\nit improves previous AUROC over 3 points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_E/0/1/0/all/0/1\">Eduardo D. C. Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staerman_G/0/1/0/all/0/1\">Guillaume Staerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noiry_N/0/1/0/all/0/1\">Nathan Noiry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How \"open\" are the conversations with open-domain chatbots? A proposal for Speech Event based evaluation. (arXiv:2211.13560v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13560","description":"<p>Open-domain chatbots are supposed to converse freely with humans without\nbeing restricted to a topic, task or domain. However, the boundaries and/or\ncontents of open-domain conversations are not clear. To clarify the boundaries\nof \"openness\", we conduct two studies: First, we classify the types of \"speech\nevents\" encountered in a chatbot evaluation data set (i.e., Meena by Google)\nand find that these conversations mainly cover the \"small talk\" category and\nexclude the other speech event categories encountered in real life human-human\ncommunication. Second, we conduct a small-scale pilot study to generate online\nconversations covering a wider range of speech event categories between two\nhumans vs. a human and a state-of-the-art chatbot (i.e., Blender by Facebook).\nA human evaluation of these generated conversations indicates a preference for\nhuman-human conversations, since the human-chatbot conversations lack coherence\nin most speech event categories. Based on these results, we suggest (a) using\nthe term \"small talk\" instead of \"open-domain\" for the current chatbots which\nare not that \"open\" in terms of conversational abilities yet, and (b) revising\nthe evaluation methods to test the chatbot conversations against other speech\nevents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dogruoz_A/0/1/0/all/0/1\">A. Seza Do&#x11f;ru&#xf6;z</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skantze_G/0/1/0/all/0/1\">Gabriel Skantze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ham2Pose: Animating Sign Language Notation into Pose Sequences. (arXiv:2211.13613v1 [cs.CV])","link":"http://arxiv.org/abs/2211.13613","description":"<p>Translating spoken languages into Sign languages is necessary for open\ncommunication between the hearing and hearing-impaired communities. To achieve\nthis goal, we propose the first method for animating a text written in\nHamNoSys, a lexical Sign language notation, into signed pose sequences. As\nHamNoSys is universal, our proposed method offers a generic solution invariant\nto the target Sign language. Our method gradually generates pose predictions\nusing transformer encoders that create meaningful representations of the text\nand poses while considering their spatial and temporal information. We use weak\nsupervision for the training process and show that our method succeeds in\nlearning from partial and inaccurate data. Additionally, we offer a new\ndistance measurement for pose sequences, normalized Dynamic Time Warping\n(nDTW), based on DTW over normalized keypoints trajectories, and validate its\ncorrectness using AUTSL, a large-scale Sign language dataset. We show that it\nmeasures the distance between pose sequences more accurately than existing\nmeasurements and use it to assess the quality of our generated pose sequences.\nCode for the data pre-processing, the model, and the distance measurement is\npublicly released for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shalev_Arkushin_R/0/1/0/all/0/1\">Rotem Shalev-Arkushin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moryossef_A/0/1/0/all/0/1\">Amit Moryossef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1\">Ohad Fried</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototypical Fine-tuning: Towards Robust Performance Under Varying Data Sizes. (arXiv:2211.13638v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13638","description":"<p>In this paper, we move towards combining large parametric models with\nnon-parametric prototypical networks. We propose prototypical fine-tuning, a\nnovel prototypical framework for fine-tuning pretrained language models (LM),\nwhich automatically learns a bias to improve predictive performance for varying\ndata sizes, especially low-resource settings. Our prototypical fine-tuning\napproach can automatically adjust the model capacity according to the number of\ndata points and the model's inherent attributes. Moreover, we propose four\nprinciples for effective prototype fine-tuning towards the optimal solution.\nExperimental results across various datasets show that our work achieves\nsignificant performance improvements under various low-resource settings, as\nwell as comparable and usually better performances in high-resource scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yiqiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yaru Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitask Learning for Low Resource Spoken Language Understanding. (arXiv:2211.13703v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13703","description":"<p>We explore the benefits that multitask learning offer to speech processing as\nwe train models on dual objectives with automatic speech recognition and intent\nclassification or sentiment classification. Our models, although being of\nmodest size, show improvements over models trained end-to-end on intent\nclassification. We compare different settings to find the optimal disposition\nof each task module compared to one another. Finally, we study the performance\nof the models in low-resource scenario by training the models with as few as\none example per class. We show that multitask learning in these scenarios\ncompete with a baseline model trained on text features and performs\nconsiderably better than a pipeline model. On sentiment classification, we\nmatch the performance of an end-to-end model with ten times as many parameters.\nWe consider 4 tasks and 4 datasets in Dutch and English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meeus_Q/0/1/0/all/0/1\">Quentin Meeus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+hamme_H/0/1/0/all/0/1\">Hugo Van hamme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Undesirable biases in NLP: Averting a crisis of measurement. (arXiv:2211.13709v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13709","description":"<p>As Natural Language Processing (NLP) technology rapidly develops and spreads\ninto daily life, it becomes crucial to anticipate how its use could harm\npeople. However, our ways of assessing the biases of NLP models have not kept\nup. While especially the detection of English gender bias in such models has\nenjoyed increasing research attention, many of the measures face serious\nproblems, as it is often unclear what they actually measure and how much they\nare subject to measurement error. In this paper, we provide an\ninterdisciplinary approach to discussing the issue of NLP model bias by\nadopting the lens of psychometrics -- a field specialized in the measurement of\nconcepts like bias that are not directly observable. We pair an introduction of\nrelevant psychometric concepts with a discussion of how they could be used to\nevaluate and improve bias measures. We also argue that adopting psychometric\nvocabulary and methodology can make NLP bias research more efficient and\ntransparent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wal_O/0/1/0/all/0/1\">Oskar van der Wal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachmann_D/0/1/0/all/0/1\">Dominik Bachmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leidinger_A/0/1/0/all/0/1\">Alina Leidinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maanen_L/0/1/0/all/0/1\">Leendert van Maanen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuidema_W/0/1/0/all/0/1\">Willem Zuidema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_K/0/1/0/all/0/1\">Katrin Schulz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion-guided Cross-domain Fake News Detection using Adversarial Domain Adaptation. (arXiv:2211.13718v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13718","description":"<p>Recent works on fake news detection have shown the efficacy of using emotions\nas a feature or emotions-based features for improved performance. However, the\nimpact of these emotion-guided features for fake news detection in cross-domain\nsettings, where we face the problem of domain shift, is still largely\nunexplored. In this work, we evaluate the impact of emotion-guided features for\ncross-domain fake news detection, and further propose an emotion-guided,\ndomain-adaptive approach using adversarial learning. We prove the efficacy of\nemotion-guided models in cross-domain settings for various combinations of\nsource and target datasets from FakeNewsAMT, Celeb, Politifact and Gossipcop\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhry_A/0/1/0/all/0/1\">Arjun Choudhry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khatri_I/0/1/0/all/0/1\">Inder Khatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Arkajyoti Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishwakarma_D/0/1/0/all/0/1\">Dinesh Kumar Vishwakarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_M/0/1/0/all/0/1\">Mukesh Prasad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question-type Identification for Academic Questions in Online Learning Platform. (arXiv:2211.13727v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13727","description":"<p>Online learning platforms provide learning materials and answers to students'\nacademic questions by experts, peers, or systems. This paper explores\nquestion-type identification as a step in content understanding for an online\nlearning platform. The aim of the question-type identifier is to categorize\nquestion types based on their structure and complexity, using the question\ntext, subject, and structural features. We have defined twelve question-type\nclasses, including Multiple-Choice Question (MCQ), essay, and others. We have\ncompiled an internal dataset of students' questions and used a combination of\nweak-supervision techniques and manual annotation. We then trained a BERT-based\nensemble model on this dataset and evaluated this model on a separate\nhuman-labeled test set. Our experiments yielded an F1-score of 0.94 for MCQ\nbinary classification and promising results for 12-class multilabel\nclassification. We deployed the model in our online learning platform as a\ncrucial enabler for content understanding to enhance the student learning\nexperience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabiee_A/0/1/0/all/0/1\">Azam Rabiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Alok Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Johnson D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanwalkar_S/0/1/0/all/0/1\">Saurabh Khanwalkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"German Phoneme Recognition with Text-to-Phoneme Data Augmentation. (arXiv:2211.13776v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13776","description":"<p>In this study, we experimented to examine the effect of adding the most\nfrequent n phoneme bigrams to the basic vocabulary on the German phoneme\nrecognition model using the text-to-phoneme data augmentation strategy. As a\nresult, compared to the baseline model, the vowel30 model and the const20 model\nshowed an increased BLEU score of more than 1 point, and the total30 model\nshowed a significant decrease in the BLEU score of more than 20 points, showing\nthat the phoneme bigrams could have a positive or negative effect on the model\nperformance. In addition, we identified the types of errors that the models\nrepeatedly showed through error analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dojun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seohyun Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PyTAIL: Interactive and Incremental Learning of NLP Models with Human in the Loop for Online Data. (arXiv:2211.13786v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13786","description":"<p>Online data streams make training machine learning models hard because of\ndistribution shift and new patterns emerging over time. For natural language\nprocessing (NLP) tasks that utilize a collection of features based on lexicons\nand rules, it is important to adapt these features to the changing data. To\naddress this challenge we introduce PyTAIL, a python library, which allows a\nhuman in the loop approach to actively train NLP models. PyTAIL enhances\ngeneric active learning, which only suggests new instances to label by also\nsuggesting new features like rules and lexicons to label. Furthermore, PyTAIL\nis flexible enough for users to accept, reject, or update rules and lexicons as\nthe model is being trained. Finally, we simulate the performance of PyTAIL on\nexisting social media benchmark datasets for text classification. We compare\nvarious active learning strategies on these benchmarks. The model closes the\ngap with as few as 10% of the training data. Finally, we also highlight the\nimportance of tracking evaluation metric on remaining data (which is not yet\nmerged with active learning) alongside the test dataset. This highlights the\neffectiveness of the model in accurately annotating the remaining dataset,\nwhich is especially suitable for batch processing of large unlabelled corpora.\nPyTAIL will be available at https://github.com/socialmediaie/pytail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shubhanshu Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diesner_J/0/1/0/all/0/1\">Jana Diesner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question Answering and Question Generation for Finnish. (arXiv:2211.13794v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13794","description":"<p>Recent advances in the field of language modeling have improved the\nstate-of-the-art in question answering (QA) and question generation (QG).\nHowever, the development of modern neural models, their benchmarks, and\ndatasets for training them has mainly focused on English. Finnish, like many\nother languages, faces a shortage of large QA/QG model training resources,\nwhich has prevented experimenting with state-of-the-art QA/QG fine-tuning\nmethods. We present the first neural QA and QG models that work with Finnish.\nTo train the models, we automatically translate the SQuAD dataset and then use\nnormalization methods to reduce the amount of problematic data created during\nthe translation. Using the synthetic data, together with the Finnish partition\nof the TyDi-QA dataset, we fine-tune several transformer-based models to both\nQA and QG and evaluate their performance. To the best of our knowledge, the\nresulting dataset is the first large-scale QA/QG resource for Finnish. This\npaper also sets the initial benchmarks for Finnish-language QA and QG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kylliainen_I/0/1/0/all/0/1\">Ilmari Kylli&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yangarber_R/0/1/0/all/0/1\">Roman Yangarber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Linguistic and Computational Requirements for Creating Face-to-Face Multimodal Human-Machine Interaction. (arXiv:2211.13804v1 [cs.HC])","link":"http://arxiv.org/abs/2211.13804","description":"<p>In this study, conversations between humans and avatars are linguistically,\norganizationally, and structurally analyzed, focusing on what is necessary for\ncreating face-to-face multimodal interfaces for machines. We videorecorded\nthirty-four human-avatar interactions, performed complete linguistic\nmicroanalysis on video excerpts, and marked all the occurrences of multimodal\nactions and events. Statistical inferences were applied to data, allowing us to\ncomprehend not only how often multimodal actions occur but also how multimodal\nevents are distributed between the speaker (emitter) and the listener\n(recipient). We also observed the distribution of multimodal occurrences for\neach modality. The data show evidence that double-loop feedback is established\nduring a face-to-face conversation. This led us to propose that knowledge from\nConversation Analysis (CA), cognitive science, and Theory of Mind (ToM), among\nothers, should be incorporated into the ones used for describing human-machine\nmultimodal interactions. Face-to-face interfaces require an additional control\nlayer to the multimodal fusion layer. This layer has to organize the flow of\nconversation, integrate the social context into the interaction, as well as\nmake plans concerning 'what' and 'how' to progress on the interaction. This\nhigher level is best understood if we incorporate insights from CA and ToM into\nthe interface system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranhel_J/0/1/0/all/0/1\">Jo&#xe3;o Ranhel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lima_C/0/1/0/all/0/1\">Cacilda Vilela de Lima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-label Few-shot ICD Coding as Autoregressive Generation with Prompt. (arXiv:2211.13813v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13813","description":"<p>Automatic International Classification of Diseases (ICD) coding aims to\nassign multiple ICD codes to a medical note with an average of 3,000+ tokens.\nThis task is challenging due to the high-dimensional space of multi-label\nassignment (155,000+ ICD code candidates) and the long-tail challenge - Many\nICD codes are infrequently assigned yet infrequent ICD codes are important\nclinically. This study addresses the long-tail challenge by transforming this\nmulti-label classification task into an autoregressive generation task.\nSpecifically, we first introduce a novel pretraining objective to generate free\ntext diagnoses and procedure using the SOAP structure, the medical logic\nphysicians use for note documentation. Second, instead of directly predicting\nthe high dimensional space of ICD codes, our model generates the lower\ndimension of text descriptions, which then infer ICD codes. Third, we designed\na novel prompt template for multi-label classification. We evaluate our\nGeneration with Prompt model with the benchmark of all code assignment\n(MIMIC-III-full) and few shot ICD code assignment evaluation benchmark\n(MIMIC-III-few). Experiments on MIMIC-III-few show that our model performs with\na marco F1 30.2, which substantially outperforms the previous MIMIC-III-full\nSOTA model (marco F1 4.3) and the model specifically designed for few/zero shot\nsetting (marco F1 18.7). Finally, we design a novel ensemble learner, a cross\nattention reranker with prompts, to integrate previous SOTA and our best\nfew-shot coding predictions. Experiments on MIMIC-III-full show that our\nensemble learner substantially improves both macro and micro F1, from 10.4 to\n14.6 and from 58.2 to 59.1, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1\">Sunjae Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zonghai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Selective Masking as a Bridge between Pre-training and Fine-tuning. (arXiv:2211.13815v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13815","description":"<p>Pre-training a language model and then fine-tuning it for downstream tasks\nhas demonstrated state-of-the-art results for various NLP tasks. Pre-training\nis usually independent of the downstream task, and previous works have shown\nthat this pre-training alone might not be sufficient to capture the\ntask-specific nuances. We propose a way to tailor a pre-trained BERT model for\nthe downstream task via task-specific masking before the standard supervised\nfine-tuning. For this, a word list is first collected specific to the task. For\nexample, if the task is sentiment classification, we collect a small sample of\nwords representing both positive and negative sentiments. Next, a word's\nimportance for the task, called the word's task score, is measured using the\nword list. Each word is then assigned a probability of masking based on its\ntask score. We experiment with different masking functions that assign the\nprobability of masking based on the word's task score. The BERT model is\nfurther trained on MLM objective, where masking is done using the above\nstrategy. Following this standard supervised fine-tuning is done for different\ndownstream tasks. Results on these tasks show that the selective masking\nstrategy outperforms random masking, indicating its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lad_T/0/1/0/all/0/1\">Tanish Lad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_H/0/1/0/all/0/1\">Himanshu Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kottukkal_S/0/1/0/all/0/1\">Shreyas Kottukkal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Entities in the Astrophysics Literature: A Comparison of Word-based and Span-based Entity Recognition Methods. (arXiv:2211.13819v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13819","description":"<p>Information Extraction from scientific literature can be challenging due to\nthe highly specialised nature of such text. We describe our entity recognition\nmethods developed as part of the DEAL (Detecting Entities in the Astrophysics\nLiterature) shared task. The aim of the task is to build a system that can\nidentify Named Entities in a dataset composed by scholarly articles from\nastrophysics literature. We planned our participation such that it enables us\nto conduct an empirical comparison between word-based tagging and span-based\nclassification methods. When evaluated on two hidden test sets provided by the\norganizer, our best-performing submission achieved $F_1$ scores of 0.8307\n(validation phase) and 0.7990 (testing phase).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimi_S/0/1/0/all/0/1\">Sarvnaz Karimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ComCLIP: Training-Free Compositional Image and Text Matching. (arXiv:2211.13854v1 [cs.CV])","link":"http://arxiv.org/abs/2211.13854","description":"<p>Contrastive Language-Image Pretraining (CLIP) has demonstrated great\nzero-shot performance for image-text matching because of its holistic use of\nnatural language supervision that covers large-scale, open-world visual\nconcepts. However, it is still challenging to adapt CLIP to compositional image\nand text matching -- a more challenging image and matching mask requiring the\nmodel understanding of compositional word concepts and visual components.\nTowards better compositional generalization in zero-shot image and text\nmatching, in this paper, we study the problem from a causal perspective: the\nerroneous semantics of individual entities are essentially confounders that\ncause the matching failure. Therefore, we propose a novel training-free\ncompositional CLIP model (ComCLIP). ComCLIP disentangles input images into\nsubjects, objects, and action sub-images and composes CLIP's vision encoder and\ntext encoder to perform evolving matching over compositional text embedding and\nsub-image embeddings. In this way, ComCLIP can mitigate spurious correlations\nintroduced by the pretrained CLIP models and dynamically assess the\ncontribution of each entity when performing image and text matching.\nExperiments on compositional image-text matching on SVO and ComVG and general\nimage-text retrieval on Flickr8K demonstrate the effectiveness of our\nplug-and-play method, which boosts the zero-shot inference ability of CLIP even\nwithout further training or fine-tuning of CLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kenan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuehai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruize Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Competency-Aware Neural Machine Translation: Can Machine Translation Know its Own Translation Quality?. (arXiv:2211.13865v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13865","description":"<p>Neural machine translation (NMT) is often criticized for failures that happen\nwithout awareness. The lack of competency awareness makes NMT untrustworthy.\nThis is in sharp contrast to human translators who give feedback or conduct\nfurther investigations whenever they are in doubt about predictions. To fill\nthis gap, we propose a novel competency-aware NMT by extending conventional NMT\nwith a self-estimator, offering abilities to translate a source sentence and\nestimate its competency. The self-estimator encodes the information of the\ndecoding procedure and then examines whether it can reconstruct the original\nsemantics of the source sentence. Experimental results on four translation\ntasks demonstrate that the proposed method not only carries out translation\ntasks intact but also delivers outstanding performance on quality estimation.\nWithout depending on any reference or annotated data typically required by\nstate-of-the-art metric and quality estimation methods, our model yields an\neven higher correlation with human quality judgments than a variety of\naforementioned methods, such as BLEURT, COMET, and BERTScore. Quantitative and\nqualitative analyses show better robustness of competency awareness in our\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Haoran Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1\">Kai Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse Relation Recognition. (arXiv:2211.13873v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13873","description":"<p>Due to the absence of explicit connectives, implicit discourse relation\nrecognition (IDRR) remains a challenging task in discourse analysis. The\ncritical step for IDRR is to learn high-quality discourse relation\nrepresentations between two arguments. Recent methods tend to integrate the\nwhole hierarchical information of senses into discourse relation\nrepresentations for multi-level sense recognition. Nevertheless, they\ninsufficiently incorporate the static hierarchical structure containing all\nsenses (defined as global hierarchy), and ignore the hierarchical sense label\nsequence corresponding to each instance (defined as local hierarchy). For the\npurpose of sufficiently exploiting global and local hierarchies of senses to\nlearn better discourse relation representations, we propose a novel GLobal and\nLOcal Hierarchy-aware Contrastive Framework (GLOF), to model two kinds of\nhierarchies with the aid of contrastive learning. Experimental results on the\nPDTB dataset demonstrate that our method remarkably outperforms the current\nstate-of-the-art model at all hierarchical levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Silver Standard Data for Zero-shot Relation Extraction. (arXiv:2211.13883v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13883","description":"<p>The superior performance of supervised relation extraction (RE) methods\nheavily relies on a large amount of gold standard data. Recent zero-shot\nrelation extraction methods converted the RE task to other NLP tasks and used\noff-the-shelf models of these NLP tasks to directly perform inference on the\ntest data without using a large amount of RE annotation data. A potentially\nvaluable by-product of these methods is the large-scale silver standard data.\nHowever, there is no further investigation on the use of potentially valuable\nsilver standard data. In this paper, we propose to first detect a small amount\nof clean data from silver standard data and then use the selected clean data to\nfinetune the pretrained model. We then use the finetuned model to infer\nrelation types. We also propose a class-aware clean data detection module to\nconsider class information when selecting clean data. The experimental results\nshow that our method can outperform the baseline by 12% and 11% on TACRED and\nWiki80 dataset in the zero-shot RE task. By using extra silver standard data of\ndifferent distributions, the performance can be further improved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziqian Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TPA-Net: Generate A Dataset for Text to Physics-based Animation. (arXiv:2211.13887v1 [cs.AI])","link":"http://arxiv.org/abs/2211.13887","description":"<p>Recent breakthroughs in Vision-Language (V&amp;L) joint research have achieved\nremarkable results in various text-driven tasks. High-quality Text-to-video\n(T2V), a task that has been long considered mission-impossible, was proven\nfeasible with reasonably good results in latest works. However, the resulting\nvideos often have undesired artifacts largely because the system is purely\ndata-driven and agnostic to the physical laws. To tackle this issue and further\npush T2V towards high-level physical realism, we present an autonomous data\ngeneration technique and a dataset, which intend to narrow the gap with a large\nnumber of multi-modal, 3D Text-to-Video/Simulation (T2V/S) data. In the\ndataset, we provide high-resolution 3D physical simulations for both solids and\nfluids, along with textual descriptions of the physical phenomena. We take\nadvantage of state-of-the-art physical simulation methods (i) Incremental\nPotential Contact (IPC) and (ii) Material Point Method (MPM) to simulate\ndiverse scenarios, including elastic deformations, material fractures,\ncollisions, turbulence, etc. Additionally, high-quality, multi-view rendering\nvideos are supplied for the benefit of T2V, Neural Radiance Fields (NeRF), and\nother communities. This work is the first step towards fully automated\nText-to-Video/Simulation (T2V/S). Live examples and subsequent work are at\nhttps://sites.google.com/view/tpa-net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yuxing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chenfanfu Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complementary Explanations for Effective In-Context Learning. (arXiv:2211.13892v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13892","description":"<p>Large language models (LLMs) have exhibited remarkable capabilities in\nlearning from explanations in prompts. Yet, there has been limited\nunderstanding of what makes explanations effective for in-context learning.\nThis work aims to better understand the mechanisms by which explanations are\nused for in-context learning. We first study the impact of two different\nfactors on prompting performance when using explanations: the computation trace\n(the way the solution is decomposed) and the natural language of the prompt. By\nperturbing explanations on three controlled tasks, we show that both factors\ncontribute to the effectiveness of explanations, indicating that LLMs do\nfaithfully follow the explanations to some extent. We further study how to form\nmaximally effective sets of explanations for solving a given test query. We\nfind that LLMs can benefit from the complementarity of the explanation set as\nthey are able to fuse different reasoning specified by individual exemplars in\nprompts. Additionally, having relevant exemplars also contributes to more\neffective prompts. Therefore, we propose a maximal-marginal-relevance-based\nexemplar selection approach for constructing exemplar sets that are both\nrelevant as well as complementary, which successfully improves the in-context\nlearning performance across three real-world tasks on multiple LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Srinivasan Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Ves Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUSIED: A Benchmark for Event Detection from Multi-Source Heterogeneous Informal Texts. (arXiv:2211.13896v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13896","description":"<p>Event detection (ED) identifies and classifies event triggers from\nunstructured texts, serving as a fundamental task for information extraction.\nDespite the remarkable progress achieved in the past several years, most\nresearch efforts focus on detecting events from formal texts (e.g., news\narticles, Wikipedia documents, financial announcements). Moreover, the texts in\neach dataset are either from a single source or multiple yet relatively\nhomogeneous sources. With massive amounts of user-generated text accumulating\non the Web and inside enterprises, identifying meaningful events in these\ninformal texts, usually from multiple heterogeneous sources, has become a\nproblem of significant practical value. As a pioneering exploration that\nexpands event detection to the scenarios involving informal and heterogeneous\ntexts, we propose a new large-scale Chinese event detection dataset based on\nuser reviews, text conversations, and phone conversations in a leading\ne-commerce platform for food service. We carefully investigate the proposed\ndataset's textual informality and multi-source heterogeneity characteristics by\ninspecting data samples quantitatively and qualitatively. Extensive experiments\nwith state-of-the-art event detection methods verify the unique challenges\nposed by these characteristics, indicating that multi-source informal event\ndetection remains an open problem and requires further efforts. Our benchmark\nand code are released at \\url{https://github.com/myeclipse/MUSIED}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xi_X/0/1/0/all/0/1\">Xiangyu Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jianwei Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaipeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1\">Guanglu Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison Study Between Token Classification and Sequence Classification In Text Classification. (arXiv:2211.13899v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13899","description":"<p>Unsupervised Machine Learning techniques have been applied to Natural\nLanguage Processing tasks and surpasses the benchmarks such as GLUE with great\nsuccess. Building language models approach achieves good results in one\nlanguage and it can be applied to multiple NLP task such as classification,\nsummarization, generation and etc as an out of box model. Among all the of the\nclassical approaches used in NLP, the masked language modeling is the most\nused. In general, the only requirement to build a language model is presence of\nthe large corpus of textual data. Text classification engines uses a variety of\nmodels from classical and state of art transformer models to classify texts for\nin order to save costs. Sequence Classifiers are mostly used in the domain of\ntext classification. However Token classifiers also are viable candidate models\nas well. Sequence Classifiers and Token Classifier both tend to improve the\nclassification predictions due to the capturing the context information\ndifferently. This work aims to compare the performance of Sequence Classifier\nand Token Classifiers and evaluate each model on the same set of data. In this\nwork, we are using a pre-trained model as the base model and Token Classifier\nand Sequence Classier heads results of these two scoring paradigms with be\ncompared..\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jafari_A/0/1/0/all/0/1\">Amir Jafari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning Anomaly Detection Method in Textual Data. (arXiv:2211.13900v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13900","description":"<p>In this article, we propose using deep learning and transformer architectures\ncombined with classical machine learning algorithms to detect and identify text\nanomalies in texts. Deep learning model provides a very crucial context\ninformation about the textual data which all textual context are converted to a\nnumerical representation. We used multiple machine learning methods such as\nSentence Transformers, Auto Encoders, Logistic Regression and Distance\ncalculation methods to predict anomalies. The method are tested on the texts\ndata and we used syntactic data from different source injected into the\noriginal text as anomalies or use them as target. Different methods and\nalgorithm are explained in the field of outlier detection and the results of\nthe best technique is presented. These results suggest that our algorithm could\npotentially reduce false positive rates compared with other anomaly detection\nmethods that we are testing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jafari_A/0/1/0/all/0/1\">Amir Jafari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRAC: A Textual Benchmark for Reasoning about Actions and Change. (arXiv:2211.13930v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13930","description":"<p>Reasoning about actions and change (RAC) is essential to understand and\ninteract with the ever-changing environment. Previous AI research has shown the\nimportance of fundamental and indispensable knowledge of actions, i.e.,\npreconditions and effects. However, traditional methods rely on logical\nformalization which hinders practical applications. With recent\ntransformer-based language models (LMs), reasoning over text is desirable and\nseemingly feasible, leading to the question of whether LMs can effectively and\nefficiently learn to solve RAC problems. We propose four essential RAC tasks as\na comprehensive textual benchmark and generate problems in a way that minimizes\nthe influence of other linguistic requirements (e.g., grounding) to focus on\nRAC. The resulting benchmark, TRAC, encompassing problems of various\ncomplexities, facilitates a more granular evaluation of LMs, precisely\ntargeting the structural generalization ability much needed for RAC.\nExperiments with three high-performing transformers indicates that additional\nefforts are needed to tackle challenges raised by TRAC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Weinan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Canming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhanhao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongmei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEAP-FAKED: Knowledge Graph based Approach for Fake News Detection. (arXiv:2107.10648v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.10648","description":"<p>Fake News on social media platforms has attracted a lot of attention in\nrecent times, primarily for events related to politics (2016 US Presidential\nelections), healthcare (infodemic during COVID-19), to name a few. Various\nmethods have been proposed for detecting Fake News. The approaches span from\nexploiting techniques related to network analysis, Natural Language Processing\n(NLP), and the usage of Graph Neural Networks (GNNs). In this work, we propose\nDEAP-FAKED, a knowleDgE grAPh FAKe nEws Detection framework for identifying\nFake News. Our approach is a combination of the NLP -- where we encode the news\ncontent, and the GNN technique -- where we encode the Knowledge Graph (KG). A\nvariety of these encodings provides a complementary advantage to our detector.\nWe evaluate our framework using two publicly available datasets containing\narticles from domains such as politics, business, technology, and healthcare.\nAs part of dataset pre-processing, we also remove the bias, such as the source\nof the articles, which could impact the performance of the models. DEAP-FAKED\nobtains an F1-score of 88% and 78% for the two datasets, which is an\nimprovement of 21%, and 3% respectively, which shows the effectiveness of the\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mayank_M/0/1/0/all/0/1\">Mohit Mayank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shakshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rajesh Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comparison of latent semantic analysis and correspondence analysis of document-term matrices. (arXiv:2108.06197v4 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.06197","description":"<p>Latent semantic analysis (LSA) and correspondence analysis (CA) are two\ntechniques that use a singular value decomposition (SVD) for dimensionality\nreduction. LSA has been extensively used to obtain low-dimensional\nrepresentations that capture relationships among documents and terms. In this\narticle, we present a theoretical analysis and comparison of the two techniques\nin the context of document-term matrices. We show that CA has some attractive\nproperties as compared to LSA, for instance that effects of margins, i.e. sums\nof row elements and column elements, arising from differing document-lengths\nand term-frequencies are effectively eliminated, so that the CA solution is\noptimally suited to focus on relationships among documents and terms. A\nunifying framework is proposed that includes both CA and LSA as special cases.\nWe empirically compare CA to various LSA based methods on text categorization\nin English and authorship attribution on historical Dutch texts, and find that\nCA performs significantly better. We also apply CA to a long-standing question\nregarding the authorship of the Dutch national anthem Wilhelmus and provide\nfurther support that it can be attributed to the author Datheen, amongst\nseveral contenders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qianqian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessen_D/0/1/0/all/0/1\">David J. Hessen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deoskar_T/0/1/0/all/0/1\">Tejaswini Deoskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heijden_P/0/1/0/all/0/1\">Peter G. M. van der Heijden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Misleading the Covid-19 vaccination discourse on Twitter: An exploratory study of infodemic around the pandemic. (arXiv:2108.10735v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10735","description":"<p>In this work, we collect a moderate-sized representative corpus of tweets\n(200,000 approx.) pertaining Covid-19 vaccination spanning over a period of\nseven months (September 2020 - March 2021). Following a Transfer Learning\napproach, we utilize the pre-trained Transformer-based XLNet model to classify\ntweets as Misleading or Non-Misleading and validate against a random subset of\nresults manually. We build on this to study and contrast the characteristics of\ntweets in the corpus that are misleading in nature against non-misleading ones.\nThis exploratory analysis enables us to design features (such as sentiments,\nhashtags, nouns, pronouns, etc) that can, in turn, be exploited for classifying\ntweets as (Non-)Misleading using various ML models in an explainable manner.\nSpecifically, several ML models are employed for prediction, with up to 90%\naccuracy, and the importance of each feature is explained using SHAP\nExplainable AI (XAI) tool. While the thrust of this work is principally\nexploratory analysis in order to obtain insights on the online discourse on\nCovid-19 vaccination, we conclude the paper by outlining how these insights\nprovide the foundations for a more actionable approach to mitigate\nmisinformation. The curated dataset and code is made available (Github\nrepository) so that the research community at large can reproduce, compare\nagainst, or build upon this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shakshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rajesh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1\">Anwitaman Datta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Universal Intrinsic Task Subspace via Prompt Tuning. (arXiv:2110.07867v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07867","description":"<p>Why can pre-trained language models (PLMs) learn universal representations\nand effectively adapt to broad NLP tasks differing a lot superficially? In this\nwork, we empirically find evidence indicating that the adaptations of PLMs to\nvarious few-shot tasks can be reparameterized as optimizing only a few free\nparameters in a unified low-dimensional intrinsic task subspace, which may help\nus understand why PLMs could easily adapt to various NLP tasks with small-scale\ndata. To find such a subspace and examine its universality, we propose an\nanalysis pipeline called intrinsic prompt tuning (IPT). Specifically, we resort\nto the recent success of prompt tuning and decompose the soft prompts of\nmultiple NLP tasks into the same low-dimensional nonlinear subspace, then we\nlearn to adapt the PLM to unseen data or tasks by only tuning parameters in\nthis subspace. In the experiments, we study diverse few-shot NLP tasks and\nsurprisingly find that in a 250-dimensional subspace found with 100 tasks, by\nonly tuning 250 free parameters, we can recover 97% and 83% of the full prompt\ntuning performance for 100 seen tasks (using different training data) and 20\nunseen tasks, respectively, showing great generalization ability of the found\nintrinsic task subspace. Besides being an analysis tool, IPT could further help\nus improve the prompt tuning stability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yusheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jing Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NewsClaims: A New Benchmark for Claim Detection from News with Attribute Knowledge. (arXiv:2112.08544v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08544","description":"<p>Claim detection and verification are crucial for news understanding and have\nemerged as promising technologies for mitigating misinformation and\ndisinformation in the news. However, most existing work has focused on claim\nsentence analysis while overlooking additional crucial attributes (e.g., the\nclaimer and the main object associated with the claim). In this work, we\npresent NewsClaims, a new benchmark for attribute-aware claim detection in the\nnews domain. We extend the claim detection problem to include extraction of\nadditional attributes related to each claim and release 889 claims annotated\nover 143 news articles. NewsClaims aims to benchmark claim detection systems in\nemerging scenarios, comprising unseen topics with little or no training data.\nTo this end, we see that zero-shot and prompt-based baselines show promising\nperformance on this benchmark, while still considerably behind human\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Revanth Gangi Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chetan_S/0/1/0/all/0/1\">Sai Chetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_Y/0/1/0/all/0/1\">Yi R. Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conger_K/0/1/0/all/0/1\">Kathryn Conger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_A/0/1/0/all/0/1\">Ahmed Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1\">Martha Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Small_K/0/1/0/all/0/1\">Kevin Small</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal effect of racial bias in data and machine learning algorithms on user persuasiveness & discriminatory decision making: An Empirical Study. (arXiv:2202.00471v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00471","description":"<p>Language data and models demonstrate various types of bias, be it ethnic,\nreligious, gender, or socioeconomic. AI/NLP models, when trained on the\nracially biased dataset, AI/NLP models instigate poor model explainability,\ninfluence user experience during decision making and thus further magnifies\nsocietal biases, raising profound ethical implications for society. The\nmotivation of the study is to investigate how AI systems imbibe bias from data\nand produce unexplainable discriminatory outcomes and influence an individual's\narticulateness of system outcome due to the presence of racial bias features in\ndatasets. The design of the experiment involves studying the counterfactual\nimpact of racial bias features present in language datasets and its associated\neffect on the model outcome. A mixed research methodology is adopted to\ninvestigate the cross implication of biased model outcome on user experience,\neffect on decision-making through controlled lab experimentation. The findings\nprovide foundation support for correlating the implication of carry-over an\nartificial intelligence model solving NLP task due to biased concept presented\nin the dataset. Further, the research outcomes justify the negative influence\non users' persuasiveness that leads to alter the decision-making quotient of an\nindividual when trying to rely on the model outcome to act. The paper bridges\nthe gap across the harm caused in establishing poor customer trustworthiness\ndue to an inequitable system design and provides strong support for\nresearchers, policymakers, and data scientists to build responsible AI\nframeworks within organizations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_K/0/1/0/all/0/1\">Kinshuk Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1\">Praveen Ranjan Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Failures of Large Language Models via Human Cognitive Biases. (arXiv:2202.12299v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12299","description":"<p>Large language models generate complex, open-ended outputs: instead of\noutputting a class label they write summaries, generate dialogue, or produce\nworking code. In order to asses the reliability of these open-ended generation\nsystems, we aim to identify qualitative categories of erroneous behavior,\nbeyond identifying individual errors. To hypothesize and test for such\nqualitative errors, we draw inspiration from human cognitive biases --\nsystematic patterns of deviation from rational judgement. Specifically, we use\ncognitive biases as motivation to (i) generate hypotheses for problems that\nmodels may have, and (ii) develop experiments that elicit these problems. Using\ncode generation as a case study, we find that OpenAI's Codex errs predictably\nbased on how the input prompt is framed, adjusts outputs towards anchors, and\nis biased towards outputs that mimic frequent training examples. We then use\nour framework to elicit high-impact errors such as incorrectly deleting files.\nOur results indicate that experimental methodology from cognitive science can\nhelp characterize how machine learning systems behave.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1\">Erik Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transition-based Semantic Role Labeling with Pointer Networks. (arXiv:2205.10023v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10023","description":"<p>Semantic role labeling (SRL) focuses on recognizing the predicate-argument\nstructure of a sentence and plays a critical role in many natural language\nprocessing tasks such as machine translation and question answering.\nPractically all available methods do not perform full SRL, since they rely on\npre-identified predicates, and most of them follow a pipeline strategy, using\nspecific models for undertaking one or several SRL subtasks. In addition,\nprevious approaches have a strong dependence on syntactic information to\nachieve state-of-the-art performance, despite being syntactic trees equally\nhard to produce. These simplifications and requirements make the majority of\nSRL systems impractical for real-world applications. In this article, we\npropose the first transition-based SRL approach that is capable of completely\nprocessing an input sentence in a single left-to-right pass, with neither\nleveraging syntactic information nor resorting to additional modules. Thanks to\nour implementation based on Pointer Networks, full SRL can be accurately and\nefficiently done in $O(n^2)$, achieving the best performance to date on the\nmajority of languages from the CoNLL-2009 shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1\">Daniel Fern&#xe1;ndez-Gonz&#xe1;lez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI. (arXiv:2205.11029v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11029","description":"<p>Task-oriented dialogue (TOD) systems have been widely used by mobile phone\nintelligent assistants to accomplish tasks such as calendar scheduling or hotel\nreservation. Current TOD systems usually focus on multi-turn text/speech\ninteraction, then they would call back-end APIs designed for TODs to perform\nthe task. However, this API-based architecture greatly limits the\ninformation-searching capability of intelligent assistants and may even lead to\ntask failure if TOD-specific APIs are not available or the task is too\ncomplicated to be executed by the provided APIs. In this paper, we propose a\nnew TOD architecture: GUI-based task-oriented dialogue system (GUI-TOD). A\nGUI-TOD system can directly perform GUI operations on real APPs and execute\ntasks without invoking TOD-specific backend APIs. Furthermore, we release\nMETA-GUI, a dataset for training a Multi-modal convErsaTional Agent on mobile\nGUI. We also propose a multi-model action prediction and response model, which\nshow promising results on META-GUI. The dataset, codes and leaderboard are\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Liangtai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Tianle Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zichen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SsciBERT: A Pre-trained Language Model for Social Science Texts. (arXiv:2206.04510v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.04510","description":"<p>The academic literature of social sciences records human civilization and\nstudies human social problems. With its large-scale growth, the ways to quickly\nfind existing research on relevant issues have become an urgent demand for\nresearchers. Previous studies, such as SciBERT, have shown that pre-training\nusing domain-specific texts can improve the performance of natural language\nprocessing tasks. However, the pre-trained language model for social sciences\nis not available so far. In light of this, the present research proposes a\npre-trained model based on the abstracts published in the Social Science\nCitation Index (SSCI) journals. The models, which are available on GitHub\n(https://github.com/S-T-Full-Text-Knowledge-Mining/SSCI-BERT), show excellent\nperformance on discipline classification, abstract structure-function\nrecognition, and named entity recognition tasks with the social sciences\nliterature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Si Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiangfeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Litao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Ying Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongbo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08657","description":"<p>Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a deep cross-modal encoder, or feed the\nlast-layer uni-modal representations from the deep pre-trained uni-modal\nencoders into the top cross-modal encoder. Both approaches potentially restrict\nvision-language representation learning and limit model performance. In this\npaper, we propose Bridge-Tower, which introduces multiple bridge layers that\nbuild a connection between the top layers of uni-modal encoders and each layer\nof the cross-modal encoder. This enables effective bottom-up cross-modal\nalignment and fusion between visual and textual representations of different\nsemantic levels of pre-trained uni-modal encoders in the cross-modal encoder.\nPre-trained with only 4M images, Bridge-Tower achieves state-of-the-art\nperformance on various downstream vision-language tasks. In particular, on the\nVQAv2 test-std set, Bridge-Tower achieves an accuracy of 78.73%, outperforming\nthe previous state-of-the-art model METER by 1.09% with the same pre-training\ndata and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, Bridge-Tower achieves an accuracy of\n81.15%, surpassing models that are pre-trained on orders-of-magnitude larger\ndatasets. Code and checkpoints are available at\n\\url{https://github.com/microsoft/BridgeTower}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenman_S/0/1/0/all/0/1\">Shachar Rosenman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks. (arXiv:2206.09059v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.09059","description":"<p>Current state-of-the-art vision-and-language models are evaluated on tasks\neither individually or in a multi-task setting, overlooking the challenges of\ncontinually learning (CL) tasks as they arrive. Existing CL benchmarks have\nfacilitated research on task adaptation and mitigating \"catastrophic\nforgetting\", but are limited to vision-only and language-only tasks. We present\nCLiMB, a benchmark to study the challenge of learning multimodal tasks in a CL\nsetting, and to systematically evaluate how upstream continual learning can\nrapidly generalize to new multimodal and unimodal tasks. CLiMB includes\nimplementations of several CL algorithms and a modified Vision-Language\nTransformer (ViLT) model that can be deployed on both multimodal and unimodal\ntasks. We find that common CL methods can help mitigate forgetting during\nmultimodal task learning, but do not enable cross-task knowledge transfer. We\nenvision that CLiMB will facilitate research on a new class of CL algorithms\nfor this challenging multimodal setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_T/0/1/0/all/0/1\">Tejas Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Ting-Yun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alva_L/0/1/0/all/0/1\">Leticia Leonor Pinto Alva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chochlakis_G/0/1/0/all/0/1\">Georgios Chochlakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents. (arXiv:2207.01206v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.01206","description":"<p>Existing benchmarks for grounding language in interactive environments either\nlack real-world linguistic elements, or prove difficult to scale up due to\nsubstantial human involvement in the collection of data or feedback signals. To\nbridge this gap, we develop WebShop -- a simulated e-commerce website\nenvironment with $1.18$ million real-world products and $12,087$ crowd-sourced\ntext instructions. Given a text instruction specifying a product requirement,\nan agent needs to navigate multiple types of webpages and issue diverse actions\nto find, customize, and purchase an item. WebShop provides several challenges\nfor language grounding including understanding compositional instructions,\nquery (re-)formulation, comprehending and acting on noisy text in webpages, and\nperforming strategic exploration. We collect over $1,600$ human demonstrations\nfor the task, and train and evaluate a diverse range of agents using\nreinforcement learning, imitation learning, and pre-trained image and language\nmodels. Our best model achieves a task success rate of $29\\%$, which\noutperforms rule-based heuristics ($9.6\\%$) but is far lower than human expert\nperformance ($59\\%$). We also analyze agent and human trajectories and ablate\nvarious model components to provide insights for developing future agents with\nstronger language understanding and decision making abilities. Finally, we show\nthat agents trained on WebShop exhibit non-trivial sim-to-real transfer when\nevaluated on amazon.com and ebay.com, indicating the potential value of WebShop\nin developing practical web-based agents that can operate in the wild.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Howard Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">John Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DHGE: Dual-view Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing. (arXiv:2207.08562v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2207.08562","description":"<p>In the field of representation learning on knowledge graphs (KGs), a\nhyper-relational fact consists of a main triple and several auxiliary\nattribute-value descriptions, which is considered more comprehensive and\nspecific than a triple-based fact. However, currently available\nhyper-relational KG embedding methods in a single view are limited in\napplication because they weaken the hierarchical structure that represents the\naffiliation between entities. To overcome this limitation, we propose a\ndual-view hyper-relational KG structure (DH-KG) that contains a\nhyper-relational instance view for entities and a hyper-relational ontology\nview for concepts that are abstracted hierarchically from the entities. This\npaper defines link prediction and entity typing tasks on DH-KG for the first\ntime and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and\nHTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding\nmodel based on GRAN encoders, HGNNs, and joint learning. DHGE outperforms\nbaseline models on DH-KG, according to experimental results. Finally, we\nprovide an example of how this technology can be used to treat hypertension.\nOur model and new datasets are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haoran Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+E_H/0/1/0/all/0/1\">Haihong E</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Ling Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Gengxian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Tianyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_K/0/1/0/all/0/1\">Kaiyang Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GSRFormer: Grounded Situation Recognition Transformer with Alternate Semantic Attention Refinement. (arXiv:2208.08965v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2208.08965","description":"<p>Grounded Situation Recognition (GSR) aims to generate structured semantic\nsummaries of images for \"human-like\" event understanding. Specifically, GSR\ntask not only detects the salient activity verb (e.g. buying), but also\npredicts all corresponding semantic roles (e.g. agent and goods). Inspired by\nobject detection and image captioning tasks, existing methods typically employ\na two-stage framework: 1) detect the activity verb, and then 2) predict\nsemantic roles based on the detected verb. Obviously, this illogical framework\nconstitutes a huge obstacle to semantic understanding. First, pre-detecting\nverbs solely without semantic roles inevitably fails to distinguish many\nsimilar daily activities (e.g., offering and giving, buying and selling).\nSecond, predicting semantic roles in a closed auto-regressive manner can hardly\nexploit the semantic relations among the verb and roles. To this end, in this\npaper we propose a novel two-stage framework that focuses on utilizing such\nbidirectional relations within verbs and roles. In the first stage, instead of\npre-detecting the verb, we postpone the detection step and assume a pseudo\nlabel, where an intermediate representation for each corresponding semantic\nrole is learned from images. In the second stage, we exploit transformer layers\nto unearth the potential semantic relations within both verbs and semantic\nroles. With the help of a set of support images, an alternate learning scheme\nis designed to simultaneously optimize the results: update the verb using nouns\ncorresponding to the image, and update nouns using verbs from support images.\nExtensive experimental results on challenging SWiG benchmarks show that our\nrenovated framework outperforms other state-of-the-art methods under various\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhi-Qi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Alexander Hauptmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PreSTU: Pre-Training for Scene-Text Understanding. (arXiv:2209.05534v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2209.05534","description":"<p>The ability to recognize and reason about text embedded in visual inputs is\noften lacking in vision-and-language (V&amp;L) models, perhaps because V&amp;L\npre-training methods have often failed to include such an ability as their\ntraining objective. In this paper, we propose PreSTU, a novel pre-training\nrecipe dedicated to scene-text understanding (STU). PreSTU introduces OCR-aware\npre-training objectives that encourage the model to recognize text from an\nimage and to connect what is recognized to the rest of the image content. We\nimplement PreSTU using a simple transformer-based encoder-decoder architecture,\ncombined with large-scale image-text datasets with scene text obtained from an\noff-the-shelf OCR system. We empirically demonstrate the effectiveness of this\npre-training approach on four visual question answering and two image\ncaptioning benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kil_J/0/1/0/all/0/1\">Jihyung Kil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_S/0/1/0/all/0/1\">Sebastian Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization. (arXiv:2210.01241v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01241","description":"<p>We tackle the problem of aligning pre-trained large language models (LMs)\nwith human preferences. If we view text generation as a sequential\ndecision-making problem, reinforcement learning (RL) appears to be a natural\nconceptual framework. However, using RL for LM-based generation faces empirical\nchallenges, including training instability due to the combinatorial action\nspace, as well as a lack of open-source libraries and benchmarks customized for\nLM alignment. Thus, a question rises in the research community: is RL a\npractical paradigm for NLP?\n</p>\n<p>To help answer this, we first introduce an open-source modular library,\nRL4LMs (Reinforcement Learning for Language Models), for optimizing language\ngenerators with RL. The library consists of on-policy RL algorithms that can be\nused to train any encoder or encoder-decoder LM in the HuggingFace library\n(Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE\n(General Reinforced-language Understanding Evaluation) benchmark, a set of 6\nlanguage generation tasks which are supervised not by target strings, but by\nreward functions which capture automated measures of human preference.GRUE is\nthe first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally,\nwe introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language\nPolicy Optimization)} that learns to effectively reduce the combinatorial\naction space in language generation. We show 1) that RL techniques are\ngenerally better than supervised methods at aligning LMs to human preferences;\nand 2) that NLPO exhibits greater stability and performance than previous\npolicy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both\nautomatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramamurthy_R/0/1/0/all/0/1\">Rajkumar Ramamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1\">Kiant&#xe9; Brantley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifa_R/0/1/0/all/0/1\">Rafet Sifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauckhage_C/0/1/0/all/0/1\">Christian Bauckhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue. (arXiv:2210.07783v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07783","description":"<p>Lifelong learning (LL) is vital for advanced task-oriented dialogue (ToD)\nsystems. To address the catastrophic forgetting issue of LL, generative replay\nmethods are widely employed to consolidate past knowledge with generated pseudo\nsamples. However, most existing generative replay methods use only a single\ntask-specific token to control their models. This scheme is usually not strong\nenough to constrain the generative model due to insufficient information\ninvolved. In this paper, we propose a novel method, prompt conditioned VAE for\nlifelong learning (PCLL), to enhance generative replay by incorporating tasks'\nstatistics. PCLL captures task-specific distributions with a conditional\nvariational autoencoder, conditioned on natural language prompts to guide the\npseudo-sample generation. Moreover, it leverages a distillation process to\nfurther consolidate past knowledge by alleviating the noise in pseudo samples.\nExperiments on natural language understanding tasks of ToD systems demonstrate\nthat PCLL significantly outperforms competitive baselines in building LL\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingxiu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhiliang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU. (arXiv:2210.12499v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12499","description":"<p>Curriculum Learning (CL) is a technique of training models via ranking\nexamples in a typically increasing difficulty trend with the aim of\naccelerating convergence and improving generalisability. Current approaches for\nNatural Language Understanding (NLU) tasks use CL to improve in-distribution\ndata performance often via heuristic-oriented or task-agnostic difficulties. In\nthis work, instead, we employ CL for NLU by taking advantage of training\ndynamics as difficulty metrics, i.e., statistics that measure the behavior of\nthe model at hand on specific task-data instances during training and propose\nmodifications of existing CL schedulers based on these statistics. Differently\nfrom existing works, we focus on evaluating models on in-distribution (ID),\nout-of-distribution (OOD) as well as zero-shot (ZS) cross-lingual transfer\ndatasets. We show across several NLU tasks that CL with training dynamics can\nresult in better performance mostly on zero-shot cross-lingual transfer and OOD\nsettings with improvements up by 8.5% in certain cases. Overall, experiments\nindicate that training dynamics can lead to better performing models with\nsmoother training compared to other difficulty metrics while being 20% faster\non average. In addition, through analysis we shed light on the correlations of\ntask-specific versus task-agnostic metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christopoulou_F/0/1/0/all/0/1\">Fenia Christopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampouras_G/0/1/0/all/0/1\">Gerasimos Lampouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iacobacci_I/0/1/0/all/0/1\">Ignacio Iacobacci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Joint Training Really Help Cascaded Speech Translation?. (arXiv:2210.13700v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2210.13700","description":"<p>Currently, in speech translation, the straightforward approach - cascading a\nrecognition system with a translation system - delivers state-of-the-art\nresults. However, fundamental challenges such as error propagation from the\nautomatic speech recognition system still remain. To mitigate these problems,\nrecently, people turn their attention to direct data and propose various joint\ntraining methods. In this work, we seek to answer the question of whether joint\ntraining really helps cascaded speech translation. We review recent papers on\nthe topic and also investigate a joint training criterion by marginalizing the\ntranscription posterior probabilities. Our findings show that a strong cascaded\nbaseline can diminish any improvements obtained using joint training, and we\nsuggest alternatives to joint training. We hope this work can serve as a\nrefresher of the current speech translation landscape, and motivate research in\nfinding more efficient and creative ways to utilize the direct data for speech\ntranslation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tran_V/0/1/0/all/0/1\">Viet Anh Khoa Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thulke_D/0/1/0/all/0/1\">David Thulke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yingbo Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herold_C/0/1/0/all/0/1\">Christian Herold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COCO-DR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning. (arXiv:2210.15212v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15212","description":"<p>We present a new zero-shot dense retrieval (ZeroDR) method, COCO-DR, to\nimprove the generalization ability of dense retrieval by combating the\ndistribution shifts between source training tasks and target scenarios. To\nmitigate the impact of document differences, COCO-DR continues pretraining the\nlanguage model on the target corpora to adapt the model to target distributions\nvia COtinuous COtrastive learning. To prepare for unseen target queries,\nCOCO-DR leverages implicit Distributionally Robust Optimization (iDRO) to\nreweight samples from different source query clusters for improving model\nrobustness over rare queries during fine-tuning. COCO-DR achieves superior\naverage performance on BEIR, the zero-shot retrieval benchmark. At BERT Base\nscale, COCO-DR Base outperforms other ZeroDR models with 60x larger size. At\nBERT Large scale, COCO-DR Large outperforms the giant GPT-3 embedding model\nwhich has 500x more parameters. Our analysis show the correlation between\nCOCO-DR's effectiveness in combating distribution shifts and improving\nzero-shot accuracy. Our code and model can be found at\n\\url{https://github.com/OpenMatch/COCO-DR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Si Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Overwijk_A/0/1/0/all/0/1\">Arnold Overwijk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages. (arXiv:2211.03263v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.03263","description":"<p>In recent years, multilingual pre-trained language models have gained\nprominence due to their remarkable performance on numerous downstream Natural\nLanguage Processing tasks (NLP). However, pre-training these large multilingual\nlanguage models requires a lot of training data, which is not available for\nAfrican Languages. Active learning is a semi-supervised learning algorithm, in\nwhich a model consistently and dynamically learns to identify the most\nbeneficial samples to train itself on, in order to achieve better optimization\nand performance on downstream tasks. Furthermore, active learning effectively\nand practically addresses real-world data scarcity. Despite all its benefits,\nactive learning, in the context of NLP and especially multilingual language\nmodels pretraining, has received little consideration. In this paper, we\npresent AfroLM, a multilingual language model pretrained from scratch on 23\nAfrican languages (the largest effort to date) using our novel self-active\nlearning framework. Pretrained on a dataset significantly (14x) smaller than\nexisting baselines, AfroLM outperforms many multilingual pretrained language\nmodels (AfriBERTa, XLMR-base, mBERT) on various NLP downstream tasks (NER, text\nclassification, and sentiment analysis). Additional out-of-domain sentiment\nanalysis experiments show that \\textbf{AfroLM} is able to generalize well\nacross various domains. We release the code source, and our datasets used in\nour framework at https://github.com/bonaventuredossou/MLM_AL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonja_A/0/1/0/all/0/1\">Atnafu Lambebo Tonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousuf_O/0/1/0/all/0/1\">Oreen Yousuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1\">Salomey Osei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oppong_A/0/1/0/all/0/1\">Abigail Oppong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shode_I/0/1/0/all/0/1\">Iyanuoluwa Shode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awoyomi_O/0/1/0/all/0/1\">Oluwabusayo Olufunke Awoyomi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSCD-IME: Correcting Spelling Errors Generated by Pinyin IME. (arXiv:2211.08788v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08788","description":"<p>Chinese Spelling Correction (CSC) is a task to detect and correct spelling\nmistakes in texts. In fact, most of Chinese input is based on pinyin input\nmethod, so the study of spelling errors in this process is more practical and\nvaluable. However, there is still no research dedicated to this essential\nscenario. In this paper, we first present a Chinese Spelling Correction Dataset\nfor errors generated by pinyin IME (CSCD-IME), including 40,000 annotated\nsentences from real posts of official media on Sina Weibo. Furthermore, we\npropose a novel method to automatically construct large-scale and high-quality\npseudo data by simulating the input through pinyin IME. A series of analyses\nand experiments on CSCD-IME show that spelling errors produced by pinyin IME\nhold a particular distribution at pinyin level and semantic level and are\nchallenging enough. Meanwhile, our proposed pseudo-data construction method can\nbetter fit this error distribution and improve the performance of CSC systems.\nFinally, we provide a useful guide to using pseudo data, including the data\nscale, the data source, and the training strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness. (arXiv:2211.11109v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11109","description":"<p>Data-driven predictive solutions predominant in commercial applications tend\nto suffer from biases and stereotypes, which raises equity concerns. Prediction\nmodels may discover, use, or amplify spurious correlations based on gender or\nother protected personal characteristics, thus discriminating against\nmarginalized groups. Mitigating gender bias has become an important research\nfocus in natural language processing (NLP) and is an area where annotated\ncorpora are available. Data augmentation reduces gender bias by adding\ncounterfactual examples to the training dataset. In this work, we show that\nsome of the examples in the augmented dataset can be not important or even\nharmful for fairness. We hence propose a general method for pruning both the\nfactual and counterfactual examples to maximize the model's fairness as\nmeasured by the demographic parity, equality of opportunity, and equality of\nodds. The fairness achieved by our method surpasses that of data augmentation\non three text classification datasets, using no more than half of the examples\nin the augmented dataset. Our experiments are conducted using models of varying\nsizes and pre-training settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zayed_A/0/1/0/all/0/1\">Abdelrahman Zayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1\">Prasanna Parthasarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordido_G/0/1/0/all/0/1\">Goncalo Mordido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabanian_S/0/1/0/all/0/1\">Samira Shabanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BotSIM: An End-to-End Bot Simulation Framework for Commercial Task-Oriented Dialog Systems. (arXiv:2211.11982v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11982","description":"<p>We present BotSIM, a data-efficient end-to-end Bot SIMulation toolkit for\ncommercial text-based task-oriented dialog (TOD) systems. BotSIM consists of\nthree major components: 1) a Generator that can infer semantic-level dialog\nacts and entities from bot definitions and generate user queries via\nmodel-based paraphrasing; 2) an agenda-based dialog user Simulator (ABUS) to\nsimulate conversations with the dialog agents; 3) a Remediator to analyze the\nsimulated conversations, visualize the bot health reports and provide\nactionable remediation suggestions for bot troubleshooting and improvement. We\ndemonstrate BotSIM's effectiveness in end-to-end evaluation, remediation and\nmulti-intent dialog generation via case studies on two commercial bot\nplatforms. BotSIM's \"generation-simulation-remediation\" paradigm accelerates\nthe end-to-end bot evaluation and iteration process by: 1) reducing manual test\ncases creation efforts; 2) enabling a holistic gauge of the bot in terms of NLU\nand end-to-end performance via extensive dialog simulation; 3) improving the\nbot troubleshooting process with actionable suggestions. A demo of our system\ncan be found at https://tinyurl.com/mryu74cd and a demo video at\nhttps://youtu.be/qLi5iSoly30.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangsen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Au_J/0/1/0/all/0/1\">Jimmy Au</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HaRiM$^+$: Evaluating Summary Quality with Hallucination Risk. (arXiv:2211.12118v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.12118","description":"<p>One of the challenges of developing a summarization model arises from the\ndifficulty in measuring the factual inconsistency of the generated text. In\nthis study, we reinterpret the decoder overconfidence-regularizing objective\nsuggested in (Miao et al., 2021) as a hallucination risk measurement to better\nestimate the quality of generated summaries. We propose a reference-free\nmetric, HaRiM+, which only requires an off-the-shelf summarization model to\ncompute the hallucination risk based on token likelihoods. Deploying it\nrequires no additional training of models or ad-hoc modules, which usually need\nalignment to human judgments. For summary-quality estimation, HaRiM+ records\nstate-of-the-art correlation to human judgment on three summary-quality\nannotation sets: FRANK, QAGS, and SummEval. We hope that our work, which merits\nthe use of summarization models, facilitates the progress of both automated\nevaluation and generation of summary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Seonil Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junsoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jeong-in Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junghwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_H/0/1/0/all/0/1\">Hyungjong Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yeonsoo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. (arXiv:2211.12588v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.12588","description":"<p>Recently, there has been significant progress in teaching language models to\nperform step-by-step reasoning to solve complex numerical reasoning tasks.\nChain-of-thoughts prompting (CoT) is by far the state-of-art method for these\ntasks. CoT uses language models to perform both reasoning and computation in\nthe multi-step `thought' process. To disentangle computation from reasoning, we\npropose `Program of Thoughts' (PoT), which uses language models (mainly Codex)\nto express the reasoning process as a program. The computation is relegated to\nan external computer, which executes the generated programs to derive the\nanswer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP,\nTabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA)\nfor both few-shot and zero-shot setups. Under both few-shot and zero-shot\nsettings, PoT can show an average performance gain over CoT by around 12\\%\nacross all the evaluated datasets. By combining PoT with self-consistency\ndecoding, we can achieve SoTA performance on all math problem datasets and\nnear-SoTA performance on financial datasets. All of our data and code are\nreleased in\nGithub\\footnote{\\url{https://github.com/wenhuchen/Program-of-Thoughts}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xueguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/"}}]}]}