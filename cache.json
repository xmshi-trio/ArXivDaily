{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-07-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Schema-learning and rebinding as mechanisms of in-context learning and emergence. (arXiv:2307.01201v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01201","description":"<p>In-context learning (ICL) is one of the most powerful and most unexpected\ncapabilities to emerge in recent transformer-based large language models\n(LLMs). Yet the mechanisms that underlie it are poorly understood. In this\npaper, we demonstrate that comparable ICL capabilities can be acquired by an\nalternative sequence prediction learning method using clone-structured causal\ngraphs (CSCGs). Moreover, a key property of CSCGs is that, unlike\ntransformer-based LLMs, they are {\\em interpretable}, which considerably\nsimplifies the task of explaining how ICL works. Specifically, we show that it\nuses a combination of (a) learning template (schema) circuits for pattern\ncompletion, (b) retrieving relevant templates in a context-sensitive manner,\nand (c) rebinding of novel tokens to appropriate slots in the templates. We go\non to marshall evidence for the hypothesis that similar mechanisms underlie ICL\nin LLMs. For example, we find that, with CSCGs as with LLMs, different\ncapabilities emerge at different levels of overparameterization, suggesting\nthat overparameterization helps in learning more complex template (schema)\ncircuits. By showing how ICL can be achieved with small models and datasets, we\nopen up a path to novel architectures, and take a vital step towards a more\ngeneral understanding of the mechanics behind this important capability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_S/0/1/0/all/0/1\">Sivaramakrishnan Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dedieu_A/0/1/0/all/0/1\">Antoine Dedieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raju_R/0/1/0/all/0/1\">Rajkumar Vasudeva Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1\">Murray Shanahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaro_Gredilla_M/0/1/0/all/0/1\">Miguel Lazaro-Gredilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_D/0/1/0/all/0/1\">Dileep George</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT. (arXiv:2307.01202v1 [cs.LG])","link":"http://arxiv.org/abs/2307.01202","description":"<p>Analysis of innovation has been fundamentally limited by conventional\napproaches to broad, structural variables. This paper pushes the boundaries,\ntaking an LLM approach to patent analysis with the groundbreaking ChatGPT\ntechnology. OpenAI's state-of-the-art textual embedding accesses complex\ninformation about the quality and impact of each invention to power deep\nlearning predictive models. The nuanced embedding drives a 24% incremental\nimprovement in R-squared predicting patent value and clearly isolates the worst\nand best applications. These models enable a revision of the contemporary\nKogan, Papanikolaou, Seru, and Stoffman (2017) valuation of patents by a median\ndeviation of 1.5 times, accounting for potential institutional predictions.\nFurthermore, the market fails to incorporate timely information about\napplications; a long-short portfolio based on predicted acceptance rates\nachieves significant abnormal returns of 3.3% annually. The models provide an\nopportunity to revolutionize startup and small-firm corporate policy vis-a-vis\npatenting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Stephen Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Dialectal Representation Learning of Sinitic Phonology. (arXiv:2307.01209v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01209","description":"<p>Machine learning techniques have shown their competence for representing and\nreasoning in symbolic systems such as language and phonology. In Sinitic\nHistorical Phonology, notable tasks that could benefit from machine learning\ninclude the comparison of dialects and reconstruction of proto-languages\nsystems. Motivated by this, this paper provides an approach for obtaining\nmulti-dialectal representations of Sinitic syllables, by constructing a\nknowledge graph from structured phonological data, then applying the BoxE\ntechnique from knowledge base learning. We applied unsupervised clustering\ntechniques to the obtained representations to observe that the representations\ncapture phonemic contrast from the input dialects. Furthermore, we trained\nclassifiers to perform inference of unobserved Middle Chinese labels, showing\nthe representations' potential for indicating archaic, proto-language features.\nThe representations can be used for performing completion of fragmented Sinitic\nphonological knowledge bases, estimating divergences between different\ncharacters, or aiding the exploration and reconstruction of archaic features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhibai Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An automated method for the ontological representation of security directives. (arXiv:2307.01211v1 [cs.AI])","link":"http://arxiv.org/abs/2307.01211","description":"<p>Large documents written in juridical language are difficult to interpret,\nwith long sentences leading to intricate and intertwined relations between the\nnouns. The present paper frames this problem in the context of recent European\nsecurity directives. The complexity of their language is here thwarted by\nautomating the extraction of the relevant information, namely of the parts of\nspeech from each clause, through a specific tailoring of Natural Language\nProcessing (NLP) techniques. These contribute, in combination with ontology\ndevelopment principles, to the design of our automated method for the\nrepresentation of security directives as ontologies. The method is showcased on\na practical problem, namely to derive an ontology representing the NIS 2\ndirective, which is the peak of cybersecurity prescripts at the European level.\nAlthough the NLP techniques adopted showed some limitations and had to be\ncomplemented by manual analysis, the overall results provide valid support for\ndirective compliance in general and for ontology development in particular.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bella_G/0/1/0/all/0/1\">Giampaolo Bella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castiglione_G/0/1/0/all/0/1\">Gianpietro Castiglione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santamaria_D/0/1/0/all/0/1\">Daniele Francesco Santamaria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Counterfactual Augmentation for Robust Text Classification Based on Word-Group Search. (arXiv:2307.01214v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01214","description":"<p>Despite large-scale pre-trained language models have achieved striking\nresults for text classificaion, recent work has raised concerns about the\nchallenge of shortcut learning. In general, a keyword is regarded as a shortcut\nif it creates a superficial association with the label, resulting in a false\nprediction. Conversely, shortcut learning can be mitigated if the model relies\non robust causal features that help produce sound predictions. To this end,\nmany studies have explored post-hoc interpretable methods to mine shortcuts and\ncausal features for robustness and generalization. However, most existing\nmethods focus only on single word in a sentence and lack consideration of\nword-group, leading to wrong causal features. To solve this problem, we propose\na new Word-Group mining approach, which captures the causal effect of any\nkeyword combination and orders the combinations that most affect the\nprediction. Our approach bases on effective post-hoc analysis and beam search,\nwhich ensures the mining effect and reduces the complexity. Then, we build a\ncounterfactual augmentation method based on the multiple word-groups, and use\nan adaptive voting mechanism to learn the influence of different augmentated\nsamples on the prediction results, so as to force the model to pay attention to\neffective causal features. We demonstrate the effectiveness of the proposed\nmethod by several tasks on 8 affective review datasets and 4 toxic language\ndatasets, including cross-domain text classificaion, text attack and gender\nfairness test.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1\">Fausto Giunchiglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Patterns of Definitions and Methods from Scientific Documents. (arXiv:2307.01216v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01216","description":"<p>The difficulties of automatic extraction of definitions and methods from\nscientific documents lie in two aspects: (1) the complexity and diversity of\nnatural language texts, which requests an analysis method to support the\ndiscovery of pattern; and, (2) a complete definition or method represented by a\nscientific paper is usually distributed within text, therefore an effective\napproach should not only extract single sentence definitions and methods but\nalso integrate the sentences to obtain a complete definition or method. This\npaper proposes an analysis method for discovering patterns of definition and\nmethod and uses the method to discover patterns of definition and method.\nCompleteness of the patterns at the semantic level is guaranteed by a complete\nset of semantic relations that identify definitions and methods respectively.\nThe completeness of the patterns at the syntactic and lexical levels is\nguaranteed by syntactic and lexical constraints. Experiments on the self-built\ndataset and two public definition datasets show that the discovered patterns\nare effective. The patterns can be used to extract definitions and methods from\nscientific documents and can be tailored or extended to suit other\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yutian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuge_H/0/1/0/all/0/1\">Hai Zhuge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT). (arXiv:2307.01225v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01225","description":"<p>Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have\nshown impressive performance in NLP. However, their vulnerability to\nadversarial examples poses a security risk. Existing defense methods lack\ninterpretability, making it hard to understand adversarial classifications and\nidentify model vulnerabilities. To address this, we propose the\nInterpretability and Transparency-Driven Detection and Transformation (IT-DT)\nframework. It focuses on interpretability and transparency in detecting and\ntransforming textual adversarial examples. IT-DT utilizes techniques like\nattention maps, integrated gradients, and model feedback for interpretability\nduring detection. This helps identify salient features and perturbed words\ncontributing to adversarial classifications. In the transformation phase, IT-DT\nuses pre-trained embeddings and model feedback to generate optimal replacements\nfor perturbed words. By finding suitable substitutions, we aim to convert\nadversarial examples into non-adversarial counterparts that align with the\nmodel's intended behavior while preserving the text's meaning. Transparency is\nemphasized through human expert involvement. Experts review and provide\nfeedback on detection and transformation results, enhancing decision-making,\nespecially in complex scenarios. The framework generates insights and threat\nintelligence empowering analysts to identify vulnerabilities and improve model\nrobustness. Comprehensive experiments demonstrate the effectiveness of IT-DT in\ndetecting and transforming adversarial examples. The approach enhances\ninterpretability, provides transparency, and enables accurate identification\nand successful transformation of adversarial inputs. By combining technical\nanalysis and human expertise, IT-DT significantly improves the resilience and\ntrustworthiness of transformer-based text classifiers against adversarial\nattacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabir_B/0/1/0/all/0/1\">Bushra Sabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1\">M. Ali Babar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuadbba_S/0/1/0/all/0/1\">Sharif Abuadbba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])","link":"http://arxiv.org/abs/2307.01226","description":"<p>Recently, Neural Topic Models (NTM), inspired by variational autoencoders,\nhave attracted a lot of research interest; however, these methods have limited\napplications in the real world due to the challenge of incorporating human\nknowledge. This work presents a semi-supervised neural topic modeling method,\nvONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and\noptimal transport. When a few keywords per topic are provided, vONTSS in the\nsemi-supervised setting generates potential topics and optimizes topic-keyword\nquality and topic classification. Experiments show that vONTSS outperforms\nexisting semi-supervised topic modeling methods in classification accuracy and\ndiversity. vONTSS also supports unsupervised topic modeling. Quantitative and\nqualitative experiments show that vONTSS in the unsupervised setting\noutperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered\nand coherent topics on benchmark datasets. It is also much faster than the\nstate-of-the-art weakly supervised text classification method while achieving\nsimilar classification performance. We further prove the equivalence of optimal\ntransport loss and cross-entropy loss at the global minimum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weijie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengamedu_S/0/1/0/all/0/1\">Srinivasan H. Sengamedu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iannacci_F/0/1/0/all/0/1\">Francis Iannacci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinjin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language and Text-to-3D Models for Engineering Design Optimization. (arXiv:2307.01230v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01230","description":"<p>The current advances in generative AI for learning large neural network\nmodels with the capability to produce essays, images, music and even 3D assets\nfrom text prompts create opportunities for a manifold of disciplines. In the\npresent paper, we study the potential of deep text-to-3D models in the\nengineering domain, with focus on the chances and challenges when integrating\nand interacting with 3D assets in computational simulation-based design\noptimization. In contrast to traditional design optimization of 3D geometries\nthat often searches for the optimum designs using numerical representations,\nsuch as B-Spline surface or deformation parameters in vehicle aerodynamic\noptimization, natural language challenges the optimization framework by\nrequiring a different interpretation of variation operators while at the same\ntime may ease and motivate the human user interaction. Here, we propose and\nrealize a fully automated evolutionary design optimization framework using\nShap-E, a recently published text-to-3D asset network by OpenAI, in the context\nof aerodynamic vehicle optimization. For representing text prompts in the\nevolutionary optimization, we evaluate (a) a bag-of-words approach based on\nprompt templates and Wordnet samples, and (b) a tokenisation approach based on\nprompt templates and the byte pair encoding method from GPT4. Our main findings\nfrom the optimizations indicate that, first, it is important to ensure that the\ndesigns generated from prompts are within the object class of application, i.e.\ndiverse and novel designs need to be realistic, and, second, that more research\nis required to develop methods where the strength of text prompt variations and\nthe resulting variations of the 3D designs share causal relations to some\ndegree to improve the optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rios_T/0/1/0/all/0/1\">Thiago Rios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menzel_S/0/1/0/all/0/1\">Stefan Menzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sendhoff_B/0/1/0/all/0/1\">Bernhard Sendhoff</a> (Honda Research Institute Europe)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Evolution of Substance Use Coverage in the Philadelphia Inquirer. (arXiv:2307.01299v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01299","description":"<p>The media's representation of illicit substance use can lead to harmful\nstereotypes and stigmatization for individuals struggling with addiction,\nultimately influencing public perception, policy, and public health outcomes.\nTo explore how the discourse and coverage of illicit drug use changed over\ntime, this study analyzes 157,476 articles published in the Philadelphia\nInquirer over a decade. Specifically, the study focuses on articles that\nmentioned at least one commonly abused substance, resulting in a sample of\n3,903 articles. Our analysis shows that cannabis and narcotics are the most\nfrequently discussed classes of drugs. Hallucinogenic drugs are portrayed more\npositively than other categories, whereas narcotics are portrayed the most\nnegatively. Our research aims to highlight the need for accurate and inclusive\nportrayals of substance use and addiction in the media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bouzoubaa_L/0/1/0/all/0/1\">Layla Bouzoubaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_R/0/1/0/all/0/1\">Ramtin Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_P/0/1/0/all/0/1\">Preetha Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezapour_R/0/1/0/all/0/1\">Rezvaneh Rezapour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Spoken Named Entity Recognition: A Cross-Lingual Perspective. (arXiv:2307.01310v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01310","description":"<p>Recent advancements in Named Entity Recognition (NER) have significantly\nimproved the identification of entities in textual data. However, spoken NER, a\nspecialized field of spoken document retrieval, lags behind due to its limited\nresearch and scarce datasets. Moreover, cross-lingual transfer learning in\nspoken NER has remained unexplored. This paper utilizes transfer learning\nacross Dutch, English, and German using pipeline and End-to-End (E2E) schemes.\nWe employ Wav2Vec2-XLS-R models on custom pseudo-annotated datasets and\ninvestigate several architectures for the adaptability of cross-lingual\nsystems. Our results demonstrate that End-to-End spoken NER outperforms\npipeline-based alternatives over our limited annotations. Notably, transfer\nlearning from German to Dutch surpasses the Dutch E2E system by 7% and the\nDutch pipeline system by 4%. This study not only underscores the feasibility of\ntransfer learning in spoken NER but also sets promising outcomes for future\nevaluations, hinting at the need for comprehensive data collection to augment\nthe results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benaicha_M/0/1/0/all/0/1\">Moncef Benaicha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thulke_D/0/1/0/all/0/1\">David Thulke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turan_M/0/1/0/all/0/1\">M. A. Tu&#x11f;tekin Turan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic enrichment towards efficient speech representations. (arXiv:2307.01323v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01323","description":"<p>Over the past few years, self-supervised learned speech representations have\nemerged as fruitful replacements for conventional surface representations when\nsolving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual\nmodels trained on massive textual data were introduced to encode language\nagnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make\nprofit from such textual models to enrich multilingual speech representations\nwith language agnostic semantics. By aiming for better semantic extraction on a\nchallenging Spoken Language Understanding task and in consideration with\ncomputation costs, this study investigates a specific in-domain semantic\nenrichment of the SAMU-XLSR model by specializing it on a small amount of\ntranscribed data from the downstream task. In addition, we show the benefits of\nthe use of same-domain French and Italian benchmarks for low-resource language\nportability and explore cross-domain capacities of the enriched SAMU-XLSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laperriere_G/0/1/0/all/0/1\">Ga&#xeb;lle Laperri&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghannay_S/0/1/0/all/0/1\">Sahar Ghannay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabaian_B/0/1/0/all/0/1\">Bassam Jabaian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Language Models are not Multicultural: A Case Study in Emotion. (arXiv:2307.01370v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01370","description":"<p>Emotions are experienced and expressed differently across the world. In order\nto use Large Language Models (LMs) for multilingual tasks that require\nemotional sensitivity, LMs must reflect this cultural variation in emotion. In\nthis study, we investigate whether the widely-used multilingual LMs in 2023\nreflect differences in emotional expressions across cultures and languages. We\nfind that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric,\nand generative LMs (e.g., ChatGPT) reflect Western norms, even when responding\nto prompts in other languages. Our results show that multilingual LMs do not\nsuccessfully learn the culturally appropriate nuances of emotion and we\nhighlight possible research directions towards correcting this.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1\">Shreya Havaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_S/0/1/0/all/0/1\">Sunny Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_B/0/1/0/all/0/1\">Bhumika Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guntuku_L/0/1/0/all/0/1\">Langchen Liu Sharath Chandra Guntuku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation. (arXiv:2307.01377v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01377","description":"<p>Transformer models using segment-based processing have been an effective\narchitecture for simultaneous speech translation. However, such models create a\ncontext mismatch between training and inference environments, hindering\npotential translation accuracy. We solve this issue by proposing Shiftable\nContext, a simple yet effective scheme to ensure that consistent segment and\ncontext sizes are maintained throughout training and inference, even with the\npresence of partially filled segments due to the streaming nature of\nsimultaneous translation. Shiftable Context is also broadly applicable to\nsegment-based transformers for streaming tasks. Our experiments on the\nEnglish-German, English-French, and English-Spanish language pairs from the\nMUST-C dataset demonstrate that when applied to the Augmented Memory\nTransformer, a state-of-the-art model for simultaneous speech translation, the\nproposed scheme achieves an average increase of 2.09, 1.83, and 1.95 BLEU\nscores across each wait-k value for the three language pairs, respectively,\nwith a minimal impact on computation-aware Average Lagging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raffel_M/0/1/0/all/0/1\">Matthew Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Penney_D/0/1/0/all/0/1\">Drew Penney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lizhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01379","description":"<p>Although Large Language Models (LLMs) have shown great potential in Natural\nLanguage Generation, it is still challenging to characterize the uncertainty of\nmodel generations, i.e., when users could trust model outputs. Our research is\nderived from the heuristic facts that tokens are created unequally in\nreflecting the meaning of generations by auto-regressive LLMs, i.e., some\ntokens are more relevant (or representative) than others, yet all the tokens\nare equally valued when estimating uncertainty. It is because of the linguistic\nredundancy where mostly a few keywords are sufficient to convey the meaning of\na long sentence. We name these inequalities as generative inequalities and\ninvestigate how they affect uncertainty estimation. Our results reveal that\nconsiderable tokens and sentences containing limited semantics are weighted\nequally or even heavily when estimating uncertainty. To tackle these biases\nposed by generative inequalities, we propose to jointly Shifting Attention to\nmore Relevant (SAR) components from both the token level and the sentence level\nwhile estimating uncertainty. We conduct experiments over popular\n\"off-the-shelf\" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful\ncommercial LLMs (e.g., Davinci from OpenAI), across various free-form\nquestion-answering tasks. Experimental results and detailed demographic\nanalysis indicate the superior performance of SAR. Code is available at\nhttps://github.com/jinhaoduan/shifting-attention-to-relevance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jinhao Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zavalny_A/0/1/0/all/0/1\">Alex Zavalny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaidi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation. (arXiv:2307.01381v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01381","description":"<p>Simultaneous speech translation is an essential communication task difficult\nfor humans whereby a translation is generated concurrently with oncoming speech\ninputs. For such a streaming task, transformers using block processing to break\nan input sequence into segments have achieved state-of-the-art performance at a\nreduced cost. Current methods to allow information to propagate across\nsegments, including left context and memory banks, have faltered as they are\nboth insufficient representations and unnecessarily expensive to compute. In\nthis paper, we propose an Implicit Memory Transformer that implicitly retains\nmemory through a new left context method, removing the need to explicitly\nrepresent memory with memory banks. We generate the left context from the\nattention output of the previous segment and include it in the keys and values\nof the current segment's attention calculation. Experiments on the MuST-C\ndataset show that the Implicit Memory Transformer provides a substantial\nspeedup on the encoder forward pass with nearly identical translation quality\nwhen compared with the state-of-the-art approach that employs both left context\nand memory banks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raffel_M/0/1/0/all/0/1\">Matthew Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lizhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis. (arXiv:2307.01387v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01387","description":"<p>The computational analysis of poetry is limited by the scarcity of tools to\nautomatically analyze and scan poems. In a multilingual settings, the problem\nis exacerbated as scansion and rhyme systems only exist for individual\nlanguages, making comparative studies very challenging and time consuming. In\nthis work, we present \\textsc{Alberti}, the first multilingual pre-trained\nlarge language model for poetry. Through domain-specific pre-training (DSP), we\nfurther trained multilingual BERT on a corpus of over 12 million verses from 12\nlanguages. We evaluated its performance on two structural poetry tasks: Spanish\nstanza type classification, and metrical pattern prediction for Spanish,\nEnglish and German. In both cases, \\textsc{Alberti} outperforms multilingual\nBERT and other transformers-based models of similar sizes, and even achieves\nstate-of-the-art results for German when compared to rule-based systems,\ndemonstrating the feasibility and effectiveness of DSP in the poetry domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_J/0/1/0/all/0/1\">Javier de la Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pozo_A/0/1/0/all/0/1\">&#xc1;lvaro P&#xe9;rez Pozo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ros_S/0/1/0/all/0/1\">Salvador Ros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Blanco_E/0/1/0/all/0/1\">Elena Gonz&#xe1;lez-Blanco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Learning Improves Performance In Deep Argument Mining Models. (arXiv:2307.01401v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01401","description":"<p>The successful analysis of argumentative techniques from user-generated text\nis central to many downstream tasks such as political and market analysis.\nRecent argument mining tools use state-of-the-art deep learning methods to\nextract and annotate argumentative techniques from various online text corpora,\nhowever each task is treated as separate and different bespoke models are\nfine-tuned for each dataset. We show that different argument mining tasks share\ncommon semantic and logical structure by implementing a multi-task approach to\nargument mining that achieves better performance than state-of-the-art methods\nfor the same problems. Our model builds a shared representation of the input\ntext that is common to all tasks and exploits similarities between tasks in\norder to further boost performance via parameter-sharing. Our results are\nimportant for argument mining as they show that different tasks share\nsubstantial similarities and suggest a holistic approach to the extraction of\nargumentative techniques from text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farzam_A/0/1/0/all/0/1\">Amirhossein Farzam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Shashank Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehlhaff_I/0/1/0/all/0/1\">Isaac Mehlhaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morucci_M/0/1/0/all/0/1\">Marco Morucci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Tag Prediction based on Question Tagging Behavior Analysis of CommunityQA Platform Users. (arXiv:2307.01420v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01420","description":"<p>In community question-answering platforms, tags play essential roles in\neffective information organization and retrieval, better question routing,\nfaster response to questions, and assessment of topic popularity. Hence,\nautomatic assistance for predicting and suggesting tags for posts is of high\nutility to users of such platforms. To develop better tag prediction across\ndiverse communities and domains, we performed a thorough analysis of users'\ntagging behavior in 17 StackExchange communities. We found various common\ninherent properties of this behavior in those diverse domains. We used the\nfindings to develop a flexible neural tag prediction architecture, which\npredicts both popular tags and more granular tags for each question. Our\nextensive experiments and obtained performance show the effectiveness of our\nmodel\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1\">Kuntal Kumar Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gamon_M/0/1/0/all/0/1\">Michael Gamon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_N/0/1/0/all/0/1\">Nirupama Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucerzan_S/0/1/0/all/0/1\">Silviu Cucerzan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Conditional and Compositional Language Model Differentiable Prompting. (arXiv:2307.01446v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01446","description":"<p>Prompts have been shown to be an effective method to adapt a frozen\nPretrained Language Model (PLM) to perform well on downstream tasks. Prompts\ncan be represented by a human-engineered word sequence or by a learned\ncontinuous embedding. In this work, we investigate conditional and\ncompositional differentiable prompting. We propose a new model, Prompt\nProduction System (PRopS), which learns to transform task instructions or input\nmetadata, into continuous prompts that elicit task-specific outputs from the\nPLM. Our model uses a modular network structure based on our neural formulation\nof Production Systems, which allows the model to learn discrete rules -- neural\nfunctions that learn to specialize in transforming particular prompt input\npatterns, making it suitable for compositional transfer learning and few-shot\nlearning. We present extensive empirical and theoretical analysis and show that\nPRopS consistently surpasses other PLM adaptation techniques, and often\nimproves upon fully fine-tuned models, on compositional generalization tasks,\ncontrollable summarization and multilingual translation, while needing fewer\ntrainable parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pilault_J/0/1/0/all/0/1\">Jonathan Pilault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Can Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1\">Markus Dreyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision. (arXiv:2307.01448v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01448","description":"<p>Structured chemical reaction information plays a vital role for chemists\nengaged in laboratory work and advanced endeavors such as computer-aided drug\ndesign. Despite the importance of extracting structured reactions from\nscientific literature, data annotation for this purpose is cost-prohibitive due\nto the significant labor required from domain experts. Consequently, the\nscarcity of sufficient training data poses an obstacle to the progress of\nrelated models in this domain. In this paper, we propose ReactIE, which\ncombines two weakly supervised approaches for pre-training. Our method utilizes\nfrequent patterns within the text as linguistic cues to identify specific\ncharacteristics of chemical reactions. Additionally, we adopt synthetic data\nfrom patent records as distant supervision to incorporate domain knowledge into\nthe model. Experiments demonstrate that ReactIE achieves substantial\nimprovements and outperforms all existing baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siru Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Minhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_V/0/1/0/all/0/1\">Vivian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yizhu Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking. (arXiv:2307.01453v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01453","description":"<p>There has been significant interest in zero and few-shot learning for\ndialogue state tracking (DST) due to the high cost of collecting and annotating\ntask-oriented dialogues. Recent work has demonstrated that in-context learning\nrequires very little data and zero parameter updates, and even outperforms\ntrained methods in the few-shot setting (Hu et al. 2022). We propose RefPyDST,\nwhich advances the state of the art with three advancements to in-context\nlearning for DST. First, we formulate DST as a Python programming task,\nexplicitly modeling language coreference as variable reference in Python.\nSecond, since in-context learning depends highly on the context examples, we\npropose a method to retrieve a diverse set of relevant examples to improve\nperformance. Finally, we introduce a novel re-weighting method during decoding\nthat takes into account probabilities of competing surface forms, and produces\na more accurate dialogue state prediction. We evaluate our approach using\nMultiWOZ and achieve state-of-the-art multi-domain joint-goal accuracy in zero\nand few-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+King_B/0/1/0/all/0/1\">Brendan King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1\">Jeffrey Flanigan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care. (arXiv:2307.01458v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01458","description":"<p>The recent advances in NLP, have led to a new trend of applying LLMs to\nreal-world scenarios. While the latest LLMs are astonishingly fluent when\ninteracting with humans, they suffer from the misinformation problem by\nunintentionally generating factually false statements. This can lead to harmful\nconsequences, especially when produced within sensitive contexts, such as\nhealthcare. Yet few previous works have focused on evaluating misinformation in\nthe long-form generation of LLMs, especially for knowledge-intensive topics.\nMoreover, although LLMs have been shown to perform well in different languages,\nmisinformation evaluation has been mostly conducted in English. To this end, we\npresent a benchmark, CARE-MI, for evaluating LLM misinformation in: 1) a\nsensitive topic, specifically the maternity and infant care domain; and 2) a\nlanguage other than English, namely Chinese. Most importantly, we provide an\ninnovative paradigm for building long-form generation evaluation benchmarks\nthat can be transferred to other knowledge-intensive domains and low-resourced\nlanguages. Our proposed benchmark fills the gap between the extensive usage of\nLLMs and the lack of datasets for assessing the misinformation generated by\nthese models. It contains 1,612 expert-checked questions, accompanied with\nhuman-selected references. Using our benchmark, we conduct extensive\nexperiments and found that current Chinese LLMs are far from perfect in the\ntopic of maternity and infant care. In an effort to minimize the reliance on\nhuman resources for performance evaluation, we offer a judgment model for\nautomatically assessing the long-form output of LLMs using the benchmark\nquestions. Moreover, we compare potential solutions for long-form generation\nevaluation and provide insights for building more robust and efficient\nautomated metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wangyue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_M/0/1/0/all/0/1\">Mingbai Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bowen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Noa Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCAT: Robust Self-supervised Contrastive Learning via Adversarial Training for Text Classification. (arXiv:2307.01488v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01488","description":"<p>Despite their promising performance across various natural language\nprocessing (NLP) tasks, current NLP systems are vulnerable to textual\nadversarial attacks. To defend against these attacks, most existing methods\napply adversarial training by incorporating adversarial examples. However,\nthese methods have to rely on ground-truth labels to generate adversarial\nexamples, rendering it impractical for large-scale model pre-training which is\ncommonly used nowadays for NLP and many other tasks. In this paper, we propose\na novel learning framework called SCAT (Self-supervised Contrastive Learning\nvia Adversarial Training), which can learn robust representations without\nrequiring labeled data. Specifically, SCAT modifies random augmentations of the\ndata in a fully labelfree manner to generate adversarial examples. Adversarial\ntraining is achieved by minimizing the contrastive loss between the\naugmentations and their adversarial counterparts. We evaluate SCAT on two text\nclassification datasets using two state-of-the-art attack schemes proposed\nrecently. Our results show that SCAT can not only train robust language models\nfrom scratch, but it can also significantly improve the robustness of existing\npre-trained language models. Moreover, to demonstrate its flexibility, we show\nthat SCAT can also be combined with supervised adversarial training to further\nenhance model robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junjie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1\">Dit-Yan Yeung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Evaluating and Mitigating Gender Biases in Multilingual Settings. (arXiv:2307.01503v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01503","description":"<p>While understanding and removing gender biases in language models has been a\nlong-standing problem in Natural Language Processing, prior research work has\nprimarily been limited to English. In this work, we investigate some of the\nchallenges with evaluating and mitigating biases in multilingual settings which\nstem from a lack of existing benchmarks and resources for bias evaluation\nbeyond English especially for non-western context. In this paper, we first\ncreate a benchmark for evaluating gender biases in pre-trained masked language\nmodels by extending DisCo to different Indian languages using human\nannotations. We extend various debiasing methods to work beyond English and\nevaluate their effectiveness for SOTA massively multilingual models on our\nproposed metric. Overall, our work highlights the challenges that arise while\nstudying social biases in multilingual settings and provides resources as well\nas mitigation techniques to take a step toward scaling to more languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vashishtha_A/0/1/0/all/0/1\">Aniket Vashishtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kabir Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Prompt in the Classroom to Understand AI Limits: A pilot study. (arXiv:2307.01540v1 [cs.HC])","link":"http://arxiv.org/abs/2307.01540","description":"<p>Artificial intelligence's progress holds great promise in assisting society\nin addressing pressing societal issues. In particular Large Language Models\n(LLM) and the derived chatbots, like ChatGPT, have highly improved the natural\nlanguage processing capabilities of AI systems allowing them to process an\nunprecedented amount of unstructured data. The consequent hype has also\nbackfired, raising negative sentiment even after novel AI methods' surprising\ncontributions. One of the causes, but also an important issue per se, is the\nrising and misleading feeling of being able to access and process any form of\nknowledge to solve problems in any domain with no effort or previous expertise\nin AI or problem domain, disregarding current LLMs limits, such as\nhallucinations and reasoning limits. Acknowledging AI fallibility is crucial to\naddress the impact of dogmatic overconfidence in possibly erroneous suggestions\ngenerated by LLMs. At the same time, it can reduce fear and other negative\nattitudes toward AI. AI literacy interventions are necessary that allow the\npublic to understand such LLM limits and learn how to use them in a more\neffective manner, i.e. learning to \"prompt\". With this aim, a pilot educational\nintervention was performed in a high school with 30 students. It involved (i)\npresenting high-level concepts about intelligence, AI, and LLM, (ii) an initial\nnaive practice with ChatGPT in a non-trivial task, and finally (iii) applying\ncurrently-accepted prompting strategies. Encouraging preliminary results have\nbeen collected such as students reporting a) high appreciation of the activity,\nb) improved quality of the interaction with the LLM during the educational\nactivity, c) decreased negative sentiments toward AI, d) increased\nunderstanding of limitations and specifically We aim to study factors that\nimpact AI acceptance and to refine and repeat this activity in more controlled\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theophilou_E/0/1/0/all/0/1\">Emily Theophilou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyuturk_C/0/1/0/all/0/1\">Cansu Koyuturk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavari_M/0/1/0/all/0/1\">Mona Yavari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bursic_S/0/1/0/all/0/1\">Sathya Bursic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donabauer_G/0/1/0/all/0/1\">Gregor Donabauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telari_A/0/1/0/all/0/1\">Alessia Telari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Testa_A/0/1/0/all/0/1\">Alessia Testa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boiano_R/0/1/0/all/0/1\">Raffaele Boiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Leo_D/0/1/0/all/0/1\">Davinia Hernandez-Leo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruskov_M/0/1/0/all/0/1\">Martin Ruskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taibi_D/0/1/0/all/0/1\">Davide Taibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbiadini_A/0/1/0/all/0/1\">Alessandro Gabbiadini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ognibene_D/0/1/0/all/0/1\">Dimitri Ognibene</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation. (arXiv:2307.01542v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01542","description":"<p>Despite the huge progress in myriad generation tasks, pretrained language\nmodels (LMs) such as GPT2 still tend to generate repetitive texts with\nmaximization-based decoding algorithms for open-ended generation. We attribute\ntheir overestimation of token-level repetition probabilities to the learning\nbias: LMs capture simple repetitive patterns faster with the MLE loss. We\npropose self-contrastive training to penalize the output of a premature\ncheckpoint of the same model when it incorrectly predicts repetition, which is\nshown to mitigate repetition effectively while maintaining fluency on two\ndatasets. Furthermore, we find that LMs use longer-range dependencies to\npredict repetitive tokens than non-repetitive ones, which may be the cause of\nsentence-level repetition loops.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jian Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases. (arXiv:2307.01595v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01595","description":"<p>As the representation capability of Pre-trained Language Models (PLMs)\nimprove, there is growing concern that they will inherit social biases from\nunprocessed corpora. Most previous debiasing techniques used Counterfactual\nData Augmentation (CDA) to balance the training corpus. However, CDA slightly\nmodifies the original corpus, limiting the representation distance between\ndifferent demographic groups to a narrow range. As a result, the debiasing\nmodel easily fits the differences between counterfactual pairs, which affects\nits debiasing performance with limited text resources. In this paper, we\npropose an adversarial training-inspired two-stage debiasing model using\nContrastive learning with Continuous Prompt Augmentation (named CCPA) to\nmitigate social biases in PLMs' encoding. In the first stage, we propose a data\naugmentation method based on continuous prompt tuning to push farther the\nrepresentation distance between sample pairs along different demographic\ngroups. In the second stage, we utilize contrastive learning to pull closer the\nrepresentation distance between the augmented sample pairs and then fine-tune\nPLMs' parameters to get debiased encoding. Our approach guides the model to\nachieve stronger debiasing performance by adding difficulty to the training\nprocess. Extensive experiments show that CCPA outperforms baselines in terms of\ndebiasing performance. Meanwhile, experimental results on the GLUE benchmark\nshow that CCPA retains the language modeling capability of PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Mengnan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Language Model for Grammatical Error Correction in L2 Russian. (arXiv:2307.01609v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01609","description":"<p>Grammatical error correction is one of the fundamental tasks in Natural\nLanguage Processing. For the Russian language, most of the spellcheckers\navailable correct typos and other simple errors with high accuracy, but often\nfail when faced with non-native (L2) writing, since the latter contains errors\nthat are not typical for native speakers. In this paper, we propose a pipeline\ninvolving a language model intended for correcting errors in L2 Russian\nwriting. The language model proposed is trained on untagged texts of the\nNewspaper subcorpus of the Russian National Corpus, and the quality of the\nmodel is validated against the RULEC-GEC corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Remnev_N/0/1/0/all/0/1\">Nikita Remnev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obiedkov_S/0/1/0/all/0/1\">Sergei Obiedkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhilina_E/0/1/0/all/0/1\">Ekaterina Rakhilina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smirnov_I/0/1/0/all/0/1\">Ivan Smirnov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyrenkova_A/0/1/0/all/0/1\">Anastasia Vyrenkova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Thought Prompting Elicits Knowledge Augmentation. (arXiv:2307.01640v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01640","description":"<p>The knowledge-augmented deep learning paradigm refers to a paradigm in which\ndomain knowledge is identified and integrated into deep models. Conventional\nmethods typically employ task-specific approaches to gather external knowledge\nfrom various sources. In contrast, large language models are extensively\npre-trained and can serve as a comprehensive source of external knowledge. In\nthis paper, we propose CoT-KA, a Chain-of-Thought-based method that augments\nknowledge for deep learning. CoT-KA avoids the need for additional knowledge\nretrieval or knowledge reasoning models, as required in conventional\naugmentation methods. Our results demonstrate that CoT-KA outperforms both pure\nCoT-based methods and the non-augmented method across the majority of eleven\npublicly available benchmarks for various reasoning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dingjun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinmei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Insert-expansions for Tool-enabled Conversational Agents. (arXiv:2307.01644v1 [cs.HC])","link":"http://arxiv.org/abs/2307.01644","description":"<p>This paper delves into an advanced implementation of\nChain-of-Thought-Prompting in Large Language Models, focusing on the use of\ntools (or \"plug-ins\") within the explicit reasoning paths generated by this\nprompting method. We find that tool-enabled conversational agents often become\nsidetracked, as additional context from tools like search engines or\ncalculators diverts from original user intents. To address this, we explore a\nconcept wherein the user becomes the tool, providing necessary details and\nrefining their requests. Through Conversation Analysis, we characterize this\ninteraction as insert-expansion - an intermediary conversation designed to\nfacilitate the preferred response. We explore possibilities arising from this\n'user-as-a-tool' approach in two empirical studies using direct comparison, and\nfind benefits in the recommendation domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goldi_A/0/1/0/all/0/1\">Andreas G&#xf6;ldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rietsche_R/0/1/0/all/0/1\">Roman Rietsche</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Conversational Models with System-Initiated Transitions between Chit-Chat and Task-Oriented Dialogues. (arXiv:2307.01664v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01664","description":"<p>Spoken dialogue systems (SDSs) have been separately developed under two\ndifferent categories, task-oriented and chit-chat. The former focuses on\nachieving functional goals and the latter aims at creating engaging social\nconversations without special goals. Creating a unified conversational model\nthat can engage in both chit-chat and task-oriented dialogue is a promising\nresearch topic in recent years. However, the potential ``initiative'' that\noccurs when there is a change between dialogue modes in one dialogue has rarely\nbeen explored. In this work, we investigate two kinds of dialogue scenarios,\none starts from chit-chat implicitly involving task-related topics and finally\nswitching to task-oriented requests; the other starts from task-oriented\ninteraction and eventually changes to casual chat after all requested\ninformation is provided. We contribute two efficient prompt models which can\nproactively generate a transition sentence to trigger system-initiated\ntransitions in a unified dialogue model. One is a discrete prompt model trained\nwith two discrete tokens, the other one is a continuous prompt model using\ncontinuous prompt embeddings automatically generated by a classifier. We\nfurthermore show that the continuous prompt model can also be used to guide the\nproactive transitions between particular domains in a multi-domain\ntask-oriented setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ultes_S/0/1/0/all/0/1\">Stefan Ultes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minker_W/0/1/0/all/0/1\">Wolfgang Minker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_W/0/1/0/all/0/1\">Wolfgang Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Norwegian Automatic Speech Recognition. (arXiv:2307.01672v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01672","description":"<p>In this paper, we present several baselines for automatic speech recognition\n(ASR) models for the two official written languages in Norway: Bokm{\\aa}l and\nNynorsk. We compare the performance of models of varying sizes and pre-training\napproaches on multiple Norwegian speech datasets. Additionally, we measure the\nperformance of these models against previous state-of-the-art ASR models, as\nwell as on out-of-domain datasets. We improve the state of the art on the\nNorwegian Parliamentary Speech Corpus (NPSC) from a word error rate (WER) of\n17.10\\% to 7.60\\%, with models achieving 5.81\\% for Bokm{\\aa}l and 11.54\\% for\nNynorsk. We also discuss the challenges and potential solutions for further\nimproving ASR models for Norwegian.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_J/0/1/0/all/0/1\">Javier de la Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braaten_R/0/1/0/all/0/1\">Rolv-Arild Braaten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummervold_P/0/1/0/all/0/1\">Per Egil Kummervold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetjen_F/0/1/0/all/0/1\">Freddy Wetjen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brygfjeld_S/0/1/0/all/0/1\">Svein Arne Brygfjeld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentanglement in a GAN for Unconditional Speech Synthesis. (arXiv:2307.01673v1 [eess.AS])","link":"http://arxiv.org/abs/2307.01673","description":"<p>Can we develop a model that can synthesize realistic speech directly from a\nlatent space, without explicit conditioning? Despite several efforts over the\nlast decade, previous adversarial and diffusion-based approaches still struggle\nto achieve this, even on small-vocabulary datasets. To address this, we propose\nAudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional\nspeech synthesis tailored to learn a disentangled latent space. Building upon\nthe StyleGAN family of image synthesis models, ASGAN maps sampled noise to a\ndisentangled latent vector which is then mapped to a sequence of audio features\nso that signal aliasing is suppressed at every layer. To successfully train\nASGAN, we introduce a number of new techniques, including a modification to\nadaptive discriminator augmentation which probabilistically skips discriminator\nupdates. We apply it on the small-vocabulary Google Speech Commands digits\ndataset, where it achieves state-of-the-art results in unconditional speech\nsynthesis. It is also substantially faster than existing top-performing\ndiffusion models. We confirm that ASGAN's latent space is disentangled: we\ndemonstrate how simple linear operations in the space can be used to perform\nseveral tasks unseen during training. Specifically, we perform evaluations in\nvoice conversion, speech enhancement, speaker verification, and keyword\nclassification. Our work indicates that GANs are still highly competitive in\nthe unconditional speech synthesis landscape, and that disentangled latent\nspaces can be used to aid generalization to unseen tasks. Code, models,\nsamples: https://github.com/RF5/simple-asgan/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baas_M/0/1/0/all/0/1\">Matthew Baas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation. (arXiv:2307.01680v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01680","description":"<p>The automatic detection of hate speech online is an active research area in\nNLP. Most of the studies to date are based on social media datasets that\ncontribute to the creation of hate speech detection models trained on them.\nHowever, data creation processes contain their own biases, and models\ninherently learn from these dataset-specific biases. In this paper, we perform\na large-scale cross-dataset comparison where we fine-tune language models on\ndifferent hate speech detection datasets. This analysis shows how some datasets\nare more generalisable than others when used as training data. Crucially, our\nexperiments show how combining hate speech detection datasets can contribute to\nthe development of robust hate speech detection models. This robustness holds\neven when controlling by data size and compared with the best individual\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1\">Dimosthenis Antypas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Racial Bias Trends in the Text of US Legal Opinions. (arXiv:2307.01693v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01693","description":"<p>Although there is widespread recognition of racial bias in US law, it is\nunclear how such bias appears in the language of law, namely judicial opinions,\nand whether it varies across time period or region. Building upon approaches\nfor measuring implicit racial bias in large-scale corpora, we approximate GloVe\nword embeddings for over 6 million US federal and state court cases from 1860\nto 2009. We find strong evidence of racial bias across nearly all regions and\ntime periods, as traditionally Black names are more closely associated with\npre-classified \"unpleasant\" terms whereas traditionally White names are more\nclosely associated with pre-classified \"pleasant\" terms. We also test whether\nlegal opinions before 1950 exhibit more implicit racial bias than those after\n1950, as well as whether opinions from Southern states exhibit less change in\nracial bias than those from Northeastern states. We do not find evidence of\nelevated bias in legal opinions before 1950, or evidence that legal opinions\nfrom Northeastern states show greater change in racial bias over time compared\nto Southern states. These results motivate further research into\ninstitutionalized racial bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jinturkar_R/0/1/0/all/0/1\">Rohan Jinturkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting. (arXiv:2307.01709v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01709","description":"<p>Knowledge Graph Completion (KGC) often requires both KG structural and\ntextual information to be effective. Pre-trained Language Models (PLMs) have\nbeen used to learn the textual information, usually under the fine-tune\nparadigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly\nfocus on the textual information and overlook structural knowledge. To tackle\nthis issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC)\nwhich maintains a balance between structural information and textual knowledge.\nCSProm-KG only tunes the parameters of Conditional Soft Prompts that are\ngenerated by the entities and relations representations. We verify the\neffectiveness of CSProm-KG on three popular static KGC benchmarks WN18RR,\nFB15K-237 and Wikidata5M, and two temporal KGC benchmarks ICEWS14 and\nICEWS05-15. CSProm-KG outperforms competitive baseline models and sets new\nstate-of-the-art on these benchmarks. We conduct further analysis to show (i)\nthe effectiveness of our proposed components, (ii) the efficiency of CSProm-KG,\nand (iii) the flexibility of CSProm-KG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Kwok-Yan Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework. (arXiv:2307.01715v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01715","description":"<p>Connectionist Temporal Classification (CTC) is a widely used criterion for\ntraining supervised sequence-to-sequence (seq2seq) models. It enables learning\nthe relations between input and output sequences, termed alignments, by\nmarginalizing over perfect alignments (that yield the ground truth), at the\nexpense of imperfect alignments. This binary differentiation of perfect and\nimperfect alignments falls short of capturing other essential alignment\nproperties that hold significance in other real-world applications. Here we\npropose $\\textit{Align With Purpose}$, a $\\textbf{general Plug-and-Play\nframework}$ for enhancing a desired property in models trained with the CTC\ncriterion. We do that by complementing the CTC with an additional loss term\nthat prioritizes alignments according to a desired property. Our method does\nnot require any intervention in the CTC loss function, enables easy\noptimization of a variety of properties, and allows differentiation between\nboth perfect and imperfect alignments. We apply our framework in the domain of\nAutomatic Speech Recognition (ASR) and show its generality in terms of property\nselection, architectural choice, and scale of training dataset (up to 280,000\nhours). To demonstrate the effectiveness of our framework, we apply it to two\nunrelated properties: emission time and word error rate (WER). For the former,\nwe report an improvement of up to 570ms in latency optimization with a minor\nreduction in WER, and for the latter, we report a relative improvement of 4.5%\nWER over the baseline models. To the best of our knowledge, these applications\nhave never been demonstrated to work on a scale of data as large as ours.\nNotably, our method can be implemented using only a few lines of code, and can\nbe extended to other alignment-free loss functions and to domains other than\nASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Segev_E/0/1/0/all/0/1\">Eliya Segev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alroy_M/0/1/0/all/0/1\">Maya Alroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsir_R/0/1/0/all/0/1\">Ronen Katsir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1\">Noam Wies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenhav_A/0/1/0/all/0/1\">Ayana Shenhav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Oren_Y/0/1/0/all/0/1\">Yael Ben-Oren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zar_D/0/1/0/all/0/1\">David Zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadmor_O/0/1/0/all/0/1\">Oren Tadmor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitterman_J/0/1/0/all/0/1\">Jacob Bitterman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenwein_T/0/1/0/all/0/1\">Tal Rosenwein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Aware Audio-Grounded Generative Slot Filling for Limited Annotated Data. (arXiv:2307.01764v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01764","description":"<p>Manually annotating fine-grained slot-value labels for task-oriented dialogue\n(ToD) systems is an expensive and time-consuming endeavour. This motivates\nresearch into slot-filling methods that operate with limited amounts of\nlabelled data. Moreover, the majority of current work on ToD is based solely on\ntext as the input modality, neglecting the additional challenges of imperfect\nautomatic speech recognition (ASR) when working with spoken language. In this\nwork, we propose a Knowledge-Aware Audio-Grounded generative slot-filling\nframework, termed KA2G, that focuses on few-shot and zero-shot slot filling for\nToD with speech input. KA2G achieves robust and data-efficient slot filling for\nspeech-based ToD by 1) framing it as a text generation task, 2) grounding text\ngeneration additionally in the audio modality, and 3) conditioning on available\nexternal knowledge (e.g. a predefined list of possible slot values). We show\nthat combining both modalities within the KA2G framework improves the\nrobustness against ASR errors. Further, the knowledge-aware slot-value\ngenerator in KA2G, implemented via a pointer generator mechanism, particularly\nbenefits few-shot and zero-shot learning. Experiments, conducted on the\nstandard speech-based single-turn SLURP dataset and a multi-turn dataset\nextracted from a commercial ToD system, display strong and consistent gains\nover prior work, especially in few-shot and zero-shot setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budzianowski_P/0/1/0/all/0/1\">Pawe&#x142; Budzianowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Inner Sentiments of a Thought. (arXiv:2307.01784v1 [cs.CL])","link":"http://arxiv.org/abs/2307.01784","description":"<p>Transformer-based large-scale language models (LLMs) are able to generate\nhighly realistic text. They are duly able to express, and at least implicitly\nrepresent, a wide range of sentiments and color, from the obvious, such as\nvalence and arousal to the subtle, such as determination and admiration. We\nprovide a first exploration of these representations and how they can be used\nfor understanding the inner sentimental workings of single sentences. We train\npredictors of the quantiles of the distributions of final sentiments of\nsentences from the hidden representations of an LLM applied to prefixes of\nincreasing lengths. After showing that predictors of distributions of valence,\ndetermination, admiration, anxiety and annoyance are well calibrated, we\nprovide examples of using these predictors for analyzing sentences,\nillustrating, for instance, how even ordinary conjunctions (e.g., \"but\") can\ndramatically alter the emotional trajectory of an utterance. We then show how\nto exploit the distributional predictions to generate sentences with sentiments\nin the tails of distributions. We discuss the implications of our results for\nthe inner workings of thoughts, for instance for psychiatric dysfunction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1\">Chris Gagne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dayan_P/0/1/0/all/0/1\">Peter Dayan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositionality as Lexical Symmetry. (arXiv:2201.12926v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12926","description":"<p>In tasks like semantic parsing, instruction following, and question\nanswering, standard deep networks fail to generalize compositionally from small\ndatasets. Many existing approaches overcome this limitation with model\narchitectures that enforce a compositional process of sentence interpretation.\nIn this paper, we present a domain-general and model-agnostic formulation of\ncompositionality as a constraint on symmetries of data distributions rather\nthan models. Informally, we prove that whenever a task can be solved by a\ncompositional model, there is a corresponding data augmentation scheme -- a\nprocedure for transforming examples into other well formed examples -- that\nimparts compositional inductive bias on any model trained to solve the same\ntask. We describe a procedure called LEXSYM that discovers these\ntransformations automatically, then applies them to training data for ordinary\nneural sequence models. Unlike existing compositional data augmentation\nprocedures, LEXSYM can be deployed agnostically across text, structured data,\nand even images. It matches or surpasses state-of-the-art, task-specific models\non COGS semantic parsing, SCAN and ALCHEMY instruction following, and\nCLEVR-COGENT visual question answering datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1\">Ekin Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces. (arXiv:2210.05098v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05098","description":"<p>The ability to extract high-quality translation dictionaries from monolingual\nword embedding spaces depends critically on the geometric similarity of the\nspaces -- their degree of \"isomorphism.\" We address the root-cause of faulty\ncross-lingual mapping: that word embedding training resulted in the underlying\nspaces being non-isomorphic. We incorporate global measures of isomorphism\ndirectly into the Skip-gram loss function, successfully increasing the relative\nisomorphism of trained word embedding spaces and improving their ability to be\nmapped to a shared cross-lingual space. The result is improved bilingual\nlexicon induction in general data conditions, under domain mismatch, and with\ntraining algorithm dissimilarities. We release IsoVec at\nhttps://github.com/kellymarchisio/isovec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1\">Kelly Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_N/0/1/0/all/0/1\">Neha Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duh_K/0/1/0/all/0/1\">Kevin Duh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exclusive Supermask Subnetwork Training for Continual Learning. (arXiv:2210.10209v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.10209","description":"<p>Continual Learning (CL) methods focus on accumulating knowledge over time\nwhile avoiding catastrophic forgetting. Recently, Wortsman et al. (2020)\nproposed a CL method, SupSup, which uses a randomly initialized, fixed base\nnetwork (model) and finds a supermask for each new task that selectively keeps\nor removes each weight to produce a subnetwork. They prevent forgetting as the\nnetwork weights are not being updated. Although there is no forgetting, the\nperformance of SupSup is sub-optimal because fixed weights restrict its\nrepresentational power. Furthermore, there is no accumulation or transfer of\nknowledge inside the model when new tasks are learned. Hence, we propose\nExSSNeT (Exclusive Supermask SubNEtwork Training), that performs exclusive and\nnon-overlapping subnetwork weight training. This avoids conflicting updates to\nthe shared weights by subsequent tasks to improve performance while still\npreventing forgetting. Furthermore, we propose a novel KNN-based Knowledge\nTransfer (KKT) module that utilizes previously acquired knowledge to learn new\ntasks better and faster. We demonstrate that ExSSNeT outperforms strong\nprevious methods on both NLP and Vision domains while preventing forgetting.\nMoreover, ExSSNeT is particularly advantageous for sparse masks that activate\n2-10% of the model parameters, resulting in an average improvement of 8.3% over\nSupSup. Furthermore, ExSSNeT scales to a large number of tasks (100). Our code\nis available at https://github.com/prateeky2806/exessnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1\">Prateek Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Detoxification with Attribute-Discriminative Latent Space. (arXiv:2210.10329v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10329","description":"<p>Transformer-based Language Models (LMs) have achieved impressive results on\nnatural language understanding tasks, but they can also generate toxic text\nsuch as insults, threats, and profanity, limiting their real-world\napplications. To overcome this issue, a few text generation approaches aim to\ndetoxify toxic texts using additional LMs or perturbations. However, previous\nmethods require excessive memory, computations, and time which are serious\nbottlenecks in their real-world application. To address such limitations, we\npropose an effective yet efficient method for language detoxification using an\nattribute-discriminative latent space. Specifically, we project the latent\nspace of an original Transformer LM onto a discriminative latent space that\nwell-separates texts by their attributes using a projection block and an\nattribute discriminator. This allows the LM to control the text generation to\nbe non-toxic with minimal memory and computation overhead. We validate our\nmodel, Attribute-Discriminative Language Model (ADLM) on detoxified language\nand dialogue generation tasks, on which our method significantly outperforms\nbaselines both in performance and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_J/0/1/0/all/0/1\">Jin Myung Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minseon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment analysis and opinion mining on E-commerce site. (arXiv:2211.15536v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15536","description":"<p>Sentiment analysis or opinion mining help to illustrate the phrase NLP\n(Natural Language Processing). Sentiment analysis has been the most significant\ntopic in recent years. The goal of this study is to solve the sentiment\npolarity classification challenges in sentiment analysis. A broad technique for\ncategorizing sentiment opposition is presented, along with comprehensive\nprocess explanations. With the results of the analysis, both sentence-level\nclassification and review-level categorization are conducted. Finally, we\ndiscuss our plans for future sentiment analysis research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anny_F/0/1/0/all/0/1\">Fatema Tuz Zohra Anny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_O/0/1/0/all/0/1\">Oahidul Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Democratizing Neural Machine Translation with OPUS-MT. (arXiv:2212.01936v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01936","description":"<p>This paper presents the OPUS ecosystem with a focus on the development of\nopen machine translation models and tools, and their integration into end-user\napplications, development platforms and professional workflows. We discuss our\non-going mission of increasing language coverage and translation quality, and\nalso describe on-going work on the development of modular translation models\nand speed-optimized compact solutions for real-time translation on regular\ndesktops and small devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiedemann_J/0/1/0/all/0/1\">J&#xf6;rg Tiedemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aulamo_M/0/1/0/all/0/1\">Mikko Aulamo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakshandaeva_D/0/1/0/all/0/1\">Daria Bakshandaeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggia_M/0/1/0/all/0/1\">Michele Boggia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gronroos_S/0/1/0/all/0/1\">Stig-Arne Gr&#xf6;nroos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieminen_T/0/1/0/all/0/1\">Tommi Nieminen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raganato_A/0/1/0/all/0/1\">Alessandro Raganato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherrer_Y/0/1/0/all/0/1\">Yves Scherrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_R/0/1/0/all/0/1\">Raul Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virpioja_S/0/1/0/all/0/1\">Sami Virpioja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning for Language Understanding. (arXiv:2212.05961v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05961","description":"<p>Data augmentation is a widely used technique in machine learning to improve\nmodel performance. However, existing data augmentation techniques in natural\nlanguage understanding (NLU) may not fully capture the complexity of natural\nlanguage variations, and they can be challenging to apply to large datasets.\nThis paper proposes the Random Position Noise (RPN) algorithm, a novel data\naugmentation technique that operates at the word vector level. RPN modifies the\nword embeddings of the original text by introducing noise based on the existing\nvalues of selected word vectors, allowing for more fine-grained modifications\nand better capturing natural language variations. Unlike traditional data\naugmentation methods, RPN does not require gradients in the computational graph\nduring virtual sample updates, making it simpler to apply to large datasets.\nExperimental results demonstrate that RPN consistently outperforms existing\ndata augmentation techniques across various NLU tasks, including sentiment\nanalysis, natural language inference, and paraphrase detection. Moreover, RPN\nperforms well in low-resource settings and is applicable to any model featuring\na word embeddings layer. The proposed RPN algorithm is a promising approach for\nenhancing NLU performance and addressing the challenges associated with\ntraditional data augmentation techniques in large-scale NLU tasks. Our\nexperimental results demonstrated that the RPN algorithm achieved\nstate-of-the-art performance in all seven NLU tasks, thereby highlighting its\neffectiveness and potential for real-world NLU applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhengqing Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaolong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xuecong Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Huiwen Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhuanzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training. (arXiv:2212.10503v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10503","description":"<p>Prior work shows that it is possible to expand pretrained Masked Language\nModels (MLMs) to new languages by learning a new set of embeddings, while\nkeeping the transformer body frozen. Despite learning a small subset of\nparameters, this approach is not compute-efficient, as training the new\nembeddings requires a full forward and backward pass over the entire model. We\npropose mini-model adaptation, a compute-efficient alternative that builds a\nshallow mini-model from a fraction of a large model's parameters. New\nlanguage-specific embeddings can then be efficiently trained over the\nmini-model and plugged into the aligned large model for rapid cross-lingual\ntransfer. We explore two approaches to learn mini-models: MiniJoint, which\njointly pretrains the primary model and the mini-model using a single\ntransformer with a secondary MLM head at a middle layer; and MiniPost, where we\nstart from a regular pretrained model, build a mini-model by extracting and\nfreezing a few layers, and learn a small number of parameters on top.\nExperiments on XNLI, MLQA and PAWS-X show that mini-model adaptation matches\nthe performance of the standard approach using 2.3x less compute on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1\">Kelly Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment. (arXiv:2212.10549v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10549","description":"<p>Despite recent progress towards scaling up multimodal vision-language models,\nthese models are still known to struggle on compositional generalization\nbenchmarks such as Winoground. We find that a critical component lacking from\ncurrent vision-language models is relation-level alignment: the ability to\nmatch directional semantic relations in text (e.g., \"mug in grass\") with\nspatial relationships in the image (e.g., the position of the mug relative to\nthe grass). To tackle this problem, we show that relation alignment can be\nenforced by encouraging the directed language attention from 'mug' to 'grass'\n(capturing the semantic relation 'in') to match the directed visual attention\nfrom the mug to the grass. Tokens and their corresponding objects are softly\nidentified using the cross-modal attention. We prove that this notion of soft\nrelation alignment is equivalent to enforcing congruence between vision and\nlanguage attention matrices under a 'change of basis' provided by the\ncross-modal attention matrix. Intuitively, our approach projects visual\nattention into the language attention space to calculate its divergence from\nthe actual language attention, and vice versa. We apply our Cross-modal\nAttention Congruence Regularization (CACR) loss to UNITER and improve on the\nstate-of-the-art approach to Winoground.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1\">Rohan Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Rulin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender Neutralization for an Inclusive Machine Translation: from Theoretical Foundations to Open Challenges. (arXiv:2301.10075v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10075","description":"<p>Gender inclusivity in language technologies has become a prominent research\ntopic. In this study, we explore gender-neutral translation (GNT) as a form of\ngender inclusivity and a goal to be achieved by machine translation (MT)\nmodels, which have been found to perpetuate gender bias and discrimination.\nSpecifically, we focus on translation from English into Italian, a language\npair representative of salient gender-related linguistic transfer problems. To\ndefine GNT, we review a selection of relevant institutional guidelines for\ngender-inclusive language, discuss its scenarios of use, and examine the\ntechnical challenges of performing GNT in MT, concluding with a discussion of\npotential solutions to encourage advancements toward greater inclusivity in MT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piergentili_A/0/1/0/all/0/1\">Andrea Piergentili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fucci_D/0/1/0/all/0/1\">Dennis Fucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savoldi_B/0/1/0/all/0/1\">Beatrice Savoldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bentivogli_L/0/1/0/all/0/1\">Luisa Bentivogli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.04391","description":"<p>In industry deep learning application, our manually labeled data has a\ncertain number of noisy data. To solve this problem and achieve more than 90\nscore in dev dataset, we present a simple method to find the noisy data and\nre-label the noisy data by human, given the model predictions as references in\nhuman labeling. In this paper, we illustrate our idea for a broad set of deep\nlearning tasks, includes classification, sequence tagging, object detection,\nsequence generation, click-through rate prediction. The experimental results\nand human evaluation results verify our idea.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEVER: Learning to Verify Language-to-Code Generation with Execution. (arXiv:2302.08468v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.08468","description":"<p>The advent of large language models trained on code (code LLMs) has led to\nsignificant progress in language-to-code generation. State-of-the-art\napproaches in this area combine LLM decoding with sample pruning and reranking\nusing test cases or heuristics based on the execution results. However, it is\nchallenging to obtain test cases for many real-world language-to-code\napplications, and heuristics cannot well capture the semantic features of the\nexecution results, such as data type and value range, which often indicates the\ncorrectness of the program. In this work, we propose LEVER, a simple approach\nto improve language-to-code generation by learning to verify the generated\nprograms with their execution results. Specifically, we train verifiers to\ndetermine whether a program sampled from the LLMs is correct or not based on\nthe natural language input, the program itself and its execution results. The\nsampled programs are reranked by combining the verification score with the LLM\ngeneration probability, and marginalizing over programs with the same execution\nresults. On four datasets across the domains of table QA, math QA and basic\nPython programming, LEVER consistently improves over the base code LLMs(4.6% to\n10.9% with code-davinci-002) and achieves new state-of-the-art results on all\nof them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Srini Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Ves Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida I. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13988","description":"<p>Large language models (LLMs) are currently at the forefront of intertwining\nAI systems with human communication and everyday life. Due to rapid\ntechnological advances and their extreme versatility, LLMs nowadays have\nmillions of users and are at the cusp of being the main go-to technology for\ninformation retrieval, content generation, problem-solving, etc. Therefore, it\nis of great importance to thoroughly assess and scrutinize their capabilities.\nDue to increasingly complex and novel behavioral patterns in current LLMs, this\ncan be done by treating them as participants in psychology experiments that\nwere originally designed to test humans. For this purpose, the paper introduces\na new field of research called \"machine psychology\". The paper outlines how\ndifferent subfields of psychology can inform behavioral tests for LLMs. It\ndefines methodological standards for machine psychology research, especially by\nfocusing on policies for prompt designs. Additionally, it describes how\nbehavioral patterns discovered in LLMs are to be interpreted. In sum, machine\npsychology aims to discover emergent abilities in LLMs that cannot be detected\nby most traditional natural language processing benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagendorff_T/0/1/0/all/0/1\">Thilo Hagendorff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BenCoref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference Annotations. (arXiv:2304.03682v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.03682","description":"<p>Coreference Resolution is a well studied problem in NLP. While widely studied\nfor English and other resource-rich languages, research on coreference\nresolution in Bengali largely remains unexplored due to the absence of relevant\ndatasets. Bengali, being a low-resource language, exhibits greater\nmorphological richness compared to English. In this article, we introduce a new\ndataset, BenCoref, comprising coreference annotations for Bengali texts\ngathered from four distinct domains. This relatively small dataset contains\n5200 mention annotations forming 502 mention clusters within 48,569 tokens. We\ndescribe the process of creating this dataset and report performance of\nmultiple models trained using BenCoref. We expect that our work provides some\nvaluable insights on the variations in coreference phenomena across several\ndomains in Bengali and encourages the development of additional resources for\nBengali. Furthermore, we found poor crosslingual performance at zero-shot\nsetting from English, highlighting the need for more language-specific\nresources for this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rohan_S/0/1/0/all/0/1\">Shadman Rohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Mojammel Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1\">Mohammad Mamun Or Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Nabeel Mohammed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02301","description":"<p>Deploying large language models (LLMs) is challenging because they are memory\ninefficient and compute-intensive for practical applications. In reaction,\nresearchers train smaller task-specific models by either finetuning with human\nlabels or distilling using LLM-generated labels. However, finetuning and\ndistillation require large amounts of training data to achieve comparable\nperformance to LLMs. We introduce Distilling step-by-step, a new mechanism that\n(a) trains smaller models that outperform LLMs, and (b) achieves so by\nleveraging less training data needed by finetuning or distillation. Our method\nextracts LLM rationales as additional supervision for training small models\nwithin a multi-task framework. We present three findings across 4 NLP\nbenchmarks: First, compared to both finetuning and distillation, our mechanism\nachieves better performance with much fewer labeled/unlabeled training\nexamples. Second, compared to few-shot prompted LLMs, we achieve better\nperformance using substantially smaller model sizes. Third, we reduce both the\nmodel size and the amount of data required to outperform LLMs; our finetuned\n770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80%\nof available data on a benchmark, whereas standard finetuning the same T5 model\nstruggles to match even by using 100% of the dataset. We release the code at:\nhttps://github.com/google-research/distilling-step-by-step .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cheng-Yu Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Chih-Kuan Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakhost_H/0/1/0/all/0/1\">Hootan Nakhost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1\">Alexander Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cross-Linguistic Analysis of Intertemporal Preferences in GPT-3.5. (arXiv:2305.02531v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02531","description":"<p>Language has a strong influence on our perceptions of time and rewards. This\nraises the question of whether large language models, when asked the same\nquestion in different languages, show different preferences for rewards over\ntime and if their choices are similar to those of humans. In this study, we\nanalyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in\nmultiple languages, exploring preferences between smaller, sooner rewards and\nlarger, later rewards. Our results show that GPT displays greater patience when\nprompted in languages with weak future tense references (FTR), such as German\nand Mandarin, compared to languages with strong FTR, like English and French.\nThese findings are consistent with the existing literature and suggest a\ncorrelation between GPT's choices and the preferences of speakers of these\nlanguages. However, further analysis reveals that the preference for earlier or\nlater rewards does not systematically change with reward gaps, indicating a\nlexicographic preference for earlier payments. While GPT may capture intriguing\nvariations across languages, our findings indicate that the choices made by\nthese models do not correspond to those of human decision-makers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goli_A/0/1/0/all/0/1\">Ali Goli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amandeep Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Sentiment Analysis: A Survey. (arXiv:2305.07611v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07611","description":"<p>Multimodal sentiment analysis has become an important research area in the\nfield of artificial intelligence. With the latest advances in deep learning,\nthis technology has reached new heights. It has great potential for both\napplication and research, making it a popular research topic. This review\nprovides an overview of the definition, background, and development of\nmultimodal sentiment analysis. It also covers recent datasets and advanced\nmodels, emphasizing the challenges and future prospects of this technology.\nFinally, it looks ahead to future research directions. It should be noted that\nthis review provides constructive suggestions for promising research directions\nand building better performing multimodal sentiment analysis models, which can\nhelp researchers in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Songning Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xifeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoxuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaoxia Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of medium-large Language Models at zero-shot closed book generative question answering. (arXiv:2305.11991v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11991","description":"<p>Large language models (LLMs) have garnered significant attention, but the\ndefinition of \"large\" lacks clarity. This paper focuses on medium-sized\nlanguage models (MLMs), defined as having at least six billion parameters but\nless than 100 billion. The study evaluates MLMs regarding zero-shot generative\nquestion answering, which requires models to provide elaborate answers without\nexternal document retrieval. The paper introduces an own test dataset and\npresents results from human evaluation. Results show that combining the best\nanswers from different MLMs yielded an overall correct answer rate of 82.7%\nwhich is better than the 60.9% of ChatGPT. The best MLM achieved 71.8% and has\n33B parameters, which highlights the importance of using appropriate training\ndata for fine-tuning rather than solely relying on the number of parameters.\nMore fine-grained feedback should be used to further improve the quality of\nanswers. The open source community is quickly closing the gap to the best\ncommercial models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peinl_R/0/1/0/all/0/1\">Ren&#xe9; Peinl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wirth_J/0/1/0/all/0/1\">Johannes Wirth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network. (arXiv:2305.12493v4 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2305.12493","description":"<p>Contextual information plays a crucial role in speech recognition\ntechnologies and incorporating it into the end-to-end speech recognition models\nhas drawn immense interest recently. However, previous deep bias methods lacked\nexplicit supervision for bias tasks. In this study, we introduce a contextual\nphrase prediction network for an attention-based deep bias method. This network\npredicts context phrases in utterances using contextual embeddings and\ncalculates bias loss to assist in the training of the contextualized model. Our\nmethod achieved a significant word error rate (WER) reduction across various\nend-to-end speech recognition models. Experiments on the LibriSpeech corpus\nshow that our proposed model obtains a 12.1% relative WER improvement over the\nbaseline model, and the WER of the context phrases decreases relatively by\n40.5%. Moreover, by applying a context phrase filtering strategy, we also\neffectively eliminate the WER degradation when using a larger biasing list.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1\">Kaixun Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Zhanheng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mu_B/0/1/0/all/0/1\">Bingshen Mu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_T/0/1/0/all/0/1\">Tianyi Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models. (arXiv:2305.14705v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14705","description":"<p>Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be\nutilized to add learnable parameters to Large Language Models (LLMs) without\nincreasing inference cost. Instruction tuning is a technique for training LLMs\nto follow instructions. We advocate combining these two approaches, as we find\nthat MoE models benefit more from instruction tuning than dense models. In\nparticular, we conduct empirical studies across three experimental setups: (i)\nDirect finetuning on individual downstream tasks devoid of instruction tuning;\n(ii) Instructiontuning followed by in-context few-shot or zero-shot\ngeneralization on downstream tasks; and (iii) Instruction tuning supplemented\nby further finetuning on individual downstream tasks. In the first scenario,\nMoE models overall underperform dense models of identical computational\ncapacity. This narrative, however, dramatically changes with the introduction\nof instruction tuning (second and third scenario), used independently or in\nconjunction with task-specific finetuning. Our most powerful model,\nFLAN-MOE-32B, surpasses the performance of FLAN-PALM-62B on four benchmark\ntasks, while using only a third of the FLOPs. The advancements embodied\nbyFLAN-MOE inspire a reevaluation of the design principles of large-scale,\nhigh-performance language models in the framework of task-agnostic learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuexin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models. (arXiv:2305.16243v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16243","description":"<p>Augmenting language models with a retrieval mechanism has been shown to\nsignificantly improve their performance while keeping the number of parameters\nlow. Retrieval-augmented models commonly rely on a semantic retrieval mechanism\nbased on the similarity between dense representations of the query chunk and\npotential neighbors. In this paper, we study the state-of-the-art Retro model\nand observe that its performance gain is better explained by surface-level\nsimilarities, such as token overlap. Inspired by this, we replace the semantic\nretrieval in Retro with a surface-level method based on BM25, obtaining a\nsignificant reduction in perplexity. As full BM25 retrieval can be\ncomputationally costly for large datasets, we also apply it in a re-ranking\nscenario, gaining part of the perplexity reduction with minimal computational\noverhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doostmohammadi_E/0/1/0/all/0/1\">Ehsan Doostmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norlund_T/0/1/0/all/0/1\">Tobias Norlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhlmann_M/0/1/0/all/0/1\">Marco Kuhlmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansson_R/0/1/0/all/0/1\">Richard Johansson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning. (arXiv:2305.18169v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18169","description":"<p>In recent years, there has been significant progress in developing\npre-trained language models for NLP. However, these models often struggle when\nfine-tuned on small datasets. To address this issue, researchers have proposed\nvarious adaptation approaches. Prompt-based tuning is arguably the most common\nway, especially for larger models. Previous research shows that adding\ncontrastive learning to prompt-based fine-tuning is effective as it helps the\nmodel generate embeddings that are more distinguishable between classes, and it\ncan also be more sample-efficient as the model learns from positive and\nnegative examples simultaneously. One of the most important components of\ncontrastive learning is data augmentation, but unlike computer vision,\neffective data augmentation for NLP is still challenging. This paper proposes\nLM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language\nModels, which leverages prompt-based few-shot paraphrasing using generative\nlanguage models, especially large language models such as GPT-3 and OPT-175B,\nfor data augmentation. Our experiments on multiple text classification\nbenchmarks show that this augmentation method outperforms other methods, such\nas easy data augmentation, back translation, and multiple templates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rothe_S/0/1/0/all/0/1\">Sascha Rothe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1\">Yadollah Yaghoobzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WHAT, WHEN, and HOW to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue. (arXiv:2306.03361v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.03361","description":"<p>This paper presents a method for building a personalized open-domain dialogue\nsystem to address the WWH (WHAT, WHEN, and HOW) problem for natural response\ngeneration in a commercial setting, where personalized dialogue responses are\nheavily interleaved with casual response turns. The proposed approach involves\nweighted dataset blending, negative persona information augmentation methods,\nand the design of personalized conversation datasets to address the challenges\nof WWH in personalized, open-domain dialogue systems. Our work effectively\nbalances dialogue fluency and tendency to ground, while also introducing a\nresponse-type label to improve the controllability and explainability of the\ngrounded responses. The combination of these methods leads to more fluent\nconversations, as evidenced by subjective human evaluations as well as\nobjective evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_D/0/1/0/all/0/1\">Deuksin Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sunwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Ki Hyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seojin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeyoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1\">Eric Davis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monolingual and Cross-Lingual Knowledge Transfer for Topic Classification. (arXiv:2306.07797v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.07797","description":"<p>This article investigates the knowledge transfer from the RuQTopics dataset.\nThis Russian topical dataset combines a large sample number (361,560\nsingle-label, 170,930 multi-label) with extensive class coverage (76 classes).\nWe have prepared this dataset from the \"Yandex Que\" raw data. By evaluating the\nRuQTopics - trained models on the six matching classes of the Russian MASSIVE\nsubset, we have proved that the RuQTopics dataset is suitable for real-world\nconversational tasks, as the Russian-only models trained on this dataset\nconsistently yield an accuracy around 85\\% on this subset. We also have figured\nout that for the multilingual BERT, trained on the RuQTopics and evaluated on\nthe same six classes of MASSIVE (for all MASSIVE languages), the language-wise\naccuracy closely correlates (Spearman correlation 0.773 with p-value 2.997e-11)\nwith the approximate size of the pretraining BERT's data for the corresponding\nlanguage. At the same time, the correlation of the language-wise accuracy with\nthe linguistical distance from Russian is not statistically significant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karpov_D/0/1/0/all/0/1\">Dmitry Karpov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail Burtsev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking. (arXiv:2306.12245v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.12245","description":"<p>Entity Linking (EL) is a fundamental task for Information Extraction and\nKnowledge Graphs. The general form of EL (i.e., end-to-end EL) aims to first\nfind mentions in the given input document and then link the mentions to\ncorresponding entities in a specific knowledge base. Recently, the paradigm of\nretriever-reader promotes the progress of end-to-end EL, benefiting from the\nadvantages of dense entity retrieval and machine reading comprehension.\nHowever, the existing study only trains the retriever and the reader separately\nin a pipeline manner, which ignores the benefit that the interaction between\nthe retriever and the reader can bring to the task. To advance the\nretriever-reader paradigm to perform more perfectly on end-to-end EL, we\npropose BEER$^2$, a Bidirectional End-to-End training framework for Retriever\nand Reader. Through our designed bidirectional end-to-end training, BEER$^2$\nguides the retriever and the reader to learn from each other, make progress\ntogether, and ultimately improve EL performance. Extensive experiments on\nbenchmarks of multiple domains demonstrate the effectiveness of our proposed\nBEER$^2$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xingyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DMNER: Biomedical Entity Recognition by Detection and Matching. (arXiv:2306.15736v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.15736","description":"<p>Biomedical named entity recognition (BNER) serves as the foundation for\nnumerous biomedical text mining tasks. Unlike general NER, BNER require a\ncomprehensive grasp of the domain, and incorporating external knowledge beyond\ntraining data poses a significant challenge. In this study, we propose a novel\nBNER framework called DMNER. By leveraging existing entity representation\nmodels SAPBERT, we tackle BNER as a two-step process: entity boundary detection\nand biomedical entity matching. DMNER exhibits applicability across multiple\nNER scenarios: 1) In supervised NER, we observe that DMNER effectively\nrectifies the output of baseline NER models, thereby further enhancing\nperformance. 2) In distantly supervised NER, combining MRC and AutoNER as span\nboundary detectors enables DMNER to achieve satisfactory results. 3) For\ntraining NER by merging multiple datasets, we adopt a framework similar to\nDS-NER but additionally leverage ChatGPT to obtain high-quality phrases in the\ntraining. Through extensive experiments conducted on 10 benchmark datasets, we\ndemonstrate the versatility and effectiveness of DMNER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Junyi Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rongze Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Weiqi Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shanfeng Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The mapKurator System: A Complete Pipeline for Extracting and Linking Text from Historical Maps. (arXiv:2306.17059v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2306.17059","description":"<p>Scanned historical maps in libraries and archives are valuable repositories\nof geographic data that often do not exist elsewhere. Despite the potential of\nmachine learning tools like the Google Vision APIs for automatically\ntranscribing text from these maps into machine-readable formats, they do not\nwork well with large-sized images (e.g., high-resolution scanned documents),\ncannot infer the relation between the recognized text and other datasets, and\nare challenging to integrate with post-processing tools. This paper introduces\nthe mapKurator system, an end-to-end system integrating machine learning models\nwith a comprehensive data processing pipeline. mapKurator empowers automated\nextraction, post-processing, and linkage of text labels from large numbers of\nlarge-dimension historical map scans. The output data, comprising bounding\npolygons and recognized text, is in the standard GeoJSON format, making it\neasily modifiable within Geographic Information Systems (GIS). The proposed\nsystem allows users to quickly generate valuable data from large numbers of\nhistorical maps for in-depth analysis of the map content and, in turn,\nencourages map findability, accessibility, interoperability, and reusability\n(FAIR principles). We deployed the mapKurator system and enabled the processing\nof over 60,000 maps and over 100 million text/place names in the David Rumsey\nHistorical Map collection. We also demonstrated a seamless integration of\nmapKurator with a collaborative web platform to enable accessing automated\napproaches for extracting and linking text labels from historical map scans and\ncollective work to improve the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jina Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yijun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namgung_M/0/1/0/all/0/1\">Min Namgung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_L/0/1/0/all/0/1\">Leeje Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_Y/0/1/0/all/0/1\">Yao-Yi Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2306.17256","description":"<p>Recommender systems play a crucial role in helping users discover information\nthat aligns with their interests based on their past behaviors. However,\ndeveloping personalized recommendation systems becomes challenging when\nhistorical records of user-item interactions are unavailable, leading to what\nis known as the system cold-start recommendation problem. This issue is\nparticularly prominent in start-up businesses or platforms with insufficient\nuser engagement history. Previous studies focus on user or item cold-start\nscenarios, where systems could make recommendations for new users or items but\nare still trained with historical user-item interactions in the same domain,\nwhich cannot solve our problem. To bridge the gap, our research introduces an\ninnovative and effective approach, capitalizing on the capabilities of\npre-trained language models. We transform the recommendation process into\nsentiment analysis of natural languages containing information of user profiles\nand item attributes, where the sentiment polarity is predicted with prompt\nlearning. By harnessing the extensive knowledge housed within language models,\nthe prediction can be made without historical user-item interaction records. A\nbenchmark is also introduced to evaluate the proposed method under the\ncold-start setting, and the results demonstrate the effectiveness of our\nmethod. To the best of our knowledge, this is the first study to tackle the\nsystem cold-start recommendation problem. The benchmark and implementation of\nthe method are available at https://github.com/JacksonWuxs/PromptRec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuansheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huachi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Language Plasticity via Pretraining with Active Forgetting. (arXiv:2307.01163v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.01163","description":"<p>Pretrained language models (PLMs) are today the primary model for natural\nlanguage processing. Despite their impressive downstream performance, it can be\ndifficult to apply PLMs to new languages, a barrier to making their\ncapabilities universally accessible. While prior work has shown it possible to\naddress this issue by learning a new embedding layer for the new language,\ndoing so is both data and compute inefficient. We propose to use an active\nforgetting mechanism during pretraining, as a simple way of creating PLMs that\ncan quickly adapt to new languages. Concretely, by resetting the embedding\nlayer every K updates during pretraining, we encourage the PLM to improve its\nability of learning new embeddings within a limited number of updates, similar\nto a meta-learning effect. Experiments with RoBERTa show that models pretrained\nwith our forgetting mechanism not only demonstrate faster convergence during\nlanguage adaptation but also outperform standard ones in a low-data regime,\nparticularly for languages that are distant from English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1\">Kelly Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1\">Roberta Raileanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?. (arXiv:2304.14796v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2304.14796","description":"<p>Dense vector representations for textual data are crucial in modern NLP. Word\nembeddings and sentence embeddings estimated from raw texts are key in\nachieving state-of-the-art results in various tasks requiring semantic\nunderstanding. However, obtaining embeddings at the document level is\nchallenging due to computational requirements and lack of appropriate data.\nInstead, most approaches fall back on computing document embeddings based on\nsentence representations. Although there exist architectures and models to\nencode documents fully, they are in general limited to English and few other\nhigh-resourced languages. In this work, we provide a systematic comparison of\nmethods to produce document-level representations from sentences based on\nLASER, LaBSE, and Sentence BERT pre-trained multilingual models. We compare\ninput token number truncation, sentence averaging as well as some simple\nwindowing and in some cases new augmented and learnable approaches, on 3 multi-\nand cross-lingual tasks in 8 languages belonging to 3 different language\nfamilies. Our task-based extrinsic evaluations show that, independently of the\nlanguage, a clever combination of sentence embeddings is usually better than\nencoding the full document as a single unit, even when this is possible. We\ndemonstrate that while a simple sentence average results in a strong baseline\nfor classification tasks, more complex combinations are necessary for semantic\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sannigrahi_S/0/1/0/all/0/1\">Sonal Sannigrahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1\">Josef van Genabith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1\">Cristina Espana-Bonet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Collaborative Reasoning. (arXiv:2307.00165v1 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/2307.00165","description":"<p>Causal reasoning and logical reasoning are two important types of reasoning\nabilities for human intelligence. However, their relationship has not been\nextensively explored under machine intelligence context. In this paper, we\nexplore how the two reasoning abilities can be jointly modeled to enhance both\naccuracy and explainability of machine learning models. More specifically, by\nintegrating two important types of reasoning ability -- counterfactual\nreasoning and (neural) logical reasoning -- we propose Counterfactual\nCollaborative Reasoning (CCR), which conducts counterfactual logic reasoning to\nimprove the performance. In particular, we use recommender system as an example\nto show how CCR alleviate data scarcity, improve accuracy and enhance\ntransparency. Technically, we leverage counterfactual reasoning to generate\n\"difficult\" counterfactual training examples for data augmentation, which --\ntogether with the original training examples -- can enhance the model\nperformance. Since the augmented data is model irrelevant, they can be used to\nenhance any model, enabling the wide applicability of the technique. Besides,\nmost of the existing data augmentation methods focus on \"implicit data\naugmentation\" over users' implicit feedback, while our framework conducts\n\"explicit data augmentation\" over users explicit feedback based on\ncounterfactual logic reasoning. Experiments on three real-world datasets show\nthat CCR achieves better performance than non-augmented models and implicitly\naugmented models, and also improves model transparency by generating\ncounterfactual explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jianchao Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_M/0/1/0/all/0/1\">Max Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Juntao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-07-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}