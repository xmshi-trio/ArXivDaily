{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"LMentry: A Language Model Benchmark of Elementary Language Tasks. (arXiv:2211.02069v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02069","description":"<p>As the performance of large language models rapidly improves, benchmarks are\ngetting larger and more complex as well. We present LMentry, a benchmark that\navoids this \"arms race\" by focusing on a compact set of tasks that are trivial\nto humans, e.g. writing a sentence containing a specific word, identifying\nwhich words in a list belong to a specific category, or choosing which of two\nwords is longer. LMentry is specifically designed to provide quick and\ninterpretable insights into the capabilities and robustness of large language\nmodels. Our experiments reveal a wide variety of failure cases that, while\nimmediately obvious to humans, pose a considerable challenge for large language\nmodels, including OpenAI's latest 175B-parameter instruction-tuned model,\nTextDavinci002. LMentry complements contemporary evaluation approaches of large\nlanguage models, providing a quick, automatic, and easy-to-run \"unit test\",\nwithout resorting to large benchmark suites of complex tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Efrat_A/0/1/0/all/0/1\">Avia Efrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honovich_O/0/1/0/all/0/1\">Or Honovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic. (arXiv:2211.02098v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02098","description":"<p>Through their transfer learning abilities, highly-parameterized large\npre-trained language models have dominated the NLP landscape for a multitude of\ndownstream language tasks. Though linguistically proficient, the inability of\nthese models to incorporate the learning of non-linguistic entities (numerals\nand arithmetic reasoning) limits their usage for tasks that require numeric\ncomprehension or strict mathematical reasoning. However, as we illustrate in\nthis paper, building a general purpose language model that also happens to be\nproficient in mathematical reasoning is not as straight-forward as training it\non a numeric dataset. In this work, we develop a novel framework that enables\nlanguage models to be mathematically proficient while retaining their\nlinguistic prowess. Specifically, we offer information-theoretic interventions\nto overcome the catastrophic forgetting of linguistic skills that occurs while\ninjecting non-linguistic skills into language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Mandar Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muralidhar_N/0/1/0/all/0/1\">Nikhil Muralidhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_N/0/1/0/all/0/1\">Naren Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logographic Information Aids Learning Better Representations for Natural Language Inference. (arXiv:2211.02136v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02136","description":"<p>Statistical language models conventionally implement representation learning\nbased on the contextual distribution of words or other formal units, whereas\nany information related to the logographic features of written text are often\nignored, assuming they should be retrieved relying on the cooccurence\nstatistics. On the other hand, as language models become larger and require\nmore data to learn reliable representations, such assumptions may start to fall\nback, especially under conditions of data sparsity. Many languages, including\nChinese and Vietnamese, use logographic writing systems where surface forms are\nrepresented as a visual organization of smaller graphemic units, which often\ncontain many semantic cues. In this paper, we present a novel study which\nexplores the benefits of providing language models with logographic information\nin learning better semantic representations. We test our hypothesis in the\nnatural language inference (NLI) task by evaluating the benefit of computing\nmulti-modal representations that combine contextual information with glyph\ninformation. Our evaluation results in six languages with different typology\nand writing systems suggest significant benefits of using multi-modal\nembeddings in languages with logograhic systems, especially for words with less\noccurence statistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zijian Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataman_D/0/1/0/all/0/1\">Duygu Ataman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time-aware Prompting for Text Generation. (arXiv:2211.02162v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02162","description":"<p>In this paper, we study the effects of incorporating timestamps, such as\ndocument creation dates, into generation systems. Two types of time-aware\nprompts are investigated: (1) textual prompts that encode document timestamps\nin natural language sentences; and (2) linear prompts that convert timestamps\ninto continuous vectors. To explore extrapolation to future data points, we\nfurther introduce a new data-to-text generation dataset, TempWikiBio,\ncontaining more than 4 millions of chronologically ordered revisions of\nbiographical articles from English Wikipedia, each paired with structured\npersonal profiles. Through data-to-text generation on TempWikiBio, text-to-text\ngeneration on the content transfer dataset, and summarization on XSum, we show\nthat linear prompts on encoder and textual prompts improve the generation\nquality on all datasets. Despite having less performance drop when testing on\ndata drawn from a later time, linear prompts focus more on non-temporal\ninformation and are less sensitive to the given timestamps, according to human\nevaluations and sensitivity analyses. Meanwhile, textual prompts establish the\nassociation between the given timestamps and the output dates, yielding more\nfactual temporal information in the output.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shuyang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Video Moment Retrieval With Off-the-Shelf Models. (arXiv:2211.02178v1 [cs.CV])","link":"http://arxiv.org/abs/2211.02178","description":"<p>For the majority of the machine learning community, the expensive nature of\ncollecting high-quality human-annotated data and the inability to efficiently\nfinetune very large state-of-the-art pretrained models on limited compute are\nmajor bottlenecks for building models for new tasks. We propose a zero-shot\nsimple approach for one such task, Video Moment Retrieval (VMR), that does not\nperform any additional finetuning and simply repurposes off-the-shelf models\ntrained on other tasks. Our three-step approach consists of moment proposal,\nmoment-query matching and postprocessing, all using only off-the-shelf models.\nOn the QVHighlights benchmark for VMR, we vastly improve performance of\nprevious zero-shot approaches by at least 2.5x on all metrics and reduce the\ngap between zero-shot and state-of-the-art supervised by over 74%. Further, we\nalso show that our zero-shot approach beats non-pretrained supervised models on\nthe Recall metrics and comes very close on mAP metrics; and that it also\nperforms better than the best pretrained supervised model on shorter moments.\nFinally, we ablate and analyze our results and propose interesting future\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diwan_A/0/1/0/all/0/1\">Anuj Diwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond J. Mooney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Miko Team: Deep Learning Approach for Legal Question Answering in ALQAC 2022. (arXiv:2211.02200v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02200","description":"<p>We introduce efficient deep learning-based methods for legal document\nprocessing including Legal Document Retrieval and Legal Question Answering\ntasks in the Automated Legal Question Answering Competition (ALQAC 2022). In\nthis competition, we achieve 1\\textsuperscript{st} place in the first task and\n3\\textsuperscript{rd} place in the second task. Our method is based on the\nXLM-RoBERTa model that is pre-trained from a large amount of unlabeled corpus\nbefore fine-tuning to the specific tasks. The experimental results showed that\nour method works well in legal retrieval information tasks with limited labeled\ndata. Besides, this method can be applied to other information retrieval tasks\nin low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Van_H/0/1/0/all/0/1\">Hieu Nguyen Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Le Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral Regularization: an Inductive Bias for Sequence Modeling. (arXiv:2211.02255v1 [cs.LG])","link":"http://arxiv.org/abs/2211.02255","description":"<p>Various forms of regularization in learning tasks strive for different\nnotions of simplicity. This paper presents a spectral regularization technique,\nwhich attaches a unique inductive bias to sequence modeling based on an\nintuitive concept of simplicity defined in the Chomsky hierarchy. From\nfundamental connections between Hankel matrices and regular grammars, we\npropose to use the trace norm of the Hankel matrix, the tightest convex\nrelaxation of its rank, as the spectral regularizer. To cope with the fact that\nthe Hankel matrix is bi-infinite, we propose an unbiased stochastic estimator\nfor its trace norm. Ultimately, we demonstrate experimental results on Tomita\ngrammars, which exhibit the potential benefits of spectral regularization and\nvalidate the proposed stochastic estimator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_K/0/1/0/all/0/1\">Kaiwen Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1\">Guillaume Rabusseau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Late Fusion with Triplet Margin Objective for Multimodal Ideology Prediction and Analysis. (arXiv:2211.02269v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02269","description":"<p>Prior work on ideology prediction has largely focused on single modalities,\ni.e., text or images. In this work, we introduce the task of multimodal\nideology prediction, where a model predicts binary or five-point scale\nideological leanings, given a text-image pair with political content. We first\ncollect five new large-scale datasets with English documents and images along\nwith their ideological leanings, covering news articles from a wide range of US\nmainstream media and social media posts from Reddit and Twitter. We conduct\nin-depth analyses of news articles and reveal differences in image content and\nusage across the political spectrum. Furthermore, we perform extensive\nexperiments and ablation studies, demonstrating the effectiveness of targeted\npretraining objectives on different model components. Our best-performing\nmodel, a late-fusion architecture pretrained with a triplet objective over\nmultimodal content, outperforms the state-of-the-art text-only model by almost\n4% and a strong multimodal baseline with no pretraining by over 3%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1\">Changyuan Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Winston Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinliang Frederick Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiWOZ-DF -- A Dataflow implementation of the MultiWOZ dataset. (arXiv:2211.02303v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02303","description":"<p>Semantic Machines (SM) have introduced the use of the dataflow (DF) paradigm\nto dialogue modelling, using computational graphs to hierarchically represent\nuser requests, data, and the dialogue history [Semantic Machines et al. 2020].\nAlthough the main focus of that paper was the SMCalFlow dataset (to date, the\nonly dataset with \"native\" DF annotations), they also reported some results of\nan experiment using a transformed version of the commonly used MultiWOZ dataset\n[Budzianowski et al. 2018] into a DF format. In this paper, we expand the\nexperiments using DF for the MultiWOZ dataset, exploring some additional\nexperimental set-ups. The code and instructions to reproduce the experiments\nreported here have been released. The contributions of this paper are: 1.) A DF\nimplementation capable of executing MultiWOZ dialogues; 2.) Several versions of\nconversion of MultiWOZ into a DF format are presented; 3.) Experimental results\non state match and translation accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meron_J/0/1/0/all/0/1\">Joram Meron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guimaraes_V/0/1/0/all/0/1\">Victor Guimar&#xe3;es</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Once-for-All Sequence Compression for Self-Supervised Speech Models. (arXiv:2211.02332v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02332","description":"<p>The sequence length along the time axis is often the dominant factor of the\ncomputational cost of self-supervised speech models. Works have been proposed\nto reduce the sequence length for lowering the computational cost. However,\ndifferent downstream tasks have different tolerance of sequence compressing, so\na model that produces a fixed compressing rate may not fit all tasks. In this\nwork, we introduce a once-for-all (OFA) sequence compression framework for\nself-supervised speech models that supports a continuous range of compressing\nrates. The framework is evaluated on various tasks, showing marginal\ndegradation compared to the fixed compressing rate variants with a smooth\nperformance-efficiency trade-off. We further explore adaptive compressing rate\nlearning, demonstrating the ability to select task-specific preferred frame\nperiods without needing a grid search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsuan-Jui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yen Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimum Latency Training of Sequence Transducers for Streaming End-to-End Speech Recognition. (arXiv:2211.02333v1 [eess.AS])","link":"http://arxiv.org/abs/2211.02333","description":"<p>Sequence transducers, such as the RNN-T and the Conformer-T, are one of the\nmost promising models of end-to-end speech recognition, especially in streaming\nscenarios where both latency and accuracy are important. Although various\nmethods, such as alignment-restricted training and FastEmit, have been studied\nto reduce the latency, latency reduction is often accompanied with a\nsignificant degradation in accuracy. We argue that this suboptimal performance\nmight be caused because none of the prior methods explicitly model and reduce\nthe latency. In this paper, we propose a new training method to explicitly\nmodel and reduce the latency of sequence transducer models. First, we define\nthe expected latency at each diagonal line on the lattice, and show that its\ngradient can be computed efficiently within the forward-backward algorithm.\nThen we augment the transducer loss with this expected latency, so that an\noptimal trade-off between latency and accuracy is achieved. Experimental\nresults on the WSJ dataset show that the proposed minimum latency training\nreduces the latency of causal Conformer-T from 220 ms to 27 ms within a WER\ndegradation of 0.7%, and outperforms conventional alignment-restricted training\n(110 ms) and FastEmit (67 ms) methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shinohara_Y/0/1/0/all/0/1\">Yusuke Shinohara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Name Entity Recognition and Intent Classification Employing Deep Learning Architectures. (arXiv:2211.02415v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02415","description":"<p>Named Entity Recognition and Intent Classification are among the most\nimportant subfields of the field of Natural Language Processing. Recent\nresearch has lead to the development of faster, more sophisticated and\nefficient models to tackle the problems posed by those two tasks. In this work\nwe explore the effectiveness of two separate families of Deep Learning networks\nfor those tasks: Bidirectional Long Short-Term networks and Transformer-based\nnetworks. The models were trained and tested on the ATIS benchmark dataset for\nboth English and Greek languages. The purpose of this paper is to present a\ncomparative study of the two groups of networks for both languages and showcase\nthe results of our experiments. The models, being the current state-of-the-art,\nyielded impressive results and achieved high performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rizou_S/0/1/0/all/0/1\">Sofia Rizou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paflioti_A/0/1/0/all/0/1\">Antonia Paflioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theofilatos_A/0/1/0/all/0/1\">Angelos Theofilatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakali_A/0/1/0/all/0/1\">Athena Vakali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarigiannidis_G/0/1/0/all/0/1\">George Sarigiannidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzisavvas_K/0/1/0/all/0/1\">Konstantinos Ch. Chatzisavvas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLSE: Corpus of Linguistically Significant Entities. (arXiv:2211.02423v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02423","description":"<p>One of the biggest challenges of natural language generation (NLG) is the\nproper handling of named entities. Named entities are a common source of\ngrammar mistakes such as wrong prepositions, wrong article handling, or\nincorrect entity inflection. Without factoring linguistic representation, such\nerrors are often underrepresented when evaluating on a small set of arbitrarily\npicked argument values, or when translating a dataset from a linguistically\nsimpler language, like English, to a linguistically complex language, like\nRussian. However, for some applications, broadly precise grammatical\ncorrectness is critical -- native speakers may find entity-related grammar\nerrors silly, jarring, or even offensive.\n</p>\n<p>To enable the creation of more linguistically diverse NLG datasets, we\nrelease a Corpus of Linguistically Significant Entities (CLSE) annotated by\nlinguist experts. The corpus includes 34 languages and covers 74 different\nsemantic types to support various applications from airline ticketing to video\ngames. To demonstrate one possible use of CLSE, we produce an augmented version\nof the Schema-Guided Dialog Dataset, SGD-CLSE. Using the CLSE's entities and a\nsmall number of human translations, we create a linguistically representative\nNLG evaluation benchmark in three languages: French (high-resource), Marathi\n(low-resource), and Russian (highly inflected language). We establish quality\nbaselines for neural, template-based, and hybrid NLG systems and discuss the\nstrengths and weaknesses of each approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuklin_A/0/1/0/all/0/1\">Aleksandr Chuklin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Justin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Kale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dealing with Abbreviations in the Slovenian Biographical Lexicon. (arXiv:2211.02429v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02429","description":"<p>Abbreviations present a significant challenge for NLP systems because they\ncause tokenization and out-of-vocabulary errors. They can also make the text\nless readable, especially in reference printed books, where they are\nextensively used. Abbreviations are especially problematic in low-resource\nsettings, where systems are less robust to begin with. In this paper, we\npropose a new method for addressing the problems caused by a high density of\ndomain-specific abbreviations in a text. We apply this method to the case of a\nSlovenian biographical lexicon and evaluate it on a newly developed\ngold-standard dataset of 51 Slovenian biographies. Our abbreviation\nidentification method performs significantly better than commonly used ad-hoc\nsolutions, especially at identifying unseen abbreviations. We also propose and\npresent the results of a method for expanding the identified abbreviations in\ncontext.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daza_A/0/1/0/all/0/1\">Angel Daza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fokkens_A/0/1/0/all/0/1\">Antske Fokkens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erjavec_T/0/1/0/all/0/1\">Toma&#x17e; Erjavec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMAuC -- The Scientific Multi-Authorship Corpus. (arXiv:2211.02477v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02477","description":"<p>With an ever-growing number of new publications each day, scientific writing\nposes an interesting domain for authorship analysis of both single-author and\nmulti-author documents. Unfortunately, most existing corpora lack either\nmaterial from the science domain or the required metadata. Hence, we present\nSMAuC, a new metadata-rich corpus designed specifically for authorship analysis\nin scientific writing. With more than three million publications from various\nscientific disciplines, SMAuC is the largest openly available corpus for\nauthorship analysis to date. It combines a wide and diverse range of scientific\ntexts from the humanities and natural sciences with rich and curated metadata,\nincluding unique and carefully disambiguated author IDs. We hope SMAuC will\ncontribute significantly to advancing the field of authorship analysis in the\nscience domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sauer_P/0/1/0/all/0/1\">Philipp Sauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bevendorff_J/0/1/0/all/0/1\">Janek Bevendorff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gienapp_L/0/1/0/all/0/1\">Lukas Gienapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kircheis_W/0/1/0/all/0/1\">Wolfgang Kircheis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korner_E/0/1/0/all/0/1\">Erik K&#xf6;rner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing. (arXiv:2211.02483v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02483","description":"<p>The explosion of e-commerce has caused the need for processing and analysis\nof product titles, like entity typing in product titles. However, the rapid\nactivity in e-commerce has led to the rapid emergence of new entities, which is\ndifficult to be solved by general entity typing. Besides, product titles in\ne-commerce have very different language styles from text data in general\ndomain. In order to handle new entities in product titles and address the\nspecial language styles problem of product titles in e-commerce domain, we\npropose our textual entailment model with continuous prompt tuning based\nhypotheses and fusion embeddings for e-commerce entity typing. First, we\nreformulate the entity typing task into a textual entailment problem to handle\nnew entities that are not present during training. Second, we design a model to\nautomatically generate textual entailment hypotheses using a continuous prompt\ntuning method, which can generate better textual entailment hypotheses without\nmanual design. Third, we utilize the fusion embeddings of BERT embedding and\nCharacterBERT embedding with a two-layer MLP classifier to solve the problem\nthat the language styles of product titles in e-commerce are different from\nthat of general domain. To analyze the effect of each contribution, we compare\nthe performance of entity typing and textual entailment model, and conduct\nablation studies on continuous prompt tuning and fusion embeddings. We also\nevaluate the impact of different prompt template initialization for the\ncontinuous prompt tuning. We show our proposed model improves the average F1\nscore by around 2% compared to the baseline BERT entity typing model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Congying Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Weakly-Supervised Streaming Multilingual Speech Model with Truly Zero-Shot Capability. (arXiv:2211.02499v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02499","description":"<p>In this paper, we introduce our work of building a Streaming Multilingual\nSpeech Model (SM2), which can transcribe or translate multiple spoken languages\ninto texts of the target language. The backbone of SM2 is Transformer\nTransducer, which has high streaming capability. Instead of human labeled\nspeech translation (ST) data, SM2 models are trained using weakly supervised\ndata generated by converting the transcriptions in speech recognition corpora\nwith a machine translation service. With 351 thousand hours of anonymized\nspeech training data from 25 languages, SM2 models achieve comparable or even\nbetter ST quality than some recent popular large-scale non-streaming speech\nmodels. More importantly, we show that SM2 has the truly zero-shot capability\nwhen expanding to new target languages, yielding high quality ST results for\n{source-speech, target-text} pairs that are not seen during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_E/0/1/0/all/0/1\">Eric Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT for Long Documents: A Case Study of Automated ICD Coding. (arXiv:2211.02519v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02519","description":"<p>Transformer models have achieved great success across many NLP problems.\nHowever, previous studies in automated ICD coding concluded that these models\nfail to outperform some of the earlier solutions such as CNN-based models. In\nthis paper we challenge this conclusion. We present a simple and scalable\nmethod to process long text with the existing transformer models such as BERT.\nWe show that this method significantly improves the previous results reported\nfor transformer models in ICD coding, and is able to outperform one of the\nprominent CNN-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afkanpour_A/0/1/0/all/0/1\">Arash Afkanpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeel_S/0/1/0/all/0/1\">Shabir Adeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassani_H/0/1/0/all/0/1\">Hansenclever Bassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Epshteyn_A/0/1/0/all/0/1\">Arkady Epshteyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hongbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_I/0/1/0/all/0/1\">Isaac Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malihi_M/0/1/0/all/0/1\">Mahan Malihi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nauth_A/0/1/0/all/0/1\">Adrian Nauth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_R/0/1/0/all/0/1\">Raj Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woonna_S/0/1/0/all/0/1\">Sanjana Woonna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_S/0/1/0/all/0/1\">Shiva Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanal_E/0/1/0/all/0/1\">Elli Kanal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fomitchev_M/0/1/0/all/0/1\">Mikhail Fomitchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_D/0/1/0/all/0/1\">Donny Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-Based Substitute Recommendation Model Incorporating Weakly Supervised Customer Behavior Data. (arXiv:2211.02533v1 [cs.IR])","link":"http://arxiv.org/abs/2211.02533","description":"<p>The substitute-based recommendation is widely used in E-commerce to provide\nbetter alternatives to customers. However, existing research typically uses the\ncustomer behavior signals like co-view and view-but-purchase-another to capture\nthe substitute relationship. Despite its intuitive soundness, we find that such\nan approach might ignore the functionality and characteristics of products. In\nthis paper, we adapt substitute recommendation into language matching problem\nby taking product title description as model input to consider product\nfunctionality. We design a new transformation method to de-noise the signals\nderived from production data. In addition, we consider multilingual support\nfrom the engineering point of view. Our proposed end-to-end transformer-based\nmodel achieves both successes from offline and online experiments. The proposed\nmodel has been deployed in a large-scale E-commerce website for 11 marketplaces\nin 6 languages. Our proposed model is demonstrated to increase revenue by 19%\nbased on an online A/B experiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wenting Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Haoyang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xingjian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neppalli_N/0/1/0/all/0/1\">Naveen Neppalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biased Self-supervised learning for ASR. (arXiv:2211.02536v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02536","description":"<p>Self-supervised learning via masked prediction pre-training (MPPT) has shown\nimpressive performance on a range of speech-processing tasks. This paper\nproposes a method to bias self-supervised learning towards a specific task. The\ncore idea is to slightly finetune the model that is used to obtain the target\nsequence. This leads to better performance and a substantial increase in\ntraining speed. Furthermore, this paper proposes a variant of MPPT that allows\nlow-footprint streaming models to be trained effectively by computing the MPPT\nloss on masked and unmasked frames. These approaches are evaluated for\nautomatic speech recognition on the Librispeech corpus, where 100 hours of data\nserved as the labelled data and 860 hours as the unlabelled data. The biased\ntraining outperforms the unbiased training by 15.5% after 250k updates and\n23.8% after 100k updates on test-other. For the streaming models, the\npre-training approach yields a reduction in word error rate of 44.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreyssig_F/0/1/0/all/0/1\">Florian L. Kreyssig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yangyang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jinxi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sari_L/0/1/0/all/0/1\">Leda Sari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generation of Chinese classical poetry based on pre-trained model. (arXiv:2211.02541v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02541","description":"<p>In order to test whether artificial intelligence can create qualified\nclassical poetry like humans, the author proposes a study of Chinese classical\npoetry generation based on a pre-trained model. This paper mainly tries to use\nBART and other pre training models, proposes FS2TEXT and RR2TEXT to generate\nmetrical poetry text and even specific style poetry text, and solves the\nproblem that the user's writing intention gradually reduces the relevance of\nthe generated poetry text.\n</p>\n<p>In order to test the model's results, the authors selected ancient poets, by\ncombining it with BART's poetic model work, developed a set of AI poetry Turing\nproblems, it was reviewed by a group of poets and poetry writing researchers.\nThere were more than 600 participants, and the final results showed that,\nhigh-level poetry lovers can't distinguish between AI activity and human\nactivity, this indicates that the author's working methods are not\nsignificantly different from human activities. The model of poetry generation\nstudied by the author generalizes works that cannot be distinguished from those\nof advanced scholars.\n</p>\n<p>The number of modern Chinese poets has reached 5 million. However, many\nmodern Chinese poets lack language ability and skills as a result of their\nchildhood learning. However, many modern poets have no creative inspiration,\nand the author's model can help them. They can look at this model when they\nchoose words and phrases and they can write works based on the poems they\nalready have, and they can write their own poems. The importance of poetry lies\nin the author's thoughts and reflections. It doesn't matter how good AI poetry\nis. The only thing that matters is for people to see and inspire them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_L/0/1/0/all/0/1\">Lujin Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanyu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning for Speech Enhancement through Synthesis. (arXiv:2211.02542v1 [eess.AS])","link":"http://arxiv.org/abs/2211.02542","description":"<p>Modern speech enhancement (SE) networks typically implement noise suppression\nthrough time-frequency masking, latent representation masking, or\ndiscriminative signal prediction. In contrast, some recent works explore SE via\ngenerative speech synthesis, where the system's output is synthesized by a\nneural vocoder after an inherently lossy feature-denoising step. In this paper,\nwe propose a denoising vocoder (DeVo) approach, where a vocoder accepts noisy\nrepresentations and learns to directly synthesize clean speech. We leverage\nrich representations from self-supervised learning (SSL) speech models to\ndiscover relevant features. We conduct a candidate search across 15 potential\nSSL front-ends and subsequently train our vocoder adversarially with the best\nSSL configuration. Additionally, we demonstrate a causal version capable of\nrunning on streaming audio with 10ms latency and minimal performance\ndegradation. Finally, we conduct both objective evaluations and subjective\nlistening studies to show our system improves objective metrics and outperforms\nan existing state-of-the-art SE model subjectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Irvin_B/0/1/0/all/0/1\">Bryce Irvin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stamenovic_M/0/1/0/all/0/1\">Marko Stamenovic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kegler_M/0/1/0/all/0/1\">Mikolaj Kegler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Li-Chia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparison of SVM against Pre-trained Language Models (PLMs) for Text Classification Tasks. (arXiv:2211.02563v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02563","description":"<p>The emergence of pre-trained language models (PLMs) has shown great success\nin many Natural Language Processing (NLP) tasks including text classification.\nDue to the minimal to no feature engineering required when using these models,\nPLMs are becoming the de facto choice for any NLP task. However, for\ndomain-specific corpora (e.g., financial, legal, and industrial), fine-tuning a\npre-trained model for a specific task has shown to provide a performance\nimprovement. In this paper, we compare the performance of four different PLMs\non three public domain-free datasets and a real-world dataset containing\ndomain-specific words, against a simple SVM linear classifier with TFIDF\nvectorized text. The experimental results on the four datasets show that using\nPLMs, even fine-tuned, do not provide significant gain over the linear SVM\nclassifier. Hence, we recommend that for text classification tasks, traditional\nSVM along with careful feature engineering can pro-vide a cheaper and superior\nperformance than PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahba_Y/0/1/0/all/0/1\">Yasmen Wahba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhavji_N/0/1/0/all/0/1\">Nazim Madhavji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinbacher_J/0/1/0/all/0/1\">John Steinbacher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The 'Problem' of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation. (arXiv:2211.02570v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02570","description":"<p>Human variation in labeling is often considered noise. Annotation projects\nfor machine learning (ML) aim at minimizing human label variation, with the\nassumption to maximize data quality and in turn optimize and maximize machine\nlearning metrics. However, this conventional practice assumes that there exists\na ground truth, and neglects that there exists genuine human variation in\nlabeling due to disagreement, subjectivity in annotation or multiple plausible\nanswers. In this position paper, we argue that this big open problem of human\nlabel variation persists and critically needs more attention to move our field\nforward. This is because human label variation impacts all stages of the ML\npipeline: data, modeling and evaluation. However, few works consider all of\nthese dimensions jointly; and existing research is fragmented. We reconcile\ndifferent previously proposed notions of human label variation, provide a\nrepository of publicly-available datasets with un-aggregated labels, depict\napproaches proposed so far, identify gaps and suggest ways forward. As datasets\nare becoming increasingly available, we hope that this synthesized view on the\n'problem' will lead to an open discussion on possible strategies to devise\nfundamentally new directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating and Improving Factuality in Multimodal Abstractive Summarization. (arXiv:2211.02580v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02580","description":"<p>Current metrics for evaluating factuality for abstractive document\nsummarization have achieved high correlations with human judgment, but they do\nnot account for the vision modality and thus are not adequate for\nvision-and-language summarization. We propose CLIPBERTScore, a simple weighted\ncombination of CLIPScore and BERTScore to leverage the robustness and strong\nfactuality detection performance between image-summary and document-summary,\nrespectively. Next, due to the lack of meta-evaluation benchmarks to evaluate\nthe quality of multimodal factuality metrics, we collect human judgments of\nfactuality with respect to documents and images. We show that this simple\ncombination of two metrics in the zero-shot setting achieves higher\ncorrelations than existing factuality metrics for document summarization,\noutperforms an existing multimodal summarization metric, and performs\ncompetitively with strong multimodal factuality metrics specifically fine-tuned\nfor the task. Our thorough analysis demonstrates the robustness and high\ncorrelation of CLIPBERTScore and its components on four factuality\nmetric-evaluation benchmarks. Finally, we demonstrate two practical downstream\napplications of our CLIPBERTScore metric: for selecting important images to\nfocus on during training, and as a reward for reinforcement learning to improve\nfactuality of multimodal summary generation w.r.t automatic and human\nevaluation. Our data and code are publicly available at\nhttps://github.com/meetdavidwan/faithful-multimodal-summ\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1\">David Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer Architecture for Online Gesture Recognition of Mathematical Expressions. (arXiv:2211.02643v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02643","description":"<p>The Transformer architecture is shown to provide a powerful framework as an\nend-to-end model for building expression trees from online handwritten gestures\ncorresponding to glyph strokes. In particular, the attention mechanism was\nsuccessfully used to encode, learn and enforce the underlying syntax of\nexpressions creating latent representations that are correctly decoded to the\nexact mathematical expression tree, providing robustness to ablated inputs and\nunseen glyphs. For the first time, the encoder is fed with spatio-temporal data\ntokens potentially forming an infinitely large vocabulary, which finds\napplications beyond that of online gesture recognition. A new supervised\ndataset of online handwriting gestures is provided for training models on\ngeneric handwriting recognition tasks and a new metric is proposed for the\nevaluation of the syntactic correctness of the output expression trees. A small\nTransformer model suitable for edge inference was successfully trained to an\naverage normalised Levenshtein accuracy of 94%, resulting in valid postfix RPN\ntree representation for 94% of predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramo_M/0/1/0/all/0/1\">Mirco Ramo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestre_G/0/1/0/all/0/1\">Gu&#xe9;nol&#xe9; C.M. Silvestre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Anti-Vaccine Users on Twitter. (arXiv:2110.11333v3 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2110.11333","description":"<p>Vaccine hesitancy, which has recently been driven by online narratives,\nsignificantly degrades the efficacy of vaccination strategies, such as those\nfor COVID-19. Despite broad agreement in the medical community about the safety\nand efficacy of available vaccines, a large number of social media users\ncontinue to be inundated with false information about vaccines and are\nindecisive or unwilling to be vaccinated. The goal of this study is to better\nunderstand anti-vaccine sentiment by developing a system capable of\nautomatically identifying the users responsible for spreading anti-vaccine\nnarratives. We introduce a publicly available Python package capable of\nanalyzing Twitter profiles to assess how likely that profile is to share\nanti-vaccine sentiment in the future. The software package is built using text\nembedding methods, neural networks, and automated dataset generation and is\ntrained on several million tweets. We find this model can accurately detect\nanti-vaccine users up to a year before they tweet anti-vaccine hashtags or\nkeywords. We also show examples of how text analysis helps us understand\nanti-vaccine discussions by detecting moral and emotional differences between\nanti-vaccine spreaders on Twitter and regular users. Our results will help\nresearchers and policy-makers understand how users become anti-vaccine and what\nthey discuss on Twitter. Policy-makers can utilize this information for better\ntargeted campaigns that debunk harmful anti-vaccination myths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmitz_M/0/1/0/all/0/1\">Matheus Schmitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muric_G/0/1/0/all/0/1\">Goran Muri&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_K/0/1/0/all/0/1\">Keith Burghardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Golden Rule as a Heuristic to Measure the Fairness of Texts Using Machine Learning. (arXiv:2111.00107v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00107","description":"<p>In this paper we present a natural language programming framework to consider\nhow the fairness of acts can be measured. For the purposes of the paper, a fair\nact is defined as one that one would be accepting of if it were done to\noneself. The approach is based on an implementation of the golden rule (GR) in\nthe digital domain. Despite the GRs prevalence as an axiom throughout history,\nno transfer of this moral philosophy into computational systems exists. In this\npaper we consider how to algorithmically operationalise this rule so that it\nmay be used to measure sentences such as: the boy harmed the girl, and\ncategorise them as fair or unfair. A review and reply to criticisms of the GR\nis made. A suggestion of how the technology may be implemented to avoid unfair\nbiases in word embeddings is made - given that individuals would typically not\nwish to be on the receiving end of an unfair act, such as racism, irrespective\nof whether the corpus being used deems such discrimination as praiseworthy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Izzidien_A/0/1/0/all/0/1\">Ahmed Izzidien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stillwell_D/0/1/0/all/0/1\">David Stillwell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Know Thy Strengths: Comprehensive Dialogue State Tracking Diagnostics. (arXiv:2112.08321v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08321","description":"<p>Recent works that revealed the vulnerability of dialogue state tracking (DST)\nmodels to distributional shifts have made holistic comparisons on robustness\nand qualitative analyses increasingly important for understanding their\nrelative performance. We present our findings from standardized and\ncomprehensive DST diagnoses, which have previously been sparse and\nuncoordinated, using our toolkit, CheckDST, a collection of robustness tests\nand failure mode analytics. We discover that different classes of DST models\nhave clear strengths and weaknesses, where generation models are more promising\nfor handling language variety while span-based classification models are more\nrobust to unseen entities. Prompted by this discovery, we also compare\ncheckpoints from the same model and find that the standard practice of\nselecting checkpoints using validation loss/accuracy is prone to overfitting\nand each model class has distinct patterns of failure. Lastly, we demonstrate\nhow our diagnoses motivate a pre-finetuning procedure with non-dialogue data\nthat offers comprehensive improvements to generation models by alleviating the\nimpact of distributional shifts through transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyundong Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Christopher Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadagopan_K/0/1/0/all/0/1\">Kaushik Ram Sadagopan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayandeh_S/0/1/0/all/0/1\">Shahin Shayandeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1\">Ahmad Beirami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Speech Recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v8 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12886","description":"<p>Preschool evaluation is crucial because it gives teachers and parents\ninfluential knowledge about children's growth and development. The COVID-19\npandemic has highlighted the necessity of online assessment for preschool\nchildren. One of the areas that should be tested is their ability to speak.\nEmploying an Automatic Speech Recognition (ASR) system would not help since\nthey are pre-trained on voices that differ from children's in terms of\nfrequency and amplitude. Because most of these are pre-trained with data in a\nspecific range of amplitude, their objectives do not make them ready for voices\nin different amplitudes. To overcome this issue, we added a new objective to\nthe masking objective of the Wav2Vec 2.0 model called Random Frequency Pitch\n(RFP). In addition, we used our newly introduced dataset to fine-tune our model\nfor Meaningless Words (MW) and Rapid Automatic Naming (RAN) tests. Using\nmasking in concatenation with RFP outperforms the masking objective of Wav2Vec\n2.0 by reaching a Word Error Rate (WER) of 1.35. Our new approach reaches a WER\nof 6.45 on the Persian section of the CommonVoice dataset. Furthermore, our\nnovel methodology produces positive outcomes in zero- and few-shot scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortazavi_F/0/1/0/all/0/1\">Fatemeh Mortazavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v6 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2203.13366","description":"<p>For a long time, different recommendation tasks typically require designing\ntask-specific architectures and training objectives. As a result, it is hard to\ntransfer the learned knowledge and representations from one task to another,\nthus restricting the generalization ability of existing recommendation\napproaches, e.g., a sequential recommendation model can hardly be applied or\ntransferred to a review generation method. To deal with such issues,\nconsidering that language can describe almost anything and language grounding\nis a powerful medium to represent various problems or tasks, we present a\nflexible and unified text-to-text paradigm called \"Pretrain, Personalized\nPrompt, and Predict Paradigm\" (P5) for recommendation, which unifies various\nrecommendation tasks in a shared framework. In P5, all data such as user-item\ninteractions, user descriptions, item metadata, and user reviews are converted\nto a common format -- natural language sequences. The rich information from\nnatural language assists P5 to capture deeper semantics for personalization and\nrecommendation. Specifically, P5 learns different tasks with the same language\nmodeling objective during pretraining. Thus, it serves as the foundation model\nfor various downstream recommendation tasks, allows easy integration with other\nmodalities, and enables instruction-based recommendation based on prompts. P5\nadvances recommender systems from shallow model to deep model to big model, and\nwill revolutionize the technical form of recommender systems towards universal\nrecommendation engine. With adaptive personalized prompt for different users,\nP5 is able to make predictions in a zero-shot or few-shot manner and largely\nreduces the necessity for extensive fine-tuning. On several recommendation\nbenchmarks, we conduct experiments to show the effectiveness of P5. We release\nthe source code at https://github.com/jeykigung/P5.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zuohui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translation between Molecules and Natural Language. (arXiv:2204.11817v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.11817","description":"<p>We present $\\textbf{MolT5}$ $-$ a self-supervised learning framework for\npretraining models on a vast amount of unlabeled natural language text and\nmolecule strings. $\\textbf{MolT5}$ allows for new, useful, and challenging\nanalogs of traditional vision-language tasks, such as molecule captioning and\ntext-based de novo molecule generation (altogether: translation between\nmolecules and language), which we explore for the first time. Since\n$\\textbf{MolT5}$ pretrains models on single-modal data, it helps overcome the\nchemistry domain shortcoming of data scarcity. Furthermore, we consider several\nmetrics, including a new cross-modal embedding-based metric, to evaluate the\ntasks of molecule captioning and text-based molecule generation. Our results\nshow that $\\textbf{MolT5}$-based models are able to generate outputs, both\nmolecules and captions, which in many cases are high quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edwards_C/0/1/0/all/0/1\">Carl Edwards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ros_K/0/1/0/all/0/1\">Kevin Ros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honke_G/0/1/0/all/0/1\">Garrett Honke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polyglot Prompt: Multilingual Multitask PrompTraining. (arXiv:2204.14264v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.14264","description":"<p>This paper aims for a potential architectural improvement for multilingual\nlearning and asks: Can different tasks from different languages be modeled in a\nmonolithic framework, i.e. without any task/language-specific module? The\nbenefit of achieving this could open new doors for future multilingual\nresearch, including allowing systems trained on low resources to be further\nassisted by other languages as well as other tasks. We approach this goal by\ndeveloping a learning framework named Polyglot Prompting to exploit prompting\nmethods for learning a unified semantic space for different languages and tasks\nwith multilingual prompt engineering. We performed a comprehensive evaluation\nof 6 tasks, namely topic classification, sentiment classification, named entity\nrecognition, question answering, natural language inference, and summarization,\ncovering 24 datasets and 49 languages. The experimental results demonstrated\nthe efficacy of multilingual multitask prompt-based learning and led to\ninspiring observations. We also present an interpretable multilingual\nevaluation methodology and show how the proposed framework, multilingual\nmultitask prompt training, works. We release all datasets prompted in the best\nsetting and code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Parametric Domain Adaptation for End-to-End Speech Translation. (arXiv:2205.11211v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11211","description":"<p>End-to-End Speech Translation (E2E-ST) has received increasing attention due\nto the potential of its less error propagation, lower latency, and fewer\nparameters. However, the effectiveness of neural-based approaches to this task\nis severely limited by the available training corpus, especially for domain\nadaptation where in-domain triplet training data is scarce or nonexistent. In\nthis paper, we propose a novel non-parametric method that leverages\ndomain-specific text translation corpus to achieve domain adaptation for the\nE2E-ST system. To this end, we first incorporate an additional encoder into the\npre-trained E2E-ST model to realize text translation modelling, and then unify\nthe decoder's output representation for text and speech translation tasks by\nreducing the correspondent representation mismatch in available triplet\ntraining data. During domain adaptation, a k-nearest-neighbor (kNN) classifier\nis introduced to produce the final translation distribution using the external\ndatastore built by the domain-specific text translation corpus, while the\nuniversal output representation is adopted to perform a similarity search.\nExperiments on the Europarl-ST benchmark demonstrate that when in-domain text\ntranslation data is involved only, our proposed approach significantly improves\nbaseline by 12.82 BLEU on average in all translation directions, even\noutperforming the strong in-domain fine-tuning method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yichao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning. (arXiv:2205.12410v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12410","description":"<p>Standard fine-tuning of large pre-trained language models (PLMs) for\ndownstream tasks requires updating hundreds of millions to billions of\nparameters, and storing a large copy of the PLM weights for every task\nresulting in increased cost for storing, sharing and serving the models. To\naddress this, parameter-efficient fine-tuning (PEFT) techniques were introduced\nwhere small trainable components are injected in the PLM and updated during\nfine-tuning. We propose AdaMix as a general PEFT method that tunes a mixture of\nadaptation modules -- given the underlying PEFT method of choice -- introduced\nin each Transformer layer while keeping most of the PLM weights frozen. For\ninstance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture\nof low rank decomposition matrices like LoRA to improve downstream task\nperformance over the corresponding PEFT methods for fully supervised and\nfew-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the\nsame computational cost and the number of tunable parameters as the underlying\nPEFT method. By only tuning 0.1-0.2% of PLM parameters, we show that AdaMix\noutperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for\nboth NLU and NLG tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sahaj Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts. (arXiv:2205.12496v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12496","description":"<p>Question-answering datasets require a broad set of reasoning skills. We show\nhow to use question decompositions to teach language models these broad\nreasoning skills in a robust fashion. Specifically, we use widely available\nQDMR representations to programmatically create hard-to-cheat synthetic\ncontexts for real questions in six multi-step reasoning datasets. These\ncontexts are carefully designed to avoid reasoning shortcuts prevalent in real\ncontexts that prevent models from learning the right skills. This results in a\npretraining dataset, named TeaBReaC, containing 525K multi-step questions (with\nassociated formal programs) covering about 900 reasoning patterns. We show that\npretraining standard language models (LMs) on TeaBReaC before fine-tuning them\non target datasets improves their performance by up to 13 F1 points across 4\nmulti-step QA datasets, with up to 21 point gain on more complex questions. The\nresulting models also demonstrate higher robustness, with a 5-8 F1 point\nimprovement on two contrast sets. Furthermore, TeaBReaC pretraining\nsubstantially improves model performance and robustness even when starting with\nnumerate LMs pretrained using recent methods (e.g., PReasM, POET). Our work\nthus shows how to effectively use decomposition-guided contexts to robustly\nteach multi-step reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1\">Harsh Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memorization in NLP Fine-tuning Methods. (arXiv:2205.12506v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12506","description":"<p>Large language models are shown to present privacy risks through memorization\nof training data, and several recent works have studied such risks for the\npre-training phase. Little attention, however, has been given to the\nfine-tuning phase and it is not well understood how different fine-tuning\nmethods (such as fine-tuning the full model, the model head, and adapter)\ncompare in terms of memorization risk. This presents increasing concern as the\n\"pre-train and fine-tune\" paradigm proliferates. In this paper, we empirically\nstudy memorization of fine-tuning methods using membership inference and\nextraction attacks, and show that their susceptibility to attacks is very\ndifferent. We observe that fine-tuning the head of the model has the highest\nsusceptibility to attacks, whereas fine-tuning smaller adapters appears to be\nless vulnerable to known extraction attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uniyal_A/0/1/0/all/0/1\">Archit Uniyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_D/0/1/0/all/0/1\">David Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Management System with NLP-Assisted Annotations: A Brief Survey and Outlook. (arXiv:2206.07304v2 [cs.DB] UPDATED)","link":"http://arxiv.org/abs/2206.07304","description":"<p>Knowledge management systems (KMS) are in high demand for industrial\nresearchers, chemical or research enterprises, or evidence-based decision\nmaking. However, existing systems have limitations in categorizing and\norganizing paper insights or relationships. Traditional databases are usually\ndisjoint with logging systems, which limit its utility in generating concise,\ncollated overviews. In this work, we briefly survey existing approaches of this\nproblem space and propose a unified framework that utilizes relational\ndatabases to log hierarchical information to facilitate the research and\nwriting process, or generate useful knowledge from references or insights from\nconnected concepts. Our framework of bidirectional knowledge management system\n(BKMS) enables novel functionalities encompassing improved hierarchical\nnote-taking, AI-assisted brainstorming, and multi-directional relationships.\nPotential applications include managing inventories and changes for manufacture\nor research enterprises, or generating analytic reports with evidence-based\ndecision making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpaceQA: Answering Questions about the Design of Space Missions and Space Craft Concepts. (arXiv:2210.03422v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03422","description":"<p>We present SpaceQA, to the best of our knowledge the first open-domain QA\nsystem in Space mission design. SpaceQA is part of an initiative by the\nEuropean Space Agency (ESA) to facilitate the access, sharing and reuse of\ninformation about Space mission design within the agency and with the public.\nWe adopt a state-of-the-art architecture consisting of a dense retriever and a\nneural reader and opt for an approach based on transfer learning rather than\nfine-tuning due to the lack of domain-specific annotated data. Our evaluation\non a test set produced by ESA is largely consistent with the results originally\nreported by the evaluated retrievers and confirms the need of fine tuning for\nreading comprehension. As of writing this paper, ESA is piloting SpaceQA\ninternally.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Silva_A/0/1/0/all/0/1\">Andr&#xe9;s Garc&#xed;a-Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrio_C/0/1/0/all/0/1\">Cristian Berr&#xed;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Perez_J/0/1/0/all/0/1\">Jos&#xe9; Manuel G&#xf3;mez-P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Heras_J/0/1/0/all/0/1\">Jos&#xe9; Antonio Mart&#xed;nez-Heras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donati_A/0/1/0/all/0/1\">Alessandro Donati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roma_I/0/1/0/all/0/1\">Ilaria Roma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Quizzes to Support Training on Quality Management and Assurance in Space Science and Engineering. (arXiv:2210.03427v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03427","description":"<p>Quality management and assurance is key for space agencies to guarantee the\nsuccess of space missions, which are high-risk and extremely costly. In this\npaper, we present a system to generate quizzes, a common resource to evaluate\nthe effectiveness of training sessions, from documents about quality assurance\nprocedures in the Space domain. Our system leverages state of the art\nauto-regressive models like T5 and BART to generate questions, and a RoBERTa\nmodel to extract answers for such questions, thus verifying their suitability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Silva_A/0/1/0/all/0/1\">Andr&#xe9;s Garc&#xed;a-Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrio_C/0/1/0/all/0/1\">Cristian Berr&#xed;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Perez_J/0/1/0/all/0/1\">Jos&#xe9; Manuel G&#xf3;mez-P&#xe9;rez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalization of CTC Speech Recognition Models using Contextual Adapters and Adaptive Boosting. (arXiv:2210.09510v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09510","description":"<p>End-to-end speech recognition models trained using joint Connectionist\nTemporal Classification (CTC)-Attention loss have gained popularity recently.\nIn these models, a non-autoregressive CTC decoder is often used at inference\ntime due to its speed and simplicity. However, such models are hard to\npersonalize because of their conditional independence assumption that prevents\noutput tokens from previous time steps to influence future predictions. To\ntackle this, we propose a novel two-way approach that first biases the encoder\nwith attention over a predefined list of rare long-tail and out-of-vocabulary\n(OOV) words and then uses dynamic boosting and phone alignment network during\ndecoding to further bias the subword predictions. We evaluate our approach on\nopen-source VoxPopuli and in-house medical datasets to showcase a 60%\nimprovement in F1 score on domain-specific rare words over a strong CTC\nbaseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dingliwal_S/0/1/0/all/0/1\">Saket Dingliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkara_M/0/1/0/all/0/1\">Monica Sunkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1\">Sravan Bodapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronanki_S/0/1/0/all/0/1\">Srikanth Ronanki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farris_J/0/1/0/all/0/1\">Jeff Farris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Legal Domain Adaptation. (arXiv:2210.13712v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13712","description":"<p>Seeking legal advice is often expensive. Recent advancements in machine\nlearning for solving complex problems can be leveraged to help make legal\nservices more accessible to the public. However, real-life applications\nencounter significant challenges. State-of-the-art language models are growing\nincreasingly large, making parameter-efficient learning increasingly important.\nUnfortunately, parameter-efficient methods perform poorly with small amounts of\ndata, which are common in the legal domain (where data labelling costs are\nhigh). To address these challenges, we propose parameter-efficient legal domain\nadaptation, which uses vast unsupervised legal data from public legal forums to\nperform legal pre-training. This method exceeds or matches the fewshot\nperformance of existing models such as LEGAL-BERT on various legal tasks while\ntuning only approximately 0.1% of model parameters. Additionally, we show that\nour method can achieve calibration comparable to existing methods across\nseveral tasks. To the best of our knowledge, this work is among the first to\nexplore parameter-efficient methods of tuning language models in the legal\ndomain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jonathan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhambhoria_R/0/1/0/all/0/1\">Rohan Bhambhoria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolyHope: Two-Level Hope Speech Detection from Tweets. (arXiv:2210.14136v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14136","description":"<p>Hope is characterized as openness of spirit toward the future, a desire,\nexpectation, and wish for something to happen or to be true that remarkably\naffects human's state of mind, emotions, behaviors, and decisions. Hope is\nusually associated with concepts of desired expectations and\npossibility/probability concerning the future. Despite its importance, hope has\nrarely been studied as a social media analysis task. This paper presents a hope\nspeech dataset that classifies each tweet first into \"Hope\" and \"Not Hope\",\nthen into three fine-grained hope categories: \"Generalized Hope\", \"Realistic\nHope\", and \"Unrealistic Hope\" (along with \"Not Hope\"). English tweets in the\nfirst half of 2022 were collected to build this dataset. Furthermore, we\ndescribe our annotation process and guidelines in detail and discuss the\nchallenges of classifying hope and the limitations of the existing hope speech\ndetection corpora. In addition, we reported several baselines based on\ndifferent learning approaches, such as traditional machine learning, deep\nlearning, and transformers, to benchmark our dataset. We evaluated our\nbaselines using weighted-averaged and macro-averaged F1-scores. Observations\nshow that a strict process for annotator selection and detailed annotation\nguidelines enhanced the dataset's quality. This strict annotation process\nresulted in promising performance for simple machine learning classifiers with\nonly bi-grams; however, binary and multiclass hope speech detection results\nreveal that contextual embedding models have higher performance in this\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balouchzahi_F/0/1/0/all/0/1\">Fazlourrahman Balouchzahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidorov_G/0/1/0/all/0/1\">Grigori Sidorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AGReE: A system for generating Automated Grammar Reading Exercises. (arXiv:2210.16302v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16302","description":"<p>We describe the AGReE system, which takes user-submitted passages as input\nand automatically generates grammar practice exercises that can be completed\nwhile reading. Multiple-choice practice items are generated for a variety of\ndifferent grammar constructs: punctuation, articles, conjunctions, pronouns,\nprepositions, verbs, and nouns. We also conducted a large-scale human\nevaluation with around 4,500 multiple-choice practice items. We notice for 95%\nof items, a majority of raters out of five were able to identify the correct\nanswer and for 85% of cases, raters agree that there is only one correct answer\namong the choices. Finally, the error analysis shows that raters made the most\nmistakes for punctuation and conjunctions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Sophia Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somasundaran_S/0/1/0/all/0/1\">Swapna Somasundaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Debanjan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengxuan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning. (arXiv:2210.16952v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16952","description":"<p>Recent research shows synthetic data as a source of supervision helps\npretrained language models (PLM) transfer learning to new target tasks/domains.\nHowever, this idea is less explored for spatial language. We provide two new\ndata resources on multiple spatial language processing tasks. The first dataset\nis synthesized for transfer learning on spatial question answering (SQA) and\nspatial role labeling (SpRL). Compared to previous SQA datasets, we include a\nlarger variety of spatial relation types and spatial expressions. Our data\ngeneration process is easily extendable with new spatial expression lexicons.\nThe second one is a real-world SQA dataset with human-generated questions built\non an existing corpus with SPRL annotations. This dataset can be used to\nevaluate spatial language processing models in realistic situations. We show\npretraining with automatically generated data significantly improves the SOTA\nresults on several SQA and SPRL benchmarks, particularly when the training data\nin the target domain is small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirzaee_R/0/1/0/all/0/1\">Roshanak Mirzaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1\">Parisa Kordjamshidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality. (arXiv:2211.00768v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.00768","description":"<p>Recent visuolinguistic pre-trained models show promising progress on various\nend tasks such as image retrieval and video captioning. Yet, they fail\nmiserably on the recently proposed Winoground dataset, which challenges models\nto match paired images and English captions, with items constructed to overlap\nlexically but differ in meaning (e.g., \"there is a mug in some grass\" vs.\n\"there is some grass in a mug\"). By annotating the dataset using new\nfine-grained tags, we show that solving the Winoground task requires not just\ncompositional language understanding, but a host of other abilities like\ncommonsense reasoning or locating small, out-of-focus objects in low-resolution\nimages. In this paper, we identify the dataset's main challenges through a\nsuite of experiments on related tasks (probing task, image retrieval task),\ndata augmentation, and manual inspection of the dataset. Our analysis suggests\nthat a main challenge in visuolinguistic models may lie in fusing visual and\ntextual representations, rather than in compositional language understanding.\nWe release our annotation and code at\nhttps://github.com/ajd12342/why-winoground-hard .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diwan_A/0/1/0/all/0/1\">Anuj Diwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berry_L/0/1/0/all/0/1\">Layne Berry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Intrinsic Compositionality in Transformers with Tree Projections. (arXiv:2211.01288v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01288","description":"<p>When trained on language data, do transformers learn some arbitrary\ncomputation that utilizes the full capacity of the architecture or do they\nlearn a simpler, tree-like computation, hypothesized to underlie compositional\nmeaning systems like human languages? There is an apparent tension between\ncompositional accounts of human language understanding, which are based on a\nrestricted bottom-up computational process, and the enormous success of neural\nmodels like transformers, which can route information arbitrarily between\ndifferent parts of their input. One possibility is that these models, while\nextremely flexible in principle, in practice learn to interpret language\nhierarchically, ultimately building sentence representations close to those\npredictable by a bottom-up, tree-structured model. To evaluate this\npossibility, we describe an unsupervised and parameter-free method to\n\\emph{functionally project} the behavior of any transformer into the space of\ntree-structured networks. Given an input sentence, we produce a binary tree\nthat approximates the transformer's representation-building process and a score\nthat captures how \"tree-like\" the transformer's behavior is on the input. While\ncalculation of this score does not require training any additional models, it\nprovably upper-bounds the fit between a transformer and any tree-structured\napproximation. Using this method, we show that transformers for three different\ntasks become more tree-like over the course of training, in some cases\nunsupervisedly recovering the same trees as supervised parsers. These trees, in\nturn, are predictive of model behavior, with more tree-like models generalizing\nbetter on tests of compositional generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murty_S/0/1/0/all/0/1\">Shikhar Murty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pratyusha Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning of Neural Machine Translation within Low Forgetting Risk Regions. (arXiv:2211.01542v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01542","description":"<p>This paper considers continual learning of large-scale pretrained neural\nmachine translation model without accessing the previous training data or\nintroducing model separation. We argue that the widely used\nregularization-based methods, which perform multi-objective learning with an\nauxiliary loss, suffer from the misestimate problem and cannot always achieve a\ngood balance between the previous and new tasks. To solve the problem, we\npropose a two-stage training method based on the local features of the real\nloss. We first search low forgetting risk regions, where the model can retain\nthe performance on the previous task as the parameters are updated, to avoid\nthe catastrophic forgetting problem. Then we can continually train the model\nwithin this region only with the new training data to fit the new task.\nSpecifically, we propose two methods to search the low forgetting risk regions,\nwhich are based on the curvature of loss and the impacts of the parameters on\nthe model output, respectively. We conduct experiments on domain adaptation and\nmore challenging language adaptation tasks, and the experimental results show\nthat our method can achieve significant improvements compared with several\nstrong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuhao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bojie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}