{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2024-01-18T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Gemini Pro Defeated by GPT-4V: Evidence from Education. (arXiv:2401.08660v1 [cs.AI])","link":"http://arxiv.org/abs/2401.08660","description":"<p>This study compared the classification performance of Gemini Pro and GPT-4V\nin educational settings. Employing visual question answering (VQA) techniques,\nthe study examined both models' abilities to read text-based rubrics and then\nautomatically score student-drawn models in science education. We employed both\nquantitative and qualitative analyses using a dataset derived from\nstudent-drawn scientific models and employing NERIF (Notation-Enhanced Rubrics\nfor Image Feedback) prompting methods. The findings reveal that GPT-4V\nsignificantly outperforms Gemini Pro in terms of scoring accuracy and Quadratic\nWeighted Kappa. The qualitative analysis reveals that the differences may be\ndue to the models' ability to process fine-grained texts in images and overall\nimage classification performance. Even adapting the NERIF approach by further\nde-sizing the input images, Gemini Pro seems not able to perform as well as\nGPT-4V. The findings suggest GPT-4V's superior capability in handling complex\nmultimodal educational tasks. The study concludes that while both models\nrepresent advancements in AI, GPT-4V's higher performance makes it a more\nsuitable tool for educational applications involving multimodal data\ninterpretation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gyeong-Geon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latif_E/0/1/0/all/0/1\">Ehsan Latif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lehong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaoming Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges. (arXiv:2401.08664v1 [cs.AI])","link":"http://arxiv.org/abs/2401.08664","description":"<p>Online education platforms, leveraging the internet to distribute education\nresources, seek to provide convenient education but often fall short in\nreal-time communication with students. They often struggle to offer\npersonalized education resources due to the challenge of addressing the diverse\nobstacles students encounter throughout their learning journey. Recently, the\nemergence of large language models (LLMs), such as ChatGPT, offers the\npossibility for resolving this issue by comprehending individual requests.\nAlthough LLMs have been successful in various fields, creating an LLM-based\neducation system is still challenging for the wide range of educational skills\nrequired. This paper reviews the recently emerged LLM researches related to\neducational capabilities, including mathematics, writing, programming,\nreasoning, and knowledge-based question answering, with the aim to explore\ntheir potential in constructing the next-generation intelligent education\nsystem. Based on the current development status, we further outline two\napproaches for an LLM-based education system: a unified approach and a\nmixture-of-expert (MoE) approach. Finally, we explore the challenges and future\ndirections, providing new research opportunities and perspectives on adapting\nLLMs for education.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1\">Lingyue Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xianyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruiming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Answer Validation using Text Similarity. (arXiv:2401.08688v1 [cs.CL])","link":"http://arxiv.org/abs/2401.08688","description":"<p>Automated answer validation can help improve learning outcomes by providing\nappropriate feedback to learners, and by making question answering systems and\nonline learning solutions more widely available. There have been some works in\nscience question answering which show that information retrieval methods\noutperform neural methods, especially in the multiple choice version of this\nproblem. We implement Siamese neural network models and produce a generalised\nsolution to this problem. We compare our supervised model with other text\nsimilarity based solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_B/0/1/0/all/0/1\">Balaji Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_A/0/1/0/all/0/1\">Arjun Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piplani_L/0/1/0/all/0/1\">Lakshay Piplani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaumik_R/0/1/0/all/0/1\">Rini Bhaumik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmanaban_D/0/1/0/all/0/1\">Dhivya Padmanaban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhamurthy_S/0/1/0/all/0/1\">Shwetha Narasimhamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adhikary_C/0/1/0/all/0/1\">Chetan Adhikary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshapogu_S/0/1/0/all/0/1\">Subhash Deshapogu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation. (arXiv:2401.08694v1 [cs.CL])","link":"http://arxiv.org/abs/2401.08694","description":"<p>Large Language Models have emerged as prime candidates to tackle\nmisinformation mitigation. However, existing approaches struggle with\nhallucinations and overconfident predictions. We propose an uncertainty\nquantification framework that leverages both direct confidence elicitation and\nsampled-based consistency methods to provide better calibration for NLP\nmisinformation mitigation solutions. We first investigate the calibration of\nsample-based consistency methods that exploit distinct features of consistency\nacross sample sizes and stochastic levels. Next, we evaluate the performance\nand distributional shift of a robust numeric verbalization prompt across single\nvs. two-step confidence elicitation procedure. We also compare the performance\nof the same prompt with different versions of GPT and different numerical\nscales. Finally, we combine the sample-based consistency and verbalized methods\nto propose a hybrid framework that yields a better uncertainty estimation for\nGPT models. Overall, our work proposes novel uncertainty quantification methods\nthat will improve the reliability of Large Language Models in misinformation\nmitigation applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rivera_M/0/1/0/all/0/1\">Mauricio Rivera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godbout_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Godbout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbany_R/0/1/0/all/0/1\">Reihaneh Rabbany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelrine_K/0/1/0/all/0/1\">Kellin Pelrine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMToM-QA: Multimodal Theory of Mind Question Answering. (arXiv:2401.08743v1 [cs.AI])","link":"http://arxiv.org/abs/2401.08743","description":"<p>Theory of Mind (ToM), the ability to understand people's minds, is an\nessential ingredient for developing machines with human-level social\nintelligence. Recent machine learning models, particularly large language\nmodels, seem to show some aspects of ToM understanding. However, existing ToM\nbenchmarks use unimodal datasets - either video or text. Human ToM, on the\nother hand, is more than video or text understanding. People can flexibly\nreason about another person's mind based on conceptual representations (e.g.,\ngoals, beliefs, plans) extracted from any available data, which can include\nvisual cues, linguistic narratives, or both. To address this, we introduce a\nmultimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA\ncomprehensively evaluates machine ToM both on multimodal data and on different\nkinds of unimodal data about a person's activity in a household environment. To\nengineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian\nInverse Planning Accelerated by Language Models). BIP-ALM extracts unified\nrepresentations from multimodal data and utilizes language models for scalable\nBayesian inverse planning. We conducted a systematic comparison of human\nperformance, BIP-ALM, and state-of-the-art models, including GPT-4. The\nexperiments demonstrate that large language models and large multimodal models\nstill lack robust ToM capacity. BIP-ALM, on the other hand, shows promising\nresults, by leveraging the power of both model-based mental inference and\nlanguage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chuanyang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yutong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jiannan Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1\">Yen-Ling Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1\">Tomer Ullman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1\">Tianmin Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical Assistance. (arXiv:2401.08772v1 [cs.CL])","link":"http://arxiv.org/abs/2401.08772","description":"<p>In this work, we present HuixiangDou, a technical assistant powered by Large\nLanguage Models (LLM). This system is designed to assist algorithm developers\nby providing insightful responses to questions related to open-source algorithm\nprojects, such as computer vision and deep learning projects from OpenMMLab. We\nfurther explore the integration of this assistant into the group chats of\ninstant messaging (IM) tools such as WeChat and Lark. Through several iterative\nimprovements and trials, we have developed a sophisticated technical chat\nassistant capable of effectively answering users' technical questions without\ncausing message flooding. This paper's contributions include: 1) Designing an\nalgorithm pipeline specifically for group chat scenarios; 2) Verifying the\nreliable performance of text2vec in task rejection; 3) Identifying three\ncritical requirements for LLMs in technical-assistant-like products, namely\nscoring ability, In-Context Learning (ICL), and Long Context. We have made the\nsoftware and source code available at https://github.com/internlm/huixiangdou\nto aid in future research and application. HuixiangDou is applicable to any\ngroup chat within IM tools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1\">Huanjun Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant Reviews and Images on Social Media. (arXiv:2401.08825v1 [cs.LG])","link":"http://arxiv.org/abs/2401.08825","description":"<p>Online reviews in the form of user-generated content (UGC) significantly\nimpact consumer decision-making. However, the pervasive issue of not only human\nfake content but also machine-generated content challenges UGC's reliability.\nRecent advances in Large Language Models (LLMs) may pave the way to fabricate\nindistinguishable fake generated content at a much lower cost. Leveraging\nOpenAI's GPT-4-Turbo and DALL-E-2 models, we craft AiGen-FoodReview, a\nmulti-modal dataset of 20,144 restaurant review-image pairs divided into\nauthentic and machine-generated. We explore unimodal and multimodal detection\nmodels, achieving 99.80% multimodal accuracy with FLAVA. We use attributes from\nreadability and photographic theories to score reviews and images,\nrespectively, demonstrating their utility as hand-crafted features in scalable\nand interpretable detection models, with comparable performance. The paper\ncontributes by open-sourcing the dataset and releasing fake review detectors,\nrecommending its use in unimodal and multimodal fake review detection tasks,\nand evaluating linguistic and visual features in synthetic versus authentic\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gambetti_A/0/1/0/all/0/1\">Alessandro Gambetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qiwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective. (arXiv:2401.08833v1 [eess.AS])","link":"http://arxiv.org/abs/2401.08833","description":"<p>Existing studies on self-supervised speech representation learning have\nfocused on developing new training methods and applying pre-trained models for\ndifferent applications. However, the quality of these models is often measured\nby the performance of different downstream tasks. How well the representations\naccess the information of interest is less studied. In this work, we take a\ncloser look into existing self-supervised methods of speech from an\ninformation-theoretic perspective. We aim to develop metrics using mutual\ninformation to help practical problems such as model design and selection. We\nuse linear probes to estimate the mutual information between the target\ninformation and learned representations, showing another insight into the\naccessibility to the target information from speech representations. Further,\nwe explore the potential of evaluating representations in a self-supervised\nfashion, where we estimate the mutual information between different parts of\nthe data without using any labels. Finally, we show that both supervised and\nunsupervised measures echo the performance of the models on layer-wise linear\nprobing and speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_A/0/1/0/all/0/1\">Alexander H. Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeh_S/0/1/0/all/0/1\">Sung-Lin Yeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving ASR Contextual Biasing with Guided Attention. (arXiv:2401.08835v1 [cs.CL])","link":"http://arxiv.org/abs/2401.08835","description":"<p>In this paper, we propose a Guided Attention (GA) auxiliary training loss,\nwhich improves the effectiveness and robustness of automatic speech recognition\n(ASR) contextual biasing without introducing additional parameters. A common\nchallenge in previous literature is that the word error rate (WER) reduction\nbrought by contextual biasing diminishes as the number of bias phrases\nincreases. To address this challenge, we employ a GA loss as an additional\ntraining objective besides the Transducer loss. The proposed GA loss aims to\nteach the cross attention how to align bias phrases with text tokens or audio\nframes. Compared to studies with similar motivations, the proposed loss\noperates directly on the cross attention weights and is easier to implement.\nThrough extensive experiments based on Conformer Transducer with Contextual\nAdapter, we demonstrate that the proposed method not only leads to a lower WER\nbut also retains its effectiveness as the number of bias phrases increases.\nSpecifically, the GA loss decreases the WER of rare vocabularies by up to 19.2%\non LibriSpeech compared to the contextual biasing baseline, and up to 49.3%\ncompared to a vanilla Transducer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiyang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwangyoun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shon_S/0/1/0/all/0/1\">Suwon Shon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Felix Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_P/0/1/0/all/0/1\">Prashant Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using i-vectors for subject-independent cross-session EEG transfer learning. (arXiv:2401.08851v1 [cs.LG])","link":"http://arxiv.org/abs/2401.08851","description":"<p>Cognitive load classification is the task of automatically determining an\nindividual's utilization of working memory resources during performance of a\ntask based on physiologic measures such as electroencephalography (EEG). In\nthis paper, we follow a cross-disciplinary approach, where tools and\nmethodologies from speech processing are used to tackle this problem. The\ncorpus we use was released publicly in 2021 as part of the first passive\nbrain-computer interface competition on cross-session workload estimation. We\npresent our approach which used i-vector-based neural network classifiers to\naccomplish inter-subject cross-session EEG transfer learning, achieving 18%\nrelative improvement over equivalent subject-dependent models. We also report\nexperiments showing how our subject-independent models perform competitively on\nheld-out subjects and improve with additional subject data, suggesting that\nsubject-dependent training is not required for effective cognitive load\ndetermination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lasko_J/0/1/0/all/0/1\">Jonathan Lasko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jeff Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicoletti_M/0/1/0/all/0/1\">Mike Nicoletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sussman_Fort_J/0/1/0/all/0/1\">Jonathan Sussman-Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1\">Sooyoung Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_W/0/1/0/all/0/1\">William Hartmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NOTSOFAR-1 Challenge: New Datasets, Baseline, and Tasks for Distant Meeting Transcription. (arXiv:2401.08887v1 [cs.SD])","link":"http://arxiv.org/abs/2401.08887","description":"<p>We introduce the first Natural Office Talkers in Settings of Far-field Audio\nRecordings (``NOTSOFAR-1'') Challenge alongside datasets and baseline system.\nThe challenge focuses on distant speaker diarization and automatic speech\nrecognition (DASR) in far-field meeting scenarios, with single-channel and\nknown-geometry multi-channel tracks, and serves as a launch platform for two\nnew datasets: First, a benchmarking dataset of 315 meetings, averaging 6\nminutes each, capturing a broad spectrum of real-world acoustic conditions and\nconversational dynamics. It is recorded across 30 conference rooms, featuring\n4-8 attendees and a total of 35 unique speakers. Second, a 1000-hour simulated\ntraining dataset, synthesized with enhanced authenticity for real-world\ngeneralization, incorporating 15,000 real acoustic transfer functions. The\ntasks focus on single-device DASR, where multi-channel devices always share the\nsame known geometry. This is aligned with common setups in actual conference\nrooms, and avoids technical complexities associated with multi-device tasks. It\nalso allows for the development of geometry-specific solutions. The NOTSOFAR-1\nChallenge aims to advance research in the field of distant conversational\nspeech recognition, providing key resources to unlock the potential of\ndata-driven methods, which we believe are currently constrained by the absence\nof comprehensive high-quality training and benchmarking datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vinnikov_A/0/1/0/all/0/1\">Alon Vinnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurvitz_A/0/1/0/all/0/1\">Aviv Hurvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abramovski_I/0/1/0/all/0/1\">Igor Abramovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koubi_S/0/1/0/all/0/1\">Sharon Koubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurvich_I/0/1/0/all/0/1\">Ilya Gurvich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pe%60er_S/0/1/0/all/0/1\">Shai Pe`er</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elizalde_B/0/1/0/all/0/1\">Benjamin Martinez Elizalde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaer_S/0/1/0/all/0/1\">Shalev Shaer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yagev_S/0/1/0/all/0/1\">Stav Yagev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asher_Y/0/1/0/all/0/1\">Yossi Asher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivasankaran_S/0/1/0/all/0/1\">Sunit Sivasankaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Min Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huaming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krupka_E/0/1/0/all/0/1\">Eyal Krupka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partial Diacritization: A Context-Contrastive Inference Approach. (arXiv:2401.08919v1 [cs.CL])","link":"http://arxiv.org/abs/2401.08919","description":"<p>Diacritization plays a pivotal role in improving readability and\ndisambiguating the meaning of Arabic texts. Efforts have so far focused on\nmarking every eligible character (Full Diacritization). Comparatively\noverlooked, Partial Diacritzation (PD) is the selection of a subset of\ncharacters to be marked to aid comprehension where needed. Research has\nindicated that excessive diacritic marks can hinder skilled readers--reducing\nreading speed and accuracy. We conduct a behavioral experiment and show that\npartially marked text is often easier to read than fully marked text, and\nsometimes easier than plain text. In this light, we introduce\nContext-Contrastive Partial Diacritization (CCPD)--a novel approach to PD which\nintegrates seamlessly with existing Arabic diacritization systems. CCPD\nprocesses each word twice, once with context and once without, and diacritizes\nonly the characters with disparities between the two inferences. Further, we\nintroduce novel indicators for measuring partial diacritization quality (SR,\nPDER, HDER, ERE), essential for establishing this as a machine learning task.\nLastly, we introduce TD2, a Transformer-variant of an established model which\noffers a markedly different per formance profile on our proposed indicators\ncompared to all other known systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+ElNokrashy_M/0/1/0/all/0/1\">Muhammad ElNokrashy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AlKhamissi_B/0/1/0/all/0/1\">Badr AlKhamissi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReFT: Reasoning with Reinforced Fine-Tuning. (arXiv:2401.08967v1 [cs.CL])","link":"http://arxiv.org/abs/2401.08967","description":"<p>One way to enhance the reasoning capability of Large Language Models (LLMs)\nis to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)\nannotations. This approach does not show sufficiently strong generalization\nability, however, because the training only relies on the given CoT data. In\nmath problem-solving, for example, there is usually only one annotated\nreasoning path for each question in the training data. Intuitively, it would be\nbetter for the algorithm to learn from multiple annotated reasoning paths given\na question. To address this issue, we propose a simple yet effective approach\ncalled Reinforced Fine-Tuning (ReFT) to enhance the generalizability of\nlearning LLMs for reasoning, with math problem-solving as an example. ReFT\nfirst warmups the model with SFT, and then employs on-line reinforcement\nlearning, specifically the PPO algorithm in this paper, to further fine-tune\nthe model, where an abundance of reasoning paths are automatically sampled\ngiven the question and the rewards are naturally derived from the ground-truth\nanswers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that\nReFT significantly outperforms SFT, and the performance can be potentially\nfurther boosted by combining inference-time strategies such as majority voting\nand re-ranking. Note that ReFT obtains the improvement by learning from the\nsame training questions as SFT, without relying on extra or augmented training\nquestions. This indicates a superior generalization ability for ReFT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luong_T/0/1/0/all/0/1\">Trung Quoc Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zhanming Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaoran Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed Reality. (arXiv:2401.08973v1 [cs.CV])","link":"http://arxiv.org/abs/2401.08973","description":"<p>One key challenge in Augmented Reality is the placement of virtual content in\nnatural locations. Most existing automated techniques can only work with a\nclosed-vocabulary, fixed set of objects. In this paper, we introduce and\nevaluate several methods for automatic object placement using recent advances\nin open-vocabulary vision-language models. Through a multifaceted evaluation,\nwe identify a new state-of-the-art method, OCTO+. We also introduce a benchmark\nfor automatically evaluating the placement of virtual objects in augmented\nreality, alleviating the need for costly user studies. Through this, in\naddition to human evaluations, we find that OCTO+ places objects in a valid\nregion over 70% of the time, outperforming other methods on a range of metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Aditya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoffe_L/0/1/0/all/0/1\">Luke Yoffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollerer_T/0/1/0/all/0/1\">Tobias H&#xf6;llerer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Adapter Finetuning for Tail Languages in Streaming Multilingual ASR. (arXiv:2401.08992v1 [cs.CL])","link":"http://arxiv.org/abs/2401.08992","description":"<p>The end-to-end ASR model is often desired in the streaming multilingual\nscenario since it is easier to deploy and can benefit from pre-trained speech\nmodels such as powerful foundation models. Meanwhile, the heterogeneous nature\nand imbalanced data abundance of different languages may cause performance\ndegradation, leading to asynchronous peak performance for different languages\nduring training, especially on tail ones. Sometimes even the data itself may\nbecome unavailable as a result of the enhanced privacy protection. Existing\nwork tend to significantly increase the model size or learn language-specific\ndecoders to accommodate each language separately. In this study, we explore\nsimple yet effective Language-Dependent Adapter (LDA) finetuning under a\ncascaded Conformer transducer framework enhanced by teacher pseudo-labeling for\ntail languages in the streaming multilingual ASR. The adapter only accounts for\n0.4% of the full model per language. It is plugged into the frozen foundation\nmodel and is the only trainable module during the finetuning process with noisy\nstudent training. The final model merges the adapter parameters from different\ncheckpoints for different languages. The model performance is validated on a\nchallenging multilingual dictation dataset, which includes 39 tail languages\nacross Latin, Greek, Arabic, etc. Our proposed method brings 12.2% word error\nrate reduction on average and up to 37.5% on a single locale. Furthermore, we\nshow that our parameter-efficient LDA can match the quality of the full model\nfinetuning, thus greatly alleviating the asynchronous peak performance issue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Junwen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiujia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models. (arXiv:2401.09002v1 [cs.CL])","link":"http://arxiv.org/abs/2401.09002","description":"<p>In our research, we pioneer a novel approach to evaluate the effectiveness of\njailbreak attacks on Large Language Models (LLMs), such as GPT-4 and LLaMa2,\ndiverging from traditional robustness-focused binary evaluations. Our study\nintroduces two distinct evaluation frameworks: a coarse-grained evaluation and\na fine-grained evaluation. Each framework, using a scoring range from 0 to 1,\noffers a unique perspective, enabling a more comprehensive and nuanced\nevaluation of attack effectiveness and empowering attackers to refine their\nattack prompts with greater understanding. Furthermore, we have developed a\ncomprehensive ground truth dataset specifically tailored for jailbreak tasks.\nThis dataset not only serves as a crucial benchmark for our current study but\nalso establishes a foundational resource for future research, enabling\nconsistent and comparative analyses in this evolving field. Upon meticulous\ncomparison with traditional evaluation methods, we discovered that our\nevaluation aligns with the baseline's trend while offering a more profound and\ndetailed assessment. We believe that by accurately evaluating the effectiveness\nof attack prompts in the Jailbreak task, our work lays a solid foundation for\nassessing a wider array of similar or even more complex tasks in the realm of\nprompt injection, potentially revolutionizing this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+shu_D/0/1/0/all/0/1\">Dong shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Mingyu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Suiyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Beichen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zihao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])","link":"http://arxiv.org/abs/2401.09003","description":"<p>Despite recent progress in improving the mathematical reasoning ability of\nlarge language models(LLMs), solving competition-level math problems without\nthe use of external tools remains challenging for open-source LLMs. In this\nwork, we introduce the MMIQC dataset, a mixture of processed web data and\nsynthetic question-response pairs, to equip base models with better\nmathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by\nfine-tuning Mistral-7B(<a href=\"/abs/2310.06825\">arXiv:2310.06825</a>) on MMIQC, achieves 36.0\\% accuracy on\nMATH(<a href=\"/abs/2103.03874\">arXiv:2103.03874</a>), 5.8\\% higher than the previous (model size $\\sim$7B)\nSOTA. Our experiments also show that a large part of the improvement attributes\nto our novel augmentation method IQC(Iterative Question Composing), where we\niteratively ask an LLM to compose new questions from the given seed problems\nand do rejection sampling from another LLM. MMIQC has now been released on\nhttps://huggingface.co/datasets/Vivacem/MMIQC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haoxiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Andrew Chi-Chih Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explain Thyself Bully: Sentiment Aided Cyberbullying Detection with Explanation. (arXiv:2401.09023v1 [cs.CL])","link":"http://arxiv.org/abs/2401.09023","description":"<p>Cyberbullying has become a big issue with the popularity of different social\nmedia networks and online communication apps. While plenty of research is going\non to develop better models for cyberbullying detection in monolingual\nlanguage, there is very little research on the code-mixed languages and\nexplainability aspect of cyberbullying. Recent laws like \"right to\nexplanations\" of General Data Protection Regulation, have spurred research in\ndeveloping interpretable models rather than focusing on performance. Motivated\nby this we develop the first interpretable multi-task model called {\\em mExCB}\nfor automatic cyberbullying detection from code-mixed languages which can\nsimultaneously solve several tasks, cyberbullying detection,\nexplanation/rationale identification, target group detection and sentiment\nanalysis. We have introduced {\\em BullyExplain}, the first benchmark dataset\nfor explainable cyberbullying detection in code-mixed language. Each post in\n{\\em BullyExplain} dataset is annotated with four labels, i.e., {\\em bully\nlabel, sentiment label, target and rationales (explainability)}, i.e., which\nphrases are being responsible for annotating the post as a bully. The proposed\nmultitask framework (mExCB) based on CNN and GRU with word and sub-sentence\n(SS) level attention is able to outperform several baselines and state of the\nart models when applied on {\\em BullyExplain} dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maity_K/0/1/0/all/0/1\">Krishanu Maity</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_P/0/1/0/all/0/1\">Prince Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Raghav Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sriparna Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Summarisation of Large Sets: Towards a General Approach. (arXiv:2401.09041v1 [cs.CL])","link":"http://arxiv.org/abs/2401.09041","description":"<p>We are developing techniques to generate summary descriptions of sets of\nobjects. In this paper, we present and evaluate a rule-based NLG technique for\nsummarising sets of bibliographical references in academic papers. This extends\nour previous work on summarising sets of consumer products and shows how our\nmodel generalises across these two very different domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuptavanich_K/0/1/0/all/0/1\">Kittipitch Kuptavanich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_E/0/1/0/all/0/1\">Ehud Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees Van Deemter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharthan_A/0/1/0/all/0/1\">Advaith Siddharthan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMs for Relational Reasoning: How Far are We?. (arXiv:2401.09042v1 [cs.AI])","link":"http://arxiv.org/abs/2401.09042","description":"<p>Large language models (LLMs) have revolutionized many areas (e.g. natural\nlanguage processing, software engineering, etc.) by achieving state-of-the-art\nperformance on extensive downstream tasks. Aiming to achieve robust and general\nartificial intelligence, there has been a surge of interest in investigating\nthe reasoning ability of the LLMs. Whereas the textual and numerical reasoning\nbenchmarks adopted by previous works are rather shallow and simple, it is hard\nto conclude that the LLMs possess strong reasoning ability by merely achieving\npositive results on these benchmarks. Recent efforts have demonstrated that the\nLLMs are poor at solving sequential decision-making problems that require\ncommon-sense planning by evaluating their performance on the reinforcement\nlearning benchmarks. In this work, we conduct an in-depth assessment of several\nstate-of-the-art LLMs' reasoning ability based on the inductive logic\nprogramming (ILP) benchmark, which is broadly recognized as a representative\nand challenging measurement for evaluating logic program induction/synthesis\nsystems as it requires inducing strict cause-effect logic to achieve robust\ndeduction on independent and identically distributed (IID) and\nout-of-distribution (OOD) test samples. Our evaluations illustrate that\ncompared with the neural program induction systems which are much smaller in\nmodel size, the state-of-the-art LLMs are much poorer in terms of reasoning\nability by achieving much lower performance and generalization using either\nnatural language prompting or truth-value matrix prompting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yushi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiufeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junzhe Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_Y/0/1/0/all/0/1\">Yon Shin Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shang-wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])","link":"http://arxiv.org/abs/2401.09074","description":"<p>We investigate the extent to which Large Language Models (LLMs) can simulate\nthe execution of computer code and algorithms. We begin by looking straight\nline programs, and show that current LLMs demonstrate poor performance even\nwith such simple programs -- performance rapidly degrades with the length of\ncode. We then investigate the ability of LLMs to simulate programs that contain\ncritical paths and redundant instructions. We also go beyond straight line\nprogram simulation with sorting algorithms and nested loops, and we show the\ncomputational complexity of a routine directly affects the ability of an LLM to\nsimulate its execution. We observe that LLMs execute instructions sequentially\nand with a low error margin only for short programs or standard procedures.\nLLMs' code simulation is in tension with their pattern recognition and\nmemorisation capabilities: on tasks where memorisation is detrimental, we\npropose a novel prompting method to simulate code execution line by line.\nEmpirically, our new Chain of Simulation (CoSm) method improves on the standard\nChain of Thought prompting approach by avoiding the pitfalls of memorisation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1\">Emanuele La Malfa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinhuber_C/0/1/0/all/0/1\">Christoph Weinhuber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torre_O/0/1/0/all/0/1\">Orazio Torre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fangru Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_A/0/1/0/all/0/1\">Anthony Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shadbolt_N/0/1/0/all/0/1\">Nigel Shadbolt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wooldridge_M/0/1/0/all/0/1\">Michael Wooldridge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What makes for a 'good' social actor? Using respect as a lens to evaluate interactions with language agents. (arXiv:2401.09082v1 [cs.CL])","link":"http://arxiv.org/abs/2401.09082","description":"<p>With the growing popularity of dialogue agents based on large language models\n(LLMs), urgent attention has been drawn to finding ways to ensure their\nbehaviour is ethical and appropriate. These are largely interpreted in terms of\nthe 'HHH' criteria: making outputs more helpful and honest, and avoiding\nharmful (biased, toxic, or inaccurate) statements. Whilst this semantic focus\nis useful from the perspective of viewing LLM agents as mere mediums for\ninformation, it fails to account for pragmatic factors that can make the same\nutterance seem more or less offensive or tactless in different social\nsituations. We propose an approach to ethics that is more centred on relational\nand situational factors, exploring what it means for a system, as a social\nactor, to treat an individual respectfully in a (series of) interaction(s). Our\nwork anticipates a set of largely unexplored risks at the level of situated\ninteraction, and offers practical suggestions to help LLM technologies behave\nas 'good' social actors and treat people respectfully.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alberts_L/0/1/0/all/0/1\">Lize Alberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keeling_G/0/1/0/all/0/1\">Geoff Keeling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCroskery_A/0/1/0/all/0/1\">Amanda McCroskery</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asynchronous Local-SGD Training for Language Modeling. (arXiv:2401.09135v1 [cs.LG])","link":"http://arxiv.org/abs/2401.09135","description":"<p>Local stochastic gradient descent (Local-SGD), also referred to as federated\naveraging, is an approach to distributed optimization where each device\nperforms more than one SGD update per communication. This work presents an\nempirical study of {\\it asynchronous} Local-SGD for training language models;\nthat is, each worker updates the global parameters as soon as it has finished\nits SGD steps. We conduct a comprehensive investigation by examining how worker\nhardware heterogeneity, model size, number of workers, and optimizer could\nimpact the learning performance. We find that with naive implementations,\nasynchronous Local-SGD takes more iterations to converge than its synchronous\ncounterpart despite updating the (global) model parameters more frequently. We\nidentify momentum acceleration on the global parameters when worker gradients\nare stale as a key challenge. We propose a novel method that utilizes a delayed\nNesterov momentum update and adjusts the workers' local training steps based on\ntheir computation speed. This approach, evaluated with models up to 150M\nparameters on the C4 dataset, matches the performance of synchronous Local-SGD\nin terms of perplexity per update step, and significantly surpasses it in terms\nof wall clock time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhaparia_R/0/1/0/all/0/1\">Rachita Chhaparia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douillard_A/0/1/0/all/0/1\">Arthur Douillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1\">Satyen Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusu_A/0/1/0/all/0/1\">Andrei A. Rusu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiajun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1\">Marc&#x27;Aurelio Ranzato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System. (arXiv:2401.09150v1 [cs.CL])","link":"http://arxiv.org/abs/2401.09150","description":"<p>In the contemporary information era, significantly accelerated by the advent\nof Large-scale Language Models, the proliferation of scientific literature is\nreaching unprecedented levels. Researchers urgently require efficient tools for\nreading and summarizing academic papers, uncovering significant scientific\nliterature, and employing diverse interpretative methodologies. To address this\nburgeoning demand, the role of automated scientific literature interpretation\nsystems has become paramount. However, prevailing models, both commercial and\nopen-source, confront notable challenges: they often overlook multimodal data,\ngrapple with summarizing over-length texts, and lack diverse user interfaces.\nIn response, we introduce an open-source multi-modal automated academic paper\ninterpretation system (MMAPIS) with three-step process stages, incorporating\nLLMs to augment its functionality. Our system first employs the hybrid modality\npreprocessing and alignment module to extract plain text, and tables or figures\nfrom documents separately. It then aligns this information based on the section\nnames they belong to, ensuring that data with identical section names are\ncategorized under the same section. Following this, we introduce a hierarchical\ndiscourse-aware summarization method. It utilizes the extracted section names\nto divide the article into shorter text segments, facilitating specific\nsummarizations both within and between sections via LLMs with specific prompts.\nFinally, we have designed four types of diversified user interfaces, including\npaper recommendation, multimodal Q\\&amp;A, audio broadcasting, and interpretation\nblog, which can be widely applied across various scenarios. Our qualitative and\nquantitative evaluations underscore the system's superiority, especially in\nscientific summarization, where it outperforms solutions relying solely on\nGPT-4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1\">Feng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Strategies for Domain Specific Question Answering under Low Annotation Budget Constraints. (arXiv:2401.09168v1 [cs.CL])","link":"http://arxiv.org/abs/2401.09168","description":"<p>The progress introduced by pre-trained language models and their fine-tuning\nhas resulted in significant improvements in most downstream NLP tasks. The\nunsupervised training of a language model combined with further target task\nfine-tuning has become the standard QA fine-tuning procedure. In this work, we\ndemonstrate that this strategy is sub-optimal for fine-tuning QA models,\nespecially under a low QA annotation budget, which is a usual setting in\npractice due to the extractive QA labeling cost. We draw our conclusions by\nconducting an exhaustive analysis of the performance of the alternatives of the\nsequential fine-tuning strategy on different QA datasets. Based on the\nexperiments performed, we observed that the best strategy to fine-tune the QA\nmodel in low-budget settings is taking a pre-trained language model (PLM) and\nthen fine-tuning PLM with a dataset composed of the target dataset and SQuAD\ndataset. With zero extra annotation effort, the best strategy outperforms the\nstandard strategy by 2.28% to 6.48%. Our experiments provide one of the first\ninvestigations on how to best fine-tune a QA system under a low budget and are\ntherefore of the utmost practical interest to the QA practitioners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kunpeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diefenbach_D/0/1/0/all/0/1\">Dennis Diefenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gourru_A/0/1/0/all/0/1\">Antoine Gourru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gravier_C/0/1/0/all/0/1\">Christophe Gravier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QAnswer: Towards Question Answering Search over Websites. (arXiv:2401.09175v1 [cs.CL])","link":"http://arxiv.org/abs/2401.09175","description":"<p>Question Answering (QA) is increasingly used by search engines to provide\nresults to their end-users, yet very few websites currently use QA technologies\nfor their search functionality. To illustrate the potential of QA technologies\nfor the website search practitioner, we demonstrate web searches that combine\nQA over knowledge graphs and QA over free text -- each being usually tackled\nseparately. We also discuss the different benefits and drawbacks of both\napproaches for web site searches. We use the case studies made of websites\nhosted by the Wikimedia Foundation (namely Wikipedia and Wikidata). Differently\nfrom a search engine (e.g. Google, Bing, etc), the data are indexed integrally,\ni.e. we do not index only a subset, and they are indexed exclusively, i.e. we\nindex only data available on the corresponding website.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kunpeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Defretiere_C/0/1/0/all/0/1\">Clement Defretiere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diefenbach_D/0/1/0/all/0/1\">Dennis Diefenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gravier_C/0/1/0/all/0/1\">Christophe Gravier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gourru_A/0/1/0/all/0/1\">Antoine Gourru</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniVIE: A Unified Label Space Approach to Visual Information Extraction from Form-like Documents. (arXiv:2401.09220v1 [cs.CL])","link":"http://arxiv.org/abs/2401.09220","description":"<p>Existing methods for Visual Information Extraction (VIE) from form-like\ndocuments typically fragment the process into separate subtasks, such as key\ninformation extraction, key-value pair extraction, and choice group extraction.\nHowever, these approaches often overlook the hierarchical structure of form\ndocuments, including hierarchical key-value pairs and hierarchical choice\ngroups. To address these limitations, we present a new perspective, reframing\nVIE as a relation prediction problem and unifying labels of different tasks\ninto a single label space. This unified approach allows for the definition of\nvarious relation types and effectively tackles hierarchical relationships in\nform-like documents. In line with this perspective, we present UniVIE, a\nunified model that addresses the VIE problem comprehensively. UniVIE functions\nusing a coarse-to-fine strategy. It initially generates tree proposals through\na tree proposal network, which are subsequently refined into hierarchical trees\nby a relation decoder module. To enhance the relation prediction capabilities\nof UniVIE, we incorporate two novel tree constraints into the relation decoder:\na tree attention mask and a tree level embedding. Extensive experimental\nevaluations on both our in-house dataset HierForms and a publicly available\ndataset SIBR, substantiate that our method achieves state-of-the-art results,\nunderscoring the effectiveness and potential of our unified approach in\nadvancing the field of VIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiawei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weihong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhuoyao Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Q/0/1/0/all/0/1\">Qiang Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Offensive Language Detection: A Systematic Review of Datasets, Transfer Approaches and Challenges. (arXiv:2401.09244v1 [cs.CL])","link":"http://arxiv.org/abs/2401.09244","description":"<p>The growing prevalence and rapid evolution of offensive language in social\nmedia amplify the complexities of detection, particularly highlighting the\nchallenges in identifying such content across diverse languages. This survey\npresents a systematic and comprehensive exploration of Cross-Lingual Transfer\nLearning (CLTL) techniques in offensive language detection in social media. Our\nstudy stands as the first holistic overview to focus exclusively on the\ncross-lingual scenario in this domain. We analyse 67 relevant papers and\ncategorise these studies across various dimensions, including the\ncharacteristics of multilingual datasets used, the cross-lingual resources\nemployed, and the specific CLTL strategies implemented. According to \"what to\ntransfer\", we also summarise three main CLTL transfer approaches: instance,\nfeature, and parameter transfer. Additionally, we shed light on the current\nchallenges and future research opportunities in this field. Furthermore, we\nhave made our survey resources available online, including two comprehensive\ntables that provide accessible references to the multilingual datasets and CLTL\nmethods used in the reviewed literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Aiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Emotions, Demographic Information and Implicit User Feedback in Task-Oriented Document-Grounded Dialogues. (arXiv:2401.09248v1 [cs.CL])","link":"http://arxiv.org/abs/2401.09248","description":"<p>The success of task-oriented and document-grounded dialogue systems depends\non users accepting and enjoying using them. To achieve this, recently published\nwork in the field of Human-Computer Interaction suggests that the combination\nof considering demographic information, user emotions and learning from the\nimplicit feedback in their utterances, is particularly important. However,\nthese findings have not yet been transferred to the field of Natural Language\nProcessing, where these data are primarily studied separately. Accordingly, no\nsufficiently annotated dataset is available. To address this gap, we introduce\nFEDI, the first English dialogue dataset for task-oriented document-grounded\ndialogues annotated with demographic information, user emotions and implicit\nfeedback. Our experiments with FLAN-T5, GPT-2 and LLaMA-2 show that these data\nhave the potential to improve task completion and the factual consistency of\nthe generated responses and user acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrak_D/0/1/0/all/0/1\">Dominic Petrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Thy Thy Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. (arXiv:2303.00915v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.00915","description":"<p>Biomedical data is inherently multimodal, comprising physical measurements\nand natural language narratives. A generalist biomedical AI model needs to\nsimultaneously process different modalities of data, including text and images.\nTherefore, training an effective generalist biomedical model requires\nhigh-quality multimodal data, such as parallel image-text pairs. Here, we\npresent PMC-15M, a novel dataset that is two orders of magnitude larger than\nexisting biomedical multimodal datasets such as MIMIC-CXR, and spans a diverse\nrange of biomedical image types. PMC-15M contains 15 million biomedical\nimage-text pairs collected from 4.4 million scientific articles. Based on\nPMC-15M, we have pretrained BiomedCLIP, a multimodal foundation model, with\ndomain-specific adaptations tailored to biomedical vision-language processing.\nWe conducted extensive experiments and ablation studies on standard biomedical\nimaging tasks from retrieval to classification to visual question-answering\n(VQA). BiomedCLIP achieved new state-of-the-art results in a wide range of\nstandard datasets, substantially outperforming prior approaches. Intriguingly,\nby large-scale pretraining on diverse biomedical image types, BiomedCLIP even\noutperforms state-of-the-art radiology-specific models such as BioViL in\nradiology-specific tasks such as RSNA pneumonia detection. In summary,\nBiomedCLIP is a fully open-access foundation model that achieves\nstate-of-the-art performance on various biomedical tasks, paving the way for\ntransformative multimodal biomedical discovery and applications. We release our\nmodels at https://aka.ms/biomedclip to facilitate future research in multimodal\nbiomedical AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanbo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hanwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagga_J/0/1/0/all/0/1\">Jaspreet Bagga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preston_S/0/1/0/all/0/1\">Sam Preston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1\">Rajesh Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valluri_N/0/1/0/all/0/1\">Naveen Valluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Cliff Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tupini_A/0/1/0/all/0/1\">Andrea Tupini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazzola_M/0/1/0/all/0/1\">Matt Mazzola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1\">Swadheen Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liden_L/0/1/0/all/0/1\">Lars Liden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v6 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2305.00969","description":"<p>This paper describes the Ubenwa CryCeleb dataset - a labeled collection of\ninfant cries - and the accompanying CryCeleb 2023 task, which is a public\nspeaker verification challenge based on cry sounds. We released more than 6\nhours of manually segmented cry sounds from 786 newborns for academic use,\naiming to encourage research in infant cry analysis. The inaugural public\ncompetition attracted 59 participants, 11 of whom improved the baseline\nperformance. The top-performing system achieved a significant improvement\nscoring 25.8% equal error rate, which is still far from the performance of\nstate-of-the-art adult speaker verification systems. Therefore, we believe\nthere is room for further research on this dataset, potentially extending\nbeyond the verification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Budaghyan_D/0/1/0/all/0/1\">David Budaghyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onu_C/0/1/0/all/0/1\">Charles C. Onu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorin_A/0/1/0/all/0/1\">Arsenii Gorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subakan_C/0/1/0/all/0/1\">Cem Subakan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Hidden Mystery of OCR in Large Multimodal Models. (arXiv:2305.07895v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.07895","description":"<p>Large models have recently played a dominant role in natural language\nprocessing and multimodal vision-language learning. However, their\neffectiveness in text-related visual tasks remains relatively unexplored. In\nthis paper, we conducted a comprehensive evaluation of Large Multimodal Models,\nsuch as GPT4V and Gemini, in various text-related visual tasks including Text\nRecognition, Scene Text-Centric Visual Question Answering (VQA),\nDocument-Oriented VQA, Key Information Extraction (KIE), and Handwritten\nMathematical Expression Recognition (HMER). To facilitate the assessment of\nOptical Character Recognition (OCR) capabilities in Large Multimodal Models, we\npropose OCRBench, a comprehensive evaluation benchmark.Our study encompasses 29\ndatasets, making it the most comprehensive OCR evaluation benchmark available.\nFurthermore, our study reveals both the strengths and weaknesses of these\nmodels, particularly in handling multilingual text, handwritten text,\nnon-semantic text, and mathematical expression recognition. Most importantly,\nthe baseline results showcased in this study could provide a foundational\nframework for the conception and assessment of innovative strategies targeted\nat enhancing zero-shot multimodal techniques. The evaluation pipeline and\nbenchmark are available at https://github.com/Yuliang-Liu/MultimodalOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Biao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xucheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cheng-lin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes. (arXiv:2306.13649v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.13649","description":"<p>Knowledge distillation (KD) is widely used for compressing a teacher model to\nreduce its inference cost and memory footprint, by training a smaller student\nmodel. However, current KD methods for auto-regressive sequence models suffer\nfrom distribution mismatch between output sequences seen during training and\nthose generated by the student during inference. To address this issue, we\nintroduce Generalized Knowledge Distillation (GKD). Instead of solely relying\non a fixed set of output sequences, GKD trains the student on its\nself-generated output sequences by leveraging feedback from the teacher on such\nsequences. Unlike supervised KD approaches, GKD also offers the flexibility to\nemploy alternative loss functions between the student and teacher, which can be\nuseful when the student lacks the expressivity to mimic the teacher's\ndistribution. Furthermore, GKD facilitates the seamless integration of\ndistillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for\ndistilling auto-regressive language models on summarization, translation, and\narithmetic reasoning tasks, and task-agnostic distillation for\ninstruction-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Rishabh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieillard_N/0/1/0/all/0/1\">Nino Vieillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongchao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanczyk_P/0/1/0/all/0/1\">Piotr Stanczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_S/0/1/0/all/0/1\">Sabela Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1\">Matthieu Geist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1\">Olivier Bachem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SummaryMixing: A Linear-Complexity Alternative to Self-Attention for Speech Recognition and Understanding. (arXiv:2307.07421v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.07421","description":"<p>Modern speech processing systems rely on self-attention. Unfortunately, token\nmixing with self-attention takes quadratic time in the length of the speech\nutterance, slowing down inference as well as training and increasing memory\nconsumption. Cheaper alternatives to self-attention for ASR have been\ndeveloped, but they fail to consistently reach the same level of accuracy. This\npaper, therefore, proposes a novel linear-time alternative to self-attention.\nIt summarises an utterance with the mean over vectors for all time steps. This\nsingle summary is then combined with time-specific information. We call this\nmethod \"SummaryMixing\". Introducing SummaryMixing in state-of-the-art ASR\nmodels makes it feasible to preserve or exceed previous speech recognition\nperformance while lowering the training and inference times by up to 28$\\%$ and\nreducing the memory budget by a factor of two. The benefits of SummaryMixing\ncan also be generalized to other speech-processing tasks, such as speech\nunderstanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalen_R/0/1/0/all/0/1\">Rogier van Dalen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shucong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Sourav Bhattacharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model Displays Emergent Ability to Interpret Novel Literary Metaphors. (arXiv:2308.01497v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.01497","description":"<p>Recent advances in the performance of large language models (LLMs) have\nsparked debate over whether, given sufficient training, high-level human\nabilities emerge in such generic forms of artificial intelligence (AI). Despite\nthe exceptional performance of LLMs on a wide range of tasks involving natural\nlanguage processing and reasoning, there has been sharp disagreement as to\nwhether their abilities extend to more creative human abilities. A core example\nis the ability to interpret novel metaphors. Given the enormous and non curated\ntext corpora used to train LLMs, a serious obstacle to designing tests is the\nrequirement of finding novel yet high quality metaphors that are unlikely to\nhave been included in the training data. Here we assessed the ability of GPT4,\na state of the art large language model, to provide natural-language\ninterpretations of novel literary metaphors drawn from Serbian poetry and\ntranslated into English. Despite exhibiting no signs of having been exposed to\nthese metaphors previously, the AI system consistently produced detailed and\nincisive interpretations. Human judges, blind to the fact that an AI model was\ninvolved, rated metaphor interpretations generated by GPT4 as superior to those\nprovided by a group of college students. In interpreting reversed metaphors,\nGPT4, as well as humans, exhibited signs of sensitivity to the Gricean\ncooperative principle. In addition, for several novel English poems GPT4\nproduced interpretations that were rated as excellent or good by a human\nliterary critic. These results indicate that LLMs such as GPT4 have acquired an\nemergent ability to interpret complex metaphors, including those embedded in\nnovel poems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ichien_N/0/1/0/all/0/1\">Nicholas Ichien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamenkovic_D/0/1/0/all/0/1\">Du&#x161;an Stamenkovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holyoak_K/0/1/0/all/0/1\">Keith J. Holyoak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rational Decision-Making Agent with Internalized Utility Judgment. (arXiv:2308.12519v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.12519","description":"<p>Large language models (LLMs) have demonstrated remarkable advancements and\nhave attracted significant efforts to develop LLMs into agents capable of\nexecuting intricate multi-step decision-making tasks beyond traditional NLP\napplications. Existing approaches to LLM-based decision-making predominantly\nbuild upon the manually-designed external performance metrics to guide the\ndecision-making process. However, reliance on the external performance metrics\nas prior is problematic in real-world scenarios, where such prior may be\nunavailable, flawed, or even erroneous. For genuine autonomous decision making,\nit is imperative for the agent to develop its rationality from its posterior\nexperiences to judge decisions independently. Central to the development of\nrationality is the construction of an internalized utility judgment, capable of\nassigning numerical utilities to each decision. This paper proposes RadAgent\n(Rational Decision-Making Agent), which fosters the development of its\nrationality through an iterative framework involving Experience Exploration and\nUtility Learning. Within this framework, Elo-based Utility Construction is\ndevised to assign Elo scores to individual decision steps to judge their\nutilities via pairwise comparisons. Consequently, these Elo scores guide the\ndecision-making process to derive optimal outcomes. Experimental results on the\nToolBench dataset demonstrate RadAgent's superiority over baselines, achieving\nover 10% improvement in Pass Rate on diverse tasks. It offers higher-quality\nsolutions and reduces costs (ChatGPT API calls), highlighting its effectiveness\nand efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yining Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1\">Xin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shizuo Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic similarity prediction is better than other semantic similarity measures. (arXiv:2309.12697v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.12697","description":"<p>Semantic similarity between natural language texts is typically measured\neither by looking at the overlap between subsequences (e.g., BLEU) or by using\nembeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we\nare only interested in measuring the semantic similarity, it is better to\ndirectly predict the similarity using a fine-tuned model for such a task. Using\na fine-tuned model for the Semantic Textual Similarity Benchmark tasks (STS-B)\nfrom the GLUE benchmark, we define the STSScore approach and show that the\nresulting similarity is better aligned with our expectations on a robust\nsemantic similarity measure than other approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Herbold_S/0/1/0/all/0/1\">Steffen Herbold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Chat About Boring Problems: Studying GPT-based text normalization. (arXiv:2309.13426v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.13426","description":"<p>Text normalization - the conversion of text from written to spoken form - is\ntraditionally assumed to be an ill-formed task for language models. In this\nwork, we argue otherwise. We empirically show the capacity of Large-Language\nModels (LLM) for text normalization in few-shot scenarios. Combining\nself-consistency reasoning with linguistic-informed prompt engineering, we find\nLLM based text normalization to achieve error rates around 40\\% lower than top\nnormalization systems. Further, upon error analysis, we note key limitations in\nthe conventional design of text normalization tasks. We create a new taxonomy\nof text normalization errors and apply it to results from GPT-3.5-Turbo and\nGPT-4.0. Through this new framework, we can identify strengths and weaknesses\nof GPT-based TN, opening opportunities for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartley_T/0/1/0/all/0/1\">Travis M. Bartley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graterol_Fuenmayor_M/0/1/0/all/0/1\">Mariana Graterol-Fuenmayor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavrukhin_V/0/1/0/all/0/1\">Vitaly Lavrukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakhturina_E/0/1/0/all/0/1\">Evelina Bakhturina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watch Your Language: Investigating Content Moderation with Large Language Models. (arXiv:2309.14517v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2309.14517","description":"<p>Large language models (LLMs) have exploded in popularity due to their ability\nto perform a wide array of natural language tasks. Text-based content\nmoderation is one LLM use case that has received recent enthusiasm, however,\nthere is little research investigating how LLMs perform in content moderation\nsettings. In this work, we evaluate a suite of commodity LLMs on two common\ncontent moderation tasks: rule-based community moderation and toxic content\ndetection. For rule-based community moderation, we instantiate 95 subcommunity\nspecific LLMs by prompting GPT-3.5 with rules from 95 Reddit subcommunities. We\nfind that GPT-3.5 is effective at rule-based moderation for many communities,\nachieving a median accuracy of 64% and a median precision of 83%. For toxicity\ndetection, we evaluate a suite of commodity LLMs (GPT-3, GPT-3.5, GPT-4, Gemini\nPro, LLAMA 2) and show that LLMs significantly outperform currently widespread\ntoxicity classifiers. However, recent increases in model size add only marginal\nbenefit to toxicity detection, suggesting a potential performance plateau for\nLLMs on toxicity detection tasks. We conclude by outlining avenues for future\nwork in studying LLMs and content moderation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Deepak Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AbuHashem_Y/0/1/0/all/0/1\">Yousef AbuHashem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durumeric_Z/0/1/0/all/0/1\">Zakir Durumeric</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. (arXiv:2309.16042v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2309.16042","description":"<p>Mechanistic interpretability seeks to understand the internal mechanisms of\nmachine learning models, where localization -- identifying the important model\ncomponents -- is a key step. Activation patching, also known as causal tracing\nor interchange intervention, is a standard technique for this task (Vig et al.,\n2020), but the literature contains many variants with little consensus on the\nchoice of hyperparameters or methodology. In this work, we systematically\nexamine the impact of methodological details in activation patching, including\nevaluation metrics and corruption methods. In several settings of localization\nand circuit discovery in language models, we find that varying these\nhyperparameters could lead to disparate interpretability results. Backed by\nempirical observations, we give conceptual arguments for why certain metrics or\nmethods may be preferred. Finally, we provide recommendations for the best\npractices of activation patching going forwards.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fred Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanda_N/0/1/0/all/0/1\">Neel Nanda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogueLLM: Context and Emotion Knowledge-Tuned Large Language Models for Emotion Recognition in Conversations. (arXiv:2310.11374v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.11374","description":"<p>Large language models (LLMs) and their variants have shown extraordinary\nefficacy across numerous downstream natural language processing (NLP) tasks,\nwhich has presented a new vision for the development of NLP. Despite their\nremarkable performance in natural language generating (NLG), LLMs lack a\ndistinct focus on the emotion understanding domain. As a result, using LLMs for\nemotion recognition may lead to suboptimal and inadequate precision. Another\nlimitation of LLMs is that they are typical trained without leveraging\nmulti-modal information. To overcome these limitations, we propose DialogueLLM,\na context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA\nmodels with 13,638 multi-modal (i.e., texts and videos) emotional dialogues.\nThe visual information is considered as the supplementary knowledge to\nconstruct high-quality instructions. We offer a comprehensive evaluation of our\nproposed model on three benchmarking emotion recognition in conversations (ERC)\ndatasets and compare the results against the SOTA baselines and other SOTA\nLLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB\nA100 GPU in 5 hours, facilitating reproducibility for other researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yazhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1\">Prayag Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiuchi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts. (arXiv:2311.01070v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.01070","description":"<p>Whisper is a multitask and multilingual speech model covering 99 languages.\nIt yields commendable automatic speech recognition (ASR) results in a subset of\nits covered languages, but the model still underperforms on a non-negligible\nnumber of under-represented languages, a problem exacerbated in smaller model\nversions. In this work, we propose DistilWhisper, an approach able to bridge\nthe performance gap in ASR for these languages while retaining the advantages\nof multitask and multilingual capabilities. Our approach involves two key\nstrategies: lightweight modular ASR fine-tuning of whisper-small using\nlanguage-specific experts, and knowledge distillation from whisper-large-v2.\nThis dual approach allows us to effectively boost ASR performance while keeping\nthe robustness inherited from the multitask and multilingual pre-training.\nResults demonstrate that our approach is more effective than standard\nfine-tuning or LoRA adapters, boosting performance in the targeted languages\nfor both in- and out-of-domain test sets, while introducing only a negligible\nparameter overhead at inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferraz_T/0/1/0/all/0/1\">Thomas Palmeira Ferraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boito_M/0/1/0/all/0/1\">Marcely Zanon Boito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brun_C/0/1/0/all/0/1\">Caroline Brun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modelling prospective memory and resilient situated communications via Wizard of Oz. (arXiv:2311.05268v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.05268","description":"<p>This abstract presents a scenario for human-robot action in a home setting\ninvolving an older adult and a robot. The scenario is designed to explore the\nenvisioned modelling of memory for communication with a socially assistive\nrobots (SAR). The scenario will enable the gathering of data on failures of\nspeech technology and human-robot communication involving shared memory that\nmay occur during daily activities such as a music-listening activity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broz_F/0/1/0/all/0/1\">Frank Broz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neerincx_M/0/1/0/all/0/1\">Mark Neerincx</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning. (arXiv:2311.12023v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.12023","description":"<p>We propose a simple approach for memory-efficient adaptation of pretrained\nlanguage models. Our approach uses an iterative algorithm to decompose each\npretrained matrix into a high-precision low-rank component and a\nmemory-efficient quantized component. During finetuning, the quantized\ncomponent remains fixed and only the low-rank component is updated. We present\nan integer linear programming formulation of the quantization component which\nenables dynamic configuration of quantization parameters (e.g., bit-width,\nblock size) for each matrix given an overall target memory budget. We further\nexplore a data-aware version of the algorithm which uses an approximation of\nthe Fisher information matrix to weight the reconstruction objective during\nmatrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and\n70B) demonstrate that our low-rank plus quantized matrix decomposition approach\n(LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables\naggressive quantization to sub-3 bits with only minor performance degradations.\nWhen finetuned on a language modeling calibration dataset, LQ-LoRA can also be\nused for model compression; in this setting our 2.75-bit LLaMA-2-70B model\n(which has 2.85 bits on average when including the low-rank components and\nrequires 27GB of GPU memory) performs respectably compared to the 16-bit\nbaseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greengard_P/0/1/0/all/0/1\">Philip Greengard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Fault Analysis in Substations Based on Knowledge Graphs. (arXiv:2311.13708v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.13708","description":"<p>To address the challenge of identifying hidden danger in substations from\nunstructured text, a novel dynamic analysis method is proposed. We first\nextract relevant information from the unstructured text, and then leverages a\nflexible distributed search engine built on Elastic-Search to handle the data.\nFollowing this, the hidden Markov model is employed to train the data within\nthe engine. The Viterbi algorithm is integrated to decipher the hidden state\nsequences, facilitating the segmentation and labeling of entities related to\nhidden dangers. The final step involves using the Neo4j graph database to\ndynamically create a knowledge graph that visualizes hidden dangers in the\nsubstation. The effectiveness of the proposed method is demonstrated through a\ncase analysis from a specific substation with hidden dangers revealed in the\ntext records.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sizhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hui Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Fault Characteristics Evaluation in Power Grid. (arXiv:2311.16522v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.16522","description":"<p>To enhance the intelligence degree in operation and maintenance, a novel\nmethod for fault detection in power grids is proposed. The proposed GNN-based\napproach first identifies fault nodes through a specialized feature extraction\nmethod coupled with a knowledge graph. By incorporating temporal data, the\nmethod leverages the status of nodes from preceding and subsequent time periods\nto help current fault detection. To validate the effectiveness of the node\nfeatures, a correlation analysis of the output features from each node was\nconducted. The results from experiments show that this method can accurately\nlocate fault nodes in simulation scenarios with a remarkable accuracy.\nAdditionally, the graph neural network based feature modeling allows for a\nqualitative examination of how faults spread across nodes, which provides\nvaluable insights for analyzing fault nodes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1\">Hao Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Si Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuanfu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Che Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sizhe Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLadder: Assessing Causal Reasoning in Language Models. (arXiv:2312.04350v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.04350","description":"<p>The ability to perform causal reasoning is widely considered a core feature\nof intelligence. In this work, we investigate whether large language models\n(LLMs) can coherently reason about causality. Much of the existing work in\nnatural language processing (NLP) focuses on evaluating commonsense causal\nreasoning in LLMs, thus failing to assess whether a model can perform causal\ninference in accordance with a set of well-defined formal rules. To address\nthis, we propose a new NLP task, causal inference in natural language, inspired\nby the \"causal inference engine\" postulated by Judea Pearl et al. We compose a\nlarge dataset, CLadder, with 10K samples: based on a collection of causal\ngraphs and queries (associational, interventional, and counterfactual), we\nobtain symbolic questions and ground-truth answers, through an oracle causal\ninference engine. These are then translated into natural language. We evaluate\nmultiple LLMs on our dataset, and we introduce and evaluate a bespoke\nchain-of-thought prompting strategy, CausalCoT. We show that our task is highly\nchallenging for LLMs, and we conduct an in-depth analysis to gain deeper\ninsights into the causal reasoning abilities of LLMs. Our data is open-sourced\nat https://huggingface.co/datasets/causalNLP/cladder, and our code can be found\nat https://github.com/causalNLP/cladder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1\">Felix Leeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gresele_L/0/1/0/all/0/1\">Luigi Gresele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamal_O/0/1/0/all/0/1\">Ojasv Kamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhiheng Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blin_K/0/1/0/all/0/1\">Kevin Blin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adauto_F/0/1/0/all/0/1\">Fernando Gonzalez Adauto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleiman_Weiner_M/0/1/0/all/0/1\">Max Kleiman-Weiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training. (arXiv:2312.08846v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2312.08846","description":"<p>Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances\nmodern Vision-Language Pre-training (VLP) models by aligning visual and\nlinguistic modalities. Due to noises in web-harvested text-image pairs,\nhowever, scaling up training data volume in SMCL presents considerable\nobstacles in terms of computational cost and data inefficiency. To improve data\nefficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates\nmix-based data augmentation techniques into SMCL, yielding significant\nperformance improvements without significantly increasing computational\noverhead. We provide a theoretical analysis of TiMixfrom a mutual information\n(MI) perspective, showing that mixed data samples for cross-modal contrastive\nlearning implicitly serve as a regularizer for the contrastive loss. The\nexperimental results demonstrate that TiMix exhibits a comparable performance\non downstream tasks, even with a reduced amount of training data and shorter\ntraining time, when benchmarked against existing methods. This work empirically\nand theoretically demonstrates the potential of data mixing for data-efficient\nand computationally viable VLP, benefiting broader VLP model adoption in\npractical scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chaoya Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ye_W/0/1/0/all/0/1\">Wei ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Modeling on a SpiNNaker 2 Neuromorphic Chip. (arXiv:2312.09084v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2312.09084","description":"<p>As large language models continue to scale in size rapidly, so too does the\ncomputational power required to run them. Event-based networks on neuromorphic\ndevices offer a potential way to reduce energy consumption for inference\nsignificantly. However, to date, most event-based networks that can run on\nneuromorphic hardware, including spiking neural networks (SNNs), have not\nachieved task performance even on par with LSTM models for language modeling.\nAs a result, language modeling on neuromorphic devices has seemed a distant\nprospect. In this work, we demonstrate the first-ever implementation of a\nlanguage model on a neuromorphic device - specifically the SpiNNaker 2 chip -\nbased on a recently published event-based architecture called the EGRU.\nSpiNNaker 2 is a many-core neuromorphic chip designed for large-scale\nasynchronous processing, while the EGRU is architected to leverage such\nhardware efficiently while maintaining competitive task performance. This\nimplementation marks the first time a neuromorphic language model matches\nLSTMs, setting the stage for taking task performance to the level of large\nlanguage models. We also demonstrate results on a gesture recognition task\nbased on inputs from a DVS camera. Overall, our results showcase the\nfeasibility of this neuro-inspired neural network in hardware, highlighting\nsignificant gains versus conventional hardware in energy efficiency for the\ncommon use case of single batch inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nazeer_K/0/1/0/all/0/1\">Khaleelulla Khan Nazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schone_M/0/1/0/all/0/1\">Mark Sch&#xf6;ne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherji_R/0/1/0/all/0/1\">Rishav Mukherji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogginger_B/0/1/0/all/0/1\">Bernhard Vogginger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayr_C/0/1/0/all/0/1\">Christian Mayr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kappel_D/0/1/0/all/0/1\">David Kappel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramoney_A/0/1/0/all/0/1\">Anand Subramoney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue. (arXiv:2312.15316v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.15316","description":"<p>Large Language Models (LLMs) have demonstrated superior abilities in tasks\nsuch as chatting, reasoning, and question-answering. However, standard LLMs may\nignore crucial paralinguistic information, such as sentiment, emotion, and\nspeaking style, which are essential for achieving natural, human-like spoken\nconversation, especially when such information is conveyed by acoustic cues. We\ntherefore propose Paralinguistics-enhanced Generative Pretrained Transformer\n(ParalinGPT), an LLM that utilizes text and speech modalities to better model\nthe linguistic content and paralinguistic attributes of spoken dialogue. The\nmodel takes the conversational context of text, speech embeddings, and\nparalinguistic attributes as input prompts within a serialized multitasking\nmultimodal framework. Specifically, our framework serializes tasks in the order\nof current paralinguistic attribute prediction, response paralinguistic\nattribute prediction, and response text generation with autoregressive\nconditioning. We utilize the Switchboard-1 corpus, including its sentiment\nlabels as the paralinguistic attribute, as our spoken dialogue dataset.\nExperimental results indicate the proposed serialized multitasking method\noutperforms typical sequence classification techniques on current and response\nsentiment classification. Furthermore, leveraging conversational context and\nspeech embeddings significantly improves both response text generation and\nsentiment prediction. Our proposed framework achieves relative improvements of\n6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment\naccuracy, and response text BLEU score, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guan-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shivakumar_P/0/1/0/all/0/1\">Prashanth Gurunath Shivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yile Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shalini Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance. (arXiv:2401.02906v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2401.02906","description":"<p>The deployment of multimodal large language models (MLLMs) has brought forth\na unique vulnerability: susceptibility to malicious attacks through visual\ninputs. We delve into the novel challenge of defending MLLMs against such\nattacks. We discovered that images act as a \"foreign language\" that is not\nconsidered during alignment, which can make MLLMs prone to producing harmful\nresponses. Unfortunately, unlike the discrete tokens considered in text-based\nLLMs, the continuous nature of image signals presents significant alignment\nchallenges, which poses difficulty to thoroughly cover the possible scenarios.\nThis vulnerability is exacerbated by the fact that open-source MLLMs are\npredominantly fine-tuned on limited image-text pairs that is much less than the\nextensive text-based pretraining corpus, which makes the MLLMs more prone to\ncatastrophic forgetting of their original abilities during explicit alignment\ntuning. To tackle these challenges, we introduce MLLM-Protector, a\nplug-and-play strategy combining a lightweight harm detector and a response\ndetoxifier. The harm detector's role is to identify potentially harmful outputs\nfrom the MLLM, while the detoxifier corrects these outputs to ensure the\nresponse stipulates to the safety standards. This approach effectively\nmitigates the risks posed by malicious visual inputs without compromising the\nmodel's overall performance. Our results demonstrate that MLLM-Protector offers\na robust solution to a previously unaddressed aspect of MLLM security.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pi_R/0/1/0/all/0/1\">Renjie Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tianyang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yueqi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1\">Rui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1\">Qing Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hanze Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.05268","description":"<p>Language agents have achieved considerable performance on various complex\ntasks. Despite the incessant exploration in this field, existing language agent\nsystems still struggle with costly, non-reproducible data reliance and face the\nchallenge of compelling a single model for multiple functions. To this end, we\nintroduce AutoAct, an automatic agent learning framework that does not rely on\nlarge-scale annotated data and synthetic trajectories from closed-source models\n(e.g., GPT-4). Given limited data with a tool library, AutoAct first\nautomatically synthesizes planning trajectories without any assistance from\nhumans or strong closed-source models. Then, AutoAct leverages a\ndivision-of-labor strategy to automatically differentiate based on the target\ntask information and synthesized trajectories, producing a sub-agent group to\ncomplete the task. We conduct comprehensive experiments with different LLMs,\nwhich demonstrates that AutoAct yields better or parallel performance compared\nto various strong baselines. We even notice that AutoAct, when using the\nLlama-2-13b model, can achieve performance comparable to that of the zero-shot\nGPT-3.5-Turbo agent. Code will be available at\nhttps://github.com/zjunlp/AutoAct.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Shuofei Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Runnan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yujie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Eleanor Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chengfei Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters. (arXiv:2401.06408v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.06408","description":"<p>Large language models' (LLMs) abilities are drawn from their pretraining\ndata, and model development begins with data curation. However, decisions\naround what data is retained or removed during this initial stage is\nunder-scrutinized. In our work, we ground web text, which is a popular\npretraining data source, to its social and geographic contexts. We create a new\ndataset of 10.3 million self-descriptions of website creators, and extract\ninformation about who they are and where they are from: their topical\ninterests, social roles, and geographic affiliations. Then, we conduct the\nfirst study investigating how ten \"quality\" and English language identification\n(langID) filters affect webpages that vary along these social dimensions. Our\nexperiments illuminate a range of implicit preferences in data curation: we\nshow that some quality classifiers act like topical domain filters, and langID\ncan overlook English content from some regions of the world. Overall, we hope\nthat our work will encourage a new line of research on pretraining data\ncuration practices and its social implications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lucy_L/0/1/0/all/0/1\">Li Lucy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamman_D/0/1/0/all/0/1\">David Bamman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_L/0/1/0/all/0/1\">Lauren Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightHouse: A Survey of AGI Hallucination. (arXiv:2401.06792v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.06792","description":"<p>With the development of artificial intelligence, large-scale models have\nbecome increasingly intelligent. However, numerous studies indicate that\nhallucinations within these large models are a bottleneck hindering the\ndevelopment of AI research. In the pursuit of achieving strong artificial\nintelligence, a significant volume of research effort is being invested in the\nAGI (Artificial General Intelligence) hallucination research. Previous\nexplorations have been conducted in researching hallucinations within LLMs\n(Large Language Models). As for multimodal AGI, research on hallucinations is\nstill in an early stage. To further the progress of research in the domain of\nhallucinatory phenomena, we present a bird's eye view of hallucinations in AGI,\nsummarizing the current work on AGI hallucinations and proposing some\ndirections for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Hallucination Detection and Editing for Language Models. (arXiv:2401.06855v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.06855","description":"<p>Large language models (LMs) are prone to generate diverse factually incorrect\nstatements, which are widely called hallucinations. Current approaches\npredominantly focus on coarse-grained automatic hallucination detection or\nediting, overlooking nuanced error levels. In this paper, we propose a novel\ntask -- automatic fine-grained hallucination detection -- and present a\ncomprehensive taxonomy encompassing six hierarchically defined types of\nhallucination. To facilitate evaluation, we introduce a new benchmark that\nincludes fine-grained human judgments on two LM outputs across various domains.\nOur analysis reveals that ChatGPT and Llama 2-Chat exhibit hallucinations in\n60% and 75% of their outputs, respectively, and a majority of these\nhallucinations fall into categories that have been underexplored. As an initial\nstep to address this, we train FAVA, a retrieval-augmented LM by carefully\ndesigning synthetic data generations to detect and correct fine-grained\nhallucinations. On our benchmark, our automatic and human evaluations show that\nFAVA significantly outperforms ChatGPT on fine-grained hallucination detection\nby a large margin though a large room for future improvement still exists.\nFAVA's suggested edits also improve the factuality of LM-generated text,\nresulting in 5-10% FActScore improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Abhika Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1\">Vidhisha Balachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"See the Unseen: Better Context-Consistent Knowledge-Editing by Noises. (arXiv:2401.07544v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.07544","description":"<p>Knowledge-editing updates knowledge of large language models (LLMs) and\ncontributes to the interpretability and application of LLMs. However, knowledge\napplying is context-consistent: LLMs can recall the same knowledge in different\ncontexts. Existing works ignore this property and the editing lacks\ngeneralization. In this paper, we empirically find that the effects of\ndifferent contexts upon LLMs in recalling the same knowledge follow a\nGaussian-like distribution. We then sample Gaussian noises to simulate the\neffects of different contexts when updating LLMs. By such, we can make LLMs see\nthe unseen contexts where the edited knowledge will be applied, therefore\nimproving the editing generalization. Experimental results on three LLMs\ndemonstrate the effectiveness of our methods and also distinguish our methods\nfrom the others of fine-tuning LLMs by noises.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Youcheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models. (arXiv:2401.08350v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.08350","description":"<p>The evolution of Neural Machine Translation (NMT) has been significantly\ninfluenced by six core challenges (Koehn and Knowles, 2017), which have acted\nas benchmarks for progress in this field. This study revisits these challenges,\noffering insights into their ongoing relevance in the context of advanced Large\nLanguage Models (LLMs): domain mismatch, amount of parallel data, rare word\nprediction, translation of long sentences, attention model as word alignment,\nand sub-optimal beam search. Our empirical findings indicate that LLMs\neffectively lessen the reliance on parallel data for major languages in the\npretraining phase. Additionally, the LLM-based translation system significantly\nenhances the translation of long sentences that contain approximately 80 words\nand shows the capability to translate documents of up to 512 words. However,\ndespite these significant improvements, the challenges of domain mismatch and\nprediction of rare words persist. While the challenges of word alignment and\nbeam search, specifically associated with NMT, may not apply to LLMs, we\nidentify three new challenges for LLMs in translation tasks: inference\nefficiency, translation of low-resource languages in the pretraining phase, and\nhuman-aligned evaluation. The datasets and models are released at\nhttps://github.com/pangjh3/LLM4MT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jianhui Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fanghua Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2024-01-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}