{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-09-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Code Representation Pre-training with Complements from Program Executions. (arXiv:2309.09980v1 [cs.SE])","link":"http://arxiv.org/abs/2309.09980","description":"<p>Large language models (LLMs) for natural language processing have been\ngrafted onto programming language modeling for advancing code intelligence.\nAlthough it can be represented in the text format, code is syntactically more\nrigorous in order to be properly compiled or interpreted to perform a desired\nset of behaviors given any inputs. In this case, existing works benefit from\nsyntactic representations to learn from code less ambiguously in the forms of\nabstract syntax tree, control-flow graph, etc. However, programs with the same\npurpose can be implemented in various ways showing different syntactic\nrepresentations while the ones with similar implementations can have distinct\nbehaviors. Though trivially demonstrated during executions, such semantics\nabout functionality are challenging to be learned directly from code,\nespecially in an unsupervised manner. Hence, in this paper, we propose\nFuzzPretrain to explore the dynamic information of programs revealed by their\ntest cases and embed it into the feature representations of code as\ncomplements. The test cases are obtained with the assistance of a customized\nfuzzer and are only required during pre-training. FuzzPretrain yielded more\nthan 6%/9% mAP improvements on code search over its counterparts trained with\nonly source code or AST, respectively. Our extensive experimental results show\nthe benefits of learning discriminative code representations with program\nexecutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiabo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yuyang Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yifeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?. (arXiv:2309.09992v1 [cs.AI])","link":"http://arxiv.org/abs/2309.09992","description":"<p>The authors explain where OpenAI got the tax law example in its livestream\ndemonstration of GPT-4, why GPT-4 got the wrong answer, and how it fails to\nreliably calculate taxes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blair_Stanek_A/0/1/0/all/0/1\">Andrew Blair-Stanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzenberger_N/0/1/0/all/0/1\">Nils Holzenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Speech Recognition for African American English With Audio Classification. (arXiv:2309.09996v1 [eess.AS])","link":"http://arxiv.org/abs/2309.09996","description":"<p>Automatic speech recognition (ASR) systems have been shown to have large\nquality disparities between the language varieties they are intended or\nexpected to recognize. One way to mitigate this is to train or fine-tune models\nwith more representative datasets. But this approach can be hindered by limited\nin-domain data for training and evaluation. We propose a new way to improve the\nrobustness of a US English short-form speech recognizer using a small amount of\nout-of-domain (long-form) African American English (AAE) data. We use CORAAL,\nYouTube and Mozilla Common Voice to train an audio classifier to approximately\noutput whether an utterance is AAE or some other variety including Mainstream\nAmerican English (MAE). By combining the classifier output with coarse\ngeographic information, we can select a subset of utterances from a large\ncorpus of untranscribed short-form queries for semi-supervised learning at\nscale. Fine-tuning on this data results in a 38.5% relative word error rate\ndisparity reduction between AAE and MAE without reducing MAE quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Garg_S/0/1/0/all/0/1\">Shefali Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwartz_S/0/1/0/all/0/1\">Suzan Schwartz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chua_M/0/1/0/all/0/1\">Mason Chua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aksenova_A/0/1/0/all/0/1\">Al&#xeb;na Aks&#xeb;nova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munkhdalai_T/0/1/0/all/0/1\">Tsendsuren Munkhdalai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+King_L/0/1/0/all/0/1\">Levi King</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wright_D/0/1/0/all/0/1\">Darryl Wright</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mengesha_Z/0/1/0/all/0/1\">Zion Mengesha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sainath_T/0/1/0/all/0/1\">Tara Sainath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mengibar_P/0/1/0/all/0/1\">Pedro Moreno Mengibar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting covariate drift in text data using document embeddings and dimensionality reduction. (arXiv:2309.10000v1 [cs.LG])","link":"http://arxiv.org/abs/2309.10000","description":"<p>Detecting covariate drift in text data is essential for maintaining the\nreliability and performance of text analysis models. In this research, we\ninvestigate the effectiveness of different document embeddings, dimensionality\nreduction techniques, and drift detection methods for identifying covariate\ndrift in text data. We explore three popular document embeddings: term\nfrequency-inverse document frequency (TF-IDF) using Latent semantic\nanalysis(LSA) for dimentionality reduction and Doc2Vec, and BERT embeddings,\nwith and without using principal component analysis (PCA) for dimensionality\nreduction. To quantify the divergence between training and test data\ndistributions, we employ the Kolmogorov-Smirnov (KS) statistic and the Maximum\nMean Discrepancy (MMD) test as drift detection methods. Experimental results\ndemonstrate that certain combinations of embeddings, dimensionality reduction\ntechniques, and drift detection methods outperform others in detecting\ncovariate drift. Our findings contribute to the advancement of reliable text\nanalysis models by providing insights into effective approaches for addressing\ncovariate drift in text data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sodar_V/0/1/0/all/0/1\">Vinayak Sodar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekseria_A/0/1/0/all/0/1\">Ankit Sekseria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10003","description":"<p>This work proposes to measure the scope of a patent claim as the reciprocal\nof the self-information contained in this claim. Grounded in information\ntheory, this approach is based on the assumption that a rare concept is more\ninformative than a usual concept, inasmuch as it is more surprising. The\nself-information is calculated from the probability of occurrence of that\nclaim, where the probability is calculated in accordance with a language model.\nFive language models are considered, ranging from the simplest models (each\nword or character is drawn from a uniform distribution) to intermediate models\n(using average word or character frequencies), to a large language model\n(GPT2). Interestingly, the simplest language models reduce the scope measure to\nthe reciprocal of the word or character count, a metric already used in\nprevious works. Application is made to nine series of patent claims directed to\ndistinct inventions, where the claims in each series have a gradually\ndecreasing scope. The performance of the language models is then assessed with\nrespect to several ad hoc tests. The more sophisticated the model, the better\nthe results. The GPT2 model outperforms models based on word and character\nfrequencies, which are themselves ahead of models based on word and character\ncounts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ragot_S/0/1/0/all/0/1\">S&#xe9;bastien Ragot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback. (arXiv:2309.10015v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10015","description":"<p>Commonsense reasoning is a critical aspect of human communication. Despite\nrecent advances in conversational AI driven by large language models,\ncommonsense reasoning remains a challenging task. In this work, we introduce\nSYNDICOM - a method for improving commonsense in dialogue response generation.\nSYNDICOM consists of two components. The first component is a dataset composed\nof commonsense dialogues created from a knowledge graph and synthesized into\nnatural language. This dataset includes both valid and invalid responses to\ndialogue contexts, along with natural language feedback (NLF) for the invalid\nresponses. The second contribution is a two-step procedure: training a model to\npredict natural language feedback (NLF) for invalid responses, and then\ntraining a response generation model conditioned on the predicted NLF, the\ninvalid response, and the dialogue. SYNDICOM is scalable and does not require\nreinforcement learning. Empirical results on three tasks are evaluated using a\nbroad range of metrics. SYNDICOM achieves a relative improvement of 53% over\nChatGPT on ROUGE1, and human evaluators prefer SYNDICOM over ChatGPT 57% of the\ntime. We will publicly release the code and the full dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richardson_C/0/1/0/all/0/1\">Christopher Richardson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundar_A/0/1/0/all/0/1\">Anirudh Sundar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_L/0/1/0/all/0/1\">Larry Heck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Foundation Models: From Specialists to General-Purpose Assistants. (arXiv:2309.10020v1 [cs.CV])","link":"http://arxiv.org/abs/2309.10020","description":"<p>This paper presents a comprehensive survey of the taxonomy and evolution of\nmultimodal foundation models that demonstrate vision and vision-language\ncapabilities, focusing on the transition from specialist models to\ngeneral-purpose assistants. The research landscape encompasses five core\ntopics, categorized into two classes. (i) We start with a survey of\nwell-established research areas: multimodal foundation models pre-trained for\nspecific purposes, including two topics -- methods of learning vision backbones\nfor visual understanding and text-to-image generation. (ii) Then, we present\nrecent advances in exploratory, open research areas: multimodal foundation\nmodels that aim to play the role of general-purpose assistants, including three\ntopics -- unified vision models inspired by large language models (LLMs),\nend-to-end training of multimodal LLMs, and chaining multimodal tools with\nLLMs. The target audiences of the paper are researchers, graduate students, and\nprofessionals in computer vision and vision-language multimodal communities who\nare eager to learn the basics and recent advances in multimodal foundation\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchy Builder: Organizing Textual Spans into a Hierarchy to Facilitate Navigation. (arXiv:2309.10057v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10057","description":"<p>Information extraction systems often produce hundreds to thousands of strings\non a specific topic. We present a method that facilitates better consumption of\nthese strings, in an exploratory setting in which a user wants to both get a\nbroad overview of what's available, and a chance to dive deeper on some\naspects. The system works by grouping similar items together and arranging the\nremaining items into a hierarchical navigable DAG structure. We apply the\nmethod to medical information extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yair_I/0/1/0/all/0/1\">Itay Yair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taub_Tabib_H/0/1/0/all/0/1\">Hillel Taub-Tabib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Personalized Impression Generation for PET Reports Using Large Language Models. (arXiv:2309.10066v1 [cs.AI])","link":"http://arxiv.org/abs/2309.10066","description":"<p>Purpose: To determine if fine-tuned large language models (LLMs) can generate\naccurate, personalized impressions for whole-body PET reports. Materials and\nMethods: Twelve language models were trained on a corpus of PET reports using\nthe teacher-forcing algorithm, with the report findings as input and the\nclinical impressions as reference. An extra input token encodes the reading\nphysician's identity, allowing models to learn physician-specific reporting\nstyles. Our corpus comprised 37,370 retrospective PET reports collected from\nour institution between 2010 and 2022. To identify the best LLM, 30 evaluation\nmetrics were benchmarked against quality scores from two nuclear medicine (NM)\nphysicians, with the most aligned metrics selecting the model for expert\nevaluation. In a subset of data, model-generated impressions and original\nclinical impressions were assessed by three NM physicians according to 6\nquality dimensions and an overall utility score (5-point scale). Each physician\nreviewed 12 of their own reports and 12 reports from other physicians.\nBootstrap resampling was used for statistical analysis. Results: Of all\nevaluation metrics, domain-adapted BARTScore and PEGASUSScore showed the\nhighest Spearman's rho correlations (0.568 and 0.563) with physician\npreferences. Based on these metrics, the fine-tuned PEGASUS model was selected\nas the top LLM. When physicians reviewed PEGASUS-generated impressions in their\nown style, 89% were considered clinically acceptable, with a mean utility score\nof 4.08/5. Physicians rated these personalized impressions as comparable in\noverall utility to the impressions dictated by other physicians (4.03, P=0.41).\nConclusion: Personalized impressions generated by PEGASUS were clinically\nuseful, highlighting its potential to expedite PET reporting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tie_X/0/1/0/all/0/1\">Xin Tie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Muheon Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirasteh_A/0/1/0/all/0/1\">Ali Pirasteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_N/0/1/0/all/0/1\">Nevein Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huemann_Z/0/1/0/all/0/1\">Zachary Huemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castellino_S/0/1/0/all/0/1\">Sharon M. Castellino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_K/0/1/0/all/0/1\">Kara M. Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrett_J/0/1/0/all/0/1\">John Garrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Steve Y. Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradshaw_T/0/1/0/all/0/1\">Tyler J. Bradshaw</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HTEC: Human Transcription Error Correction. (arXiv:2309.10089v1 [eess.AS])","link":"http://arxiv.org/abs/2309.10089","description":"<p>High-quality human transcription is essential for training and improving\nAutomatic Speech Recognition (ASR) models. Recent study~\\cite{libricrowd} has\nfound that every 1% worse transcription Word Error Rate (WER) increases\napproximately 2% ASR WER by using the transcriptions to train ASR models.\nTranscription errors are inevitable for even highly-trained annotators.\nHowever, few studies have explored human transcription correction. Error\ncorrection methods for other problems, such as ASR error correction and\ngrammatical error correction, do not perform sufficiently for this problem.\nTherefore, we propose HTEC for Human Transcription Error Correction. HTEC\nconsists of two stages: Trans-Checker, an error detection model that predicts\nand masks erroneous words, and Trans-Filler, a sequence-to-sequence generative\nmodel that fills masked positions. We propose a holistic list of correction\noperations, including four novel operations handling deletion errors. We\nfurther propose a variant of embeddings that incorporates phoneme information\ninto the input of the transformer. HTEC outperforms other methods by a large\nmargin and surpasses human annotators by 2.2% to 4.5% in WER. Finally, we\ndeployed HTEC to assist human annotators and showed HTEC is particularly\neffective as a co-pilot, which improves transcription quality by 15.1% without\nsacrificing transcription velocity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1\">Hanbo Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_J/0/1/0/all/0/1\">Jian Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xiaomin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_A/0/1/0/all/0/1\">Anjie Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_C/0/1/0/all/0/1\">Cheng Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1\">Zheng Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Coarse-to-Fine Alignment for Video-Text Retrieval. (arXiv:2309.10091v1 [cs.CV])","link":"http://arxiv.org/abs/2309.10091","description":"<p>The canonical approach to video-text retrieval leverages a coarse-grained or\nfine-grained alignment between visual and textual information. However,\nretrieving the correct video according to the text query is often challenging\nas it requires the ability to reason about both high-level (scene) and\nlow-level (object) visual clues and how they relate to the text query. To this\nend, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA.\nSpecifically, our model captures the cross-modal similarity information at\ndifferent granularity levels. To alleviate the effect of irrelevant visual\nclues, we also apply an Interactive Similarity Aggregation module (ISA) to\nconsider the importance of different visual features while aggregating the\ncross-modal similarity to obtain a similarity score for each granularity.\nFinally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of\neach level before summing them, alleviating over- and under-representation\nissues at different levels. By jointly considering the crossmodal similarity of\ndifferent granularity, UCoFiA allows the effective unification of multi-grained\nalignments. Empirically, UCoFiA outperforms previous state-of-the-art\nCLIP-based methods on multiple video-text retrieval benchmarks, achieving 2.4%,\n1.4% and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT,\nActivity-Net, and DiDeMo, respectively. Our code is publicly available at\nhttps://github.com/Ziyang412/UCoFiA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yi-Lin Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Feng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Catastrophic Forgetting in Language Models via Implicit Inference. (arXiv:2309.10105v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10105","description":"<p>Fine-tuning (via methods such as instruction-tuning or reinforcement learning\nfrom human feedback) is a crucial step in training language models to robustly\ncarry out tasks of interest. However, we lack a systematic understanding of the\neffects of fine-tuning, particularly on tasks outside the narrow fine-tuning\ndistribution. In a simplified scenario, we demonstrate that improving\nperformance on tasks within the fine-tuning data distribution comes at the\nexpense of suppressing model capabilities on other tasks. This degradation is\nespecially pronounced for tasks \"closest\" to the fine-tuning distribution. We\nhypothesize that language models implicitly infer the task of the prompt\ncorresponds, and the fine-tuning process predominantly skews this task\ninference towards tasks in the fine-tuning distribution. To test this\nhypothesis, we propose Conjugate Prompting to see if we can recover pretrained\ncapabilities. Conjugate prompting artificially makes the task look farther from\nthe fine-tuning distribution while requiring the same capability. We find that\nconjugate prompting systematically recovers some of the pretraining\ncapabilities on our synthetic setup. We then apply conjugate prompting to\nreal-world LLMs using the observation that fine-tuning distributions are\ntypically heavily skewed towards English. We find that simply translating the\nprompts to different languages can cause the fine-tuned models to respond like\ntheir pretrained counterparts instead. This allows us to recover the in-context\nlearning abilities lost via instruction tuning, and more concerningly, to\nrecover harmful content generation suppressed by safety fine-tuning in chatbots\nlike ChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotha_S/0/1/0/all/0/1\">Suhas Kotha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Springer_J/0/1/0/all/0/1\">Jacob Mitchell Springer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Aditi Raghunathan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Adaptation for Parsing Contextual Utterances with LLMs. (arXiv:2309.10168v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10168","description":"<p>We evaluate the ability of semantic parsers based on large language models\n(LLMs) to handle contextual utterances. In real-world settings, there typically\nexists only a limited number of annotated contextual utterances due to\nannotation cost, resulting in an imbalance compared to non-contextual\nutterances. Therefore, parsers must adapt to contextual utterances with a few\ntraining examples. We examine four major paradigms for doing so in\nconversational semantic parsing i.e., Parse-with-Utterance-History,\nParse-with-Reference-Program, Parse-then-Resolve, and Rewrite-then-Parse. To\nfacilitate such cross-paradigm comparisons, we construct\nSMCalFlow-EventQueries, a subset of contextual examples from SMCalFlow with\nadditional annotations. Experiments with in-context learning and fine-tuning\nsuggest that Rewrite-then-Parse is the most promising paradigm when\nholistically considering parsing accuracy, annotation cost, and error types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1\">Patrick Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Hao Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Positive and Risky Message Assessment for Music Products. (arXiv:2309.10182v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10182","description":"<p>In this work, we propose a novel research problem: assessing positive and\nrisky messages from music products. We first establish a benchmark for\nmulti-angle multi-level music content assessment and then present an effective\nmulti-task prediction model with ordinality-enforcement to solve this problem.\nOur result shows the proposed method not only significantly outperforms strong\ntask-specific counterparts but can concurrently evaluate multiple aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yigeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafaei_M/0/1/0/all/0/1\">Mahsa Shafaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Fabio Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stabilizing RLHF through Advantage Model and Selective Rehearsal. (arXiv:2309.10202v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10202","description":"<p>Large Language Models (LLMs) have revolutionized natural language processing,\nyet aligning these models with human values and preferences using RLHF remains\na significant challenge. This challenge is characterized by various\ninstabilities, such as reward hacking and catastrophic forgetting. In this\ntechnical report, we propose two innovations to stabilize RLHF training: 1)\nAdvantage Model, which directly models advantage score i.e., extra reward\ncompared to the expected rewards and regulates score distributions across tasks\nto prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic\nforgetting by strategically selecting data for PPO training and knowledge\nrehearsing. Our experimental analysis on public and proprietary datasets\nreveals that the proposed methods not only increase stability in RLHF training\nbut also achieve higher reward scores and win rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linfeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Ye Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lifeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models. (arXiv:2309.10238v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10238","description":"<p>Privacy policies serve as the primary conduit through which online service\nproviders inform users about their data collection and usage procedures.\nHowever, in a bid to be comprehensive and mitigate legal risks, these policy\ndocuments are often quite verbose. In practical use, users tend to click the\nAgree button directly rather than reading them carefully. This practice exposes\nusers to risks of privacy leakage and legal issues. Recently, the advent of\nLarge Language Models (LLM) such as ChatGPT and GPT-4 has opened new\npossibilities for text analysis, especially for lengthy documents like privacy\npolicies. In this study, we investigate a privacy policy text analysis\nframework PolicyGPT based on the LLM. This framework was tested using two\ndatasets. The first dataset comprises of privacy policies from 115 websites,\nwhich were meticulously annotated by legal experts, categorizing each segment\ninto one of 10 classes. The second dataset consists of privacy policies from\n304 popular mobile applications, with each sentence manually annotated and\nclassified into one of another 10 categories. Under zero-shot learning\nconditions, PolicyGPT demonstrated robust performance. For the first dataset,\nit achieved an accuracy rate of 97%, while for the second dataset, it attained\nan 87% accuracy rate, surpassing that of the baseline machine learning and\nneural network models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chenhao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Quanzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lei Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What is the Best Automated Metric for Text to Motion Generation?. (arXiv:2309.10248v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10248","description":"<p>There is growing interest in generating skeleton-based human motions from\nnatural language descriptions. While most efforts have focused on developing\nbetter neural architectures for this task, there has been no significant work\non determining the proper evaluation metric. Human evaluation is the ultimate\naccuracy measure for this task, and automated metrics should correlate well\nwith human quality judgments. Since descriptions are compatible with many\nmotions, determining the right metric is critical for evaluating and designing\neffective generative models. This paper systematically studies which metrics\nbest align with human evaluations and proposes new metrics that align even\nbetter. Our findings indicate that none of the metrics currently used for this\ntask show even a moderate correlation with human judgments on a sample level.\nHowever, for assessing average model performance, commonly used metrics such as\nR-Precision and less-used coordinate errors show strong correlations.\nAdditionally, several recently developed metrics are not recommended due to\ntheir low correlation compared to alternatives. We also introduce a novel\nmetric based on a multimodal BERT-like model, MoBERT, which offers strongly\nhuman-correlated sample-level evaluations while maintaining near-perfect\nmodel-level correlation. Our results demonstrate that this new metric exhibits\nextensive benefits over all current alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voas_J/0/1/0/all/0/1\">Jordan Voas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qixing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond Mooney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins. (arXiv:2309.10254v1 [cs.CR])","link":"http://arxiv.org/abs/2309.10254","description":"<p>Large language model (LLM) platforms, such as ChatGPT, have recently begun\noffering a plugin ecosystem to interface with third-party services on the\ninternet. While these plugins extend the capabilities of LLM platforms, they\nare developed by arbitrary third parties and thus cannot be implicitly trusted.\nPlugins also interface with LLM platforms and users using natural language,\nwhich can have imprecise interpretations. In this paper, we propose a framework\nthat lays a foundation for LLM platform designers to analyze and improve the\nsecurity, privacy, and safety of current and future plugin-integrated LLM\nplatforms. Our framework is a formulation of an attack taxonomy that is\ndeveloped by iteratively exploring how LLM platform stakeholders could leverage\ntheir capabilities and responsibilities to mount attacks against each other. As\npart of our iterative process, we apply our framework in the context of\nOpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the\npotential for the types of issues that we outline in our attack taxonomy. We\nconclude by discussing novel challenges and by providing recommendations to\nimprove the security, privacy, and safety of present and future LLM-based\ncomputing platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_U/0/1/0/all/0/1\">Umar Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohno_T/0/1/0/all/0/1\">Tadayoshi Kohno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roesner_F/0/1/0/all/0/1\">Franziska Roesner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and Hindi. (arXiv:2309.10272v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10272","description":"<p>One of the most popular downstream tasks in the field of Natural Language\nProcessing is text classification. Text classification tasks have become more\ndaunting when the texts are code-mixed. Though they are not exposed to such\ntext during pre-training, different BERT models have demonstrated success in\ntackling Code-Mixed NLP challenges. Again, in order to enhance their\nperformance, Code-Mixed NLP models have depended on combining synthetic data\nwith real-world data. It is crucial to understand how the BERT models'\nperformance is impacted when they are pretrained using corresponding code-mixed\nlanguages. In this paper, we introduce Tri-Distil-BERT, a multilingual model\npre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model\nfine-tuned on code-mixed data. Both models are evaluated across multiple NLP\ntasks and demonstrate competitive performance against larger models like mBERT\nand XLM-R. Our two-tiered pre-training approach offers efficient alternatives\nfor multilingual and code-mixed language understanding, contributing to\nadvancements in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raihan_M/0/1/0/all/0/1\">Md Nishat Raihan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_D/0/1/0/all/0/1\">Dhiman Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_A/0/1/0/all/0/1\">Antara Mahmud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion Recognition. (arXiv:2309.10294v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10294","description":"<p>In this paper, we explored how to boost speech emotion recognition (SER) with\nthe state-of-the-art speech pre-trained model (PTM), data2vec, text generation\ntechnique, GPT-4, and speech synthesis technique, Azure TTS. First, we\ninvestigated the representation ability of different speech self-supervised\npre-trained models, and we found that data2vec has a good representation\nability on the SER task. Second, we employed a powerful large language model\n(LLM), GPT-4, and emotional text-to-speech (TTS) model, Azure TTS, to generate\nemotionally congruent text and speech. We carefully designed the text prompt\nand dataset construction, to obtain the synthetic emotional speech data with\nhigh quality. Third, we studied different ways of data augmentation to promote\nthe SER task with synthetic speech, including random mixing, adversarial\ntraining, transfer learning, and curriculum learning. Experiments and ablation\nstudies on the IEMOCAP dataset demonstrate the effectiveness of our method,\ncompared with other data augmentation methods, and data augmentation with other\nsynthetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhisheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using fine-tuning and min lookahead beam search to improve Whisper. (arXiv:2309.10299v1 [eess.AS])","link":"http://arxiv.org/abs/2309.10299","description":"<p>The performance of Whisper in low-resource languages is still far from\nperfect. In addition to a lack of training data on low-resource languages, we\nidentify some limitations in the beam search algorithm used in Whisper. To\naddress these issues, we fine-tune Whisper on additional data and propose an\nimproved decoding algorithm. On the Vietnamese language, fine-tuning\nWhisper-Tiny with LoRA leads to an improvement of 38.49 in WER over the\nzero-shot Whisper-Tiny setting which is a further reduction of 1.45 compared to\nfull-parameter fine-tuning. Additionally, by using Filter-Ends and Min\nLookahead decoding algorithms, the WER reduces by 2.26 on average over a range\nof languages compared to standard beam search. These results generalise to\nlarger Whisper model sizes. We also prove a theorem that Min Lookahead\noutperforms the standard beam search algorithm used in Whisper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Do_A/0/1/0/all/0/1\">Andrea Do</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brown_O/0/1/0/all/0/1\">Oscar Brown</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengjie Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mathew_N/0/1/0/all/0/1\">Nikhil Mathew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zixin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_J/0/1/0/all/0/1\">Jawwad Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_C/0/1/0/all/0/1\">Cheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Baichuan 2: Open Large-scale Language Models. (arXiv:2309.10305v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10305","description":"<p>Large language models (LLMs) have demonstrated remarkable performance on a\nvariety of natural language tasks based on just a few examples of natural\nlanguage instructions, reducing the need for extensive feature engineering.\nHowever, most powerful LLMs are closed-source or limited in their capability\nfor languages other than English. In this technical report, we present Baichuan\n2, a series of large-scale multilingual language models containing 7 billion\nand 13 billion parameters, trained from scratch, on 2.6 trillion tokens.\nBaichuan 2 matches or outperforms other open-source models of similar size on\npublic benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan\n2 excels in vertical domains such as medicine and law. We will release all\npre-training model checkpoints to benefit the research community in better\nunderstanding the training dynamics of Baichuan 2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Borong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Chao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chenxu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_D/0/1/0/all/0/1\">Da Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">Dong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1\">Fei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_G/0/1/0/all/0/1\">Guangwei Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guosheng Dong Haizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haoze Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongda Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jiaming Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Juntao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1\">Kun Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Lei Su Liang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ru_L/0/1/0/all/0/1\">Liyun Ru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Luyao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mickel Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">MingAn Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_N/0/1/0/all/0/1\">Nuolan Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Peidong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruiyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangrong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaochuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_X/0/1/0/all/0/1\">Xin Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xuehai Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yanjun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiding Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Youxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuchen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yupeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zenan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiying Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rigorously Assessing Natural Language Explanations of Neurons. (arXiv:2309.10312v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10312","description":"<p>Natural language is an appealing medium for explaining how large language\nmodels process and store information, but evaluating the faithfulness of such\nexplanations is challenging. To help address this, we develop two modes of\nevaluation for natural language explanations that claim individual neurons\nrepresent a concept in a text input. In the observational mode, we evaluate\nclaims that a neuron $a$ activates on all and only input strings that refer to\na concept picked out by the proposed explanation $E$. In the intervention mode,\nwe construe $E$ as a claim that the neuron $a$ is a causal mediator of the\nconcept denoted by $E$. We apply our framework to the GPT-4-generated\nexplanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the\nmost confident explanations have high error rates and little to no causal\nefficacy. We close the paper by critically assessing whether natural language\nis a good choice for explanations and whether neurons are the best level of\nanalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Atticus Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DOosterlinck_K/0/1/0/all/0/1\">Karel D&#x27;Oosterlinck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10313","description":"<p>Following the success of GPT4, there has been a surge in interest in\nmultimodal large language model (MLLM) research. This line of research focuses\non developing general-purpose LLMs through fine-tuning pre-trained LLMs and\nvision models. However, catastrophic forgetting, a notorious phenomenon where\nthe fine-tuned model fails to retain similar performance compared to the\npre-trained model, still remains an inherent problem in multimodal LLMs (MLLM).\nIn this paper, we introduce EMT: Evaluating MulTimodality for evaluating the\ncatastrophic forgetting in MLLMs, by treating each MLLM as an image classifier.\nWe first apply EMT to evaluate several open-source fine-tuned MLLMs and we\ndiscover that almost all evaluated MLLMs fail to retain the same performance\nlevels as their vision encoders on standard image classification tasks.\nMoreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess\nperformance throughout the fine-tuning. Interestingly, our results suggest that\nearly-stage fine-tuning on an image dataset improves performance across other\nimage datasets, by enhancing the alignment of text and visual features.\nHowever, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in\na significant loss of generalizability, even when the image encoder remains\nfrozen. Our results suggest that MLLMs have yet to demonstrate performance on\npar with their vision models on standard image classification tasks and the\ncurrent MLLM fine-tuning procedure still has room for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yuexiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1\">Shengbang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Mu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Q/0/1/0/all/0/1\">Qing Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation. (arXiv:2309.10326v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10326","description":"<p>Recent years have witnessed the success of question answering (QA),\nespecially its potential to be a foundation paradigm for tackling diverse NLP\ntasks. However, obtaining sufficient data to build an effective and stable QA\nsystem still remains an open problem. For this problem, we introduce an\niterative bootstrapping framework for QA data augmentation (named QASnowball),\nwhich can iteratively generate large-scale high-quality QA data based on a seed\nset of supervised examples. Specifically, QASnowball consists of three modules,\nan answer extractor to extract core phrases in unlabeled documents as candidate\nanswers, a question generator to generate questions based on documents and\ncandidate answers, and a QA data filter to filter out high-quality QA data.\nMoreover, QASnowball can be self-enhanced by reseeding the seed set to\nfine-tune itself in different iterations, leading to continual improvements in\nthe generation quality. We conduct experiments in the high-resource English\nscenario and the medium-resource Chinese scenario, and the experimental results\nshow that the data generated by QASnowball can facilitate QA models: (1)\ntraining models on the generated data achieves comparable results to using\nsupervised data, and (2) pre-training on the generated data and fine-tuning on\nsupervised data can achieve better performance. Our code and generated data\nwill be released to advance further work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kunlun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shihao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guoyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KoBigBird-large: Transformation of Transformer for Korean Language Understanding. (arXiv:2309.10339v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10339","description":"<p>This work presents KoBigBird-large, a large size of Korean BigBird that\nachieves state-of-the-art performance and allows long sequence processing for\nKorean language understanding. Without further pretraining, we only transform\nthe architecture and extend the positional encoding with our proposed Tapered\nAbsolute Positional Encoding Representations (TAPER). In experiments,\nKoBigBird-large shows state-of-the-art overall performance on Korean language\nunderstanding benchmarks and the best performance on document classification\nand question answering tasks for longer sequences against the competitive\nbaseline models. We publicly release our model here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kisu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Yoonna Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Taewoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seong_J/0/1/0/all/0/1\">Jinwoo Seong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyungjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hwanseok Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Agent Behavior with Large Language Models. (arXiv:2309.10346v1 [cs.LG])","link":"http://arxiv.org/abs/2309.10346","description":"<p>Intelligent agents such as robots are increasingly deployed in real-world,\nsafety-critical settings. It is vital that these agents are able to explain the\nreasoning behind their decisions to human counterparts, however, their behavior\nis often produced by uninterpretable models such as deep neural networks. We\npropose an approach to generate natural language explanations for an agent's\nbehavior based only on observations of states and actions, agnostic to the\nunderlying model representation. We show how a compact representation of the\nagent's behavior can be learned and used to produce plausible explanations with\nminimal hallucination while affording user interaction with a pre-trained large\nlanguage model. Through user studies and empirical experiments, we show that\nour approach generates explanations as helpful as those generated by a human\ndomain expert while enabling beneficial interactions such as clarification and\ncounterfactual queries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xijia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yue Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stepputtis_S/0/1/0/all/0/1\">Simon Stepputtis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1\">Katia Sycara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_J/0/1/0/all/0/1\">Joseph Campbell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt, Condition, and Generate: Classification of Unsupported Claims with In-Context Learning. (arXiv:2309.10359v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10359","description":"<p>Unsupported and unfalsifiable claims we encounter in our daily lives can\ninfluence our view of the world. Characterizing, summarizing, and -- more\ngenerally -- making sense of such claims, however, can be challenging. In this\nwork, we focus on fine-grained debate topics and formulate a new task of\ndistilling, from such claims, a countable set of narratives. We present a\ncrowdsourced dataset of 12 controversial topics, comprising more than 120k\narguments, claims, and comments from heterogeneous sources, each annotated with\na narrative label. We further investigate how large language models (LLMs) can\nbe used to synthesise claims using In-Context Learning. We find that generated\nclaims with supported evidence can be used to improve the performance of\nnarrative classification models and, additionally, that the same model can\ninfer the stance and aspect using a few training examples. Such a model can be\nuseful in applications which rely on narratives , e.g. fact-checking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christensen_P/0/1/0/all/0/1\">Peter Ebert Christensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_S/0/1/0/all/0/1\">Srishti Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. (arXiv:2309.10400v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10400","description":"<p>In this paper, we introduce Positional Skip-wisE (PoSE) training for\nefficient adaptation of large language models~(LLMs) to extremely long context\nwindows. PoSE decouples train length from target context window size by\nsimulating long inputs using a fixed context window with manipulated position\nindices during training. Concretely, we select several short chunks from a long\ninput sequence, and introduce distinct skipping bias terms to modify the\nposition indices of each chunk. These bias terms, along with the length of each\nchunk, are altered for each training example, allowing the model to adapt to\nall positions within the target context window without training on full length\ninputs. Experiments show that, compared with fine-tuning on the full length,\nPoSE greatly reduces memory and time overhead with minimal impact on\nperformance. Leveraging this advantage, we have successfully extended the LLaMA\nmodel to 128k tokens. Furthermore, we empirically confirm that PoSE is\ncompatible with all RoPE-based LLMs and various position interpolation\nstrategies. Notably, by decoupling fine-tuning length from target context\nwindow, PoSE can theoretically extend the context window infinitely,\nconstrained only by memory usage for inference. With ongoing advancements for\nefficient inference, we believe PoSE holds great promise for scaling the\ncontext window even further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dawei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PICK: Polished & Informed Candidate Scoring for Knowledge-Grounded Dialogue Systems. (arXiv:2309.10413v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10413","description":"<p>Grounding dialogue response generation on external knowledge is proposed to\nproduce informative and engaging responses. However, current knowledge-grounded\ndialogue (KGD) systems often fail to align the generated responses with\nhuman-preferred qualities due to several issues like hallucination and the lack\nof coherence. Upon analyzing multiple language model generations, we observe\nthe presence of alternative generated responses within a single decoding\nprocess. These alternative responses are more faithful and exhibit a comparable\nor higher level of relevance to prior conversational turns compared to the\noptimal responses prioritized by the decoding processes. To address these\nchallenges and driven by these observations, we propose Polished \\&amp; Informed\nCandidate Scoring (PICK), a generation re-scoring framework that empowers\nmodels to generate faithful and relevant responses without requiring additional\nlabeled data or model tuning. Through comprehensive automatic and human\nevaluations, we demonstrate the effectiveness of PICK in generating responses\nthat are more faithful while keeping them relevant to the dialogue history.\nFurthermore, PICK consistently improves the system's performance with both\noracle and retrieved knowledge in all decoding strategies. We provide the\ndetailed implementation in https://github.com/bryanwilie/pick .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_W/0/1/0/all/0/1\">Willy Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Writer-Defined AI Personas for On-Demand Feedback Generation. (arXiv:2309.10433v1 [cs.HC])","link":"http://arxiv.org/abs/2309.10433","description":"<p>Compelling writing is tailored to its audience. This is challenging, as\nwriters may struggle to empathize with readers, get feedback in time, or gain\naccess to the target group. We propose a concept that generates on-demand\nfeedback, based on writer-defined AI personas of any target audience. We\nexplore this concept with a prototype (using GPT-3.5) in two user studies (N=5\nand N=11): Writers appreciated the concept and strategically used personas for\ngetting different perspectives. The feedback was seen as helpful and inspired\nrevisions of text and personas, although it was often verbose and unspecific.\nWe discuss the impact of on-demand feedback, the limited representativity of\ncontemporary AI systems, and further ideas for defining AI personas. This work\ncontributes to the vision of supporting writers with AI by expanding the\nsocio-technical perspective in AI tool design: To empower creators, we also\nneed to keep in mind their relationship to an audience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benharrak_K/0/1/0/all/0/1\">Karim Benharrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zindulka_T/0/1/0/all/0/1\">Tim Zindulka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_F/0/1/0/all/0/1\">Florian Lehmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heuer_H/0/1/0/all/0/1\">Hendrik Heuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buschek_D/0/1/0/all/0/1\">Daniel Buschek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling. (arXiv:2309.10435v1 [cs.IR])","link":"http://arxiv.org/abs/2309.10435","description":"<p>Recommender systems are essential for online applications, and sequential\nrecommendation has enjoyed significant prevalence due to its expressive ability\nto capture dynamic user interests. However, previous sequential modeling\nmethods still have limitations in capturing contextual information. The primary\nreason for this issue is that language models often lack an understanding of\ndomain-specific knowledge and item-related textual content. To address this\nissue, we adopt a new sequential recommendation paradigm and propose LANCER,\nwhich leverages the semantic understanding capabilities of pre-trained language\nmodels to generate personalized recommendations. Our approach bridges the gap\nbetween language models and recommender systems, resulting in more human-like\nrecommendations. We demonstrate the effectiveness of our approach through\nexperiments on several benchmark datasets, showing promising results and\nproviding valuable insights into the influence of our model on sequential\nrecommendation tasks. Furthermore, our experimental codes are publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junzhe Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1\">Shang Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Mingyue Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v1 [cs.AI])","link":"http://arxiv.org/abs/2309.10444","description":"<p>Learnersourcing involves students generating and sharing learning resources\nwith their peers. When learnersourcing multiple-choice questions, creating\nexplanations for the generated questions is a crucial step as it facilitates a\ndeeper understanding of the related concepts. However, it is often difficult\nfor students to craft effective explanations due to limited subject\nunderstanding and a tendency to merely restate the question stem, distractors,\nand correct answer. To help scaffold this task, in this work we propose a\nself-reinforcement large-language-model framework, with the goal of generating\nand evaluating explanations automatically. Comprising three modules, the\nframework generates student-aligned explanations, evaluates these explanations\nto ensure their quality and iteratively enhances the explanations. If an\nexplanation's evaluation score falls below a defined threshold, the framework\niteratively refines and reassesses the explanation. Importantly, our framework\nemulates the manner in which students compose explanations at the relevant\ngrade level. For evaluation, we had a human subject-matter expert compare the\nexplanations generated by students with the explanations created by the\nopen-source large language model Vicuna-13B, a version of Vicuna-13B that had\nbeen fine-tuned using our method, and by GPT-4. We observed that, when compared\nto other large language models, GPT-4 exhibited a higher level of creativity in\ngenerating explanations. We also found that explanations generated by GPT-4\nwere ranked higher by the human expert than both those created by the other\nmodels and the original student-created explanations. Our findings represent a\nsignificant advancement in enriching the learnersourcing experience for\nstudents and enhancing the capabilities of large language models in educational\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1\">Qiming Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leinonen_J/0/1/0/all/0/1\">Juho Leinonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1\">Alex Yuxuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pistotti_T/0/1/0/all/0/1\">Tim Pistotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1\">Alice Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1\">Paul Denny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1\">Michael Witbrock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiamou Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Unified Controllable Text Generation via Regular Expression Instruction. (arXiv:2309.10447v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10447","description":"<p>Controllable text generation is a fundamental aspect of natural language\ngeneration, with numerous methods proposed for different constraint types.\nHowever, these approaches often require significant architectural or decoding\nmodifications, making them challenging to apply to additional constraints or\nresolve different constraint combinations. To address this, our paper\nintroduces Regular Expression Instruction (REI), which utilizes an\ninstruction-based mechanism to fully exploit regular expressions' advantages to\nuniformly model diverse constraints. Specifically, our REI supports all popular\nfine-grained controllable generation constraints, i.e., lexical, positional,\nand length, as well as their complex combinations, via regular expression-style\ninstructions. Our method only requires fine-tuning on medium-scale language\nmodels or few-shot, in-context learning on large language models, and requires\nno further adjustment when applied to various constraint combinations.\nExperiments demonstrate that our straightforward approach yields high success\nrates and adaptability to various constraints while maintaining competitiveness\nin automatic metrics and outperforming most previous baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Speaker Diarization using Semantic Information: Joint Pairwise Constraints Propagation. (arXiv:2309.10456v1 [cs.SD])","link":"http://arxiv.org/abs/2309.10456","description":"<p>Speaker diarization has gained considerable attention within speech\nprocessing research community. Mainstream speaker diarization rely primarily on\nspeakers' voice characteristics extracted from acoustic signals and often\noverlook the potential of semantic information. Considering the fact that\nspeech signals can efficiently convey the content of a speech, it is of our\ninterest to fully exploit these semantic cues utilizing language models. In\nthis work we propose a novel approach to effectively leverage semantic\ninformation in clustering-based speaker diarization systems. Firstly, we\nintroduce spoken language understanding modules to extract speaker-related\nsemantic information and utilize these information to construct pairwise\nconstraints. Secondly, we present a novel framework to integrate these\nconstraints into the speaker diarization pipeline, enhancing the performance of\nthe entire system. Extensive experiments conducted on the public dataset\ndemonstrate the consistent superiority of our proposed approach over\nacoustic-only speaker diarization systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Luyao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yafeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Evaluation of GPT-4 on the ETHICS Dataset. (arXiv:2309.10492v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10492","description":"<p>This report summarizes a short study of the performance of GPT-4 on the\nETHICS dataset. The ETHICS dataset consists of five sub-datasets covering\ndifferent fields of ethics: Justice, Deontology, Virtue Ethics, Utilitarianism,\nand Commonsense Ethics. The moral judgments were curated so as to have a high\ndegree of agreement with the aim of representing shared human values rather\nthan moral dilemmas. GPT-4's performance is much better than that of previous\nmodels and suggests that learning to work with common human values is not the\nhard problem for AI ethics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodionov_S/0/1/0/all/0/1\">Sergey Rodionov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goertzel_Z/0/1/0/all/0/1\">Zarathustra Amadeus Goertzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goertzel_B/0/1/0/all/0/1\">Ben Goertzel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Open-Domain Table Question Answering via Syntax- and Structure-aware Dense Retrieval. (arXiv:2309.10506v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10506","description":"<p>Open-domain table question answering aims to provide answers to a question by\nretrieving and extracting information from a large collection of tables.\nExisting studies of open-domain table QA either directly adopt text retrieval\nmethods or consider the table structure only in the encoding layer for table\nretrieval, which may cause syntactical and structural information loss during\ntable scoring. To address this issue, we propose a syntax- and structure-aware\nretrieval method for the open-domain table QA task. It provides syntactical\nrepresentations for the question and uses the structural header and value\nrepresentations for the tables to avoid the loss of fine-grained syntactical\nand structural information. Then, a syntactical-to-structural aggregator is\nused to obtain the matching score between the question and a candidate table by\nmimicking the human retrieval process. Experimental results show that our\nmethod achieves the state-of-the-art on the NQ-tables dataset and overwhelms\nstrong baselines on a newly curated open-domain Text-to-SQL dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_N/0/1/0/all/0/1\">Nengzheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongfang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebert_J/0/1/0/all/0/1\">Joanna Siebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model in End-to-End Speech Recognition. (arXiv:2309.10524v1 [eess.AS])","link":"http://arxiv.org/abs/2309.10524","description":"<p>We present a novel integration of an instruction-tuned large language model\n(LLM) and end-to-end automatic speech recognition (ASR). Modern LLMs can\nperform a wide range of linguistic tasks within zero-shot learning when\nprovided with a precise instruction or a prompt to guide the text generation\nprocess towards the desired task. We explore using this zero-shot capability of\nLLMs to extract linguistic information that can contribute to improving ASR\nperformance. Specifically, we direct an LLM to correct grammatical errors in an\nASR hypothesis and harness the embedded linguistic knowledge to conduct\nend-to-end ASR. The proposed model is built on the hybrid connectionist\ntemporal classification (CTC) and attention architecture, where an\ninstruction-tuned LLM (i.e., Llama2) is employed as a front-end of the decoder.\nAn ASR hypothesis, subject to correction, is obtained from the encoder via CTC\ndecoding, which is then fed into the LLM along with an instruction. The decoder\nsubsequently takes as input the LLM embeddings to perform sequence generation,\nincorporating acoustic information from the encoder output. Experimental\nresults and analyses demonstrate that the proposed integration yields promising\nperformance improvements, and our approach largely benefits from LLM-based\nrescoring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_Y/0/1/0/all/0/1\">Yosuke Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ogawa_T/0/1/0/all/0/1\">Tetsuji Ogawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kobayashi_T/0/1/0/all/0/1\">Tetsunori Kobayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NSOAMT -- New Search Only Approach to Machine Translation. (arXiv:2309.10526v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10526","description":"<p>Translation automation mechanisms and tools have been developed for several\nyears to bring people who speak different languages together. A \"new search\nonly approach to machine translation\" was adopted to tackle some of the\nslowness and inaccuracy of the other technologies. The idea is to develop a\nsolution that, by indexing an incremental set of words that combine a certain\nsemantic meaning, makes it possible to create a process of correspondence\nbetween their native language record and the language of translation. This\nresearch principle assumes that the vocabulary used in a given type of\npublication/document is relatively limited in terms of language style and word\ndiversity, which enhances the greater effect of instantaneously and rigor in\nthe translation process through the indexing process. A volume of electronic\ntext documents where processed and loaded into a database, and analyzed and\nmeasured in order confirm the previous premise. Although the observed and\nprojected metric values did not give encouraging results, it was possible to\ndevelop and make available a translation tool using this approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luis_J/0/1/0/all/0/1\">Jo&#xe3;o Lu&#xed;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_D/0/1/0/all/0/1\">Diogo Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_J/0/1/0/all/0/1\">Jos&#xe9; Marques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_L/0/1/0/all/0/1\">Lu&#xed;s Campos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenMSD: Towards Multilingual Scientific Documents Similarity Measurement. (arXiv:2309.10539v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10539","description":"<p>We develop and evaluate multilingual scientific documents similarity\nmeasurement models in this work. Such models can be used to find related works\nin different languages, which can help multilingual researchers find and\nexplore papers more efficiently. We propose the first multilingual scientific\ndocuments dataset, Open-access Multilingual Scientific Documents (OpenMSD),\nwhich has 74M papers in 103 languages and 778M citation pairs. With OpenMSD, we\npretrain science-specialized language models, and explore different strategies\nto derive \"related\" paper pairs to fine-tune the models, including using a\nmixture of citation, co-citation, and bibliographic-coupling pairs. To further\nimprove the models' performance for non-English papers, we explore the use of\ngenerative language models to enrich the non-English papers with English\nsummaries. This allows us to leverage the models' English capabilities to\ncreate better representations for non-English papers. Our best model\nsignificantly outperforms strong baselines by 7-16% (in mean average\nprecision).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Ji Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korotkov_I/0/1/0/all/0/1\">Ivan Korotkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_K/0/1/0/all/0/1\">Keith Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_D/0/1/0/all/0/1\">Dana Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Don Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Leeching: An Extraction Attack Targeting LLMs. (arXiv:2309.10544v1 [cs.LG])","link":"http://arxiv.org/abs/2309.10544","description":"<p>Model Leeching is a novel extraction attack targeting Large Language Models\n(LLMs), capable of distilling task-specific knowledge from a target LLM into a\nreduced parameter model. We demonstrate the effectiveness of our attack by\nextracting task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match\n(EM) similarity, and SQuAD EM and F1 accuracy scores of 75% and 87%,\nrespectively for only $50 in API cost. We further demonstrate the feasibility\nof adversarial attack transferability from an extracted model extracted via\nModel Leeching to perform ML attack staging against a target LLM, resulting in\nan 11% increase to attack success rate when applied to ChatGPT-3.5-Turbo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Birch_L/0/1/0/all/0/1\">Lewis Birch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hackett_W/0/1/0/all/0/1\">William Hackett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trawicki_S/0/1/0/all/0/1\">Stefan Trawicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suri_N/0/1/0/all/0/1\">Neeraj Suri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garraghan_P/0/1/0/all/0/1\">Peter Garraghan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neighbourhood-Aware Differential Privacy Mechanism for Static Word Embeddings. (arXiv:2309.10551v1 [cs.LG])","link":"http://arxiv.org/abs/2309.10551","description":"<p>We propose a Neighbourhood-Aware Differential Privacy (NADP) mechanism\nconsidering the neighbourhood of a word in a pretrained static word embedding\nspace to determine the minimal amount of noise required to guarantee a\nspecified privacy level. We first construct a nearest neighbour graph over the\nwords using their embeddings, and factorise it into a set of connected\ncomponents (i.e. neighbourhoods). We then separately apply different levels of\nGaussian noise to the words in each neighbourhood, determined by the set of\nwords in that neighbourhood. Experiments show that our proposed NADP mechanism\nconsistently outperforms multiple previously proposed DP mechanisms such as\nLaplacian, Gaussian, and Mahalanobis in multiple downstream tasks, while\nguaranteeing higher levels of privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otake_S/0/1/0/all/0/1\">Shuichi Otake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machide_T/0/1/0/all/0/1\">Tomoya Machide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawarabayashi_K/0/1/0/all/0/1\">Ken-ichi Kawarabayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Modeling For Spoken Language Identification. (arXiv:2309.10567v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10567","description":"<p>Spoken language identification refers to the task of automatically predicting\nthe spoken language in a given utterance. Conventionally, it is modeled as a\nspeech-based language identification task. Prior techniques have been\nconstrained to a single modality; however in the case of video data there is a\nwealth of other metadata that may be beneficial for this task. In this work, we\npropose MuSeLI, a Multimodal Spoken Language Identification method, which\ndelves into the use of various metadata sources to enhance language\nidentification. Our study reveals that metadata such as video title,\ndescription and geographic location provide substantial information to identify\nthe spoken language of the multimedia recording. We conduct experiments using\ntwo diverse public datasets of YouTube videos, and obtain state-of-the-art\nresults on the language identification task. We additionally conduct an\nablation study that describes the distinct contribution of each modality for\nlanguage recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1\">Shikhar Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Min Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1\">Shikhar Vashishth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganapathy_S/0/1/0/all/0/1\">Sriram Ganapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axelrod_V/0/1/0/all/0/1\">Vera Axelrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esch_D/0/1/0/all/0/1\">Daan van Esch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_S/0/1/0/all/0/1\">Sandy Ritchie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesa_J/0/1/0/all/0/1\">Jason Riesa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Deep Cross-Language Entity Alignment. (arXiv:2309.10598v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10598","description":"<p>Cross-lingual entity alignment is the task of finding the same semantic\nentities from different language knowledge graphs. In this paper, we propose a\nsimple and novel unsupervised method for cross-language entity alignment. We\nutilize the deep learning multi-language encoder combined with a machine\ntranslator to encode knowledge graph text, which reduces the reliance on label\ndata. Unlike traditional methods that only emphasize global or local alignment,\nour method simultaneously considers both alignment strategies. We first view\nthe alignment task as a bipartite matching problem and then adopt the\nre-exchanging idea to accomplish alignment. Compared with the traditional\nbipartite matching algorithm that only gives one optimal solution, our\nalgorithm generates ranked matching results which enabled many potentials\ndownstream tasks. Additionally, our method can adapt two different types of\noptimization (minimal and maximal) in the bipartite matching process, which\nprovides more flexibility. Our evaluation shows, we each scored 0.966, 0.990,\nand 0.996 Hits@1 rates on the DBP15K dataset in Chinese, Japanese, and French\nto English alignment tasks. We outperformed the state-of-the-art method in\nunsupervised and semi-supervised categories. Compared with the state-of-the-art\nsupervised method, our method outperforms 2.6% and 0.4% in Ja-En and Fr-En\nalignment tasks while marginally lower by 0.2% in the Zh-En alignment task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chuanyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yiming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xia Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FRACAS: A FRench Annotated Corpus of Attribution relations in newS. (arXiv:2309.10604v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10604","description":"<p>Quotation extraction is a widely useful task both from a sociological and\nfrom a Natural Language Processing perspective. However, very little data is\navailable to study this task in languages other than English. In this paper, we\npresent a manually annotated corpus of 1676 newswire texts in French for\nquotation extraction and source attribution. We first describe the composition\nof our corpus and the choices that were made in selecting the data. We then\ndetail the annotation guidelines and annotation process, as well as a few\nstatistics about the final corpus and the obtained balance between quote types\n(direct, indirect and mixed, which are particularly challenging). We end by\ndetailing our inter-annotator agreement between the 8 annotators who worked on\nmanual labelling, which is substantially high for such a difficult linguistic\nphenomenon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richard_A/0/1/0/all/0/1\">Ange Richard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonzo_Canul_L/0/1/0/all/0/1\">Laura Alonzo-Canul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portet_F/0/1/0/all/0/1\">Fran&#xe7;ois Portet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Medical Dialogue Generation with Abstract Meaning Representations. (arXiv:2309.10608v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10608","description":"<p>Medical Dialogue Generation serves a critical role in telemedicine by\nfacilitating the dissemination of medical expertise to patients. Existing\nstudies focus on incorporating textual representations, which have limited\ntheir ability to represent the semantics of text, such as ignoring important\nmedical entities. To enhance the model's understanding of the textual semantics\nand the medical knowledge including entities and relations, we introduce the\nuse of Abstract Meaning Representations (AMR) to construct graphical\nrepresentations that delineate the roles of language constituents and medical\nentities within the dialogues. In this paper, We propose a novel framework that\nmodels dialogues between patients and healthcare professionals using AMR\ngraphs, where the neural networks incorporate textual and graphical knowledge\nwith a dual attention mechanism. Experimental results show that our framework\noutperforms strong baseline models in medical dialogue generation,\ndemonstrating the effectiveness of AMR graphs in enhancing the representations\nof medical knowledge and logical relationships. Furthermore, to support future\nresearch in this domain, we provide the corresponding source code at\nhttps://github.com/Bernard-Yang/MedDiaAMR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bohao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])","link":"http://arxiv.org/abs/2309.10621","description":"<p>Relevance labels, which indicate whether a search result is valuable to a\nsearcher, are key to evaluating and optimising search systems. The best way to\ncapture the true preferences of users is to ask them for their careful feedback\non which results would be useful, but this approach does not scale to produce a\nlarge number of labels. Getting relevance labels at scale is usually done with\nthird-party labellers, who judge on behalf of the user, but there is a risk of\nlow-quality data if the labeller doesn't understand user needs. To improve\nquality, one standard approach is to study real users through interviews, user\nstudies and direct feedback, find areas where labels are systematically\ndisagreeing with users, then educate labellers about user needs through judging\nguidelines, training and monitoring. This paper introduces an alternate\napproach for improving label quality. It takes careful feedback from real\nusers, which by definition is the highest-quality first-party gold data that\ncan be derived, and develops an large language model prompt that agrees with\nthat data.\n</p>\n<p>We present ideas and observations from deploying language models for\nlarge-scale relevance labelling at Bing, and illustrate with data from TREC. We\nhave found large language models can be effective, with accuracy as good as\nhuman labellers and similar capability to pick the hardest queries, best runs,\nand best groups. Systematic changes to the prompts make a difference in\naccuracy, but so too do simple paraphrases. To measure agreement with real\nsearchers needs high-quality ``gold'' labels, but with these we find that\nmodels produce better labels than third-party workers, for a fraction of the\ncost, and these labels let us train notably better rankers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomas_P/0/1/0/all/0/1\">Paul Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spielman_S/0/1/0/all/0/1\">Seth Spielman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Craswell_N/0/1/0/all/0/1\">Nick Craswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_B/0/1/0/all/0/1\">Bhaskar Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CFGPT: Chinese Financial Assistant with Large Language Model. (arXiv:2309.10654v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10654","description":"<p>Large language models (LLMs) have demonstrated great potential in natural\nlanguage processing tasks within the financial domain. In this work, we present\na Chinese Financial Generative Pre-trained Transformer framework, named CFGPT,\nwhich includes a dataset~(CFData) for pre-training and supervised fine-tuning,\na financial LLM~(CFLLM) to adeptly manage financial texts, and a deployment\nframework~(CFAPP) designed to navigate real-world financial applications. The\nCFData comprising both a pre-training dataset and a supervised fine-tuning\ndataset, where the pre-training dataset collates Chinese financial data and\nanalytics, alongside a smaller subset of general-purpose text with 584M\ndocuments and 141B tokens in total, and the supervised fine-tuning dataset is\ntailored for six distinct financial tasks, embodying various facets of\nfinancial analysis and decision-making with 1.5M instruction pairs and 1.5B\ntokens in total. The CFLLM, which is based on InternLM-7B to balance the model\ncapability and size, is trained on CFData in two stage, continued pre-training\nand supervised fine-tuning. The CFAPP is centered on large language models\n(LLMs) and augmented with additional modules to ensure multifaceted\nfunctionality in real-world application. Our codes are released at\nhttps://github.com/TongjiFinLab/CFGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangtong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1\">Yuxuan Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Dawei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhijun Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Changjun Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. (arXiv:1910.10683v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1910.10683","description":"<p>Transfer learning, where a model is first pre-trained on a data-rich task\nbefore being fine-tuned on a downstream task, has emerged as a powerful\ntechnique in natural language processing (NLP). The effectiveness of transfer\nlearning has given rise to a diversity of approaches, methodology, and\npractice. In this paper, we explore the landscape of transfer learning\ntechniques for NLP by introducing a unified framework that converts all\ntext-based language problems into a text-to-text format. Our systematic study\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\napproaches, and other factors on dozens of language understanding tasks. By\ncombining the insights from our exploration with scale and our new ``Colossal\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\ncovering summarization, question answering, text classification, and more. To\nfacilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matena_M/0/1/0/all/0/1\">Michael Matena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peter J. Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Demonstration Tuning for Pre-trained Language Models. (arXiv:2204.04392v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04392","description":"<p>Pretrained language models can be effectively stimulated by textual prompts\nor demonstrations, especially in low-data scenarios. Recent works have focused\non automatically searching discrete or continuous prompts or optimized\nverbalizers, yet studies for the demonstration are still limited. Concretely,\nthe demonstration examples are crucial for an excellent final performance of\nprompt-tuning. In this paper, we propose a novel pluggable, extensible, and\nefficient approach named contrastive demonstration tuning, which is free of\ndemonstration sampling. Furthermore, the proposed approach can be: (i) Plugged\ninto any previous prompt-tuning approaches; (ii) Extended to widespread\nclassification tasks with a large number of categories. Experimental results on\n16 datasets illustrate that our method integrated with previous approaches\nLM-BFF and P-tuning can yield better performance. Code is available in\nhttps://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning. (arXiv:2205.02355v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02355","description":"<p>Pre-trained language models have contributed significantly to relation\nextraction by demonstrating remarkable few-shot learning abilities. However,\nprompt tuning methods for relation extraction may still fail to generalize to\nthose rare or hard patterns. Note that the previous parametric learning\nparadigm can be viewed as memorization regarding training data as a book and\ninference as the close-book test. Those long-tailed or hard patterns can hardly\nbe memorized in parameters given few-shot instances. To this end, we regard RE\nas an open-book examination and propose a new semiparametric paradigm of\nretrieval-enhanced prompt tuning for relation extraction. We construct an\nopen-book datastore for retrieval regarding prompt-based instance\nrepresentations and corresponding relation labels as memorized key-value pairs.\nDuring inference, the model can infer relations by linearly interpolating the\nbase output of PLM with the non-parametric nearest neighbor distribution over\nthe datastore. In this way, our model not only infers relation through\nknowledge stored in the weights during training but also assists\ndecision-making by unwinding and querying examples in the open-book datastore.\nExtensive experiments on benchmark datasets show that our method can achieve\nstate-of-the-art in both standard supervised and few-shot settings. Code are\navailable in https://github.com/zjunlp/PromptKG/tree/main/research/RetrievalRE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14704","description":"<p>Prompt learning approaches have made waves in natural language processing by\ninducing better few-shot performance while they still follow a parametric-based\nlearning paradigm; the oblivion and rote memorization problems in learning may\nencounter unstable generalization issues. Specifically, vanilla prompt learning\nmay struggle to utilize atypical instances by rote during fully-supervised\ntraining or overfit shallow patterns with low-shot data. To alleviate such\nlimitations, we develop RetroPrompt with the motivation of decoupling knowledge\nfrom memorization to help the model strike a balance between generalization and\nmemorization. In contrast with vanilla prompt learning, RetroPrompt constructs\nan open-book knowledge-store from training instances and implements a retrieval\nmechanism during the process of input, training and inference, thus equipping\nthe model with the ability to retrieve related contexts from the training\ncorpus as cues for enhancement. Extensive experiments demonstrate that\nRetroPrompt can obtain better performance in both few-shot and zero-shot\nsettings. Besides, we further illustrate that our proposed RetroPrompt can\nyield better generalization abilities with new datasets. Detailed analysis of\nmemorization indeed reveals RetroPrompt can reduce the reliance of language\nmodels on memorization; thus, improving generalization for downstream tasks.\nCode is available in\nhttps://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Decoupled Retrieval Representation for Nearest Neighbour Neural Machine Translation. (arXiv:2209.08738v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.08738","description":"<p>K-Nearest Neighbor Neural Machine Translation (kNN-MT) successfully\nincorporates external corpus by retrieving word-level representations at test\ntime. Generally, kNN-MT borrows the off-the-shelf context representation in the\ntranslation task, e.g., the output of the last decoder layer, as the query\nvector of the retrieval task. In this work, we highlight that coupling the\nrepresentations of these two tasks is sub-optimal for fine-grained retrieval.\nTo alleviate it, we leverage supervised contrastive learning to learn the\ndistinctive retrieval representation derived from the original context\nrepresentation. We also propose a fast and effective approach to constructing\nhard negative samples. Experimental results on five domains show that our\napproach improves the retrieval accuracy and BLEU score compared to vanilla\nkNN-MT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_R/0/1/0/all/0/1\">Rongxiang Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Ming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentially Private Optimization on Large Model at Small Cost. (arXiv:2210.00038v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.00038","description":"<p>Differentially private (DP) optimization is the standard paradigm to learn\nlarge neural networks that are accurate and privacy-preserving. The\ncomputational cost for DP deep learning, however, is notoriously heavy due to\nthe per-sample gradient clipping. Existing DP implementations are 2-1000X more\ncostly in time and space complexity than the standard (non-private) training.\nIn this work, we develop a novel Book-Keeping (BK) technique that implements\nexisting DP optimizers (thus achieving the same accuracy), with a substantial\nimprovement on the computational cost. Specifically, BK enables DP training on\nlarge models and high dimensional data to be roughly as fast and memory-saving\nas the standard training, whereas previous DP algorithms can be inefficient or\nincapable of training due to memory error. The computational advantage of BK is\nsupported by the complexity analysis as well as extensive experiments on vision\nand language tasks. Our implementation achieves state-of-the-art (SOTA)\naccuracy with very small extra cost: on GPT2 and at almost the same memory cost\n(&lt;1% overhead), BK has 1.03X the time complexity of the standard training\n(0.83X training speed in practice), and 0.61X the time complexity of the most\nefficient DP implementation (1.36X training speed in practice). We open-source\nthe codebase for the BK algorithm at the FastDP library\n(https://github.com/awslabs/fast-differential-privacy).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1\">Sheng Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Study Incorporating Linguistic Knowledge on Filled Pauses for Personalized Spontaneous Speech Synthesis. (arXiv:2210.07559v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2210.07559","description":"<p>We present a comprehensive empirical study for personalized spontaneous\nspeech synthesis on the basis of linguistic knowledge. With the advent of voice\ncloning for reading-style speech synthesis, a new voice cloning paradigm for\nhuman-like and spontaneous speech synthesis is required. We, therefore, focus\non personalized spontaneous speech synthesis that can clone both the\nindividual's voice timbre and speech disfluency. Specifically, we deal with\nfilled pauses, a major source of speech disfluency, which is known to play an\nimportant role in speech generation and communication in psychology and\nlinguistics. To comparatively evaluate personalized filled pause insertion and\nnon-personalized filled pause prediction methods, we developed a speech\nsynthesis method with a non-personalized external filled pause predictor\ntrained with a multi-speaker corpus. The results clarify the position-word\nentanglement of filled pauses, i.e., the necessity of precisely predicting\npositions for naturalness and the necessity of precisely predicting words for\nindividuality on the evaluation of synthesized speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsunaga_Y/0/1/0/all/0/1\">Yuta Matsunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeki_T/0/1/0/all/0/1\">Takaaki Saeki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamichi_S/0/1/0/all/0/1\">Shinnosuke Takamichi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saruwatari_H/0/1/0/all/0/1\">Hiroshi Saruwatari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying Automated Machine Translation to Educational Video Courses. (arXiv:2301.03141v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.03141","description":"<p>We studied the capability of automated machine translation in the online\nvideo education space by automatically translating Khan Academy videos with\nstate-of-the-art translation models and applying text-to-speech synthesis and\naudio/video synchronization to build engaging videos in target languages. We\nalso analyzed and established two reliable translation confidence estimators\nbased on round-trip translations in order to efficiently manage translation\nquality and reduce human translation effort. Finally, we developed a deployable\nsystem to deliver translated videos to end users and collect user corrections\nfor iterative improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linden Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Reliable Neural Machine Translation with Consistency-Aware Meta-Learning. (arXiv:2303.10966v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.10966","description":"<p>Neural machine translation (NMT) has achieved remarkable success in producing\nhigh-quality translations. However, current NMT systems suffer from a lack of\nreliability, as their outputs that are often affected by lexical or syntactic\nchanges in inputs, resulting in large variations in quality. This limitation\nhinders the practicality and trustworthiness of NMT. A contributing factor to\nthis problem is that NMT models trained with the one-to-one paradigm struggle\nto handle the source diversity phenomenon, where inputs with the same meaning\ncan be expressed differently. In this work, we treat this problem as a bilevel\noptimization problem and present a consistency-aware meta-learning (CAML)\nframework derived from the model-agnostic meta-learning (MAML) algorithm to\naddress it. Specifically, the NMT model with CAML (named CoNMT) first learns a\nconsistent meta representation of semantically equivalent sentences in the\nouter loop. Subsequently, a mapping from the meta representation to the output\nsentence is learned in the inner loop, allowing the NMT model to translate\nsemantically equivalent sentences to the same target sentence. We conduct\nexperiments on the NIST Chinese to English task, three WMT translation tasks,\nand the TED M2O task. The results demonstrate that CoNMT effectively improves\noverall translation quality and reliably handles diverse inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_R/0/1/0/all/0/1\">Rongxiang Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wensen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Changfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Diverse Role-Players for Summarization Evaluation. (arXiv:2303.15078v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.15078","description":"<p>Text summarization has a wide range of applications in many scenarios. The\nevaluation of the quality of the generated text is a complex problem. A big\nchallenge to language evaluation is that there is a clear divergence between\nexisting metrics and human evaluation. A document summary's quality can be\nassessed by human annotators on various criteria, both objective ones like\ngrammar and correctness, and subjective ones like informativeness,\nsuccinctness, and appeal. Most of the automatic evaluation methods like\nBLUE/ROUGE may be not able to adequately capture the above dimensions. In this\npaper, we propose a new evaluation framework based on LLMs, which provides a\ncomprehensive evaluation framework by comparing generated text and reference\ntext from both objective and subjective aspects. First, we propose to model\nobjective and subjective dimensions of generated text based on roleplayers\nprompting mechanism. Furthermore, we introduce a context-based prompting\nmechanism that is able to generate dynamic roleplayer profiles based on input\ncontext. Finally, we design a multi-roleplayer prompting technology based on\nbatch prompting and integrate multiple outputs into the final evaluation\nresults. Experimental results on three real datasets for summarization show\nthat our model is highly competitive and has a very high consistency with human\nannotators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Ning Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shining Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs. (arXiv:2305.03513v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03513","description":"<p>ChatGPT, as a recently launched large language model (LLM), has shown\nsuperior performance in various natural language processing (NLP) tasks.\nHowever, two major limitations hinder its potential applications: (1) the\ninflexibility of finetuning on downstream tasks and (2) the lack of\ninterpretability in the decision-making process. To tackle these limitations,\nwe propose a novel framework that leverages the power of ChatGPT for specific\ntasks, such as text classification, while improving its interpretability. The\nproposed framework conducts a knowledge graph extraction task to extract\nrefined and structural knowledge from the raw data using ChatGPT. The rich\nknowledge is then converted into a graph, which is further used to train an\ninterpretable linear classifier to make predictions. To evaluate the\neffectiveness of our proposed method, we conduct experiments on four datasets.\nThe result shows that our method can significantly improve the performance\ncompared to directly utilizing ChatGPT for text classification tasks. And our\nmethod provides a more transparent decision-making process compared with\nprevious text classification methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yucheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hehuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wenliang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1\">Qiaoyu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1\">Gengchen Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Speaking Styles Using a Large Language Model. (arXiv:2305.10321v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10321","description":"<p>Reference-based Text-to-Speech (TTS) models can generate multiple,\nprosodically-different renditions of the same target text. Such models jointly\nlearn a latent acoustic space during training, which can be sampled from during\ninference. Controlling these models during inference typically requires finding\nan appropriate reference utterance, which is non-trivial.\n</p>\n<p>Large generative language models (LLMs) have shown excellent performance in\nvarious language-related tasks. Given only a natural language query text (the\nprompt), such models can be used to solve specific, context-dependent tasks.\nRecent work in TTS has attempted similar prompt-based control of novel speaking\nstyle generation. Those methods do not require a reference utterance and can,\nunder ideal conditions, be controlled with only a prompt. But existing methods\ntypically require a prompt-labelled speech corpus for jointly training a\nprompt-conditioned encoder.\n</p>\n<p>In contrast, we instead employ an LLM to directly suggest prosodic\nmodifications for a controllable TTS model, using contextual information\nprovided in the prompt. The prompt can be designed for a multitude of tasks.\nHere, we give two demonstrations: control of speaking style; prosody\nappropriate for a given dialogue context. The proposed method is rated most\nappropriate in 50% of cases vs. 31% for a baseline model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sigurgeirsson_A/0/1/0/all/0/1\">Atli Thor Sigurgeirsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_S/0/1/0/all/0/1\">Simon King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v4 [q-fin.ST] UPDATED)","link":"http://arxiv.org/abs/2306.03763","description":"<p>ChatGPT has demonstrated remarkable capabilities across various natural\nlanguage processing (NLP) tasks. However, its potential for inferring dynamic\nnetwork structures from temporal textual data, specifically financial news,\nremains an unexplored frontier. In this research, we introduce a novel\nframework that leverages ChatGPT's graph inference capabilities to enhance\nGraph Neural Networks (GNN). Our framework adeptly extracts evolving network\nstructures from textual data, and incorporates these networks into graph neural\nnetworks for subsequent predictive tasks. The experimental results from stock\nmovement forecasting indicate our model has consistently outperformed the\nstate-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios\nconstructed based on our model's outputs demonstrate higher annualized\ncumulative returns, alongside reduced volatility and maximum drawdown. This\nsuperior performance highlights the potential of ChatGPT for text-based network\ninferences and underscores its promising implications for the financial sector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_Z/0/1/0/all/0/1\">Zihan Chen</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zheng_L/0/1/0/all/0/1\">Lei Nico Zheng</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Yuan_J/0/1/0/all/0/1\">Jialu Yuan</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhu_D/0/1/0/all/0/1\">Di Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Large Enterprise Language Models via Ontological Reasoning. (arXiv:2306.10723v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.10723","description":"<p>Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to\ndiverse goals, thanks to task-specific training data. Task specificity should\ngo hand in hand with domain orientation, that is, the specialization of an LLM\nto accurately address the tasks of a given realm of interest. However, models\nare usually fine-tuned over publicly available data or, at most, over ground\ndata from databases, ignoring business-level definitions and domain experience.\nOn the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and\naugment such domain knowledge via ontological reasoning. With the goal of\ncombining LLM flexibility with the domain orientation of EKGs, we propose a\nnovel neurosymbolic architecture that leverages the power of ontological\nreasoning to build task- and domain-specific corpora for LLM fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baldazzi_T/0/1/0/all/0/1\">Teodoro Baldazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellomarini_L/0/1/0/all/0/1\">Luigi Bellomarini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceri_S/0/1/0/all/0/1\">Stefano Ceri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_A/0/1/0/all/0/1\">Andrea Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gentili_A/0/1/0/all/0/1\">Andrea Gentili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sallinger_E/0/1/0/all/0/1\">Emanuel Sallinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.06985","description":"<p>Aiming to populate generalizable engineering design knowledge, we propose a\nmethod to extract facts of the form &lt;head entity, relationship, tail entity&gt;\nfrom sentences found in patent documents. These facts could be combined within\nand across patent documents to form knowledge graphs that serve as schemes for\nrepresenting as well as storing design knowledge. Existing methods in\nengineering design literature often utilise a set of predefined relationships\nto populate triples that are statistical approximations rather than facts. In\nour method, we train a tagger to identify both entities and relationships from\na sentence. Given a pair of entities, we train another tagger to identify the\nspecific relationship tokens. For training these taggers, we manually construct\na dataset of 44,227 sentences and corresponding facts. We benchmark our method\nagainst two typically recommended approaches. We apply our method by extracting\nfacts from sentences found in patents related to fan systems. We build a\nknowledge base using these facts to demonstrate how domain ontologies could be\nconstructed and contextualised knowledge of subsystems could be visualised. We\nthen search the knowledge base for key issues prevailing in fan systems. We\norganize the responses into knowledge graphs and hold a comparative discussion\nagainst the opinions about the key issues from ChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_L/0/1/0/all/0/1\">L Siddharth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph. (arXiv:2307.07697v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.07697","description":"<p>Large language models (LLMs) have made significant strides in various tasks,\nyet they often struggle with complex reasoning and exhibit poor performance in\nscenarios where knowledge traceability, timeliness, and accuracy are crucial.\nTo address these limitations, we present Think-on-Graph (ToG), a novel\nframework that leverages knowledge graphs to enhance LLMs' ability for deep and\nresponsible reasoning. By employing ToG, we can identify entities relevant to a\ngiven question and conduct exploration and reasoning to retrieve related\ntriples from an external knowledge database. This iterative procedure generates\nmultiple reasoning pathways consisting of sequentially connected triplets until\nsufficient information is gathered to answer the question or the maximum depth\nis reached. Through experiments on complex multi-hop reasoning\nquestion-answering tasks, we demonstrate that ToG outperforms existing methods,\neffectively addressing the aforementioned limitations of LLMs without incurring\nadditional training costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiashuo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengjin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lumingyuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Saizhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Heung-Yeung Shum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive Review. (arXiv:2308.04306v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.04306","description":"<p>The history of metaphor research also marks the evolution of knowledge\ninfusion research. With the continued advancement of deep learning techniques\nin recent years, the natural language processing community has shown great\ninterest in applying knowledge to successful results in metaphor recognition\ntasks. Although there has been a gradual increase in the number of approaches\ninvolving knowledge injection in the field of metaphor recognition, there is a\nlack of a complete review article on knowledge injection based approaches.\nTherefore, the goal of this paper is to provide a comprehensive review of\nresearch advances in the application of deep learning for knowledge injection\nin metaphor recognition tasks. In this paper, we systematically summarize and\ngeneralize the mainstream knowledge and knowledge injection principles, as well\nas review the datasets, evaluation metrics, and benchmark models used in\nmetaphor recognition tasks. Finally, we explore the current issues facing\nknowledge injection methods and provide an outlook on future research\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenye Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingbao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Is Not All You Need Anymore. (arXiv:2308.07661v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2308.07661","description":"<p>In recent years, the popular Transformer architecture has achieved great\nsuccess in many application areas, including natural language processing and\ncomputer vision. Many existing works aim to reduce the computational and memory\ncomplexity of the self-attention mechanism in the Transformer by trading off\nperformance. However, performance is key for the continuing success of the\nTransformer. In this paper, a family of drop-in replacements for the\nself-attention mechanism in the Transformer, called the Extractors, is\nproposed. Four types of the Extractors, namely the super high-performance\nExtractor (SHE), the higher-performance Extractor (HE), the worthwhile\nExtractor (WE), and the minimalist Extractor (ME), are proposed as examples.\nExperimental results show that replacing the self-attention mechanism with the\nSHE evidently improves the performance of the Transformer, whereas the\nsimplified versions of the SHE, i.e., the HE, the WE, and the ME, perform close\nto or better than the self-attention mechanism with less computational and\nmemory complexity. Furthermore, the proposed Extractors have the potential or\nare able to run faster than the self-attention mechanism since their critical\npaths of computation are much shorter. Additionally, the sequence prediction\nproblem in the context of text generation is formulated using variable-length\ndiscrete-time Markov chains, and the Transformer is reviewed based on our\nunderstanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2308.15126","description":"<p>Large Vision-Language Models (LVLMs) have recently achieved remarkable\nsuccess. However, LVLMs are still plagued by the hallucination problem, which\nlimits the practicality in many scenarios. Hallucination refers to the\ninformation of LVLMs' responses that does not exist in the visual input, which\nposes potential risks of substantial consequences. There has been limited work\nstudying hallucination evaluation in LVLMs. In this paper, we propose\nHallucination Evaluation based on Large Language Models (HaELM), an LLM-based\nhallucination evaluation framework. HaELM achieves an approximate 95%\nperformance comparable to ChatGPT and has additional advantages including low\ncost, reproducibility, privacy preservation and local deployment. Leveraging\nthe HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we\nanalyze the factors contributing to hallucination in LVLMs and offer helpful\nsuggestions to mitigate the hallucination problem. Our training data and human\nannotation hallucination data will be made public soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Pengcheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenlin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1\">Jitao Sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoyu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FOLLOWUPQG: Towards Information-Seeking Follow-up Question Generation. (arXiv:2309.05007v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.05007","description":"<p>Humans ask follow-up questions driven by curiosity, which reflects a creative\nhuman cognitive process. We introduce the task of real-world\ninformation-seeking follow-up question generation (FQG), which aims to generate\nfollow-up questions seeking a more in-depth understanding of an initial\nquestion and answer. We construct FOLLOWUPQG, a dataset of over 3K real-world\n(initial question, answer, follow-up question) tuples collected from a Reddit\nforum providing layman-friendly explanations for open-ended questions. In\ncontrast to existing datasets, questions in FOLLOWUPQG use more diverse\npragmatic strategies to seek information, and they also show higher-order\ncognitive skills (such as applying and relating). We evaluate current question\ngeneration models on their efficacy for generating follow-up questions,\nexploring how to generate specific types of follow-up questions based on\nstep-by-step demonstrations. Our results validate FOLLOWUPQG as a challenging\nbenchmark, as model-generated questions are adequate but far from human-raised\nquestions in terms of informativeness and complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liangming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of NetOps Capability of Pre-Trained Large Language Models. (arXiv:2309.05557v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.05557","description":"<p>Nowadays, the versatile capabilities of Pre-trained Large Language Models\n(LLMs) have attracted much attention from the industry. However, some vertical\ndomains are more interested in the in-domain capabilities of LLMs. For the\nNetworks domain, we present NetEval, an evaluation set for measuring the\ncomprehensive capabilities of LLMs in Network Operations (NetOps). NetEval is\ndesigned for evaluating the commonsense knowledge and inference ability in\nNetOps in a multi-lingual context. NetEval consists of 5,732 questions about\nNetOps, covering five different sub-domains of NetOps. With NetEval, we\nsystematically evaluate the NetOps capability of 26 publicly available LLMs.\nThe results show that only GPT-4 can achieve a performance competitive to\nhumans. However, some open models like LLaMA 2 demonstrate significant\npotential.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yukai Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haifeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xizheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziqiu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yanyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Dapeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiuting Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Chao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinchi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models. (arXiv:2309.06085v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.06085","description":"<p>The rapid development of Large Language Models (LLMs) and the emergence of\nnovel abilities with scale have necessitated the construction of holistic,\ndiverse and challenging benchmarks such as HELM and BIG-bench. However, at the\nmoment, most of these benchmarks focus only on performance in English and\nevaluations that include Southeast Asian (SEA) languages are few in number. We\ntherefore propose BHASA, a holistic linguistic and cultural evaluation suite\nfor LLMs in SEA languages. It comprises three components: (1) a NLP benchmark\ncovering eight tasks across Natural Language Understanding (NLU), Generation\n(NLG) and Reasoning (NLR) tasks, (2) LINDSEA, a linguistic diagnostic toolkit\nthat spans the gamut of linguistic phenomena including syntax, semantics and\npragmatics, and (3) a cultural diagnostics dataset that probes for both\ncultural representation and sensitivity. For this preliminary effort, we\nimplement the NLP benchmark only for Indonesian, Vietnamese, Thai and Tamil,\nand we only include Indonesian and Tamil for LINDSEA and the cultural\ndiagnostics dataset. As GPT-4 is purportedly one of the best-performing\nmultilingual LLMs at the moment, we use it as a yardstick to gauge the\ncapabilities of LLMs in the context of SEA languages. Our initial experiments\non GPT-4 with BHASA find it lacking in various aspects of linguistic\ncapabilities, cultural representation and sensitivity in the targeted SEA\nlanguages. BHASA is a work in progress and will continue to be improved and\nexpanded in the future. The repository for this paper can be found at:\nhttps://github.com/aisingapore/BHASA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leong_W/0/1/0/all/0/1\">Wei Qi Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngui_J/0/1/0/all/0/1\">Jian Gang Ngui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susanto_Y/0/1/0/all/0/1\">Yosephine Susanto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rengarajan_H/0/1/0/all/0/1\">Hamsawardhini Rengarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarveswaran_K/0/1/0/all/0/1\">Kengatharaiyer Sarveswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjhi_W/0/1/0/all/0/1\">William Chandra Tjhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.07315","description":"<p>Transformers have significantly advanced the field of natural language\nprocessing, but comprehending their internal mechanisms remains a challenge. In\nthis paper, we introduce a novel geometric perspective that elucidates the\ninner mechanisms of transformer operations. Our primary contribution is\nillustrating how layer normalization confines the latent features to a\nhyper-sphere, subsequently enabling attention to mold the semantic\nrepresentation of words on this surface. This geometric viewpoint seamlessly\nconnects established properties such as iterative refinement and contextual\nembeddings. We validate our insights by probing a pre-trained 124M parameter\nGPT-2 model. Our findings reveal clear query-key attention patterns in early\nlayers and build upon prior observations regarding the subject-specific nature\nof attention heads at deeper layers. Harnessing these geometric insights, we\npresent an intuitive understanding of transformers, depicting them as processes\nthat model the trajectory of word particles along the hyper-sphere.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Molina_R/0/1/0/all/0/1\">Raul Molina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Rise and Potential of Large Language Model Based Agents: A Survey. (arXiv:2309.07864v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2309.07864","description":"<p>For a long time, humanity has pursued artificial intelligence (AI) equivalent\nto or surpassing the human level, with AI agents considered a promising vehicle\nfor this pursuit. AI agents are artificial entities that sense their\nenvironment, make decisions, and take actions. Many efforts have been made to\ndevelop intelligent agents, but they mainly focus on advancement in algorithms\nor training strategies to enhance specific capabilities or performance on\nparticular tasks. Actually, what the community lacks is a general and powerful\nmodel to serve as a starting point for designing AI agents that can adapt to\ndiverse scenarios. Due to the versatile capabilities they demonstrate, large\nlanguage models (LLMs) are regarded as potential sparks for Artificial General\nIntelligence (AGI), offering hope for building general AI agents. Many\nresearchers have leveraged LLMs as the foundation to build AI agents and have\nachieved significant progress. In this paper, we perform a comprehensive survey\non LLM-based agents. We start by tracing the concept of agents from its\nphilosophical origins to its development in AI, and explain why LLMs are\nsuitable foundations for agents. Building upon this, we present a general\nframework for LLM-based agents, comprising three main components: brain,\nperception, and action, and the framework can be tailored for different\napplications. Subsequently, we explore the extensive applications of LLM-based\nagents in three aspects: single-agent scenarios, multi-agent scenarios, and\nhuman-agent cooperation. Following this, we delve into agent societies,\nexploring the behavior and personality of LLM-based agents, the social\nphenomena that emerge from an agent society, and the insights they offer for\nhuman society. Finally, we discuss several key topics and open problems within\nthe field. A repository for the related papers at\nhttps://github.com/WooooDyy/LLM-Agent-Paper-List.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1\">Zhiheng Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yiwen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_B/0/1/0/all/0/1\">Boyang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junzhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Senjie Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1\">Enyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiaoran Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Limao Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Changhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhangyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shihan Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_R/0/1/0/all/0/1\">Rongxiang Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wensen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_W/0/1/0/all/0/1\">Wenjuan Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yongyan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Autoencoders Find Highly Interpretable Features in Language Models. (arXiv:2309.08600v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2309.08600","description":"<p>One of the roadblocks to a better understanding of neural networks' internals\nis \\textit{polysemanticity}, where neurons appear to activate in multiple,\nsemantically distinct contexts. Polysemanticity prevents us from identifying\nconcise, human-understandable explanations for what neural networks are doing\ninternally. One hypothesised cause of polysemanticity is\n\\textit{superposition}, where neural networks represent more features than they\nhave neurons by assigning features to an overcomplete set of directions in\nactivation space, rather than to individual neurons. Here, we attempt to\nidentify those directions, using sparse autoencoders to reconstruct the\ninternal activations of a language model. These autoencoders learn sets of\nsparsely activating features that are more interpretable and monosemantic than\ndirections identified by alternative approaches, where interpretability is\nmeasured by automated methods. Ablating these features enables precise model\nediting, for example, by removing capabilities such as pronoun prediction,\nwhile disrupting model behaviour less than prior techniques. This work\nindicates that it is possible to resolve superposition in language models using\na scalable, unsupervised method. Our method may serve as a foundation for\nfuture mechanistic interpretability work, which we hope will enable greater\nmodel transparency and steerability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_H/0/1/0/all/0/1\">Hoagy Cunningham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewart_A/0/1/0/all/0/1\">Aidan Ewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riggs_L/0/1/0/all/0/1\">Logan Riggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huben_R/0/1/0/all/0/1\">Robert Huben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharkey_L/0/1/0/all/0/1\">Lee Sharkey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Media of Langue. (arXiv:2309.08609v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.08609","description":"<p>This paper aims to archive the materials behind \"Media of Langue\" by Goki\nMuramoto et al. Media of Langue is a new dictionary and public sculpture that\ndepicts the map of meaning on the boundary between languages solely from the\nvast events of \"this word was translated into that word\" and two forces:\nrepulsion between all words in the same language and attraction between\ntranslated words in different languages. First, the three new concepts\nproposed, Inter-Langue Map/Dictionary, Inter-Langue Space, and then\nInter-Langue Network, are introduced, comparing them to the three domains of\ndictionary, semantic space, and semantic network. The specific algorithms and\ndesigns implemented in the work are then described.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muramoto_G/0/1/0/all/0/1\">Goki Muramoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_A/0/1/0/all/0/1\">Atsuki Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyama_T/0/1/0/all/0/1\">Takayoshi Koyama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild. (arXiv:2309.08637v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.08637","description":"<p>Large language models with instruction-following abilities have\nrevolutionized the field of artificial intelligence. These models show\nexceptional generalizability to tackle various real-world tasks through their\nnatural language interfaces. However, their performance heavily relies on\nhigh-quality exemplar data, which is often difficult to obtain. This challenge\nis further exacerbated when it comes to multimodal instruction following. We\nintroduce TextBind, an almost annotation-free framework for empowering larger\nlanguage models with the multi-turn interleaved multimodal\ninstruction-following capabilities. Our approach requires only image-caption\npairs and generates multi-turn multimodal instruction-response conversations\nfrom a language model. To accommodate interleaved image-text inputs and\noutputs, we devise MIM, a language model-centric architecture that seamlessly\nintegrates image encoder and decoder models. We release our dataset, model, and\ndemo to foster future research in the area of multimodal instruction following.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Taro Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?. (arXiv:2309.08963v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.08963","description":"<p>Despite the power of Large Language Models (LLMs) like GPT-4, they still\nstruggle with tasks that require generating complex, structured outputs. In\nthis study, we assess the capability of Current LLMs in generating complex\nstructured data and propose a structure-aware fine-tuning approach as a\nsolution to improve this ability. To perform a comprehensive evaluation, we\npropose Struc-Bench, include five representative LLMs (i.e., GPT-NeoX 20B,\nGPT-3.5, GPT-4, and Vicuna) and evaluate them on our carefully constructed\ndatasets spanning raw text, HTML, and LaTeX tables. Based on our analysis of\ncurrent model performance, we identify specific common formatting errors and\nareas of potential improvement. To address complex formatting requirements, we\nutilize FormatCoT (Chain-of-Thought) to generate format instructions from\ntarget outputs. Our experiments show that our structure-aware fine-tuning\nmethod, when applied to LLaMA-7B, significantly improves adherence to natural\nlanguage constraints, outperforming other evaluated LLMs. Based on these\nresults, we present an ability map of model capabilities from six dimensions\n(i.e., coverage, formatting, reasoning, comprehension, pragmatics, and\nhallucination). This map highlights the weaknesses of LLMs in handling complex\nstructured outputs and suggests promising directions for future work. Our code\nand models can be found at https://github.com/gersteinlab/Struc-Bench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_Y/0/1/0/all/0/1\">Yiming Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yilun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1\">Mark Gerstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Multilingual Speech Recognition through Language Prompt Tuning and Frame-Level Language Adapter. (arXiv:2309.09443v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2309.09443","description":"<p>Multilingual intelligent assistants, such as ChatGPT, have recently gained\npopularity. To further expand the applications of multilingual artificial\nintelligence assistants and facilitate international communication, it is\nessential to enhance the performance of multilingual speech recognition, which\nis a crucial component of speech interaction. In this paper, we propose two\nsimple and parameter-efficient methods: language prompt tuning and frame-level\nlanguage adapter, to respectively enhance language-configurable and\nlanguage-agnostic multilingual speech recognition. Additionally, we explore the\nfeasibility of integrating these two approaches using parameter-efficient\nfine-tuning methods. Our experiments demonstrate significant performance\nimprovements across seven languages using our proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Song Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+You_Y/0/1/0/all/0/1\">Yongbin You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_K/0/1/0/all/0/1\">Ke Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_G/0/1/0/all/0/1\">Guanglu Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models. (arXiv:2309.09506v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2309.09506","description":"<p>Graphic layout generation, a growing research field, plays a significant role\nin user engagement and information perception. Existing methods primarily treat\nlayout generation as a numerical optimization task, focusing on quantitative\naspects while overlooking the semantic information of layout, such as the\nrelationship between each layout element. In this paper, we propose LayoutNUWA,\nthe first model that treats layout generation as a code generation task to\nenhance semantic information and harness the hidden layout expertise of large\nlanguage models~(LLMs). More concretely, we develop a Code Instruct Tuning\n(CIT) approach comprising three interconnected modules: 1) the Code\nInitialization (CI) module quantifies the numerical conditions and initializes\nthem as HTML code with strategically placed masks; 2) the Code Completion (CC)\nmodule employs the formatting knowledge of LLMs to fill in the masked portions\nwithin the HTML code; 3) the Code Rendering (CR) module transforms the\ncompleted code into the final layout output, ensuring a highly interpretable\nand transparent layout generation procedure that directly maps code to a\nvisualized layout. We attain significant state-of-the-art performance (even\nover 50\\% improvements) on multiple datasets, showcasing the strong\ncapabilities of LayoutNUWA. Our code is available at\nhttps://github.com/ProjectNUWA/LayoutNUWA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zecheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models. (arXiv:2309.09708v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.09708","description":"<p>Automated occupation extraction and standardization from free-text job\npostings and resumes are crucial for applications like job recommendation and\nlabor market policy formation. This paper introduces LLM4Jobs, a novel\nunsupervised methodology that taps into the capabilities of large language\nmodels (LLMs) for occupation coding. LLM4Jobs uniquely harnesses both the\nnatural language understanding and generation capacities of LLMs. Evaluated on\nrigorous experimentation on synthetic and real-world datasets, we demonstrate\nthat LLM4Jobs consistently surpasses unsupervised state-of-the-art benchmarks,\ndemonstrating its versatility across diverse datasets and granularities. As a\nside result of our work, we present both synthetic and real-world datasets,\nwhich may be instrumental for subsequent research in this domain. Overall, this\ninvestigation highlights the promise of contemporary LLMs for the intricate\ntask of occupation extraction and standardization, laying the foundation for a\nrobust and adaptable framework relevant to both research and industrial\ncontexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Bo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_T/0/1/0/all/0/1\">Tijl De Bie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge Distillation. (arXiv:2309.09749v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.09749","description":"<p>NSFW (Not Safe for Work) content, in the context of a dialogue, can have\nsevere side effects on users in open-domain dialogue systems. However, research\non detecting NSFW language, especially sexually explicit content, within a\ndialogue context has significantly lagged behind. To address this issue, we\nintroduce CensorChat, a dialogue monitoring dataset aimed at NSFW dialogue\ndetection. Leveraging knowledge distillation techniques involving GPT-4 and\nChatGPT, this dataset offers a cost-effective means of constructing NSFW\ncontent detectors. The process entails collecting real-life human-machine\ninteraction data and breaking it down into single utterances and single-turn\ndialogues, with the chatbot delivering the final utterance. ChatGPT is employed\nto annotate unlabeled data, serving as a training set. Rationale validation and\ntest sets are constructed using ChatGPT and GPT-4 as annotators, with a\nself-criticism strategy for resolving discrepancies in labeling. A BERT model\nis fine-tuned as a text classifier on pseudo-labeled data, and its performance\nis assessed. The study emphasizes the importance of AI systems prioritizing\nuser safety and well-being in digital conversations while respecting freedom of\nexpression. The proposed approach not only advances NSFW content detection but\nalso aligns with evolving user protection needs in AI-driven dialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Huachuan Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Anqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhenzhong Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watch the Speakers: A Hybrid Continuous Attribution Network for Emotion Recognition in Conversation With Emotion Disentanglement. (arXiv:2309.09799v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.09799","description":"<p>Emotion Recognition in Conversation (ERC) has attracted widespread attention\nin the natural language processing field due to its enormous potential for\npractical applications. Existing ERC methods face challenges in achieving\ngeneralization to diverse scenarios due to insufficient modeling of context,\nambiguous capture of dialogue relationships and overfitting in speaker\nmodeling. In this work, we present a Hybrid Continuous Attributive Network\n(HCAN) to address these issues in the perspective of emotional continuation and\nemotional attribution. Specifically, HCAN adopts a hybrid recurrent and\nattention-based module to model global emotion continuity. Then a novel\nEmotional Attribution Encoding (EAE) is proposed to model intra- and\ninter-emotional attribution for each utterance. Moreover, aiming to enhance the\nrobustness of the model in speaker modeling and improve its performance in\ndifferent scenarios, A comprehensive loss function emotional cognitive loss\n$\\mathcal{L}_{\\rm EC}$ is proposed to alleviate emotional drift and overcome\nthe overfitting of the model to speaker modeling. Our model achieves\nstate-of-the-art performance on three datasets, demonstrating the superiority\nof our work. Another extensive comparative experiments and ablation studies on\nthree benchmarks are conducted to provided evidence to support the efficacy of\neach module. Further exploration of generalization ability experiments shows\nthe plug-and-play nature of the EAE module in our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shanglin Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guanting Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingjian Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HypR: A comprehensive study for ASR hypothesis revising with a reference corpus. (arXiv:2309.09838v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.09838","description":"<p>With the development of deep learning, automatic speech recognition (ASR) has\nmade significant progress. To further enhance the performance, revising\nrecognition results is one of the lightweight but efficient manners. Various\nmethods can be roughly classified into N-best reranking methods and error\ncorrection models. The former aims to select the hypothesis with the lowest\nerror rate from a set of candidates generated by ASR for a given input speech.\nThe latter focuses on detecting recognition errors in a given hypothesis and\ncorrecting these errors to obtain an enhanced result. However, we observe that\nthese studies are hardly comparable to each other as they are usually evaluated\non different corpora, paired with different ASR models, and even use different\ndatasets to train the models. Accordingly, we first concentrate on releasing an\nASR hypothesis revising (HypR) dataset in this study. HypR contains several\ncommonly used corpora (AISHELL-1, TED-LIUM 2, and LibriSpeech) and provides 50\nrecognition hypotheses for each speech utterance. The checkpoint models of the\nASR are also published. In addition, we implement and compare several classic\nand representative methods, showing the recent research progress in revising\nspeech recognition results. We hope the publicly available HypR dataset can\nbecome a reference benchmark for subsequent research and promote the school of\nresearch to an advanced level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi-Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Ke-Han Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuan-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-09-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}