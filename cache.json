{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"CascadeXML: Rethinking Transformers for End-to-end Multi-resolution Training in Extreme Multi-label Classification. (arXiv:2211.00640v1 [cs.LG])","link":"http://arxiv.org/abs/2211.00640","description":"<p>Extreme Multi-label Text Classification (XMC) involves learning a classifier\nthat can assign an input with a subset of most relevant labels from millions of\nlabel choices. Recent approaches, such as XR-Transformer and LightXML, leverage\na transformer instance to achieve state-of-the-art performance. However, in\nthis process, these approaches need to make various trade-offs between\nperformance and computational requirements. A major shortcoming, as compared to\nthe Bi-LSTM based AttentionXML, is that they fail to keep separate feature\nrepresentations for each resolution in a label tree. We thus propose\nCascadeXML, an end-to-end multi-resolution learning pipeline, which can harness\nthe multi-layered architecture of a transformer model for attending to\ndifferent label resolutions with separate feature representations. CascadeXML\nsignificantly outperforms all existing approaches with non-trivial gains\nobtained on benchmark datasets consisting of up to three million labels. Code\nfor CascadeXML will be made publicly available at\n\\url{https://github.com/xmc-aalto/cascadexml}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kharbanda_S/0/1/0/all/0/1\">Siddhant Kharbanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Atmadeep Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schultheis_E/0/1/0/all/0/1\">Erik Schultheis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babbar_R/0/1/0/all/0/1\">Rohit Babbar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Inter-character Relationship-driven Story Generation. (arXiv:2211.00676v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00676","description":"<p>In this paper, we introduce the task of modeling interpersonal relationships\nfor story generation. For addressing this task, we propose Relationships as\nLatent Variables for Story Generation, (ReLiSt). ReLiSt generates stories\nsentence by sentence and has two major components - a relationship selector and\na story continuer. The relationship selector specifies a latent variable to\npick the relationship to exhibit in the next sentence and the story continuer\ngenerates the next sentence while expressing the selected relationship in a\ncoherent way. Our automatic and human evaluations demonstrate that ReLiSt is\nable to generate stories with relationships that are more faithful to desired\nrelationships while maintaining the content quality. The relationship\nassignments to sentences during inference bring interpretability to ReLiSt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vijjini_A/0/1/0/all/0/1\">Anvesh Rao Vijjini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TOE: A Grid-Tagging Discontinuous NER Model Enhanced by Embedding Tag/Word Relations and More Fine-Grained Tags. (arXiv:2211.00684v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00684","description":"<p>So far, discontinuous named entity recognition (NER) has received increasing\nresearch attention and many related methods have surged such as\nhypergraph-based methods, span-based methods, and sequence-to-sequence\n(Seq2Seq) methods, etc. However, these methods more or less suffer from some\nproblems such as decoding ambiguity and efficiency, which limit their\nperformance. Recently, grid-tagging methods, which benefit from the flexible\ndesign of tagging systems and model architectures, have shown superiority to\nadapt for various information extraction tasks. In this paper, we follow the\nline of such methods and propose a competitive grid-tagging model for\ndiscontinuous NER. We call our model TOE because we incorporate two kinds of\nTag-Oriented Enhancement mechanisms into a state-of-the-art (SOTA) grid-tagging\nmodel that casts the NER problem into word-word relationship prediction. First,\nwe design a Tag Representation Embedding Module (TREM) to force our model to\nconsider not only word-word relationships but also word-tag and tag-tag\nrelationships. Concretely, we construct tag representations and embed them into\nTREM, so that TREM can treat tag and word representations as\nqueries/keys/values and utilize self-attention to model their relationships. On\nthe other hand, motivated by the Next-Neighboring-Word (NNW) and Tail-Head-Word\n(THW) tags in the SOTA model, we add two new symmetric tags, namely\nPrevious-Neighboring-Word (PNW) and Head-Tail-Word (HTW), to model more\nfine-grained word-word relationships and alleviate error propagation from tag\nprediction. In the experiments of three benchmark datasets, namely CADEC,\nShARe13 and ShARe14, our TOE model pushes the SOTA results by about 0.83%,\n0.05% and 0.66% in F1, demonstrating its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Dongdong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1\">Chong Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Solve Voxel Building Embodied Tasks from Pixels and Natural Language Instructions. (arXiv:2211.00688v1 [cs.AI])","link":"http://arxiv.org/abs/2211.00688","description":"<p>The adoption of pre-trained language models to generate action plans for\nembodied agents is a promising research strategy. However, execution of\ninstructions in real or simulated environments requires verification of the\nfeasibility of actions as well as their relevance to the completion of a goal.\nWe propose a new method that combines a language model and reinforcement\nlearning for the task of building objects in a Minecraft-like environment\naccording to the natural language instructions. Our method first generates a\nset of consistently achievable sub-goals from the instructions and then\ncompletes associated sub-tasks with a pre-trained RL policy. The proposed\nmethod formed the RL baseline at the IGLU 2022 competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skrynnik_A/0/1/0/all/0/1\">Alexey Skrynnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volovikova_Z/0/1/0/all/0/1\">Zoya Volovikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voronov_A/0/1/0/all/0/1\">Anton Voronov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zholus_A/0/1/0/all/0/1\">Artem Zholus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabzadeh_N/0/1/0/all/0/1\">Negar Arabzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1\">Shrestha Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teruel_M/0/1/0/all/0/1\">Milagro Teruel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1\">Aleksandr Panov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail Burtsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Pivoting Model for Effective Event Detection. (arXiv:2211.00709v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00709","description":"<p>Event Detection, which aims to identify and classify mentions of event\ninstances from unstructured articles, is an important task in Natural Language\nProcessing (NLP). Existing techniques for event detection only use homogeneous\none-hot vectors to represent the event type classes, ignoring the fact that the\nsemantic meaning of the types is important to the task. Such an approach is\ninefficient and prone to overfitting. In this paper, we propose a Semantic\nPivoting Model for Effective Event Detection (SPEED), which explicitly\nincorporates prior information during training and captures semantically\nmeaningful correlations between input and events. Experimental results show\nthat our proposed model achieves state-of-the-art performance and outperforms\nthe baselines in multiple settings without using any external resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_A/0/1/0/all/0/1\">Anran Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_S/0/1/0/all/0/1\">Siu Cheung Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jian Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Natural Language Generation on Near-Term Devices. (arXiv:2211.00727v1 [quant-ph])","link":"http://arxiv.org/abs/2211.00727","description":"<p>The emergence of noisy medium-scale quantum devices has led to\nproof-of-concept applications for quantum computing in various domains.\nExamples include Natural Language Processing (NLP) where sentence\nclassification experiments have been carried out, as well as procedural\ngeneration, where tasks such as geopolitical map creation, and image\nmanipulation have been performed. We explore applications at the intersection\nof these two areas by designing a hybrid quantum-classical algorithm for\nsentence generation.\n</p>\n<p>Our algorithm is based on the well-known simulated annealing technique for\ncombinatorial optimisation. An implementation is provided and used to\ndemonstrate successful sentence generation on both simulated and real quantum\nhardware. A variant of our algorithm can also be used for music generation.\n</p>\n<p>This paper aims to be self-contained, introducing all the necessary\nbackground on NLP and quantum computing along the way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Karamlou_A/0/1/0/all/0/1\">Amin Karamlou</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pfaffhauser_M/0/1/0/all/0/1\">Marcel Pfaffhauser</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wootton_J/0/1/0/all/0/1\">James Wootton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia. (arXiv:2211.00732v1 [cs.IR])","link":"http://arxiv.org/abs/2211.00732","description":"<p>Online encyclopedias, such as Wikipedia, have been well-developed and\nresearched in the last two decades. One can find any attributes or other\ninformation of a wiki item on a wiki page edited by a community of volunteers.\nHowever, the traditional text along with images can hardly express some other\naspects of an item. For example, when we talk about \"Shiba Inu\", one may care\nmore about \"How to feed it\" or \"How to train it to not protect its food\".\nCurrently, short-video platforms have become a hallmark in the online world.\nWhether you're on TikTok, Instagram, Kuaishou, or YouTube Shorts, short-video\napps have changed how we consume and create content today. Except for\nentertainment short videos, we can find more and more authors sharing\ninsightful knowledge widely across all walks of life. These short videos, which\nwe call knowledge videos, can easily express any aspects (E.g. hair or\nhow-to-feed) consumers want to know about an item (E.g. Shiba Inu), and they\ncan be systematically analyzed and organized like an online encyclopedia. In\nthis paper, we propose Kuaipedia, a massive multi-modal encyclopedia consisting\nof items, aspects, and short videos linking to them, which is extracted from\nbillions of videos of Kuaishou, a well-known short-video platform in China. We\nfirst collected items from multiple sources and mined user-centered aspects\nfrom millions of users' queries to build an item-aspect tree. Then we propose a\nnew task called \"multi-modal item-aspect linking\" as an expansion of \"entity\nlinking\" to link short videos into item-aspect pairs and build the whole short\nvideo encyclopedia. Intrinsic evaluations show that our encyclopedia is of\nlarge scale and highly accurate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Haojie Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuzhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Z/0/1/0/all/0/1\">Zepeng Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Ruiji Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality. (arXiv:2211.00768v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00768","description":"<p>Recent visuolinguistic pre-trained models show promising progress on various\nend tasks such as image retrieval and video captioning. Yet, they fail\nmiserably on the recently proposed Winoground dataset, which challenges models\nto match paired images and English captions, with items constructed to overlap\nlexically but differ in meaning (e.g., \"there is a mug in some grass\" vs.\n\"there is some grass in a mug\"). By annotating the dataset using new\nfine-grained tags, we show that solving the Winoground task requires not just\ncompositional language understanding, but a host of other abilities like\ncommonsense reasoning or locating small, out-of-focus objects in low-resolution\nimages. In this paper, we identify the dataset's main challenges through a\nsuite of experiments on related tasks (probing task, image retrieval task),\ndata augmentation, and manual inspection of the dataset. Our analysis suggests\nthat a main challenge in visuolinguistic models may lie in fusing visual and\ntextual representations, rather than in compositional language understanding.\nWe release our annotation and code at\nhttps://github.com/ajd12342/why-winoground-hard .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diwan_A/0/1/0/all/0/1\">Anuj Diwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berry_L/0/1/0/all/0/1\">Layne Berry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified End-to-End Speech Recognition and Endpointing for Fast and Efficient Speech Systems. (arXiv:2211.00786v1 [cs.SD])","link":"http://arxiv.org/abs/2211.00786","description":"<p>Automatic speech recognition (ASR) systems typically rely on an external\nendpointer (EP) model to identify speech boundaries. In this work, we propose a\nmethod to jointly train the ASR and EP tasks in a single end-to-end (E2E)\nmultitask model, improving EP quality by optionally leveraging information from\nthe ASR audio encoder. We introduce a \"switch\" connection, which trains the EP\nto consume either the audio frames directly or low-level latent representations\nfrom the ASR model. This results in a single E2E model that can be used during\ninference to perform frame filtering at low cost, and also make high quality\nend-of-query (EOQ) predictions based on ongoing ASR computation. We present\nresults on a voice search test set showing that, compared to separate\nsingle-task models, this approach reduces median endpoint latency by 120 ms\n(30.8% reduction), and 90th percentile latency by 170 ms (23.0% reduction),\nwithout regressing word error rate. For continuous recognition, WER improves by\n10.6% (relative).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bijwadia_S/0/1/0/all/0/1\">Shaan Bijwadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuo-yiin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BECTRA: Transducer-based End-to-End ASR with BERT-Enhanced Encoder. (arXiv:2211.00792v1 [eess.AS])","link":"http://arxiv.org/abs/2211.00792","description":"<p>We present BERT-CTC-Transducer (BECTRA), a novel end-to-end automatic speech\nrecognition (E2E-ASR) model formulated by the transducer with a BERT-enhanced\nencoder. Integrating a large-scale pre-trained language model (LM) into E2E-ASR\nhas been actively studied, aiming to utilize versatile linguistic knowledge for\ngenerating accurate text. One crucial factor that makes this integration\nchallenging lies in the vocabulary mismatch; the vocabulary constructed for a\npre-trained LM is generally too large for E2E-ASR training and is likely to\nhave a mismatch against a target ASR domain. To overcome such an issue, we\npropose BECTRA, an extended version of our previous BERT-CTC, that realizes\nBERT-based E2E-ASR using a vocabulary of interest. BECTRA is a transducer-based\nmodel, which adopts BERT-CTC for its encoder and trains an ASR-specific decoder\nusing a vocabulary suitable for a target task. With the combination of the\ntransducer and BERT-CTC, we also propose a novel inference algorithm for taking\nadvantage of both autoregressive and non-autoregressive decoding. Experimental\nresults on several ASR tasks, varying in amounts of data, speaking styles, and\nlanguages, demonstrate that BECTRA outperforms BERT-CTC by effectively dealing\nwith the vocabulary mismatch while exploiting BERT knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_Y/0/1/0/all/0/1\">Yosuke Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ogawa_T/0/1/0/all/0/1\">Tetsuji Ogawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kobayashi_T/0/1/0/all/0/1\">Tetsunori Kobayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InterMPL: Momentum Pseudo-Labeling with Intermediate CTC Loss. (arXiv:2211.00795v1 [eess.AS])","link":"http://arxiv.org/abs/2211.00795","description":"<p>This paper presents InterMPL, a semi-supervised learning method of end-to-end\nautomatic speech recognition (ASR) that performs pseudo-labeling (PL) with\nintermediate supervision. Momentum PL (MPL) trains a connectionist temporal\nclassification (CTC)-based model on unlabeled data by continuously generating\npseudo-labels on the fly and improving their quality. In contrast to\nautoregressive formulations, such as the attention-based encoder-decoder and\ntransducer, CTC is well suited for MPL, or PL-based semi-supervised ASR in\ngeneral, owing to its simple/fast inference algorithm and robustness against\ngenerating collapsed labels. However, CTC generally yields inferior performance\nthan the autoregressive models due to the conditional independence assumption,\nthereby limiting the performance of MPL. We propose to enhance MPL by\nintroducing intermediate loss, inspired by the recent advances in CTC-based\nmodeling. Specifically, we focus on self-conditional and hierarchical\nconditional CTC, that apply auxiliary CTC losses to intermediate layers such\nthat the conditional independence assumption is explicitly relaxed. We also\nexplore how pseudo-labels should be generated and used as supervision for\nintermediate losses. Experimental results in different semi-supervised settings\ndemonstrate that the proposed approach outperforms MPL and improves an ASR\nmodel by up to a 12.1% absolute performance gain. In addition, our detailed\nanalysis validates the importance of the intermediate loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_Y/0/1/0/all/0/1\">Yosuke Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ogawa_T/0/1/0/all/0/1\">Tetsuji Ogawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kobayashi_T/0/1/0/all/0/1\">Tetsunori Kobayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Title2Event: Benchmarking Open Event Extraction with a Large-scale Chinese Title Dataset. (arXiv:2211.00869v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00869","description":"<p>Event extraction (EE) is crucial to downstream tasks such as new aggregation\nand event knowledge graph construction. Most existing EE datasets manually\ndefine fixed event types and design specific schema for each of them, failing\nto cover diverse events emerging from the online text. Moreover, news titles,\nan important source of event mentions, have not gained enough attention in\ncurrent EE research. In this paper, We present Title2Event, a large-scale\nsentence-level dataset benchmarking Open Event Extraction without restricting\nevent types. Title2Event contains more than 42,000 news titles in 34 topics\ncollected from Chinese web pages. To the best of our knowledge, it is currently\nthe largest manually-annotated Chinese dataset for open event extraction. We\nfurther conduct experiments on Title2Event with different models and show that\nthe characteristics of titles make it challenging for event extraction,\naddressing the significance of advanced study on this problem. The dataset and\nbaseline codes are available at https://open-event-hub.github.io/title2event.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Haolin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yangfan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_W/0/1/0/all/0/1\">Wangyang Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiaoling Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianhua Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Syntactically Controlled Paraphrase Generation with Abstract Meaning Representations. (arXiv:2211.00881v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00881","description":"<p>Syntactically controlled paraphrase generation has become an emerging\nresearch direction in recent years. Most existing approaches require annotated\nparaphrase pairs for training and are thus costly to extend to new domains.\nUnsupervised approaches, on the other hand, do not need paraphrase pairs but\nsuffer from relatively poor performance in terms of syntactic control and\nquality of generated paraphrases. In this paper, we demonstrate that leveraging\nAbstract Meaning Representations (AMR) can greatly improve the performance of\nunsupervised syntactically controlled paraphrase generation. Our proposed\nmodel, AMR-enhanced Paraphrase Generator (AMRPG), separately encodes the AMR\ngraph and the constituency parse of the input sentence into two disentangled\nsemantic and syntactic embeddings. A decoder is then learned to reconstruct the\ninput sentence from the semantic and syntactic embeddings. Our experiments show\nthat AMRPG generates more accurate syntactically controlled paraphrases, both\nquantitatively and qualitatively, compared to the existing unsupervised\napproaches. We also demonstrate that the paraphrases generated by AMRPG can be\nused for data augmentation to improve the robustness of NLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_V/0/1/0/all/0/1\">Varun Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anoop Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatapathy_S/0/1/0/all/0/1\">Sriram Venkatapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIMD-size aware weight regularization for fast neural vocoding on CPU. (arXiv:2211.00898v1 [cs.SD])","link":"http://arxiv.org/abs/2211.00898","description":"<p>This paper proposes weight regularization for a faster neural vocoder.\nPruning time-consuming DNN modules is a promising way to realize a real-time\nvocoder on a CPU (e.g. WaveRNN, LPCNet). Regularization that encourages\nsparsity is also effective in avoiding the quality degradation created by\npruning. However, the orders of weight matrices must be contiguous in SIMD size\nfor fast vocoding. To ensure this order, we propose explicit SIMD size aware\nregularization. Our proposed method reshapes a weight matrix into a tensor so\nthat the weights are aligned by group size in advance, and then computes the\ngroup Lasso-like regularization loss. Experiments on 70% sparse subband WaveRNN\nshow that pruning in conventional Lasso and column-wise group Lasso degrades\nthe synthetic speech's naturalness. The vocoder with proposed regularization 1)\nachieves comparable naturalness to that without pruning and 2) performs\nmeaningfully faster than other conventional vocoders using regularization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanagawa_H/0/1/0/all/0/1\">Hiroki Kanagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ijima_Y/0/1/0/all/0/1\">Yusuke Ijima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLATO-K: Internal and External Knowledge Enhanced Dialogue Generation. (arXiv:2211.00910v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00910","description":"<p>Recently, the practical deployment of open-domain dialogue systems has been\nplagued by the knowledge issue of information deficiency and factual\ninaccuracy. To this end, we introduce PLATO-K based on two-stage dialogic\nlearning to strengthen internal knowledge memorization and external knowledge\nexploitation. In the first stage, PLATO-K learns through massive dialogue\ncorpora and memorizes essential knowledge into model parameters. In the second\nstage, PLATO-K mimics human beings to search for external information and to\nleverage the knowledge in response generation. Extensive experiments reveal\nthat the knowledge issue is alleviated significantly in PLATO-K with such\ncomprehensive internal and external knowledge enhancement. Compared to the\nexisting state-of-the-art Chinese dialogue model, the overall engagingness of\nPLATO-K is improved remarkably by 36.2% and 49.2% on chit-chat and\nknowledge-intensive conversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Siqi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Han Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenquan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zheng-Yu Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Passage-Mask: A Learnable Regularization Strategy for Retriever-Reader Models. (arXiv:2211.00915v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00915","description":"<p>Retriever-reader models achieve competitive performance across many different\nNLP tasks such as open question answering and dialogue conversations. In this\nwork, we notice these models easily overfit the top-rank retrieval passages and\nstandard training fails to reason over the entire retrieval passages. We\nintroduce a learnable passage mask mechanism which desensitizes the impact from\nthe top-rank retrieval passages and prevents the model from overfitting.\nControlling the gradient variance with fewer mask candidates and selecting the\nmask candidates with one-shot bi-level optimization, our learnable\nregularization strategy enforces the answer generation to focus on the entire\nretrieval passages. Experiments on different tasks across open question\nanswering, dialogue conversation, and fact verification show that our method\nconsistently outperforms its baselines. Extensive experiments and ablation\nstudies demonstrate that our method can be general, effective, and beneficial\nfor many NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shujian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chengyue Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingchao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialect-robust Evaluation of Generated Text. (arXiv:2211.00922v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00922","description":"<p>Evaluation metrics that are not robust to dialect variation make it\nimpossible to tell how well systems perform for many groups of users, and can\neven penalize systems for producing text in lower-resource dialects. However,\ncurrently, there exists no way to quantify how metrics respond to change in the\ndialect of a generated utterance. We thus formalize dialect robustness and\ndialect awareness as goals for NLG evaluation metrics. We introduce a suite of\nmethods and corresponding statistical tests one can use to assess metrics in\nlight of the two goals. Applying the suite to current state-of-the-art metrics,\nwe demonstrate that they are not dialect-robust and that semantic perturbations\nfrequently lead to smaller decreases in a metric than the introduction of\ndialect features. As a first step to overcome this limitation, we propose a\ntraining schema, NANO, which introduces regional and language information to\nthe pretraining process of a metric. We demonstrate that NANO provides a\nsize-efficient way for models to improve the dialect robustness while\nsimultaneously improving their performance on the standard metric benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellam_T/0/1/0/all/0/1\">Thibault Sellam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_E/0/1/0/all/0/1\">Elizabeth Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozat_T/0/1/0/all/0/1\">Timothy Dozat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrette_D/0/1/0/all/0/1\">Dan Garrette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1\">Aditya Siddhant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation. (arXiv:2211.00923v1 [cs.SD])","link":"http://arxiv.org/abs/2211.00923","description":"<p>One of the biggest challenges in designing mispronunciation detection models\nis the unavailability of labeled L2 speech data. To overcome such data\nscarcity, we introduce SpeechBlender -- a fine-grained data augmentation\npipeline for generating mispronunciation errors. The SpeechBlender utilizes\nvarieties of masks to target different regions of a phonetic unit, and use the\nmixing factors to linearly interpolate raw speech signals while generating\nerroneous pronunciation instances. The masks facilitate smooth blending of the\nsignals, thus generating more effective samples than the `Cut/Paste' method. We\nshow the effectiveness of our augmentation technique in a phoneme-level\npronunciation quality assessment task, leveraging only a good pronunciation\ndataset. With SpeechBlender augmentation, we observed a 3% and 2% increase in\nPearson correlation coefficient (PCC) compared to no-augmentation and goodness\nof pronunciation augmentation scenarios respectively for Speechocean762\ntestset. Moreover, a 2% rise in PCC is observed when comparing our single-task\nphoneme-level mispronunciation detection model with a multi-task learning model\nusing multiple-granularity information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kheir_Y/0/1/0/all/0/1\">Yassine El Kheir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afzal_S/0/1/0/all/0/1\">Shazia Afzal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Internal Language Model Estimation based Adaptive Language Model Fusion for Domain Adaptation. (arXiv:2211.00968v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00968","description":"<p>ASR model deployment environment is ever-changing, and the incoming speech\ncan be switched across different domains during a session. This brings a\nchallenge for effective domain adaptation when only target domain text data is\navailable, and our objective is to obtain obviously improved performance on the\ntarget domain while the performance on the general domain is less undermined.\nIn this paper, we propose an adaptive LM fusion approach called internal\nlanguage model estimation based adaptive domain adaptation (ILME-ADA). To\nrealize such an ILME-ADA, an interpolated log-likelihood score is calculated\nbased on the maximum of the scores from the internal LM and the external LM\n(ELM) respectively. We demonstrate the efficacy of the proposed ILME-ADA method\nwith both RNN-T and LAS modeling frameworks employing neural network and n-gram\nLMs as ELMs respectively on two domain specific (target) test sets. The\nproposed method can achieve significantly better performance on the target test\nsets while it gets minimal performance degradation on the general test set,\ncompared with both shallow and ILME-based LM fusion methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaobo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yanan Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haihua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Processing Long Legal Documents with Pre-trained Transformers: Modding LegalBERT and Longformer. (arXiv:2211.00974v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00974","description":"<p>Pre-trained Transformers currently dominate most NLP tasks. They impose,\nhowever, limits on the maximum input length (512 sub-words in BERT), which are\ntoo restrictive in the legal domain. Even sparse-attention models, such as\nLongformer and BigBird, which increase the maximum input length to 4,096\nsub-words, severely truncate texts in three of the six datasets of LexGLUE.\nSimpler linear classifiers with TF-IDF features can handle texts of any length,\nrequire far less resources to train and deploy, but are usually outperformed by\npre-trained Transformers. We explore two directions to cope with long legal\ntexts: (i) modifying a Longformer warm-started from LegalBERT to handle even\nlonger texts (up to 8,192 sub-words), and (ii) modifying LegalBERT to use\nTF-IDF representations. The first approach is the best in terms of performance,\nsurpassing a hierarchical version of LegalBERT, which was the previous state of\nthe art in LexGLUE. The second approach leads to computationally more efficient\nmodels at the expense of lower performance, but the resulting models still\noutperform overall a linear SVM with TF-IDF features in long legal document\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mamakas_D/0/1/0/all/0/1\">Dimitris Mamakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsotsi_P/0/1/0/all/0/1\">Petros Tsotsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monolingual Recognizers Fusion for Code-switching Speech Recognition. (arXiv:2211.01046v1 [eess.AS])","link":"http://arxiv.org/abs/2211.01046","description":"<p>The bi-encoder structure has been intensively investigated in code-switching\n(CS) automatic speech recognition (ASR). However, most existing methods require\nthe structures of two monolingual ASR models (MAMs) should be the same and only\nuse the encoder of MAMs. This leads to the problem that pre-trained MAMs cannot\nbe timely and fully used for CS ASR. In this paper, we propose a monolingual\nrecognizers fusion method for CS ASR. It has two stages: the speech awareness\n(SA) stage and the language fusion (LF) stage. In the SA stage, acoustic\nfeatures are mapped to two language-specific predictions by two independent\nMAMs. To keep the MAMs focused on their own language, we further extend the\nlanguage-aware training strategy for the MAMs. In the LF stage, the BELM fuses\ntwo language-specific predictions to get the final prediction. Moreover, we\npropose a text simulation strategy to simplify the training process of the BELM\nand reduce reliance on CS data. Experiments on a Mandarin-English corpus show\nthe efficiency of the proposed method. The mix error rate is significantly\nreduced on the test set after using open-source pre-trained MAMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Song_T/0/1/0/all/0/1\">Tongtong Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Q/0/1/0/all/0/1\">Qiang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_H/0/1/0/all/0/1\">Haoyu Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Longbiao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_H/0/1/0/all/0/1\">Hao Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Yuqin Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yanbing Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dang_J/0/1/0/all/0/1\">Jianwu Dang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient Knowledge Distillation for Pre-trained Language Models. (arXiv:2211.01071v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01071","description":"<p>Knowledge distillation (KD) is an effective framework to transfer knowledge\nfrom a large-scale teacher to a compact yet well-performing student. Previous\nKD practices for pre-trained language models mainly transfer knowledge by\naligning instance-wise outputs between the teacher and student, while\nneglecting an important knowledge source, i.e., the gradient of the teacher.\nThe gradient characterizes how the teacher responds to changes in inputs, which\nwe assume is beneficial for the student to better approximate the underlying\nmapping function of the teacher. Therefore, we propose Gradient Knowledge\nDistillation (GKD) to incorporate the gradient alignment objective into the\ndistillation process. Experimental results show that GKD outperforms previous\nKD methods regarding student performance. Further analysis shows that\nincorporating gradient knowledge makes the student behave more consistently\nwith the teacher, improving the interpretability greatly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lean Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based encoder-encoder architecture for Spoken Term Detection. (arXiv:2211.01089v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01089","description":"<p>The paper presents a method for spoken term detection based on the\nTransformer architecture. We propose the encoder-encoder architecture employing\ntwo BERT-like encoders with additional modifications, including convolutional\nand upsampling layers, attention masking, and shared parameters. The encoders\nproject a recognized hypothesis and a searched term into a shared embedding\nspace, where the score of the putative hit is computed using the calibrated dot\nproduct. In the experiments, we used the Wav2Vec 2.0 speech recognizer, and the\nproposed system outperformed a baseline method based on deep LSTMs on the\nEnglish and Czech STD datasets based on USC Shoah Foundation Visual History\nArchive (MALACH).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Svec_J/0/1/0/all/0/1\">Jan &#x160;vec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smidl_L/0/1/0/all/0/1\">Lubo&#x161; &#x160;m&#xed;dl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehecka_J/0/1/0/all/0/1\">Jan Lehe&#x10d;ka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User-Entity Differential Privacy in Learning Natural Language Models. (arXiv:2211.01141v1 [cs.CR])","link":"http://arxiv.org/abs/2211.01141","description":"<p>In this paper, we introduce a novel concept of user-entity differential\nprivacy (UeDP) to provide formal privacy protection simultaneously to both\nsensitive entities in textual data and data owners in learning natural language\nmodels (NLMs). To preserve UeDP, we developed a novel algorithm, called\nUeDP-Alg, optimizing the trade-off between privacy loss and model utility with\na tight sensitivity bound derived from seamlessly combining user and sensitive\nentity sampling processes. An extensive theoretical analysis and evaluation\nshow that our UeDP-Alg outperforms baseline approaches in model utility under\nthe same privacy budget consumption on several NLM tasks, using benchmark\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_P/0/1/0/all/0/1\">Phung Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_N/0/1/0/all/0/1\">NhatHai Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rajiv Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barmpalios_N/0/1/0/all/0/1\">Nikolaos Barmpalios</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for Multilingual Speech to Image Retrieval. (arXiv:2211.01180v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01180","description":"<p>This work investigates the use of large-scale, pre-trained models (CLIP and\nHuBERT) for multilingual speech-image retrieval. For non-English speech-image\nretrieval, we outperform the current state-of-the-art performance by a wide\nmargin when training separate models for each language, and show that a single\nmodel which processes speech in all three languages still achieves retrieval\nscores comparable with the prior state-of-the-art. We identify key differences\nin model behavior and performance between English and non-English settings,\npresumably attributable to the English-only pre-training of CLIP and HuBERT.\nFinally, we show that our models can be used for mono- and cross-lingual\nspeech-text retrieval and cross-lingual speech-speech retrieval, despite never\nhaving seen any parallel speech-text or speech-speech data during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berry_L/0/1/0/all/0/1\">Layne Berry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_Y/0/1/0/all/0/1\">Yi-Jen Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsuan-Fu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-level Distillation of Semantic Knowledge for Pre-training Multilingual Language Model. (arXiv:2211.01200v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01200","description":"<p>Pre-trained multilingual language models play an important role in\ncross-lingual natural language understanding tasks. However, existing methods\ndid not focus on learning the semantic structure of representation, and thus\ncould not optimize their performance. In this paper, we propose Multi-level\nMultilingual Knowledge Distillation (MMKD), a novel method for improving\nmultilingual language models. Specifically, we employ a teacher-student\nframework to adopt rich semantic representation knowledge in English BERT. We\npropose token-, word-, sentence-, and structure-level alignment objectives to\nencourage multiple levels of consistency between source-target pairs and\ncorrelation similarity between teacher and student models. We conduct\nexperiments on cross-lingual evaluation benchmarks including XNLI, PAWS-X, and\nXQuAD. Experimental results show that MMKD outperforms other baseline models of\nsimilar size on XNLI and XQuAD and obtains comparable performance on PAWS-X.\nEspecially, MMKD obtains significant performance gains on low-resource\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1\">Fei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Long Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hongxin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Feng Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"data2vec-aqc: Search for the right Teaching Assistant in the Teacher-Student training setup. (arXiv:2211.01246v1 [eess.AS])","link":"http://arxiv.org/abs/2211.01246","description":"<p>In this paper, we propose a new Self-Supervised Learning (SSL) algorithm\ncalled data2vec-aqc, for speech representation learning from unlabeled speech\ndata. Our goal is to improve SSL for speech in domains where both unlabeled and\nlabeled data are limited. Building on the recently introduced data2vec, we\nintroduce additional modules to the data2vec framework that leverage the\nbenefit of data augmentations, quantized representations, and clustering. The\ninteraction between these modules helps solve the cross-contrastive loss as an\nadditional self-supervised objective. data2vec-aqc achieves up to 14.1% and\n20.9% relative WER improvement over the existing state-of-the-art data2vec\nsystem on the test-clean and test-other sets, respectively, of LibriSpeech,\nwithout the use of any language model. Our proposed model also achieves up to\n17.8% relative WER improvement over the baseline data2vec when fine-tuned on\nSwitchboard data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lodagala_V/0/1/0/all/0/1\">Vasista Sai Lodagala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Vector Retrieval as Sparse Alignment. (arXiv:2211.01267v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01267","description":"<p>Multi-vector retrieval models improve over single-vector dual encoders on\nmany information retrieval tasks. In this paper, we cast the multi-vector\nretrieval problem as sparse alignment between query and document tokens. We\npropose AligneR, a novel multi-vector retrieval model that learns sparsified\npairwise alignments between query and document tokens (e.g. `dog' vs. `puppy')\nand per-token unary saliences reflecting their relative importance for\nretrieval. We show that controlling the sparsity of pairwise token alignments\noften brings significant performance gains. While most factoid questions\nfocusing on a specific part of a document require a smaller number of\nalignments, others requiring a broader understanding of a document favor a\nlarger number of alignments. Unary saliences, on the other hand, decide whether\na token ever needs to be aligned with others for retrieval (e.g. `kind' from\n`kind of currency is used in new zealand}'). With sparsified unary saliences,\nwe are able to prune a large number of query and document token vectors and\nimprove the efficiency of multi-vector retrieval. We learn the sparse unary\nsaliences with entropy-regularized linear programming, which outperforms other\nmethods to achieve sparsity. In a zero-shot setting, AligneR scores 51.1 points\nnDCG@10, achieving a new retriever-only state-of-the-art on 13 tasks in the\nBEIR benchmark. In addition, adapting pairwise alignments with a few examples\n(&lt;= 8) further improves the performance up to 15.7 points nDCG@10 for argument\nretrieval tasks. The unary saliences of AligneR helps us to keep only 20% of\nthe document token representations with minimal performance loss. We further\nshow that our model often produces interpretable alignments and significantly\nimproves its performance when initialized from larger language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yujie Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duddu_S/0/1/0/all/0/1\">Sai Meher Karthik Duddu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhuyun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1\">Siddhartha Brahma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naim_I/0/1/0/all/0/1\">Iftekhar Naim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Y. Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Impact of Social Media Posts by Executives on Stock Prices. (arXiv:2211.01287v1 [q-fin.ST])","link":"http://arxiv.org/abs/2211.01287","description":"<p>Predicting stock market movements has always been of great interest to\ninvestors and an active area of research. Research has proven that popularity\nof products is highly influenced by what people talk about. Social media like\nTwitter, Reddit have become hotspots of such influences. This paper\ninvestigates the impact of social media posts on close price prediction of\nstocks using Twitter and Reddit posts. Our objective is to integrate sentiment\nof social media data with historical stock data and study its effect on closing\nprices using time series models. We carried out rigorous experiments and deep\nanalysis using multiple deep learning based models on different datasets to\nstudy the influence of posts by executives and general people on the close\nprice. Experimental results on multiple stocks (Apple and Tesla) and\ndecentralised currencies (Bitcoin and Ethereum) consistently show improvements\nin prediction on including social media data and greater improvements on\nincluding executive posts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Sarkar_A/0/1/0/all/0/1\">Anubhav Sarkar</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Chakraborty_S/0/1/0/all/0/1\">Swagata Chakraborty</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Ghosh_S/0/1/0/all/0/1\">Sohom Ghosh</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Naskar_S/0/1/0/all/0/1\">Sudip Kumar Naskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Intrinsic Compositionality In Transformers With Tree Projections. (arXiv:2211.01288v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01288","description":"<p>When trained on language data, do transformers learn some arbitrary\ncomputation that utilizes the full capacity of the architecture or do they\nlearn a simpler, tree-like computation, hypothesized to underlie compositional\nmeaning systems like human languages? There is an apparent tension between\ncompositional accounts of human language understanding, which are based on a\nrestricted bottom-up computational process, and the enormous success of neural\nmodels like transformers, which can route information arbitrarily between\ndifferent parts of their input. One possibility is that these models, while\nextremely flexible in principle, in practice learn to interpret language\nhierarchically, ultimately building sentence representations close to those\npredictable by a bottom-up, tree-structured model. To evaluate this\npossibility, we describe an unsupervised and parameter-free method to\n\\emph{functionally project} the behavior of any transformer into the space of\ntree-structured networks. Given an input sentence, we produce a binary tree\nthat approximates the transformer's representation-building process and a score\nthat captures how \"tree-like\" the transformer's behavior is on the input. While\ncalculation of this score does not require training any additional models, it\nprovably upper-bounds the fit between a transformer and any tree-structured\napproximation. Using this method, we show that transformers for three different\ntasks become more tree-like over the course of training, in some cases\nunsupervisedly recovering the same trees as supervised parsers. These trees, in\nturn, are predictive of model behavior, with more tree-like models generalizing\nbetter on tests of compositional generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murty_S/0/1/0/all/0/1\">Shikhar Murty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pratyusha Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting word frequencies in authorship attribution. (arXiv:2211.01289v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01289","description":"<p>In this paper, I introduce a simple method of computing relative word\nfrequencies for authorship attribution and similar stylometric tasks. Rather\nthan computing relative frequencies as the number of occurrences of a given\nword divided by the total number of tokens in a text, I argue that a more\nefficient normalization factor is the total number of relevant tokens only. The\nnotion of relevant words includes synonyms and, usually, a few dozen other\nwords in some ways semantically similar to a word in question. To determine\nsuch a semantic background, one of word embedding models can be used. The\nproposed method outperforms classical most-frequent-word approaches\nsubstantially, usually by a few percentage points depending on the input\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eder_M/0/1/0/all/0/1\">Maciej Eder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Algebraic Framework for Stock & Flow Diagrams and Dynamical Systems Using Category Theory. (arXiv:2211.01290v1 [cs.LO])","link":"http://arxiv.org/abs/2211.01290","description":"<p>Mathematical modeling of infectious disease at scale is important, but\nchallenging. Some of these difficulties can be alleviated by an approach that\ntakes diagrams seriously as mathematical formalisms in their own right. Stock &amp;\nflow diagrams are widely used as broadly accessible building blocks for\ninfectious disease modeling. In this chapter, rather than focusing on the\nunderlying mathematics, we informally use communicable disease examples created\nby the implemented software of StockFlow.jl to explain the basics,\ncharacteristics, and benefits of the categorical framework. We first\ncharacterize categorical stock &amp; flow diagrams, and note the clear separation\nbetween the syntax of stock &amp; flow diagrams and their semantics, demonstrating\nthree examples of semantics already implemented in the software: ODEs, causal\nloop diagrams, and system structure diagrams. We then establish composition and\nstratification frameworks and examples for stock &amp; flow diagrams. Applying\ncategory theory, these frameworks can build large diagrams from smaller ones in\na modular fashion. Finally, we introduce the open-source ModelCollab software\nfor diagram-centric real-time collaborative modeling. Using the graphical user\ninterface, this web-based software allows the user to undertake the types of\ncategorically-rooted operations discussed above, but without any knowledge of\ntheir categorical foundations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baez_J/0/1/0/all/0/1\">John Baez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Libkind_S/0/1/0/all/0/1\">Sophie Libkind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redekopp_E/0/1/0/all/0/1\">Eric Redekopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_L/0/1/0/all/0/1\">Long Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osgood_N/0/1/0/all/0/1\">Nathaniel D Osgood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning an Artificial Language for Knowledge-Sharing in Multilingual Translation. (arXiv:2211.01292v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01292","description":"<p>The cornerstone of multilingual neural translation is shared representations\nacross languages. Given the theoretically infinite representation power of\nneural networks, semantically identical sentences are likely represented\ndifferently. While representing sentences in the continuous latent space\nensures expressiveness, it introduces the risk of capturing of irrelevant\nfeatures which hinders the learning of a common representation. In this work,\nwe discretize the encoder output latent space of multilingual models by\nassigning encoder states to entries in a codebook, which in effect represents\nsource sentences in a new artificial language. This discretization process not\nonly offers a new way to interpret the otherwise black-box model\nrepresentations, but, more importantly, gives potential for increasing\nrobustness in unseen testing conditions. We validate our approach on\nlarge-scale experiments with realistic data volumes and domains. When tested in\nzero-shot conditions, our approach is competitive with two strong alternatives\nfrom the literature. We also use the learned artificial language to analyze\nmodel behavior, and discover that using a similar bridge language increases\nknowledge-sharing among the remaining languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards End-to-end Speaker Diarization in the Wild. (arXiv:2211.01299v1 [eess.AS])","link":"http://arxiv.org/abs/2211.01299","description":"<p>Speaker diarization algorithms address the \"who spoke when\" problem in audio\nrecordings. Algorithms trained end-to-end have proven superior to classical\nmodular-cascaded systems in constrained scenarios with a small number of\nspeakers. However, their performance for in-the-wild recordings containing more\nspeakers with shorter utterance lengths remains to be investigated. In this\npaper, we address this gap, showing that an attractor-based end-to-end system\ncan also perform remarkably well in the latter scenario when first pre-trained\non a carefully-designed simulated dataset that matches the distribution of\nin-the-wild recordings. We also propose to use an attention mechanism to\nincrease the network capacity in decoding more speaker attractors, and to\njointly train the attractors on a speaker recognition task to improve the\nspeaker attractor representation. Even though the model we propose is\naudio-only, we find it significantly outperforms both audio-only and\naudio-visual baselines on the AVA-AVD benchmark dataset, achieving\nstate-of-the-art results with an absolute reduction in diarization error of\n23.3%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_Z/0/1/0/all/0/1\">Zexu Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wichern_G/0/1/0/all/0/1\">Gordon Wichern</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Germain_F/0/1/0/all/0/1\">Fran&#xe7;ois G. Germain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Subramanian_A/0/1/0/all/0/1\">Aswin Subramanian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting phoneme-level prosody latents using AR and flow-based Prior Networks for expressive speech synthesis. (arXiv:2211.01327v1 [cs.SD])","link":"http://arxiv.org/abs/2211.01327","description":"<p>A large part of the expressive speech synthesis literature focuses on\nlearning prosodic representations of the speech signal which are then modeled\nby a prior distribution during inference. In this paper, we compare different\nprior architectures at the task of predicting phoneme level prosodic\nrepresentations extracted with an unsupervised FVAE model. We use both\nsubjective and objective metrics to show that normalizing flow based prior\nnetworks can result in more expressive speech at the cost of a slight drop in\nquality. Furthermore, we show that the synthesized speech has higher\nvariability, for a given text, due to the nature of normalizing flows. We also\npropose a Dynamical VAE model, that can generate higher quality speech although\nwith decreased expressiveness and variability compared to the flow based\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klapsas_K/0/1/0/all/0/1\">Konstantinos Klapsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikitaras_K/0/1/0/all/0/1\">Karolos Nikitaras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellinas_N/0/1/0/all/0/1\">Nikolaos Ellinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_J/0/1/0/all/0/1\">June Sig Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_I/0/1/0/all/0/1\">Inchul Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raptis_S/0/1/0/all/0/1\">Spyros Raptis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalamandaris_A/0/1/0/all/0/1\">Aimilios Chalamandaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiakoulis_P/0/1/0/all/0/1\">Pirros Tsiakoulis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese. (arXiv:2211.01335v1 [cs.CV])","link":"http://arxiv.org/abs/2211.01335","description":"<p>The tremendous success of CLIP (Radford et al., 2021) has promoted the\nresearch and application of contrastive learning for vision-language\npretraining. However, while the publicly available CLIP models are mostly\npretrained on English data, it is hard to search for a CLIP pretrained on\nChinese data. We assume that pretraining a Chinese CLIP is essential to\nresearch and industry for the following reasons. First, it can benefit the\nvision-language retrieval in Chinese and thus promote the language-specific\nmultimodal representation learning. Second, the distribution of images in\nChinese websites should be different from that of images in English websites.\nIn this work, we construct a large-scale dataset of image-text pairs in\nChinese, where most data are retrieved from publicly available datasets, and we\npretrain Chinese CLIP models on the new dataset. We develop 5 Chinese CLIP\nmodels of multiple sizes, spanning from 77 to 958 million parameters.\nFurthermore, we propose a two-stage pretraining method, where the model is\nfirst trained with the image encoder frozen and then trained with all\nparameters being optimized, to achieve enhanced model performance. Our\ncomprehensive experiments demonstrate that Chinese CLIP can achieve the\nstate-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups\nof zero-shot learning and finetuning, and it is able to achieve competitive\nperformance in zero-shot image classification based on the evaluation on the\nELEVATER benchmark (Li et al., 2022). Furthermore, through the ablation study\nwe show that the two-stage pretraining method is the most effective compared\nwith the other options. We release our code in\nhttps://github.com/OFA-Sys/Chinese-CLIP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junshu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technology Pipeline for Large Scale Cross-Lingual Dubbing of Lecture Videos into Multiple Indian Languages. (arXiv:2211.01338v1 [eess.AS])","link":"http://arxiv.org/abs/2211.01338","description":"<p>Cross-lingual dubbing of lecture videos requires the transcription of the\noriginal audio, correction and removal of disfluencies, domain term discovery,\ntext-to-text translation into the target language, chunking of text using\ntarget language rhythm, text-to-speech synthesis followed by isochronous\nlipsyncing to the original video. This task becomes challenging when the source\nand target languages belong to different language families, resulting in\ndifferences in generated audio duration. This is further compounded by the\noriginal speaker's rhythm, especially for extempore speech. This paper\ndescribes the challenges in regenerating English lecture videos in Indian\nlanguages semi-automatically. A prototype is developed for dubbing lectures\ninto 9 Indian languages. A mean-opinion-score (MOS) is obtained for two\nlanguages, Hindi and Tamil, on two different courses. The output video is\ncompared with the original video in terms of MOS (1-5) and lip synchronisation\nwith scores of 4.09 and 3.74, respectively. The human effort also reduces by\n75%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Prakash_A/0/1/0/all/0/1\">Anusha Prakash</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Arun Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seth_A/0/1/0/all/0/1\">Ashish Seth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukherjee_B/0/1/0/all/0/1\">Bhagyashree Mukherjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_I/0/1/0/all/0/1\">Ishika Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuriakose_J/0/1/0/all/0/1\">Jom Kuriakose</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fernandes_J/0/1/0/all/0/1\">Jordan Fernandes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vikram_K/0/1/0/all/0/1\">K V Vikram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+M_M/0/1/0/all/0/1\">Mano Ranjith Kumar M</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mary_M/0/1/0/all/0/1\">Metilda Sagaya Mary</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wajahat_M/0/1/0/all/0/1\">Mohammad Wajahat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+N_M/0/1/0/all/0/1\">Mohana N</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Batra_M/0/1/0/all/0/1\">Mudit Batra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+K_N/0/1/0/all/0/1\">Navina K</a>, <a href=\"http://arxiv.org/find/eess/1/au:+George_N/0/1/0/all/0/1\">Nihal John George</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravi_N/0/1/0/all/0/1\">Nithya Ravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mishra_P/0/1/0/all/0/1\">Pruthwik Mishra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Srivastava_S/0/1/0/all/0/1\">Sudhanshu Srivastava</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lodagala_V/0/1/0/all/0/1\">Vasista Sai Lodagala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mujadia_V/0/1/0/all/0/1\">Vandan Mujadia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vineeth_K/0/1/0/all/0/1\">Kada Sai Venkata Vineeth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sukhadia_V/0/1/0/all/0/1\">Vrunda Sukhadia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharma_D/0/1/0/all/0/1\">Dipti Sharma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Murthy_H/0/1/0/all/0/1\">Hema Murthy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhattacharya_P/0/1/0/all/0/1\">Pushpak Bhattacharya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Umesh_S/0/1/0/all/0/1\">S Umesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sangal_R/0/1/0/all/0/1\">Rajeev Sangal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Emerging Technologies in Artificial Intelligence Scientific Ecosystem Using an Indicator-based Model. (arXiv:2211.01348v1 [cs.DL])","link":"http://arxiv.org/abs/2211.01348","description":"<p>Early identification of emergent topics is of eminent importance due to their\npotential impacts on society. There are many methods for detecting emerging\nterms and topics, all with advantages and drawbacks. However, there is no\nconsensus about the attributes and indicators of emergence. In this study, we\nevaluate emerging topic detection in the field of artificial intelligence using\na new method to evaluate emergence. We also introduce two new attributes of\ncollaboration and technological impact which can help us use both paper and\npatent information simultaneously. Our results confirm that the proposed new\nmethod can successfully identify the emerging topics in the period of the\nstudy. Moreover, this new method can provide us with the score of each\nattribute and a final emergence score, which enable us to rank the emerging\ntopics with their emergence scores and each attribute score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghaemmaghami_A/0/1/0/all/0/1\">Ali Ghaemmaghami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiffauerova_A/0/1/0/all/0/1\">Andrea Schiffauerova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebadi_A/0/1/0/all/0/1\">Ashkan Ebadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Named Entity Recognition in Telephone Conversations via Effective Active Learning with Human in the Loop. (arXiv:2211.01354v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01354","description":"<p>Telephone transcription data can be very noisy due to speech recognition\nerrors, disfluencies, etc. Not only that annotating such data is very\nchallenging for the annotators, but also such data may have lots of annotation\nerrors even after the annotation job is completed, resulting in a very poor\nmodel performance. In this paper, we present an active learning framework that\nleverages human in the loop learning to identify data samples from the\nannotated dataset for re-annotation that are more likely to contain annotation\nerrors. In this way, we largely reduce the need for data re-annotation for the\nwhole dataset. We conduct extensive experiments with our proposed approach for\nNamed Entity Recognition and observe that by re-annotating only about 6%\ntraining instances out of the whole dataset, the F1 score for a certain entity\ntype can be significantly improved by about 25%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xue-Yong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+TN_S/0/1/0/all/0/1\">Shashi Bhushan TN</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MT-GenEval: A Counterfactual and Contextual Dataset for Evaluating Gender Accuracy in Machine Translation. (arXiv:2211.01355v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01355","description":"<p>As generic machine translation (MT) quality has improved, the need for\ntargeted benchmarks that explore fine-grained aspects of quality has increased.\nIn particular, gender accuracy in translation can have implications in terms of\noutput fluency, translation accuracy, and ethics. In this paper, we introduce\nMT-GenEval, a benchmark for evaluating gender accuracy in translation from\nEnglish into eight widely-spoken languages. MT-GenEval complements existing\nbenchmarks by providing realistic, gender-balanced, counterfactual data in\neight language pairs where the gender of individuals is unambiguous in the\ninput segment, including multi-sentence segments requiring inter-sentential\ngender agreement. Our data and code is publicly available under a CC BY SA 3.0\nlicense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Currey_A/0/1/0/all/0/1\">Anna Currey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadejde_M/0/1/0/all/0/1\">Maria N&#x103;dejde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappagari_R/0/1/0/all/0/1\">Raghavendra Pappagari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayer_M/0/1/0/all/0/1\">Mia Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauly_S/0/1/0/all/0/1\">Stanislas Lauly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_B/0/1/0/all/0/1\">Benjamin Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_G/0/1/0/all/0/1\">Georgiana Dinu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Expand: Reinforced Pseudo-relevance Feedback Selection for Information-seeking Conversations. (arXiv:2011.12771v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.12771","description":"<p>Information-seeking conversation systems are increasingly popular in\nreal-world applications, especially for e-commerce companies. To retrieve\nappropriate responses for users, it is necessary to compute the matching\ndegrees between candidate responses and users' queries with historical dialogue\nutterances. As the contexts are usually much longer than responses, it is thus\nnecessary to expand the responses (usually short) with richer information.\nRecent studies on pseudo-relevance feedback (PRF) have demonstrated its\neffectiveness in query expansion for search engines, hence we consider\nexpanding response using PRF information. However, existing PRF approaches are\neither based on heuristic rules or require heavy manual labeling, which are not\nsuitable for solving our task. To alleviate this problem, we treat the PRF\nselection for response expansion as a learning task and propose a reinforced\nlearning method that can be trained in an end-to-end manner without any human\nannotations. More specifically, we propose a reinforced selector to extract\nuseful PRF terms to enhance response candidates and a BERT-based response\nranker to rank the PRF-enhanced responses. The performance of the ranker serves\nas a reward to guide the selector to extract useful PRF terms, which boosts the\noverall task performance. Extensive experiments on both standard benchmarks and\ncommercial datasets prove the superiority of our reinforced PRF term selector\ncompared with other potential soft or hard selection methods. Both case studies\nand quantitative analysis show that our model is capable of selecting\nmeaningful PRF terms to expand response candidates and also achieving the best\nresults compared with all baselines on a variety of evaluation metrics. We have\nalso deployed our method on online production in an e-commerce company, which\nshows a significant improvement over the existing online ranking system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Haojie Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_F/0/1/0/all/0/1\">Feng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains. (arXiv:2012.01266v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.01266","description":"<p>Pre-trained language models have been applied to various NLP tasks with\nconsiderable performance gains. However, the large model sizes, together with\nthe long inference time, limit the deployment of such models in real-time\napplications. One line of model compression approaches considers knowledge\ndistillation to distill large teacher models into small student models. Most of\nthese studies focus on single-domain only, which ignores the transferable\nknowledge from other domains. We notice that training a teacher with\ntransferable knowledge digested across domains can achieve better\ngeneralization capability to help knowledge distillation. Hence we propose a\nMeta-Knowledge Distillation (Meta-KD) framework to build a meta-teacher model\nthat captures transferable knowledge across domains and passes such knowledge\nto students. Specifically, we explicitly force the meta-teacher to capture\ntransferable knowledge at both instance-level and feature-level from multiple\ndomains, and then propose a meta-distillation algorithm to learn single-domain\nstudent models with guidance from the meta-teacher. Experiments on public\nmulti-domain NLP tasks show the effectiveness and superiority of the proposed\nMeta-KD framework. Further, we also demonstrate the capability of Meta-KD in\nthe settings where the training data is scarce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Haojie Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Scheduled Sampling with Elastic Weight Consolidation for Neural Machine Translation. (arXiv:2109.06308v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06308","description":"<p>Despite strong performance in many sequence-to-sequence tasks, autoregressive\nmodels trained with maximum likelihood estimation suffer from exposure bias,\ni.e. the discrepancy between the ground-truth prefixes used during training and\nthe model-generated prefixes used at inference time. Scheduled sampling is a\nsimple and empirically successful approach which addresses this issue by\nincorporating model-generated prefixes into training. However, it has been\nargued that it is an inconsistent training objective leading to models ignoring\nthe prefixes altogether. In this paper, we conduct systematic experiments and\nfind that scheduled sampling, while it ameliorates exposure bias by increasing\nmodel reliance on the input sequence, worsens performance when the prefix at\ninference time is correct, a form of catastrophic forgetting. We propose to use\nElastic Weight Consolidation to better balance mitigating exposure bias with\nretaining performance. Experiments on four IWSLT'14 and WMT'14 translation\ndatasets demonstrate that our approach alleviates catastrophic forgetting and\nsignificantly outperforms maximum likelihood estimation and scheduled sampling\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korakakis_M/0/1/0/all/0/1\">Michalis Korakakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let the CAT out of the bag: Contrastive Attributed explanations for Text. (arXiv:2109.07983v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07983","description":"<p>Contrastive explanations for understanding the behavior of black box models\nhas gained a lot of attention recently as they provide potential for recourse.\nIn this paper, we propose a method Contrastive Attributed explanations for Text\n(CAT) which provides contrastive explanations for natural language text data\nwith a novel twist as we build and exploit attribute classifiers leading to\nmore semantically meaningful explanations. To ensure that our contrastive\ngenerated text has the fewest possible edits with respect to the original text,\nwhile also being fluent and close to a human generated contrastive, we resort\nto a minimal perturbation approach regularized using a BERT language model and\nattribute classifiers trained on available attributes. We show through\nqualitative examples and a user study that our method not only conveys more\ninsight because of these attributes, but also leads to better quality\n(contrastive) text. Quantitatively, we show that our method outperforms other\nstate-of-the-art methods across four data sets on four benchmark metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chemmengath_S/0/1/0/all/0/1\">Saneem Chemmengath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azad_A/0/1/0/all/0/1\">Amar Prakash Azad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luss_R/0/1/0/all/0/1\">Ronny Luss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhurandhar_A/0/1/0/all/0/1\">Amit Dhurandhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation. (arXiv:2203.00281v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00281","description":"<p>Recently CKY-based models show great potential in unsupervised grammar\ninduction thanks to their human-like encoding paradigm, which runs recursively\nand hierarchically, but requires $O(n^3)$ time-complexity. Recursive\nTransformer based on Differentiable Trees (R2D2) makes it possible to scale to\nlarge language model pre-training even with complex tree encoder by introducing\na heuristic pruning method. However, the rule-based pruning approach suffers\nfrom local optimum and slow inference issues. In this paper, we fix those\nissues in a unified method. We propose to use a top-down parser as a\nmodel-based pruning method, which also enables parallel encoding during\ninference. Typically, our parser casts parsing as a split point scoring task,\nwhich first scores all split points for a given sentence, and then recursively\nsplits a span into two by picking a split point with the highest score in the\ncurrent span. The reverse order of the splits is considered as the order of\npruning in R2D2 encoder. Beside the bi-directional language model loss, we also\noptimize the parser by minimizing the KL distance between tree probabilities\nfrom parser and R2D2. Our experiments show that our Fast-R2D2 improves\nperformance significantly in grammar induction and achieves competitive results\nin downstream classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structurally Diverse Sampling for Sample-Efficient Training and Comprehensive Evaluation. (arXiv:2203.08445v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08445","description":"<p>A growing body of research has demonstrated the inability of NLP models to\ngeneralize compositionally and has tried to alleviate it through specialized\narchitectures, training schemes, and data augmentation, among other approaches.\nIn this work, we study a different approach: training on instances with diverse\nstructures. We propose a model-agnostic algorithm for subsampling such sets of\ninstances from a labeled instance pool with structured outputs. Evaluating on\nboth compositional template splits and traditional IID splits of 5 semantic\nparsing datasets of varying complexity, we show that structurally diverse\ntraining using our algorithm leads to comparable or better generalization than\nprior algorithms in 9 out of 10 dataset-split type pairs. In general, we find\nstructural diversity to consistently improve sample efficiency compared to\nrandom train sets. Moreover, we show that structurally diverse sampling yields\ncomprehensive test sets that are a lot more challenging than IID test sets.\nFinally, we provide two explanations for improved generalization from diverse\ntrain sets: 1) improved coverage of output substructures, and 2) a reduction in\nspurious correlations between these substructures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shivanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations. (arXiv:2203.16965v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16965","description":"<p>While self-supervised speech representation learning (SSL) models serve a\nvariety of downstream tasks, these models have been observed to overfit to the\ndomain from which the unlabelled data originates. To alleviate this issue, we\npropose PADA (Pruning Assisted Domain Adaptation) and zero out redundant\nweights from models pre-trained on large amounts of out-of-domain (OOD) data.\nIntuitively, this helps to make space for the target-domain ASR finetuning. The\nredundant weights can be identified through various pruning strategies which\nhave been discussed in detail as a part of this work. Specifically, we\ninvestigate the effect of the recently discovered Task-Agnostic and Task-Aware\npruning on PADA and propose a new pruning paradigm based on the latter, which\nwe call Cross-Domain Task-Aware Pruning (CD-TAW). CD-TAW obtains the initial\npruning mask from a well fine-tuned OOD model, which makes it starkly different\nfrom the rest of the pruning strategies discussed in the paper. Our proposed\nCD-TAW methodology achieves up to 20.6% relative WER improvement over our\nbaseline when fine-tuned on a 2-hour subset of Switchboard data without\nlanguage model (LM) decoding. Furthermore, we conduct a detailed analysis to\nhighlight the key design choices of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_L/0/1/0/all/0/1\">Lodagala V S V Durga Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Descartes: Generating Short Descriptions of Wikipedia Articles. (arXiv:2205.10012v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10012","description":"<p>Wikipedia is one of the richest knowledge sources on the Web today. In order\nto facilitate navigating, searching, and maintaining its content, Wikipedia's\nguidelines state that all articles should be annotated with a so-called short\ndescription indicating the article's topic (e.g., the short description of beer\nis \"Alcoholic drink made from fermented cereal grains\"). Nonetheless, a large\nfraction of articles (ranging from 10.2% in Dutch to 99.7% in Kazakh) have no\nshort description yet, with detrimental effects for millions of Wikipedia\nusers. Motivated by this problem, we introduce the novel task of automatically\ngenerating short descriptions for Wikipedia articles and propose Descartes, a\nmultilingual model for tackling it. Descartes integrates three sources of\ninformation to generate an article description in a target language: the text\nof the article in all its language versions, the already-existing descriptions\n(if any) of the article in other languages, and semantic type information\nobtained from a knowledge graph. We evaluate a Descartes model trained for\nhandling 25 languages simultaneously, showing that it beats baselines\n(including a strong translation-based baseline) and performs on par with\nmonolingual models tailored for specific languages. A human evaluation on three\nlanguages further shows that the quality of Descartes's descriptions is largely\nindistinguishable from that of human-written descriptions; e.g., 91.3% of our\nEnglish descriptions (vs. 92.1% of human-written descriptions) pass the bar for\ninclusion in Wikipedia, suggesting that Descartes is ready for production, with\nthe potential to support human editors in filling a major gap in today's\nWikipedia across languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakota_M/0/1/0/all/0/1\">Marija Sakota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"kNN-Prompt: Nearest Neighbor Zero-Shot Inference. (arXiv:2205.13792v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.13792","description":"<p>Retrieval-augmented language models (LMs) use non-parametric memory to\nsubstantially outperform their non-retrieval counterparts on perplexity-based\nevaluations, but it is an open question whether they achieve similar gains in\nfew- and zero-shot end-task accuracy. We extensively study one such model, the\nk-nearest neighbor LM (kNN-LM), showing that the gains marginally transfer. The\nmain challenge is to achieve coverage of the verbalizer tokens that define the\ndifferent end-task class labels. To address this challenge, we also introduce\nkNN-Prompt, a simple and effective kNN-LM with automatically expanded fuzzy\nverbalizers (e.g. to expand terrible to also include silly and other\ntask-specific synonyms for sentiment classification). Across nine diverse\nend-tasks, using kNN-Prompt with GPT-2 large yields significant performance\nboosts over strong zero-shot baselines (13.4% absolute improvement over the\nbase LM on average). We also show that other advantages of non-parametric\naugmentation hold for end tasks; kNN-Prompt is effective for domain adaptation\nwith no further training, and gains increase with the size of the retrieval\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michael_J/0/1/0/all/0/1\">Julian Michael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TVLT: Textless Vision-Language Transformer. (arXiv:2209.14156v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2209.14156","description":"<p>In this work, we present the Textless Vision-Language Transformer (TVLT),\nwhere homogeneous transformer blocks take raw visual and audio inputs for\nvision-and-language representation learning with minimal modality-specific\ndesign, and do not use text-specific modules such as tokenization or automatic\nspeech recognition (ASR). TVLT is trained by reconstructing masked patches of\ncontinuous video frames and audio spectrograms (masked autoencoding) and\ncontrastive modeling to align video and audio. TVLT attains performance\ncomparable to its text-based counterpart on various multimodal tasks, such as\nvisual question answering, image retrieval, video retrieval, and multimodal\nsentiment analysis, with 28x faster inference speed and only 1/3 of the\nparameters. Our findings suggest the possibility of learning compact and\nefficient visual-linguistic representations from low-level visual and audio\nsignals without assuming the prior existence of text. Our code and checkpoints\nare available at: https://github.com/zinengtang/TVLT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zineng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yixin Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised learning of speech representations. (arXiv:2210.02592v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02592","description":"<p>While Self-Supervised Learning has helped reap the benefit of the scale from\nthe available unlabeled data, the learning paradigms are continuously being\nbettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which\nuses clustering and an augmentation-based cross-contrastive loss as its\nself-supervised objective. Through the clustering module, we scale down the\ninfluence of those negative examples that are highly similar to the positive.\nThe Cross-Contrastive loss is computed between the encoder output of the\noriginal sample and the quantizer output of its augmentation and vice-versa,\nbringing robustness to the pre-training strategy. ccc-wav2vec 2.0 achieves up\nto 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on\nthe test-clean and test-other sets, respectively, of LibriSpeech, without the\nuse of any language model. The proposed method also achieves up to 14.9%\nrelative WER improvement over the baseline wav2vec 2.0 when fine-tuned on\nSwitchboard data. We make all our codes publicly available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lodagala_V/0/1/0/all/0/1\">Vasista Sai Lodagala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v2 [stat.ME] UPDATED)","link":"http://arxiv.org/abs/2210.08964","description":"<p>This paper studies the time series forecasting problem from a whole new\nperspective. In the existing SOTA time-series representation learning methods,\nthe forecasting models take a sequence of numerical values as input and yield\nnumerical values as output. The existing SOTA models are largely based on\nTransformer architecture, modified with multiple encoding mechanisms to\nincorporate the context and semantics around the historical data. In this\npaper, we approach representation learning of time-series from the paradigm of\nprompt-based natural language modeling. Inspired by the successes of\npre-trained language foundation models, we pose a question about whether these\nmodels can also be adapted to solve time-series forecasting. Thus, we propose a\nnew forecasting paradigm: prompt-based time series forecasting (PromptCast). In\nthis novel task, the numerical input and output are transformed into prompts.\nWe frame the forecasting task in a sentence-to-sentence manner which makes it\npossible to directly apply language models for forecasting purposes. To support\nand facilitate the research of this task, we also present a large-scale dataset\n(PISA) that includes three real-world forecasting scenarios. We evaluate\ndifferent SOTA numerical-based forecasting methods and language generation\nmodels such as Bart. The benchmark results with single- and multi-step\nforecasting settings demonstrate that the proposed prompt-based time series\nforecasting with language generation models is a promising research direction.\nIn addition, in comparison to conventional numerical-based forecasting,\nPromptCast shows a much better generalization ability under the zero-shot\nsetting. We believe that the proposed PromptCast task as well as our PISA\ndataset could provide novel insights and further lead to new research\ndirections in the domain of time-series representation learning and\nforecasting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Salim_F/0/1/0/all/0/1\">Flora D.Salim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding. (arXiv:2210.14169v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14169","description":"<p>Dialogue understanding tasks often necessitate abundant annotated data to\nachieve good performance and that presents challenges in low-resource settings.\nTo alleviate this barrier, we explore few-shot data augmentation for dialogue\nunderstanding by prompting large pre-trained language models and present a\nnovel approach that iterates on augmentation quality by applying\nweakly-supervised filters. We evaluate our methods on the emotion and act\nclassification tasks in DailyDialog and the intent classification task in\nFacebook Multilingual Task-Oriented Dialogue. Models fine-tuned on our\naugmented data mixed with few-shot ground truth data are able to approach or\nsurpass existing state-of-the-art performance on both datasets. For DailyDialog\nspecifically, using 10% of the ground truth data we outperform the current\nstate-of-the-art model which uses 100% of the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maximillian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenbaum_A/0/1/0/all/0/1\">Andy Rosenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$N$-gram is Back: Residual Learning of Neural Text Generation with $n$-gram Language Model. (arXiv:2210.14431v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14431","description":"<p>$N$-gram language models (LM) have been largely superseded by neural LMs as\nthe latter exhibits better performance. However, we find that $n$-gram models\ncan achieve satisfactory performance on a large proportion of testing cases,\nindicating they have already captured abundant knowledge of the language with\nrelatively low computational cost. With this observation, we propose to learn a\nneural LM that fits the residual between an $n$-gram LM and the real-data\ndistribution. The combination of $n$-gram and neural LMs not only allows the\nneural part to focus on the deeper understanding of language but also provides\na flexible way to customize an LM by switching the underlying $n$-gram model\nwithout changing the neural model. Experimental results on three typical\nlanguage tasks (i.e., language modeling, machine translation, and\nsummarization) demonstrate that our approach attains additional performance\ngains over popular standalone neural models consistently. We also show that our\napproach allows for effective domain adaptation by simply switching to a\ndomain-specific $n$-gram model, without any extra training. Our code is\nreleased at https://github.com/ghrua/NgramRes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Taro Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling structure-building in the brain with CCG parsing and large language models. (arXiv:2210.16147v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16147","description":"<p>To model behavioral and neural correlates of language comprehension in\nnaturalistic environments, researchers have turned to broad-coverage tools from\nnatural-language processing and machine learning. Where syntactic structure is\nexplicitly modeled, prior work has relied predominantly on context-free\ngrammars (CFG), yet such formalisms are not sufficiently expressive for human\nlanguages. Combinatory Categorial Grammars (CCGs) are sufficiently expressive\ndirectly compositional models of grammar with flexible constituency that\naffords incremental interpretation. In this work we evaluate whether a more\nexpressive CCG provides a better model than a CFG for human neural signals\ncollected with fMRI while participants listen to an audiobook story. We further\ntest between variants of CCG that differ in how they handle optional adjuncts.\nThese evaluations are carried out against a baseline that includes estimates of\nnext-word predictability from a Transformer neural network language model. Such\na comparison reveals unique contributions of CCG structure-building\npredominantly in the left posterior temporal lobe: CCG-derived measures offer a\nsuperior fit to neural signals compared to those derived from a CFG. These\neffects are spatially distinct from bilateral superior temporal effects that\nare unique to predictability. Neural effects for structure-building are thus\nseparable from predictability during naturalistic listening, and those effects\nare best characterized by a grammar whose expressive power is motivated on\nindependent linguistic grounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stanojevic_M/0/1/0/all/0/1\">Milo&#x161; Stanojevi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brennan_J/0/1/0/all/0/1\">Jonathan R. Brennan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunagan_D/0/1/0/all/0/1\">Donald Dunagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_J/0/1/0/all/0/1\">John T. Hale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning. (arXiv:2210.17451v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.17451","description":"<p>Standard fine-tuning of large pre-trained language models (PLMs) for\ndownstream tasks requires updating hundreds of millions to billions of\nparameters, and storing a large copy of the PLM weights for every task\nresulting in increased cost for storing, sharing and serving the models. To\naddress this, parameter-efficient fine-tuning (PEFT) techniques were introduced\nwhere small trainable components are injected in the PLM and updated during\nfine-tuning. We propose AdaMix as a general PEFT method that tunes a mixture of\nadaptation modules -- given the underlying PEFT method of choice -- introduced\nin each Transformer layer while keeping most of the PLM weights frozen. For\ninstance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture\nof low rank decomposition matrices like LoRA to improve downstream task\nperformance over the corresponding PEFT methods for fully supervised and\nfew-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the\nsame computational cost and the number of tunable parameters as the underlying\nPEFT method. By only tuning 0.1-0.2% of PLM parameters, we show that AdaMix\noutperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for\nboth NLU and NLG tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sahaj Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where to start? Analyzing the potential value of intermediate models. (arXiv:2211.00107v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.00107","description":"<p>Previous studies observed that finetuned models may be better base models\nthan the vanilla pretrained model. Such a model, finetuned on some source\ndataset, may provide a better starting point for a new finetuning process on a\ndesired target dataset. Here, we perform a systematic analysis of this\n\\emph{intertraining} scheme, over a wide range of English classification tasks.\nSurprisingly, our analysis suggests that the potential intertraining gain can\nbe analyzed \\emph{independently} for the target dataset under consideration,\nand for a base model being considered as a starting point. This is in contrast\nto current perception that the alignment between the target dataset and the\nsource dataset used to generate the base model is a major factor in determining\nintertraining success. We analyze different aspects that contribute to each.\nFurthermore, we leverage our analysis to propose a practical and efficient\napproach to determine if and how to select a base model in real-world settings.\nLast, we release an updating ranking of best models in the HuggingFace hub per\narchitecture https://ibm.github.io/model-recycling/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venezian_E/0/1/0/all/0/1\">Elad Venezian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Don_Yehia_S/0/1/0/all/0/1\">Shachar Don-Yehia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia. (arXiv:2106.01601v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2106.01601","description":"<p>Human activities can be seen as sequences of events, which are crucial to\nunderstanding societies. Disproportional event distribution for different\ndemographic groups can manifest and amplify social stereotypes, and potentially\njeopardize the ability of members in some groups to pursue certain goals. In\nthis paper, we present the first event-centric study of gender biases in a\nWikipedia corpus. To facilitate the study, we curate a corpus of career and\npersonal life descriptions with demographic information consisting of 7,854\nfragments from 10,412 celebrities. Then we detect events with a\nstate-of-the-art event detection model, calibrate the results using\nstrategically generated templates, and extract events that have asymmetric\nassociations with genders. Our study discovers that the Wikipedia pages tend to\nintermingle personal life events with professional events for females but not\nfor males, which calls for the awareness of the Wikipedia community to\nformalize guidelines and train the editors to mind the implicit biases that\ncontributors carry. Our work also lays the foundation for future works on\nquantifying and discovering event biases at the corpus level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}