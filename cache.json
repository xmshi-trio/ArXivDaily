{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Creating a Large Language Model of a Philosopher. (arXiv:2302.01339v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01339","description":"<p>Can large language models be trained to produce philosophical texts that are\ndifficult to distinguish from texts produced by human philosophers? To address\nthis question, we fine-tuned OpenAI's GPT-3 with the works of philosopher\nDaniel C. Dennett as additional training data. To explore the Dennett model, we\nasked the real Dennett ten philosophical questions and then posed the same\nquestions to the language model, collecting four responses for each question\nwithout cherry-picking. We recruited 425 participants to distinguish Dennett's\nanswer from the four machine-generated answers. Experts on Dennett's work (N =\n25) succeeded 51% of the time, above the chance rate of 20% but short of our\nhypothesized rate of 80% correct. For two of the ten questions, the language\nmodel produced at least one answer that experts selected more frequently than\nDennett's own answer. Philosophy blog readers (N = 302) performed similarly to\nthe experts, while ordinary research participants (N = 98) were near chance\ndistinguishing GPT-3's responses from those of an \"actual human philosopher\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwitzgebel_E/0/1/0/all/0/1\">Eric Schwitzgebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwitzgebel_D/0/1/0/all/0/1\">David Schwitzgebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strasser_A/0/1/0/all/0/1\">Anna Strasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curriculum-Guided Abstractive Summarization. (arXiv:2302.01342v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01342","description":"<p>Recent Transformer-based summarization models have provided a promising\napproach to abstractive summarization. They go beyond sentence selection and\nextractive strategies to deal with more complicated tasks such as novel word\ngeneration and sentence paraphrasing. Nonetheless, these models have two\nshortcomings: (1) they often perform poorly in content selection, and (2) their\ntraining strategy is not quite efficient, which restricts model performance. In\nthis paper, we explore two orthogonal ways to compensate for these pitfalls.\nFirst, we augment the Transformer network with a sentence cross-attention\nmodule in the decoder, encouraging more abstraction of salient content. Second,\nwe include a curriculum learning approach to reweight the training samples,\nbringing about an efficient learning procedure. Our second approach to enhance\nthe training strategy of Transformers networks makes stronger gains as compared\nto the first approach. We apply our model on extreme summarization dataset of\nReddit TIFU posts. We further look into three cross-domain summarization\ndatasets (Webis-TLDR-17, CNN/DM, and XSum), measuring the efficacy of\ncurriculum learning when applied in summarization. Moreover, a human evaluation\nis conducted to show the efficacy of the proposed method in terms of\nqualitative criteria, namely, fluency, informativeness, and overall quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sotudeh_S/0/1/0/all/0/1\">Sajad Sotudeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deilamsalehy_H/0/1/0/all/0/1\">Hanieh Deilamsalehy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goharian_N/0/1/0/all/0/1\">Nazli Goharian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The unreasonable effectiveness of few-shot learning for machine translation. (arXiv:2302.01398v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01398","description":"<p>We demonstrate the potential of few-shot translation systems, trained with\nunpaired language data, for both high and low-resource language pairs. We show\nthat with only 5 examples of high-quality translation data shown at inference,\na transformer decoder-only model trained solely with self-supervised learning,\nis able to match specialized supervised state-of-the-art models as well as more\ngeneral commercial translation systems. In particular, we outperform the best\nperforming system on the WMT'21 English - Chinese news translation task by only\nusing five examples of English - Chinese parallel data at inference. Moreover,\nour approach in building these models does not necessitate joint multilingual\ntraining or back-translation, is conceptually simple and shows the potential to\nextend to the multilingual setting. Furthermore, the resulting models are two\norders of magnitude smaller than state-of-the-art language models. We then\nanalyze the factors which impact the performance of few-shot translation\nsystems, and highlight that the quality of the few-shot demonstrations heavily\ndetermines the quality of the translations generated by our models. Finally, we\nshow that the few-shot paradigm also provides a way to control certain\nattributes of the translation -- we show that we are able to control for\nregional varieties and formality using only a five examples at inference,\npaving the way towards controllable machine translation systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_Y/0/1/0/all/0/1\">Yamini Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_G/0/1/0/all/0/1\">George Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krikun_M/0/1/0/all/0/1\">Maxim Krikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fangxiaoyu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Commonsense-Aware Prompting for Controllable Empathetic Dialogue Generation. (arXiv:2302.01441v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01441","description":"<p>Improving the emotional awareness of pre-trained language models is an\nemerging important problem for dialogue generation tasks. Although prior\nstudies have introduced methods to improve empathetic dialogue generation, few\nhave discussed how to incorporate commonsense knowledge into pre-trained\nlanguage models for controllable dialogue generation. In this study, we propose\na novel framework that improves empathetic dialogue generation using\npre-trained language models by 1) incorporating commonsense knowledge through\nprompt verbalization, and 2) controlling dialogue generation using a\nstrategy-driven future discriminator. We conducted experiments to reveal that\nboth the incorporation of social commonsense knowledge and enforcement of\ncontrol over generation help to improve generation performance. Finally, we\ndiscuss the implications of our study for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiren Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilicoglu_H/0/1/0/all/0/1\">Halil Kilicoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTE: A Dataset for Contextualized Table Extraction. (arXiv:2302.01451v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01451","description":"<p>Relevant information in documents is often summarized in tables, helping the\nreader to identify useful facts. Most benchmark datasets support either\ndocument layout analysis or table understanding, but lack in providing data to\napply both tasks in a unified way. We define the task of Contextualized Table\nExtraction (CTE), which aims to extract and define the structure of tables\nconsidering the textual context of the document. The dataset comprises 75k\nfully annotated pages of scientific papers, including more than 35k tables.\nData are gathered from PubMed Central, merging the information provided by\nannotations in the PubTables-1M and PubLayNet datasets. The dataset can support\nCTE and adds new classes to the original ones. The generated annotations can be\nused to develop end-to-end pipelines for various tasks, including document\nlayout analysis, table detection, structure recognition, and functional\nanalysis. We formally define CTE and evaluation metrics, showing which subtasks\ncan be tackled, describing advantages, limitations, and future works of this\ncollection of data. Annotations and code will be accessible a\nhttps://github.com/AILab-UniFI/cte-dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gemelli_A/0/1/0/all/0/1\">Andrea Gemelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vivoli_E/0/1/0/all/0/1\">Emanuele Vivoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinai_S/0/1/0/all/0/1\">Simone Marinai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Domain Adaptation for Speech Foundation Models. (arXiv:2302.01496v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01496","description":"<p>Foundation models (FMs), that are trained on broad data at scale and are\nadaptable to a wide range of downstream tasks, have brought large interest in\nthe research community. Benefiting from the diverse data sources such as\ndifferent modalities, languages and application domains, foundation models have\ndemonstrated strong generalization and knowledge transfer capabilities. In this\npaper, we present a pioneering study towards building an efficient solution for\nFM-based speech recognition systems. We adopt the recently developed\nself-supervised BEST-RQ for pretraining, and propose the joint finetuning with\nboth source and unsupervised target domain data using JUST Hydra. The FM\nencoder adapter and decoder are then finetuned to the target domain with a\nsmall amount of supervised in-domain data. On a large-scale YouTube and Voice\nSearch task, our method is shown to be both data and model parameter efficient.\nIt achieves the same quality with only 21.6M supervised in-domain data and\n130.8M finetuned parameters, compared to the 731.1M model trained from scratch\non additional 300M supervised in-domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Junwen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_G/0/1/0/all/0/1\">Guru Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaufays_F/0/1/0/all/0/1\">Francoise Beaufays</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Intermediate Layer Distillation for Compressing Language Models: An Overfitting Perspective. (arXiv:2302.01530v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01530","description":"<p>Knowledge distillation (KD) is a highly promising method for mitigating the\ncomputational problems of pre-trained language models (PLMs). Among various KD\napproaches, Intermediate Layer Distillation (ILD) has been a de facto standard\nKD method with its performance efficacy in the NLP field. In this paper, we\nfind that existing ILD methods are prone to overfitting to training datasets,\nalthough these methods transfer more information than the original KD. Next, we\npresent the simple observations to mitigate the overfitting of ILD: distilling\nonly the last Transformer layer and conducting ILD on supplementary tasks.\nBased on our two findings, we propose a simple yet effective\nconsistency-regularized ILD (CR-ILD), which prevents the student model from\noverfitting the training dataset. Substantial experiments on distilling BERT on\nthe GLUE benchmark and several synthetic datasets demonstrate that our proposed\nILD method outperforms other KD techniques. Our code is available at\nhttps://github.com/jongwooko/CR-ILD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Jongwoo Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seungjoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Minchan Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sukjin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_E/0/1/0/all/0/1\">Euijai Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Du-Seong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using natural language processing and structured medical data to phenotype patients hospitalized due to COVID-19. (arXiv:2302.01536v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01536","description":"<p>To identify patients who are hospitalized because of COVID-19 as opposed to\nthose who were admitted for other indications, we compared the performance of\ndifferent computable phenotype definitions for COVID-19 hospitalizations that\nuse different types of data from the electronic health records (EHR), including\nstructured EHR data elements, provider notes, or a combination of both data\ntypes. And conduct a retrospective data analysis utilizing chart review-based\nvalidation. Participants are 586 hospitalized individuals who tested positive\nfor SARS-CoV-2 during January 2022. We used natural language processing to\nincorporate data from provider notes and LASSO regression and Random Forests to\nfit classification algorithms that incorporated structured EHR data elements,\nprovider notes, or a combination of structured data and provider notes.\nResults: Based on a chart review, 38% of 586 patients were determined to be\nhospitalized for reasons other than COVID-19 despite having tested positive for\nSARS-CoV-2. A classification algorithm that used provider notes had\nsignificantly better discrimination than one that used structured EHR data\nelements (AUROC: 0.894 vs 0.841, p &lt; 0.001), and performed similarly to a model\nthat combined provider notes with structured data elements (AUROC: 0.894 vs\n0.893). Assessments of hospital outcome metrics significantly differed based on\nwhether the population included all hospitalized patients who tested positive\nfor SARS-CoV-2 versus those who were determined to have been hospitalized due\nto COVID-19. This work demonstrates the utility of natural language processing\napproaches to derive information related to patient hospitalizations in cases\nwhere there may be multiple conditions that could serve as the primary\nindication for hospitalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_F/0/1/0/all/0/1\">Feier Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_J/0/1/0/all/0/1\">Jay Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurst_J/0/1/0/all/0/1\">Jillian H Hurst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarrington_M/0/1/0/all/0/1\">Michael E Yarrington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_D/0/1/0/all/0/1\">Deverick J Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_E/0/1/0/all/0/1\">Emily C O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_B/0/1/0/all/0/1\">Benjamin A Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Witgenstein's influence on artificial intelligence. (arXiv:2302.01570v1 [cs.AI])","link":"http://arxiv.org/abs/2302.01570","description":"<p>We examine how much of the contemporary progress in artificial intelligence\n(and, specifically, in natural language processing), can be, more or less\ndirectly, traced back to the seminal work and ideas of the Austrian-British\nphilosopher Ludwig Wittgenstein, with particular focus on his late views.\nDiscussing Wittgenstein's original theses will give us the chance to survey the\nstate of artificial intelligence, and comment on both its strengths and\nweaknesses. A similar text appeared first in Spanish as a chapter of CENTENARIO\nDEL SILENCIO (2021), a book celebrating 100 years since the publication of the\nTractatus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Molino_P/0/1/0/all/0/1\">Piero Molino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling for Stereotypes in Multimodal Language Model Evaluation. (arXiv:2302.01582v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01582","description":"<p>We propose a methodology and design two benchmark sets for measuring to what\nextent language-and-vision language models use the visual signal in the\npresence or absence of stereotypes. The first benchmark is designed to test for\nstereotypical colors of common objects, while the second benchmark considers\ngender stereotypes. The key idea is to compare predictions when the image\nconforms to the stereotype to predictions when it does not.\n</p>\n<p>Our results show that there is significant variation among multimodal models:\nthe recent Transformer-based FLAVA seems to be more sensitive to the choice of\nimage and less affected by stereotypes than older CNN-based models such as\nVisualBERT and LXMERT. This effect is more discernible in this type of\ncontrolled setting than in traditional evaluations where we do not know whether\nthe model relied on the stereotype or the visual signal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malik_M/0/1/0/all/0/1\">Manuj Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansson_R/0/1/0/all/0/1\">Richard Johansson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bioformer: an efficient transformer language model for biomedical text mining. (arXiv:2302.01588v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01588","description":"<p>Pretrained language models such as Bidirectional Encoder Representations from\nTransformers (BERT) have achieved state-of-the-art performance in natural\nlanguage processing (NLP) tasks. Recently, BERT has been adapted to the\nbiomedical domain. Despite the effectiveness, these models have hundreds of\nmillions of parameters and are computationally expensive when applied to\nlarge-scale NLP applications. We hypothesized that the number of parameters of\nthe original BERT can be dramatically reduced with minor impact on performance.\nIn this study, we present Bioformer, a compact BERT model for biomedical text\nmining. We pretrained two Bioformer models (named Bioformer8L and Bioformer16L)\nwhich reduced the model size by 60% compared to BERTBase. Bioformer uses a\nbiomedical vocabulary and was pre-trained from scratch on PubMed abstracts and\nPubMed Central full-text articles. We thoroughly evaluated the performance of\nBioformer as well as existing biomedical BERT models including BioBERT and\nPubMedBERT on 15 benchmark datasets of four different biomedical NLP tasks:\nnamed entity recognition, relation extraction, question answering and document\nclassification. The results show that with 60% fewer parameters, Bioformer16L\nis only 0.1% less accurate than PubMedBERT while Bioformer8L is 0.9% less\naccurate than PubMedBERT. Both Bioformer16L and Bioformer8L outperformed\nBioBERTBase-v1.1. In addition, Bioformer16L and Bioformer8L are 2-3 fold as\nfast as PubMedBERT/BioBERTBase-v1.1. Bioformer has been successfully deployed\nto PubTator Central providing gene annotations over 35 million PubMed abstracts\nand 5 million PubMed Central full-text articles. We make Bioformer publicly\navailable via https://github.com/WGLab/bioformer, including pre-trained models,\ndatasets, and instructions for downstream use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Li Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chih-Hsuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Around the world in 60 words: A generative vocabulary test for online research. (arXiv:2302.01614v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01614","description":"<p>Conducting experiments with diverse participants in their native languages\ncan uncover insights into culture, cognition, and language that may not be\nrevealed otherwise. However, conducting these experiments online makes it\ndifficult to validate self-reported language proficiency. Furthermore, existing\nproficiency tests are small and cover only a few languages. We present an\nautomated pipeline to generate vocabulary tests using text from Wikipedia. Our\npipeline samples rare nouns and creates pseudowords with the same low-level\nstatistics. Six behavioral experiments (N=236) in six countries and eight\nlanguages show that (a) our test can distinguish between native speakers of\nclosely related languages, (b) the test is reliable ($r=0.82$), and (c)\nperformance strongly correlates with existing tests (LexTale) and self-reports.\nWe further show that test accuracy is negatively correlated with the linguistic\ndistance between the tested and the native language. Our test, available in\neight languages, can easily be extended to other languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rijn_P/0/1/0/all/0/1\">Pol van Rijn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yue Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Harin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1\">Raja Marjieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sucholutsky_I/0/1/0/all/0/1\">Ilia Sucholutsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanzarini_F/0/1/0/all/0/1\">Francesca Lanzarini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1\">Elisabeth Andr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Sequential Sentence Relation to Improve Cross-lingual Dense Retrieval. (arXiv:2302.01626v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01626","description":"<p>Recently multi-lingual pre-trained language models (PLM) such as mBERT and\nXLM-R have achieved impressive strides in cross-lingual dense retrieval.\nDespite its successes, they are general-purpose PLM while the multilingual PLM\ntailored for cross-lingual retrieval is still unexplored. Motivated by an\nobservation that the sentences in parallel documents are approximately in the\nsame order, which is universal across languages, we propose to model this\nsequential sentence relation to facilitate cross-lingual representation\nlearning. Specifically, we propose a multilingual PLM called masked sentence\nmodel (MSM), which consists of a sentence encoder to generate the sentence\nrepresentations, and a document encoder applied to a sequence of sentence\nvectors from a document. The document encoder is shared for all languages to\nmodel the universal sequential sentence relation across languages. To train the\nmodel, we propose a masked sentence prediction task, which masks and predicts\nthe sentence vector via a hierarchical contrastive loss with sampled negatives.\nComprehensive experiments on four cross-lingual retrieval tasks show MSM\nsignificantly outperforms existing advanced pre-training models, demonstrating\nthe effectiveness and stronger cross-lingual retrieval capabilities of our\napproach. Code and model will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shunyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Show me your NFT and I tell you how it will perform: Multimodal representation learning for NFT selling price prediction. (arXiv:2302.01676v1 [cs.LG])","link":"http://arxiv.org/abs/2302.01676","description":"<p>Non-Fungible Tokens (NFTs) represent deeds of ownership, based on blockchain\ntechnologies and smart contracts, of unique crypto assets on digital art forms\n(e.g., artworks or collectibles). In the spotlight after skyrocketing in 2021,\nNFTs have attracted the attention of crypto enthusiasts and investors intent on\nplacing promising investments in this profitable market. However, the NFT\nfinancial performance prediction has not been widely explored to date.\n</p>\n<p>In this work, we address the above problem based on the hypothesis that NFT\nimages and their textual descriptions are essential proxies to predict the NFT\nselling prices. To this purpose, we propose MERLIN, a novel multimodal deep\nlearning framework designed to train Transformer-based language and visual\nmodels, along with graph neural network models, on collections of NFTs' images\nand texts. A key aspect in MERLIN is its independence on financial features, as\nit exploits only the primary data a user interested in NFT trading would like\nto deal with, i.e., NFT images and textual descriptions. By learning dense\nrepresentations of such data, a price-category classification task is performed\nby MERLIN models, which can also be tuned according to user preferences in the\ninference phase to mimic different risk-return investment profiles.\nExperimental evaluation on a publicly available dataset has shown that MERLIN\nmodels achieve significant performances according to several financial\nassessment criteria, fostering profitable investments, and also beating\nbaseline machine-learning classifiers based on financial features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Costa_D/0/1/0/all/0/1\">Davide Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cava_L/0/1/0/all/0/1\">Lucio La Cava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagarelli_A/0/1/0/all/0/1\">Andrea Tagarelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LIQUID: A Framework for List Question Answering Dataset Generation. (arXiv:2302.01691v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01691","description":"<p>Question answering (QA) models often rely on large-scale training datasets,\nwhich necessitates the development of a data generation framework to reduce the\ncost of manual annotations. Although several recent studies have aimed to\ngenerate synthetic questions with single-span answers, no study has been\nconducted on the creation of list questions with multiple, non-contiguous spans\nas answers. To address this gap, we propose \\ours, an automated framework for\ngenerating list QA datasets from unlabeled corpora. We first convert a passage\nfrom Wikipedia or PubMed into a summary and extract named entities from the\nsummarized text as candidate answers. This allows us to select answers that are\nsemantically correlated in context and is, therefore, suitable for constructing\nlist questions. We then create questions using an off-the-shelf question\ngenerator with the extracted entities and original passage. Finally, iterative\nfiltering and answer expansion are performed to ensure the accuracy and\ncompleteness of the answers. Using our synthetic data, we significantly improve\nthe performance of the previous best list QA models by exact-match F1 scores of\n5.0 on MultiSpanQA, 1.9 on Quoref, and 2.8 averaged across three BioASQ\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seongyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Command Line Interface Risk Modeling. (arXiv:2302.01749v1 [cs.CR])","link":"http://arxiv.org/abs/2302.01749","description":"<p>Protecting sensitive data is an essential part of security in cloud\ncomputing. However, only specific privileged individuals have access to view or\ninteract with this data; therefore, it is unscalable to depend on these\nindividuals also to maintain the software. A solution to this is to allow\nnon-privileged individuals access to maintain these systems but mask sensitive\ninformation from egressing. To this end, we have created a machine-learning\nmodel to predict and redact fields with sensitive data. This work concentrates\non Azure PowerShell, showing how it applies to other command-line interfaces\nand APIs. Using the F5-score as a weighted metric, we demonstrate different\ntransformation techniques to map this problem from an unknown field to the\nwell-researched area of natural language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faulds_D/0/1/0/all/0/1\">Dr Anthony L. Faulds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DANES: Deep Neural Network Ensemble Architecture for Social and Textual Context-aware Fake News Detection. (arXiv:2302.01756v1 [cs.AI])","link":"http://arxiv.org/abs/2302.01756","description":"<p>The growing popularity of social media platforms has simplified the creation\nand distribution of news articles but also creates a conduit for spreading fake\nnews. In consequence, the need arises for effective context-aware fake news\ndetection mechanisms, where the contextual information can be built either from\nthe textual content of posts or from available social data (e.g., information\nabout the users, reactions to posts, or the social network). In this paper, we\npropose DANES, a Deep Neural Network Ensemble Architecture for Social and\nTextual Context-aware Fake News Detection. DANES comprises a Text Branch for a\ntextual content-based context and a Social Branch for the social context. These\ntwo branches are used to create a novel Network Embedding. Preliminary ablation\nresults on 3 real-world datasets, i.e., BuzzFace, Twitter15, and Twitter16, are\npromising, with an accuracy that outperforms state-of-the-art solutions when\nemploying both social and textual content features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truica_C/0/1/0/all/0/1\">Ciprian-Octavian Truic&#x103;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostol_E/0/1/0/all/0/1\">Elena-Simona Apostol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karras_P/0/1/0/all/0/1\">Panagiotis Karras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Data Scarcity for Large Language Models. (arXiv:2302.01806v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01806","description":"<p>In recent years, pretrained neural language models (PNLMs) have taken the\nfield of natural language processing by storm, achieving new benchmarks and\nstate-of-the-art performances. These models often rely heavily on annotated\ndata, which may not always be available. Data scarcity are commonly found in\nspecialized domains, such as medical, or in low-resource languages that are\nunderexplored by AI research. In this dissertation, we focus on mitigating data\nscarcity using data augmentation and neural ensemble learning techniques for\nneural language models. In both research directions, we implement neural\nnetwork algorithms and evaluate their impact on assisting neural language\nmodels in downstream NLP tasks. Specifically, for data augmentation, we explore\ntwo techniques: 1) creating positive training data by moving an answer span\naround its original context and 2) using text simplification techniques to\nintroduce a variety of writing styles to the original training data. Our\nresults indicate that these simple and effective solutions improve the\nperformance of neural language models considerably in low-resource NLP domains\nand tasks. For neural ensemble learning, we use a multilabel neural classifier\nto select the best prediction outcome from a variety of individual pretrained\nneural language models trained for a low-resource medical text simplification\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Van_H/0/1/0/all/0/1\">Hoang Van</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexical Simplification using multi level and modular approach. (arXiv:2302.01823v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01823","description":"<p>Text Simplification is an ongoing problem in Natural Language Processing,\nsolution to which has varied implications. In conjunction with the TSAR-2022\nWorkshop @EMNLP2022 Lexical Simplification is the process of reducing the\nlexical complexity of a text by replacing difficult words with easier to read\n(or understand) expressions while preserving the original information and\nmeaning. This paper explains the work done by our team \"teamPN\" for English sub\ntask. We created a modular pipeline which combines modern day transformers\nbased models with traditional NLP methods like paraphrasing and verb sense\ndisambiguation. We created a multi level and modular pipeline where the target\ntext is treated according to its semantics(Part of Speech Tag). Pipeline is\nmulti level as we utilize multiple source models to find potential candidates\nfor replacement, It is modular as we can switch the source models and their\nweight-age in the final re-ranking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katyal_N/0/1/0/all/0/1\">Nikita Katyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_P/0/1/0/all/0/1\">Pawan Kumar Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Stylistic Profiles for the Task of Empathy Classification in Medical Narrative Essays. (arXiv:2302.01839v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01839","description":"<p>One important aspect of language is how speakers generate utterances and\ntexts to convey their intended meanings. In this paper, we bring various\naspects of the Construction Grammar (CxG) and the Systemic Functional Grammar\n(SFG) theories in a deep learning computational framework to model empathic\nlanguage. Our corpus consists of 440 essays written by premed students as\nnarrated simulated patient-doctor interactions. We start with baseline\nclassifiers (state-of-the-art recurrent neural networks and transformer\nmodels). Then, we enrich these models with a set of linguistic constructions\nproving the importance of this novel approach to the task of empathy\nclassification for this dataset. Our results indicate the potential of such\nconstructions to contribute to the overall empathy profile of first-person\nnarrative essays.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_P/0/1/0/all/0/1\">Priyanka Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girju_R/0/1/0/all/0/1\">Roxana Girju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Case Study for Compliance as Code with Graphs and Language Models: Public release of the Regulatory Knowledge Graph. (arXiv:2302.01842v1 [cs.AI])","link":"http://arxiv.org/abs/2302.01842","description":"<p>The paper presents a study on using language models to automate the\nconstruction of executable Knowledge Graph (KG) for compliance. The paper\nfocuses on Abu Dhabi Global Market regulations and taxonomy, involves manual\ntagging a portion of the regulations, training BERT-based models, which are\nthen applied to the rest of the corpus. Coreference resolution and syntax\nanalysis were used to parse the relationships between the tagged entities and\nto form KG stored in a Neo4j database. The paper states that the use of machine\nlearning models released by regulators to automate the interpretation of rules\nis a vital step towards compliance automation, demonstrates the concept\nquerying with Cypher, and states that the produced sub-graphs combined with\nGraph Neural Networks (GNN) will achieve expandability in judgment automation\nsystems. The graph is open sourced on GitHub to provide structured data for\nfuture advancements in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ershov_V/0/1/0/all/0/1\">Vladimir Ershov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding. (arXiv:2302.01849v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01849","description":"<p>We propose an entity-agnostic representation learning method for handling the\nproblem of inefficient parameter storage costs brought by embedding knowledge\ngraphs. Conventional knowledge graph embedding methods map elements in a\nknowledge graph, including entities and relations, into continuous vector\nspaces by assigning them one or multiple specific embeddings (i.e., vector\nrepresentations). Thus the number of embedding parameters increases linearly as\nthe growth of knowledge graphs. In our proposed model, Entity-Agnostic\nRepresentation Learning (EARL), we only learn the embeddings for a small set of\nentities and refer to them as reserved entities. To obtain the embeddings for\nthe full set of entities, we encode their distinguishable information from\ntheir connected relations, k-nearest reserved entities, and multi-hop\nneighbors. We learn universal and entity-agnostic encoders for transforming\ndistinguishable information into entity embeddings. This approach allows our\nproposed EARL to have a static, efficient, and lower parameter count than\nconventional knowledge graph embedding methods. Experimental results show that\nEARL uses fewer parameters and performs better on link prediction tasks than\nbaselines, reflecting its parameter efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yushan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing to Unseen Elements: A Survey on Knowledge Extrapolation for Knowledge Graphs. (arXiv:2302.01859v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01859","description":"<p>Knowledge graphs (KGs) have become effective knowledge resources in diverse\napplications, and knowledge graph embedding (KGE) methods have attracted\nincreasing attention in recent years. However, it's still challenging for\nconventional KGE methods to handle unseen entities or relations during the\nmodel test. Much effort has been made in various fields of KGs to address this\nproblem. In this paper, we use a set of general terminologies to unify these\nmethods and refer to them as Knowledge Extrapolation. We comprehensively\nsummarize these methods classified by our proposed taxonomy and describe their\ncorrelations. Next, we introduce the benchmarks and provide comparisons of\nthese methods from aspects that are not reflected by the taxonomy. Finally, we\nsuggest some potential directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yuxia Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zezhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLADIS: A General and Large Acronym Disambiguation Benchmark. (arXiv:2302.01860v1 [cs.CL])","link":"http://arxiv.org/abs/2302.01860","description":"<p>Acronym Disambiguation (AD) is crucial for natural language understanding on\nvarious sources, including biomedical reports, scientific papers, and search\nengine queries. However, existing acronym disambiguation benchmarks and tools\nare limited to specific domains, and the size of prior benchmarks is rather\nsmall. To accelerate the research on acronym disambiguation, we construct a new\nbenchmark named GLADIS with three components: (1) a much larger acronym\ndictionary with 1.5M acronyms and 6.4M long forms; (2) a pre-training corpus\nwith 160 million sentences; (3) three datasets that cover the general,\nscientific, and biomedical domains. We then pre-train a language model,\n\\emph{AcroBERT}, on our constructed corpus for general acronym disambiguation,\nand show the challenges and values of our new benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lihu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varoquaux_G/0/1/0/all/0/1\">Ga&#xeb;l Varoquaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suchanek_F/0/1/0/all/0/1\">Fabian M. Suchanek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keyword Assisted Topic Models. (arXiv:2004.05964v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.05964","description":"<p>In recent years, fully automated content analysis based on probabilistic\ntopic models has become popular among social scientists because of their\nscalability. The unsupervised nature of the models makes them suitable for\nexploring topics in a corpus without prior knowledge. However, researchers find\nthat these models often fail to measure specific concepts of substantive\ninterest by inadvertently creating multiple topics with similar content and\ncombining distinct themes into a single topic. In this paper, we empirically\ndemonstrate that providing a small number of keywords can substantially enhance\nthe measurement performance of topic models. An important advantage of the\nproposed keyword assisted topic model (keyATM) is that the specification of\nkeywords requires researchers to label topics prior to fitting a model to the\ndata. This contrasts with a widespread practice of post-hoc topic\ninterpretation and adjustments that compromises the objectivity of empirical\nfindings. In our application, we find that keyATM provides more interpretable\nresults, has better document classification performance, and is less sensitive\nto the number of topics than the standard topic models. Finally, we show that\nkeyATM can also incorporate covariates and model time trends. An open-source\nsoftware package is available for implementing the proposed methodology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eshima_S/0/1/0/all/0/1\">Shusei Eshima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imai_K/0/1/0/all/0/1\">Kosuke Imai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomoya Sasaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Solvability of Interpretability Evaluation Metrics. (arXiv:2205.08696v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.08696","description":"<p>Feature attribution methods are popular for explaining neural network\npredictions, and they are often evaluated on metrics such as comprehensiveness\nand sufficiency. In this paper, we highlight an intriguing property of these\nmetrics: their solvability. Concretely, we can define the problem of optimizing\nan explanation for a metric, which can be solved by beam search. This\nobservation leads to the obvious yet unaddressed question: why do we use\nexplainers (e.g., LIME) not based on solving the target metric, if the metric\nvalue represents explanation quality? We present a series of investigations\nshowing strong performance of this beam search explainer and discuss its\nbroader implication: a definition-evaluation duality of interpretability\nconcepts. We implement the explainer and release the Python solvex package for\nmodels of text, image and tabular domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yilun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Julie Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoNT: Contrastive Neural Text Generation. (arXiv:2205.14690v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14690","description":"<p>Recently, contrastive learning attracts increasing interests in neural text\ngeneration as a new solution to alleviate the exposure bias problem. It\nintroduces a sequence-level training signal which is crucial to generation\ntasks that always rely on auto-regressive decoding. However, previous methods\nusing contrastive learning in neural text generation usually lead to inferior\nperformance. In this paper, we analyse the underlying reasons and propose a new\nContrastive Neural Text generation framework, CoNT. CoNT addresses bottlenecks\nthat prevent contrastive learning from being widely adopted in generation tasks\nfrom three aspects -- the construction of contrastive examples, the choice of\nthe contrastive loss, and the strategy in decoding. We validate CoNT on five\ngeneration tasks with ten benchmarks, including machine translation,\nsummarization, code comment generation, data-to-text generation and commonsense\ngeneration. Experimental results show that CoNT clearly outperforms the\nconventional training framework on all the ten benchmarks with a convincing\nmargin. Especially, CoNT surpasses previous the most competitive contrastive\nlearning method for text generation, by 1.50 BLEU on machine translation and\n1.77 ROUGE-1 on summarization, respectively. It achieves new state-of-the-art\non summarization, code comment generation (without external data) and\ndata-to-text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_C/0/1/0/all/0/1\">Chenxin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1\">Kai Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaPrompting: Learning to Learn Better Prompts. (arXiv:2209.11486v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.11486","description":"<p>Prompting method is regarded as one of the crucial progress for few-shot\nnature language processing. Recent research on prompting moves from discrete\ntokens based ``hard prompts'' to continuous ``soft prompts'', which employ\nlearnable vectors as pseudo prompt tokens and achieve better performance.\nThough showing promising prospects, these soft-prompting methods are observed\nto rely heavily on good initialization to take effect. Unfortunately, obtaining\na perfect initialization for soft prompts requires understanding of inner\nlanguage models working and elaborate design, which is no easy task and has to\nrestart from scratch for each new task. To remedy this, we propose a\ngeneralized soft prompting method called MetaPrompting, which adopts the\nwell-recognized model-agnostic meta-learning algorithm to automatically find\nbetter prompt initialization that facilitates fast adaptation to new prompting\ntasks.Extensive experiments show MetaPrompting tackles soft prompt\ninitialization problem and brings significant improvement on four different\ndatasets (over 6 points improvement in accuracy for 1-shot setting), achieving\nnew state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yutai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hongyuan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinghao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemEval 2023 Task 9: Multilingual Tweet Intimacy Analysis. (arXiv:2210.01108v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01108","description":"<p>We propose MINT, a new Multilingual INTimacy analysis dataset covering 13,372\ntweets in 10 languages including English, French, Spanish, Italian, Portuguese,\nKorean, Dutch, Chinese, Hindi, and Arabic. We benchmarked a list of popular\nmultilingual pre-trained language models. The dataset is released along with\nthe SemEval 2023 Task 9: Multilingual Tweet Intimacy Analysis\n(https://sites.google.com/umich.edu/semeval-2023-tweet-intimacy).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiaxin Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1\">V&#xed;tor Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bos_M/0/1/0/all/0/1\">Maarten Bos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yozon Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_L/0/1/0/all/0/1\">Leonardo Neves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbieri_F/0/1/0/all/0/1\">Francesco Barbieri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualize Before You Write: Imagination-Guided Open-Ended Text Generation. (arXiv:2210.03765v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03765","description":"<p>Recent advances in text-to-image synthesis make it possible to visualize\nmachine imaginations for a given context. On the other hand, when generating\ntext, human writers are gifted at creative visualization, which enhances their\nwritings by forming imaginations as blueprints before putting down the stories\nin words. Inspired by such a cognitive process, we ask the natural question of\nwhether we can endow machines with the same ability to utilize visual\ninformation and construct a general picture of the context to guide text\ngeneration. In this work, we propose iNLG that uses machine-generated images to\nguide language models in open-ended text generation. The experiments and\nanalyses demonstrate the effectiveness of iNLG on open-ended text generation\ntasks, including text completion, story generation, and concept-to-text\ngeneration in both few-shot and full-data scenarios. Both automatic metrics and\nhuman evaluations verify that the text snippets generated by our iNLG are\ncoherent and informative while displaying minor degeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1\">An Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"John is 50 years old, can his son be 65?\" Evaluating NLP Models' Understanding of Feasibility. (arXiv:2210.07471v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07471","description":"<p>In current NLP research, large-scale language models and their abilities are\nwidely being discussed. Some recent works have also found notable failures of\nthese models. Often these failure examples involve complex reasoning abilities.\nThis work focuses on a simple commonsense ability, reasoning about when an\naction (or its effect) is feasible. To this end, we introduce FeasibilityQA, a\nquestion-answering dataset involving binary classification (BCQ) and\nmulti-choice multi-correct questions (MCQ) that test understanding of\nfeasibility. We show that even state-of-the-art models such as GPT-3, GPT-2,\nand T5 struggle to answer the feasibility questions correctly. Specifically, on\nMCQ and BCQ questions, GPT-3 achieves an accuracy of just (19%, 62%) and (25%,\n64%) in zero-shot and few-shot settings, respectively. We also evaluate models\nby providing relevant knowledge statements required to answer the question. We\nfind that the additional knowledge leads to a 7% gain in performance, but the\noverall performance still remains low. These results make one wonder how much\ncommonsense knowledge about action feasibility is encoded in state-of-the-art\nmodels and how well they can reason about it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1\">Kuntal Kumar Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawant_S/0/1/0/all/0/1\">Saurabh Arjun Sawant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaria_K/0/1/0/all/0/1\">Kevin Scaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Siddharth Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Active Learning for Natural Language Processing. (arXiv:2210.10109v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10109","description":"<p>In this work, we provide a survey of active learning (AL) for its\napplications in natural language processing (NLP). In addition to a\nfine-grained categorization of query strategies, we also investigate several\nother important aspects of applying AL to NLP problems. These include AL for\nstructured prediction tasks, annotation cost, model learning (especially with\ndeep neural models), and starting and stopping AL. Finally, we conclude with a\ndiscussion of related topics and future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhisong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resolving Open-textured Rules with Templated Interpretive Arguments. (arXiv:2212.09700v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09700","description":"<p>Open-textured terms in written rules are typically settled through\ninterpretive argumentation. Ongoing work has attempted to catalogue the schemes\nused in such interpretive argumentation. But how can the use of these schemes\naffect the way in which people actually use and reason over the proper\ninterpretations of open-textured terms? Using the interpretive\nargument-eliciting game Aporia as our framework, we carried out an empirical\nstudy to answer this question. Differing from previous work, we did not allow\nparticipants to argue for interpretations arbitrarily, but to only use\narguments that fit with a given set of interpretive argument templates.\nFinally, we analyze the results captured by this new dataset, specifically\nfocusing on practical implications for the development of\ninterpretation-capable artificial reasoners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Licato_J/0/1/0/all/0/1\">John Licato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fields_L/0/1/0/all/0/1\">Logan Fields</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marji_Z/0/1/0/all/0/1\">Zaid Marji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VaxxHesitancy: A Dataset for Studying Hesitancy Towards COVID-19 Vaccination on Twitter. (arXiv:2301.06660v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.06660","description":"<p>Vaccine hesitancy has been a common concern, probably since vaccines were\ncreated and, with the popularisation of social media, people started to express\ntheir concerns about vaccines online alongside those posting pro- and\nanti-vaccine content. Predictably, since the first mentions of a COVID-19\nvaccine, social media users posted about their fears and concerns or about\ntheir support and belief into the effectiveness of these rapidly developing\nvaccines. Identifying and understanding the reasons behind public hesitancy\ntowards COVID-19 vaccines is important for policy markers that need to develop\nactions to better inform the population with the aim of increasing vaccine\ntake-up. In the case of COVID-19, where the fast development of the vaccines\nwas mirrored closely by growth in anti-vaxx disinformation, automatic means of\ndetecting citizen attitudes towards vaccination became necessary. This is an\nimportant computational social sciences task that requires data analysis in\norder to gain in-depth understanding of the phenomena at hand. Annotated data\nis also necessary for training data-driven models for more nuanced analysis of\nattitudes towards vaccination. To this end, we created a new collection of over\n3,101 tweets annotated with users' attitudes towards COVID-19 vaccination\n(stance). Besides, we also develop a domain-specific language model (VaxxBERT)\nthat achieves the best predictive performance (73.0 accuracy and 69.3 F1-score)\nas compared to a robust set of baselines. To the best of our knowledge, these\nare the first dataset and model that model vaccine hesitancy as a category\ndistinct from pro- and anti-vaccine stance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yida Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Mali Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimshaw_C/0/1/0/all/0/1\">Charlie Grimshaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1\">Kalina Bontcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Effects of Physical Actions in a Multi-modal Environment. (arXiv:2301.11845v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11845","description":"<p>Large Language Models (LLMs) handle physical commonsense information\ninadequately. As a result of being trained in a disembodied setting, LLMs often\nfail to predict an action's outcome in a given environment. However, predicting\nthe effects of an action before it is executed is crucial in planning, where\ncoherent sequences of actions are often needed to achieve a goal. Therefore, we\nintroduce the multi-modal task of predicting the outcomes of actions solely\nfrom realistic sensory inputs (images and text). Next, we extend an LLM to\nmodel latent representations of objects to better predict action outcomes in an\nenvironment. We show that multi-modal models can capture physical commonsense\nwhen augmented with visual information. Finally, we evaluate our model's\nperformance on novel actions and objects and find that combining modalities\nhelp models to generalize and learn physical commonsense reasoning better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dagan_G/0/1/0/all/0/1\">Gautier Dagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lascarides_A/0/1/0/all/0/1\">Alex Lascarides</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vicarious Offense and Noise Audit of Offensive Speech Classifiers. (arXiv:2301.12534v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12534","description":"<p>This paper examines social web content moderation from two key perspectives:\nautomated methods (machine moderators) and human evaluators (human moderators).\nWe conduct a noise audit at an unprecedented scale using nine machine\nmoderators trained on well-known offensive speech data sets evaluated on a\ncorpus sampled from 92 million YouTube comments discussing a multitude of\nissues relevant to US politics. We introduce a first-of-its-kind data set of\nvicarious offense. We ask annotators: (1) if they find a given social media\npost offensive; and (2) how offensive annotators sharing different political\nbeliefs would find the same content. Our experiments with machine moderators\nreveal that moderation outcomes wildly vary across different machine\nmoderators. Our experiments with human moderators suggest that (1) political\nleanings considerably affect first-person offense perspective; (2) Republicans\nare the worst predictors of vicarious offense; (3) predicting vicarious offense\nfor the Republicans is most challenging than predicting vicarious offense for\nthe Independents and the Democrats; and (4) disagreement across political\nidentity groups considerably increases when sensitive issues such as\nreproductive rights or gun control/rights are discussed. Both experiments\nsuggest that offense, is indeed, highly subjective and raise important\nquestions concerning content moderation practices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weerasooriya_T/0/1/0/all/0/1\">Tharindu Cyril Weerasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sujan Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homan_C/0/1/0/all/0/1\">Christopher M. Homan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+KhudaBukhsh_A/0/1/0/all/0/1\">Ashiqur R. KhudaBukhsh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TopoBERT: Plug and Play Toponym Recognition Module Harnessing Fine-tuned BERT. (arXiv:2301.13631v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.13631","description":"<p>Extracting precise geographical information from textual contents is crucial\nin a plethora of applications. For example, during hazardous events, a robust\nand unbiased toponym extraction framework can provide an avenue to tie the\nlocation concerned to the topic discussed by news media posts and pinpoint\nhumanitarian help requests or damage reports from social media. Early studies\nhave leveraged rule-based, gazetteer-based, deep learning, and hybrid\napproaches to address this problem. However, the performance of existing tools\nis deficient in supporting operations like emergency rescue, which relies on\nfine-grained, accurate geographic information. The emerging pretrained language\nmodels can better capture the underlying characteristics of text information,\nincluding place names, offering a promising pathway to optimize toponym\nrecognition to underpin practical applications. In this paper, TopoBERT, a\ntoponym recognition module based on a one dimensional Convolutional Neural\nNetwork (CNN1D) and Bidirectional Encoder Representation from Transformers\n(BERT), is proposed and fine-tuned. Three datasets (CoNLL2003-Train,\nWikipedia3000, WNUT2017) are leveraged to tune the hyperparameters, discover\nthe best training strategy, and train the model. Another two datasets\n(CoNLL2003-Test and Harvey2017) are used to evaluate the performance. Three\ndistinguished classifiers, linear, multi-layer perceptron, and CNN1D, are\nbenchmarked to determine the optimal model architecture. TopoBERT achieves\nstate-of-the-art performance (f1-score=0.865) compared to the other five\nbaseline models and can be applied to diverse toponym recognition tasks without\nadditional training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Lei Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yingjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_Y/0/1/0/all/0/1\">Yi Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_D/0/1/0/all/0/1\">Daniel Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero Shot Transfer of Legal Judgement Prediction as Article-aware Entailment for the European Court of Human Rights. (arXiv:2302.00609v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00609","description":"<p>In this paper, we cast Legal Judgment Prediction (LJP) from text on European\nCourt of Human Rights cases as an entailment task, where the case outcome is\nclassified from a combined input of case facts and convention articles. This\nconfiguration facilitates the model learning legal reasoning ability in mapping\narticle text to specific fact text. It also provides the opportunity to\nevaluate the model's ability to generalize to zero-shot settings when asked to\nclassify the case outcome with respect to articles not seen during training. We\ndevise zero-shot LJP experiments and apply domain adaptation methods based on\ndomain discriminator and Wasserstein distance. Our results demonstrate that the\nentailment architecture outperforms straightforward fact classification. We\nalso find that domain adaptation methods improve zero-shot transfer\nperformance, with article relatedness and encoder pre-training influencing the\neffect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santosh_T/0/1/0/all/0/1\">T.Y.S.S Santosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichim_O/0/1/0/all/0/1\">Oana Ichim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabmair_M/0/1/0/all/0/1\">Matthias Grabmair</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AmbiCoref: Evaluating Human and Model Sensitivity to Ambiguous Coreference. (arXiv:2302.00762v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00762","description":"<p>Given a sentence \"Abby told Brittney that she upset Courtney\", one would\nstruggle to understand who \"she\" refers to, and ask for clarification. However,\nif the word \"upset\" were replaced with \"hugged\", \"she\" unambiguously refers to\nAbby. We study if modern coreference resolution models are sensitive to such\npronominal ambiguity. To this end, we construct AmbiCoref, a diagnostic corpus\nof minimal sentence pairs with ambiguous and unambiguous referents. Our\nexamples generalize psycholinguistic studies of human perception of ambiguity\naround particular arrangements of verbs and their arguments. Analysis shows\nthat (1) humans are less sure of referents in ambiguous AmbiCoref examples than\nunambiguous ones, and (2) most coreference models show little difference in\noutput between ambiguous and unambiguous pairs. We release AmbiCoref as a\ndiagnostic corpus for testing whether models treat ambiguity similarly to\nhumans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuewei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malaviya_C/0/1/0/all/0/1\">Chaitanya Malaviya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yatskar_M/0/1/0/all/0/1\">Mark Yatskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging task dependency and contrastive learning for Legal Judgement Prediction on the European Court of Human Rights. (arXiv:2302.00768v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00768","description":"<p>We report on an experiment in legal judgement prediction on European Court of\nHuman Rights cases where our model first learns to predict the convention\narticles allegedly violated by the state from case facts descriptions, and\nsubsequently utilizes that information to predict a finding of a violation by\nthe court. We assess the dependency between these two tasks at the feature and\noutcome level. Furthermore, we leverage a hierarchical contrastive loss to pull\ntogether article specific representations of cases at the higher level level,\nleading to distinctive article clusters, and further pulls the cases in each\narticle cluster based on their outcome leading to sub-clusters of cases with\nsimilar outcomes. Our experiment results demonstrate that, given a static\npre-trained encoder, our models produce a small but consistent improvement in\nprediction performance over single-task and joint models without contrastive\nloss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santosh_T/0/1/0/all/0/1\">T.Y.S.S Santosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blas_M/0/1/0/all/0/1\">Marcel Perez San Blas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemper_P/0/1/0/all/0/1\">Phillip Kemper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabmair_M/0/1/0/all/0/1\">Matthias Grabmair</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment. (arXiv:2302.00902v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.00902","description":"<p>Recent progress in scaling up large language models has shown impressive\ncapabilities in performing few-shot learning across a wide range of text-based\ntasks. However, a key limitation is that these language models fundamentally\nlack visual perception - a crucial attribute needed to extend these models to\nbe able to interact with the real world and solve vision tasks, such as in\nvisual-question answering and robotics. Prior works have largely connected\nimage to text through pretraining and/or fine-tuning on curated image-text\ndatasets, which can be a costly and expensive process. In order to resolve this\nlimitation, we propose a simple yet effective approach called\nLanguage-Quantized AutoEncoder (LQAE), a modification of VQ-VAE that learns to\nalign text-image data in an unsupervised manner by leveraging pretrained\nlanguage models (e.g., BERT, RoBERTa). Our main idea is to encode image as\nsequences of text tokens by directly quantizing image embeddings using a\npretrained language codebook. We then apply random masking followed by a BERT\nmodel, and have the decoder reconstruct the original image from BERT predicted\ntext token embeddings. By doing so, LQAE learns to represent similar images\nwith similar clusters of text tokens, thereby aligning these two modalities\nwithout the use of aligned text-image pairs. This enables few-shot image\nclassification with large language models (e.g., GPT-3) as well as linear\nclassification of images based on BERT text features. To the best of our\nknowledge, our work is the first work that uses unaligned images for multimodal\ntasks by leveraging the power of pretrained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wilson Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v3 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2202.08063","description":"<p>Knowledge Extraction (KE), aiming to extract structural information from\nunstructured texts, often suffers from data scarcity and emerging unseen types,\ni.e., low-resource scenarios. Many neural approaches to low-resource KE have\nbeen widely investigated and achieved impressive performance. In this paper, we\npresent a literature review towards KE in low-resource scenarios, and\nsystematically categorize existing works into three paradigms: (1) exploiting\nhigher-resource data, (2) exploiting stronger models, and (3) exploiting data\nand models together. In addition, we highlight promising applications and\noutline some potential directions for future research. We hope that our survey\ncan help both the academic and industrial communities to better understand this\nfield, inspire more ideas, and boost broader applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}