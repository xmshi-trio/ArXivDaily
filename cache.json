{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"INO at Factify 2: Structure Coherence based Multi-Modal Fact Verification. (arXiv:2303.01510v1 [cs.LG])","link":"http://arxiv.org/abs/2303.01510","description":"<p>This paper describes our approach to the multi-modal fact verification\n(FACTIFY) challenge at AAAI2023. In recent years, with the widespread use of\nsocial media, fake news can spread rapidly and negatively impact social\nsecurity. Automatic claim verification becomes more and more crucial to combat\nfake news. In fact verification involving multiple modal data, there should be\na structural coherence between claim and document. Therefore, we proposed a\nstructure coherence-based multi-modal fact verification scheme to classify fake\nnews. Our structure coherence includes the following four aspects: sentence\nlength, vocabulary similarity, semantic similarity, and image similarity.\nSpecifically, CLIP and Sentence BERT are combined to extract text features, and\nResNet50 is used to extract image features. In addition, we also extract the\nlength of the text as well as the lexical similarity. Then the features were\nconcatenated and passed through the random forest classifier. Finally, our\nweighted average F1 score has reached 0.8079, achieving 2nd place in FACTIFY2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhulin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tongyue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixture of Soft Prompts for Controllable Data Generation. (arXiv:2303.01580v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01580","description":"<p>Large language models (LLMs) effectively generate fluent text when the target\noutput follows natural language patterns. However, structured prediction tasks\nconfine the output format to a limited ontology, causing even very large models\nto struggle since they were never trained with such restrictions in mind. The\ndifficulty of using LLMs for direct prediction is exacerbated in few-shot\nlearning scenarios, which commonly arise due to domain shift and resource\nlimitations. We flip the problem on its head by leveraging the LLM as a tool\nfor data augmentation rather than direct prediction. Our proposed Mixture of\nSoft Prompts (MSP) serves as a parameter-efficient procedure for generating\ndata in a controlled manner. Denoising mechanisms are further applied to\nimprove the quality of synthesized data. Automatic metrics show our method is\ncapable of producing diverse and natural text, while preserving label\nsemantics. Moreover, MSP achieves state-of-the-art results on three benchmarks\nwhen compared against strong baselines. Our method offers an alternate\ndata-centric approach for applying LLMs to complex prediction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Derek Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Celine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yunan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosati_D/0/1/0/all/0/1\">Domenic Rosati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technical report: Graph Neural Networks go Grammatical. (arXiv:2303.01590v1 [cs.LG])","link":"http://arxiv.org/abs/2303.01590","description":"<p>This paper proposes a new GNN design strategy. This strategy relies on\nContext-Free Grammars (CFG) generating the matrix language MATLANG. It enables\nus to ensure both WL-expressive power, substructure counting abilities and\nspectral properties. Applying our strategy, we design Grammatical Graph Neural\nNetwork G$ ^2$N$^2$, a provably 3-WL GNN able to count at edge-level cycles of\nlength up to 6 and able to reach band-pass filters. A large number of\nexperiments covering these properties corroborate the presented theoretical\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piquenot_J/0/1/0/all/0/1\">Jason Piquenot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moscatelli_A/0/1/0/all/0/1\">Aldo Moscatelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berar_M/0/1/0/all/0/1\">Maxime B&#xe9;rar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heroux_P/0/1/0/all/0/1\">Pierre H&#xe9;roux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramel_J/0/1/0/all/0/1\">Jean-Yves Ramel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+raveaux_R/0/1/0/all/0/1\">Romain raveaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_S/0/1/0/all/0/1\">S&#xe9;bastien Adam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QAID: Question Answering Inspired Few-shot Intent Detection. (arXiv:2303.01593v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01593","description":"<p>Intent detection with semantically similar fine-grained intents is a\nchallenging task. To address it, we reformulate intent detection as a\nquestion-answering retrieval task by treating utterances and intent names as\nquestions and answers. To that end, we utilize a question-answering retrieval\narchitecture and adopt a two stages training schema with batch contrastive\nloss. In the pre-training stage, we improve query representations through\nself-supervised training. Then, in the fine-tuning stage, we increase\ncontextualized token-level similarity scores between queries and answers from\nthe same intent. Our results on three few-shot intent detection benchmarks\nachieve state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_A/0/1/0/all/0/1\">Asaf Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetzler_M/0/1/0/all/0/1\">Matan Vetzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mass_Y/0/1/0/all/0/1\">Yosi Mass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazar_K/0/1/0/all/0/1\">Koren Lazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1\">Doron Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmeli_B/0/1/0/all/0/1\">Boaz Carmeli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APIContext2Com: Code Comment Generation by Incorporating Pre-Defined API Documentation. (arXiv:2303.01645v1 [cs.SE])","link":"http://arxiv.org/abs/2303.01645","description":"<p>Code comments are significantly helpful in comprehending software programs\nand also aid developers to save a great deal of time in software maintenance.\nCode comment generation aims to automatically predict comments in natural\nlanguage given a code snippet. Several works investigate the effect of\nintegrating external knowledge on the quality of generated comments. In this\nstudy, we propose a solution, namely APIContext2Com, to improve the\neffectiveness of generated comments by incorporating the pre-defined\nApplication Programming Interface (API) context. The API context includes the\ndefinition and description of the pre-defined APIs that are used within the\ncode snippets. As the detailed API information expresses the functionality of a\ncode snippet, it can be helpful in better generating the code summary. We\nintroduce a seq-2-seq encoder-decoder neural network model with different sets\nof multiple encoders to effectively transform distinct inputs into target\ncomments. A ranking mechanism is also developed to exclude non-informative\nAPIs, so that we can filter out unrelated APIs. We evaluate our approach using\nthe Java dataset from CodeSearchNet. The findings reveal that the proposed\nmodel improves the best baseline by 1.88 (8.24 %), 2.16 (17.58 %), 1.38 (18.3\n%), 0.73 (14.17 %), 1.58 (14.98 %) and 1.9 (6.92 %) for BLEU1, BLEU2, BLEU3,\nBLEU4, METEOR, ROUGE-L respectively. Human evaluation and ablation studies\nconfirm the quality of the generated comments and the effect of architecture\nand ranking APIs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahbazi_R/0/1/0/all/0/1\">Ramin Shahbazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1\">Fatemeh Fard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DWFormer: Dynamic Window transFormer for Speech Emotion Recognition. (arXiv:2303.01694v1 [cs.SD])","link":"http://arxiv.org/abs/2303.01694","description":"<p>Speech emotion recognition is crucial to human-computer interaction. The\ntemporal regions that represent different emotions scatter in different parts\nof the speech locally. Moreover, the temporal scales of important information\nmay vary over a large range within and across speech segments. Although\ntransformer-based models have made progress in this field, the existing models\ncould not precisely locate important regions at different temporal scales. To\naddress the issue, we propose Dynamic Window transFormer (DWFormer), a new\narchitecture that leverages temporal importance by dynamically splitting\nsamples into windows. Self-attention mechanism is applied within windows for\ncapturing temporal important information locally in a fine-grained way.\nCross-window information interaction is also taken into account for global\ncommunication. DWFormer is evaluated on both the IEMOCAP and the MELD datasets.\nExperimental results show that the proposed model achieves better performance\nthan the previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuaiqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1\">Xiaofen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weibin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weidong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangmin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NCL: Textual Backdoor Defense Using Noise-augmented Contrastive Learning. (arXiv:2303.01742v1 [cs.CR])","link":"http://arxiv.org/abs/2303.01742","description":"<p>At present, backdoor attacks attract attention as they do great harm to deep\nlearning models. The adversary poisons the training data making the model being\ninjected with a backdoor after being trained unconsciously by victims using the\npoisoned dataset. In the field of text, however, existing works do not provide\nsufficient defense against backdoor attacks. In this paper, we propose a\nNoise-augmented Contrastive Learning (NCL) framework to defend against textual\nbackdoor attacks when training models with untrustworthy data. With the aim of\nmitigating the mapping between triggers and the target label, we add\nappropriate noise perturbing possible backdoor triggers, augment the training\ndataset, and then pull homology samples in the feature space utilizing\ncontrastive learning objective. Experiments demonstrate the effectiveness of\nour method in defending three types of textual backdoor attacks, outperforming\nthe prior works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1\">Shengfang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1\">Qingni Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuejian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghai Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meme Sentiment Analysis Enhanced with Multimodal Spatial Encoding and Facial Embedding. (arXiv:2303.01781v1 [cs.CV])","link":"http://arxiv.org/abs/2303.01781","description":"<p>Internet memes are characterised by the interspersing of text amongst visual\nelements. State-of-the-art multimodal meme classifiers do not account for the\nrelative positions of these elements across the two modalities, despite the\nlatent meaning associated with where text and visual elements are placed.\nAgainst two meme sentiment classification datasets, we systematically show\nperformance gains from incorporating the spatial position of visual objects,\nfaces, and text clusters extracted from memes. In addition, we also present\nfacial embedding as an impactful enhancement to image representation in a\nmultimodal meme classifier. Finally, we show that incorporating this spatial\ninformation allows our fully automated approaches to outperform their\ncorresponding baselines that rely on additional human validation of\nOCR-extracted text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hazman_M/0/1/0/all/0/1\">Muzhaffar Hazman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeever_S/0/1/0/all/0/1\">Susan McKeever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffith_J/0/1/0/all/0/1\">Josephine Griffith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Language Relatedness in Machine Translation Through Domain Adaptation Techniques. (arXiv:2303.01793v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01793","description":"<p>One of the significant challenges of Machine Translation (MT) is the scarcity\nof large amounts of data, mainly parallel sentence aligned corpora. If the\nevaluation is as rigorous as resource-rich languages, both Neural Machine\nTranslation (NMT) and Statistical Machine Translation (SMT) can produce good\nresults with such large amounts of data. However, it is challenging to improve\nthe quality of MT output for low resource languages, especially in NMT and SMT.\nIn order to tackle the challenges faced by MT, we present a novel approach of\nusing a scaled similarity score of sentences, especially for related languages\nbased on a 5-gram KenLM language model with Kneser-ney smoothing technique for\nfiltering in-domain data from out-of-domain corpora that boost the translation\nquality of MT. Furthermore, we employ other domain adaptation techniques such\nas multi-domain, fine-tuning and iterative back-translation approach to compare\nour novel approach on the Hindi-Nepali language pair for NMT and SMT. Our\napproach succeeds in increasing ~2 BLEU point on multi-domain approach, ~3 BLEU\npoint on fine-tuning for NMT and ~2 BLEU point on iterative back-translation\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Amit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baruah_R/0/1/0/all/0/1\">Rupjyoti Baruah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratap_A/0/1/0/all/0/1\">Ajay Pratap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swarnkar_M/0/1/0/all/0/1\">Mayank Swarnkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anil Kumar Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Team Hitachi at SemEval-2023 Task 3: Exploring Cross-lingual Multi-task Strategies for Genre and Framing Detection in Online News. (arXiv:2303.01794v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01794","description":"<p>This paper explains the participation of team Hitachi to SemEval-2023 Task 3\n\"Detecting the genre, the framing, and the persuasion techniques in online news\nin a multi-lingual setup.\" Based on the multilingual, multi-task nature of the\ntask and the setting that training data is limited, we investigated different\nstrategies for training the pretrained language models under low resource\nsettings. Through extensive experiments, we found that (a)\ncross-lingual/multi-task training, and (b) collecting an external balanced\ndataset, can benefit the genre and framing detection. We constructed ensemble\nmodels from the results and achieved the highest macro-averaged F1 scores in\nItalian and Russian genre categorization subtasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koreeda_Y/0/1/0/all/0/1\">Yuta Koreeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokote_K/0/1/0/all/0/1\">Ken-ichi Yokote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozaki_H/0/1/0/all/0/1\">Hiroaki Ozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1\">Atsuki Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsunokake_M/0/1/0/all/0/1\">Masaya Tsunokake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogawa_Y/0/1/0/all/0/1\">Yasuhiro Sogawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAGE: A Position-Aware Graph-Based Model for Emotion Cause Entailment in Conversation. (arXiv:2303.01795v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01795","description":"<p>Conversational Causal Emotion Entailment (C2E2) is a task that aims at\nrecognizing the causes corresponding to a target emotion in a conversation. The\norder of utterances in the conversation affects the causal inference. However,\nmost current position encoding strategies ignore the order relation among\nutterances and speakers. To address the issue, we devise a novel position-aware\ngraph to encode the entire conversation, fully modeling causal relations among\nutterances. The comprehensive experiments show that our method consistently\nachieves state-of-the-art performance on two challenging test sets, proving the\neffectiveness of our model. Our source code is available on Github:\nhttps://github.com/XiaojieGu/PAGE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaojie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1\">Renze Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shangxin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping Wordnets on the Fly with Permanent Sense Keys. (arXiv:2303.01847v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01847","description":"<p>Most of the major databases on the semantic web have links to Princeton\nWordNet (PWN) synonym set (synset) identifiers, which differ for each PWN\nrelease, and are thus incompatible between versions. On the other hand, both\nPWN and the more recent Open English Wordnet (OEWN) provide permanent word\nsense identifiers (the sense keys), which can solve this interoperability\nproblem.\n</p>\n<p>We present an algorithm that runs in linear time, to automatically derive a\nsynset mapping between any pair of Wordnet versions that use PWN sense keys.\nThis allows to update old WordNet links, and seamlessly interoperate with newer\nEnglish Wordnet versions for which no prior mapping exists.\n</p>\n<p>By applying the proposed algorithm on the fly, at load time, we combine the\nOpen Multilingual Wordnet (OMW 1.4, which uses old PWN 3.0 identifiers) with\nOEWN Edition 2021, and obtain almost perfect precision and recall. We compare\nthe results of our approach using respectively synset offsets, versus the\nCollaborative InterLingual Index (CILI version 1.0) as synset identifiers, and\nfind that the synset offsets perform better than CILI 1.0 in all cases, except\na few ties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kafe_E/0/1/0/all/0/1\">Eric Kafe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering. (arXiv:2303.01903v1 [cs.CV])","link":"http://arxiv.org/abs/2303.01903","description":"<p>Knowledge-based visual question answering (VQA) requires external knowledge\nbeyond the image to answer the question. Early studies retrieve required\nknowledge from explicit knowledge bases (KBs), which often introduces\nirrelevant information to the question, hence restricting the performance of\ntheir models. Recent works have sought to use a large language model (i.e.,\nGPT-3) as an implicit knowledge engine to acquire the necessary knowledge for\nanswering. Despite the encouraging results achieved by these methods, we argue\nthat they have not fully activated the capacity of GPT-3 as the provided input\ninformation is insufficient. In this paper, we present Prophet -- a\nconceptually simple framework designed to prompt GPT-3 with answer heuristics\nfor knowledge-based VQA. Specifically, we first train a vanilla VQA model on a\nspecific knowledge-based VQA dataset without external knowledge. After that, we\nextract two types of complementary answer heuristics from the model: answer\ncandidates and answer-aware examples. Finally, the two types of answer\nheuristics are encoded into the prompts to enable GPT-3 to better comprehend\nthe task thus enhancing its capacity. Prophet significantly outperforms all\nexisting state-of-the-art methods on two challenging knowledge-based VQA\ndatasets, OK-VQA and A-OKVQA, delivering 61.1% and 55.7% accuracies on their\ntesting sets, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhenwei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Translation Performance of a Large Multilingual Language Model: the Case of BLOOM. (arXiv:2303.01911v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01911","description":"<p>The NLP community recently saw the release of a new large open-access\nmultilingual language model, BLOOM (BigScience et al., 2022) covering 46\nlanguages. We focus on BLOOM's multilingual ability by evaluating its machine\ntranslation performance across several datasets (WMT, Flores-101 and DiaBLa)\nand language pairs (high- and low-resourced). Our results show that 0-shot\nperformance suffers from overgeneration and generating in the wrong language,\nbut this is greatly improved in the few-shot setting, with very good results\nfor a number of language pairs. We study several aspects including prompt\ndesign, model sizes, cross-lingual transfer and the use of discursive context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1\">Rachel Bawden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ancient Chinese Word Segmentation and Part-of-Speech Tagging Using Distant Supervision. (arXiv:2303.01912v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01912","description":"<p>Ancient Chinese word segmentation (WSG) and part-of-speech tagging (POS) are\nimportant to study ancient Chinese, but the amount of ancient Chinese WSG and\nPOS tagging data is still rare. In this paper, we propose a novel augmentation\nmethod of ancient Chinese WSG and POS tagging data using distant supervision\nover parallel corpus. However, there are still mislabeled and unlabeled ancient\nChinese words inevitably in distant supervision. To address this problem, we\ntake advantage of the memorization effects of deep neural networks and a small\namount of annotated data to get a model with much knowledge and a little noise,\nand then we use this model to relabel the ancient Chinese sentences in parallel\ncorpus. Experiments show that the model trained over the relabeled data\noutperforms the model trained over the data generated from distant supervision\nand the annotated data. Our code is available at\nhttps://github.com/farlit/ACDS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shuo Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Mitigate Spurious Correlations for Open-Domain Dialogue Response Generation Models by Causal Discovery. (arXiv:2303.01962v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01962","description":"<p>In this paper, we conduct the first study on spurious correlations for\nopen-domain response generation models based on a corpus CGDIALOG curated in\nour work. The cur rent models indeed suffer from spurious correlations and have\na tendency of generating irrelevant and generic responses. Inspired by causal\ndiscovery algorithms, we propose a novel model-agnostic method for training and\ninference of response generation model using a conditional independence\nclassifier. The classifier is trained by a constrained self-training method,\ncoined CONSTRAIN, to overcome data scarcity. The experimental results based on\nboth human and automatic evaluation show that our method significantly\noutperforms the competitive baselines in terms of relevance, informativeness,\nand fluency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Tao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Who could be behind QAnon? Authorship attribution with supervised machine-learning. (arXiv:2303.02078v1 [cs.CL])","link":"http://arxiv.org/abs/2303.02078","description":"<p>A series of social media posts signed under the pseudonym \"Q\", started a\nmovement known as QAnon, which led some of its most radical supporters to\nviolent and illegal actions. To identify the person(s) behind Q, we evaluate\nthe coincidence between the linguistic properties of the texts written by Q and\nto those written by a list of suspects provided by journalistic investigation.\nTo identify the authors of these posts, serious challenges have to be\naddressed. The \"Q drops\" are very short texts, written in a way that constitute\na sort of literary genre in itself, with very peculiar features of style. These\ntexts might have been written by different authors, whose other writings are\noften hard to find. After an online ethnology of the movement, necessary to\ncollect enough material written by these thirteen potential authors, we use\nsupervised machine learning to build stylistic profiles for each of them. We\nthen performed a rolling analysis on Q's writings, to see if any of those\nlinguistic profiles match the so-called 'QDrops' in part or entirety. We\nconclude that two different individuals, Paul F. and Ron W., are the closest\nmatch to Q's linguistic signature, and they could have successively written Q's\ntexts. These potential authors are not high-ranked personality from the U.S.\nadministration, but rather social media activists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cafiero_F/0/1/0/all/0/1\">Florian Cafiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camps_J/0/1/0/all/0/1\">Jean-Baptiste Camps</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners. (arXiv:2303.02151v1 [cs.CV])","link":"http://arxiv.org/abs/2303.02151","description":"<p>Visual recognition in low-data regimes requires deep neural networks to learn\ngeneralized representations from limited training samples. Recently, CLIP-based\nmethods have shown promising few-shot performance benefited from the\ncontrastive language-image pre-training. We then question, if the more diverse\npre-training knowledge can be cascaded to further assist few-shot\nrepresentation learning. In this paper, we propose CaFo, a Cascade of\nFoundation models that incorporates diverse prior knowledge of various\npre-training paradigms for better few-shot learning. Our CaFo incorporates\nCLIP's language-contrastive knowledge, DINO's vision-contrastive knowledge,\nDALL-E's vision-generative knowledge, and GPT-3's language-generative\nknowledge. Specifically, CaFo works by 'Prompt, Generate, then Cache'. Firstly,\nwe leverage GPT-3 to produce textual inputs for prompting CLIP with rich\ndownstream linguistic semantics. Then, we generate synthetic images via DALL-E\nto expand the few-shot training data without any manpower. At last, we\nintroduce a learnable cache model to adaptively blend the predictions from CLIP\nand DINO. By such collaboration, CaFo can fully unleash the potential of\ndifferent pre-training methods and unify them to perform state-of-the-art for\nfew-shot classification. Code is available at\nhttps://github.com/ZrrSkywalker/CaFo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiangfei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hanqiu Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Preserving Adversarial Text Attacks. (arXiv:2108.10015v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10015","description":"<p>Deep neural networks (DNNs) are known to be vulnerable to adversarial images,\nwhile their robustness in text classification is rarely studied. Several lines\nof text attack methods have been proposed in the literature, including\ncharacter-level, word-level, and sentence-level attacks. However, it is still a\nchallenge to minimize the number of word changes necessary to induce\nmisclassification, while simultaneously ensuring lexical correctness, syntactic\nsoundness, and semantic similarity. In this paper, we propose a Bigram and\nUnigram based adaptive Semantic Preservation Optimization (BU-SPO) method to\nexamine the vulnerability of deep models. Our method has four major merits.\nFirstly, we propose to attack text documents not only at the unigram word level\nbut also at the bigram level which better keeps semantics and avoids producing\nmeaningless outputs. Secondly, we propose a hybrid method to replace the input\nwords with options among both their synonyms candidates and sememe candidates,\nwhich greatly enriches the potential substitutions compared to only using\nsynonyms. Thirdly, we design an optimization algorithm, i.e., Semantic\nPreservation Optimization (SPO), to determine the priority of word\nreplacements, aiming to reduce the modification cost. Finally, we further\nimprove the SPO with a semantic Filter (named SPOF) to find the adversarial\nexample with the highest semantic similarity. We evaluate the effectiveness of\nour BU-SPO and BU-SPOF on IMDB, AG's News, and Yahoo! Answers text datasets by\nattacking four popular DNNs models. Results show that our methods achieve the\nhighest attack success rates and semantics rates by changing the smallest\nnumber of words compared with existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinghao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1\">James Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are We Really Making Much Progress? Bag-of-Words vs. Sequence vs. Graph vs. Hierarchy for Single- and Multi-Label Text Classification. (arXiv:2204.03954v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03954","description":"<p>The popularity of graph neural networks has triggered a resurgence of\ngraph-based methods for single-label and multi-label text classification.\nHowever, it is unclear whether these graph-based methods are beneficial\ncompared to standard machine learning methods and modern pretrained language\nmodels. We compare a rich selection of bag-of-words, sequence-based,\ngraph-based, and hierarchical methods for text classification. We aggregate\nresults from the literature over 5 single-label and 7 multi-label datasets and\nrun our own experiments. Our findings unambiguously demonstrate that for\nsingle-label and multi-label classification tasks, the graph-based methods fail\nto outperform fine-tuned language models and sometimes even perform worse than\nstandard machine learning methods like multilayer perceptron (MLP) on a\nbag-of-words. This questions the enormous amount of effort put into the\ndevelopment of new graph-based methods in the last years and the promises they\nmake for text classification. Given our extensive experiments, we confirm that\npretrained language models remain state-of-the-art in text classification\ndespite all recent specialized advances. We argue that future work in text\nclassification should thoroughly test against strong baselines like MLPs to\nproperly assess the true scientific progress.\n</p>\n<p>The source code is available: https://github.com/drndr/multilabel-text-clf\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diera_A/0/1/0/all/0/1\">Andor Diera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bao Xin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khera_B/0/1/0/all/0/1\">Bhakti Khera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuser_T/0/1/0/all/0/1\">Tim Meuser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1\">Tushar Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karl_F/0/1/0/all/0/1\">Fabian Karl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FashionCLIP: Connecting Language and Images for Product Representations. (arXiv:2204.03972v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2204.03972","description":"<p>The steady rise of online shopping goes hand in hand with the development of\nincreasingly complex ML and NLP models. While most use cases are cast as\nspecialized supervised learning problems, we argue that practitioners would\ngreatly benefit from more transferable representations of products. In this\nwork, we build on recent developments in contrastive learning to train\nFashionCLIP, a CLIP-like model for the fashion industry. We showcase its\ncapabilities for retrieval, classification and grounding, and release our model\nand code to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chia_P/0/1/0/all/0/1\">Patrick John Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1\">Giuseppe Attanasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terragni_S/0/1/0/all/0/1\">Silvia Terragni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magalhaes_A/0/1/0/all/0/1\">Ana Rita Magalh&#xe3;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_D/0/1/0/all/0/1\">Diogo Goncalves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greco_C/0/1/0/all/0/1\">Ciro Greco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Better Masking for Better Language Model Pre-training. (arXiv:2208.10806v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.10806","description":"<p>Masked Language Modeling (MLM) has been widely used as the denoising\nobjective in pre-training language models (PrLMs). Existing PrLMs commonly\nadopt a Random-Token Masking strategy where a fixed masking ratio is applied\nand different contents are masked by an equal probability throughout the entire\ntraining. However, the model may receive complicated impact from pre-training\nstatus, which changes accordingly as training time goes on. In this paper, we\nshow that such time-invariant MLM settings on masking ratio and masked content\nare unlikely to deliver an optimal outcome, which motivates us to explore the\ninfluence of time-variant MLM settings. We propose two scheduled masking\napproaches that adaptively tune the masking ratio and masked content in\ndifferent training stages, which improves the pre-training efficiency and\neffectiveness verified on the downstream tasks. Our work is a pioneer study on\ntime-variant masking strategy on ratio and content and gives a better\nunderstanding of how masking ratio and masked content influence the MLM\npre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dongjie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval. (arXiv:2208.13661v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.13661","description":"<p>Retrieval models based on dense representations in semantic space have become\nan indispensable branch for first-stage retrieval. These retrievers benefit\nfrom surging advances in representation learning towards compressive global\nsequence-level embeddings. However, they are prone to overlook local salient\nphrases and entity mentions in texts, which usually play pivot roles in\nfirst-stage retrieval. To mitigate this weakness, we propose to make a dense\nretriever align a well-performing lexicon-aware representation model. The\nalignment is achieved by weakened knowledge distillations to enlighten the\nretriever via two aspects -- 1) a lexicon-augmented contrastive objective to\nchallenge the dense encoder and 2) a pair-wise rank-consistent regularization\nto make dense model's behavior incline to the other. We evaluate our model on\nthree public benchmarks, which shows that with a comparable lexicon-aware\nretriever as the teacher, our proposed dense one can bring consistent and\nsignificant improvements, and even outdo its teacher. In addition, we found our\nimprovement on the dense retriever is complementary to the standard ranker\ndistillation, which can further lift state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxing Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linearly Mapping from Image to Text Space. (arXiv:2209.15162v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.15162","description":"<p>The extent to which text-only language models (LMs) learn to represent\nfeatures of the non-linguistic world is an open question. Prior work has shown\nthat pretrained LMs can be taught to caption images when a vision model's\nparameters are optimized to encode images in the language space. We test a\nstronger hypothesis: that the conceptual representations learned by frozen\ntext-only models and vision-only models are similar enough that this can be\nachieved with a linear map. We show that the image representations from vision\nmodels can be transferred as continuous prompts to frozen LMs by training only\na single linear projection. Using these to prompt the LM achieves competitive\nperformance on captioning and visual question answering tasks compared to\nmodels that tune both the image encoder and text decoder (such as the MAGMA\nmodel). We compare three image encoders with increasing amounts of linguistic\nsupervision seen during pretraining: BEIT (no linguistic information),\nNF-ResNET (lexical category information), and CLIP (full natural language\ndescriptions). We find that all three encoders perform equally well at\ntransferring visual property information to the language model (e.g., whether\nan animal is large or small), but that image encoders pretrained with\nlinguistic supervision more saliently encode category information (e.g.,\ndistinguishing hippo vs. elephant) and thus perform significantly better on\nbenchmark language-and-vision tasks. Our results indicate that LMs encode\nconceptual information structurally similarly to vision-based models, even\nthose that are solely trained on images. Code is available here:\nhttps://github.com/jmerullo/limber\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merullo_J/0/1/0/all/0/1\">Jack Merullo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castricato_L/0/1/0/all/0/1\">Louis Castricato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Security Vulnerabilities of Text-to-SQL Models. (arXiv:2211.15363v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15363","description":"<p>Although it has been demonstrated that Natural Language Processing (NLP)\nalgorithms are vulnerable to deliberate attacks, the question of whether such\nweaknesses can lead to software security threats is under-explored. To bridge\nthis gap, we conducted vulnerability tests on Text-to-SQL systems that are\ncommonly used to create natural language interfaces to databases. We showed\nthat the Text-to-SQL modules within six commercial applications can be\nmanipulated to produce malicious code, potentially leading to data breaches and\nDenial of Service attacks. This is the first demonstration that NLP models can\nbe exploited as attack vectors in the wild. In addition, experiments using four\nopen-source language models verified that straightforward backdoor attacks on\nText-to-SQL systems achieve a 100% success rate without affecting their\nperformance. The aim of this work is to draw the community's attention to\npotential software security issues associated with NLP algorithms and encourage\nexploration of methods to mitigate against them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xutan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_M/0/1/0/all/0/1\">Mark Stevenson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Qualitative Analysis of a Graph Transformer Approach to Addressing Hate Speech: Adapting to Dynamically Changing Content. (arXiv:2301.10871v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2301.10871","description":"<p>Our work advances an approach for predicting hate speech in social media,\ndrawing out the critical need to consider the discussions that follow a post to\nsuccessfully detect when hateful discourse may arise. Using graph transformer\nnetworks, coupled with modelling attention and BERT-level natural language\nprocessing, our approach can capture context and anticipate upcoming\nanti-social behaviour. In this paper, we offer a detailed qualitative analysis\nof this solution for hate speech detection in social networks, leading to\ninsights into where the method has the most impressive outcomes in comparison\nwith competitors and identifying scenarios where there are challenges to\nachieving ideal performance. Included is an exploration of the kinds of posts\nthat permeate social media today, including the use of hateful images. This\nsuggests avenues for extending our model to be more comprehensive. A key\ninsight is that the focus on reasoning about the concept of context positions\nus well to be able to support multi-modal analysis of online posts. We conclude\nwith a reflection on how the problem we are addressing relates especially well\nto the theme of dynamic change, a critical concern for all AI solutions for\nsocial impact. We also comment briefly on how mental health well-being can be\nadvanced with our work, through curated content attuned to the extent of hate\nin posts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hebert_L/0/1/0/all/0/1\">Liam Hebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1\">Robin Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golab_L/0/1/0/all/0/1\">Lukasz Golab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning of Language Models. (arXiv:2302.03241v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03241","description":"<p>Language models (LMs) have been instrumental for the rapid advance of natural\nlanguage processing. This paper studies continual learning of LMs, in\nparticular, continual domain-adaptive pre-training (or continual DAP-training).\nExisting research has shown that further pre-training an LM using a domain\ncorpus to adapt the LM to the domain can improve the end-task performance in\nthe domain. This paper proposes a novel method to continually DAP-train an LM\nwith a sequence of unlabeled domain corpora to adapt the LM to these domains to\nimprove their end-task performances. The key novelty of our method is a\nsoft-masking mechanism that directly controls the update to the LM. A novel\nproxy is also proposed to preserve the general knowledge in the original LM.\nAdditionally, it contrasts the representations of the previously learned domain\nknowledge (including the general knowledge in the pre-trained LM) and the\nknowledge from the current full network to achieve knowledge integration. The\nmethod not only overcomes catastrophic forgetting, but also achieves knowledge\ntransfer to improve end-task performances. Empirical evaluation demonstrates\nthe effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zixuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yijia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haowei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konishi_T/0/1/0/all/0/1\">Tatsuya Konishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyuhak Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NL2CMD: An Updated Workflow for Natural Language to Bash Commands Translation. (arXiv:2302.07845v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07845","description":"<p>Translating natural language into Bash Commands is an emerging research field\nthat has gained attention in recent years. Most efforts have focused on\nproducing more accurate translation models. To the best of our knowledge, only\ntwo datasets are available, with one based on the other. Both datasets involve\nscraping through known data sources (through platforms like stack overflow,\ncrowdsourcing, etc.) and hiring experts to validate and correct either the\nEnglish text or Bash Commands. This paper provides two contributions to\nresearch on synthesizing Bash Commands from scratch. First, we describe a\nstate-of-the-art translation model used to generate Bash Commands from the\ncorresponding English text. Second, we introduce a new NL2CMD dataset that is\nautomatically generated, involves minimal human intervention, and is over six\ntimes larger than prior datasets. Since the generation pipeline does not rely\non existing Bash Commands, the distribution and types of commands can be custom\nadjusted. We evaluate the performance of ChatGPT on this task and discuss the\npotential of using it as a data generator. Our empirical results show how the\nscale and diversity of our dataset can offer unique opportunities for semantic\nparsing researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Quchen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhongwei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgaklis_M/0/1/0/all/0/1\">Marco Georgaklis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Jules White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1\">Douglas C. Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models are Few-shot Learners for Prognostic Prediction. (arXiv:2302.12692v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12692","description":"<p>Clinical prediction is an essential task in the healthcare industry. However,\nthe recent success of transformers, on which large language models are built,\nhas not been extended to this domain. In this research, we explore the use of\ntransformers and language models in prognostic prediction for immunotherapy\nusing real-world patients' clinical data and molecular profiles. This paper\ninvestigates the potential of transformers to improve clinical prediction\ncompared to conventional machine learning approaches and addresses the\nchallenge of few-shot learning in predicting rare disease areas. The study\nbenchmarks the efficacy of baselines and language models on prognostic\nprediction across multiple cancer types and investigates the impact of\ndifferent pretrained language models under few-shot regimes. The results\ndemonstrate significant improvements in accuracy and highlight the potential of\nNLP in clinical research to improve early detection and intervention for\ndifferent diseases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Z. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balan_M/0/1/0/all/0/1\">M. M. Balan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_K/0/1/0/all/0/1\">K. Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages. (arXiv:2303.01037v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.01037","description":"<p>We introduce the Universal Speech Model (USM), a single large model that\nperforms automatic speech recognition (ASR) across 100+ languages. This is\nachieved by pre-training the encoder of the model on a large unlabeled\nmultilingual dataset of 12 million (M) hours spanning over 300 languages, and\nfine-tuning on a smaller labeled dataset. We use multilingual pre-training with\nrandom-projection quantization and speech-text modality matching to achieve\nstate-of-the-art performance on downstream multilingual ASR and speech-to-text\ntranslation tasks. We also demonstrate that despite using a labeled training\nset 1/7-th the size of that used for the Whisper model, our model exhibits\ncomparable or better performance on both in-domain and out-of-domain speech\nrecognition tasks across many languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nanxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axelrod_V/0/1/0/all/0/1\">Vera Axelrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gary Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Ke Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Daniel S. Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghani_P/0/1/0/all/0/1\">Parisa Haghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesa_J/0/1/0/all/0/1\">Jason Riesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perng_G/0/1/0/all/0/1\">Ginger Perng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltau_H/0/1/0/all/0/1\">Hagen Soltau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_P/0/1/0/all/0/1\">Pedro Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schalkwyk_J/0/1/0/all/0/1\">Johan Schalkwyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}