{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-12-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Evaluating Self-supervised Speech Models on a Taiwanese Hokkien Corpus. (arXiv:2312.06668v1 [cs.CL])","link":"http://arxiv.org/abs/2312.06668","description":"<p>Taiwanese Hokkien is declining in use and status due to a language shift\ntowards Mandarin in Taiwan. This is partly why it is a low resource language in\nNLP and speech research today. To ensure that the state of the art in speech\nprocessing does not leave Taiwanese Hokkien behind, we contribute a 1.5-hour\ndataset of Taiwanese Hokkien to ML-SUPERB's hidden set. Evaluating ML-SUPERB's\nsuite of self-supervised learning (SSL) speech representations on our dataset,\nwe find that model size does not consistently determine performance. In fact,\ncertain smaller models outperform larger ones. Furthermore, linguistic\nalignment between pretraining data and the target language plays a crucial\nrole.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1\">Yi-Hui Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kalvin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Meng-Ju Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1\">Winston Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_A/0/1/0/all/0/1\">Alice Wen-Hsin Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carol Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bryan Y. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pai_R/0/1/0/all/0/1\">Rong-Wei Pai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_P/0/1/0/all/0/1\">Po-Yen Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1\">Jo-Peng Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phoann_I/0/1/0/all/0/1\">Iu-Tshian Phoann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1\">Winnie Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chenxuan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Noel Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations. (arXiv:2312.06674v1 [cs.CL])","link":"http://arxiv.org/abs/2312.06674","description":"<p>We introduce Llama Guard, an LLM-based input-output safeguard model geared\ntowards Human-AI conversation use cases. Our model incorporates a safety risk\ntaxonomy, a valuable tool for categorizing a specific set of safety risks found\nin LLM prompts (i.e., prompt classification). This taxonomy is also\ninstrumental in classifying the responses generated by LLMs to these prompts, a\nprocess we refer to as response classification. For the purpose of both prompt\nand response classification, we have meticulously gathered a dataset of high\nquality. Llama Guard, a Llama2-7b model that is instruction-tuned on our\ncollected dataset, albeit low in volume, demonstrates strong performance on\nexisting benchmarks such as the OpenAI Moderation Evaluation dataset and\nToxicChat, where its performance matches or exceeds that of currently available\ncontent moderation tools. Llama Guard functions as a language model, carrying\nout multi-class classification and generating binary decision scores.\nFurthermore, the instruction fine-tuning of Llama Guard allows for the\ncustomization of tasks and the adaptation of output formats. This feature\nenhances the model's capabilities, such as enabling the adjustment of taxonomy\ncategories to align with specific use cases, and facilitating zero-shot or\nfew-shot prompting with diverse taxonomies at the input. We are making Llama\nGuard model weights available and we encourage researchers to further develop\nand adapt them to meet the evolving needs of the community for AI safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1\">Hakan Inan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upasani_K/0/1/0/all/0/1\">Kartikeya Upasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_J/0/1/0/all/0/1\">Jianfeng Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rungta_R/0/1/0/all/0/1\">Rashi Rungta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_K/0/1/0/all/0/1\">Krithika Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tontchev_M/0/1/0/all/0/1\">Michael Tontchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuller_B/0/1/0/all/0/1\">Brian Fuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Testuggine_D/0/1/0/all/0/1\">Davide Testuggine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1\">Madian Khabsa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intelligent Virtual Assistants with LLM-based Process Automation. (arXiv:2312.06677v1 [cs.LG])","link":"http://arxiv.org/abs/2312.06677","description":"<p>While intelligent virtual assistants like Siri, Alexa, and Google Assistant\nhave become ubiquitous in modern life, they still face limitations in their\nability to follow multi-step instructions and accomplish complex goals\narticulated in natural language. However, recent breakthroughs in large\nlanguage models (LLMs) show promise for overcoming existing barriers by\nenhancing natural language processing and reasoning capabilities. Though\npromising, applying LLMs to create more advanced virtual assistants still faces\nchallenges like ensuring robust performance and handling variability in\nreal-world user commands. This paper proposes a novel LLM-based virtual\nassistant that can automatically perform multi-step operations within mobile\napps based on high-level user requests. The system represents an advance in\nassistants by providing an end-to-end solution for parsing instructions,\nreasoning about goals, and executing actions. LLM-based Process Automation\n(LLMPA) has modules for decomposing instructions, generating descriptions,\ndetecting interface elements, predicting next actions, and error checking.\nExperiments demonstrate the system completing complex mobile operation tasks in\nAlipay based on natural language instructions. This showcases how large\nlanguage models can enable automated assistants to accomplish real-world tasks.\nThe main contributions are the novel LLMPA architecture optimized for app\nprocess automation, the methodology for applying LLMs to mobile apps, and\ndemonstrations of multi-step task completion in a real-world environment.\nNotably, this work represents the first real-world deployment and extensive\nevaluation of a large language model-based virtual assistant in a widely used\nmobile application with an enormous user base numbering in the hundreds of\nmillions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yanchu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zhixuan Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_F/0/1/0/all/0/1\">Feiyue Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Longfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1\">Chenyi Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Steering Llama 2 via Contrastive Activation Addition. (arXiv:2312.06681v1 [cs.CL])","link":"http://arxiv.org/abs/2312.06681","description":"<p>We introduce Contrastive Activation Addition (CAA), an innovative method for\nsteering language models by modifying activations during their forward passes.\nCAA computes ``steering vectors'' by averaging the difference in residual\nstream activations between pairs of positive and negative examples of a\nparticular behavior such as factual versus hallucinatory responses. During\ninference, these steering vectors are added at all token positions after the\nuser's prompt with either a positive or negative coefficient, allowing precise\ncontrol over the degree of the targeted behavior. We evaluate CAA's\neffectiveness on Llama 2 Chat using both multiple-choice behavioral question\ndatasets and open-ended generation tasks. We demonstrate that CAA significantly\nalters model behavior, outperforms traditional methods like finetuning and\nfew-shot prompting, and minimally reduces capabilities. Moreover, by employing\nvarious activation space interpretation methods, we gain deeper insights into\nCAA's mechanisms. CAA both accurately steers model outputs and also sheds light\non how high-level concepts are represented in Large Language Models (LLMs).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rimsky_N/0/1/0/all/0/1\">Nina Rimsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabrieli_N/0/1/0/all/0/1\">Nick Gabrieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_J/0/1/0/all/0/1\">Julian Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1\">Meg Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1\">Evan Hubinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_A/0/1/0/all/0/1\">Alexander Matt Turner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceiving University Student's Opinions from Google App Reviews. (arXiv:2312.06705v1 [cs.CL])","link":"http://arxiv.org/abs/2312.06705","description":"<p>Google app market captures the school of thought of users from every corner\nof the globe via ratings and text reviews, in a multilinguistic arena. The\npotential information from the reviews cannot be extracted manually, due to its\nexponential growth. So, Sentiment analysis, by machine learning and deep\nlearning algorithms employing NLP, explicitly uncovers and interprets the\nemotions. This study performs the sentiment classification of the app reviews\nand identifies the university student's behavior towards the app market via\nexploratory analysis. We applied machine learning algorithms using the TP, TF,\nand TF IDF text representation scheme and evaluated its performance on Bagging,\nan ensemble learning method. We used word embedding, Glove, on the deep\nlearning paradigms. Our model was trained on Google app reviews and tested on\nStudent's App Reviews(SAR). The various combinations of these algorithms were\ncompared amongst each other using F score and accuracy and inferences were\nhighlighted graphically. SVM, amongst other classifiers, gave fruitful\naccuracy(93.41%), F score(89%) on bigram and TF IDF scheme. Bagging enhanced\nthe performance of LR and NB with accuracy of 87.88% and 86.69% and F score of\n86% and 78% respectively. Overall, LSTM on Glove embedding recorded the highest\naccuracy(95.2%) and F score(88%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_S/0/1/0/all/0/1\">Sakshi Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Subhankar Mishra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models. (arXiv:2312.06722v1 [cs.CV])","link":"http://arxiv.org/abs/2312.06722","description":"<p>Multimodal Large Language Models (MLLMs), building upon the powerful Large\nLanguage Models (LLMs) with exceptional reasoning and generalization\ncapability, have opened up new avenues for embodied task planning. MLLMs excel\nin their ability to integrate diverse environmental inputs, such as real-time\ntask progress, visual observations, and open-form language instructions, which\nare crucial for executable task planning. In this work, we introduce a\nbenchmark with human annotations, EgoPlan-Bench, to quantitatively investigate\nthe potential of MLLMs as embodied task planners in real-world scenarios. Our\nbenchmark is distinguished by realistic tasks derived from real-world videos, a\ndiverse set of actions involving interactions with hundreds of different\nobjects, and complex visual observations from varied environments. We evaluate\nvarious open-source MLLMs, revealing that these models have not yet evolved\ninto embodied planning generalists (even GPT-4V). We further construct an\ninstruction-tuning dataset EgoPlan-IT from videos of human-object interactions,\nto facilitate the learning of high-level task planning in intricate real-world\nsituations. The experiment results demonstrate that the model tuned on\nEgoPlan-IT not only significantly improves performance on our benchmark, but\nalso effectively acts as embodied planner in simulations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yuying Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xihui Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Honeybee: Locality-enhanced Projector for Multimodal LLM. (arXiv:2312.06742v1 [cs.CV])","link":"http://arxiv.org/abs/2312.06742","description":"<p>In Multimodal Large Language Models (MLLMs), a visual projector plays a\ncrucial role in bridging pre-trained vision encoders with LLMs, enabling\nprofound visual understanding while harnessing the LLMs' robust capabilities.\nDespite the importance of the visual projector, it has been relatively less\nexplored. In this study, we first identify two essential projector properties:\n(i) flexibility in managing the number of visual tokens, crucial for MLLMs'\noverall efficiency, and (ii) preservation of local context from visual\nfeatures, vital for spatial understanding. Based on these findings, we propose\na novel projector design that is both flexible and locality-enhanced,\neffectively satisfying the two desirable properties. Additionally, we present\ncomprehensive strategies to effectively utilize multiple and multifaceted\ninstruction datasets. Through extensive experiments, we examine the impact of\nindividual design choices. Finally, our proposed MLLM, Honeybee, remarkably\noutperforms previous state-of-the-art methods across various benchmarks,\nincluding MME, MMBench, SEED-Bench, and LLaVA-Bench, achieving significantly\nhigher efficiency. Code and models are available at\nhttps://github.com/kakaobrain/honeybee.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1\">Junbum Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Wooyoung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mun_J/0/1/0/all/0/1\">Jonghwan Mun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roh_B/0/1/0/all/0/1\">Byungseok Roh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety. (arXiv:2312.06798v1 [cs.AI])","link":"http://arxiv.org/abs/2312.06798","description":"<p>Explainability and Safety engender Trust. These require a model to exhibit\nconsistency and reliability. To achieve these, it is necessary to use and\nanalyze data and knowledge with statistical and symbolic AI methods relevant to\nthe AI application - neither alone will do. Consequently, we argue and seek to\ndemonstrate that the NeuroSymbolic AI approach is better suited for making AI a\ntrusted AI system. We present the CREST framework that shows how Consistency,\nReliability, user-level Explainability, and Safety are built on NeuroSymbolic\nmethods that use data and knowledge to support requirements for critical\napplications such as health and well-being. This article focuses on Large\nLanguage Models (LLMs) as the chosen AI system within the CREST framework. LLMs\nhave garnered substantial attention from researchers due to their versatility\nin handling a broad array of natural language processing (NLP) scenarios. For\nexample, ChatGPT and Google's MedPaLM have emerged as highly promising\nplatforms for providing information in general and health-related queries,\nrespectively. Nevertheless, these models remain black boxes despite\nincorporating human feedback and instruction-guided tuning. For instance,\nChatGPT can generate unsafe responses despite instituting safety guardrails.\nCREST presents a plausible approach harnessing procedural and graph-based\nknowledge within a NeuroSymbolic framework to shed light on the challenges\nassociated with LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1\">Manas Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and In-context Learning. (arXiv:2312.06820v1 [cs.AI])","link":"http://arxiv.org/abs/2312.06820","description":"<p>Microsoft Windows Feedback Hub is designed to receive customer feedback on a\nwide variety of subjects including critical topics such as power and battery.\nFeedback is one of the most effective ways to have a grasp of users' experience\nwith Windows and its ecosystem. However, the sheer volume of feedback received\nby Feedback Hub makes it immensely challenging to diagnose the actual cause of\nreported issues. To better understand and triage issues, we leverage Double\nMachine Learning (DML) to associate users' feedback with telemetry signals. One\nof the main challenges we face in the DML pipeline is the necessity of domain\nknowledge for model design (e.g., causal graph), which sometimes is either not\navailable or hard to obtain. In this work, we take advantage of reasoning\ncapabilities in Large Language Models (LLMs) to generate a prior model that\nwhich to some extent compensates for the lack of domain knowledge and could be\nused as a heuristic for measuring feedback informativeness. Our LLM-based\napproach is able to extract previously known issues, uncover new bugs, and\nidentify sequences of events that lead to a bug, while minimizing out-of-domain\noutputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1\">Sara Abdali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Anjali Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Steve Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiciman_E/0/1/0/all/0/1\">Emre Kiciman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utilization of Non-verbal Behaviour and Social Gaze in Classroom Human-Robot Interaction Communications. (arXiv:2312.06825v1 [cs.RO])","link":"http://arxiv.org/abs/2312.06825","description":"<p>This abstract explores classroom Human-Robot Interaction (HRI) scenarios with\nan emphasis on the adaptation of human-inspired social gaze models in robot\ncognitive architecture to facilitate a more seamless social interaction. First,\nwe detail the HRI scenarios explored by us in our studies followed by a\ndescription of the social gaze model utilized for our research. We highlight\nthe advantages of utilizing such an attentional model in classroom HRI\nscenarios. We also detail the intended goals of our upcoming study involving\nthis social gaze model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaghaghi_S/0/1/0/all/0/1\">Sahand Shaghaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliasghari_P/0/1/0/all/0/1\">Pourya Aliasghari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripp_B/0/1/0/all/0/1\">Bryan Tripp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dautenhahn_K/0/1/0/all/0/1\">Kerstin Dautenhahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nehaniv_C/0/1/0/all/0/1\">Chrystopher Nehaniv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Pretraining of Medical Time Series and Notes. (arXiv:2312.06855v1 [cs.LG])","link":"http://arxiv.org/abs/2312.06855","description":"<p>Within the intensive care unit (ICU), a wealth of patient data, including\nclinical measurements and clinical notes, is readily available. This data is a\nvaluable resource for comprehending patient health and informing medical\ndecisions, but it also contains many challenges in analysis. Deep learning\nmodels show promise in extracting meaningful patterns, but they require\nextensive labeled data, a challenge in critical care. To address this, we\npropose a novel approach employing self-supervised pretraining, focusing on the\nalignment of clinical measurements and notes. Our approach combines contrastive\nand masked token prediction tasks during pretraining. Semi-supervised\nexperiments on the MIMIC-III dataset demonstrate the effectiveness of our\nself-supervised pretraining. In downstream tasks, including in-hospital\nmortality prediction and phenotyping, our pretrained model outperforms\nbaselines in settings where only a fraction of the data is labeled, emphasizing\nits ability to enhance ICU data analysis. Notably, our method excels in\nsituations where very few labels are available, as evidenced by an increase in\nthe AUC-ROC for in-hospital mortality by 0.17 and in AUC-PR for phenotyping by\n0.1 when only 1% of labels are accessible. This work advances self-supervised\nlearning in the healthcare domain, optimizing clinical insights from abundant\nyet challenging ICU data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+King_R/0/1/0/all/0/1\">Ryan King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortazavi_B/0/1/0/all/0/1\">Bobak Mortazavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates. (arXiv:2312.06861v1 [cs.CY])","link":"http://arxiv.org/abs/2312.06861","description":"<p>Perception of offensiveness is inherently subjective, shaped by the lived\nexperiences and socio-cultural values of the perceivers. Recent years have seen\nsubstantial efforts to build AI-based tools that can detect offensive language\nat scale, as a means to moderate social media platforms, and to ensure safety\nof conversational AI technologies such as ChatGPT and Bard. However, existing\napproaches treat this task as a technical endeavor, built on top of data\nannotated for offensiveness by a global crowd workforce without any attention\nto the crowd workers' provenance or the values their perceptions reflect. We\nargue that cultural and psychological factors play a vital role in the\ncognitive processing of offensiveness, which is critical to consider in this\ncontext. We re-frame the task of determining offensiveness as essentially a\nmatter of moral judgment -- deciding the boundaries of ethically wrong vs.\nright language within an implied set of socio-cultural norms. Through a\nlarge-scale cross-cultural study based on 4309 participants from 21 countries\nacross 8 cultural regions, we demonstrate substantial cross-cultural\ndifferences in perceptions of offensiveness. More importantly, we find that\nindividual moral values play a crucial role in shaping these variations: moral\nconcerns about Care and Purity are significant mediating factors driving\ncross-cultural differences. These insights are of crucial importance as we\nbuild AI models for the pluralistic world, where the values they espouse should\naim to respect and account for moral values in diverse geo-cultural contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davani_A/0/1/0/all/0/1\">Aida Davani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Mark D&#xed;az</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_D/0/1/0/all/0/1\">Dylan Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Get an A in Math: Progressive Rectification Prompting. (arXiv:2312.06867v1 [cs.CL])","link":"http://arxiv.org/abs/2312.06867","description":"<p>Chain-of-Thought (CoT) prompting methods have enabled large language models\n(LLMs) to generate reasoning paths and solve math word problems (MWPs).\nHowever, they are sensitive to mistakes in the paths, as any mistake can result\nin an incorrect answer. We propose a novel method named Progressive\nRectification Prompting (PRP) to improve average accuracy on eight MWP datasets\nfrom 77.3 to 90.5. Given an initial answer from CoT, PRP iterates a\nverify-then-rectify process to progressively identify incorrect answers and\nrectify the reasoning paths. With the most likely correct answer, the LLM\npredicts a masked numerical value in the question; if the prediction does not\nmatch the masked value, the answer is likely incorrect. Then the LLM is\nprompted to re-generate the reasoning path hinted with a set of incorrect\nanswers to prevent itself from repeating previous mistakes. PRP achieves the\nbest performance compared against the CoT methods. Our implementation is made\npublicly available at https://wzy6642.github.io/prp.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhenyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dozerformer: Sequence Adaptive Sparse Transformer for Multivariate Time Series Forecasting. (arXiv:2312.06874v1 [cs.LG])","link":"http://arxiv.org/abs/2312.06874","description":"<p>Transformers have achieved remarkable performance in multivariate time\nseries(MTS) forecasting due to their capability to capture long-term\ndependencies. However, the canonical attention mechanism has two key\nlimitations: (1) its quadratic time complexity limits the sequence length, and\n(2) it generates future values from the entire historical sequence. To address\nthis, we propose a Dozer Attention mechanism consisting of three sparse\ncomponents: (1) Local, each query exclusively attends to keys within a\nlocalized window of neighboring time steps. (2) Stride, enables each query to\nattend to keys at predefined intervals. (3) Vary, allows queries to selectively\nattend to keys from a subset of the historical sequence. Notably, the size of\nthis subset dynamically expands as forecasting horizons extend. Those three\ncomponents are designed to capture essential attributes of MTS data, including\nlocality, seasonality, and global temporal dependencies. Additionally, we\npresent the Dozerformer Framework, incorporating the Dozer Attention mechanism\nfor the MTS forecasting task. We evaluated the proposed Dozerformer framework\nwith recent state-of-the-art methods on nine benchmark datasets and confirmed\nits superior performance. The code will be released after the manuscript is\naccepted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dascalu_S/0/1/0/all/0/1\">Sergiu M. Dascalu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_F/0/1/0/all/0/1\">Frederick C. Harris Jr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DYAD: A Descriptive Yet Abjuring Density efficient approximation to linear neural network layers. (arXiv:2312.06881v1 [cs.LG])","link":"http://arxiv.org/abs/2312.06881","description":"<p>We devise, implement and performance-asses DYAD, a layer which can serve as a\nfaster and more memory-efficient approximate replacement for linear layers,\n(nn.Linear() in Pytorch). These layers appear in common subcomponents, such as\nin the ff module of Transformers. DYAD is based on a bespoke near-sparse matrix\nstructure which approximates the dense \"weight\" matrix W that matrix-multiplies\nthe input in the typical realization of such a layer, a.k.a DENSE. Our\nalternative near-sparse matrix structure is decomposable to a sum of 2 matrices\npermutable to a block-sparse counterpart. These can be represented as 3D\ntensors, which in unison allow a faster execution of matrix multiplication with\nthe mini-batched input matrix X compared to DENSE (O(rows(W ) x cols(W )) --&gt;\nO( rows(W ) x cols(W ) # of blocks )). As the crux of our experiments, we\npretrain both DYAD and DENSE variants of 2 sizes of the OPT arch and 1 size of\nthe Pythia arch, including at different token scales of the babyLM benchmark.\nWe find DYAD to be competitive (&gt;= 90%) of DENSE performance on zero-shot (e.g.\nBLIMP), few-shot (OPENLM) and finetuning (GLUE) benchmarks, while being &gt;=7-15%\nfaster to train on-GPU even at 125m scale, besides surfacing larger speedups at\nincreasing scale and model width.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandy_S/0/1/0/all/0/1\">Sarin Chandy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maggiotti_G/0/1/0/all/0/1\">Gabriel Maggiotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack. (arXiv:2312.06924v1 [cs.CL])","link":"http://arxiv.org/abs/2312.06924","description":"<p>Recent developments in balancing the usefulness and safety of Large Language\nModels (LLMs) have raised a critical question: Are mainstream NLP tasks\nadequately aligned with safety consideration? Our study, focusing on\nsafety-sensitive documents obtained through adversarial attacks, reveals\nsignificant disparities in the safety alignment of various NLP tasks. For\ninstance, LLMs can effectively summarize malicious long documents but often\nrefuse to translate them. This discrepancy highlights a previously unidentified\nvulnerability: attacks exploiting tasks with weaker safety alignment, like\nsummarization, can potentially compromise the integraty of tasks traditionally\ndeemed more robust, such as translation and question-answering (QA). Moreover,\nthe concurrent use of multiple NLP tasks with lesser safety alignment increases\nthe risk of LLMs inadvertently processing harmful content. We demonstrate these\nvulnerabilities in various safety-aligned LLMs, particularly Llama2 models and\nGPT-4, indicating an urgent need for strengthening safety alignments across a\nbroad spectrum of NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yue Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Content-Localization based Neural Machine Translation for Informal Dialectal Arabic: Spanish/French to Levantine/Gulf Arabic. (arXiv:2312.06926v1 [cs.CL])","link":"http://arxiv.org/abs/2312.06926","description":"<p>Resources in high-resource languages have not been efficiently exploited in\nlow-resource languages to solve language-dependent research problems. Spanish\nand French are considered high resource languages in which an adequate level of\ndata resources for informal online social behavior modeling, is observed.\nHowever, a machine translation system to access those data resources and\ntransfer their context and tone to a low-resource language like dialectal\nArabic, does not exist. In response, we propose a framework that localizes\ncontents of high-resource languages to a low-resource language/dialects by\nutilizing AI power. To the best of our knowledge, we are the first work to\nprovide a parallel translation dataset from/to informal Spanish and French\nto/from informal Arabic dialects. Using this, we aim to enrich the\nunder-resource-status dialectal Arabic and fast-track the research of diverse\nonline social behaviors within and across smart cities in different\ngeo-regions. The experimental results have illustrated the capability of our\nproposed solution in exploiting the resources between high and low resource\nlanguages and dialects. Not only this, but it has also been proven that\nignoring dialects within the same language could lead to misleading analysis of\nonline social behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alzamzami_F/0/1/0/all/0/1\">Fatimah Alzamzami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1\">Abdulmotaleb El Saddik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"READ-PVLA: Recurrent Adapter with Partial Video-Language Alignment for Parameter-Efficient Transfer Learning in Low-Resource Video-Language Modeling. (arXiv:2312.06950v1 [cs.CV])","link":"http://arxiv.org/abs/2312.06950","description":"<p>Fully fine-tuning pretrained large-scale transformer models has become a\npopular paradigm for video-language modeling tasks, such as temporal language\ngrounding and video-language summarization. With a growing number of tasks and\nlimited training data, such full fine-tuning approach leads to costly model\nstorage and unstable training. To overcome these shortcomings, we introduce\nlightweight adapters to the pre-trained model and only update them at\nfine-tuning time. However, existing adapters fail to capture intrinsic temporal\nrelations among video frames or textual words. Moreover, they neglect the\npreservation of critical task-related information that flows from the raw\nvideo-language input into the adapter's low-dimensional space. To address these\nissues, we first propose a novel REcurrent ADapter (READ) that employs\nrecurrent computation to enable temporal modeling capability. Second, we\npropose Partial Video-Language Alignment (PVLA) objective via the use of\npartial optimal transport to maintain task-related information flowing into our\nREAD modules. We validate our READ-PVLA framework through extensive experiments\nwhere READ-PVLA significantly outperforms all existing fine-tuning strategies\non multiple low-resource temporal language grounding and video-language\nsummarization benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaobao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinshuai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_K/0/1/0/all/0/1\">Khoi Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cong-Duy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1\">Luu Anh Tuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SM70: A Large Language Model for Medical Devices. (arXiv:2312.06974v1 [cs.CL])","link":"http://arxiv.org/abs/2312.06974","description":"<p>We are introducing SM70, a 70 billion-parameter Large Language Model that is\nspecifically designed for SpassMed's medical devices under the brand name\n'JEE1' (pronounced as G1 and means 'Life'). This large language model provides\nmore accurate and safe responses to medical-domain questions. To fine-tune\nSM70, we used around 800K data entries from the publicly available dataset\nMedAlpaca. The Llama2 70B open-sourced model served as the foundation for SM70,\nand we employed the QLoRA technique for fine-tuning. The evaluation is\nconducted across three benchmark datasets - MEDQA - USMLE, PUBMEDQA, and USMLE\n- each representing a unique aspect of medical knowledge and reasoning. The\nperformance of SM70 is contrasted with other notable LLMs, including Llama2\n70B, Clinical Camel 70 (CC70), GPT 3.5, GPT 4, and Med-Palm, to provide a\ncomparative understanding of its capabilities within the medical domain. Our\nresults indicate that SM70 outperforms several established models in these\ndatasets, showcasing its proficiency in handling a range of medical queries,\nfrom fact-based questions derived from PubMed abstracts to complex clinical\ndecision-making scenarios. The robust performance of SM70, particularly in the\nUSMLE and PUBMEDQA datasets, suggests its potential as an effective tool in\nclinical decision support and medical information retrieval. Despite its\npromising results, the paper also acknowledges the areas where SM70 lags behind\nthe most advanced model, GPT 4, thereby highlighting the need for further\ndevelopment, especially in tasks demanding extensive medical knowledge and\nintricate reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatti_A/0/1/0/all/0/1\">Anubhav Bhatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_S/0/1/0/all/0/1\">Surajsinh Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">San Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alignment for Honesty. (arXiv:2312.07000v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07000","description":"<p>Recent research has made significant strides in applying alignment techniques\nto enhance the helpfulness and harmlessness of large language models (LLMs) in\naccordance with human intentions. In this paper, we argue for the importance of\nalignment for honesty, ensuring that LLMs proactively refuse to answer\nquestions when they lack knowledge, while still not being overly conservative.\nHowever, a pivotal aspect of alignment for honesty involves discerning the\nlimits of an LLM's knowledge, which is far from straightforward. This challenge\ndemands comprehensive solutions in terms of metric development, benchmark\ncreation, and training methodologies. In this paper, we address these\nchallenges by first establishing a precise problem definition and defining\n``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone\nfor developing metrics that effectively measure an LLM's honesty by quantifying\nits progress post-alignment. Furthermore, we introduce a flexible training\nframework which is further instantiated by several efficient fine-tuning\ntechniques that emphasize honesty without sacrificing performance on other\ntasks. Our extensive experiments reveal that these aligned models show a marked\nincrease in honesty, as indicated by our proposed metrics. We open-source a\nwealth of resources to facilitate future research at\nhttps://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned\nmodels, training and evaluation datasets for honesty alignment, concept\nglossary, as well as all relevant source code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chern_E/0/1/0/all/0/1\">Ethan Chern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Corrective Self-Distillation for Better Fine-Tuning of Pretrained Models. (arXiv:2312.07028v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07028","description":"<p>We tackle the challenging issue of aggressive fine-tuning encountered during\nthe process of transfer learning of pre-trained language models (PLMs) with\nlimited labeled downstream data. This problem primarily results in a decline in\nperformance on the subsequent task. Inspired by the adaptive boosting method in\ntraditional machine learning, we present an effective dynamic corrective\nself-distillation (DCS) approach to improve the fine-tuning of the PLMs. Our\ntechnique involves performing a self-distillation mechanism where, at each\niteration, the student model actively adapts and corrects itself by dynamically\nadjusting the weights assigned to individual data points. This iterative\nself-correcting process significantly enhances the overall fine-tuning\ncapability of PLMs, leading to improved performance and robustness. We\nconducted comprehensive evaluations using the GLUE benchmark demonstrating the\nefficacy of our method in enhancing the fine-tuning process for various PLMs\nacross diverse downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amara_I/0/1/0/all/0/1\">Ibtihel Amara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1\">Vinija Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Compression: Reduced Order Modelling of Latent Features in Large Language Models. (arXiv:2312.07046v1 [cs.LG])","link":"http://arxiv.org/abs/2312.07046","description":"<p>Due to the substantial scale of Large Language Models (LLMs), the direct\napplication of conventional compression methodologies proves impractical. The\ncomputational demands associated with even minimal gradient updates present\nchallenges, particularly on consumer-grade hardware. This paper introduces an\ninnovative approach for the parametric and practical compression of LLMs based\non reduced order modelling, which entails low-rank decomposition within the\nfeature space and re-parameterization in the weight space. Notably, this\ncompression technique operates in a layer-wise manner, obviating the need for a\nGPU device and enabling the compression of billion-scale models within\nstringent constraints of both memory and time. Our method represents a\nsignificant advancement in model compression by leveraging matrix\ndecomposition, demonstrating superior efficacy compared to the prevailing\nstate-of-the-art structured pruning method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chavan_A/0/1/0/all/0/1\">Arnav Chavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lele_N/0/1/0/all/0/1\">Nahush Lele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Factual Error Correction by Learning to Inject Factual Errors. (arXiv:2312.07049v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07049","description":"<p>Factual error correction (FEC) aims to revise factual errors in false claims\nwith minimal editing, making them faithful to the provided evidence. This task\nis crucial for alleviating the hallucination problem encountered by large\nlanguage models. Given the lack of paired data (i.e., false claims and their\ncorresponding correct claims), existing methods typically adopt the\nmask-then-correct paradigm. This paradigm relies solely on unpaired false\nclaims and correct claims, thus being referred to as distantly supervised\nmethods. These methods require a masker to explicitly identify factual errors\nwithin false claims before revising with a corrector. However, the absence of\npaired data to train the masker makes accurately pinpointing factual errors\nwithin claims challenging. To mitigate this, we propose to improve FEC by\nLearning to Inject Factual Errors (LIFE), a three-step distantly supervised\nmethod: mask-corrupt-correct. Specifically, we first train a corruptor using\nthe mask-then-corrupt procedure, allowing it to deliberately introduce factual\nerrors into correct text. The corruptor is then applied to correct claims,\ngenerating a substantial amount of paired data. After that, we filter out\nlow-quality data, and use the remaining data to train a corrector. Notably, our\ncorrector does not require a masker, thus circumventing the bottleneck\nassociated with explicit factual error identification. Our experiments on a\npublic dataset verify the effectiveness of LIFE in two key aspects: Firstly, it\noutperforms the previous best-performing distantly supervised method by a\nnotable margin of 10.59 points in SARI Final (19.3% improvement). Secondly,\neven compared to ChatGPT prompted with in-context examples, LIFE achieves a\nsuperiority of 7.16 points in SARI Final.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_A/0/1/0/all/0/1\">A-Long Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiu_S/0/1/0/all/0/1\">Siu Ming Yiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models. (arXiv:2312.07066v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07066","description":"<p>Recent advances in image and video creation, especially AI-based image\nsynthesis, have led to the production of numerous visual scenes that exhibit a\nhigh level of abstractness and diversity. Consequently, Visual Storytelling\n(VST), a task that involves generating meaningful and coherent narratives from\na collection of images, has become even more challenging and is increasingly\ndesired beyond real-world imagery. While existing VST techniques, which\ntypically use autoregressive decoders, have made significant progress, they\nsuffer from low inference speed and are not well-suited for synthetic scenes.\nTo this end, we propose a novel diffusion-based system DiffuVST, which models\nthe generation of a series of visual descriptions as a single conditional\ndenoising process. The stochastic and non-autoregressive nature of DiffuVST at\ninference time allows it to generate highly diverse narratives more\nefficiently. In addition, DiffuVST features a unique design with bi-directional\ntext history guidance and multimodal adapter modules, which effectively improve\ninter-sentence coherence and image-to-text fidelity. Extensive experiments on\nthe story generation task covering four fictional visual-story datasets\ndemonstrate the superiority of DiffuVST over traditional autoregressive models\nin terms of both text quality and inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengguang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qi Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Matter: Data-Efficient Augmentation of Large Language Models for Scientific Applications. (arXiv:2312.07069v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07069","description":"<p>In this paper, we explore the challenges inherent to Large Language Models\n(LLMs) like GPT-4, particularly their propensity for hallucinations, logic\nmistakes, and incorrect conclusions when tasked with answering complex\nquestions. The capacity of LLMs to present erroneous answers in a coherent and\nsemantically rigorous manner further complicates the detection of factual\ninaccuracies. This issue is especially pronounced in fields that require\nspecialized expertise. Our work delves into these challenges, aiming to enhance\nthe understanding and mitigation of such errors, thereby contributing to the\nimprovement of LLM accuracy and reliability in scientific and other specialized\ndomains. Our findings reveal a non-linear relationship between the context's\nrelevancy and the answers' measured quality. In addition, we demonstrate that\nwith the correct calibration, it is possible to automate the grading procedure\n-- a finding suggesting that, at least to some degree, the LLMs can be used to\nself-examine the quality of their own performance. Finally, we describe an\nexperimental platform that can be seen as a proof-of-concept of the techniques\ndescribed in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoran Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maravi_A/0/1/0/all/0/1\">Anurag Maravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abram_M/0/1/0/all/0/1\">Marcin Abram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BED: Bi-Encoder-Decoder Model for Canonical Relation Extraction. (arXiv:2312.07088v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07088","description":"<p>Canonical relation extraction aims to extract relational triples from\nsentences, where the triple elements (entity pairs and their relationship) are\nmapped to the knowledge base. Recently, methods based on the encoder-decoder\narchitecture are proposed and achieve promising results. However, these methods\ncannot well utilize the entity information, which is merely used as augmented\ntraining data. Moreover, they are incapable of representing novel entities,\nsince no embeddings have been learned for them. In this paper, we propose a\nnovel framework, Bi-Encoder-Decoder (BED), to solve the above issues.\nSpecifically, to fully utilize entity information, we employ an encoder to\nencode semantics of this information, leading to high-quality entity\nrepresentations. For novel entities, given a trained entity encoder, their\nrepresentations can be easily generated. Experimental results on two datasets\nshow that, our method achieves a significant performance improvement over the\nprevious state-of-the-art and handle novel entities well without retraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nantao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siyu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xinyu Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMs Perform Poorly at Concept Extraction in Cyber-security Research Literature. (arXiv:2312.07110v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07110","description":"<p>The cybersecurity landscape evolves rapidly and poses threats to\norganizations. To enhance resilience, one needs to track the latest\ndevelopments and trends in the domain. It has been demonstrated that standard\nbibliometrics approaches show their limits in such a fast-evolving domain. For\nthis purpose, we use large language models (LLMs) to extract relevant knowledge\nentities from cybersecurity-related texts. We use a subset of arXiv preprints\non cybersecurity as our data and compare different LLMs in terms of entity\nrecognition (ER) and relevance. The results suggest that LLMs do not produce\ngood knowledge entities that reflect the cybersecurity context, but our results\nshow some potential for noun extractors. For this reason, we developed a noun\nextractor boosted with some statistical analysis to extract specific and\nrelevant compound nouns from the domain. Later, we tested our model to identify\ntrends in the LLM domain. We observe some limitations, but it offers promising\nresults to monitor the evolution of emergent trends.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wursch_M/0/1/0/all/0/1\">Maxime W&#xfc;rsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucharavy_A/0/1/0/all/0/1\">Andrei Kucharavy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_D/0/1/0/all/0/1\">Dimitri Percia David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mermoud_A/0/1/0/all/0/1\">Alain Mermoud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual large language models leak human stereotypes across language boundaries. (arXiv:2312.07141v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07141","description":"<p>Multilingual large language models have been increasingly popular for their\nproficiency in comprehending and generating text across various languages.\nPrevious research has shown that the presence of stereotypes and biases in\nmonolingual large language models can be attributed to the nature of their\ntraining data, which is collected from humans and reflects societal biases.\nMultilingual language models undergo the same training procedure as monolingual\nones, albeit with training data sourced from various languages. This raises the\nquestion: do stereotypes present in one social context leak across languages\nwithin the model? In our work, we first define the term ``stereotype leakage''\nand propose a framework for its measurement. With this framework, we\ninvestigate how stereotypical associations leak across four languages: English,\nRussian, Chinese, and Hindi. To quantify the stereotype leakage, we employ an\napproach from social psychology, measuring stereotypes via group-trait\nassociations. We evaluate human stereotypes and stereotypical associations\nmanifested in multilingual large language models such as mBERT, mT5, and\nChatGPT. Our findings show a noticeable leakage of positive, negative, and\nnon-polar associations across all languages. Notably, Hindi within multilingual\nmodels appears to be the most susceptible to influence from other languages,\nwhile Chinese is the least. Additionally, ChatGPT exhibits a better alignment\nwith human scores than other models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Trista Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotnikova_A/0/1/0/all/0/1\">Anna Sotnikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jieyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Linda X. Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1\">Rachel Rudinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1\">Hal Daume III</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classifying complex documents: comparing bespoke solutions to large language models. (arXiv:2312.07182v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07182","description":"<p>Here we search for the best automated classification approach for a set of\ncomplex legal documents. Our classification task is not trivial: our aim is to\nclassify ca 30,000 public courthouse records from 12 states and 267 counties at\ntwo different levels using nine sub-categories. Specifically, we investigated\nwhether a fine-tuned large language model (LLM) can achieve the accuracy of a\nbespoke custom-trained model, and what is the amount of fine-tuning necessary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hopkins_G/0/1/0/all/0/1\">Glen Hopkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalm_K/0/1/0/all/0/1\">Kristjan Kalm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Verbreitungsmechanismen sch\\\"adigender Sprache im Netz: Anatomie zweier Shitstorms. (arXiv:2312.07194v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07194","description":"<p>In this working paper, we turn our attention to two exemplary, cross-media\nshitstorms directed against well-known individuals from the business world.\nBoth have in common, first, the trigger, a controversial statement by the\nperson who thereby becomes the target of the shitstorm, and second, the\nidentity of this target as relatively privileged: cis-male, white, successful.\nWe examine the spread of the outrage wave across two media at a time and test\nthe applicability of computational linguistic methods for analyzing its time\ncourse. Assuming that harmful language spreads like a virus in digital space,\nwe are primarily interested in the events and constellations that lead to the\nuse of harmful language, and whether and how a linguistic formation of \"tribes\"\noccurs. Our research therefore focuses, first, on the distribution of\nlinguistic features within the overall shitstorm: are individual words or\nphrases increasingly used after their introduction, and through which pathways\nthey spread. Second, we ask whether \"tribes,\" for example, one group of\nsupporters and one of opponents of the target, have a distinguished linguistic\nform. Our hypothesis is that supporters remain equally active over time, while\nthe dynamic \"ripple\" effect of the shitstorm is based on the varying\nparticipation of opponents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scheffler_T/0/1/0/all/0/1\">Tatjana Scheffler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solopova_V/0/1/0/all/0/1\">Veronika Solopova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popa_Wyatt_M/0/1/0/all/0/1\">Mihaela Popa-Wyatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toxic language detection: a systematic survey of Arabic datasets. (arXiv:2312.07228v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07228","description":"<p>This paper offers a comprehensive survey of Arabic datasets focused on online\ntoxic language. We systematically gathered a total of 49 available datasets and\ntheir corresponding papers and conducted a thorough analysis, considering 16\ncriteria across three primary dimensions: content, annotation process, and\nreusability. This analysis enabled us to identify existing gaps and make\nrecommendations for future research works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bensalem_I/0/1/0/all/0/1\">Imene Bensalem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitouni_H/0/1/0/all/0/1\">Hanane Zitouni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Machine Translation of Clinical Text: An Empirical Investigation into Multilingual Pre-Trained Language Models and Transfer-Learning. (arXiv:2312.07250v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07250","description":"<p>We conduct investigations on clinical text machine translation by examining\nmultilingual neural network models using deep learning such as Transformer\nbased structures. Furthermore, to address the language resource imbalance\nissue, we also carry out experiments using a transfer learning methodology\nbased on massive multilingual pre-trained language models (MMPLMs). The\nexperimental results on three subtasks including 1) clinical case (CC), 2)\nclinical terminology (CT), and 3) ontological concept (OC) show that our models\nachieved top-level performances in the ClinSpEn-2022 shared task on\nEnglish-Spanish clinical domain data. Furthermore, our expert-based human\nevaluations demonstrate that the small-sized pre-trained language model (PLM)\nwon over the other two extra-large language models by a large margin, in the\nclinical domain fine-tuning, which finding was never reported in the field.\nFinally, the transfer learning method works well in our experimental setting\nusing the WMT21fb model to accommodate a new language space Spanish that was\nnot seen at the pre-training stage within WMT21fb itself, which deserves more\nexploitation for clinical knowledge transformation, e.g. to investigate into\nmore languages. These research findings can shed some light on domain-specific\nmachine translation development, especially in clinical and healthcare fields.\nFurther research projects can be carried out based on our work to improve\nhealthcare text analytics and knowledge transformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1\">Serge Gladkoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1\">Gleb Erofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorokina_I/0/1/0/all/0/1\">Irina Sorokina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galiano_B/0/1/0/all/0/1\">Betty Galiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The GUA-Speech System Description for CNVSRC Challenge 2023. (arXiv:2312.07254v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07254","description":"<p>This study describes our system for Task 1 Single-speaker Visual Speech\nRecognition (VSR) fixed track in the Chinese Continuous Visual Speech\nRecognition Challenge (CNVSRC) 2023. Specifically, we use intermediate\nconnectionist temporal classification (Inter CTC) residual modules to relax the\nconditional independence assumption of CTC in our model. Then we use a\nbi-transformer decoder to enable the model to capture both past and future\ncontextual information. In addition, we use Chinese characters as the modeling\nunits to improve the recognition accuracy of our model. Finally, we use a\nrecurrent neural network language model (RNNLM) for shallow fusion in the\ninference stage. Experiments show that our system achieves a character error\nrate (CER) of 38.09% on the Eval set which reaches a relative CER reduction of\n21.63% over the official baseline, and obtains a second place in the challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1\">Chao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Baozhong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fuping Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIST: Improving Parameter Efficient Fine Tuning via Knowledge Interaction. (arXiv:2312.07255v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07255","description":"<p>The Parameter-Efficient Fine-Tuning (PEFT) method, which adjusts or\nintroduces fewer trainable parameters to calibrate pre-trained models on\ndownstream tasks, has become a recent research interest. However, existing PEFT\nmethods within the traditional fine-tiuning framework have two main\nshortcomings: 1) They overlook the explicit association between trainable\nparameters and downstream task knowledge. 2) They neglect the interaction\nbetween the intrinsic task-agnostic knowledge of pre-trained models and the\ntask-specific knowledge in downstream tasks. To address this gap, we propose a\nnovel fine-tuning framework, named GIST, in a plug-and-play manner.\nSpecifically, our framework first introduces a trainable token, called the Gist\ntoken, when applying PEFT methods on downstream tasks. This token serves as an\naggregator of the task-specific knowledge learned by the PEFT methods and forms\nan explicit association with downstream knowledge. Furthermore, to facilitate\nexplicit interaction between task-agnostic and task-specific knowledge, we\nintroduce the concept of Knowledge Interaction via a Bidirectional\nKullback-Leibler Divergence objective. As a result, PEFT methods within our\nframework can make the pre-trained model understand downstream tasks more\ncomprehensively by leveraging the knowledge interaction. Extensive experiments\ndemonstrate the universality and scalability of our framework. Notably, on the\nVTAB-1K benchmark, we employ the Adapter (a prevalent PEFT method) within our\nGIST framework and achieve a performance boost of 2.25%, with an increase of\nonly 0.8K parameters. The Code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_J/0/1/0/all/0/1\">Jiacheng Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jingsheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Mingye Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Suncheng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zefang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuzhuo Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Equipping Transformer with the Ability of Systematic Compositionality. (arXiv:2312.07280v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07280","description":"<p>One of the key factors in language productivity and human cognition is the\nability of systematic compositionality, which refers to understanding composed\nunseen examples of seen primitives. However, recent evidence reveals that the\nTransformers have difficulty generalizing the composed context based on the\nseen primitives. To this end, we take the first step to propose a\ncompositionality-aware Transformer called CAT and two novel pre-training tasks\nto facilitate systematic compositionality. We tentatively provide a successful\nimplementation of a multi-layer CAT on the basis of the especially popular\nBERT. The experimental results demonstrate that CAT outperforms baselines on\ncompositionality-aware tasks with minimal impact on the effectiveness on\nstandardized language understanding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_P/0/1/0/all/0/1\">Peixin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCCA: Shifted Cross Chunk Attention for long contextual semantic expansion. (arXiv:2312.07305v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07305","description":"<p>Sparse attention as a efficient method can significantly decrease the\ncomputation cost, but current sparse attention tend to rely on window self\nattention which block the global information flow. For this problem, we present\nShifted Cross Chunk Attention (SCCA), using different KV shifting strategy to\nextend respective field in each attention layer. Except, we combine Dilated\nAttention(DA) and Dilated Neighborhood Attention(DNA) to present Shifted\nDilated Attention(SDA). Both SCCA and SDA can accumulate attention results in\nmulti head attention to obtain approximate respective field in full attention.\nIn this paper, we conduct language modeling experiments using different pattern\nof SCCA and combination of SCCA and SDA. The proposed shifted cross chunk\nattention (SCCA) can effectively extend large language models (LLMs) to longer\ncontext combined with Positional interpolation(PI) and LoRA than current sparse\nattention. Notably, SCCA adopts LLaMA2 7B from 4k context to 8k in single V100.\nThis attention pattern can provide a Plug-and-play fine-tuning method to extend\nmodel context while retaining their original architectures, and is compatible\nwith most existing techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuxiang Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Adaptive Pre-training of Multilingual Speech Models for Language and Dialect Identification. (arXiv:2312.07338v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07338","description":"<p>Pre-trained Transformer-based speech models have shown striking performance\nwhen fine-tuned on various downstream tasks such as automatic speech\nrecognition and spoken language identification (SLID). However, the problem of\ndomain mismatch remains a challenge in this area, where the domain of the\npre-training data might differ from that of the downstream labeled data used\nfor fine-tuning. In multilingual tasks such as SLID, the pre-trained speech\nmodel may not support all the languages in the downstream task. To address this\nchallenge, we propose self-supervised adaptive pre-training (SAPT) to adapt the\npre-trained model to the target domain and languages of the downstream task. We\napply SAPT to the XLSR-128 model and investigate the effectiveness of this\napproach for the SLID task. First, we demonstrate that SAPT improves XLSR\nperformance on the FLEURS benchmark with substantial gains up to 40.1% for\nunder-represented languages. Second, we apply SAPT on four different datasets\nin a few-shot learning setting, showing that our approach improves the sample\nefficiency of XLSR during fine-tuning. Our experiments provide strong empirical\nevidence that continual adaptation via self-supervision improves downstream\nperformance for multilingual speech models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaik_M/0/1/0/all/0/1\">Mohammed Maqsood Shaik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_B/0/1/0/all/0/1\">Badr M. Abdullah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames. (arXiv:2312.07395v1 [cs.CV])","link":"http://arxiv.org/abs/2312.07395","description":"<p>Understanding long, real-world videos requires modeling of long-range visual\ndependencies. To this end, we explore video-first architectures, building on\nthe common paradigm of transferring large-scale, image--text models to video\nvia shallow temporal fusion. However, we expose two limitations to the\napproach: (1) decreased spatial capabilities, likely due to poor\nvideo--language alignment in standard video datasets, and (2) higher memory\nconsumption, bottlenecking the number of frames that can be processed. To\nmitigate the memory bottleneck, we systematically analyze the memory/accuracy\ntrade-off of various efficient methods: factorized attention,\nparameter-efficient image-to-video adaptation, input masking, and\nmulti-resolution patchification. Surprisingly, simply masking large portions of\nthe video (up to 75%) during contrastive pre-training proves to be one of the\nmost robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our\nsimple approach for training long video-to-text models, which scales to 1B\nparameters, does not add new architectural complexity and is able to outperform\nthe popular paradigm of using much larger LLMs as an information aggregator\nover segment-based information on benchmarks with long-range temporal\ndependencies (YouCook2, EgoSchema).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papalampidi_P/0/1/0/all/0/1\">Pinelopi Papalampidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1\">Skanda Koppula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_S/0/1/0/all/0/1\">Shreya Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1\">Justin Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heyward_J/0/1/0/all/0/1\">Joe Heyward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1\">Viorica Patraucean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiajun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzdeh_A/0/1/0/all/0/1\">Aida Nematzdeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMEval: A Preliminary Study on How to Evaluate Large Language Models. (arXiv:2312.07398v1 [cs.AI])","link":"http://arxiv.org/abs/2312.07398","description":"<p>Recently, the evaluation of Large Language Models has emerged as a popular\narea of research. The three crucial questions for LLM evaluation are ``what,\nwhere, and how to evaluate''. However, the existing research mainly focuses on\nthe first two questions, which are basically what tasks to give the LLM during\ntesting and what kind of knowledge it should deal with. As for the third\nquestion, which is about what standards to use, the types of evaluators, how to\nscore, and how to rank, there hasn't been much discussion. In this paper, we\nanalyze evaluation methods by comparing various criteria with both manual and\nautomatic evaluation, utilizing onsite, crowd-sourcing, public annotators and\nGPT-4, with different scoring methods and ranking systems. We propose a new\ndataset, LLMEval and conduct evaluations on 20 LLMs. A total of 2,186\nindividuals participated, leading to the generation of 243,337 manual\nannotations and 57,511 automatic evaluation results. We perform comparisons and\nanalyses of different settings and conduct 10 conclusions that can provide some\ninsights for evaluating LLM in the future. The dataset and the results are\npublicly available at https://github.com/llmeval .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Haipeng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shichun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yongyao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales. (arXiv:2312.07399v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07399","description":"<p>Machine reasoning has made great progress in recent years owing to large\nlanguage models (LLMs). In the clinical domain, however, most NLP-driven\nprojects mainly focus on clinical classification or reading comprehension, and\nunder-explore clinical reasoning for disease diagnosis due to the expensive\nrationale annotation with clinicians. In this work, we present a\n``reasoning-aware'' diagnosis framework that rationalizes the diagnostic\nprocess via prompt-based learning in a time- and labor-efficient manner, and\nlearns to reason over the prompt-generated rationales. Specifically, we address\nthe clinical reasoning for disease diagnosis, where the LLM generates\ndiagnostic rationales providing its insight on presented patient data and the\nreasoning path towards the diagnosis, namely Clinical Chain-of-Thought\n(Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical\nreasoning via extensive experiments and analyses on both rationale generation\nand disease diagnosis in various settings. We further propose a novel set of\ncriteria for evaluating machine-generated rationales' potential for real-world\nclinical settings, facilitating and benefiting future research in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_T/0/1/0/all/0/1\">Taeyoon Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_K/0/1/0/all/0/1\">Kai Tzu-iunn Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongjin Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seungjun Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jeong Ryong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Dosik Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_Y/0/1/0/all/0/1\">Yongsik Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_B/0/1/0/all/0/1\">Beomseok Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongha Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1\">Jinyoung Yeo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ICL Markup: Structuring In-Context Learning using Soft-Token Tags. (arXiv:2312.07405v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07405","description":"<p>Large pretrained language models (LLMs) can be rapidly adapted to a wide\nvariety of tasks via a text-to-text approach, where the instruction and input\nare fed to the model in natural language. Combined with in-context learning\n(ICL), this paradigm is impressively flexible and powerful. However, it also\nburdens users with an overwhelming number of choices, many of them arbitrary.\nInspired by markup languages like HTML, we contribute a method of using\nsoft-token tags to compose prompt templates. This approach reduces arbitrary\ndecisions and streamlines the application of ICL. Our method is a form of\nmeta-learning for ICL; it learns these tags in advance during a\nparameter-efficient fine-tuning ``warm-up'' process. The tags can subsequently\nbe used in templates for ICL on new, unseen tasks without any additional\nfine-tuning. Our experiments with this approach yield promising initial\nresults, improving LLM performance on important enterprise applications such as\nfew-shot and open-world intent detection, as well as text classification in\nnews and legal domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brunet_M/0/1/0/all/0/1\">Marc-Etienne Brunet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Ashton Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Faster k-Nearest-Neighbor Machine Translation. (arXiv:2312.07419v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07419","description":"<p>Recent works have proven the effectiveness of k-nearest-neighbor machine\ntranslation(a.k.a kNN-MT) approaches to produce remarkable improvement in\ncross-domain translations. However, these models suffer from heavy retrieve\noverhead on the entire datastore when decoding each token. We observe that\nduring the decoding phase, about 67% to 84% of tokens are unvaried after\nsearching over the corpus datastore, which means most of the tokens cause\nfutile retrievals and introduce unnecessary computational costs by initiating\nk-nearest-neighbor searches. We consider this phenomenon is explainable in\nlinguistics and propose a simple yet effective multi-layer perceptron (MLP)\nnetwork to predict whether a token should be translated jointly by the neural\nmachine translation model and probabilities produced by the kNN or just by the\nneural model. The results show that our method succeeds in reducing redundant\nretrieval operations and significantly reduces the overhead of kNN retrievals\nby up to 53% at the expense of a slight decline in translation quality.\nMoreover, our method could work together with all existing kNN-MT systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Contrastive Learning with Asymmetric Co-attention Network for Video Moment Retrieval. (arXiv:2312.07435v1 [cs.CV])","link":"http://arxiv.org/abs/2312.07435","description":"<p>Video moment retrieval is a challenging task requiring fine-grained\ninteractions between video and text modalities. Recent work in image-text\npretraining has demonstrated that most existing pretrained models suffer from\ninformation asymmetry due to the difference in length between visual and\ntextual sequences. We question whether the same problem also exists in the\nvideo-text domain with an auxiliary need to preserve both spatial and temporal\ninformation. Thus, we evaluate a recently proposed solution involving the\naddition of an asymmetric co-attention network for video grounding tasks.\nAdditionally, we incorporate momentum contrastive loss for robust,\ndiscriminative representation learning in both modalities. We note that the\nintegration of these supplementary modules yields better performance compared\nto state-of-the-art models on the TACoS dataset and comparable results on\nActivityNet Captions, all while utilizing significantly fewer parameters with\nrespect to baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panta_L/0/1/0/all/0/1\">Love Panta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrestha_P/0/1/0/all/0/1\">Prashant Shrestha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapkota_B/0/1/0/all/0/1\">Brabeem Sapkota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_A/0/1/0/all/0/1\">Amrita Bhattarai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manandhar_S/0/1/0/all/0/1\">Suresh Manandhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sah_A/0/1/0/all/0/1\">Anand Kumar Sah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection. (arXiv:2312.07476v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07476","description":"<p>In-Context Learning (ICL) is an important paradigm for adapting Large\nLanguage Models (LLMs) to downstream tasks through a few demonstrations.\nDespite the great success of ICL, the limitation of the demonstration number\nmay lead to demonstration bias, i.e. the input-label mapping induced by LLMs\nmisunderstands the task's essence. Inspired by human experience, we attempt to\nmitigate such bias through the perspective of the inter-demonstration\nrelationship. Specifically, we construct Comparable Demonstrations (CDs) by\nminimally editing the texts to flip the corresponding labels, in order to\nhighlight the task's essence and eliminate potential spurious correlations\nthrough the inter-demonstration comparison. Through a series of experiments on\nCDs, we find that (1) demonstration bias does exist in LLMs, and CDs can\nsignificantly reduce such bias; (2) CDs exhibit good performance in ICL,\nespecially in out-of-distribution scenarios. In summary, this study explores\nthe ICL mechanisms from a novel perspective, providing a deeper insight into\nthe demonstration selection strategy for ICL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Caoyun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jidong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaohui Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models. (arXiv:2312.07492v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07492","description":"<p>Current datasets for unwanted social bias auditing are limited to studying\nprotected demographic features such as race and gender. In this work, we\nintroduce a comprehensive benchmark that is meant to capture the amplification\nof social bias, via stigmas, in generative language models. We start with a\ncomprehensive list of 93 stigmas documented in social science literature and\ncurate a question-answering (QA) dataset which involves simple social\nsituations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a\nvariety of prompt styles, carefully constructed to systematically test for both\nsocial bias and model robustness. We present results for SocialStigmaQA with\ntwo widely used open source generative language models and we demonstrate that\nthe output generated by these models considerably amplifies existing social\nbias against stigmatized groups. Specifically, we find that the proportion of\nsocially biased output ranges from 45% to 59% across a variety of decoding\nstrategies and prompting styles. We discover that the deliberate design of the\ntemplates in our benchmark (e.g., by adding biasing text to the prompt or\nvarying the answer that indicates bias) impact the model tendencies to generate\nsocially biased output. Additionally, we report on patterns in the generated\nchain-of-thought output, finding a variety of problems from subtle bias to\nevidence of a lack of reasoning.\n</p>\n<p>Warning: This paper contains examples of text which is toxic, biased, and\nharmful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagireddy_M/0/1/0/all/0/1\">Manish Nagireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiazor_L/0/1/0/all/0/1\">Lamogha Chiazor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Moninder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1\">Ioana Baldini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detect, Retrieve, Comprehend: A Flexible Framework for Zero-Shot Document-Level Question Answering. (arXiv:2210.01959v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01959","description":"<p>Researchers produce thousands of scholarly documents containing valuable\ntechnical knowledge. The community faces the laborious task of reading these\ndocuments to identify, extract, and synthesize information. To automate\ninformation gathering, document-level question answering (QA) offers a flexible\nframework where human-posed questions can be adapted to extract diverse\nknowledge. Finetuning QA systems requires access to labeled data (tuples of\ncontext, question and answer). However, data curation for document QA is\nuniquely challenging because the context (i.e. answer evidence passage) needs\nto be retrieved from potentially long, ill-formatted documents. Existing QA\ndatasets sidestep this challenge by providing short, well-defined contexts that\nare unrealistic in real-world applications. We present a three-stage document\nQA approach: (1) text extraction from PDF; (2) evidence retrieval from\nextracted texts to form well-posed contexts; (3) QA to extract knowledge from\ncontexts to return high-quality answers -- extractive, abstractive, or Boolean.\nUsing QASPER for evaluation, our detect-retrieve-comprehend (DRC) system\nachieves a +7.19 improvement in Answer-F1 over existing baselines while\ndelivering superior context selection. Our results demonstrate that DRC holds\ntremendous promise as a flexible framework for practical scientific document\nQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McDonald_T/0/1/0/all/0/1\">Tavish McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsan_B/0/1/0/all/0/1\">Brian Tsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_A/0/1/0/all/0/1\">Amar Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_J/0/1/0/all/0/1\">Juanita Ordonez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_L/0/1/0/all/0/1\">Luis Gutierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mason_B/0/1/0/all/0/1\">Blake Mason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_B/0/1/0/all/0/1\">Brenda Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature. (arXiv:2210.09932v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09932","description":"<p>Lay summarisation aims to jointly summarise and simplify a given text, thus\nmaking its content more comprehensible to non-experts. Automatic approaches for\nlay summarisation can provide significant value in broadening access to\nscientific literature, enabling a greater degree of both interdisciplinary\nknowledge sharing and public understanding when it comes to research findings.\nHowever, current corpora for this task are limited in their size and scope,\nhindering the development of broadly applicable data-driven approaches. Aiming\nto rectify these issues, we present two novel lay summarisation datasets, PLOS\n(large-scale) and eLife (medium-scale), each of which contains biomedical\njournal articles alongside expert-written lay summaries. We provide a thorough\ncharacterisation of our lay summaries, highlighting differing levels of\nreadability and abstractiveness between datasets that can be leveraged to\nsupport the needs of different applications. Finally, we benchmark our datasets\nusing mainstream summarisation approaches and perform a manual evaluation with\ndomain experts, demonstrating their utility and casting light on the key\nchallenges of this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goldsack_T/0/1/0/all/0/1\">Tomas Goldsack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Highlighting Named Entities in Input for Auto-Formulation of Optimization Problems. (arXiv:2212.13201v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.13201","description":"<p>Operations research deals with modeling and solving real-world problems as\nmathematical optimization problems. While solving mathematical systems is\naccomplished by analytical software, formulating a problem as a set of\nmathematical operations has been typically done manually by domain experts.\nRecent machine learning methods have shown promise in converting textual\nproblem descriptions to corresponding mathematical formulations. This paper\npresents an approach that converts linear programming word problems into\nmathematical formulations. We leverage the named entities in the input and\naugment the input to highlight these entities. Our approach achieves the\nhighest accuracy among all submissions to the NL4Opt Competition, securing\nfirst place in the generation track.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gangwar_N/0/1/0/all/0/1\">Neeraj Gangwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kani_N/0/1/0/all/0/1\">Nickvash Kani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GTRL: An Entity Group-Aware Temporal Knowledge Graph Representation Learning Method. (arXiv:2302.11091v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.11091","description":"<p>Temporal Knowledge Graph (TKG) representation learning embeds entities and\nevent types into a continuous low-dimensional vector space by integrating the\ntemporal information, which is essential for downstream tasks, e.g., event\nprediction and question answering. Existing methods stack multiple graph\nconvolution layers to model the influence of distant entities, leading to the\nover-smoothing problem. To alleviate the problem, recent studies infuse\nreinforcement learning to obtain paths that contribute to modeling the\ninfluence of distant entities. However, due to the limited number of hops,\nthese studies fail to capture the correlation between entities that are far\napart and even unreachable. To this end, we propose GTRL, an entity Group-aware\nTemporal knowledge graph Representation Learning method. GTRL is the first work\nthat incorporates the entity group modeling to capture the correlation between\nentities by stacking only a finite number of layers. Specifically, the entity\ngroup mapper is proposed to generate entity groups from entities in a learning\nway. Based on entity groups, the implicit correlation encoder is introduced to\ncapture implicit correlations between any pairwise entity groups. In addition,\nthe hierarchical GCNs are exploited to accomplish the message aggregation and\nrepresentation updating on the entity group graph and the entity graph.\nFinally, GRUs are employed to capture the temporal dependency in TKGs.\nExtensive experiments on three real-world datasets demonstrate that GTRL\nachieves the state-of-the-art performances on the event prediction task,\noutperforming the best baseline by an average of 13.44%, 9.65%, 12.15%, and\n15.12% in MRR, Hits@1, Hits@3, and Hits@10, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xing Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Ling Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents. (arXiv:2303.00855v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2303.00855","description":"<p>Recent progress in large language models (LLMs) has demonstrated the ability\nto learn and leverage Internet-scale knowledge through pre-training with\nautoregressive models. Unfortunately, applying such models to settings with\nembodied agents, such as robots, is challenging due to their lack of experience\nwith the physical world, inability to parse non-language observations, and\nignorance of rewards or safety constraints that robots may require. On the\nother hand, language-conditioned robotic policies that learn from interaction\ndata can provide the necessary grounding that allows the agent to be correctly\nsituated in the real world, but such policies are limited by the lack of\nhigh-level semantic understanding due to the limited breadth of the interaction\ndata available for training them. Thus, if we want to make use of the semantic\nknowledge in a language model while still situating it in an embodied setting,\nwe must construct an action sequence that is both likely according to the\nlanguage model and also realizable according to grounded models of the\nenvironment. We frame this as a problem similar to probabilistic filtering:\ndecode a sequence that both has high probability under the language model and\nhigh probability under a set of grounded model objectives. We demonstrate how\nsuch grounded models can be obtained across three simulation and real-world\ndomains, and that the proposed decoding strategy is able to solve complex,\nlong-horizon embodiment tasks in a robotic setting by leveraging the knowledge\nof both models. The project's website can be found at\ngrounded-decoding.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenlong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Dhruv Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driess_D/0/1/0/all/0/1\">Danny Driess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1\">Karol Hausman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Clustering Framework for Unsupervised and Semi-supervised New Intent Discovery. (arXiv:2304.07699v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.07699","description":"<p>New intent discovery is of great value to natural language processing,\nallowing for a better understanding of user needs and providing friendly\nservices. However, most existing methods struggle to capture the complicated\nsemantics of discrete text representations when limited or no prior knowledge\nof labeled data is available. To tackle this problem, we propose a novel\nclustering framework, USNID, for unsupervised and semi-supervised new intent\ndiscovery, which has three key technologies. First, it fully utilizes\nunsupervised or semi-supervised data to mine shallow semantic similarity\nrelations and provide well-initialized representations for clustering. Second,\nit designs a centroid-guided clustering mechanism to address the issue of\ncluster allocation inconsistency and provide high-quality self-supervised\ntargets for representation learning. Third, it captures high-level semantics in\nunsupervised or semi-supervised data to discover fine-grained intent-wise\nclusters by optimizing both cluster-level and instance-level objectives. We\nalso propose an effective method for estimating the cluster number in\nopen-world scenarios without knowing the number of new intents beforehand.\nUSNID performs exceptionally well on several benchmark intent datasets,\nachieving new state-of-the-art results in unsupervised and semi-supervised new\nintent discovery and demonstrating robust performance with different cluster\nnumbers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_F/0/1/0/all/0/1\">Fei Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kai Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synergistic Interplay between Search and Large Language Models for Information Retrieval. (arXiv:2305.07402v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07402","description":"<p>Information retrieval (IR) plays a crucial role in locating relevant\nresources from vast amounts of data, and its applications have evolved from\ntraditional knowledge bases to modern retrieval models (RMs). The emergence of\nlarge language models (LLMs) has further revolutionized the IR field by\nenabling users to interact with search systems in natural languages. In this\npaper, we explore the advantages and disadvantages of LLMs and RMs,\nhighlighting their respective strengths in understanding user-issued queries\nand retrieving up-to-date information. To leverage the benefits of both\nparadigms while circumventing their limitations, we propose InteR, a novel\nframework that facilitates information refinement through synergy between RMs\nand LLMs. InteR allows RMs to expand knowledge in queries using LLM-generated\nknowledge collections and enables LLMs to enhance prompt formulation using\nretrieved documents. This iterative refinement process augments the inputs of\nRMs and LLMs, leading to more accurate retrieval. Experiments on large-scale\nretrieval benchmarks involving web search and low-resource retrieval tasks\ndemonstrate that InteR achieves overall superior zero-shot retrieval\nperformance compared to state-of-the-art methods, even those using relevance\njudgment. Source code is available at https://github.com/Cyril-JZ/InteR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiazhan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear-Time Modeling of Linguistic Structure: An Order-Theoretic Perspective. (arXiv:2305.15057v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15057","description":"<p>Tasks that model the relation between pairs of tokens in a string are a vital\npart of understanding natural language. Such tasks, in general, require\nexhaustive pair-wise comparisons of tokens, thus having a quadratic runtime\ncomplexity in the length of the string. We show that these exhaustive\ncomparisons can be avoided, and, moreover, the complexity of such tasks can be\nreduced to linear by casting the relation between tokens as a partial order\nover the string. Our method predicts real numbers for each token in a string in\nparallel and sorts the tokens accordingly, resulting in total orders of the\ntokens in the string. Each total order implies a set of arcs oriented from\nsmaller to greater tokens, sorted by their predicted numbers. The intersection\nof total orders results in a partial order over the set of tokens in the\nstring, which is then decoded into a directed graph representing the desired\nlinguistic structure. Our experiments on dependency parsing and coreference\nresolution show that our method achieves state-of-the-art or comparable\nperformance. Moreover, the linear complexity and parallelism of our method\ndouble the speed of graph-based coreference resolution models, and bring a\n10-times speed-up over graph-based dependency parsers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Afra Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Early Weight Averaging meets High Learning Rates for LLM Pre-training. (arXiv:2306.03241v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.03241","description":"<p>Training Large Language Models (LLMs) incurs significant cost; hence, any\nstrategy that accelerates model convergence is helpful. In this paper, we\ninvestigate the ability of a simple idea checkpoint averaging along the\ntrajectory of a training run to improve both convergence and generalization\nquite early on during training. Here we show that models trained with high\nlearning rates observe higher gains due to checkpoint averaging. Furthermore,\nthese gains are amplified when checkpoints are sampled with considerable\nspacing in training steps. Our training recipe outperforms conventional\ntraining and popular checkpoint averaging baselines such as exponential moving\naverage (EMA) and stochastic moving average (SWA). We evaluate our training\nrecipe by pre-training LLMs, where high learning rates are inherently preferred\ndue to extremely large batch sizes. Specifically, we pre-trained nanoGPT-2\nmodels of varying sizes, small (125M), medium (335M), and large (770M)on the\nOpenWebText dataset, comprised of 9B tokens. Additionally, we present results\nfor publicly available Pythia LLMs, ranging from 1B to 12B, which were trained\non the PILE-deduped dataset containing 207B tokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Sunny Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neerkaje_A/0/1/0/all/0/1\">Atula Neerkaje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1\">Jean Kaddour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1\">Sujay Sanghavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution. (arXiv:2306.12424v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2306.12424","description":"<p>We introduce VisoGender, a novel dataset for benchmarking gender bias in\nvision-language models. We focus on occupation-related biases within a\nhegemonic system of binary gender, inspired by Winograd and Winogender schemas,\nwhere each image is associated with a caption containing a pronoun relationship\nof subjects and objects in the scene. VisoGender is balanced by gender\nrepresentation in professional roles, supporting bias evaluation in two ways:\ni) resolution bias, where we evaluate the difference between pronoun resolution\naccuracies for image subjects with gender presentations perceived as masculine\nversus feminine by human annotators and ii) retrieval bias, where we compare\nratios of professionals perceived to have masculine and feminine gender\npresentations retrieved for a gender-neutral search query. We benchmark several\nstate-of-the-art vision-language models and find that they demonstrate bias in\nresolving binary gender in complex scenes. While the direction and magnitude of\ngender bias depends on the task and the model being evaluated, captioning\nmodels are generally less biased than Vision-Language Encoders. Dataset and\ncode are available at https://github.com/oxai/visogender\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1\">Siobhan Mackenzie Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrantes_F/0/1/0/all/0/1\">Fernanda Gon&#xe7;alves Abrantes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hanwen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sodunke_G/0/1/0/all/0/1\">Grace Sodunke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1\">Aleksandar Shtedritski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling. (arXiv:2308.06077v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.06077","description":"<p>Generative language models (LMs) have become omnipresent across data science.\nFor a wide variety of tasks, inputs can be phrased as natural language prompts\nfor an LM, from whose output the solution can then be extracted. LM performance\nhas consistently been increasing with model size - but so has the monetary cost\nof querying the ever larger models. Importantly, however, not all inputs are\nequally hard: some require larger LMs for obtaining a satisfactory solution,\nwhereas for others smaller LMs suffice. Based on this fact, we design a\nframework for Cost-Effective Language Model Choice (CELMOC). Given a set of\ninputs and a set of candidate LMs, CELMOC judiciously assigns each input to an\nLM predicted to do well on the input according to a so-called meta-model,\naiming to achieve high overall performance at low cost. The cost-performance\ntrade-off can be flexibly tuned by the user. Options include, among others,\nmaximizing total expected performance (or the number of processed inputs) while\nstaying within a given cost budget, or minimizing total cost while processing\nall inputs. We evaluate CELMOC on 14 datasets covering five natural language\ntasks, using four candidate LMs of vastly different size and cost. With CELMOC,\nwe match the performance of the largest available LM while achieving a cost\nreduction of 63%. Via our publicly available library, researchers as well as\npractitioners can thus save large amounts of money without sacrificing\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakota_M/0/1/0/all/0/1\">Marija &#x160;akota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Agents: An Open-source Framework for Autonomous Language Agents. (arXiv:2309.07870v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.07870","description":"<p>Recent advances on large language models (LLMs) enable researchers and\ndevelopers to build autonomous language agents that can automatically solve\nvarious tasks and interact with environments, humans, and other agents using\nnatural language interfaces. We consider language agents as a promising\ndirection towards artificial general intelligence and release Agents, an\nopen-source library with the goal of opening up these advances to a wider\nnon-specialist audience. Agents is carefully engineered to support important\nfeatures including planning, memory, tool usage, multi-agent communication, and\nfine-grained symbolic control. Agents is user-friendly as it enables\nnon-specialists to build, customize, test, tune, and deploy state-of-the-art\nautonomous language agents without much coding. The library is also\nresearch-friendly as its modularized design makes it easily extensible for\nresearchers. Agents is available at https://github.com/aiwaves-cn/agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Eleanor Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Long Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jintian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruipu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shiding Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wentao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.09357","description":"<p>Despite the plethora of telehealth applications to assist home-based older\nadults and healthcare providers, basic messaging and phone calls are still the\nmost common communication methods, which suffer from limited availability,\ninformation loss, and process inefficiencies. One promising solution to\nfacilitate patient-provider communication is to leverage large language models\n(LLMs) with their powerful natural conversation and summarization capability.\nHowever, there is a limited understanding of LLMs' role during the\ncommunication. We first conducted two interview studies with both older adults\n(N=10) and healthcare providers (N=9) to understand their needs and\nopportunities for LLMs in patient-provider asynchronous communication. Based on\nthe insights, we built an LLM-powered communication system, Talk2Care, and\ndesigned interactive components for both groups: (1) For older adults, we\nleveraged the convenience and accessibility of voice assistants (VAs) and built\nan LLM-powered VA interface for effective information collection. (2) For\nhealth providers, we built an LLM-based dashboard to summarize and present\nimportant health information based on older adults' conversations with the VA.\nWe further conducted two user studies with older adults and providers to\nevaluate the usability of the system. The results showed that Talk2Care could\nfacilitate the communication process, enrich the health information collected\nfrom older adults, and considerably save providers' efforts and time. We\nenvision our work as an initial exploration of LLMs' capability in the\nintersection of healthcare and interpersonal communication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuhai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_E/0/1/0/all/0/1\">Ethan Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Intille_S/0/1/0/all/0/1\">Stephen Intille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shara_N/0/1/0/all/0/1\">Nawar Shara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guodong Gordon Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can LLM-Generated Misinformation Be Detected?. (arXiv:2309.13788v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.13788","description":"<p>The advent of Large Language Models (LLMs) has made a transformative impact.\nHowever, the potential that LLMs such as ChatGPT can be exploited to generate\nmisinformation has posed a serious concern to online safety and public trust. A\nfundamental research question is: will LLM-generated misinformation cause more\nharm than human-written misinformation? We propose to tackle this question from\nthe perspective of detection difficulty. We first build a taxonomy of\nLLM-generated misinformation. Then we categorize and validate the potential\nreal-world methods for generating misinformation with LLMs. Then, through\nextensive empirical investigation, we discover that LLM-generated\nmisinformation can be harder to detect for humans and detectors compared to\nhuman-written misinformation with the same semantics, which suggests it can\nhave more deceptive styles and potentially cause more harm. We also discuss the\nimplications of our discovery on combating misinformation in the age of LLMs\nand the countermeasures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Canyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1\">Kai Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Streaming Language Models with Attention Sinks. (arXiv:2309.17453v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.17453","description":"<p>Deploying Large Language Models (LLMs) in streaming applications such as\nmulti-round dialogue, where long interactions are expected, is urgently needed\nbut poses two major challenges. Firstly, during the decoding stage, caching\nprevious tokens' Key and Value states (KV) consumes extensive memory. Secondly,\npopular LLMs cannot generalize to longer texts than the training sequence\nlength. Window attention, where only the most recent KVs are cached, is a\nnatural approach -- but we show that it fails when the text length surpasses\nthe cache size. We observe an interesting phenomenon, namely attention sink,\nthat keeping the KV of initial tokens will largely recover the performance of\nwindow attention. In this paper, we first demonstrate that the emergence of\nattention sink is due to the strong attention scores towards initial tokens as\na ``sink'' even if they are not semantically important. Based on the above\nanalysis, we introduce StreamingLLM, an efficient framework that enables LLMs\ntrained with a finite length attention window to generalize to infinite\nsequence lengths without any fine-tuning. We show that StreamingLLM can enable\nLlama-2, MPT, Falcon, and Pythia to perform stable and efficient language\nmodeling with up to 4 million tokens and more. In addition, we discover that\nadding a placeholder token as a dedicated attention sink during pre-training\ncan further improve streaming deployment. In streaming settings, StreamingLLM\noutperforms the sliding window recomputation baseline by up to 22.2x speedup.\nCode and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Guangxuan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beidi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment analysis with adaptive multi-head attention in Transformer. (arXiv:2310.14505v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.14505","description":"<p>We propose a novel framework based on the attention mechanism to identify the\nsentiment of a movie review document. Previous efforts on deep neural networks\nwith attention mechanisms focus on encoder and decoder with fixed numbers of\nmulti-head attention. Therefore, we need a mechanism to stop the attention\nprocess automatically if no more useful information can be read from the\nmemory.In this paper, we propose an adaptive multi-head attention architecture\n(AdaptAttn) which varies the number of attention heads based on length of\nsentences. AdaptAttn has a data preprocessing step where each document is\nclassified into any one of the three bins small, medium or large based on\nlength of the sentence. The document classified as small goes through two heads\nin each layer, the medium group passes four heads and the large group is\nprocessed by eight heads. We examine the merit of our model on the Stanford\nlarge movie review dataset. The experimental results show that the F1 score\nfrom our model is on par with the baseline model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fanfei Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeter_D/0/1/0/all/0/1\">David Demeter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Muslim-Violence Bias Persists in Debiased GPT Models. (arXiv:2310.18368v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.18368","description":"<p>Abid et al. (2021) showed a tendency in GPT-3 to generate mostly violent\ncompletions when prompted about Muslims, compared with other religions. Two\npre-registered replication attempts found few violent completions and only a\nweak anti-Muslim bias in the more recent InstructGPT, fine-tuned to eliminate\nbiased and toxic outputs. However, more pre-registered experiments showed that\nusing common names associated with the religions in prompts increases\nseveral-fold the rate of violent completions, revealing a significant\nsecond-order anti-Muslim bias. ChatGPT showed a bias many times stronger\nregardless of prompt format, suggesting that the effects of debiasing were\nreduced with continued model development. Our content analysis revealed\nreligion-specific themes containing offensive stereotypes across all\nexperiments. Our results show the need for continual de-biasing of models in\nways that address both explicit and higher-order associations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hemmatian_B/0/1/0/all/0/1\">Babak Hemmatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltaji_R/0/1/0/all/0/1\">Razan Baltaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_L/0/1/0/all/0/1\">Lav R. Varshney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration. (arXiv:2311.06062v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.06062","description":"<p>Membership Inference Attacks (MIA) aim to infer whether a target data record\nhas been utilized for model training or not. Prior attempts have quantified the\nprivacy risks of language models (LMs) via MIAs, but there is still no\nconsensus on whether existing MIA algorithms can cause remarkable privacy\nleakage on practical Large Language Models (LLMs). Existing MIAs designed for\nLMs can be classified into two categories: reference-free and reference-based\nattacks. They are both based on the hypothesis that training records\nconsistently strike a higher probability of being sampled. Nevertheless, this\nhypothesis heavily relies on the overfitting of target models, which will be\nmitigated by multiple regularization methods and the generalization of LLMs.\nThe reference-based attack seems to achieve promising effectiveness in LLMs,\nwhich measures a more reliable membership signal by comparing the probability\ndiscrepancy between the target model and the reference model. However, the\nperformance of reference-based attack is highly dependent on a reference\ndataset that closely resembles the training dataset, which is usually\ninaccessible in the practical scenario. Overall, existing MIAs are unable to\neffectively unveil privacy leakage over practical fine-tuned LLMs that are\noverfitting-free and private. We propose a Membership Inference Attack based on\nSelf-calibrated Probabilistic Variation (SPV-MIA). Specifically, since\nmemorization in LLMs is inevitable during the training process and occurs\nbefore overfitting, we introduce a more reliable membership signal,\nprobabilistic variation, which is based on memorization rather than\noverfitting. Furthermore, we introduce a self-prompt approach, which constructs\nthe dataset to fine-tune the reference model by prompting the target LLM\nitself. In this manner, the adversary can collect a dataset with a similar\ndistribution from public APIs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1\">Wenjie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huandong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanghua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Transient Nature of Emergent In-Context Learning in Transformers. (arXiv:2311.08360v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.08360","description":"<p>Transformer neural networks can exhibit a surprising capacity for in-context\nlearning (ICL) despite not being explicitly trained for it. Prior work has\nprovided a deeper understanding of how ICL emerges in transformers, e.g.\nthrough the lens of mechanistic interpretability, Bayesian inference, or by\nexamining the distributional properties of training data. However, in each of\nthese cases, ICL is treated largely as a persistent phenomenon; namely, once\nICL emerges, it is assumed to persist asymptotically. Here, we show that the\nemergence of ICL during transformer training is, in fact, often transient. We\ntrain transformers on synthetic data designed so that both ICL and in-weights\nlearning (IWL) strategies can lead to correct predictions. We find that ICL\nfirst emerges, then disappears and gives way to IWL, all while the training\nloss decreases, indicating an asymptotic preference for IWL. The transient\nnature of ICL is observed in transformers across a range of model sizes and\ndatasets, raising the question of how much to \"overtrain\" transformers when\nseeking compact, cheaper-to-run models. We find that L2 regularization may\noffer a path to more persistent ICL that removes the need for early stopping\nbased on ICL-style validation tasks. Finally, we present initial evidence that\nICL transience may be caused by competition between ICL and IWL circuits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya K. Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stephanie C.Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moskovitz_T/0/1/0/all/0/1\">Ted Moskovitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1\">Erin Grant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1\">Andrew M. Saxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies. (arXiv:2312.04344v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.04344","description":"<p>OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued\nconsiderable interest for its potential in medical applications. Despite its\npromise, recent studies and internal reviews highlight its underperformance in\nspecialized medical tasks. This paper explores the boundary of GPT-4V's\ncapabilities in medicine, particularly in processing complex imaging data from\nendoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we\nassessed its foundational competencies, identifying substantial areas for\nenhancement. Our research emphasizes prompt engineering, an often-underutilized\nstrategy for improving AI responsiveness. Through iterative testing, we refined\nthe model's prompts, significantly improving its interpretative accuracy and\nrelevance in medical imaging. From our comprehensive evaluations, we distilled\n10 effective prompt engineering techniques, each fortifying GPT-4V's medical\nacumen. These methodical enhancements facilitate more reliable, precise, and\nclinically valuable insights from GPT-4V, advancing its operability in critical\nhealthcare environments. Our findings are pivotal for those employing AI in\nmedicine, providing clear, actionable guidance on harnessing GPT-4V's full\ndiagnostic potential.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pengcheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhongying Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yanzhou Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junjun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models. (arXiv:2312.04691v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.04691","description":"<p>Large language models (LLMs) with billions of parameters and pretrained on\nmassive amounts of data are now capable of near or better than state-of-the-art\nperformance in a variety of downstream natural language processing tasks.\nNeural machine translation (NMT) is one such task that LLMs have been applied\nto with great success. However, little research has focused on applying LLMs to\nthe more difficult subset of NMT called simultaneous translation (SimulMT),\nwhere translation begins before the entire source context is available to the\nmodel. In this paper, we address key challenges facing LLMs fine-tuned for\nSimulMT, validate classical SimulMT concepts and practices in the context of\nLLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT,\nand introduce Simul-LLM, the first open-source fine-tuning and evaluation\npipeline development framework for LLMs focused on SimulMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agostinelli_V/0/1/0/all/0/1\">Victor Agostinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wild_M/0/1/0/all/0/1\">Max Wild</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_M/0/1/0/all/0/1\">Matthew Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuad_K/0/1/0/all/0/1\">Kazi Ahmed Asif Fuad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lizhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localized Symbolic Knowledge Distillation for Visual Commonsense Models. (arXiv:2312.04837v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2312.04837","description":"<p>Instruction following vision-language (VL) models offer a flexible interface\nthat supports a broad range of multimodal tasks in a zero-shot fashion.\nHowever, interfaces that operate on full images do not directly enable the user\nto \"point to\" and access specific regions within images. This capability is\nimportant not only to support reference-grounded VL benchmarks, but also, for\npractical applications that require precise within-image reasoning. We build\nLocalized Visual Commonsense models, which allow users to specify (multiple)\nregions as input. We train our model by sampling localized commonsense\nknowledge from a large language model (LLM): specifically, we prompt an LLM to\ncollect commonsense knowledge given a global literal image description and a\nlocal literal region description automatically generated by a set of VL models.\nWith a separately trained critic model that selects high-quality examples, we\nfind that training on the localized commonsense corpus can successfully distill\nexisting VL models to support a reference-as-input interface. Empirical results\nand human evaluations in a zero-shot setup demonstrate that our distillation\nmethod results in more precise VL models of reasoning compared to a baseline of\npassing a generated referring expression to an LLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jae Sung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1\">Khyathi Raghavi Chandu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiuyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PathFinder: Guided Search over Multi-Step Reasoning Paths. (arXiv:2312.05180v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.05180","description":"<p>With recent advancements in large language models, methods like\nchain-of-thought prompting to elicit reasoning chains have been shown to\nimprove results on reasoning tasks. However, tasks that require multiple steps\nof reasoning still pose significant challenges to state-of-the-art models.\nDrawing inspiration from the beam search algorithm, we propose PathFinder, a\ntree-search-based reasoning path generation approach. It enhances diverse\nbranching and multi-hop reasoning through the integration of dynamic decoding,\nenabled by varying sampling methods and parameters. Using constrained\nreasoning, PathFinder integrates novel quality constraints, pruning, and\nexploration methods to enhance the efficiency and the quality of generation.\nMoreover, it includes scoring and ranking features to improve candidate\nselection. Our approach outperforms competitive baselines on three complex\narithmetic and commonsense reasoning tasks by 6% on average. Our model\ngeneralizes well to longer, unseen reasoning chains, reflecting similar\ncomplexities to beam search with large branching factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golovneva_O/0/1/0/all/0/1\">Olga Golovneva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_S/0/1/0/all/0/1\">Sean O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazel_Zarandi_M/0/1/0/all/0/1\">Maryam Fazel-Zarandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis. (arXiv:2312.05488v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2312.05488","description":"<p>Game theory, as an analytical tool, is frequently utilized to analyze human\nbehavior in social science research. With the high alignment between the\nbehavior of Large Language Models (LLMs) and humans, a promising research\ndirection is to employ LLMs as substitutes for humans in game experiments,\nenabling social science research. However, despite numerous empirical\nresearches on the combination of LLMs and game theory, the capability\nboundaries of LLMs in game theory remain unclear. In this research, we endeavor\nto systematically analyze LLMs in the context of game theory. Specifically,\nrationality, as the fundamental principle of game theory, serves as the metric\nfor evaluating players' behavior -- building a clear desire, refining belief\nabout uncertainty, and taking optimal actions. Accordingly, we select three\nclassical games (dictator game, Rock-Paper-Scissors, and ring-network game) to\nanalyze to what extent LLMs can achieve rationality in these three aspects. The\nexperimental results indicate that even the current state-of-the-art LLM\n(GPT-4) exhibits substantial disparities compared to humans in game theory. For\ninstance, LLMs struggle to build desires based on uncommon preferences, fail to\nrefine belief from many simple patterns, and may overlook or modify refined\nbelief when taking actions. Therefore, we consider that introducing LLMs into\ngame experiments in the field of social science should be approached with\ngreater caution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Caoyun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jindou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaohui Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"History Matters: Temporal Knowledge Editing in Large Language Model. (arXiv:2312.05497v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.05497","description":"<p>The imperative task of revising or updating the knowledge stored within large\nlanguage models arises from two distinct sources: intrinsic errors inherent in\nthe model which should be corrected and outdated knowledge due to external\nshifts in the real world which should be updated. Prevailing efforts in model\nediting conflate these two distinct categories of edits arising from distinct\nreasons and directly modify the original knowledge in models into new\nknowledge. However, we argue that preserving the model's original knowledge\nremains pertinent. Specifically, if a model's knowledge becomes outdated due to\nevolving worldly dynamics, it should retain recollection of the historical\nknowledge while integrating the newfound knowledge. In this work, we introduce\nthe task of Temporal Knowledge Editing (TKE) and establish a benchmark AToKe\n(Assessment of TempOral Knowledge Editing) to evaluate current model editing\nmethods. We find that while existing model editing methods are effective at\nmaking models remember new knowledge, the edited model catastrophically forgets\nhistorical knowledge. To address this gap, we propose a simple and general\nframework termed Multi-Editing with Time Objective (METO) for enhancing\nexisting editing models, which edits both historical and new knowledge\nconcurrently and optimizes the model's prediction for the time of each fact.\nOur assessments demonstrate that while AToKe is still difficult, METO maintains\nthe effectiveness of learning new knowledge and meanwhile substantially\nimproves the performance of edited models on utilizing historical knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xunjian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim-GPT: Text Similarity via GPT Annotated Data. (arXiv:2312.05603v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.05603","description":"<p>Due to the lack of a large collection of high-quality labeled sentence pairs\nwith textual similarity scores, existing approaches for Semantic Textual\nSimilarity (STS) mostly rely on unsupervised techniques or training signals\nthat are only partially correlated with textual similarity, e.g., NLI-based\ndatasets. To tackle this issue, in this paper, we propose the strategy of\nmeasuring text similarity via GPT annotated data (Sim-GPT for short). The core\nidea of Sim-GPT is to generate data with STS labels using GPT-4, based on which\nan STS model is trained. Sim-GPT framework utilizes LLMs to provide a\nsubstantial amount of reliable annotated data filling the gap of the lack of\ntraining signals for STS. Sim-GPT is trained on a one-time generated dataset\nusing BERT or RoBERTa as the backbone, which offers long-term savings in cost\nand speed compared to repeatedly invoking LLMs for each sentence pair. Trained\non the examples from GPT-4 (371K), Sim-GPT yields SOTA performances on the\nwidely-used seven STS benchmarks: +0.99 over supervised-SimCSE, and +0.42 over\nthe current SOTA PromCSE model. To encourage further advancements of the field,\nwe release both models and the 371K annotated examples from GPT-4. Code, models\nand annotated data are available at: https://github.com/ShuheWang1998/Sim-GPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Beiming Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Effect of Model Compression on Social Bias in Large Language Models. (arXiv:2312.05662v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.05662","description":"<p>Large Language Models (LLMs) trained with self-supervision on vast corpora of\nweb text fit to the social biases of that text. Without intervention, these\nsocial biases persist in the model's predictions in downstream tasks, leading\nto representational harm. Many strategies have been proposed to mitigate the\neffects of inappropriate social biases learned during pretraining.\nSimultaneously, methods for model compression have become increasingly popular\nto reduce the computational burden of LLMs. Despite the popularity and need for\nboth approaches, little work has been done to explore the interplay between\nthese two. We perform a carefully controlled study of the impact of model\ncompression via quantization and knowledge distillation on measures of social\nbias in LLMs. Longer pretraining and larger models led to higher social bias,\nand quantization showed a regularizer effect with its best trade-off around 20%\nof the original pretraining time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_G/0/1/0/all/0/1\">Gustavo Gon&#xe7;alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples. (arXiv:2312.06363v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2312.06363","description":"<p>Although In-Context Learning (ICL) brings remarkable performance gains to\nLarge Language Models (LLMs), the improvements remain lower than fine-tuning on\ndownstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),\na novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by\nfully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We\npropose the Multi-Modal Hub (M-Hub), a unified module that captures various\nmulti-modal features according to different inputs and objectives. Based on\nM-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual\nfeatures and subsequently generate outputs conditioned on the textual-guided\nvisual features. Moreover, leveraging the flexibility of M-Hub, we design a\nvariety of in-context demonstrations. Extensive experiments on a diverse range\nof downstream multi-modal tasks demonstrate that MMICT significantly\noutperforms traditional fine-tuning strategy and the vanilla ICT method that\ndirectly takes the concatenation of all information from different modalities\nas input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1\">Enwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gated Linear Attention Transformers with Hardware-Efficient Training. (arXiv:2312.06635v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2312.06635","description":"<p>Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear (with respect to output length) inference\ncomplexity. Recent works such as RetNet (Sun et al., 2023) and TransNormerLLM\n(Qin et al., 2023a) observe that adding a global decay term to the additive RNN\nupdate rule greatly improves performance, sometimes outperforming standard\nTransformers with softmax attention when trained at scale. In this work we show\nthat adding a data-dependent gating mechanism further improves performance. We\nderive a parallel form of this gated linear attention layer that enables\nefficient training. However, a straightforward, numerically stable\nimplementation of this parallel form requires generalized matrix\nmultiplications in log-space for numerical stability, and thus cannot take\nadvantage of tensor cores on modern GPUs which are optimized for standard\nmatrix multiplications. We develop a hardware-efficient version of the parallel\nform that can still make use of tensor cores through block-parallel\ncomputations over sequence chunks. Experiments on moderate-scale language\nmodeling (340M-parameter models trained on 15B tokens, 1.3B-parameter models\ntrained on 100B tokens) show that gated linear attention (GLA) Transformers\nperform competitively against a strong LLaMA-architecture Transformer baseline\n(Touvron et al., 2023) as well as Mamba (Gu &amp; Dao, 2023), a recently introduced\nstate-space model with a data-dependent state transition mechanism. For\ntraining speed, our Triton-based implementation performs comparably to\nCUDA-optimized FlashAttention-2 (Dao, 2023) under the regular 2048 training\nlength setting, while outperforming FlashAttention-2 when training on longer\nsequences beyond 4096.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yikang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense X Retrieval: What Retrieval Granularity Should We Use?. (arXiv:2312.06648v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.06648","description":"<p>Dense retrieval has become a prominent method to obtain relevant context or\nworld knowledge in open-domain NLP tasks. When we use a learned dense retriever\non a retrieval corpus at inference time, an often-overlooked design choice is\nthe retrieval unit in which the corpus is indexed, e.g. document, passage, or\nsentence. We discover that the retrieval unit choice significantly impacts the\nperformance of both retrieval and downstream tasks. Distinct from the typical\napproach of using passages or sentences, we introduce a novel retrieval unit,\nproposition, for dense retrieval. Propositions are defined as atomic\nexpressions within text, each encapsulating a distinct factoid and presented in\na concise, self-contained natural language format. We conduct an empirical\ncomparison of different retrieval granularity. Our results reveal that\nproposition-based retrieval significantly outperforms traditional passage or\nsentence-based methods in dense retrieval. Moreover, retrieval by proposition\nalso enhances the performance of downstream QA tasks, since the retrieved texts\nare more condensed with question-relevant information, reducing the need for\nlengthy input tokens and minimizing the inclusion of extraneous, irrelevant\ninformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinran Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-12-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}