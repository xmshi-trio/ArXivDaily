{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-09-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Classifying Organizations for Food System Ontologies using Natural Language Processing. (arXiv:2309.10880v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10880","description":"<p>Our research explores the use of natural language processing (NLP) methods to\nautomatically classify entities for the purpose of knowledge graph population\nand integration with food system ontologies. We have created NLP models that\ncan automatically classify organizations with respect to categories associated\nwith environmental issues as well as Standard Industrial Classification (SIC)\ncodes, which are used by the U.S. government to characterize business\nactivities. As input, the NLP models are provided with text snippets retrieved\nby the Google search engine for each organization, which serves as a textual\ndescription of the organization that is used for learning. Our experimental\nresults show that NLP models can achieve reasonably good performance for these\ntwo classification tasks, and they rely on a general framework that could be\napplied to many other classification problems as well. We believe that NLP\nmodels represent a promising approach for automatically harvesting information\nto populate knowledge graphs and aligning the information with existing\nontologies through shared categories and concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinogradova_S/0/1/0/all/0/1\">Sonia Vinogradova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stringham_N/0/1/0/all/0/1\">Nathan Stringham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earl_E/0/1/0/all/0/1\">E. Louise Earl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollander_A/0/1/0/all/0/1\">Allan D. Hollander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1\">Patrick R. Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riloff_E/0/1/0/all/0/1\">Ellen Riloff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schillo_R/0/1/0/all/0/1\">R. Sandra Schillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ubbiali_G/0/1/0/all/0/1\">Giorgio A. Ubbiali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_M/0/1/0/all/0/1\">Matthew Lange</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer. (arXiv:2309.10891v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10891","description":"<p>Zero-shot cross-lingual transfer is a central task in multilingual NLP,\nallowing models trained in languages with more sufficient training resources to\ngeneralize to other low-resource languages. Earlier efforts on this task use\nparallel corpora, bilingual dictionaries, or other annotated alignment data to\nimprove cross-lingual transferability, which are typically expensive to obtain.\nIn this paper, we propose a simple yet effective method, SALT, to improve the\nzero-shot cross-lingual transfer of the multilingual pretrained language models\nwithout the help of such external data. By incorporating code-switching and\nembedding mixup with self-augmentation, SALT effectively distills cross-lingual\nknowledge from the multilingual PLM and enhances its transferability on\ndownstream tasks. Experimental results on XNLI and PAWS-X show that our method\nis able to improve zero-shot cross-lingual transferability without external\ndata. Our code is available at https://github.com/luka-group/SALT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RedPenNet for Grammatical Error Correction: Outputs to Tokens, Attentions to Spans. (arXiv:2309.10898v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10898","description":"<p>The text editing tasks, including sentence fusion, sentence splitting and\nrephrasing, text simplification, and Grammatical Error Correction (GEC), share\na common trait of dealing with highly similar input and output sequences. This\narea of research lies at the intersection of two well-established fields: (i)\nfully autoregressive sequence-to-sequence approaches commonly used in tasks\nlike Neural Machine Translation (NMT) and (ii) sequence tagging techniques\ncommonly used to address tasks such as Part-of-speech tagging, Named-entity\nrecognition (NER), and similar. In the pursuit of a balanced architecture,\nresearchers have come up with numerous imaginative and unconventional\nsolutions, which we're discussing in the Related Works section. Our approach to\naddressing text editing tasks is called RedPenNet and is aimed at reducing\narchitectural and parametric redundancies presented in specific\nSequence-To-Edits models, preserving their semi-autoregressive advantages. Our\nmodels achieve $F_{0.5}$ scores of 77.60 on the BEA-2019 (test), which can be\nconsidered as state-of-the-art the only exception for system combination and\n67.71 on the UAGEC+Fluency (test) benchmarks.\n</p>\n<p>This research is being conducted in the context of the UNLP 2023 workshop,\nwhere it was presented as a paper as a paper for the Shared Task in Grammatical\nError Correction (GEC) for Ukrainian. This study aims to apply the RedPenNet\napproach to address the GEC problem in the Ukrainian language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Didenko_B/0/1/0/all/0/1\">Bohdan Didenko</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sameliuk_A/0/1/0/all/0/1\">Andrii Sameliuk</a> (1) ((1) WebSpellChecker LLC / Ukraine)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples. (arXiv:2309.10916v1 [cs.LG])","link":"http://arxiv.org/abs/2309.10916","description":"<p>Adversarial examples, deliberately crafted using small perturbations to fool\ndeep neural networks, were first studied in image processing and more recently\nin NLP. While approaches to detecting adversarial examples in NLP have largely\nrelied on search over input perturbations, image processing has seen a range of\ntechniques that aim to characterise adversarial subspaces over the learned\nrepresentations.\n</p>\n<p>In this paper, we adapt two such approaches to NLP, one based on nearest\nneighbors and influence functions and one on Mahalanobis distances. The former\nin particular produces a state-of-the-art detector when compared against\nseveral strong baselines; moreover, the novel use of influence functions\nprovides insight into how the nature of adversarial example subspaces in NLP\nrelate to those in image processing, and also how they differ depending on the\nkind of NLP task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tonni_S/0/1/0/all/0/1\">Shakila Mahjabin Tonni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dras_M/0/1/0/all/0/1\">Mark Dras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Speech Recognition Contextualization with Large Language Models. (arXiv:2309.10917v1 [eess.AS])","link":"http://arxiv.org/abs/2309.10917","description":"<p>In recent years, Large Language Models (LLMs) have garnered significant\nattention from the research community due to their exceptional performance and\ngeneralization capabilities. In this paper, we introduce a novel method for\ncontextualizing speech recognition models incorporating LLMs. Our approach\ncasts speech recognition as a mixed-modal language modeling task based on a\npretrained LLM. We provide audio features, along with optional text tokens for\ncontext, to train the system to complete transcriptions in a decoder-only\nfashion. As a result, the system is implicitly incentivized to learn how to\nleverage unstructured contextual information during training. Our empirical\nresults demonstrate a significant improvement in performance, with a 6% WER\nreduction when additional textual context is provided. Moreover, we find that\nour method performs competitively and improve by 7.5% WER overall and 17% WER\non rare words against a baseline contextualized RNN-T system that has been\ntrained on more than twenty five times larger speech dataset. Overall, we\ndemonstrate that by only adding a handful number of trainable parameters via\nadapters, we can unlock contextualized speech recognition capability for the\npretrained LLM while keeping the same text-only input functionality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lakomkin_E/0/1/0/all/0/1\">Egor Lakomkin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chunyang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fathullah_Y/0/1/0/all/0/1\">Yassir Fathullah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seltzer_M/0/1/0/all/0/1\">Michael L. Seltzer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fuegen_C/0/1/0/all/0/1\">Christian Fuegen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-automatic staging area for high-quality structured data extraction from scientific literature. (arXiv:2309.10923v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10923","description":"<p>In this study, we propose a staging area for ingesting new superconductors'\nexperimental data in SuperCon that is machine-collected from scientific\narticles. Our objective is to enhance the efficiency of updating SuperCon while\nmaintaining or enhancing the data quality. We present a semi-automatic staging\narea driven by a workflow combining automatic and manual processes on the\nextracted database. An anomaly detection automatic process aims to pre-screen\nthe collected data. Users can then manually correct any errors through a user\ninterface tailored to simplify the data verification on the original PDF\ndocuments. Additionally, when a record is corrected, its raw data is collected\nand utilised to improve machine learning models as training data. Evaluation\nexperiments demonstrate that our staging area significantly improves curation\nquality. We compare the interface with the traditional manual approach of\nreading PDF documents and recording information in an Excel document. Using the\ninterface boosts the precision and recall by 6% and 50%, respectively to an\naverage increase of 40% in F1-score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foppiano_L/0/1/0/all/0/1\">Luca Foppiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mato_T/0/1/0/all/0/1\">Tomoya Mato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terashima_K/0/1/0/all/0/1\">Kensei Terashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_P/0/1/0/all/0/1\">Pedro Ortiz Suarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_T/0/1/0/all/0/1\">Taku Tou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakai_C/0/1/0/all/0/1\">Chikako Sakai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei-Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amagasa_T/0/1/0/all/0/1\">Toshiyuki Amagasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takano_Y/0/1/0/all/0/1\">Yoshihiko Takano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_M/0/1/0/all/0/1\">Masashi Ishii</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Autoregressive Streaming ASR With Label Context. (arXiv:2309.10926v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10926","description":"<p>Non-autoregressive (NAR) modeling has gained significant interest in speech\nprocessing since these models achieve dramatically lower inference time than\nautoregressive (AR) models while also achieving good transcription accuracy.\nSince NAR automatic speech recognition (ASR) models must wait for the\ncompletion of the entire utterance before processing, some works explore\nstreaming NAR models based on blockwise attention for low-latency applications.\nHowever, streaming NAR models significantly lag in accuracy compared to\nstreaming AR and non-streaming NAR models. To address this, we propose a\nstreaming \"semi-autoregressive\" ASR model that incorporates the labels emitted\nin previous blocks as additional context using a Language Model (LM)\nsubnetwork. We also introduce a novel greedy decoding algorithm that addresses\ninsertion and deletion errors near block boundaries while not significantly\nincreasing the inference time. Experiments show that our method outperforms the\nexisting streaming NAR model by 19% relative on Tedlium2, 16%/8% on\nLibrispeech-100 clean/other test sets, and 19%/8% on the Switchboard(SWB) /\nCallhome(CH) test sets. It also reduced the accuracy gap with streaming AR and\nnon-streaming NAR models while achieving 2.5x lower latency. We also\ndemonstrate that our approach can effectively utilize external text data to\npre-train the LM subnetwork to further improve streaming ASR accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saon_G/0/1/0/all/0/1\">George Saon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training. (arXiv:2309.10929v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10929","description":"<p>In this work, we introduce the concept of complex text style transfer tasks,\nand constructed complex text datasets based on two widely applicable scenarios.\nOur dataset is the first large-scale data set of its kind, with 700 rephrased\nsentences and 1,000 sentences from the game Genshin Impact. While large\nlanguage models (LLM) have shown promise in complex text style transfer, they\nhave drawbacks such as data privacy concerns, network instability, and high\ndeployment costs. To address these issues, we explore the effectiveness of\nsmall models (less than T5-3B) with implicit style pre-training through\ncontrastive learning. We also propose a method for automated evaluation of text\ngeneration quality based on alignment with human evaluations using ChatGPT.\nFinally, we compare our approach with existing methods and show that our model\nachieves state-of-art performances of few-shot text style transfer models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruiqi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Family of Pretrained Transformer Language Models for Russian. (arXiv:2309.10931v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10931","description":"<p>Nowadays, Transformer language models (LMs) represent a fundamental component\nof the NLP research methodologies and applications. However, the development of\nsuch models specifically for the Russian language has received little\nattention. This paper presents a collection of 13 Russian Transformer LMs based\non the encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and\nencoder-decoder (ruT5, FRED-T5) models in multiple sizes. Access to these\nmodels is readily available via the HuggingFace platform. We provide a report\nof the model architecture design and pretraining, and the results of evaluating\ntheir generalization abilities on Russian natural language understanding and\ngeneration datasets and benchmarks. By pretraining and releasing these\nspecialized Transformer LMs, we hope to broaden the scope of the NLP research\ndirections and enable the development of industrial solutions for the Russian\nlanguage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zmitrovich_D/0/1/0/all/0/1\">Dmitry Zmitrovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abramov_A/0/1/0/all/0/1\">Alexander Abramov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalmykov_A/0/1/0/all/0/1\">Andrey Kalmykov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonova_M/0/1/0/all/0/1\">Maria Tikhonova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taktasheva_E/0/1/0/all/0/1\">Ekaterina Taktasheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astafurov_D/0/1/0/all/0/1\">Danil Astafurov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baushenko_M/0/1/0/all/0/1\">Mark Baushenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snegirev_A/0/1/0/all/0/1\">Artem Snegirev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavrina_T/0/1/0/all/0/1\">Tatiana Shavrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markov_S/0/1/0/all/0/1\">Sergey Markov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fenogenova_A/0/1/0/all/0/1\">Alena Fenogenova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarks for Pir\\'a 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change. (arXiv:2309.10945v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10945","description":"<p>Pir\\'a is a reading comprehension dataset focused on the ocean, the Brazilian\ncoast, and climate change, built from a collection of scientific abstracts and\nreports on these topics. This dataset represents a versatile language resource,\nparticularly useful for testing the ability of current machine learning models\nto acquire expert scientific knowledge. Despite its potential, a detailed set\nof baselines has not yet been developed for Pir\\'a. By creating these\nbaselines, researchers can more easily utilize Pir\\'a as a resource for testing\nmachine learning models across a wide range of question answering tasks. In\nthis paper, we define six benchmarks over the Pir\\'a dataset, covering closed\ngenerative question answering, machine reading comprehension, information\nretrieval, open question answering, answer triggering, and multiple choice\nquestion answering. As part of this effort, we have also produced a curated\nversion of the original dataset, where we fixed a number of grammar issues,\nrepetitions, and other shortcomings. Furthermore, the dataset has been extended\nin several new directions, so as to face the aforementioned benchmarks:\ntranslation of supporting texts from English into Portuguese, classification\nlabels for answerability, automatic paraphrases of questions and answers, and\nmultiple choice candidates. The results described in this paper provide several\npoints of reference for researchers interested in exploring the challenges\nprovided by the Pir\\'a dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pirozelli_P/0/1/0/all/0/1\">Paulo Pirozelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_M/0/1/0/all/0/1\">Marcos M. Jos&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silveira_I/0/1/0/all/0/1\">Igor Silveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakasato_F/0/1/0/all/0/1\">Fl&#xe1;vio Nakasato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peres_S/0/1/0/all/0/1\">Sarajane M. Peres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandao_A/0/1/0/all/0/1\">Anarosa A. F. Brand&#xe3;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1\">Anna H. R. Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cozman_F/0/1/0/all/0/1\">Fabio G. Cozman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMDX: Language Model-based Document Information Extraction and Localization. (arXiv:2309.10952v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10952","description":"<p>Large Language Models (LLM) have revolutionized Natural Language Processing\n(NLP), improving state-of-the-art on many existing tasks and exhibiting\nemergent capabilities. However, LLMs have not yet been successfully applied on\nsemi-structured document information extraction, which is at the core of many\ndocument processing workflows and consists of extracting key entities from a\nvisually rich document (VRD) given a predefined target schema. The main\nobstacles to LLM adoption in that task have been the absence of layout encoding\nwithin LLMs, critical for a high quality extraction, and the lack of a\ngrounding mechanism ensuring the answer is not hallucinated. In this paper, we\nintroduce Language Model-based Document Information Extraction and Localization\n(LMDX), a methodology to adapt arbitrary LLMs for document information\nextraction. LMDX can do extraction of singular, repeated, and hierarchical\nentities, both with and without training data, while providing grounding\nguarantees and localizing the entities within the document. In particular, we\napply LMDX to the PaLM 2-S LLM and evaluate it on VRDU and CORD benchmarks,\nsetting a new state-of-the-art and showing how LMDX enables the creation of\nhigh quality, data-efficient parsers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perot_V/0/1/0/all/0/1\">Vincent Perot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_K/0/1/0/all/0/1\">Kai Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luisier_F/0/1/0/all/0/1\">Florian Luisier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guolong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boppana_R/0/1/0/all/0/1\">Ramya Sree Boppana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jiaqi Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_N/0/1/0/all/0/1\">Nan Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Learning for Text Classification with Many Labels. (arXiv:2309.10954v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10954","description":"<p>In-context learning (ICL) using large language models for tasks with many\nlabels is challenging due to the limited context window, which makes it\ndifficult to fit a sufficient number of examples in the prompt. In this paper,\nwe use a pre-trained dense retrieval model to bypass this limitation, giving\nthe model only a partial view of the full label space for each inference call.\nTesting with recent open-source LLMs (OPT, LLaMA), we set new state of the art\nperformance in few-shot settings for three common intent classification\ndatasets, with no finetuning. We also surpass fine-tuned performance on\nfine-grained sentiment classification in certain cases. We analyze the\nperformance across number of in-context examples and different model scales,\nshowing that larger models are necessary to effectively and consistently make\nuse of larger context lengths for ICL. By running several ablations, we analyze\nthe model's use of: a) the similarity of the in-context examples to the current\ninput, b) the semantic content of the class names, and c) the correct\ncorrespondence between examples and labels. We demonstrate that all three are\nneeded to varying degrees depending on the domain, contrary to certain recent\nworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Milios_A/0/1/0/all/0/1\">Aristides Milios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods. (arXiv:2309.10966v1 [cs.CL])","link":"http://arxiv.org/abs/2309.10966","description":"<p>Recent research in decoding methods for Natural Language Generation (NLG)\ntasks has shown that the traditional beam search and greedy decoding algorithms\nare not optimal, because model probabilities do not always align with human\npreferences. Stronger decoding methods, including Quality Estimation (QE)\nreranking and Minimum Bayes' Risk (MBR) decoding, have since been proposed to\nmitigate the model-perplexity-vs-quality mismatch. While these decoding methods\nachieve state-of-the-art performance, they are prohibitively expensive to\ncompute. In this work, we propose MBR finetuning and QE finetuning which\ndistill the quality gains from these decoding methods at training time, while\nusing an efficient decoding algorithm at inference time. Using the canonical\nNLG task of Neural Machine Translation (NMT), we show that even with\nself-training, these finetuning methods significantly outperform the base\nmodel. Moreover, when using an external LLM as a teacher model, these\nfinetuning methods outperform finetuning on human-generated references. These\nfindings suggest new ways to leverage monolingual data to achieve improvements\nin model quality that are on par with, or even exceed, improvements from\nhuman-curated data, while maintaining maximum efficiency during decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Finkelstein_M/0/1/0/all/0/1\">Mara Finkelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Joint Modeling of Dialogue Response and Speech Synthesis based on Large Language Model. (arXiv:2309.11000v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11000","description":"<p>This paper explores the potential of constructing an AI spoken dialogue\nsystem that \"thinks how to respond\" and \"thinks how to speak\" simultaneously,\nwhich more closely aligns with the human speech production process compared to\nthe current cascade pipeline of independent chatbot and Text-to-Speech (TTS)\nmodules. We hypothesize that Large Language Models (LLMs) with billions of\nparameters possess significant speech understanding capabilities and can\njointly model dialogue responses and linguistic features. We conduct two sets\nof experiments: 1) Prosodic structure prediction, a typical front-end task in\nTTS, demonstrating the speech understanding ability of LLMs, and 2) Further\nintegrating dialogue response and a wide array of linguistic features using a\nunified encoding format. Our results indicate that the LLM-based approach is a\npromising direction for building unified spoken dialogue systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Delong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yudong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Entity Recognition via Machine Reading Comprehension: A Multi-Task Learning Approach. (arXiv:2309.11027v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11027","description":"<p>Named Entity Recognition (NER) aims to extract and classify entity mentions\nin the text into pre-defined types (e.g., organization or person name).\nRecently, many works have been proposed to shape the NER as a machine reading\ncomprehension problem (also termed MRC-based NER), in which entity recognition\nis achieved by answering the formulated questions related to pre-defined entity\ntypes through MRC, based on the contexts. However, these works ignore the label\ndependencies among entity types, which are critical for precisely recognizing\nnamed entities. In this paper, we propose to incorporate the label dependencies\namong entity types into a multi-task learning framework for better MRC-based\nNER. We decompose MRC-based NER into multiple tasks and use a self-attention\nmodule to capture label dependencies. Comprehensive experiments on both nested\nNER and flat NER datasets are conducted to validate the effectiveness of the\nproposed Multi-NER. Experimental results show that Multi-NER can achieve better\nperformance on all datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenting Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhongfen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters. (arXiv:2309.11042v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11042","description":"<p>Recently, Large Language Models (LLMs) have achieved amazing zero-shot\nlearning performance over a variety of Natural Language Processing (NLP) tasks,\nespecially for text generative tasks. Yet, the large size of LLMs often leads\nto the high computational cost of model training and online deployment. In our\nwork, we present ALTER, a system that effectively builds the multi-tAsk\nLearners with mixTure-of-task-adaptERs upon small language models (with &lt;1B\nparameters) to address multiple NLP tasks simultaneously, capturing the\ncommonalities and differences between tasks, in order to support\ndomain-specific applications. Specifically, in ALTER, we propose the\nMixture-of-Task-Adapters (MTA) module as an extension to the transformer\narchitecture for the underlying model to capture the intra-task and inter-task\nknowledge. A two-stage training method is further proposed to optimize the\ncollaboration between adapters at a small computational cost. Experimental\nresults over a mixture of NLP tasks show that our proposed MTA architecture and\nthe two-stage training method achieve good performance. Based on ALTER, we have\nalso produced MTA-equipped language models for various domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yukang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junbing Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiyong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1\">Feiqi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heterogeneous Entity Matching with Complex Attribute Associations using BERT and Neural Networks. (arXiv:2309.11046v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11046","description":"<p>Across various domains, data from different sources such as Baidu Baike and\nWikipedia often manifest in distinct forms. Current entity matching\nmethodologies predominantly focus on homogeneous data, characterized by\nattributes that share the same structure and concise attribute values. However,\nthis orientation poses challenges in handling data with diverse formats.\nMoreover, prevailing approaches aggregate the similarity of attribute values\nbetween corresponding attributes to ascertain entity similarity. Yet, they\noften overlook the intricate interrelationships between attributes, where one\nattribute may have multiple associations. The simplistic approach of pairwise\nattribute comparison fails to harness the wealth of information encapsulated\nwithin entities.To address these challenges, we introduce a novel entity\nmatching model, dubbed Entity Matching Model for Capturing Complex Attribute\nRelationships(EMM-CCAR),built upon pre-trained models. Specifically, this model\ntransforms the matching task into a sequence matching problem to mitigate the\nimpact of varying data formats. Moreover, by introducing attention mechanisms,\nit identifies complex relationships between attributes, emphasizing the degree\nof matching among multiple attributes rather than one-to-one correspondences.\nThrough the integration of the EMM-CCAR model, we adeptly surmount the\nchallenges posed by data heterogeneity and intricate attribute\ninterdependencies. In comparison with the prevalent DER-SSM and Ditto\napproaches, our model achieves improvements of approximately 4% and 1% in F1\nscores, respectively. This furnishes a robust solution for addressing the\nintricacies of attribute complexity in entity matching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shitao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiamin Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables. (arXiv:2309.11049v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11049","description":"<p>Question answering on tabular data (TableQA), which aims at generating\nanswers to questions grounded on a given table, has attracted increasing\nattention in recent years. Existing work tends to generate factual short-form\nanswers by extracting information from one or a few table cells without\nreasoning over selected table cells. However, the free-form TableQA, requiring\na more complex relevant table cell selection strategy and the complex\nintegration and inference of separate pieces of information, has been\nunder-explored. To this end, this paper proposes a generalized three-stage\napproach: Table-to-Graph conversion and cell localizing, external knowledge\nretrieval and table-text fusion (called TAG-QA), addressing the challenge of\ninferring long free-form answer for generative TableQA. In particular, TAG-QA\n(1) locates relevant table cells using a graph neural network to gather\nintersecting cells between relevant rows and columns; (2) leverages external\nknowledge from Wikipedia and (3) generates answers by integrating both tabular\ndata and natural linguistic information. Experiments with a human evaluation\ndemonstrate that TAG-QA is capable of generating more faithful and coherent\nsentence when compared with several state-of-the-art baselines. Especially,\nTAG-QA outperforms the strong pipeline-based baseline TAPAS by 17% and 14%, in\nterms of BLEU-4 and PARENT F-score, respectively. Moreover, TAG-QA outperforms\nend-to-end model T5 by 16% and 12% on BLEU-4 and PARENT F-score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenting Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhongfen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fake News BR: A Fake News Detection Platform for Brazilian Portuguese. (arXiv:2309.11052v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11052","description":"<p>The proliferation of fake news has become a significant concern in recent\ntimes due to its potential to spread misinformation and manipulate public\nopinion. In this paper, we present a comprehensive study on the detection of\nfake news in Brazilian Portuguese, focusing on journalistic-type news. We\npropose a machine learning-based approach that leverages natural language\nprocessing techniques, including TF-IDF and Word2Vec, to extract features from\ntextual data. We evaluate the performance of various classification algorithms,\nsuch as logistic regression, support vector machine, random forest, AdaBoost,\nand LightGBM, on a dataset containing both true and fake news articles. The\nproposed approach achieves a high level of accuracy and F1-Score, demonstrating\nits effectiveness in identifying fake news. Additionally, we develop a\nuser-friendly web platform, FAKENEWSBR.COM, to facilitate the verification of\nnews articles' veracity. Our platform provides real-time analysis, allowing\nusers to assess the likelihood of news articles being fake. Through empirical\nanalysis and comparative studies, we demonstrate the potential of our approach\nto contribute to the fight against the spread of fake news and promote more\ninformed media consumption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giordani_L/0/1/0/all/0/1\">Luiz Giordani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daru_G/0/1/0/all/0/1\">Gilsiley Dar&#xfa;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Queiroz_R/0/1/0/all/0/1\">Rhenan Queiroz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buzinaro_V/0/1/0/all/0/1\">Vitor Buzinaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiva_D/0/1/0/all/0/1\">Davi Keglevich Neiva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_D/0/1/0/all/0/1\">Daniel Camilo Fuentes Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henriques_M/0/1/0/all/0/1\">Marcos Jardel Henriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_O/0/1/0/all/0/1\">Oilson Alberto Gonzatto Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louzada_F/0/1/0/all/0/1\">Francisco Louzada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design of Chain-of-Thought in Math Problem Solving. (arXiv:2309.11054v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11054","description":"<p>Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem\nsolving. We conduct a comprehensive examination of methods for designing CoT,\ncomparing conventional natural language CoT with various program CoTs,\nincluding the self-describing program, the comment-describing program, and the\nnon-describing program. Furthermore, we investigate the impact of programming\nlanguage on program CoTs, comparing Python and Wolfram Language. Through\nextensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs\noften have superior effectiveness in math problem solving. Notably, the best\nperforming combination with 30B parameters beats GPT-3.5-turbo by a significant\nmargin. The results show that self-describing program offers greater diversity\nand thus can generally achieve higher performance. We also find that Python is\na better choice of language than Wolfram for program CoTs. The experimental\nresults provide a valuable guideline for future CoT designs that take into\naccount both programming language and coding style for further advancements.\nOur datasets and code are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zhanming Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_T/0/1/0/all/0/1\">Trung Quoc Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaoran Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates. (arXiv:2309.11063v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11063","description":"<p>Text editing is a crucial task that involves modifying text to better align\nwith user intents. However, existing text editing benchmark datasets have\nlimitations in providing only coarse-grained instructions. Consequently,\nalthough the edited output may seem reasonable, it often deviates from the\nintended changes outlined in the gold reference, resulting in low evaluation\nscores. To comprehensively investigate the text editing capabilities of large\nlanguage models, this paper introduces XATU, the first benchmark specifically\ndesigned for fine-grained instruction-based explainable text editing. XATU\ncovers a wide range of topics and text types, incorporating lexical, syntactic,\nsemantic, and knowledge-intensive edits. To enhance interpretability, we\nleverage high-quality data sources and human annotation, resulting in a\nbenchmark that includes fine-grained instructions and gold-standard edit\nexplanations. By evaluating existing open and closed large language models\nagainst our benchmark, we demonstrate the effectiveness of instruction tuning\nand the impact of underlying architecture across various editing tasks.\nFurthermore, extensive experimentation reveals the significant role of\nexplanations in fine-tuning language models for text editing tasks. The\nbenchmark will be open-sourced to support reproduction and facilitate future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1\">Hayate Iso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurajada_S/0/1/0/all/0/1\">Sairam Gurajada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1\">Nikita Bhutani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt. (arXiv:2309.11065v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11065","description":"<p>Recent research has shown that multi-task pre-training greatly improves the\nmodel's robustness and transfer ability, which is crucial for building a\nhigh-quality dialog system. However, most previous works on multi-task\npre-training rely heavily on human-defined input format or prompt, which is not\noptimal in quality and quantity. In this work, we propose to use Task-based\nAutomatic Prompt generation (TAP) to automatically generate high-quality\nprompts. Using the high-quality prompts generated, we scale the corpus of the\npre-trained conversation model to 122 datasets from 15 dialog-related tasks,\nresulting in Universal Pre-trained Conversation Model (UniPCM), a powerful\nfoundation model for various conversational tasks and different dialog systems.\nExtensive experiments have shown that UniPCM is robust to input prompts and\ncapable of various dialog-related tasks. Moreover, UniPCM has strong transfer\nability and excels at low resource scenarios, achieving SOTA results on 9\ndifferent datasets ranging from task-oriented dialog to open-domain\nconversation. Furthermore, we are amazed to find that TAP can generate prompts\non par with those collected with crowdsourcing. The code is released with the\npaper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yucheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wentao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shuzheng Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yuan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning. (arXiv:2309.11082v1 [cs.CV])","link":"http://arxiv.org/abs/2309.11082","description":"<p>In recent years, the explosion of web videos makes text-video retrieval\nincreasingly essential and popular for video filtering, recommendation, and\nsearch. Text-video retrieval aims to rank relevant text/video higher than\nirrelevant ones. The core of this task is to precisely measure the cross-modal\nsimilarity between texts and videos. Recently, contrastive learning methods\nhave shown promising results for text-video retrieval, most of which focus on\nthe construction of positive and negative pairs to learn text and video\nrepresentations. Nevertheless, they do not pay enough attention to hard\nnegative pairs and lack the ability to model different levels of semantic\nsimilarity. To address these two issues, this paper improves contrastive\nlearning using two novel techniques. First, to exploit hard examples for robust\ndiscriminative power, we propose a novel Dual-Modal Attention-Enhanced Module\n(DMAE) to mine hard negative pairs from textual and visual clues. By further\nintroducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively\nidentify all these hard negatives and explicitly highlight their impacts in the\ntraining loss. Second, our work argues that triplet samples can better model\nfine-grained semantic similarity compared to pairwise samples. We thereby\npresent a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to\nconstruct partial order triplet samples by automatically generating\nfine-grained hard negatives for matched text-video pairs. The proposed TPM-CL\ndesigns an adaptive token masking strategy with cross-modal interaction to\nmodel subtle semantic differences. Extensive experiments demonstrate that the\nproposed approach outperforms existing methods on four widely-used text-video\nretrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xuzheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qingpei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuan Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling. (arXiv:2309.11093v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11093","description":"<p>Lyric translation, a field studied for over a century, is now attracting\ncomputational linguistics researchers. We identified two limitations in\nprevious studies. Firstly, lyric translation studies have predominantly focused\non Western genres and languages, with no previous study centering on K-pop\ndespite its popularity. Second, the field of lyric translation suffers from a\nlack of publicly available datasets; to the best of our knowledge, no such\ndataset exists. To broaden the scope of genres and languages in lyric\ntranslation studies, we introduce a novel singable lyric translation dataset,\napproximately 89\\% of which consists of K-pop song lyrics. This dataset aligns\nKorean and English lyrics line-by-line and section-by-section. We leveraged\nthis dataset to unveil unique characteristics of K-pop lyric translation,\ndistinguishing it from other extensively studied genres, and to construct a\nneural lyric translation model, thereby underscoring the importance of a\ndedicated dataset for singable lyric translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Haven Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jongmin Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_D/0/1/0/all/0/1\">Dasaem Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1\">Juhan Nam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AttentionMix: Data augmentation method that relies on BERT attention mechanism. (arXiv:2309.11104v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11104","description":"<p>The Mixup method has proven to be a powerful data augmentation technique in\nComputer Vision, with many successors that perform image mixing in a guided\nmanner. One of the interesting research directions is transferring the\nunderlying Mixup idea to other domains, e.g. Natural Language Processing (NLP).\nEven though there already exist several methods that apply Mixup to textual\ndata, there is still room for new, improved approaches. In this work, we\nintroduce AttentionMix, a novel mixing method that relies on attention-based\ninformation. While the paper focuses on the BERT attention mechanism, the\nproposed approach can be applied to generally any attention-based model.\nAttentionMix is evaluated on 3 standard sentiment classification datasets and\nin all three cases outperforms two benchmark approaches that utilize Mixup\nmechanism, as well as the vanilla BERT method. The results confirm that the\nattention-based information can be effectively used for data augmentation in\nthe NLP domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lewy_D/0/1/0/all/0/1\">Dominik Lewy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandziuk_J/0/1/0/all/0/1\">Jacek Ma&#x144;dziuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation. (arXiv:2309.11127v1 [eess.SP])","link":"http://arxiv.org/abs/2309.11127","description":"<p>By integrating recent advances in large language models (LLMs) and generative\nmodels into the emerging semantic communication (SC) paradigm, in this article\nwe put forward to a novel framework of language-oriented semantic communication\n(LSC). In LSC, machines communicate using human language messages that can be\ninterpreted and manipulated via natural language processing (NLP) techniques\nfor SC efficiency. To demonstrate LSC's potential, we introduce three\ninnovative algorithms: 1) semantic source coding (SSC) which compresses a text\nprompt into its key head words capturing the prompt's syntactic essence while\nmaintaining their appearance order to keep the prompt's context; 2) semantic\nchannel coding (SCC) that improves robustness against errors by substituting\nhead words with their lenghthier synonyms; and 3) semantic knowledge\ndistillation (SKD) that produces listener-customized prompts via in-context\nlearning the listener's language style. In a communication task for progressive\ntext-to-image generation, the proposed methods achieve higher perceptual\nsimilarities with fewer transmissions while enhancing robustness in noisy\ncommunication channels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nam_H/0/1/0/all/0/1\">Hyelin Nam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_J/0/1/0/all/0/1\">Jihong Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_J/0/1/0/all/0/1\">Jinho Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bennis_M/0/1/0/all/0/1\">Mehdi Bennis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Seong-Lyun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototype of a robotic system to assist the learning process of English language with text-generation through DNN. (arXiv:2309.11142v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11142","description":"<p>In the last ongoing years, there has been a significant ascending on the\nfield of Natural Language Processing (NLP) for performing multiple tasks\nincluding English Language Teaching (ELT). An effective strategy to favor the\nlearning process uses interactive devices to engage learners in their\nself-learning process. In this work, we present a working prototype of a\nhumanoid robotic system to assist English language self-learners through text\ngeneration using Long Short Term Memory (LSTM) Neural Networks. The learners\ninteract with the system using a Graphic User Interface that generates text\naccording to the English level of the user. The experimentation was conducted\nusing English learners and the results were measured accordingly to\nInternational English Language Testing System (IELTS) rubric. Preliminary\nresults show an increment in the Grammatical Range of learners who interacted\nwith the system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morales_Torres_C/0/1/0/all/0/1\">Carlos Morales-Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_Soberanis_M/0/1/0/all/0/1\">Mario Campos-Soberanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_Sobrino_D/0/1/0/all/0/1\">Diego Campos-Sobrino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought. (arXiv:2309.11143v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11143","description":"<p>Unsupervised sentence representation learning aims to transform input\nsentences into fixed-length vectors enriched with intricate semantic\ninformation while obviating the reliance on labeled data. Recent progress\nwithin this field, propelled by contrastive learning and prompt engineering,\nhas significantly bridged the gap between unsupervised and supervised\nstrategies. Nonetheless, the potential utilization of Chain-of-Thought, remains\nlargely untapped within this trajectory. To unlock latent capabilities within\npre-trained models, such as BERT, we propose a two-stage approach for sentence\nrepresentation: comprehension and summarization. Subsequently, the output of\nthe latter phase is harnessed as the vectorized representation of the input\nsentence. For further performance enhancement, we meticulously refine both the\ncontrastive learning loss function and the template denoising technique for\nprompt engineering. Rigorous experimentation substantiates our method,\nCoT-BERT, transcending a suite of robust baselines without necessitating other\ntext representation models or external databases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kehua Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunping Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessment of Pre-Trained Models Across Languages and Grammars. (arXiv:2309.11165v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11165","description":"<p>We present an approach for assessing how multilingual large language models\n(LLMs) learn syntax in terms of multi-formalism syntactic structures. We aim to\nrecover constituent and dependency structures by casting parsing as sequence\nlabeling. To do so, we select a few LLMs and study them on 13 diverse UD\ntreebanks for dependency parsing and 10 treebanks for constituent parsing. Our\nresults show that: (i) the framework is consistent across encodings, (ii)\npre-trained word vectors do not favor constituency representations of syntax\nover dependencies, (iii) sub-word tokenization is needed to represent syntax,\nin contrast to character-based models, and (iv) occurrence of a language in the\npretraining data is more important than the amount of task data when recovering\nsyntax from the word vectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munoz_Ortiz_A/0/1/0/all/0/1\">Alberto Mu&#xf1;oz-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilares_D/0/1/0/all/0/1\">David Vilares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Large Language Models Really Robust to Word-Level Perturbations?. (arXiv:2309.11166v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11166","description":"<p>The swift advancement in the scale and capabilities of Large Language Models\n(LLMs) positions them as promising tools for a variety of downstream tasks. In\naddition to the pursuit of better performance and the avoidance of violent\nfeedback on a certain prompt, to ensure the responsibility of the LLM, much\nattention is drawn to the robustness of LLMs. However, existing evaluation\nmethods mostly rely on traditional question answering datasets with predefined\nsupervised labels, which do not align with the superior generation capabilities\nof contemporary LLMs. To address this issue, we propose a novel rational\nevaluation approach that leverages pre-trained reward models as diagnostic\ntools to evaluate the robustness of LLMs, which we refer to as the Reward Model\nfor Reasonable Robustness Evaluation (TREvaL). Our extensive empirical\nexperiments have demonstrated that TREval provides an accurate method for\nevaluating the robustness of an LLM, especially when faced with more\nchallenging open questions. Furthermore, our results demonstrate that LLMs\nfrequently exhibit vulnerability to word-level perturbations, which are\ncommonplace in daily language usage. Notably, we were surprised to discover\nthat robustness tends to decrease as fine-tuning (SFT and RLHF) is conducted.\nThe code of TREval is available in https://github.com/Harry-mic/TREval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guozheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_N/0/1/0/all/0/1\">Ning Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Suwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yongzhe Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute. (arXiv:2309.11197v1 [cs.LG])","link":"http://arxiv.org/abs/2309.11197","description":"<p>The Languini Kitchen serves as both a research collective and codebase\ndesigned to empower researchers with limited computational resources to\ncontribute meaningfully to the field of language modelling. We introduce an\nexperimental protocol that enables model comparisons based on equivalent\ncompute, measured in accelerator hours. The number of tokens on which a model\nis trained is defined by the model's throughput and the chosen compute class.\nNotably, this approach avoids constraints on critical hyperparameters which\naffect total parameters or floating-point operations. For evaluation, we\npre-process an existing large, diverse, and high-quality dataset of books that\nsurpasses existing academic benchmarks in quality, diversity, and document\nlength. On it, we compare methods based on their empirical scaling trends which\nare estimated through experiments at various levels of compute. This work also\nprovides two baseline models: a feed-forward model derived from the GPT-2\narchitecture and a recurrent model in the form of a novel LSTM with ten-fold\nthroughput. While the GPT baseline achieves better perplexity throughout all\nour levels of compute, our LSTM baseline exhibits a predictable and more\nfavourable scaling law. This is due to the improved throughput and the need for\nfewer training tokens to achieve the same decrease in test perplexity.\nExtrapolating the scaling laws leads of both models results in an intersection\nat roughly 50,000 accelerator hours. We hope this work can serve as the\nfoundation for meaningful and reproducible language modelling research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stanic_A/0/1/0/all/0/1\">Aleksandar Stani&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashley_D/0/1/0/all/0/1\">Dylan Ashley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serikov_O/0/1/0/all/0/1\">Oleg Serikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirsch_L/0/1/0/all/0/1\">Louis Kirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faccio_F/0/1/0/all/0/1\">Francesco Faccio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlag_I/0/1/0/all/0/1\">Imanol Schlag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering. (arXiv:2309.11206v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11206","description":"<p>Despite their competitive performance on knowledge-intensive tasks, large\nlanguage models (LLMs) still have limitations in memorizing all world knowledge\nespecially long tail knowledge. In this paper, we study the KG-augmented\nlanguage model approach for solving the knowledge graph question answering\n(KGQA) task that requires rich world knowledge. Existing work has shown that\nretrieving KG knowledge to enhance LLMs prompting can significantly improve\nLLMs performance in KGQA. However, their approaches lack a well-formed\nverbalization of KG knowledge, i.e., they ignore the gap between KG\nrepresentations and textual representations. To this end, we propose an\nanswer-sensitive KG-to-Text approach that can transform KG knowledge into\nwell-textualized statements most informative for KGQA. Based on this approach,\nwe propose a KG-to-Text enhanced LLMs framework for solving the KGQA task.\nExperiments on several KGQA benchmarks show that the proposed KG-to-Text\naugmented LLMs approach outperforms previous KG-augmented LLMs approaches\nregarding answer accuracy and usefulness of knowledge statements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yike Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1\">Nan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sheng Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_A/0/1/0/all/0/1\">Anhuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speak While You Think: Streaming Speech Synthesis During Text Generation. (arXiv:2309.11210v1 [eess.AS])","link":"http://arxiv.org/abs/2309.11210","description":"<p>Large Language Models (LLMs) demonstrate impressive capabilities, yet\ninteraction with these models is mostly facilitated through text. Using\nText-To-Speech to synthesize LLM outputs typically results in notable latency,\nwhich is impractical for fluent voice conversations. We propose LLM2Speech, an\narchitecture to synthesize speech while text is being generated by an LLM which\nyields significant latency reduction. LLM2Speech mimics the predictions of a\nnon-streaming teacher model while limiting the exposure to future context in\norder to enable streaming. It exploits the hidden embeddings of the LLM, a\nby-product of the text generation that contains informative semantic context.\nExperimental results show that LLM2Speech maintains the teacher's quality while\nreducing the latency to enable natural conversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dekel_A/0/1/0/all/0/1\">Avihu Dekel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shechtman_S/0/1/0/all/0/1\">Slava Shechtman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fernandez_R/0/1/0/all/0/1\">Raul Fernandez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haws_D/0/1/0/all/0/1\">David Haws</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kons_Z/0/1/0/all/0/1\">Zvi Kons</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoory_R/0/1/0/all/0/1\">Ron Hoory</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenChat: Advancing Open-source Language Models with Mixed-Quality Data. (arXiv:2309.11235v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11235","description":"<p>Nowadays, open-source large language models like LLaMA have emerged. Recent\ndevelopments have incorporated supervised fine-tuning (SFT) and reinforcement\nlearning fine-tuning (RLFT) to align these models with human goals. However,\nSFT methods treat all training data with mixed quality equally, while RLFT\nmethods require high-quality pairwise or ranking-based preference data. In this\nstudy, we present a novel framework, named OpenChat, to advance open-source\nlanguage models with mixed-quality data. Specifically, we consider the general\nSFT training data, consisting of a small amount of expert data mixed with a\nlarge proportion of sub-optimal data, without any preference labels. We propose\nthe C(onditioned)-RLFT, which regards different data sources as coarse-grained\nreward labels and learns a class-conditioned policy to leverage complementary\ndata quality information. Interestingly, the optimal policy in C-RLFT can be\neasily solved through single-stage, RL-free supervised learning, which is\nlightweight and avoids costly human preference labeling. Through extensive\nexperiments on three standard benchmarks, our openchat-13b fine-tuned with\nC-RLFT achieves the highest average performance among all 13b open-source\nlanguage models. Moreover, we use AGIEval to validate the model generalization\nperformance, in which only openchat-13b surpasses the base model. Finally, we\nconduct a series of analyses to shed light on the effectiveness and robustness\nof OpenChat. Our code, data, and models are publicly available at\nhttps://github.com/imoneoi/openchat.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Sijie Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xianyuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Sen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Scenario Refiner: Grounding subjects in images at the morphological level. (arXiv:2309.11252v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11252","description":"<p>Derivationally related words, such as \"runner\" and \"running\", exhibit\nsemantic differences which also elicit different visual scenarios. In this\npaper, we ask whether Vision and Language (V\\&amp;L) models capture such\ndistinctions at the morphological level, using a a new methodology and dataset.\nWe compare the results from V\\&amp;L models to human judgements and find that\nmodels' predictions differ from those of human participants, in particular\ndisplaying a grammatical bias. We further investigate whether the human-model\nmisalignment is related to model architecture. Our methodology, developed on\none specific morphological contrast, can be further extended for testing models\non capturing other nuanced language features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tagliaferri_C/0/1/0/all/0/1\">Claudia Tagliaferri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axioti_S/0/1/0/all/0/1\">Sofia Axioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paperno_D/0/1/0/all/0/1\">Denis Paperno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Spanish Pre-trained Language Models. (arXiv:2309.11259v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11259","description":"<p>In recent years, substantial advancements in pre-trained language models have\npaved the way for the development of numerous non-English language versions,\nwith a particular focus on encoder-only and decoder-only architectures. While\nSpanish language models encompassing BERT, RoBERTa, and GPT have exhibited\nprowess in natural language understanding and generation, there remains a\nscarcity of encoder-decoder models designed for sequence-to-sequence tasks\ninvolving input-output pairs. This paper breaks new ground by introducing the\nimplementation and evaluation of renowned encoder-decoder architectures,\nexclusively pre-trained on Spanish corpora. Specifically, we present Spanish\nversions of BART, T5, and BERT2BERT-style models and subject them to a\ncomprehensive assessment across a diverse range of sequence-to-sequence tasks,\nspanning summarization, rephrasing, and generative question answering. Our\nfindings underscore the competitive performance of all models, with BART and T5\nemerging as top performers across all evaluated tasks. As an additional\ncontribution, we have made all models publicly available to the research\ncommunity, fostering future exploration and development in Spanish language\nprocessing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Araujo_V/0/1/0/all/0/1\">Vladimir Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trusca_M/0/1/0/all/0/1\">Maria Mihaela Trusca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufino_R/0/1/0/all/0/1\">Rodrigo Tufi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounded Complex Task Segmentation for Conversational Assistants. (arXiv:2309.11271v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11271","description":"<p>Following complex instructions in conversational assistants can be quite\ndaunting due to the shorter attention and memory spans when compared to reading\nthe same instructions. Hence, when conversational assistants walk users through\nthe steps of complex tasks, there is a need to structure the task into\nmanageable pieces of information of the right length and complexity. In this\npaper, we tackle the recipes domain and convert reading structured instructions\ninto conversational structured ones. We annotated the structure of instructions\naccording to a conversational scenario, which provided insights into what is\nexpected in this setting. To computationally model the conversational step's\ncharacteristics, we tested various Transformer-based architectures, showing\nthat a token-based approach delivers the best results. A further user study\nshowed that users tend to favor steps of manageable complexity and length, and\nthat the proposed methodology can improve the original web-based instructional\ntext. Specifically, 86% of the evaluated tasks were improved from a\nconversational suitability point of view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_R/0/1/0/all/0/1\">Rafael Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semedo_D/0/1/0/all/0/1\">David Semedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magalhaes_J/0/1/0/all/0/1\">Jo&#xe3;o Magalh&#xe3;es</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Wizard of Curiosities: Enriching Dialogues with Fun Facts. (arXiv:2309.11283v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11283","description":"<p>Introducing curiosities in a conversation is a way to teach something new to\nthe person in a pleasant and enjoyable way. Enriching dialogues with\ncontextualized curiosities can improve the users' perception of a dialog system\nand their overall user experience. In this paper, we introduce a set of curated\ncuriosities, targeting dialogues in the cooking and DIY domains. In particular,\nwe use real human-agent conversations collected in the context of the Amazon\nAlexa TaskBot challenge, a multimodal and multi-turn conversational setting.\nAccording to an A/B test with over 1000 conversations, curiosities not only\nincrease user engagement, but provide an average relative rating improvement of\n9.7%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vicente_F/0/1/0/all/0/1\">Frederico Vicente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_R/0/1/0/all/0/1\">Rafael Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semedo_D/0/1/0/all/0/1\">David Semedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magalhaes_J/0/1/0/all/0/1\">Jo&#xe3;o Magalh&#xe3;es</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains. (arXiv:2309.11285v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11285","description":"<p>This paper presents the overview of the AuTexTification shared task as part\nof the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within the\nframework of the SEPLN 2023 conference. AuTexTification consists of two\nsubtasks: for Subtask 1, participants had to determine whether a text is\nhuman-authored or has been generated by a large language model. For Subtask 2,\nparticipants had to attribute a machine-generated text to one of six different\ntext generation models. Our AuTexTification 2023 dataset contains more than\n160.000 texts across two languages (English and Spanish) and five domains\n(tweets, reviews, news, legal, and how-to articles). A total of 114 teams\nsigned up to participate, of which 36 sent 175 runs, and 20 of them sent their\nworking notes. In this overview, we present the AuTexTification dataset and\ntask, the submitted participating systems, and the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarvazyan_A/0/1/0/all/0/1\">Areg Mikael Sarvazyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Jos&#xe9; &#xc1;ngel Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franco_Salvador_M/0/1/0/all/0/1\">Marc Franco-Salvador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangel_F/0/1/0/all/0/1\">Francisco Rangel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chulvi_B/0/1/0/all/0/1\">Berta Chulvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPLLM: Clinical Prediction with Large Language Models. (arXiv:2309.11295v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11295","description":"<p>We present Clinical Prediction with Large Language Models (CPLLM), a method\nthat involves fine-tuning a pre-trained Large Language Model (LLM) for clinical\ndisease prediction. We utilized quantization and fine-tuned the LLM using\nprompts, with the task of predicting whether patients will be diagnosed with a\ntarget disease during their next visit or in the subsequent diagnosis,\nleveraging their historical diagnosis records. We compared our results versus\nvarious baselines, including Logistic Regression, RETAIN, and Med-BERT, which\nis the current state-of-the-art model for disease prediction using structured\nEHR data. Our experiments have shown that CPLLM surpasses all the tested models\nin terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements\ncompared to the baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shoham_O/0/1/0/all/0/1\">Ofir Ben Shoham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rappoport_N/0/1/0/all/0/1\">Nadav Rappoport</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features. (arXiv:2309.11307v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11307","description":"<p>Predicting the success of Conversational Task Assistants (CTA) can be\ncritical to understand user behavior and act accordingly. In this paper, we\npropose TB-Rater, a Transformer model which combines conversational-flow\nfeatures with user behavior features for predicting user ratings in a CTA\nscenario. In particular, we use real human-agent conversations and ratings\ncollected in the Alexa TaskBot challenge, a novel multimodal and multi-turn\nconversational context. Our results show the advantages of modeling both the\nconversational-flow and behavioral aspects of the conversation in a single\nmodel for offline rating prediction. Additionally, an analysis of the\nCTA-specific behavioral features brings insights into this setting and can be\nused to bootstrap future systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_R/0/1/0/all/0/1\">Rafael Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semedo_D/0/1/0/all/0/1\">David Semedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magalhaes_J/0/1/0/all/0/1\">Jo&#xe3;o Magalh&#xe3;es</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services. (arXiv:2309.11325v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11325","description":"<p>We propose DISC-LawLLM, an intelligent legal system utilizing large language\nmodels (LLMs) to provide a wide range of legal services. We adopt legal\nsyllogism prompting strategies to construct supervised fine-tuning datasets in\nthe Chinese Judicial domain and fine-tune LLMs with legal reasoning capability.\nWe augment LLMs with a retrieval module to enhance models' ability to access\nand utilize external legal knowledge. A comprehensive legal benchmark,\nDISC-Law-Eval, is presented to evaluate intelligent legal systems from both\nobjective and subjective dimensions. Quantitative and qualitative results on\nDISC-Law-Eval demonstrate the effectiveness of our system in serving various\nusers across diverse legal scenarios. The detailed resources are available at\nhttps://github.com/FudanDISC/DISC-LawLLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Shengbin Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenchen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Song Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition. (arXiv:2309.11327v1 [eess.AS])","link":"http://arxiv.org/abs/2309.11327","description":"<p>Crafting an effective Automatic Speech Recognition (ASR) solution for\ndialects demands innovative approaches that not only address the data scarcity\nissue but also navigate the intricacies of linguistic diversity. In this paper,\nwe address the aforementioned ASR challenge, focusing on the Tunisian dialect.\nFirst, textual and audio data is collected and in some cases annotated. Second,\nwe explore self-supervision, semi-supervision and few-shot code-switching\napproaches to push the state-of-the-art on different Tunisian test sets;\ncovering different acoustic, linguistic and prosodic conditions. Finally, and\ngiven the absence of conventional spelling, we produce a human evaluation of\nour transcripts to avoid the noise coming from spelling inadequacies in our\ntesting references. Our models, allowing to transcribe audio samples in a\nlinguistic mix involving Tunisian Arabic, English and French, and all the data\nused during training and testing are released for public use and further\nimprovements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Abdallah_A/0/1/0/all/0/1\">Ahmed Amine Ben Abdallah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kabboudi_A/0/1/0/all/0/1\">Ata Kabboudi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kanoun_A/0/1/0/all/0/1\">Amir Kanoun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zaiem_S/0/1/0/all/0/1\">Salah Zaiem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRAVID: An End-to-End Video Translation Framework. (arXiv:2309.11338v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11338","description":"<p>In today's globalized world, effective communication with people from diverse\nlinguistic backgrounds has become increasingly crucial. While traditional\nmethods of language translation, such as written text or voice-only\ntranslations, can accomplish the task, they often fail to capture the complete\ncontext and nuanced information conveyed through nonverbal cues like facial\nexpressions and lip movements. In this paper, we present an end-to-end video\ntranslation system that not only translates spoken language but also\nsynchronizes the translated speech with the lip movements of the speaker. Our\nsystem focuses on translating educational lectures in various Indian languages,\nand it is designed to be effective even in low-resource system settings. By\nincorporating lip movements that align with the target language and matching\nthem with the speaker's voice using voice cloning techniques, our application\noffers an enhanced experience for students and users. This additional feature\ncreates a more immersive and realistic learning environment, ultimately making\nthe learning process more effective and engaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adhikary_P/0/1/0/all/0/1\">Prottay Kumar Adhikary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugandhi_B/0/1/0/all/0/1\">Bandaru Sugandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghimire_S/0/1/0/all/0/1\">Subhojit Ghimire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Santanu Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakray_P/0/1/0/all/0/1\">Partha Pakray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Article Classification with Edge-Heterogeneous Graph Neural Networks. (arXiv:2309.11341v1 [cs.LG])","link":"http://arxiv.org/abs/2309.11341","description":"<p>Classifying research output into context-specific label taxonomies is a\nchallenging and relevant downstream task, given the volume of existing and\nnewly published articles. We propose a method to enhance the performance of\narticle classification by enriching simple Graph Neural Networks (GNN)\npipelines with edge-heterogeneous graph representations. SciBERT is used for\nnode feature generation to capture higher-order semantics within the articles'\ntextual metadata. Fully supervised transductive node classification experiments\nare conducted on the Open Graph Benchmark (OGB) ogbn-arxiv dataset and the\nPubMed diabetes dataset, augmented with additional metadata from Microsoft\nAcademic Graph (MAG) and PubMed Central, respectively. The results demonstrate\nthat edge-heterogeneous graphs consistently improve the performance of all GNN\nmodels compared to the edge-homogeneous graphs. The transformed data enable\nsimple and shallow GNN pipelines to achieve results on par with more complex\narchitectures. On ogbn-arxiv, we achieve a top-15 result in the OGB competition\nwith a 2-layer GCN (accuracy 74.61%), being the highest-scoring solution with\nsub-1 million parameters. On PubMed, we closely trail SOTA GNN architectures\nusing a 2-layer GraphSAGE by including additional co-authorship edges in the\ngraph (accuracy 89.88%). The implementation is available at:\n$\\href{https://github.com/lyvykhang/edgehetero-nodeproppred}{\\text{https://github.com/lyvykhang/edgehetero-nodeproppred}}$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ly_K/0/1/0/all/0/1\">Khang Ly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashnitsky_Y/0/1/0/all/0/1\">Yury Kashnitsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chamezopoulos_S/0/1/0/all/0/1\">Savvas Chamezopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krzhizhanovskaya_V/0/1/0/all/0/1\">Valeria Krzhizhanovskaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GECTurk: Grammatical Error Correction and Detection Dataset for Turkish. (arXiv:2309.11346v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11346","description":"<p>Grammatical Error Detection and Correction (GEC) tools have proven useful for\nnative speakers and second language learners. Developing such tools requires a\nlarge amount of parallel, annotated data, which is unavailable for most\nlanguages. Synthetic data generation is a common practice to overcome the\nscarcity of such data. However, it is not straightforward for morphologically\nrich languages like Turkish due to complex writing rules that require\nphonological, morphological, and syntactic information. In this work, we\npresent a flexible and extensible synthetic data generation pipeline for\nTurkish covering more than 20 expert-curated grammar and spelling rules\n(a.k.a., writing rules) implemented through complex transformation functions.\nUsing this pipeline, we derive 130,000 high-quality parallel sentences from\nprofessionally edited articles. Additionally, we create a more realistic test\nset by manually annotating a set of movie reviews. We implement three baselines\nformulating the task as i) neural machine translation, ii) sequence tagging,\nand iii) prefix tuning with a pretrained decoder-only model, achieving strong\nresults. Furthermore, we perform exhaustive experiments on out-of-domain\ndatasets to gain insights on the transferability and robustness of the proposed\napproaches. Our results suggest that our corpus, GECTurk, is high-quality and\nallows knowledge transfer for the out-of-domain setting. To encourage further\nresearch on Turkish GEC, we release our datasets, baseline models, and the\nsynthetic data generation pipeline at https://github.com/GGLAB-KU/gecturk.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kara_A/0/1/0/all/0/1\">Atakan Kara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sofian_F/0/1/0/all/0/1\">Farrin Marouf Sofian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bond_A/0/1/0/all/0/1\">Andrew Bond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_G/0/1/0/all/0/1\">G&#xf6;zde G&#xfc;l &#x15e;ahin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff. (arXiv:2309.11379v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11379","description":"<p>Blockwise self-attentional encoder models have recently emerged as one\npromising end-to-end approach to simultaneous speech translation. These models\nemploy a blockwise beam search with hypothesis reliability scoring to determine\nwhen to wait for more input speech before translating further. However, this\nmethod maintains multiple hypotheses until the entire speech input is consumed\n-- this scheme cannot directly show a single \\textit{incremental} translation\nto users. Further, this method lacks mechanisms for \\textit{controlling} the\nquality vs. latency tradeoff. We propose a modified incremental blockwise beam\nsearch incorporating local agreement or hold-$n$ policies for quality-latency\ncontrol. We apply our framework to models trained for online or offline\ntranslation and demonstrate that both types can be effectively used in online\nmode.\n</p>\n<p>Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing\nlatency or 0.8-1.4 s latency improvement without changing quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polak_P/0/1/0/all/0/1\">Peter Pol&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Studying Lobby Influence in the European Parliament. (arXiv:2309.11381v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11381","description":"<p>We present a method based on natural language processing (NLP), for studying\nthe influence of interest groups (lobbies) in the law-making process in the\nEuropean Parliament (EP). We collect and analyze novel datasets of lobbies'\nposition papers and speeches made by members of the EP (MEPs). By comparing\nthese texts on the basis of semantic similarity and entailment, we are able to\ndiscover interpretable links between MEPs and lobbies. In the absence of a\nground-truth dataset of such links, we perform an indirect validation by\ncomparing the discovered links with a dataset, which we curate, of retweet\nlinks between MEPs and lobbies, and with the publicly disclosed meetings of\nMEPs. Our best method achieves an AUC score of 0.77 and performs significantly\nbetter than several baselines. Moreover, an aggregate analysis of the\ndiscovered links, between groups of related lobbies and political groups of\nMEPs, correspond to the expectations from the ideology of the groups (e.g.,\ncenter-left groups are associated with social causes). We believe that this\nwork, which encompasses the methodology, datasets, and results, is a step\ntowards enhancing the transparency of the intricate decision-making processes\nwithin democratic institutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1\">Aswin Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radojevic_L/0/1/0/all/0/1\">Lazar Radojevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvi_F/0/1/0/all/0/1\">Francesco Salvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magron_A/0/1/0/all/0/1\">Antoine Magron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kristof_V/0/1/0/all/0/1\">Victor Kristof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grossglauser_M/0/1/0/all/0/1\">Matthias Grossglauser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions. (arXiv:2309.11382v1 [cs.RO])","link":"http://arxiv.org/abs/2309.11382","description":"<p>Visual language navigation (VLN) is an embodied task demanding a wide range\nof skills encompassing understanding, perception, and planning. For such a\nmultifaceted challenge, previous VLN methods totally rely on one model's own\nthinking to make predictions within one round. However, existing models, even\nthe most advanced large language model GPT4, still struggle with dealing with\nmultiple tasks by single-round self-thinking. In this work, drawing inspiration\nfrom the expert consultation meeting, we introduce a novel zero-shot VLN\nframework. Within this framework, large models possessing distinct abilities\nare served as domain experts. Our proposed navigation agent, namely DiscussNav,\ncan actively discuss with these experts to collect essential information before\nmoving at every step. These discussions cover critical navigation subtasks like\ninstruction understanding, environment perception, and completion estimation.\nThrough comprehensive experiments, we demonstrate that discussions with domain\nexperts can effectively facilitate navigation by perceiving\ninstruction-relevant information, correcting inadvertent errors, and sifting\nthrough in-consistent movement decisions. The performances on the\nrepresentative VLN task R2R show that our method surpasses the leading\nzero-shot VLN model by a large margin on all metrics. Additionally, real-robot\nexperiments display the obvious advantages of our method over single-round\nself-thinking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yuxing Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Wenzhe Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-Form End-to-End Speech Translation via Latent Alignment Segmentation. (arXiv:2309.11384v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11384","description":"<p>Current simultaneous speech translation models can process audio only up to a\nfew seconds long. Contemporary datasets provide an oracle segmentation into\nsentences based on human-annotated transcripts and translations. However, the\nsegmentation into sentences is not available in the real world. Current speech\nsegmentation approaches either offer poor segmentation quality or have to trade\nlatency for quality. In this paper, we propose a novel segmentation approach\nfor a low-latency end-to-end speech translation. We leverage the existing\nspeech translation encoder-decoder architecture with ST CTC and show that it\ncan perform the segmentation task without supervision or additional parameters.\nTo the best of our knowledge, our method is the first that allows an actual\nend-to-end simultaneous speech translation, as the same model is used for\ntranslation and segmentation at the same time. On a diverse set of language\npairs and in- and out-of-domain data, we show that the proposed approach\nachieves state-of-the-art quality at no additional computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polak_P/0/1/0/all/0/1\">Peter Pol&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safurai 001: New Qualitative Approach for Code LLM Evaluation. (arXiv:2309.11385v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11385","description":"<p>This paper presents Safurai-001, a new Large Language Model (LLM) with\nsignificant potential in the domain of coding assistance. Driven by recent\nadvancements in coding LLMs, Safurai-001 competes in performance with the\nlatest models like WizardCoder [Xu et al., 2023], PanguCoder [Shen et al.,\n2023] and Phi-1 [Gunasekar et al., 2023] but aims to deliver a more\nconversational interaction. By capitalizing on the progress in data engineering\n(including latest techniques of data transformation and prompt engineering) and\ninstruction tuning, this new model promises to stand toe-to-toe with recent\nclosed and open source developments. Recognizing the need for an efficacious\nevaluation metric for coding LLMs, this paper also introduces GPT4-based\nMultiParameters, an evaluation benchmark that harnesses varied parameters to\npresent a comprehensive insight into the models functioning and performance.\nOur assessment shows that Safurai-001 can outperform GPT-3.5 by 1.58% and\nWizardCoder by 18.78% in the Code Readability parameter and more.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cifarelli_D/0/1/0/all/0/1\">Davide Cifarelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boiardi_L/0/1/0/all/0/1\">Leonardo Boiardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puppo_A/0/1/0/all/0/1\">Alessandro Puppo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kosmos-2.5: A Multimodal Literate Model. (arXiv:2309.11419v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11419","description":"<p>We present Kosmos-2.5, a multimodal literate model for machine reading of\ntext-intensive images. Pre-trained on large-scale text-intensive images,\nKosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1)\ngenerating spatially-aware text blocks, where each block of text is assigned\nits spatial coordinates within the image, and (2) producing structured text\noutput that captures styles and structures into the markdown format. This\nunified multimodal literate capability is achieved through a shared Transformer\narchitecture, task-specific prompts, and flexible text representations. We\nevaluate Kosmos-2.5 on end-to-end document-level text recognition and\nimage-to-markdown text generation. Furthermore, the model can be readily\nadapted for any text-intensive image understanding task with different prompts\nthrough supervised fine-tuning, making it a general-purpose tool for real-world\napplications involving text-rich images. This work also paves the way for the\nfuture scaling of multimodal large language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yupan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingye Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yaoyao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weiyao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shaoxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Look at Screens: Multimodal Chain-of-Action Agents. (arXiv:2309.11436v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11436","description":"<p>Autonomous user interface (UI) agents aim to facilitate task automation by\ninteracting with the user interface without manual intervention. Recent studies\nhave investigated eliciting the capabilities of large language models (LLMs)\nfor effective engagement in diverse environments. To align with the\ninput-output requirement of LLMs, existing approaches are developed under a\nsandbox setting where they rely on external tools and application-specific APIs\nto parse the environment into textual elements and interpret the predicted\nactions. Consequently, those approaches often grapple with inference\ninefficiency and error propagation risks. To mitigate the challenges, we\nintroduce Auto-UI, a multimodal solution that directly interacts with the\ninterface, bypassing the need for environment parsing or reliance on\napplication-dependent APIs. Moreover, we propose a chain-of-action technique --\nleveraging a series of intermediate previous action histories and future action\nplans -- to help the agent decide what action to execute. We evaluate our\napproach on a new device-control benchmark AITW with 30K unique instructions,\nspanning multi-step tasks such as application operation, web searching, and web\nshopping. Experimental results show that Auto-UI achieves state-of-the-art\nperformance with an action type prediction accuracy of 90% and an overall\naction success rate of 74%. Code is publicly available at\nhttps://github.com/cooelf/Auto-UI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1\">Zhuosheng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlled Generation with Prompt Insertion for Natural Language Explanations in Grammatical Error Correction. (arXiv:2309.11439v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11439","description":"<p>In Grammatical Error Correction (GEC), it is crucial to ensure the user's\ncomprehension of a reason for correction. Existing studies present tokens,\nexamples, and hints as to the basis for correction but do not directly explain\nthe reasons for corrections. Although methods that use Large Language Models\n(LLMs) to provide direct explanations in natural language have been proposed\nfor various tasks, no such method exists for GEC. Generating explanations for\nGEC corrections involves aligning input and output tokens, identifying\ncorrection points, and presenting corresponding explanations consistently.\nHowever, it is not straightforward to specify a complex format to generate\nexplanations, because explicit control of generation is difficult with prompts.\nThis study introduces a method called controlled generation with Prompt\nInsertion (PI) so that LLMs can explain the reasons for corrections in natural\nlanguage. In PI, LLMs first correct the input text, and then we automatically\nextract the correction points based on the rules. The extracted correction\npoints are sequentially inserted into the LLM's explanation output as prompts,\nguiding the LLMs to generate explanations for the correction points. We also\ncreate an Explainable GEC (XGEC) dataset of correction reasons by annotating\nNUCLE, CoNLL2013, and CoNLL2014. Although generations from GPT-3 and ChatGPT\nusing original prompts miss some correction points, the generation control\nusing PI can explicitly guide to describe explanations for all correction\npoints, contributing to improved performance in generating correction reasons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_M/0/1/0/all/0/1\">Masahiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])","link":"http://arxiv.org/abs/2309.11489","description":"<p>Designing reward functions is a longstanding challenge in reinforcement\nlearning (RL); it requires specialized knowledge or domain data, leading to\nhigh costs for development. To address this, we introduce Text2Reward, a\ndata-free framework that automates the generation of dense reward functions\nbased on large language models (LLMs). Given a goal described in natural\nlanguage, Text2Reward generates dense reward functions as an executable program\ngrounded in a compact representation of the environment. Unlike inverse RL and\nrecent work that uses LLMs to write sparse reward codes, Text2Reward produces\ninterpretable, free-form dense reward codes that cover a wide range of tasks,\nutilize existing packages, and allow iterative refinement with human feedback.\nWe evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2,\nMetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17\nmanipulation tasks, policies trained with generated reward codes achieve\nsimilar or better task success rates and convergence speed than expert-written\nreward codes. For locomotion tasks, our method learns six novel locomotion\nbehaviors with a success rate exceeding 94%. Furthermore, we show that the\npolicies trained in the simulator with our method can be deployed in the real\nworld. Finally, Text2Reward further improves the policies by refining their\nreward functions with human feedback. Video results are available at\nhttps://text-to-reward.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianbao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Siheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Henry Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yitao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1\">Qian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1\">Victor Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Verification Reduces Hallucination in Large Language Models. (arXiv:2309.11495v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11495","description":"<p>Generation of plausible yet incorrect factual information, termed\nhallucination, is an unsolved issue in large language models. We study the\nability of language models to deliberate on the responses they give in order to\ncorrect their mistakes. We develop the Chain-of-Verification (CoVe) method\nwhereby the model first (i) drafts an initial response; then (ii) plans\nverification questions to fact-check its draft; (iii) answers those questions\nindependently so the answers are not biased by other responses; and (iv)\ngenerates its final verified response. In experiments, we show CoVe decreases\nhallucinations across a variety of tasks, from list-based questions from\nWikidata, closed book MultiSpanQA and longform text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Mojtaba Komeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1\">Roberta Raileanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DreamLLM: Synergistic Multimodal Comprehension and Creation. (arXiv:2309.11499v1 [cs.CV])","link":"http://arxiv.org/abs/2309.11499","description":"<p>This paper presents DreamLLM, a learning framework that first achieves\nversatile Multimodal Large Language Models (MLLMs) empowered with frequently\noverlooked synergy between multimodal comprehension and creation. DreamLLM\noperates on two fundamental principles. The first focuses on the generative\nmodeling of both language and image posteriors by direct sampling in the raw\nmultimodal space. This approach circumvents the limitations and information\nloss inherent to external feature extractors like CLIP, and a more thorough\nmultimodal understanding is obtained. Second, DreamLLM fosters the generation\nof raw, interleaved documents, modeling both text and image contents, along\nwith unstructured layouts. This allows DreamLLM to learn all conditional,\nmarginal, and joint multimodal distributions effectively. As a result, DreamLLM\nis the first MLLM capable of generating free-form interleaved content.\nComprehensive experiments highlight DreamLLM's superior performance as a\nzero-shot multimodal generalist, reaping from the enhanced learning synergy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Runpei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chunrui Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zekun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinrong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jianjian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Haoran Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiangwen Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Procedures as Programs: Hierarchical Control of Situated Agents through Natural Language. (arXiv:2109.08214v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08214","description":"<p>When humans conceive how to perform a particular task, they do so\nhierarchically: splitting higher-level tasks into smaller sub-tasks. However,\nin the literature on natural language (NL) command of situated agents, most\nworks have treated the procedures to be executed as flat sequences of simple\nactions, or any hierarchies of procedures have been shallow at best. In this\npaper, we propose a formalism of procedures as programs, a powerful yet\nintuitive method of representing hierarchical procedural knowledge for agent\ncommand and control. We further propose a modeling paradigm of hierarchical\nmodular networks, which consist of a planner and reactors that convert NL\nintents to predictions of executable programs and probe the environment for\ninformation necessary to complete the program execution. We instantiate this\nframework on the IQA and ALFRED datasets for NL instruction following. Our\nmodel outperforms reactive baselines by a large margin on both datasets. We\nalso demonstrate that our framework is more data-efficient, and that it allows\nfor fast iterative development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attacking Open-domain Question Answering by Injecting Misinformation. (arXiv:2110.07803v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07803","description":"<p>With a rise in false, inaccurate, and misleading information in propaganda,\nnews, and social media, real-world Question Answering (QA) systems face the\nchallenges of synthesizing and reasoning over misinformation-polluted contexts\nto derive correct answers. This urgency gives rise to the need to make QA\nsystems robust to misinformation, a topic previously unexplored. We study the\nrisk of misinformation to QA models by investigating the sensitivity of\nopen-domain QA models to corpus pollution with misinformation documents. We\ncurate both human-written and model-generated false documents that we inject\ninto the evidence corpus of QA models and assess the impact on the performance\nof these systems. Experiments show that QA models are vulnerable to even small\namounts of evidence contamination brought by misinformation, with large\nabsolute performance drops on all models. Misinformation attack brings more\nthreat when fake documents are produced at scale by neural models or the\nattacker targets hacking specific questions of interest. To defend against such\na threat, we discuss the necessity of building a misinformation-aware QA system\nthat integrates question-answering and misinformation detection in a joint\nfashion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liangming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of Hybrid ASR Systems for Low Resource Medical Domain Conversational Telephone Speech. (arXiv:2210.13397v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13397","description":"<p>Language barriers present a great challenge in our increasingly connected and\nglobal world. Especially within the medical domain, e.g. hospital or emergency\nroom, communication difficulties and delays may lead to malpractice and\nnon-optimal patient care. In the HYKIST project, we consider patient-physician\ncommunication, more specifically between a German-speaking physician and an\nArabic- or Vietnamese-speaking patient. Currently, a doctor can call the\nTriaphon service to get assistance from an interpreter in order to help\nfacilitate communication. The HYKIST goal is to support the usually\nnon-professional bilingual interpreter with an automatic speech translation\nsystem to improve patient care and help overcome language barriers. In this\nwork, we present our ASR system development efforts for this conversational\ntelephone speech translation task in the medical domain for two languages\npairs, data collection, various acoustic model architectures and\ndialect-induced difficulties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luscher_C/0/1/0/all/0/1\">Christoph L&#xfc;scher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zijian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieting_P/0/1/0/all/0/1\">Peter Vieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Duc_K/0/1/0/all/0/1\">Khai Le-Duc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Triplet Extraction by Template Infilling. (arXiv:2212.10708v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10708","description":"<p>The task of triplet extraction aims to extract pairs of entities and their\ncorresponding relations from unstructured text. Most existing methods train an\nextraction model on training data involving specific target relations, and are\nincapable of extracting new relations that were not observed at training time.\nGeneralizing the model to unseen relations typically requires fine-tuning on\nsynthetic training data which is often noisy and unreliable. We show that by\nreducing triplet extraction to a template infilling task over a pre-trained\nlanguage model (LM), we can equip the extraction model with zero-shot learning\ncapabilities and eliminate the need for additional training data. We propose a\nnovel framework, ZETT (ZEro-shot Triplet extraction by Template infilling),\nthat aligns the task objective to the pre-training objective of generative\ntransformers to generalize to unseen relations. Experiments on FewRel and\nWiki-ZSL datasets demonstrate that ZETT shows consistent and stable\nperformance, outperforming previous state-of-the-art methods, even when using\nautomatically generated templates. https://github.com/megagonlabs/zett/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Bosung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1\">Hayate Iso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1\">Nikita Bhutani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1\">Estevam Hruschka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashole_N/0/1/0/all/0/1\">Ndapa Nakashole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1\">Tom Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing And Improving Neural Speaker Embeddings for ASR. (arXiv:2301.04571v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.04571","description":"<p>Neural speaker embeddings encode the speaker's speech characteristics through\na DNN model and are prevalent for speaker verification tasks. However, few\nstudies have investigated the usage of neural speaker embeddings for an ASR\nsystem. In this work, we present our efforts w.r.t integrating neural speaker\nembeddings into a conformer based hybrid HMM ASR system. For ASR, our improved\nembedding extraction pipeline in combination with the Weighted-Simple-Add\nintegration method results in x-vector and c-vector reaching on par performance\nwith i-vectors. We further compare and analyze different speaker embeddings. We\npresent our acoustic model improvements obtained by switching from newbob\nlearning rate schedule to one cycle learning schedule resulting in a ~3%\nrelative WER reduction on Switchboard, additionally reducing the overall\ntraining time by 17%. By further adding neural speaker embeddings, we gain\nadditional ~3% relative WER improvement on Hub5'00. Our best Conformer-based\nhybrid ASR system with speaker embeddings achieves 9.0% WER on Hub5'00 and\nHub5'01 with training on SWB 300h.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luscher_C/0/1/0/all/0/1\">Christoph L&#xfc;scher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the Question Answering Performance of the GPT LLM Family. (arXiv:2303.07992v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.07992","description":"<p>ChatGPT is a powerful large language model (LLM) that covers knowledge\nresources such as Wikipedia and supports natural language question answering\nusing its own knowledge. Therefore, there is growing interest in exploring\nwhether ChatGPT can replace traditional knowledge-based question answering\n(KBQA) models. Although there have been some works analyzing the question\nanswering performance of ChatGPT, there is still a lack of large-scale,\ncomprehensive testing of various types of complex questions to analyze the\nlimitations of the model. In this paper, we present a framework that follows\nthe black-box testing specifications of CheckList proposed by Ribeiro et. al.\nWe evaluate ChatGPT and its family of LLMs on eight real-world KB-based complex\nquestion answering datasets, which include six English datasets and two\nmultilingual datasets. The total number of test cases is approximately 190,000.\nIn addition to the GPT family of LLMs, we also evaluate the well-known FLAN-T5\nto identify commonalities between the GPT family and other LLMs. The dataset\nand code are available at\nhttps://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yiming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_D/0/1/0/all/0/1\">Dehai Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1\">Nan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MasakhaNEWS: News Topic Classification for African languages. (arXiv:2304.09972v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.09972","description":"<p>African languages are severely under-represented in NLP research due to lack\nof datasets covering several NLP tasks. While there are individual language\nspecific datasets that are being expanded to different tasks, only a handful of\nNLP tasks (e.g. named entity recognition and machine translation) have\nstandardized benchmark datasets covering several geographical and\ntypologically-diverse African languages. In this paper, we develop MasakhaNEWS\n-- a new benchmark dataset for news topic classification covering 16 languages\nwidely spoken in Africa. We provide an evaluation of baseline models by\ntraining classical machine learning models and fine-tuning several language\nmodels. Furthermore, we explore several alternatives to full fine-tuning of\nlanguage models that are better suited for zero-shot and few-shot learning such\nas cross-lingual parameter-efficient fine-tuning (like MAD-X), pattern\nexploiting training (PET), prompting language models (like ChatGPT), and\nprompt-free sentence transformer fine-tuning (SetFit and Cohere Embedding API).\nOur evaluation in zero-shot setting shows the potential of prompting ChatGPT\nfor news topic classification in low-resource African languages, achieving an\naverage performance of 70 F1 points without leveraging additional supervision\nlike MAD-X. In few-shot setting, we show that with as little as 10 examples per\nlabel, we achieved more than 90\\% (i.e. 86.0 F1 points) of the performance of\nfull supervised training (92.6 F1 points) leveraging the PET approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masiak_M/0/1/0/all/0/1\">Marek Masiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azime_I/0/1/0/all/0/1\">Israel Abebe Azime</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonja_A/0/1/0/all/0/1\">Atnafu Lambebo Tonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mwase_C/0/1/0/all/0/1\">Christine Mwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogundepo_O/0/1/0/all/0/1\">Odunayo Ogundepo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oladipo_A/0/1/0/all/0/1\">Akintunde Oladipo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nixdorf_D/0/1/0/all/0/1\">Doreen Nixdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+al_azzawi_s/0/1/0/all/0/1\">sana al-azzawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sibanda_B/0/1/0/all/0/1\">Blessing Sibanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_D/0/1/0/all/0/1\">Davis David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ndolela_L/0/1/0/all/0/1\">Lolwethu Ndolela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukiibi_J/0/1/0/all/0/1\">Jonathan Mukiibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajayi_T/0/1/0/all/0/1\">Tunde Ajayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moteu_T/0/1/0/all/0/1\">Tatiana Moteu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odhiambo_B/0/1/0/all/0/1\">Brian Odhiambo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owodunni_A/0/1/0/all/0/1\">Abraham Owodunni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obiefuna_N/0/1/0/all/0/1\">Nnaemeka Obiefuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Muhidin Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ababu_T/0/1/0/all/0/1\">Teshome Mulugeta Ababu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salahudeen_S/0/1/0/all/0/1\">Saheed Abdullahi Salahudeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yigezu_M/0/1/0/all/0/1\">Mesay Gemeda Yigezu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwadabe_T/0/1/0/all/0/1\">Tajuddeen Gwadabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taye_M/0/1/0/all/0/1\">Mahlet Taye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awoyomi_O/0/1/0/all/0/1\">Oluwabusayo Awoyomi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shode_I/0/1/0/all/0/1\">Iyanuoluwa Shode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_T/0/1/0/all/0/1\">Tolulope Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulganiyu_H/0/1/0/all/0/1\">Habiba Abdulganiyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omotayo_A/0/1/0/all/0/1\">Abdul-Hakeem Omotayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeeko_A/0/1/0/all/0/1\">Adetola Adeeko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afolabi_A/0/1/0/all/0/1\">Abeeb Afolabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aremu_A/0/1/0/all/0/1\">Anuoluwapo Aremu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samuel_O/0/1/0/all/0/1\">Olanrewaju Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siro_C/0/1/0/all/0/1\">Clemencia Siro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimotho_W/0/1/0/all/0/1\">Wangari Kimotho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogbu_O/0/1/0/all/0/1\">Onyekachi Ogbu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mbonu_C/0/1/0/all/0/1\">Chinedu Mbonu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chukwuneke_C/0/1/0/all/0/1\">Chiamaka Chukwuneke</a>, et al. (22 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot Setting and Performance Boosting Through Prompts. (arXiv:2305.12477v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12477","description":"<p>Large Language Models (LLMs) have exhibited remarkable performance on various\nNatural Language Processing (NLP) tasks. However, there is a current hot debate\nregarding their reasoning capacity. In this paper, we examine the performance\nof GPT-3.5, GPT-4, and BARD models, by performing a thorough technical\nevaluation on different reasoning tasks across eleven distinct datasets. Our\npaper provides empirical evidence showcasing the superior performance of\nChatGPT-4 in comparison to both ChatGPT-3.5 and BARD in zero-shot setting\nthroughout almost all evaluated tasks. While the superiority of GPT-4 compared\nto GPT-3.5 might be explained by its larger size and NLP efficiency, this was\nnot evident for BARD. We also demonstrate that the three models show limited\nproficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks. To\nbolster our findings, we present a detailed and comprehensive analysis of the\nresults from these three models. Furthermore, we propose a set of engineered\nprompts that enhances the zero-shot setting performance of all three models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Espejel_J/0/1/0/all/0/1\">Jessica L&#xf3;pez Espejel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ettifouri_E/0/1/0/all/0/1\">El Hassane Ettifouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alassan_M/0/1/0/all/0/1\">Mahaman Sanoussi Yahaya Alassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chouham_E/0/1/0/all/0/1\">El Mehdi Chouham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahhane_W/0/1/0/all/0/1\">Walid Dahhane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Data Augmentation for Document-grounded Dialog Systems in Low Resource Languages. (arXiv:2305.14949v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14949","description":"<p>This paper proposes a framework to address the issue of data scarcity in\nDocument-Grounded Dialogue Systems(DGDS). Our model leverages high-resource\nlanguages to enhance the capability of dialogue generation in low-resource\nlanguages. Specifically, We present a novel pipeline CLEM (Cross-Lingual\nEnhanced Model) including adversarial training retrieval (Retriever and\nRe-ranker), and Fid (fusion-in-decoder) generator. To further leverage\nhigh-resource language, we also propose an innovative architecture to conduct\nalignment across different languages with translated training. Extensive\nexperiment results demonstrate the effectiveness of our model and we achieved\n4th place in the DialDoc 2023 Competition. Therefore, CLEM can serve as a\nsolution to resource scarcity in DGDS and provide useful guidance for\nmulti-lingual alignment tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gou_Q/0/1/0/all/0/1\">Qi Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zehua Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wenzhe Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concept-Oriented Deep Learning with Large Language Models. (arXiv:2306.17089v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.17089","description":"<p>Large Language Models (LLMs) have been successfully used in many\nnatural-language tasks and applications including text generation and AI\nchatbots. They also are a promising new technology for concept-oriented deep\nlearning (CODL). However, the prerequisite is that LLMs understand concepts and\nensure conceptual consistency. We discuss these in this paper, as well as major\nuses of LLMs for CODL including concept extraction from text, concept graph\nextraction from text, and concept learning. Human knowledge consists of both\nsymbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only\nLLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal\nLLMs, on the other hand, are capable of representing the full range (conceptual\nand sensory) of human knowledge. We discuss conceptual understanding in\nvisual-language LLMs, the most important multimodal LLMs, and major uses of\nthem for CODL including concept extraction from image, concept graph extraction\nfrom image, and concept learning. While uses of LLMs for CODL are valuable\nstandalone, they are particularly valuable as part of LLM applications such as\nAI chatbots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Daniel T. Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Understand and Can be Enhanced by Emotional Stimuli. (arXiv:2307.11760v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.11760","description":"<p>Emotional intelligence significantly impacts our daily behaviors and\ninteractions. Although Large Language Models (LLMs) are increasingly viewed as\na stride toward artificial general intelligence, exhibiting impressive\nperformance in numerous tasks, it is still uncertain if LLMs can genuinely\ngrasp psychological emotional stimuli. Understanding and responding to\nemotional cues gives humans a distinct advantage in problem-solving. In this\npaper, we take the first step towards exploring the ability of LLMs to\nunderstand emotional stimuli. To this end, we first conduct automatic\nexperiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna,\nLlama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative\napplications that represent comprehensive evaluation scenarios. Our automatic\nexperiments show that LLMs have a grasp of emotional intelligence, and their\nperformance can be improved with emotional prompts (which we call\n\"EmotionPrompt\" that combines the original prompt with emotional stimuli),\ne.g., 8.00% relative performance improvement in Instruction Induction and 115%\nin BIG-Bench. In addition to those deterministic tasks that can be\nautomatically evaluated using existing metrics, we conducted a human study with\n106 participants to assess the quality of generative tasks using both vanilla\nand emotional prompts. Our human study results demonstrate that EmotionPrompt\nsignificantly boosts the performance of generative tasks (10.9% average\nimprovement in terms of performance, truthfulness, and responsibility metrics).\nWe provide an in-depth discussion regarding why EmotionPrompt works for LLMs\nand the factors that may influence its performance. We posit that EmotionPrompt\nheralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs\ninteraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kaijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1\">Jianxun Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MT4CrossOIE: Multi-stage Tuning for Cross-lingual Open Information Extraction. (arXiv:2308.06552v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.06552","description":"<p>Cross-lingual open information extraction aims to extract structured\ninformation from raw text across multiple languages. Previous work uses a\nshared cross-lingual pre-trained model to handle the different languages but\nunderuses the potential of the language-specific representation. In this paper,\nwe propose an effective multi-stage tuning framework called MT4CrossIE,\ndesigned for enhancing cross-lingual open information extraction by injecting\nlanguage-specific knowledge into the shared model. Specifically, the\ncross-lingual pre-trained model is first tuned in a shared semantic space\n(e.g., embedding matrix) in the fixed encoder and then other components are\noptimized in the second stage. After enough training, we freeze the pre-trained\nmodel and tune the multiple extra low-rank language-specific modules using\nmixture-of-LoRAs for model-based cross-lingual transfer. In addition, we\nleverage two-stage prompting to encourage the large language model (LLM) to\nannotate the multi-lingual raw data for data-based cross-lingual transfer. The\nmodel is trained with multi-lingual objectives on our proposed dataset\nOpenIE4++ by combing the model-based and data-based transfer techniques.\nExperimental results on various benchmarks emphasize the importance of\naggregating multiple plug-in-and-play language-specific modules and demonstrate\nthe effectiveness of MT4CrossIE in cross-lingual\nOIE\\footnote{\\url{https://github.com/CSJianYang/Multilingual-Multimodal-NLP}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tongliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1\">Linzheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaqi Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuwei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liqun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+el_abidine_H/0/1/0/all/0/1\">Hebboul Zine el-abidine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v3 [cs.DB] UPDATED)","link":"http://arxiv.org/abs/2308.15363","description":"<p>Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL\ntask. However, the absence of a systematical benchmark inhibits the development\nof designing effective, efficient and economic LLM-based Text-to-SQL solutions.\nTo address this challenge, in this paper, we first conduct a systematical and\nextensive comparison over existing prompt engineering methods, including\nquestion representation, example selection and example organization, and with\nthese experimental results, we elaborate their pros and cons. Based on these\nfindings, we propose a new integrated solution, named DAIL-SQL, which refreshes\nthe Spider leaderboard with 86.6% execution accuracy and sets a new bar. To\nexplore the potential of open-source LLM, we investigate them in various\nscenarios, and further enhance their performance with supervised fine-tuning.\nOur explorations highlight open-source LLMs' potential in Text-to-SQL, as well\nas the advantages and disadvantages of the supervised fine-tuning.\nAdditionally, towards an efficient and economic LLM-based Text-to-SQL solution,\nwe emphasize the token efficiency in prompt engineering and compare the prior\nstudies under this metric. We hope that our work provides a deeper\nunderstanding of Text-to-SQL with LLMs, and inspires further investigations and\nbroad applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dawei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haibin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yichen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bolin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptASR for contextualized ASR with controllable style. (arXiv:2309.07414v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2309.07414","description":"<p>Prompts are crucial to large language models as they provide context\ninformation such as topic or logical relationships. Inspired by this, we\npropose PromptASR, a framework that integrates prompts in end-to-end automatic\nspeech recognition (E2E ASR) systems to achieve contextualized ASR with\ncontrollable style of transcriptions. Specifically, a dedicated text encoder\nencodes the text prompts and the encodings are injected into the speech encoder\nby cross-attending the features from two modalities. When using the ground\ntruth text from preceding utterances as content prompt, the proposed system\nachieves 21.9% and 6.8% relative word error rate reductions on a book reading\ndataset and an in-house dataset compared to a baseline ASR system. The system\ncan also take word-level biasing lists as prompt to improve recognition\naccuracy on rare words. An additional style prompt can be given to the text\nencoder and guide the ASR system to output different styles of transcriptions.\nThe code is available at icefall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_W/0/1/0/all/0/1\">Wei Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_Z/0/1/0/all/0/1\">Zengwei Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yifan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1\">Liyong Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuang_F/0/1/0/all/0/1\">Fangjun Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Long Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Povey_D/0/1/0/all/0/1\">Daniel Povey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level. (arXiv:2309.08182v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.08182","description":"<p>Our work demonstrates that large language model (LLM) pre-trained on texts\ncan not only solve pure math word problems, but also physics word problems,\nwhose solution requires calculation and inference based on prior physical\nknowledge. We collect and annotate the first physics word problem\ndataset-PhysQA, which contains over 1000 junior high school physics word\nproblems (covering Kinematics, Mass&amp;Density, Mechanics, Heat, Electricity).\nThen we use OpenAI' s GPT3.5 to generate the answer of these problems and found\nthat GPT3.5 could automatically solve 49.3% of the problems through zero-shot\nlearning and 73.2% through few-shot learning. This result demonstrates that by\nusing similar problems and their answers as prompt, LLM could solve elementary\nphysics word problems approaching human level performance. In addition to\nsolving problems, GPT3.5 can also summarize the knowledge or topics covered by\nthe problems, provide relevant explanations, and generate new physics word\nproblems based on the input. Our work is the first research to focus on the\nautomatic solving, explanation, and generation of physics word problems across\nvarious types and scenarios, and we achieve an acceptable and state-of-the-art\naccuracy. This underscores the potential of LLMs for further applications in\nsecondary education.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jingzhe Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cen_Y/0/1/0/all/0/1\">Yan Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xinyuan Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Baichuan 2: Open Large-scale Language Models. (arXiv:2309.10305v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.10305","description":"<p>Large language models (LLMs) have demonstrated remarkable performance on a\nvariety of natural language tasks based on just a few examples of natural\nlanguage instructions, reducing the need for extensive feature engineering.\nHowever, most powerful LLMs are closed-source or limited in their capability\nfor languages other than English. In this technical report, we present Baichuan\n2, a series of large-scale multilingual language models containing 7 billion\nand 13 billion parameters, trained from scratch, on 2.6 trillion tokens.\nBaichuan 2 matches or outperforms other open-source models of similar size on\npublic benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan\n2 excels in vertical domains such as medicine and law. We will release all\npre-training model checkpoints to benefit the research community in better\nunderstanding the training dynamics of Baichuan 2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Borong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1\">Ce Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Chao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chenxu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_D/0/1/0/all/0/1\">Da Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">Dong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1\">Fei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_G/0/1/0/all/0/1\">Guangwei Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guosheng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haoze Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongda Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jiaming Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">JunTao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1\">Kun Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Lei Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Liang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ru_L/0/1/0/all/0/1\">Liyun Ru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Luyao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mickel Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">MingAn Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_N/0/1/0/all/0/1\">Nuolan Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Peidong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruiyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangrong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaochuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_X/0/1/0/all/0/1\">Xin Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xuehai Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yanjun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiding Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Youxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuchen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yupeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zenan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiying Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation. (arXiv:2309.10326v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.10326","description":"<p>Recent years have witnessed the success of question answering (QA),\nespecially its potential to be a foundation paradigm for tackling diverse NLP\ntasks. However, obtaining sufficient data to build an effective and stable QA\nsystem still remains an open problem. For this problem, we introduce an\niterative bootstrapping framework for QA data augmentation (named QASnowball),\nwhich can iteratively generate large-scale high-quality QA data based on a seed\nset of supervised examples. Specifically, QASnowball consists of three modules,\nan answer extractor to extract core phrases in unlabeled documents as candidate\nanswers, a question generator to generate questions based on documents and\ncandidate answers, and a QA data filter to filter out high-quality QA data.\nMoreover, QASnowball can be self-enhanced by reseeding the seed set to\nfine-tune itself in different iterations, leading to continual improvements in\nthe generation quality. We conduct experiments in the high-resource English\nscenario and the medium-resource Chinese scenario, and the experimental results\nshow that the data generated by QASnowball can facilitate QA models: (1)\ntraining models on the generated data achieves comparable results to using\nsupervised data, and (2) pre-training on the generated data and fine-tuning on\nsupervised data can achieve better performance. Our code and generated data\nwill be released to advance further work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kunlun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shihao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guoyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Unified Controllable Text Generation via Regular Expression Instruction. (arXiv:2309.10447v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.10447","description":"<p>Controllable text generation is a fundamental aspect of natural language\ngeneration, with numerous methods proposed for different constraint types.\nHowever, these approaches often require significant architectural or decoding\nmodifications, making them challenging to apply to additional constraints or\nresolve different constraint combinations. To address this, our paper\nintroduces Regular Expression Instruction (REI), which utilizes an\ninstruction-based mechanism to fully exploit regular expressions' advantages to\nuniformly model diverse constraints. Specifically, our REI supports all popular\nfine-grained controllable generation constraints, i.e., lexical, positional,\nand length, as well as their complex combinations, via regular expression-style\ninstructions. Our method only requires fine-tuning on medium-scale language\nmodels or few-shot, in-context learning on large language models, and requires\nno further adjustment when applied to various constraint combinations.\nExperiments demonstrate that our straightforward approach yields high success\nrates and adaptability to various constraints while maintaining competitiveness\nin automatic metrics and outperforming most previous baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages. (arXiv:2309.10661v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.10661","description":"<p>Democratizing access to natural language processing (NLP) technology is\ncrucial, especially for underrepresented and extremely low-resource languages.\nPrevious research has focused on developing labeled and unlabeled corpora for\nthese languages through online scraping and document translation. While these\nmethods have proven effective and cost-efficient, we have identified\nlimitations in the resulting corpora, including a lack of lexical diversity and\ncultural relevance to local communities. To address this gap, we conduct a case\nstudy on Indonesian local languages. We compare the effectiveness of online\nscraping, human translation, and paragraph writing by native speakers in\nconstructing datasets. Our findings demonstrate that datasets generated through\nparagraph writing by native speakers exhibit superior quality in terms of\nlexical diversity and cultural content. In addition, we present the\n\\datasetname{} benchmark, encompassing 12 underrepresented and extremely\nlow-resource languages spoken by millions of individuals in Indonesia. Our\nempirical experiment results using existing multilingual large language models\nconclude the need to extend these models to more underrepresented languages. We\nrelease the NusaWrites dataset at https://github.com/IndoNLP/nusa-writes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1\">Fajri Koto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adhista_D/0/1/0/all/0/1\">Dea Adhista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_E/0/1/0/all/0/1\">Emmanuel Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktavianti_S/0/1/0/all/0/1\">Sarah Oktavianti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbar_S/0/1/0/all/0/1\">Salsabil Maulana Akbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jhonson Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shadieq_N/0/1/0/all/0/1\">Nuur Shadieq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cenggoro_T/0/1/0/all/0/1\">Tjeng Wawan Cenggoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linuwih_H/0/1/0/all/0/1\">Hanung Wahyuning Linuwih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muridan_G/0/1/0/all/0/1\">Galih Pradipta Muridan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeljadi_D/0/1/0/all/0/1\">David Moeljadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1\">Ayu Purwarianti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MelodyGLM: Multi-task Pre-training for Symbolic Melody Generation. (arXiv:2309.10738v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2309.10738","description":"<p>Pre-trained language models have achieved impressive results in various music\nunderstanding and generation tasks. However, existing pre-training methods for\nsymbolic melody generation struggle to capture multi-scale, multi-dimensional\nstructural information in note sequences, due to the domain knowledge\ndiscrepancy between text and music. Moreover, the lack of available large-scale\nsymbolic melody datasets limits the pre-training improvement. In this paper, we\npropose MelodyGLM, a multi-task pre-training framework for generating melodies\nwith long-term structure. We design the melodic n-gram and long span sampling\nstrategies to create local and global blank infilling tasks for modeling the\nlocal and global structures in melodies. Specifically, we incorporate pitch\nn-grams, rhythm n-grams, and their combined n-grams into the melodic n-gram\nblank infilling tasks for modeling the multi-dimensional structures in\nmelodies. To this end, we have constructed a large-scale symbolic melody\ndataset, MelodyNet, containing more than 0.4 million melody pieces. MelodyNet\nis utilized for large-scale pre-training and domain-specific n-gram lexicon\nconstruction. Both subjective and objective evaluations demonstrate that\nMelodyGLM surpasses the standard and previous pre-training methods. In\nparticular, subjective evaluations show that, on the melody continuation task,\nMelodyGLM gains average improvements of 0.82, 0.87, 0.78, and 0.94 in\nconsistency, rhythmicity, structure, and overall quality, respectively.\nNotably, MelodyGLM nearly matches the quality of human-composed melodies on the\nmelody inpainting task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinda Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhijie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kejun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiaxing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tieyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lingyun Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. (arXiv:2309.10400v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2309.10400","description":"<p>In this paper, we introduce Positional Skip-wisE (PoSE) training for\nefficient adaptation of large language models~(LLMs) to extremely long context\nwindows. PoSE decouples train length from target context window size by\nsimulating long inputs using a fixed context window with manipulated position\nindices during training. Concretely, we select several short chunks from a long\ninput sequence, and introduce distinct skipping bias terms to modify the\nposition indices of each chunk. These bias terms, along with the length of each\nchunk, are altered for each training example, allowing the model to adapt to\nall positions within the target context window without training on full length\ninputs. Experiments show that, compared with fine-tuning on the full length,\nPoSE greatly reduces memory and time overhead with minimal impact on\nperformance. Leveraging this advantage, we have successfully extended the LLaMA\nmodel to 128k tokens. Furthermore, we empirically confirm that PoSE is\ncompatible with all RoPE-based LLMs and various position interpolation\nstrategies. Notably, by decoupling fine-tuning length from target context\nwindow, PoSE can theoretically extend the context window infinitely,\nconstrained only by memory usage for inference. With ongoing advancements for\nefficient inference, we believe PoSE holds great promise for scaling the\ncontext window even further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dawei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-09-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}